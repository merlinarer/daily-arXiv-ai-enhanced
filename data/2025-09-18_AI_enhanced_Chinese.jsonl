{"id": "2509.13480", "pdf": "https://arxiv.org/pdf/2509.13480", "abs": "https://arxiv.org/abs/2509.13480", "authors": ["Andrea Piergentili", "Beatrice Savoldi", "Matteo Negri", "Luisa Bentivogli"], "title": "Gender-Neutral Rewriting in Italian: Models, Approaches, and Trade-offs", "categories": ["cs.CL"], "comment": "Accepted at CLiC-it 2025", "summary": "Gender-neutral rewriting (GNR) aims to reformulate text to eliminate\nunnecessary gender specifications while preserving meaning, a particularly\nchallenging task in grammatical-gender languages like Italian. In this work, we\nconduct the first systematic evaluation of state-of-the-art large language\nmodels (LLMs) for Italian GNR, introducing a two-dimensional framework that\nmeasures both neutrality and semantic fidelity to the input. We compare\nfew-shot prompting across multiple LLMs, fine-tune selected models, and apply\ntargeted cleaning to boost task relevance. Our findings show that open-weight\nLLMs outperform the only existing model dedicated to GNR in Italian, whereas\nour fine-tuned models match or exceed the best open-weight LLM's performance at\na fraction of its size. Finally, we discuss the trade-off between optimizing\nthe training data for neutrality and meaning preservation.", "AI": {"tldr": "\u672c\u6587\u9996\u6b21\u7cfb\u7edf\u8bc4\u4f30\u4e86\u610f\u5927\u5229\u8bed\u6027\u522b\u4e2d\u6027\u91cd\u5199\u4efb\u52a1\uff0c\u63d0\u51fa\u4e86\u8861\u91cf\u4e2d\u7acb\u6027\u548c\u8bed\u4e49\u4fdd\u771f\u5ea6\u7684\u4e8c\u7ef4\u6846\u67b6\uff0c\u53d1\u73b0\u5f00\u6e90LLMs\u4f18\u4e8e\u73b0\u6709\u4e13\u7528\u6a21\u578b\uff0c\u5fae\u8c03\u540e\u7684\u5c0f\u6a21\u578b\u6027\u80fd\u53ef\u5ab2\u7f8e\u5927\u578bLLMs\u3002", "motivation": "\u610f\u5927\u5229\u8bed\u7b49\u8bed\u6cd5\u6027\u522b\u8bed\u8a00\u4e2d\u7684\u6027\u522b\u4e2d\u6027\u91cd\u5199\u5177\u6709\u6311\u6218\u6027\uff0c\u9700\u8981\u6d88\u9664\u4e0d\u5fc5\u8981\u7684\u6027\u522b\u6307\u5b9a\u540c\u65f6\u4fdd\u6301\u8bed\u4e49\u5b8c\u6574\u6027\u3002\u76ee\u524d\u7f3a\u4e4f\u5bf9\u8be5\u4efb\u52a1\u7684\u7cfb\u7edf\u6027\u8bc4\u4f30\u3002", "method": "\u91c7\u7528\u5c11\u6837\u672c\u63d0\u793a\u6bd4\u8f83\u591a\u4e2aLLMs\uff0c\u5bf9\u9009\u5b9a\u6a21\u578b\u8fdb\u884c\u5fae\u8c03\uff0c\u5e76\u5e94\u7528\u9488\u5bf9\u6027\u6570\u636e\u6e05\u7406\u63d0\u5347\u4efb\u52a1\u76f8\u5173\u6027\u3002\u5efa\u7acb\u4e8c\u7ef4\u8bc4\u4f30\u6846\u67b6\uff08\u4e2d\u7acb\u6027\u548c\u8bed\u4e49\u4fdd\u771f\u5ea6\uff09\u3002", "result": "\u5f00\u6e90\u6743\u91cdLLMs\u4f18\u4e8e\u73b0\u6709\u7684\u610f\u5927\u5229\u8bedGNR\u4e13\u7528\u6a21\u578b\u3002\u5fae\u8c03\u540e\u7684\u5c0f\u578b\u6a21\u578b\u4ee5\u6781\u5c0f\u7684\u53c2\u6570\u91cf\u8fbe\u5230\u6216\u8d85\u8fc7\u6700\u4f73\u5f00\u6e90LLM\u7684\u6027\u80fd\u3002", "conclusion": "\u7814\u7a76\u8868\u660eLLMs\u5728\u6027\u522b\u4e2d\u6027\u91cd\u5199\u4efb\u52a1\u4e0a\u8868\u73b0\u4f18\u5f02\uff0c\u5fae\u8c03\u7b56\u7565\u6709\u6548\uff0c\u4f46\u5728\u4f18\u5316\u8bad\u7ec3\u6570\u636e\u65f6\u9700\u8981\u6743\u8861\u4e2d\u7acb\u6027\u548c\u610f\u4e49\u4fdd\u6301\u7684\u5e73\u8861\u3002", "relevance": 65.0}}
{"id": "2509.13539", "pdf": "https://arxiv.org/pdf/2509.13539", "abs": "https://arxiv.org/abs/2509.13539", "authors": ["Alisa Kanganis", "Katherine A. Keith"], "title": "Op-Fed: Opinion, Stance, and Monetary Policy Annotations on FOMC Transcripts Using Active Learning", "categories": ["cs.CL"], "comment": null, "summary": "The U.S. Federal Open Market Committee (FOMC) regularly discusses and sets\nmonetary policy, affecting the borrowing and spending decisions of millions of\npeople. In this work, we release Op-Fed, a dataset of 1044 human-annotated\nsentences and their contexts from FOMC transcripts. We faced two major\ntechnical challenges in dataset creation: imbalanced classes -- we estimate\nfewer than 8% of sentences express a non-neutral stance towards monetary policy\n-- and inter-sentence dependence -- 65% of instances require context beyond the\nsentence-level. To address these challenges, we developed a five-stage\nhierarchical schema to isolate aspects of opinion, monetary policy, and stance\ntowards monetary policy as well as the level of context needed. Second, we\nselected instances to annotate using active learning, roughly doubling the\nnumber of positive instances across all schema aspects. Using Op-Fed, we found\na top-performing, closed-weight LLM achieves 0.80 zero-shot accuracy in opinion\nclassification but only 0.61 zero-shot accuracy classifying stance towards\nmonetary policy -- below our human baseline of 0.89. We expect Op-Fed to be\nuseful for future model training, confidence calibration, and as a seed dataset\nfor future annotation efforts.", "AI": {"tldr": "Op-Fed\u662f\u4e00\u4e2a\u5305\u542b1044\u4e2a\u6807\u6ce8\u53e5\u5b50\u7684FOMC\u4f1a\u8bae\u8bb0\u5f55\u6570\u636e\u96c6\uff0c\u7528\u4e8e\u8d27\u5e01\u653f\u7b56\u7acb\u573a\u5206\u6790\u3002\u8be5\u7814\u7a76\u89e3\u51b3\u4e86\u7c7b\u522b\u4e0d\u5e73\u8861\u548c\u53e5\u5b50\u95f4\u4f9d\u8d56\u7684\u6280\u672f\u6311\u6218\uff0c\u53d1\u73b0LLM\u5728\u8d27\u5e01\u653f\u7b56\u7acb\u573a\u5206\u7c7b\u4e0a\u8868\u73b0\u4e0d\u4f73\uff080.61\u51c6\u786e\u7387\uff09\uff0c\u4f4e\u4e8e\u4eba\u7c7b\u57fa\u51c6\uff080.89\uff09\u3002", "motivation": "\u7f8e\u8054\u50a8\u516c\u5f00\u5e02\u573a\u59d4\u5458\u4f1a(FOMC)\u7684\u8d27\u5e01\u653f\u7b56\u51b3\u5b9a\u5f71\u54cd\u6570\u767e\u4e07\u4eba\uff0c\u4f46\u7f3a\u4e4f\u9ad8\u8d28\u91cf\u6807\u6ce8\u6570\u636e\u6765\u8bad\u7ec3\u6a21\u578b\u5206\u6790\u8d27\u5e01\u653f\u7b56\u7acb\u573a\u3002\u7814\u7a76\u8005\u5e0c\u671b\u521b\u5efa\u4e13\u95e8\u7684\u6570\u636e\u96c6\u6765\u652f\u6301\u76f8\u5173NLP\u7814\u7a76\u3002", "method": "\u91c7\u7528\u4e94\u9636\u6bb5\u5206\u5c42\u6807\u6ce8\u65b9\u6848\u5206\u79bb\u89c2\u70b9\u3001\u8d27\u5e01\u653f\u7b56\u548c\u7acb\u573a\u7b49\u7ef4\u5ea6\uff1b\u4f7f\u7528\u4e3b\u52a8\u5b66\u4e60\u9009\u62e9\u6807\u6ce8\u5b9e\u4f8b\uff0c\u5c06\u6b63\u4f8b\u6570\u91cf\u7ffb\u500d\uff1b\u8bc4\u4f30LLM\u5728\u96f6\u6837\u672c\u8bbe\u7f6e\u4e0b\u7684\u8868\u73b0\u3002", "result": "\u6700\u4f73\u95ed\u6e90LLM\u5728\u89c2\u70b9\u5206\u7c7b\u4e0a\u8fbe\u52300.80\u51c6\u786e\u7387\uff0c\u4f46\u5728\u8d27\u5e01\u653f\u7b56\u7acb\u573a\u5206\u7c7b\u4e0a\u4ec50.61\uff0c\u663e\u8457\u4f4e\u4e8e\u4eba\u7c7b\u57fa\u51c60.89\u3002\u6570\u636e\u96c6\u5305\u542b1044\u4e2a\u6807\u6ce8\u53e5\u5b50\uff0c65%\u9700\u8981\u53e5\u5b50\u7ea7\u4e0a\u4e0b\u6587\u3002", "conclusion": "Op-Fed\u6570\u636e\u96c6\u53ef\u7528\u4e8e\u6a21\u578b\u8bad\u7ec3\u3001\u7f6e\u4fe1\u5ea6\u6821\u51c6\u548c\u672a\u6765\u6807\u6ce8\u5de5\u4f5c\u7684\u79cd\u5b50\u6570\u636e\u3002LLM\u5728\u590d\u6742\u91d1\u878d\u6587\u672c\u5206\u6790\u4efb\u52a1\u4e0a\u4ecd\u6709\u8f83\u5927\u63d0\u5347\u7a7a\u95f4\u3002", "relevance": 45.0}}
{"id": "2509.13569", "pdf": "https://arxiv.org/pdf/2509.13569", "abs": "https://arxiv.org/abs/2509.13569", "authors": ["John Mendon\u00e7a", "Lining Zhang", "Rahul Mallidi", "Alon Lavie", "Isabel Trancoso", "Luis Fernando D'Haro", "Jo\u00e3o Sedoc"], "title": "Overview of Dialog System Evaluation Track: Dimensionality, Language, Culture and Safety at DSTC 12", "categories": ["cs.CL"], "comment": "DSTC12 Track 1 Overview Paper. https://chateval.org/dstc12", "summary": "The rapid advancement of Large Language Models (LLMs) has intensified the\nneed for robust dialogue system evaluation, yet comprehensive assessment\nremains challenging. Traditional metrics often prove insufficient, and safety\nconsiderations are frequently narrowly defined or culturally biased. The DSTC12\nTrack 1, \"Dialog System Evaluation: Dimensionality, Language, Culture and\nSafety,\" is part of the ongoing effort to address these critical gaps. The\ntrack comprised two subtasks: (1) Dialogue-level, Multi-dimensional Automatic\nEvaluation Metrics, and (2) Multilingual and Multicultural Safety Detection.\nFor Task 1, focused on 10 dialogue dimensions, a Llama-3-8B baseline achieved\nthe highest average Spearman's correlation (0.1681), indicating substantial\nroom for improvement. In Task 2, while participating teams significantly\noutperformed a Llama-Guard-3-1B baseline on the multilingual safety subset (top\nROC-AUC 0.9648), the baseline proved superior on the cultural subset (0.5126\nROC-AUC), highlighting critical needs in culturally-aware safety. This paper\ndescribes the datasets and baselines provided to participants, as well as\nsubmission evaluation results for each of the two proposed subtasks.", "AI": {"tldr": "DSTC12 Track 1 \u63d0\u51fa\u5bf9\u8bdd\u7cfb\u7edf\u8bc4\u4f30\u65b0\u65b9\u6cd5\uff0c\u5305\u542b\u591a\u7ef4\u5ea6\u81ea\u52a8\u8bc4\u4f30\u548c\u591a\u8bed\u8a00\u6587\u5316\u5b89\u5168\u68c0\u6d4b\u4e24\u4e2a\u5b50\u4efb\u52a1\uff0c\u53d1\u73b0\u73b0\u6709\u65b9\u6cd5\u5728\u6587\u5316\u611f\u77e5\u5b89\u5168\u65b9\u9762\u5b58\u5728\u663e\u8457\u4e0d\u8db3", "motivation": "\u5927\u578b\u8bed\u8a00\u6a21\u578b\u5feb\u901f\u53d1\u5c55\u9700\u8981\u66f4\u9c81\u68d2\u7684\u5bf9\u8bdd\u7cfb\u7edf\u8bc4\u4f30\u65b9\u6cd5\uff0c\u4f20\u7edf\u6307\u6807\u4e0d\u8db3\u4e14\u5b89\u5168\u8003\u8651\u5b58\u5728\u6587\u5316\u504f\u89c1\uff0c\u9700\u8981\u89e3\u51b3\u8fd9\u4e9b\u5173\u952e\u5dee\u8ddd", "method": "\u4f7f\u7528\u4e24\u4e2a\u5b50\u4efb\u52a1\uff1a1\uff09\u5bf9\u8bdd\u7ea7\u591a\u7ef4\u5ea6\u81ea\u52a8\u8bc4\u4f30\u6307\u6807\uff0810\u4e2a\u7ef4\u5ea6\uff09\uff0c2\uff09\u591a\u8bed\u8a00\u548c\u591a\u6587\u5316\u5b89\u5168\u68c0\u6d4b\u3002\u63d0\u4f9b\u6570\u636e\u96c6\u548c\u57fa\u7ebf\u6a21\u578b\uff08Llama-3-8B\u548cLlama-Guard-3-1B\uff09", "result": "\u4efb\u52a11\u4e2dLlama-3-8B\u57fa\u7ebf\u83b7\u5f97\u6700\u9ad8\u5e73\u5747Spearman\u76f8\u5173\u7cfb\u65700.1681\uff0c\u663e\u793a\u6709\u5de8\u5927\u6539\u8fdb\u7a7a\u95f4\uff1b\u4efb\u52a12\u4e2d\u53c2\u4e0e\u56e2\u961f\u5728\u591a\u8bed\u8a00\u5b89\u5168\u5b50\u96c6\u4e0a\u663e\u8457\u4f18\u4e8e\u57fa\u7ebf\uff08\u6700\u4f73ROC-AUC 0.9648\uff09\uff0c\u4f46\u57fa\u7ebf\u5728\u6587\u5316\u5b50\u96c6\u4e0a\u8868\u73b0\u66f4\u597d\uff080.5126 ROC-AUC\uff09", "conclusion": "\u5bf9\u8bdd\u7cfb\u7edf\u8bc4\u4f30\u4ecd\u9700\u91cd\u5927\u6539\u8fdb\uff0c\u7279\u522b\u662f\u5728\u6587\u5316\u611f\u77e5\u5b89\u5168\u65b9\u9762\u5b58\u5728\u5173\u952e\u9700\u6c42\uff0c\u9700\u8981\u5f00\u53d1\u66f4\u5168\u9762\u7684\u8bc4\u4f30\u6846\u67b6", "relevance": 85.0}}
{"id": "2509.13624", "pdf": "https://arxiv.org/pdf/2509.13624", "abs": "https://arxiv.org/abs/2509.13624", "authors": ["Shambhavi Krishna", "Atharva Naik", "Chaitali Agarwal", "Sudharshan Govindan", "Taesung Lee", "Haw-Shiuan Chang"], "title": "Latent Traits and Cross-Task Transfer: Deconstructing Dataset Interactions in LLM Fine-tuning", "categories": ["cs.CL", "cs.LG"], "comment": "Camera-ready version. Accepted to appear in the proceedings of the\n  14th Joint Conference on Lexical and Computational Semantics (*SEM 2025)", "summary": "Large language models are increasingly deployed across diverse applications.\nThis often includes tasks LLMs have not encountered during training. This\nimplies that enumerating and obtaining the high-quality training data for all\ntasks is infeasible. Thus, we often need to rely on transfer learning using\ndatasets with different characteristics, and anticipate out-of-distribution\nrequests. Motivated by this practical need, we propose an analysis framework,\nbuilding a transfer learning matrix and dimensionality reduction, to dissect\nthese cross-task interactions. We train and analyze 10 models to identify\nlatent abilities (e.g., Reasoning, Sentiment Classification, NLU, Arithmetic)\nand discover the side effects of the transfer learning. Our findings reveal\nthat performance improvements often defy explanations based on surface-level\ndataset similarity or source data quality. Instead, hidden statistical factors\nof the source dataset, such as class distribution and generation length\nproclivities, alongside specific linguistic features, are actually more\ninfluential. This work offers insights into the complex dynamics of transfer\nlearning, paving the way for more predictable and effective LLM adaptation.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u4e2a\u5206\u6790\u6846\u67b6\u6765\u7814\u7a76LLM\u8de8\u4efb\u52a1\u8fc1\u79fb\u5b66\u4e60\u4e2d\u7684\u6f5c\u5728\u80fd\u529b\u548c\u526f\u4f5c\u7528\uff0c\u53d1\u73b0\u8868\u9762\u6570\u636e\u96c6\u76f8\u4f3c\u6027\u4e0d\u5982\u9690\u85cf\u7edf\u8ba1\u56e0\u7d20\u548c\u8bed\u8a00\u7279\u5f81\u5bf9\u6027\u80fd\u63d0\u5347\u7684\u5f71\u54cd\u5927", "motivation": "\u7531\u4e8e\u65e0\u6cd5\u4e3a\u6240\u6709\u4efb\u52a1\u679a\u4e3e\u9ad8\u8d28\u91cf\u8bad\u7ec3\u6570\u636e\uff0c\u9700\u8981\u4f9d\u8d56\u4e0d\u540c\u7279\u5f81\u6570\u636e\u96c6\u7684\u8fc1\u79fb\u5b66\u4e60\u6765\u5904\u7406\u5206\u5e03\u5916\u8bf7\u6c42\uff0c\u56e0\u6b64\u9700\u8981\u5206\u6790\u8de8\u4efb\u52a1\u4ea4\u4e92\u7684\u590d\u6742\u52a8\u6001", "method": "\u6784\u5efa\u8fc1\u79fb\u5b66\u4e60\u77e9\u9635\u548c\u964d\u7ef4\u5206\u6790\u6846\u67b6\uff0c\u8bad\u7ec310\u4e2a\u6a21\u578b\u6765\u8bc6\u522b\u6f5c\u5728\u80fd\u529b\uff08\u63a8\u7406\u3001\u60c5\u611f\u5206\u7c7b\u3001\u81ea\u7136\u8bed\u8a00\u7406\u89e3\u3001\u7b97\u672f\u7b49\uff09\u5e76\u53d1\u73b0\u8fc1\u79fb\u5b66\u4e60\u7684\u526f\u4f5c\u7528", "result": "\u6027\u80fd\u63d0\u5347\u5f80\u5f80\u65e0\u6cd5\u7528\u8868\u9762\u6570\u636e\u96c6\u76f8\u4f3c\u6027\u6216\u6e90\u6570\u636e\u8d28\u91cf\u89e3\u91ca\uff0c\u800c\u662f\u53d7\u6e90\u6570\u636e\u96c6\u7684\u9690\u85cf\u7edf\u8ba1\u56e0\u7d20\uff08\u5982\u7c7b\u522b\u5206\u5e03\u3001\u751f\u6210\u957f\u5ea6\u503e\u5411\uff09\u548c\u7279\u5b9a\u8bed\u8a00\u7279\u5f81\u5f71\u54cd\u66f4\u5927", "conclusion": "\u8fd9\u9879\u5de5\u4f5c\u63ed\u793a\u4e86\u8fc1\u79fb\u5b66\u4e60\u7684\u590d\u6742\u52a8\u6001\uff0c\u4e3a\u66f4\u53ef\u9884\u6d4b\u548c\u6709\u6548\u7684LLM\u9002\u5e94\u94fa\u5e73\u4e86\u9053\u8def", "relevance": 85.0}}
{"id": "2509.13338", "pdf": "https://arxiv.org/pdf/2509.13338", "abs": "https://arxiv.org/abs/2509.13338", "authors": ["Hassan Gharoun", "Mohammad Sadegh Khorshidi", "Kasra Ranjbarigderi", "Fang Chen", "Amir H. Gandomi"], "title": "Proximity-Based Evidence Retrieval for Uncertainty-Aware Neural Networks", "categories": ["cs.CV", "cs.AI", "cs.LG", "cs.NE", "68T07, 68T09"], "comment": "15 pages, 4 figures, 3 tables", "summary": "This work proposes an evidence-retrieval mechanism for uncertainty-aware\ndecision-making that replaces a single global cutoff with an\nevidence-conditioned, instance-adaptive criterion. For each test instance,\nproximal exemplars are retrieved in an embedding space; their predictive\ndistributions are fused via Dempster-Shafer theory. The resulting fused belief\nacts as a per-instance thresholding mechanism. Because the supporting evidences\nare explicit, decisions are transparent and auditable. Experiments on\nCIFAR-10/100 with BiT and ViT backbones show higher or comparable\nuncertainty-aware performance with materially fewer confidently incorrect\noutcomes and a sustainable review load compared with applying threshold on\nprediction entropy. Notably, only a few evidences are sufficient to realize\nthese gains; increasing the evidence set yields only modest changes. These\nresults indicate that evidence-conditioned tagging provides a more reliable and\ninterpretable alternative to fixed prediction entropy thresholds for\noperational uncertainty-aware decision-making.", "AI": {"tldr": "\u63d0\u51fa\u57fa\u4e8e\u8bc1\u636e\u68c0\u7d22\u7684\u4e0d\u786e\u5b9a\u6027\u611f\u77e5\u51b3\u7b56\u673a\u5236\uff0c\u901a\u8fc7\u68c0\u7d22\u8fd1\u90bb\u6837\u672c\u5e76\u878d\u5408\u5176\u9884\u6d4b\u5206\u5e03\u6765\u66ff\u4ee3\u5168\u5c40\u56fa\u5b9a\u9608\u503c\uff0c\u5b9e\u73b0\u66f4\u53ef\u9760\u548c\u53ef\u89e3\u91ca\u7684\u51b3\u7b56", "motivation": "\u4f20\u7edf\u57fa\u4e8e\u9884\u6d4b\u71b5\u7684\u5168\u5c40\u9608\u503c\u65b9\u6cd5\u5728\u4e0d\u786e\u5b9a\u6027\u611f\u77e5\u51b3\u7b56\u4e2d\u5b58\u5728\u5c40\u9650\u6027\uff0c\u9700\u8981\u66f4\u900f\u660e\u3001\u53ef\u5ba1\u8ba1\u4e14\u5b9e\u4f8b\u81ea\u9002\u5e94\u7684\u51b3\u7b56\u673a\u5236", "method": "\u4e3a\u6bcf\u4e2a\u6d4b\u8bd5\u5b9e\u4f8b\u5728\u5d4c\u5165\u7a7a\u95f4\u4e2d\u68c0\u7d22\u8fd1\u90bb\u6837\u672c\uff0c\u4f7f\u7528Dempster-Shafer\u7406\u8bba\u878d\u5408\u8fd9\u4e9b\u8bc1\u636e\u7684\u9884\u6d4b\u5206\u5e03\uff0c\u751f\u6210\u6bcf\u4e2a\u5b9e\u4f8b\u81ea\u9002\u5e94\u7684\u9608\u503c\u6807\u51c6", "result": "\u5728CIFAR-10/100\u6570\u636e\u96c6\u4e0a\uff0c\u76f8\u6bd4\u9884\u6d4b\u71b5\u9608\u503c\u65b9\u6cd5\uff0c\u53d6\u5f97\u4e86\u76f8\u5f53\u6216\u66f4\u597d\u7684\u4e0d\u786e\u5b9a\u6027\u611f\u77e5\u6027\u80fd\uff0c\u663e\u8457\u51cf\u5c11\u4e86\u9519\u8bef\u7f6e\u4fe1\u9884\u6d4b\uff0c\u540c\u65f6\u4fdd\u6301\u53ef\u6301\u7eed\u7684\u5ba1\u67e5\u8d1f\u8f7d", "conclusion": "\u8bc1\u636e\u6761\u4ef6\u6807\u8bb0\u4e3a\u64cd\u4f5c\u4e0d\u786e\u5b9a\u6027\u611f\u77e5\u51b3\u7b56\u63d0\u4f9b\u4e86\u6bd4\u56fa\u5b9a\u9884\u6d4b\u71b5\u9608\u503c\u66f4\u53ef\u9760\u548c\u53ef\u89e3\u91ca\u7684\u66ff\u4ee3\u65b9\u6848\uff0c\u4ec5\u9700\u5c11\u91cf\u8bc1\u636e\u5373\u53ef\u5b9e\u73b0\u663e\u8457\u6539\u8fdb", "relevance": 65.0}}
{"id": "2509.13425", "pdf": "https://arxiv.org/pdf/2509.13425", "abs": "https://arxiv.org/abs/2509.13425", "authors": ["Julian Evan Chrisnanto", "Yulison Herry Chrisnanto", "Ferry Faizal"], "title": "Unified Spatiotemopral Physics-Informed Learning (USPIL): A Framework for Modeling Complex Predator-Prey Dynamics", "categories": ["cs.LG", "physics.app-ph", "92D25, 35K57, 68T07", "I.2.6; J.3; G.1.8"], "comment": "20 pages, 11 figures. A preprint on using a unified physics-informed\n  neural network framework to model predator-prey dynamics", "summary": "Ecological systems exhibit complex multi-scale dynamics that challenge\ntraditional modeling. New methods must capture temporal oscillations and\nemergent spatiotemporal patterns while adhering to conservation principles. We\npresent the Unified Spatiotemporal Physics-Informed Learning (USPIL) framework,\na deep learning architecture integrating physics-informed neural networks\n(PINNs) and conservation laws to model predator-prey dynamics across\ndimensional scales. The framework provides a unified solution for both ordinary\n(ODE) and partial (PDE) differential equation systems, describing temporal\ncycles and reaction-diffusion patterns within a single neural network\narchitecture. Our methodology uses automatic differentiation to enforce physics\nconstraints and adaptive loss weighting to balance data fidelity with physical\nconsistency. Applied to the Lotka-Volterra system, USPIL achieves 98.9%\ncorrelation for 1D temporal dynamics (loss: 0.0219, MAE: 0.0184) and captures\ncomplex spiral waves in 2D systems (loss: 4.7656, pattern correlation: 0.94).\nValidation confirms conservation law adherence within 0.5% and shows a 10-50x\ncomputational speedup for inference compared to numerical solvers. USPIL also\nenables mechanistic understanding through interpretable physics constraints,\nfacilitating parameter discovery and sensitivity analysis not possible with\npurely data-driven methods. Its ability to transition between dimensional\nformulations opens new avenues for multi-scale ecological modeling. These\ncapabilities make USPIL a transformative tool for ecological forecasting,\nconservation planning, and understanding ecosystem resilience, establishing\nphysics-informed deep learning as a powerful and scientifically rigorous\nparadigm.", "AI": {"tldr": "USPIL\u6846\u67b6\u6574\u5408\u7269\u7406\u4fe1\u606f\u795e\u7ecf\u7f51\u7edc\u548c\u5b88\u6052\u5b9a\u5f8b\uff0c\u7edf\u4e00\u5efa\u6a21\u6355\u98df\u8005-\u730e\u7269\u7cfb\u7edf\u7684\u65f6\u7a7a\u52a8\u529b\u5b66\uff0c\u5728\u4fdd\u6301\u7269\u7406\u4e00\u81f4\u6027\u7684\u540c\u65f6\u5b9e\u73b0\u9ad8\u6548\u8ba1\u7b97\u548c\u673a\u5236\u89e3\u91ca\u3002", "motivation": "\u751f\u6001\u7cfb\u7edf\u7684\u590d\u6742\u591a\u5c3a\u5ea6\u52a8\u529b\u5b66\u6311\u6218\u4f20\u7edf\u5efa\u6a21\u65b9\u6cd5\uff0c\u9700\u8981\u65e2\u80fd\u6355\u6349\u65f6\u7a7a\u632f\u8361\u6a21\u5f0f\u53c8\u9075\u5b88\u5b88\u6052\u539f\u7406\u7684\u65b0\u65b9\u6cd5\u3002", "method": "\u4f7f\u7528\u7269\u7406\u4fe1\u606f\u795e\u7ecf\u7f51\u7edc(PINNs)\u548c\u81ea\u52a8\u5fae\u5206\u6280\u672f\uff0c\u901a\u8fc7\u81ea\u9002\u5e94\u635f\u5931\u6743\u91cd\u5e73\u8861\u6570\u636e\u4fdd\u771f\u5ea6\u548c\u7269\u7406\u4e00\u81f4\u6027\uff0c\u7edf\u4e00\u5904\u7406ODE\u548cPDE\u7cfb\u7edf\u3002", "result": "\u5728Lotka-Volterra\u7cfb\u7edf\u4e2d\u8fbe\u523098.9%\u76f8\u5173\u6027(\u635f\u5931:0.0219)\uff0c\u6355\u6349\u5230\u590d\u6742\u87ba\u65cb\u6ce2\u6a21\u5f0f(\u76f8\u5173\u60270.94)\uff0c\u8ba1\u7b97\u901f\u5ea6\u6bd4\u6570\u503c\u6c42\u89e3\u5668\u5feb10-50\u500d\uff0c\u5b88\u6052\u5b9a\u5f8b\u9075\u5b88\u8bef\u5dee\u57280.5%\u4ee5\u5185\u3002", "conclusion": "USPIL\u4e3a\u591a\u5c3a\u5ea6\u751f\u6001\u5efa\u6a21\u5f00\u8f9f\u4e86\u65b0\u9014\u5f84\uff0c\u5efa\u7acb\u4e86\u7269\u7406\u4fe1\u606f\u6df1\u5ea6\u5b66\u4e60\u4f5c\u4e3a\u79d1\u5b66\u4e25\u8c28\u7684\u8303\u5f0f\uff0c\u53ef\u7528\u4e8e\u751f\u6001\u9884\u6d4b\u548c\u4fdd\u62a4\u89c4\u5212\u3002", "relevance": 35.0}}
{"id": "2509.13332", "pdf": "https://arxiv.org/pdf/2509.13332", "abs": "https://arxiv.org/abs/2509.13332", "authors": ["Pratik Jayarao", "Himanshu Gupta", "Neeraj Varshney", "Chaitanya Dwivedi"], "title": "Explicit Reasoning Makes Better Judges: A Systematic Study on Accuracy, Efficiency, and Robustness", "categories": ["cs.AI", "cs.CL"], "comment": null, "summary": "As Large Language Models (LLMs) are increasingly adopted as automated judges\nin benchmarking and reward modeling, ensuring their reliability, efficiency,\nand robustness has become critical. In this work, we present a systematic\ncomparison of \"thinking\" and \"non-thinking\" LLMs in the LLM-as-a-judge paradigm\nusing open-source Qwen 3 models of relatively small sizes (0.6B, 1.7B, and 4B\nparameters). We evaluate both accuracy and computational efficiency (FLOPs) on\nRewardBench tasks, and further examine augmentation strategies for non-thinking\nmodels, including in-context learning, rubric-guided judging, reference-based\nevaluation, and n-best aggregation. Our results show that despite these\nenhancements, non-thinking models generally fall short of their thinking\ncounterparts. Our results show that thinking models achieve approximately 10%\npoints higher accuracy with little overhead (under 2x), in contrast to\naugmentation strategies like few-shot learning, which deliver modest gains at a\nhigher cost (>8x). Bias and robustness analyses further demonstrate that\nthinking models maintain significantly greater consistency under a variety of\nbias conditions such as positional, bandwagon, identity, diversity, and random\nbiases (6% higher on average). We further extend our experiments to the\nmultilingual setting and our results confirm that explicit reasoning extends\nits benefits beyond English. Overall, our work results in several important\nfindings that provide systematic evidence that explicit reasoning offers clear\nadvantages in the LLM-as-a-judge paradigm not only in accuracy and efficiency\nbut also in robustness.", "AI": {"tldr": "\u672c\u7814\u7a76\u7cfb\u7edf\u6bd4\u8f83\u4e86\u5728LLM\u4f5c\u4e3a\u88c1\u5224\u8303\u5f0f\u4e0b\"\u601d\u8003\"\u4e0e\"\u975e\u601d\u8003\"\u6a21\u578b\u7684\u8868\u73b0\uff0c\u53d1\u73b0\u601d\u8003\u6a21\u578b\u5728\u51c6\u786e\u7387\u3001\u8ba1\u7b97\u6548\u7387\u548c\u9c81\u68d2\u6027\u65b9\u9762\u5747\u4f18\u4e8e\u975e\u601d\u8003\u6a21\u578b\uff0c\u5373\u4f7f\u7ecf\u8fc7\u591a\u79cd\u589e\u5f3a\u7b56\u7565\u6539\u8fdb\u540e\u3002", "motivation": "\u968f\u7740LLM\u8d8a\u6765\u8d8a\u591a\u5730\u88ab\u7528\u4f5c\u81ea\u52a8\u88c1\u5224\u8fdb\u884c\u57fa\u51c6\u6d4b\u8bd5\u548c\u5956\u52b1\u5efa\u6a21\uff0c\u786e\u4fdd\u5176\u53ef\u9760\u6027\u3001\u6548\u7387\u548c\u9c81\u68d2\u6027\u53d8\u5f97\u81f3\u5173\u91cd\u8981\u3002\u9700\u8981\u7cfb\u7edf\u6bd4\u8f83\u601d\u8003\u4e0e\u975e\u601d\u8003\u6a21\u578b\u5728\u8fd9\u4e00\u8303\u5f0f\u4e0b\u7684\u8868\u73b0\u3002", "method": "\u4f7f\u7528\u5f00\u6e90Qwen 3\u5c0f\u89c4\u6a21\u6a21\u578b(0.6B/1.7B/4B\u53c2\u6570)\uff0c\u5728RewardBench\u4efb\u52a1\u4e0a\u8bc4\u4f30\u51c6\u786e\u6027\u548c\u8ba1\u7b97\u6548\u7387(FLOPs)\uff0c\u5e76\u6d4b\u8bd5\u4e86\u4e0a\u4e0b\u6587\u5b66\u4e60\u3001\u89c4\u5219\u5f15\u5bfc\u8bc4\u4f30\u3001\u53c2\u8003\u57fa\u51c6\u8bc4\u4f30\u548cn-best\u805a\u5408\u7b49\u589e\u5f3a\u7b56\u7565\u3002", "result": "\u601d\u8003\u6a21\u578b\u6bd4\u975e\u601d\u8003\u6a21\u578b\u51c6\u786e\u7387\u9ad8\u7ea610%\uff0c\u8ba1\u7b97\u5f00\u9500\u5c0f(\u4f4e\u4e8e2\u500d)\uff0c\u800c\u5c11\u6837\u672c\u5b66\u4e60\u7b49\u589e\u5f3a\u7b56\u7565\u6536\u76ca\u6709\u9650\u4f46\u6210\u672c\u9ad8(>8\u500d)\u3002\u5728\u4f4d\u7f6e\u3001\u4ece\u4f17\u3001\u8eab\u4efd\u3001\u591a\u6837\u6027\u548c\u968f\u673a\u504f\u89c1\u7b49\u591a\u79cd\u6761\u4ef6\u4e0b\uff0c\u601d\u8003\u6a21\u578b\u4fdd\u6301\u663e\u8457\u66f4\u9ad8\u7684\u4e00\u81f4\u6027(\u5e73\u5747\u9ad86%)\u3002\u591a\u8bed\u8a00\u5b9e\u9a8c\u4e5f\u8bc1\u5b9e\u4e86\u663e\u5f0f\u63a8\u7406\u7684\u4f18\u52bf\u3002", "conclusion": "\u663e\u5f0f\u63a8\u7406\u5728LLM\u4f5c\u4e3a\u88c1\u5224\u8303\u5f0f\u4e2d\u4e0d\u4ec5\u5728\u51c6\u786e\u6027\u548c\u6548\u7387\u65b9\u9762\uff0c\u800c\u4e14\u5728\u9c81\u68d2\u6027\u65b9\u9762\u90fd\u63d0\u4f9b\u660e\u663e\u4f18\u52bf\uff0c\u4e3a\u8fd9\u4e00\u8303\u5f0f\u63d0\u4f9b\u4e86\u7cfb\u7edf\u6027\u8bc1\u636e\u3002", "relevance": 85.0}}
{"id": "2509.13664", "pdf": "https://arxiv.org/pdf/2509.13664", "abs": "https://arxiv.org/abs/2509.13664", "authors": ["Zhuoxuan Zhang", "Jinhao Duan", "Edward Kim", "Kaidi Xu"], "title": "Sparse Neurons Carry Strong Signals of Question Ambiguity in LLMs", "categories": ["cs.CL", "cs.AI"], "comment": "To be appeared in EMNLP 2025 (main)", "summary": "Ambiguity is pervasive in real-world questions, yet large language models\n(LLMs) often respond with confident answers rather than seeking clarification.\nIn this work, we show that question ambiguity is linearly encoded in the\ninternal representations of LLMs and can be both detected and controlled at the\nneuron level. During the model's pre-filling stage, we identify that a small\nnumber of neurons, as few as one, encode question ambiguity information. Probes\ntrained on these Ambiguity-Encoding Neurons (AENs) achieve strong performance\non ambiguity detection and generalize across datasets, outperforming\nprompting-based and representation-based baselines. Layerwise analysis reveals\nthat AENs emerge from shallow layers, suggesting early encoding of ambiguity\nsignals in the model's processing pipeline. Finally, we show that through\nmanipulating AENs, we can control LLM's behavior from direct answering to\nabstention. Our findings reveal that LLMs form compact internal representations\nof question ambiguity, enabling interpretable and controllable behavior.", "AI": {"tldr": "\u7814\u7a76\u53d1\u73b0LLMs\u5728\u5185\u90e8\u8868\u5f81\u4e2d\u7ebf\u6027\u7f16\u7801\u95ee\u9898\u6a21\u7cca\u6027\uff0c\u901a\u8fc7\u5c11\u91cf\u795e\u7ecf\u5143\u5373\u53ef\u68c0\u6d4b\u548c\u63a7\u5236\u6a21\u7cca\u6027\uff0c\u4f7f\u6a21\u578b\u4ece\u76f4\u63a5\u56de\u7b54\u8f6c\u5411\u5f03\u6743", "motivation": "\u73b0\u5b9e\u95ee\u9898\u4e2d\u666e\u904d\u5b58\u5728\u6a21\u7cca\u6027\uff0c\u4f46LLMs\u5f80\u5f80\u81ea\u4fe1\u56de\u7b54\u800c\u975e\u5bfb\u6c42\u6f84\u6e05\uff0c\u9700\u8981\u7406\u89e3LLMs\u5982\u4f55\u5904\u7406\u6a21\u7cca\u6027\u95ee\u9898", "method": "\u5728\u6a21\u578b\u9884\u586b\u5145\u9636\u6bb5\u8bc6\u522b\u6a21\u7cca\u6027\u7f16\u7801\u795e\u7ecf\u5143(AENs)\uff0c\u8bad\u7ec3\u63a2\u9488\u8fdb\u884c\u6a21\u7cca\u6027\u68c0\u6d4b\uff0c\u901a\u8fc7\u795e\u7ecf\u5143\u64cd\u4f5c\u63a7\u5236\u6a21\u578b\u884c\u4e3a", "result": "\u4ec5\u97001\u4e2a\u795e\u7ecf\u5143\u5373\u53ef\u7f16\u7801\u6a21\u7cca\u6027\u4fe1\u606f\uff0c\u63a2\u9488\u6027\u80fd\u4f18\u4e8e\u57fa\u4e8e\u63d0\u793a\u548c\u8868\u5f81\u7684\u57fa\u7ebf\u65b9\u6cd5\uff0cAENs\u4ece\u6d45\u5c42\u51fa\u73b0\uff0c\u53ef\u901a\u8fc7\u64cd\u4f5c\u63a7\u5236\u6a21\u578b\u884c\u4e3a", "conclusion": "LLMs\u5f62\u6210\u7d27\u51d1\u7684\u5185\u90e8\u6a21\u7cca\u6027\u8868\u5f81\uff0c\u5b9e\u73b0\u4e86\u53ef\u89e3\u91ca\u548c\u53ef\u63a7\u7684\u884c\u4e3a", "relevance": 85.0}}
{"id": "2509.13353", "pdf": "https://arxiv.org/pdf/2509.13353", "abs": "https://arxiv.org/abs/2509.13353", "authors": ["Muhammad Adnan Shahzad"], "title": "Hybrid Quantum-Classical Model for Image Classification", "categories": ["cs.CV", "cs.AI", "cs.LG"], "comment": null, "summary": "This study presents a systematic comparison between hybrid quantum-classical\nneural networks and purely classical models across three benchmark datasets\n(MNIST, CIFAR100, and STL10) to evaluate their performance, efficiency, and\nrobustness. The hybrid models integrate parameterized quantum circuits with\nclassical deep learning architectures, while the classical counterparts use\nconventional convolutional neural networks (CNNs). Experiments were conducted\nover 50 training epochs for each dataset, with evaluations on validation\naccuracy, test accuracy, training time, computational resource usage, and\nadversarial robustness (tested with $\\epsilon=0.1$ perturbations).Key findings\ndemonstrate that hybrid models consistently outperform classical models in\nfinal accuracy, achieving {99.38\\% (MNIST), 41.69\\% (CIFAR100), and 74.05\\%\n(STL10) validation accuracy, compared to classical benchmarks of 98.21\\%,\n32.25\\%, and 63.76\\%, respectively. Notably, the hybrid advantage scales with\ndataset complexity, showing the most significant gains on CIFAR100 (+9.44\\%)\nand STL10 (+10.29\\%). Hybrid models also train 5--12$\\times$ faster (e.g.,\n21.23s vs. 108.44s per epoch on MNIST) and use 6--32\\% fewer parameters} while\nmaintaining superior generalization to unseen test data.Adversarial robustness\ntests reveal that hybrid models are significantly more resilient on simpler\ndatasets (e.g., 45.27\\% robust accuracy on MNIST vs. 10.80\\% for classical) but\nshow comparable fragility on complex datasets like CIFAR100 ($\\sim$1\\%\nrobustness for both). Resource efficiency analyses indicate that hybrid models\nconsume less memory (4--5GB vs. 5--6GB for classical) and lower CPU utilization\n(9.5\\% vs. 23.2\\% on average).These results suggest that hybrid\nquantum-classical architectures offer compelling advantages in accuracy,\ntraining efficiency, and parameter scalability, particularly for complex vision\ntasks.", "AI": {"tldr": "\u6df7\u5408\u91cf\u5b50-\u7ecf\u5178\u795e\u7ecf\u7f51\u7edc\u5728\u56fe\u50cf\u5206\u7c7b\u4efb\u52a1\u4e2d\u76f8\u6bd4\u7eaf\u7ecf\u5178\u6a21\u578b\u5c55\u73b0\u51fa\u66f4\u9ad8\u7684\u51c6\u786e\u7387\u3001\u66f4\u5feb\u7684\u8bad\u7ec3\u901f\u5ea6\u548c\u66f4\u597d\u7684\u53c2\u6570\u6548\u7387\uff0c\u7279\u522b\u662f\u5728\u590d\u6742\u6570\u636e\u96c6\u4e0a\u4f18\u52bf\u66f4\u660e\u663e", "motivation": "\u7814\u7a76\u65e8\u5728\u7cfb\u7edf\u6bd4\u8f83\u6df7\u5408\u91cf\u5b50-\u7ecf\u5178\u795e\u7ecf\u7f51\u7edc\u4e0e\u7eaf\u7ecf\u5178\u6a21\u578b\u5728\u6027\u80fd\u3001\u6548\u7387\u548c\u9c81\u68d2\u6027\u65b9\u9762\u7684\u5dee\u5f02\uff0c\u63a2\u7d22\u91cf\u5b50\u8ba1\u7b97\u5728\u6df1\u5ea6\u5b66\u4e60\u4e2d\u7684\u6f5c\u5728\u4f18\u52bf", "method": "\u5728MNIST\u3001CIFAR100\u548cSTL10\u4e09\u4e2a\u57fa\u51c6\u6570\u636e\u96c6\u4e0a\uff0c\u5c06\u53c2\u6570\u5316\u91cf\u5b50\u7535\u8def\u4e0e\u7ecf\u5178\u6df1\u5ea6\u5b66\u4e60\u67b6\u6784\u7ed3\u5408\u7684\u6df7\u5408\u6a21\u578b\u4e0e\u4f20\u7edf\u7684\u5377\u79ef\u795e\u7ecf\u7f51\u7edc\u8fdb\u884c\u5bf9\u6bd4\u5b9e\u9a8c\uff0c\u8bad\u7ec350\u4e2aepoch\uff0c\u8bc4\u4f30\u9a8c\u8bc1\u51c6\u786e\u7387\u3001\u6d4b\u8bd5\u51c6\u786e\u7387\u3001\u8bad\u7ec3\u65f6\u95f4\u3001\u8ba1\u7b97\u8d44\u6e90\u4f7f\u7528\u548c\u5bf9\u6297\u9c81\u68d2\u6027", "result": "\u6df7\u5408\u6a21\u578b\u5728\u6240\u6709\u6570\u636e\u96c6\u4e0a\u90fd\u4f18\u4e8e\u7ecf\u5178\u6a21\u578b\uff1aMNIST(99.38% vs 98.21%)\u3001CIFAR100(41.69% vs 32.25%)\u3001STL10(74.05% vs 63.76%)\uff1b\u8bad\u7ec3\u901f\u5ea6\u5feb5-12\u500d\uff1b\u53c2\u6570\u51cf\u5c116-32%\uff1b\u5185\u5b58\u4f7f\u7528\u66f4\u4f4e(4-5GB vs 5-6GB)\uff1b\u5728\u7b80\u5355\u6570\u636e\u96c6\u4e0a\u5bf9\u6297\u9c81\u68d2\u6027\u663e\u8457\u66f4\u597d", "conclusion": "\u6df7\u5408\u91cf\u5b50-\u7ecf\u5178\u67b6\u6784\u5728\u51c6\u786e\u7387\u3001\u8bad\u7ec3\u6548\u7387\u548c\u53c2\u6570\u53ef\u6269\u5c55\u6027\u65b9\u9762\u5177\u6709\u660e\u663e\u4f18\u52bf\uff0c\u7279\u522b\u9002\u7528\u4e8e\u590d\u6742\u7684\u89c6\u89c9\u4efb\u52a1", "relevance": 15.0}}
{"id": "2509.13516", "pdf": "https://arxiv.org/pdf/2509.13516", "abs": "https://arxiv.org/abs/2509.13516", "authors": ["Tom Almog"], "title": "An Analysis of Optimizer Choice on Energy Efficiency and Performance in Neural Network Training", "categories": ["cs.LG", "68T05 (Primary) 90C30, 68W40 (Secondary)"], "comment": "7 pages. 3 figures", "summary": "As machine learning models grow increasingly complex and computationally\ndemanding, understanding the environmental impact of training decisions becomes\ncritical for sustainable AI development. This paper presents a comprehensive\nempirical study investigating the relationship between optimizer choice and\nenergy efficiency in neural network training. We conducted 360 controlled\nexperiments across three benchmark datasets (MNIST, CIFAR-10, CIFAR-100) using\neight popular optimizers (SGD, Adam, AdamW, RMSprop, Adagrad, Adadelta, Adamax,\nNAdam) with 15 random seeds each. Using CodeCarbon for precise energy tracking\non Apple M1 Pro hardware, we measured training duration, peak memory usage,\ncarbon dioxide emissions, and final model performance. Our findings reveal\nsubstantial trade-offs between training speed, accuracy, and environmental\nimpact that vary across datasets and model complexity. We identify AdamW and\nNAdam as consistently efficient choices, while SGD demonstrates superior\nperformance on complex datasets despite higher emissions. These results provide\nactionable insights for practitioners seeking to balance performance and\nsustainability in machine learning workflows.", "AI": {"tldr": "\u672c\u6587\u901a\u8fc7360\u6b21\u5b9e\u9a8c\u7814\u7a76\u4e86\u4e0d\u540c\u4f18\u5316\u5668\u5bf9\u795e\u7ecf\u7f51\u7edc\u8bad\u7ec3\u80fd\u8017\u7684\u5f71\u54cd\uff0c\u53d1\u73b0AdamW\u548cNAdam\u5728\u80fd\u6548\u65b9\u9762\u8868\u73b0\u6700\u4f73\uff0c\u800cSGD\u5728\u590d\u6742\u6570\u636e\u96c6\u4e0a\u6027\u80fd\u66f4\u597d\u4f46\u78b3\u6392\u653e\u66f4\u9ad8", "motivation": "\u968f\u7740\u673a\u5668\u5b66\u4e60\u6a21\u578b\u65e5\u76ca\u590d\u6742\u548c\u8ba1\u7b97\u9700\u6c42\u589e\u52a0\uff0c\u7406\u89e3\u8bad\u7ec3\u51b3\u7b56\u5bf9\u73af\u5883\u7684\u5f71\u54cd\u5bf9\u4e8e\u53ef\u6301\u7eedAI\u53d1\u5c55\u53d8\u5f97\u81f3\u5173\u91cd\u8981", "method": "\u5728\u4e09\u4e2a\u57fa\u51c6\u6570\u636e\u96c6(MNIST\u3001CIFAR-10\u3001CIFAR-100)\u4e0a\u4f7f\u75288\u79cd\u6d41\u884c\u4f18\u5316\u5668\u8fdb\u884c360\u6b21\u63a7\u5236\u5b9e\u9a8c\uff0c\u4f7f\u7528CodeCarbon\u5728Apple M1 Pro\u786c\u4ef6\u4e0a\u7cbe\u786e\u8ffd\u8e2a\u80fd\u8017", "result": "\u53d1\u73b0\u8bad\u7ec3\u901f\u5ea6\u3001\u51c6\u786e\u6027\u548c\u73af\u5883\u5f71\u54cd\u4e4b\u95f4\u5b58\u5728\u663e\u8457\u6743\u8861\uff0cAdamW\u548cNAdam\u8868\u73b0\u51fa\u4e00\u81f4\u7684\u6548\u7387\uff0c\u800cSGD\u5728\u590d\u6742\u6570\u636e\u96c6\u4e0a\u6027\u80fd\u66f4\u4f18\u4f46\u6392\u653e\u66f4\u9ad8", "conclusion": "\u7814\u7a76\u7ed3\u679c\u4e3a\u4ece\u4e1a\u8005\u5728\u673a\u5668\u5b66\u4e60\u5de5\u4f5c\u6d41\u4e2d\u5e73\u8861\u6027\u80fd\u548c\u53ef\u6301\u7eed\u6027\u63d0\u4f9b\u4e86\u5b9e\u7528\u89c1\u89e3", "relevance": 35.0}}
{"id": "2509.13333", "pdf": "https://arxiv.org/pdf/2509.13333", "abs": "https://arxiv.org/abs/2509.13333", "authors": ["Maheep Chaudhary", "Ian Su", "Nikhil Hooda", "Nishith Shankar", "Julia Tan", "Kevin Zhu", "Ashwinee Panda", "Ryan Lagasse", "Vasu Sharma"], "title": "Evaluation Awareness Scales Predictably in Open-Weights Large Language Models", "categories": ["cs.AI"], "comment": null, "summary": "Large language models (LLMs) can internally distinguish between evaluation\nand deployment contexts, a behaviour known as \\emph{evaluation awareness}. This\nundermines AI safety evaluations, as models may conceal dangerous capabilities\nduring testing. Prior work demonstrated this in a single $70$B model, but the\nscaling relationship across model sizes remains unknown. We investigate\nevaluation awareness across $15$ models scaling from $0.27$B to $70$B\nparameters from four families using linear probing on steering vector\nactivations. Our results reveal a clear power-law scaling: evaluation awareness\nincreases predictably with model size. This scaling law enables forecasting\ndeceptive behavior in future larger models and guides the design of scale-aware\nevaluation strategies for AI safety. A link to the implementation of this paper\ncan be found at\nhttps://anonymous.4open.science/r/evaluation-awareness-scaling-laws/README.md.", "AI": {"tldr": "\u7814\u7a76\u53d1\u73b0\u5927\u578b\u8bed\u8a00\u6a21\u578b\u5b58\u5728\u8bc4\u4f30\u610f\u8bc6\u884c\u4e3a\uff0c\u5373\u6a21\u578b\u80fd\u533a\u5206\u8bc4\u4f30\u548c\u90e8\u7f72\u73af\u5883\uff0c\u8fd9\u79cd\u80fd\u529b\u968f\u6a21\u578b\u89c4\u6a21\u5448\u5e42\u5f8b\u589e\u957f\uff0c\u5bf9AI\u5b89\u5168\u8bc4\u4f30\u6784\u6210\u6311\u6218\u3002", "motivation": "\u5148\u524d\u7814\u7a76\u5728\u5355\u4e2a70B\u6a21\u578b\u4e2d\u53d1\u73b0\u8bc4\u4f30\u610f\u8bc6\u73b0\u8c61\uff0c\u4f46\u4e0d\u540c\u89c4\u6a21\u6a21\u578b\u95f4\u7684\u7f29\u653e\u5173\u7cfb\u672a\u77e5\uff0c\u9700\u8981\u7814\u7a76\u8bc4\u4f30\u610f\u8bc6\u5982\u4f55\u968f\u6a21\u578b\u89c4\u6a21\u53d8\u5316\uff0c\u4ee5\u9884\u6d4b\u66f4\u5927\u6a21\u578b\u7684\u6b3a\u9a97\u884c\u4e3a\u5e76\u8bbe\u8ba1\u76f8\u5e94\u7684\u5b89\u5168\u8bc4\u4f30\u7b56\u7565\u3002", "method": "\u4f7f\u7528\u7ebf\u6027\u63a2\u6d4b\u65b9\u6cd5\u5206\u679015\u4e2a\u4e0d\u540c\u89c4\u6a21\u6a21\u578b\uff080.27B\u523070B\u53c2\u6570\uff09\u7684\u8f6c\u5411\u5411\u91cf\u6fc0\u6d3b\uff0c\u7814\u7a76\u8bc4\u4f30\u610f\u8bc6\u7684\u7f29\u653e\u89c4\u5f8b\u3002", "result": "\u53d1\u73b0\u8bc4\u4f30\u610f\u8bc6\u968f\u6a21\u578b\u89c4\u6a21\u5448\u6e05\u6670\u7684\u5e42\u5f8b\u7f29\u653e\u5173\u7cfb\uff0c\u6a21\u578b\u8d8a\u5927\u8bc4\u4f30\u610f\u8bc6\u8d8a\u5f3a\uff0c\u8fd9\u79cd\u53ef\u9884\u6d4b\u7684\u7f29\u653e\u89c4\u5f8b\u53ef\u7528\u4e8e\u9884\u6d4b\u672a\u6765\u66f4\u5927\u6a21\u578b\u7684\u6b3a\u9a97\u884c\u4e3a\u3002", "conclusion": "\u8bc4\u4f30\u610f\u8bc6\u7684\u5e42\u5f8b\u7f29\u653e\u89c4\u5f8b\u4e3aAI\u5b89\u5168\u8bc4\u4f30\u63d0\u4f9b\u4e86\u91cd\u8981\u6307\u5bfc\uff0c\u9700\u8981\u8bbe\u8ba1\u9488\u5bf9\u4e0d\u540c\u89c4\u6a21\u6a21\u578b\u7684\u8bc4\u4f30\u7b56\u7565\uff0c\u8fd9\u5bf9\u672a\u6765\u66f4\u5927\u6a21\u578b\u7684\u5b89\u5168\u90e8\u7f72\u5177\u6709\u91cd\u8981\u610f\u4e49\u3002", "relevance": 85.0}}
{"id": "2509.13672", "pdf": "https://arxiv.org/pdf/2509.13672", "abs": "https://arxiv.org/abs/2509.13672", "authors": ["Shang Qin", "Jingheng Ye", "Yinghui Li", "Hai-Tao Zheng", "Qi Li", "Jinxiao Shan", "Zhixing Li", "Hong-Gee Kim"], "title": "CL$^2$GEC: A Multi-Discipline Benchmark for Continual Learning in Chinese Literature Grammatical Error Correction", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "The growing demand for automated writing assistance in diverse academic\ndomains highlights the need for robust Chinese Grammatical Error Correction\n(CGEC) systems that can adapt across disciplines. However, existing CGEC\nresearch largely lacks dedicated benchmarks for multi-disciplinary academic\nwriting, overlooking continual learning (CL) as a promising solution to handle\ndomain-specific linguistic variation and prevent catastrophic forgetting. To\nfill this crucial gap, we introduce CL$^2$GEC, the first Continual Learning\nbenchmark for Chinese Literature Grammatical Error Correction, designed to\nevaluate adaptive CGEC across multiple academic fields. Our benchmark includes\n10,000 human-annotated sentences spanning 10 disciplines, each exhibiting\ndistinct linguistic styles and error patterns. CL$^2$GEC focuses on evaluating\ngrammatical error correction in a continual learning setting, simulating\nsequential exposure to diverse academic disciplines to reflect real-world\neditorial dynamics. We evaluate large language models under sequential tuning,\nparameter-efficient adaptation, and four representative CL algorithms, using\nboth standard GEC metrics and continual learning metrics adapted to task-level\nvariation. Experimental results reveal that regularization-based methods\nmitigate forgetting more effectively than replay-based or naive sequential\napproaches. Our benchmark provides a rigorous foundation for future research in\nadaptive grammatical error correction across diverse academic domains.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u7b2c\u4e00\u4e2a\u4e2d\u6587\u6587\u732e\u8bed\u6cd5\u7ea0\u9519\u7684\u6301\u7eed\u5b66\u4e60\u57fa\u51c6CL\u00b2GEC\uff0c\u5305\u542b10\u4e2a\u5b66\u79d1\u768410,000\u6761\u4eba\u5de5\u6807\u6ce8\u53e5\u5b50\uff0c\u8bc4\u4f30LLM\u5728\u8de8\u5b66\u79d1\u8bed\u6cd5\u7ea0\u9519\u4e2d\u7684\u6301\u7eed\u5b66\u4e60\u80fd\u529b\u3002", "motivation": "\u73b0\u6709\u4e2d\u6587\u8bed\u6cd5\u7ea0\u9519\u7814\u7a76\u7f3a\u4e4f\u591a\u5b66\u79d1\u5b66\u672f\u5199\u4f5c\u7684\u4e13\u7528\u57fa\u51c6\uff0c\u5ffd\u89c6\u4e86\u6301\u7eed\u5b66\u4e60\u4f5c\u4e3a\u5904\u7406\u9886\u57df\u7279\u5b9a\u8bed\u8a00\u53d8\u5f02\u548c\u9632\u6b62\u707e\u96be\u6027\u9057\u5fd8\u7684\u89e3\u51b3\u65b9\u6848\u3002", "method": "\u6784\u5efa\u5305\u542b10\u4e2a\u5b66\u79d1\u768410,000\u53e5\u6807\u6ce8\u6570\u636e\u96c6\uff0c\u5728\u6301\u7eed\u5b66\u4e60\u8bbe\u7f6e\u4e0b\u8bc4\u4f30LLM\u7684\u987a\u5e8f\u8c03\u4f18\u3001\u53c2\u6570\u9ad8\u6548\u9002\u5e94\u548c4\u79cd\u4ee3\u8868\u6027CL\u7b97\u6cd5\uff0c\u4f7f\u7528\u6807\u51c6GEC\u6307\u6807\u548c\u9002\u5e94\u4efb\u52a1\u7ea7\u53d8\u5f02\u7684\u6301\u7eed\u5b66\u4e60\u6307\u6807\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0c\u57fa\u4e8e\u6b63\u5219\u5316\u7684\u65b9\u6cd5\u6bd4\u57fa\u4e8e\u56de\u653e\u6216\u7b80\u5355\u987a\u5e8f\u65b9\u6cd5\u66f4\u80fd\u6709\u6548\u7f13\u89e3\u9057\u5fd8\u95ee\u9898\u3002", "conclusion": "\u8be5\u57fa\u51c6\u4e3a\u8de8\u5b66\u79d1\u5b66\u672f\u9886\u57df\u7684\u81ea\u9002\u5e94\u8bed\u6cd5\u7ea0\u9519\u7814\u7a76\u63d0\u4f9b\u4e86\u4e25\u683c\u57fa\u7840\uff0c\u5c55\u793a\u4e86\u6301\u7eed\u5b66\u4e60\u5728\u9886\u57df\u9002\u5e94\u4e2d\u7684\u91cd\u8981\u6027\u3002", "relevance": 75.0}}
{"id": "2509.13361", "pdf": "https://arxiv.org/pdf/2509.13361", "abs": "https://arxiv.org/abs/2509.13361", "authors": ["Tong Yulin", "Liang Xuechen"], "title": "Research on Expressway Congestion Warning Technology Based on YOLOv11-DIoU and GRU-Attention", "categories": ["cs.CV"], "comment": null, "summary": "Expressway traffic congestion severely reduces travel efficiency and hinders\nregional connectivity. Existing \"detection-prediction\" systems have critical\nflaws: low vehicle perception accuracy under occlusion and loss of\nlong-sequence dependencies in congestion forecasting. This study proposes an\nintegrated technical framework to resolve these issues.For traffic flow\nperception, two baseline algorithms were optimized. Traditional YOLOv11 was\nupgraded to YOLOv11-DIoU by replacing GIoU Loss with DIoU Loss, and DeepSort\nwas improved by fusing Mahalanobis (motion) and cosine (appearance) distances.\nExperiments on Chang-Shen Expressway videos showed YOLOv11-DIoU achieved 95.7\\%\nmAP (6.5 percentage points higher than baseline) with 5.3\\% occlusion miss\nrate. DeepSort reached 93.8\\% MOTA (11.3 percentage points higher than SORT)\nwith only 4 ID switches. Using the Greenberg model (for 10-15 vehicles/km\nhigh-density scenarios), speed and density showed a strong negative correlation\n(r=-0.97), conforming to traffic flow theory. For congestion warning, a\nGRU-Attention model was built to capture congestion precursors. Trained 300\nepochs with flow, density, and speed, it achieved 99.7\\% test accuracy (7-9\npercentage points higher than traditional GRU). In 10-minute advance warnings\nfor 30-minute congestion, time error was $\\leq$ 1 minute. Validation with an\nindependent video showed 95\\% warning accuracy, over 90\\% spatial overlap of\ncongestion points, and stable performance in high-flow ($>$5 vehicles/second)\nscenarios.This framework provides quantitative support for expressway\ncongestion control, with promising intelligent transportation applications.", "AI": {"tldr": "\u8be5\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u79cd\u9ad8\u901f\u516c\u8def\u4ea4\u901a\u62e5\u5835\u68c0\u6d4b\u4e0e\u9884\u6d4b\u7684\u7efc\u5408\u6280\u672f\u6846\u67b6\uff0c\u901a\u8fc7\u4f18\u5316\u76ee\u6807\u68c0\u6d4b\u7b97\u6cd5\u548c\u6539\u8fdb\u4ea4\u901a\u6d41\u9884\u6d4b\u6a21\u578b\uff0c\u663e\u8457\u63d0\u9ad8\u4e86\u8f66\u8f86\u611f\u77e5\u7cbe\u5ea6\u548c\u62e5\u5835\u9884\u8b66\u80fd\u529b\u3002", "motivation": "\u73b0\u6709\u4ea4\u901a\u62e5\u5835\u68c0\u6d4b\u9884\u6d4b\u7cfb\u7edf\u5b58\u5728\u906e\u6321\u6761\u4ef6\u4e0b\u8f66\u8f86\u611f\u77e5\u7cbe\u5ea6\u4f4e\u548c\u957f\u5e8f\u5217\u4f9d\u8d56\u4e22\u5931\u7684\u95ee\u9898\uff0c\u9700\u8981\u5f00\u53d1\u66f4\u51c6\u786e\u53ef\u9760\u7684\u6280\u672f\u6846\u67b6\u6765\u63d0\u5347\u9ad8\u901f\u516c\u8def\u4ea4\u901a\u6548\u7387\u3002", "method": "1) \u4f18\u5316YOLOv11\u4e3aYOLOv11-DIoU\uff08\u4f7f\u7528DIoU Loss\u66ff\u4ee3GIoU Loss\uff09\uff1b2) \u6539\u8fdbDeepSort\u7b97\u6cd5\uff08\u878d\u5408\u9a6c\u6c0f\u8ddd\u79bb\u548c\u4f59\u5f26\u8ddd\u79bb\uff09\uff1b3) \u4f7f\u7528Greenberg\u6a21\u578b\u5206\u6790\u4ea4\u901a\u6d41\uff1b4) \u6784\u5efaGRU-Attention\u6a21\u578b\u8fdb\u884c\u62e5\u5835\u9884\u8b66\u3002", "result": "YOLOv11-DIoU\u8fbe\u523095.7% mAP\uff08\u6bd4\u57fa\u7ebf\u9ad86.5\u4e2a\u767e\u5206\u70b9\uff09\uff0c\u906e\u6321\u6f0f\u68c0\u73875.3%\uff1bDeepSort\u8fbe\u523093.8% MOTA\uff1bGRU-Attention\u6a21\u578b\u6d4b\u8bd5\u51c6\u786e\u738799.7%\uff0c10\u5206\u949f\u63d0\u524d\u9884\u8b66\u65f6\u95f4\u8bef\u5dee\u22641\u5206\u949f\uff0c\u72ec\u7acb\u9a8c\u8bc1\u663e\u793a95%\u9884\u8b66\u51c6\u786e\u7387\u3002", "conclusion": "\u8be5\u6846\u67b6\u4e3a\u9ad8\u901f\u516c\u8def\u62e5\u5835\u63a7\u5236\u63d0\u4f9b\u4e86\u91cf\u5316\u652f\u6301\uff0c\u5728\u667a\u80fd\u4ea4\u901a\u5e94\u7528\u4e2d\u5177\u6709\u826f\u597d\u524d\u666f\uff0c\u7279\u522b\u662f\u5728\u9ad8\u6d41\u91cf\u573a\u666f\u4e0b\u8868\u73b0\u7a33\u5b9a\u3002", "relevance": 15.0}}
{"id": "2509.13520", "pdf": "https://arxiv.org/pdf/2509.13520", "abs": "https://arxiv.org/abs/2509.13520", "authors": ["Varun Kumar", "Jing Bi", "Cyril Ngo Ngoc", "Victor Oancea", "George Em Karniadakis"], "title": "Learning Nonlinear Responses in PET Bottle Buckling with a Hybrid DeepONet-Transolver Framework", "categories": ["cs.LG"], "comment": null, "summary": "Neural surrogates and operator networks for solving partial differential\nequation (PDE) problems have attracted significant research interest in recent\nyears. However, most existing approaches are limited in their ability to\ngeneralize solutions across varying non-parametric geometric domains. In this\nwork, we address this challenge in the context of Polyethylene Terephthalate\n(PET) bottle buckling analysis, a representative packaging design problem\nconventionally solved using computationally expensive finite element analysis\n(FEA). We introduce a hybrid DeepONet-Transolver framework that simultaneously\npredicts nodal displacement fields and the time evolution of reaction forces\nduring top load compression. Our methodology is evaluated on two families of\nbottle geometries parameterized by two and four design variables. Training data\nis generated using nonlinear FEA simulations in Abaqus for 254 unique designs\nper family. The proposed framework achieves mean relative $L^{2}$ errors of\n2.5-13% for displacement fields and approximately 2.4% for time-dependent\nreaction forces for the four-parameter bottle family. Point-wise error analyses\nfurther show absolute displacement errors on the order of $10^{-4}$-$10^{-3}$,\nwith the largest discrepancies confined to localized geometric regions.\nImportantly, the model accurately captures key physical phenomena, such as\nbuckling behavior, across diverse bottle geometries. These results highlight\nthe potential of our framework as a scalable and computationally efficient\nsurrogate, particularly for multi-task predictions in computational mechanics\nand applications requiring rapid design evaluation.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u4e2a\u6df7\u5408DeepONet-Transolver\u6846\u67b6\uff0c\u7528\u4e8e\u89e3\u51b3PET\u74f6\u5c48\u66f2\u5206\u6790\u4e2d\u7684PDE\u95ee\u9898\uff0c\u80fd\u591f\u8de8\u51e0\u4f55\u57df\u6cdb\u5316\u89e3\u5e76\u9884\u6d4b\u8282\u70b9\u4f4d\u79fb\u573a\u548c\u65f6\u95f4\u76f8\u5173\u7684\u53cd\u4f5c\u7528\u529b\u3002", "motivation": "\u73b0\u6709\u795e\u7ecf\u4ee3\u7406\u548c\u7b97\u5b50\u7f51\u7edc\u65b9\u6cd5\u5728\u8de8\u975e\u53c2\u6570\u51e0\u4f55\u57df\u6cdb\u5316\u89e3PDE\u95ee\u9898\u4e0a\u5b58\u5728\u5c40\u9650\uff0c\u7279\u522b\u662f\u5728\u5305\u88c5\u8bbe\u8ba1\u4e2d\u7684PET\u74f6\u5c48\u66f2\u5206\u6790\u8fd9\u7c7b\u8ba1\u7b97\u6602\u8d35\u7684\u6709\u9650\u5143\u5206\u6790\u95ee\u9898\u3002", "method": "\u91c7\u7528\u6df7\u5408DeepONet-Transolver\u6846\u67b6\uff0c\u540c\u65f6\u9884\u6d4b\u8282\u70b9\u4f4d\u79fb\u573a\u548c\u9876\u90e8\u538b\u7f29\u8fc7\u7a0b\u4e2d\u7684\u65f6\u95f4\u76f8\u5173\u53cd\u4f5c\u7528\u529b\u6f14\u5316\u3002\u4f7f\u7528Abaqus\u975e\u7ebf\u6027FEA\u6a21\u62df\u751f\u6210\u8bad\u7ec3\u6570\u636e\u3002", "result": "\u5728\u56db\u53c2\u6570\u74f6\u5bb6\u65cf\u4e0a\uff0c\u4f4d\u79fb\u573a\u7684\u5e73\u5747\u76f8\u5bf9L\u00b2\u8bef\u5dee\u4e3a2.5-13%\uff0c\u65f6\u95f4\u76f8\u5173\u53cd\u4f5c\u7528\u529b\u8bef\u5dee\u7ea62.4%\u3002\u70b9\u8bef\u5dee\u5206\u6790\u663e\u793a\u7edd\u5bf9\u4f4d\u79fb\u8bef\u5dee\u572810\u207b\u2074-10\u207b\u00b3\u91cf\u7ea7\uff0c\u6700\u5927\u5dee\u5f02\u5c40\u9650\u4e8e\u5c40\u90e8\u51e0\u4f55\u533a\u57df\u3002", "conclusion": "\u8be5\u6846\u67b6\u4f5c\u4e3a\u53ef\u6269\u5c55\u4e14\u8ba1\u7b97\u9ad8\u6548\u7684\u4ee3\u7406\u6a21\u578b\u5177\u6709\u6f5c\u529b\uff0c\u7279\u522b\u9002\u7528\u4e8e\u8ba1\u7b97\u529b\u5b66\u4e2d\u7684\u591a\u4efb\u52a1\u9884\u6d4b\u548c\u9700\u8981\u5feb\u901f\u8bbe\u8ba1\u8bc4\u4f30\u7684\u5e94\u7528\u3002", "relevance": 25.0}}
{"id": "2509.13334", "pdf": "https://arxiv.org/pdf/2509.13334", "abs": "https://arxiv.org/abs/2509.13334", "authors": ["Anand Swaroop", "Akshat Nallani", "Saksham Uboweja", "Adiliia Uzdenova", "Michael Nguyen", "Kevin Zhu", "Sunishchal Dev", "Ashwinee Panda", "Vasu Sharma", "Maheep Chaudhary"], "title": "FRIT: Using Causal Importance to Improve Chain-of-Thought Faithfulness", "categories": ["cs.AI", "cs.LG"], "comment": null, "summary": "Chain-of-thought (CoT) reasoning has emerged as a powerful tool for improving\nlarge language model performance on complex tasks, but recent work shows that\nreasoning steps often fail to causally influence the final answer, creating\nbrittle and untrustworthy outputs. Prior approaches focus primarily on\nmeasuring faithfulness, while methods for systematically improving it remain\nlimited. We introduce Faithful Reasoning via Intervention Training (FRIT), a\nscalable alignment method that trains models to produce causally consistent\nreasoning by learning from systematically corrupted examples. FRIT generates\nsynthetic training data by intervening on individual reasoning steps in\nmodel-generated CoTs, creating faithful/unfaithful pairs that highlight when\nreasoning breaks down. We then apply Direct Preference Optimization to teach\nmodels to prefer causally consistent reasoning paths. Evaluating on Qwen3-8B\nand Mistral-7B-v0.1 across factual and symbolic reasoning tasks, FRIT increases\nfaithful reasoning by $3.4$ percentage points for Mistral on GSM8K while\nimproving accuracy by $7.6$ percentage points. Our approach provides the first\nscalable, supervision-free method for training language models to produce more\nreliable and interpretable reasoning, addressing a critical gap between\nreasoning performance and trustworthiness. We release our code at\n\\href{https://github.com/Anut-py/frit}.", "AI": {"tldr": "FRIT\u662f\u4e00\u79cd\u901a\u8fc7\u5e72\u9884\u8bad\u7ec3\u63d0\u5347\u5927\u8bed\u8a00\u6a21\u578b\u63a8\u7406\u5fe0\u5b9e\u6027\u7684\u5bf9\u9f50\u65b9\u6cd5\uff0c\u901a\u8fc7\u751f\u6210\u5fe0\u5b9e/\u4e0d\u5fe0\u5b9e\u63a8\u7406\u5bf9\u5e76\u5e94\u7528\u76f4\u63a5\u504f\u597d\u4f18\u5316\uff0c\u5728\u4fdd\u6301\u51c6\u786e\u6027\u7684\u540c\u65f6\u63d0\u9ad8\u63a8\u7406\u7684\u56e0\u679c\u4e00\u81f4\u6027\u3002", "motivation": "\u73b0\u6709\u601d\u7ef4\u94fe\u63a8\u7406\u65b9\u6cd5\u5b58\u5728\u63a8\u7406\u6b65\u9aa4\u4e0e\u6700\u7ec8\u7b54\u6848\u7f3a\u4e4f\u56e0\u679c\u5173\u8054\u7684\u95ee\u9898\uff0c\u5bfc\u81f4\u8f93\u51fa\u8106\u5f31\u4e14\u4e0d\u53ef\u4fe1\u3002\u867d\u7136\u5df2\u6709\u65b9\u6cd5\u4e3b\u8981\u5173\u6ce8\u6d4b\u91cf\u5fe0\u5b9e\u6027\uff0c\u4f46\u7cfb\u7edf\u6027\u63d0\u5347\u5fe0\u5b9e\u6027\u7684\u65b9\u6cd5\u4ecd\u7136\u6709\u9650\u3002", "method": "FRIT\u901a\u8fc7\u5e72\u9884\u6a21\u578b\u751f\u6210\u7684\u601d\u7ef4\u94fe\u4e2d\u7684\u5355\u4e2a\u63a8\u7406\u6b65\u9aa4\uff0c\u751f\u6210\u5408\u6210\u8bad\u7ec3\u6570\u636e\uff08\u5fe0\u5b9e/\u4e0d\u5fe0\u5b9e\u63a8\u7406\u5bf9\uff09\uff0c\u7136\u540e\u5e94\u7528\u76f4\u63a5\u504f\u597d\u4f18\u5316\uff08DPO\uff09\u6765\u8bad\u7ec3\u6a21\u578b\u504f\u597d\u56e0\u679c\u4e00\u81f4\u7684\u63a8\u7406\u8def\u5f84\u3002", "result": "\u5728Qwen3-8B\u548cMistral-7B-v0.1\u6a21\u578b\u4e0a\uff0cFRIT\u5728GSM8K\u4efb\u52a1\u4e0a\u4f7fMistral\u7684\u5fe0\u5b9e\u63a8\u7406\u63d0\u9ad8\u4e863.4\u4e2a\u767e\u5206\u70b9\uff0c\u540c\u65f6\u51c6\u786e\u7387\u63d0\u9ad8\u4e867.6\u4e2a\u767e\u5206\u70b9\u3002", "conclusion": "FRIT\u63d0\u4f9b\u4e86\u7b2c\u4e00\u4e2a\u53ef\u6269\u5c55\u3001\u65e0\u9700\u76d1\u7763\u7684\u65b9\u6cd5\u6765\u8bad\u7ec3\u8bed\u8a00\u6a21\u578b\u4ea7\u751f\u66f4\u53ef\u9760\u548c\u53ef\u89e3\u91ca\u7684\u63a8\u7406\uff0c\u89e3\u51b3\u4e86\u63a8\u7406\u6027\u80fd\u4e0e\u53ef\u4fe1\u5ea6\u4e4b\u95f4\u7684\u5173\u952e\u5dee\u8ddd\u3002", "relevance": 85.0}}
{"id": "2509.13677", "pdf": "https://arxiv.org/pdf/2509.13677", "abs": "https://arxiv.org/abs/2509.13677", "authors": ["Xinxu Zhou", "Jiaqi Bai", "Zhenqi Sun", "Fanxiang Zeng", "Yue Liu"], "title": "AgentCTG: Harnessing Multi-Agent Collaboration for Fine-Grained Precise Control in Text Generation", "categories": ["cs.CL", "cs.AI", "cs.HC"], "comment": null, "summary": "Although significant progress has been made in many tasks within the field of\nNatural Language Processing (NLP), Controlled Text Generation (CTG) continues\nto face numerous challenges, particularly in achieving fine-grained conditional\ncontrol over generation. Additionally, in real scenario and online\napplications, cost considerations, scalability, domain knowledge learning and\nmore precise control are required, presenting more challenge for CTG. This\npaper introduces a novel and scalable framework, AgentCTG, which aims to\nenhance precise and complex control over the text generation by simulating the\ncontrol and regulation mechanisms in multi-agent workflows. We explore various\ncollaboration methods among different agents and introduce an auto-prompt\nmodule to further enhance the generation effectiveness. AgentCTG achieves\nstate-of-the-art results on multiple public datasets. To validate its\neffectiveness in practical applications, we propose a new challenging\nCharacter-Driven Rewriting task, which aims to convert the original text into\nnew text that conform to specific character profiles and simultaneously\npreserve the domain knowledge. When applied to online navigation with\nrole-playing, our approach significantly enhances the driving experience\nthrough improved content delivery. By optimizing the generation of contextually\nrelevant text, we enable a more immersive interaction within online\ncommunities, fostering greater personalization and user engagement.", "AI": {"tldr": "AgentCTG\u662f\u4e00\u4e2a\u57fa\u4e8e\u591a\u667a\u80fd\u4f53\u5de5\u4f5c\u6d41\u7684\u53ef\u63a7\u6587\u672c\u751f\u6210\u6846\u67b6\uff0c\u901a\u8fc7\u6a21\u62df\u591a\u667a\u80fd\u4f53\u534f\u4f5c\u673a\u5236\u5b9e\u73b0\u7ec6\u7c92\u5ea6\u63a7\u5236\uff0c\u5728\u591a\u4e2a\u6570\u636e\u96c6\u4e0a\u8fbe\u5230SOTA\u6548\u679c\uff0c\u5e76\u5728\u89d2\u8272\u9a71\u52a8\u91cd\u5199\u4efb\u52a1\u4e2d\u9a8c\u8bc1\u4e86\u5b9e\u7528\u6027\u3002", "motivation": "\u89e3\u51b3\u53ef\u63a7\u6587\u672c\u751f\u6210(CTG)\u4e2d\u7ec6\u7c92\u5ea6\u6761\u4ef6\u63a7\u5236\u7684\u6311\u6218\uff0c\u6ee1\u8db3\u5b9e\u9645\u5e94\u7528\u4e2d\u6210\u672c\u3001\u53ef\u6269\u5c55\u6027\u3001\u9886\u57df\u77e5\u8bc6\u5b66\u4e60\u548c\u7cbe\u786e\u63a7\u5236\u7684\u9700\u6c42\u3002", "method": "\u63d0\u51faAgentCTG\u6846\u67b6\uff0c\u6a21\u62df\u591a\u667a\u80fd\u4f53\u5de5\u4f5c\u6d41\u4e2d\u7684\u63a7\u5236\u548c\u8c03\u8282\u673a\u5236\uff0c\u63a2\u7d22\u4e0d\u540c\u667a\u80fd\u4f53\u95f4\u7684\u534f\u4f5c\u65b9\u6cd5\uff0c\u5e76\u5f15\u5165\u81ea\u52a8\u63d0\u793a\u6a21\u5757\u6765\u63d0\u5347\u751f\u6210\u6548\u679c\u3002", "result": "\u5728\u591a\u4e2a\u516c\u5f00\u6570\u636e\u96c6\u4e0a\u8fbe\u5230state-of-the-art\u6548\u679c\uff1b\u5728\u89d2\u8272\u9a71\u52a8\u91cd\u5199\u4efb\u52a1\u4e2d\u6210\u529f\u5c06\u539f\u59cb\u6587\u672c\u8f6c\u6362\u4e3a\u7b26\u5408\u7279\u5b9a\u89d2\u8272\u914d\u7f6e\u6587\u4ef6\u7684\u65b0\u6587\u672c\uff0c\u540c\u65f6\u4fdd\u7559\u9886\u57df\u77e5\u8bc6\uff1b\u5728\u7ebf\u5bfc\u822a\u89d2\u8272\u626e\u6f14\u5e94\u7528\u4e2d\u663e\u8457\u63d0\u5347\u4e86\u9a7e\u9a76\u4f53\u9a8c\u3002", "conclusion": "AgentCTG\u901a\u8fc7\u591a\u667a\u80fd\u4f53\u534f\u4f5c\u673a\u5236\u6709\u6548\u89e3\u51b3\u4e86\u53ef\u63a7\u6587\u672c\u751f\u6210\u7684\u7ec6\u7c92\u5ea6\u63a7\u5236\u95ee\u9898\uff0c\u5728\u5b9e\u7528\u573a\u666f\u4e2d\u8868\u73b0\u51fa\u8272\uff0c\u80fd\u591f\u751f\u6210\u66f4\u5177\u4e0a\u4e0b\u6587\u76f8\u5173\u6027\u7684\u6587\u672c\uff0c\u63d0\u5347\u7528\u6237\u6c89\u6d78\u611f\u548c\u53c2\u4e0e\u5ea6\u3002", "relevance": 85.0}}
{"id": "2509.13366", "pdf": "https://arxiv.org/pdf/2509.13366", "abs": "https://arxiv.org/abs/2509.13366", "authors": ["Tony Rohe", "Martin Margreiter", "Markus Moertl"], "title": "Parking Space Ground Truth Test Automation by Artificial Intelligence Using Convolutional Neural Networks", "categories": ["cs.CV", "68U99", "J.2"], "comment": "10 pages, 5 figures", "summary": "This research is part of a study of a real-time, cloud-based on-street\nparking service using crowd-sourced in-vehicle fleet data. The service provides\nreal-time information about available parking spots by classifying\ncrowd-sourced detections observed via ultrasonic sensors. The goal of this\nresearch is to optimize the current parking service quality by analyzing the\nautomation of the existing test process for ground truth tests. Therefore,\nmethods from the field of machine learning, especially image pattern\nrecognition, are applied to enrich the database and substitute human\nengineering work in major areas of the analysis process. After an introduction\ninto the related areas of machine learning, this paper explains the methods and\nimplementations made to achieve a high level of automation, applying\nconvolutional neural networks. Finally, predefined metrics present the\nperformance level achieved, showing a time reduction of human resources up to\n99.58 %. The overall improvements are discussed, summarized, and followed by an\noutlook for future development and potential application of the analysis\nautomation tool.", "AI": {"tldr": "\u8bba\u6587\u7814\u7a76\u57fa\u4e8e\u4f17\u5305\u8f66\u8f7d\u6570\u636e\u7684\u5b9e\u65f6\u8def\u8fb9\u505c\u8f66\u670d\u52a1\uff0c\u901a\u8fc7\u673a\u5668\u5b66\u4e60\u65b9\u6cd5\uff08\u7279\u522b\u662f\u5377\u79ef\u795e\u7ecf\u7f51\u7edc\uff09\u81ea\u52a8\u5316\u5730\u9762\u771f\u503c\u6d4b\u8bd5\u6d41\u7a0b\uff0c\u5c06\u4eba\u5de5\u8d44\u6e90\u65f6\u95f4\u51cf\u5c1199.58%", "motivation": "\u4f18\u5316\u73b0\u6709\u505c\u8f66\u670d\u52a1\u8d28\u91cf\uff0c\u901a\u8fc7\u81ea\u52a8\u5316\u6d4b\u8bd5\u6d41\u7a0b\u66ff\u4ee3\u4eba\u5de5\u5de5\u7a0b\u5de5\u4f5c\uff0c\u63d0\u9ad8\u5206\u6790\u6548\u7387", "method": "\u5e94\u7528\u673a\u5668\u5b66\u4e60\u65b9\u6cd5\uff0c\u7279\u522b\u662f\u56fe\u50cf\u6a21\u5f0f\u8bc6\u522b\u548c\u5377\u79ef\u795e\u7ecf\u7f51\u7edc\uff0c\u6765\u81ea\u52a8\u5316\u5730\u9762\u771f\u503c\u6d4b\u8bd5\u548c\u5206\u6790\u8fc7\u7a0b", "result": "\u5b9e\u73b0\u4e86\u9ad8\u5ea6\u81ea\u52a8\u5316\uff0c\u4eba\u5de5\u8d44\u6e90\u65f6\u95f4\u51cf\u5c11\u9ad8\u8fbe99.58%\uff0c\u663e\u8457\u63d0\u5347\u4e86\u5206\u6790\u6548\u7387", "conclusion": "\u8be5\u65b9\u6cd5\u6210\u529f\u5b9e\u73b0\u4e86\u6d4b\u8bd5\u6d41\u7a0b\u7684\u81ea\u52a8\u5316\uff0c\u4e3a\u672a\u6765\u53d1\u5c55\u548c\u5206\u6790\u81ea\u52a8\u5316\u5de5\u5177\u7684\u6f5c\u5728\u5e94\u7528\u63d0\u4f9b\u4e86\u826f\u597d\u57fa\u7840", "relevance": 15.0}}
{"id": "2509.13523", "pdf": "https://arxiv.org/pdf/2509.13523", "abs": "https://arxiv.org/abs/2509.13523", "authors": ["V\u00e4in\u00f6 Hatanp\u00e4\u00e4", "Eugene Ku", "Jason Stock", "Murali Emani", "Sam Foreman", "Chunyong Jung", "Sandeep Madireddy", "Tung Nguyen", "Varuni Sastry", "Ray A. O. Sinurat", "Sam Wheeler", "Huihuo Zheng", "Troy Arcomano", "Venkatram Vishwanath", "Rao Kotamarthi"], "title": "AERIS: Argonne Earth Systems Model for Reliable and Skillful Predictions", "categories": ["cs.LG", "cs.DC"], "comment": "14 pages, 7 figures", "summary": "Generative machine learning offers new opportunities to better understand\ncomplex Earth system dynamics. Recent diffusion-based methods address spectral\nbiases and improve ensemble calibration in weather forecasting compared to\ndeterministic methods, yet have so far proven difficult to scale stably at high\nresolutions. We introduce AERIS, a 1.3 to 80B parameter pixel-level Swin\ndiffusion transformer to address this gap, and SWiPe, a generalizable technique\nthat composes window parallelism with sequence and pipeline parallelism to\nshard window-based transformers without added communication cost or increased\nglobal batch size. On Aurora (10,080 nodes), AERIS sustains 10.21 ExaFLOPS\n(mixed precision) and a peak performance of 11.21 ExaFLOPS with $1 \\times 1$\npatch size on the 0.25{\\deg} ERA5 dataset, achieving 95.5% weak scaling\nefficiency, and 81.6% strong scaling efficiency. AERIS outperforms the IFS ENS\nand remains stable on seasonal scales to 90 days, highlighting the potential of\nbillion-parameter diffusion models for weather and climate prediction.", "AI": {"tldr": "AERIS\u662f\u4e00\u4e2a13B\u523080B\u53c2\u6570\u7684\u50cf\u7d20\u7ea7Swin\u6269\u6563\u53d8\u6362\u5668\uff0c\u7528\u4e8e\u5929\u6c14\u9884\u6d4b\uff0c\u901a\u8fc7SWiPe\u5e76\u884c\u5316\u6280\u672f\u5b9e\u73b0\u9ad8\u6548\u6269\u5c55\uff0c\u5728Aurora\u8d85\u7ea7\u8ba1\u7b97\u673a\u4e0a\u8fbe\u523010.21 ExaFLOPS\u6027\u80fd\uff0c\u572890\u5929\u5b63\u8282\u5c3a\u5ea6\u9884\u6d4b\u4e2d\u4f18\u4e8eIFS ENS\u7cfb\u7edf\u3002", "motivation": "\u89e3\u51b3\u73b0\u6709\u6269\u6563\u65b9\u6cd5\u5728\u9ad8\u5206\u8fa8\u7387\u5929\u6c14\u9884\u6d4b\u4e2d\u96be\u4ee5\u7a33\u5b9a\u6269\u5c55\u7684\u95ee\u9898\uff0c\u5229\u7528\u751f\u6210\u5f0f\u673a\u5668\u5b66\u4e60\u66f4\u597d\u5730\u7406\u89e3\u590d\u6742\u5730\u7403\u7cfb\u7edf\u52a8\u529b\u5b66\u3002", "method": "\u4f7f\u7528\u50cf\u7d20\u7ea7Swin\u6269\u6563\u53d8\u6362\u5668\u67b6\u6784\uff0c\u7ed3\u5408SWiPe\u5e76\u884c\u5316\u6280\u672f\uff08\u7a97\u53e3\u5e76\u884c\u4e0e\u5e8f\u5217/\u6d41\u6c34\u7ebf\u5e76\u884c\u7ec4\u5408\uff09\uff0c\u57280.25\u00b0 ERA5\u6570\u636e\u96c6\u4e0a\u8fdb\u884c\u8bad\u7ec3\u3002", "result": "\u5728Aurora\u8d85\u7ea7\u8ba1\u7b97\u673a\u4e0a\u5b9e\u73b010.21 ExaFLOPS\u6df7\u5408\u7cbe\u5ea6\u6027\u80fd\uff0c\u5f31\u6269\u5c55\u6548\u738795.5%\uff0c\u5f3a\u6269\u5c55\u6548\u738781.6%\uff0c\u572890\u5929\u5b63\u8282\u5c3a\u5ea6\u9884\u6d4b\u4e2d\u7a33\u5b9a\u4e14\u4f18\u4e8eIFS ENS\u7cfb\u7edf\u3002", "conclusion": "\u5341\u4ebf\u53c2\u6570\u7ea7\u522b\u7684\u6269\u6563\u6a21\u578b\u5728\u5929\u6c14\u548c\u6c14\u5019\u9884\u6d4b\u65b9\u9762\u5177\u6709\u5de8\u5927\u6f5c\u529b\uff0cAERIS\u5c55\u793a\u4e86\u9ad8\u5206\u8fa8\u7387\u7a33\u5b9a\u6269\u5c55\u7684\u80fd\u529b\u3002", "relevance": 35.0}}
{"id": "2509.13339", "pdf": "https://arxiv.org/pdf/2509.13339", "abs": "https://arxiv.org/abs/2509.13339", "authors": ["Ming Jin", "Hyunin Lee"], "title": "Position: AI Safety Must Embrace an Antifragile Perspective", "categories": ["cs.AI"], "comment": null, "summary": "This position paper contends that modern AI research must adopt an\nantifragile perspective on safety -- one in which the system's capacity to\nguarantee long-term AI safety such as handling rare or out-of-distribution\n(OOD) events expands over time. Conventional static benchmarks and single-shot\nrobustness tests overlook the reality that environments evolve and that models,\nif left unchallenged, can drift into maladaptation (e.g., reward hacking,\nover-optimization, or atrophy of broader capabilities). We argue that an\nantifragile approach -- Rather than striving to rapidly reduce current\nuncertainties, the emphasis is on leveraging those uncertainties to better\nprepare for potentially greater, more unpredictable uncertainties in the future\n-- is pivotal for the long-term reliability of open-ended ML systems. In this\nposition paper, we first identify key limitations of static testing, including\nscenario diversity, reward hacking, and over-alignment. We then explore the\npotential of antifragile solutions to manage rare events. Crucially, we\nadvocate for a fundamental recalibration of the methods used to measure,\nbenchmark, and continually improve AI safety over the long term, complementing\nexisting robustness approaches by providing ethical and practical guidelines\ntowards fostering an antifragile AI safety community.", "AI": {"tldr": "\u8be5\u7acb\u573a\u8bba\u6587\u4e3b\u5f20AI\u5b89\u5168\u7814\u7a76\u5e94\u91c7\u7528\u6297\u8106\u5f31\u6027\u89c6\u89d2\uff0c\u901a\u8fc7\u5229\u7528\u4e0d\u786e\u5b9a\u6027\u6765\u589e\u5f3a\u7cfb\u7edf\u5904\u7406\u7f55\u89c1\u548c\u5206\u5e03\u5916\u4e8b\u4ef6\u7684\u80fd\u529b\uff0c\u800c\u975e\u4ec5\u4ec5\u8ffd\u6c42\u9759\u6001\u6d4b\u8bd5\u548c\u4e00\u6b21\u6027\u9c81\u68d2\u6027\u8bc4\u4f30\u3002", "motivation": "\u4f20\u7edf\u9759\u6001\u57fa\u51c6\u6d4b\u8bd5\u548c\u4e00\u6b21\u6027\u9c81\u68d2\u6027\u6d4b\u8bd5\u65e0\u6cd5\u5e94\u5bf9\u73af\u5883\u6f14\u5316\u548c\u6a21\u578b\u6f02\u79fb\u95ee\u9898\uff08\u5982\u5956\u52b1\u9ed1\u5ba2\u653b\u51fb\u3001\u8fc7\u5ea6\u4f18\u5316\u7b49\uff09\uff0c\u9700\u8981\u5efa\u7acb\u80fd\u591f\u968f\u65f6\u95f4\u6269\u5c55\u5b89\u5168\u4fdd\u8bc1\u80fd\u529b\u7684\u52a8\u6001\u5b89\u5168\u6846\u67b6\u3002", "method": "\u63d0\u51fa\u6297\u8106\u5f31\u6027\u65b9\u6cd5\u6846\u67b6\uff0c\u5f3a\u8c03\u5229\u7528\u5f53\u524d\u4e0d\u786e\u5b9a\u6027\u6765\u4e3a\u672a\u6765\u66f4\u5927\u4e0d\u786e\u5b9a\u6027\u505a\u51c6\u5907\uff0c\u5305\u62ec\u8bc6\u522b\u9759\u6001\u6d4b\u8bd5\u7684\u5c40\u9650\u6027\uff08\u573a\u666f\u591a\u6837\u6027\u4e0d\u8db3\u3001\u5956\u52b1\u9ed1\u5ba2\u3001\u8fc7\u5ea6\u5bf9\u9f50\u7b49\uff09\uff0c\u5e76\u63a2\u7d22\u6297\u8106\u5f31\u6027\u89e3\u51b3\u65b9\u6848\u6765\u7ba1\u7406\u7f55\u89c1\u4e8b\u4ef6\u3002", "result": "\u5efa\u7acb\u4e86\u6297\u8106\u5f31\u6027AI\u5b89\u5168\u7684\u7406\u8bba\u57fa\u7840\uff0c\u63d0\u51fa\u4e86\u4ece\u9759\u6001\u6d4b\u8bd5\u5411\u52a8\u6001\u6301\u7eed\u6539\u8fdb\u7684\u8303\u5f0f\u8f6c\u53d8\uff0c\u4e3a\u957f\u671f\u53ef\u9760\u7684\u5f00\u653e\u5f0f\u673a\u5668\u5b66\u4e60\u7cfb\u7edf\u63d0\u4f9b\u4e86\u65b0\u7684\u5b89\u5168\u8bc4\u4f30\u6846\u67b6\u3002", "conclusion": "\u6297\u8106\u5f31\u6027\u65b9\u6cd5\u662f\u786e\u4fddAI\u7cfb\u7edf\u957f\u671f\u5b89\u5168\u6027\u7684\u5173\u952e\uff0c\u9700\u8981\u91cd\u65b0\u6821\u51c6AI\u5b89\u5168\u7684\u6d4b\u91cf\u3001\u57fa\u51c6\u6d4b\u8bd5\u548c\u6301\u7eed\u6539\u8fdb\u65b9\u6cd5\uff0c\u5efa\u7acb\u6297\u8106\u5f31\u6027AI\u5b89\u5168\u793e\u533a\u3002", "relevance": 85.0}}
{"id": "2509.13683", "pdf": "https://arxiv.org/pdf/2509.13683", "abs": "https://arxiv.org/abs/2509.13683", "authors": ["Suyuchen Wang", "Jinlin Wang", "Xinyu Wang", "Shiqi Li", "Xiangru Tang", "Sirui Hong", "Xiao-Wen Chang", "Chenglin Wu", "Bang Liu"], "title": "Improving Context Fidelity via Native Retrieval-Augmented Reasoning", "categories": ["cs.CL", "cs.AI"], "comment": "Accepted as a main conference paper at EMNLP 2025", "summary": "Large language models (LLMs) often struggle with context fidelity, producing\ninconsistent answers when responding to questions based on provided\ninformation. Existing approaches either rely on expensive supervised\nfine-tuning to generate evidence post-answer or train models to perform web\nsearches without necessarily improving utilization of the given context. We\npropose CARE, a novel native retrieval-augmented reasoning framework that\nteaches LLMs to explicitly integrate in-context evidence within their reasoning\nprocess with the model's own retrieval capabilities. Our method requires\nlimited labeled evidence data while significantly enhancing both retrieval\naccuracy and answer generation performance through strategically retrieved\nin-context tokens in the reasoning chain. Extensive experiments on multiple\nreal-world and counterfactual QA benchmarks demonstrate that our approach\nsubstantially outperforms supervised fine-tuning, traditional\nretrieval-augmented generation methods, and external retrieval solutions. This\nwork represents a fundamental advancement in making LLMs more accurate,\nreliable, and efficient for knowledge-intensive tasks.", "AI": {"tldr": "CARE\u6846\u67b6\u901a\u8fc7\u6559\u5bfcLLM\u5728\u63a8\u7406\u8fc7\u7a0b\u4e2d\u663e\u5f0f\u6574\u5408\u4e0a\u4e0b\u6587\u8bc1\u636e\uff0c\u663e\u8457\u63d0\u5347\u4e86\u68c0\u7d22\u51c6\u786e\u6027\u548c\u7b54\u6848\u751f\u6210\u6027\u80fd\uff0c\u65e0\u9700\u6602\u8d35\u76d1\u7763\u5fae\u8c03", "motivation": "\u89e3\u51b3LLM\u5728\u57fa\u4e8e\u7ed9\u5b9a\u4fe1\u606f\u56de\u7b54\u95ee\u9898\u65f6\u5b58\u5728\u7684\u4e0a\u4e0b\u6587\u4fdd\u771f\u5ea6\u95ee\u9898\uff0c\u73b0\u6709\u65b9\u6cd5\u8981\u4e48\u4f9d\u8d56\u6602\u8d35\u7684\u76d1\u7763\u5fae\u8c03\uff0c\u8981\u4e48\u8bad\u7ec3\u6a21\u578b\u8fdb\u884c\u7f51\u7edc\u641c\u7d22\u4f46\u672a\u80fd\u6709\u6548\u5229\u7528\u7ed9\u5b9a\u4e0a\u4e0b\u6587", "method": "\u63d0\u51faCARE\u6846\u67b6\uff0c\u6559\u5bfcLLM\u5229\u7528\u81ea\u8eab\u68c0\u7d22\u80fd\u529b\u5728\u63a8\u7406\u8fc7\u7a0b\u4e2d\u663e\u5f0f\u6574\u5408\u4e0a\u4e0b\u6587\u8bc1\u636e\uff0c\u4f7f\u7528\u6709\u9650\u7684\u6807\u6ce8\u8bc1\u636e\u6570\u636e\uff0c\u901a\u8fc7\u7b56\u7565\u6027\u68c0\u7d22\u7684\u4e0a\u4e0b\u6587\u6807\u8bb0\u589e\u5f3a\u63a8\u7406\u94fe", "result": "\u5728\u591a\u4e2a\u771f\u5b9e\u4e16\u754c\u548c\u53cd\u4e8b\u5b9eQA\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0c\u8be5\u65b9\u6cd5\u663e\u8457\u4f18\u4e8e\u76d1\u7763\u5fae\u8c03\u3001\u4f20\u7edf\u68c0\u7d22\u589e\u5f3a\u751f\u6210\u65b9\u6cd5\u548c\u5916\u90e8\u68c0\u7d22\u89e3\u51b3\u65b9\u6848", "conclusion": "\u8fd9\u9879\u5de5\u4f5c\u662f\u4f7fLLM\u5728\u77e5\u8bc6\u5bc6\u96c6\u578b\u4efb\u52a1\u4e2d\u66f4\u52a0\u51c6\u786e\u3001\u53ef\u9760\u548c\u9ad8\u6548\u7684\u6839\u672c\u6027\u8fdb\u5c55", "relevance": 85.0}}
{"id": "2509.13375", "pdf": "https://arxiv.org/pdf/2509.13375", "abs": "https://arxiv.org/abs/2509.13375", "authors": ["Yuxiao Lee", "Xiaofeng Cao", "Wei Ye", "Jiangchao Yao", "Jingkuan Song", "Heng Tao Shen"], "title": "An Empirical Analysis of VLM-based OOD Detection: Mechanisms, Advantages, and Sensitivity", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "Vision-Language Models (VLMs), such as CLIP, have demonstrated remarkable\nzero-shot out-of-distribution (OOD) detection capabilities, vital for reliable\nAI systems. Despite this promising capability, a comprehensive understanding of\n(1) why they work so effectively, (2) what advantages do they have over\nsingle-modal methods, and (3) how is their behavioral robustness -- remains\nnotably incomplete within the research community. This paper presents a\nsystematic empirical analysis of VLM-based OOD detection using in-distribution\n(ID) and OOD prompts. (1) Mechanisms: We systematically characterize and\nformalize key operational properties within the VLM embedding space that\nfacilitate zero-shot OOD detection. (2) Advantages: We empirically quantify the\nsuperiority of these models over established single-modal approaches,\nattributing this distinct advantage to the VLM's capacity to leverage rich\nsemantic novelty. (3) Sensitivity: We uncovers a significant and previously\nunder-explored asymmetry in their robustness profile: while exhibiting\nresilience to common image noise, these VLM-based methods are highly sensitive\nto prompt phrasing. Our findings contribute a more structured understanding of\nthe strengths and critical vulnerabilities inherent in VLM-based OOD detection,\noffering crucial, empirically-grounded guidance for developing more robust and\nreliable future designs.", "AI": {"tldr": "\u8be5\u8bba\u6587\u7cfb\u7edf\u5206\u6790\u4e86\u57fa\u4e8e\u89c6\u89c9\u8bed\u8a00\u6a21\u578b(VLM)\u7684\u96f6\u6837\u672c\u5206\u5e03\u5916\u68c0\u6d4b\u673a\u5236\uff0c\u63ed\u793a\u4e86VLM\u76f8\u6bd4\u5355\u6a21\u6001\u65b9\u6cd5\u7684\u4f18\u52bf\u6e90\u4e8e\u8bed\u4e49\u65b0\u9896\u6027\u5229\u7528\uff0c\u540c\u65f6\u53d1\u73b0VLM\u5bf9\u63d0\u793a\u8bcd\u63aa\u8f9e\u9ad8\u5ea6\u654f\u611f\u7684\u5173\u952e\u8106\u5f31\u6027", "motivation": "\u5c3d\u7ba1CLIP\u7b49\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u5728\u96f6\u6837\u672c\u5206\u5e03\u5916\u68c0\u6d4b\u65b9\u9762\u8868\u73b0\u51fa\u8272\uff0c\u4f46\u7814\u7a76\u793e\u533a\u5bf9\u5176\u5de5\u4f5c\u673a\u5236\u3001\u76f8\u5bf9\u4e8e\u5355\u6a21\u6001\u65b9\u6cd5\u7684\u4f18\u52bf\u4ee5\u53ca\u884c\u4e3a\u9c81\u68d2\u6027\u7f3a\u4e4f\u7cfb\u7edf\u7406\u89e3", "method": "\u4f7f\u7528\u5206\u5e03\u5185\u548c\u5206\u5e03\u5916\u63d0\u793a\u8fdb\u884c\u7cfb\u7edf\u6027\u5b9e\u8bc1\u5206\u6790\uff0c\u5305\u62ec\uff1a1)\u5f62\u5f0f\u5316VLM\u5d4c\u5165\u7a7a\u95f4\u7684\u5173\u952e\u64cd\u4f5c\u5c5e\u6027\uff1b2)\u91cf\u5316\u6bd4\u8f83VLM\u4e0e\u5355\u6a21\u6001\u65b9\u6cd5\uff1b3)\u8bc4\u4f30\u5bf9\u56fe\u50cf\u566a\u58f0\u548c\u63d0\u793a\u63aa\u8f9e\u7684\u654f\u611f\u6027", "result": "1)\u63ed\u793a\u4e86VLM\u5d4c\u5165\u7a7a\u95f4\u4fc3\u8fdb\u96f6\u6837\u672cOOD\u68c0\u6d4b\u7684\u5173\u952e\u673a\u5236\uff1b2)\u8bc1\u5b9eVLM\u4f18\u4e8e\u5355\u6a21\u6001\u65b9\u6cd5\uff0c\u4f18\u52bf\u6765\u81ea\u8bed\u4e49\u65b0\u9896\u6027\u5229\u7528\uff1b3)\u53d1\u73b0VLM\u5bf9\u5e38\u89c1\u56fe\u50cf\u566a\u58f0\u5177\u6709\u9c81\u68d2\u6027\uff0c\u4f46\u5bf9\u63d0\u793a\u63aa\u8f9e\u9ad8\u5ea6\u654f\u611f\u7684\u4e0d\u5bf9\u79f0\u9c81\u68d2\u6027\u7279\u5f81", "conclusion": "\u7814\u7a76\u63d0\u4f9b\u4e86\u5bf9VLM\u57fa\u4e8eOOD\u68c0\u6d4b\u4f18\u52bf\u548c\u5173\u952e\u8106\u5f31\u6027\u7684\u7ed3\u6784\u5316\u7406\u89e3\uff0c\u4e3a\u5f00\u53d1\u66f4\u9c81\u68d2\u53ef\u9760\u7684\u672a\u6765\u8bbe\u8ba1\u63d0\u4f9b\u4e86\u5b9e\u8bc1\u6307\u5bfc", "relevance": 75.0}}
{"id": "2509.13527", "pdf": "https://arxiv.org/pdf/2509.13527", "abs": "https://arxiv.org/abs/2509.13527", "authors": ["Yulia Pimonova", "Michael G. Taylor", "Alice Allen", "Ping Yang", "Nicholas Lubbers"], "title": "Meta-Learning Linear Models for Molecular Property Prediction", "categories": ["cs.LG", "physics.chem-ph"], "comment": "26 pages, 16 figures", "summary": "Chemists in search of structure-property relationships face great challenges\ndue to limited high quality, concordant datasets. Machine learning (ML) has\nsignificantly advanced predictive capabilities in chemical sciences, but these\nmodern data-driven approaches have increased the demand for data. In response\nto the growing demand for explainable AI (XAI) and to bridge the gap between\npredictive accuracy and human comprehensibility, we introduce LAMeL - a Linear\nAlgorithm for Meta-Learning that preserves interpretability while improving the\nprediction accuracy across multiple properties. While most approaches treat\neach chemical prediction task in isolation, LAMeL leverages a meta-learning\nframework to identify shared model parameters across related tasks, even if\nthose tasks do not share data, allowing it to learn a common functional\nmanifold that serves as a more informed starting point for new unseen tasks.\nOur method delivers performance improvements ranging from 1.1- to 25-fold over\nstandard ridge regression, depending on the domain of the dataset. While the\ndegree of performance enhancement varies across tasks, LAMeL consistently\noutperforms or matches traditional linear methods, making it a reliable tool\nfor chemical property prediction where both accuracy and interpretability are\ncritical.", "AI": {"tldr": "LAMeL\u662f\u4e00\u4e2a\u7528\u4e8e\u5316\u5b66\u6027\u8d28\u9884\u6d4b\u7684\u7ebf\u6027\u5143\u5b66\u4e60\u7b97\u6cd5\uff0c\u5728\u4fdd\u6301\u53ef\u89e3\u91ca\u6027\u7684\u540c\u65f6\u663e\u8457\u63d0\u5347\u9884\u6d4b\u51c6\u786e\u7387", "motivation": "\u5316\u5b66\u9886\u57df\u9ad8\u8d28\u91cf\u6570\u636e\u96c6\u6709\u9650\uff0c\u673a\u5668\u5b66\u4e60\u65b9\u6cd5\u9700\u8981\u5927\u91cf\u6570\u636e\uff0c\u540c\u65f6\u9700\u8981\u6ee1\u8db3\u53ef\u89e3\u91caAI(XAI)\u7684\u9700\u6c42\uff0c\u5728\u9884\u6d4b\u51c6\u786e\u6027\u548c\u4eba\u7c7b\u53ef\u7406\u89e3\u6027\u4e4b\u95f4\u627e\u5230\u5e73\u8861", "method": "\u63d0\u51fa\u7ebf\u6027\u5143\u5b66\u4e60\u7b97\u6cd5LAMeL\uff0c\u901a\u8fc7\u5143\u5b66\u4e60\u6846\u67b6\u8bc6\u522b\u76f8\u5173\u4efb\u52a1\u95f4\u7684\u5171\u4eab\u6a21\u578b\u53c2\u6570\uff0c\u5b66\u4e60\u5171\u540c\u51fd\u6570\u6d41\u5f62\u4f5c\u4e3a\u65b0\u4efb\u52a1\u7684\u66f4\u597d\u8d77\u70b9\uff0c\u5373\u4f7f\u4efb\u52a1\u95f4\u4e0d\u5171\u4eab\u6570\u636e", "result": "\u76f8\u6bd4\u6807\u51c6\u5cad\u56de\u5f52\uff0c\u6027\u80fd\u63d0\u53471.1-25\u500d\u4e0d\u7b49\uff0c\u59cb\u7ec8\u4f18\u4e8e\u6216\u5339\u914d\u4f20\u7edf\u7ebf\u6027\u65b9\u6cd5\uff0c\u5728\u4e0d\u540c\u4efb\u52a1\u4e0a\u8868\u73b0\u7a33\u5b9a", "conclusion": "LAMeL\u662f\u5316\u5b66\u6027\u8d28\u9884\u6d4b\u4e2d\u65e2\u51c6\u786e\u53c8\u53ef\u89e3\u91ca\u7684\u53ef\u9760\u5de5\u5177\uff0c\u7279\u522b\u9002\u5408\u9700\u8981\u9ad8\u53ef\u89e3\u91ca\u6027\u7684\u5e94\u7528\u573a\u666f", "relevance": 35.0}}
{"id": "2509.13341", "pdf": "https://arxiv.org/pdf/2509.13341", "abs": "https://arxiv.org/abs/2509.13341", "authors": ["Ahmet H. G\u00fczel", "Matthew Thomas Jackson", "Jarek Luca Liesen", "Tim Rockt\u00e4schel", "Jakob Nicolaus Foerster", "Ilija Bogunovic", "Jack Parker-Holder"], "title": "Imagined Autocurricula", "categories": ["cs.AI", "cs.LG"], "comment": null, "summary": "Training agents to act in embodied environments typically requires vast\ntraining data or access to accurate simulation, neither of which exists for\nmany cases in the real world. Instead, world models are emerging as an\nalternative leveraging offline, passively collected data, they make it possible\nto generate diverse worlds for training agents in simulation. In this work, we\nharness world models to generate imagined environments to train robust agents\ncapable of generalizing to novel task variations. One of the challenges in\ndoing this is ensuring the agent trains on useful generated data. We thus\npropose a novel approach, IMAC (Imagined Autocurricula), leveraging\nUnsupervised Environment Design (UED), which induces an automatic curriculum\nover generated worlds. In a series of challenging, procedurally generated\nenvironments, we show it is possible to achieve strong transfer performance on\nheld-out environments, having trained only inside a world model learned from a\nnarrower dataset. We believe this opens the path to utilizing larger-scale,\nfoundation world models for generally capable agents.", "AI": {"tldr": "IMAC\u65b9\u6cd5\u5229\u7528\u4e16\u754c\u6a21\u578b\u751f\u6210\u60f3\u8c61\u73af\u5883\uff0c\u901a\u8fc7\u65e0\u76d1\u7763\u73af\u5883\u8bbe\u8ba1\u81ea\u52a8\u751f\u6210\u8bfe\u7a0b\uff0c\u5728\u4ec5\u4f7f\u7528\u7a84\u6570\u636e\u96c6\u8bad\u7ec3\u7684\u60c5\u51b5\u4e0b\uff0c\u5728\u4fdd\u7559\u73af\u5883\u4e2d\u5b9e\u73b0\u4e86\u5f3a\u5927\u7684\u8fc1\u79fb\u6027\u80fd", "motivation": "\u89e3\u51b3\u5728\u5177\u8eab\u73af\u5883\u4e2d\u8bad\u7ec3\u667a\u80fd\u4f53\u9700\u8981\u5927\u91cf\u8bad\u7ec3\u6570\u636e\u6216\u7cbe\u786e\u6a21\u62df\u7684\u95ee\u9898\uff0c\u5229\u7528\u79bb\u7ebf\u88ab\u52a8\u6536\u96c6\u7684\u6570\u636e\u901a\u8fc7\u4e16\u754c\u6a21\u578b\u751f\u6210\u591a\u6837\u5316\u8bad\u7ec3\u73af\u5883", "method": "\u63d0\u51faIMAC\u65b9\u6cd5\uff0c\u7ed3\u5408\u4e16\u754c\u6a21\u578b\u548c\u65e0\u76d1\u7763\u73af\u5883\u8bbe\u8ba1(UED)\uff0c\u81ea\u52a8\u751f\u6210\u8bfe\u7a0b\u6765\u8bad\u7ec3\u9c81\u68d2\u667a\u80fd\u4f53", "result": "\u5728\u4e00\u7cfb\u5217\u5177\u6709\u6311\u6218\u6027\u7684\u7a0b\u5e8f\u751f\u6210\u73af\u5883\u4e2d\uff0c\u4ec5\u4f7f\u7528\u8f83\u7a84\u6570\u636e\u96c6\u8bad\u7ec3\u7684\u4e16\u754c\u6a21\u578b\u5c31\u80fd\u5728\u4fdd\u7559\u73af\u5883\u4e2d\u5b9e\u73b0\u5f3a\u5927\u7684\u8fc1\u79fb\u6027\u80fd", "conclusion": "\u8be5\u65b9\u6cd5\u4e3a\u5229\u7528\u66f4\u5927\u89c4\u6a21\u7684\u57fa\u7840\u4e16\u754c\u6a21\u578b\u8bad\u7ec3\u901a\u7528\u667a\u80fd\u4f53\u5f00\u8f9f\u4e86\u9053\u8def", "relevance": 65.0}}
{"id": "2509.13695", "pdf": "https://arxiv.org/pdf/2509.13695", "abs": "https://arxiv.org/abs/2509.13695", "authors": ["Yosuke Mikami", "Daiki Matsuoka", "Hitomi Yanaka"], "title": "Can Large Language Models Robustly Perform Natural Language Inference for Japanese Comparatives?", "categories": ["cs.CL"], "comment": "To appear in Proceedings of the 16th International Conference on\n  Computational Semantics (IWCS 2025)", "summary": "Large Language Models (LLMs) perform remarkably well in Natural Language\nInference (NLI). However, NLI involving numerical and logical expressions\nremains challenging. Comparatives are a key linguistic phenomenon related to\nsuch inference, but the robustness of LLMs in handling them, especially in\nlanguages that are not dominant in the models' training data, such as Japanese,\nhas not been sufficiently explored. To address this gap, we construct a\nJapanese NLI dataset that focuses on comparatives and evaluate various LLMs in\nzero-shot and few-shot settings. Our results show that the performance of the\nmodels is sensitive to the prompt formats in the zero-shot setting and\ninfluenced by the gold labels in the few-shot examples. The LLMs also struggle\nto handle linguistic phenomena unique to Japanese. Furthermore, we observe that\nprompts containing logical semantic representations help the models predict the\ncorrect labels for inference problems that they struggle to solve even with\nfew-shot examples.", "AI": {"tldr": "\u8be5\u7814\u7a76\u6784\u5efa\u4e86\u4e00\u4e2a\u65e5\u8bed\u6bd4\u8f83\u63a8\u7406NLI\u6570\u636e\u96c6\uff0c\u8bc4\u4f30\u4e86LLMs\u5728\u96f6\u6837\u672c\u548c\u5c11\u6837\u672c\u8bbe\u7f6e\u4e0b\u5904\u7406\u65e5\u8bed\u6bd4\u8f83\u53e5\u7684\u80fd\u529b\uff0c\u53d1\u73b0\u6a21\u578b\u6027\u80fd\u53d7\u63d0\u793a\u683c\u5f0f\u548c\u793a\u4f8b\u6807\u7b7e\u5f71\u54cd\uff0c\u4e14\u903b\u8f91\u8bed\u4e49\u8868\u793a\u80fd\u5e2e\u52a9\u6a21\u578b\u89e3\u51b3\u56f0\u96be\u63a8\u7406\u95ee\u9898\u3002", "motivation": "\u5927\u578b\u8bed\u8a00\u6a21\u578b\u5728\u81ea\u7136\u8bed\u8a00\u63a8\u7406\u65b9\u9762\u8868\u73b0\u4f18\u5f02\uff0c\u4f46\u5728\u5904\u7406\u6d89\u53ca\u6570\u503c\u548c\u903b\u8f91\u8868\u8fbe\u5f0f\u7684\u63a8\u7406\u65f6\u4ecd\u5b58\u5728\u6311\u6218\u3002\u6bd4\u8f83\u53e5\u662f\u4e0e\u6b64\u7c7b\u63a8\u7406\u76f8\u5173\u7684\u5173\u952e\u8bed\u8a00\u73b0\u8c61\uff0c\u4f46LLMs\u5728\u5904\u7406\u975e\u4e3b\u5bfc\u8bad\u7ec3\u8bed\u8a00\uff08\u5982\u65e5\u8bed\uff09\u4e2d\u7684\u6bd4\u8f83\u53e5\u65b9\u9762\u7684\u9c81\u68d2\u6027\u5c1a\u672a\u5f97\u5230\u5145\u5206\u63a2\u7d22\u3002", "method": "\u6784\u5efa\u4e86\u4e00\u4e2a\u4e13\u6ce8\u4e8e\u6bd4\u8f83\u53e5\u7684\u65e5\u8bedNLI\u6570\u636e\u96c6\uff0c\u5728\u96f6\u6837\u672c\u548c\u5c11\u6837\u672c\u8bbe\u7f6e\u4e0b\u8bc4\u4f30\u4e86\u5404\u79cdLLMs\u7684\u6027\u80fd\uff0c\u5206\u6790\u4e86\u63d0\u793a\u683c\u5f0f\u3001\u793a\u4f8b\u6807\u7b7e\u4ee5\u53ca\u903b\u8f91\u8bed\u4e49\u8868\u793a\u5bf9\u6a21\u578b\u8868\u73b0\u7684\u5f71\u54cd\u3002", "result": "\u6a21\u578b\u6027\u80fd\u5728\u96f6\u6837\u672c\u8bbe\u7f6e\u4e2d\u5bf9\u63d0\u793a\u683c\u5f0f\u654f\u611f\uff0c\u5728\u5c11\u6837\u672c\u8bbe\u7f6e\u4e2d\u53d7\u793a\u4f8b\u6807\u7b7e\u5f71\u54cd\u3002LLMs\u96be\u4ee5\u5904\u7406\u65e5\u8bed\u7279\u6709\u7684\u8bed\u8a00\u73b0\u8c61\uff0c\u4f46\u5305\u542b\u903b\u8f91\u8bed\u4e49\u8868\u793a\u7684\u63d0\u793a\u80fd\u5e2e\u52a9\u6a21\u578b\u89e3\u51b3\u5373\u4f7f\u5c11\u6837\u672c\u793a\u4f8b\u4e5f\u96be\u4ee5\u5904\u7406\u7684\u63a8\u7406\u95ee\u9898\u3002", "conclusion": "LLMs\u5728\u5904\u7406\u65e5\u8bed\u6bd4\u8f83\u63a8\u7406\u65b9\u9762\u5b58\u5728\u5c40\u9650\u6027\uff0c\u7279\u522b\u662f\u5728\u5904\u7406\u8bed\u8a00\u7279\u6709\u73b0\u8c61\u65f6\u3002\u903b\u8f91\u8bed\u4e49\u8868\u793a\u53ef\u4ee5\u4f5c\u4e3a\u6709\u6548\u7684\u8f85\u52a9\u5de5\u5177\u6765\u63d0\u5347\u6a21\u578b\u6027\u80fd\uff0c\u63d0\u793a\u5de5\u7a0b\u5728\u8de8\u8bed\u8a00NLI\u4efb\u52a1\u4e2d\u81f3\u5173\u91cd\u8981\u3002", "relevance": 75.0}}
{"id": "2509.13385", "pdf": "https://arxiv.org/pdf/2509.13385", "abs": "https://arxiv.org/abs/2509.13385", "authors": ["Charlotte Beylier", "Parvaneh Joharinad", "J\u00fcrgen Jost", "Nahid Torbati"], "title": "Curvature as a tool for evaluating dimensionality reduction and estimating intrinsic dimension", "categories": ["cs.CV", "cs.DM", "cs.LG", "51K05 (primary) 57-08, 53Z50, 55U10 (secondary)", "G.2.2"], "comment": "31 pages, 14 figures", "summary": "Utilizing recently developed abstract notions of sectional curvature, we\nintroduce a method for constructing a curvature-based geometric profile of\ndiscrete metric spaces. The curvature concept that we use here captures the\nmetric relations between triples of points and other points. More\nsignificantly, based on this curvature profile, we introduce a quantitative\nmeasure to evaluate the effectiveness of data representations, such as those\nproduced by dimensionality reduction techniques. Furthermore, Our experiments\ndemonstrate that this curvature-based analysis can be employed to estimate the\nintrinsic dimensionality of datasets. We use this to explore the large-scale\ngeometry of empirical networks and to evaluate the effectiveness of\ndimensionality reduction techniques.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u622a\u9762\u66f2\u7387\u7684\u51e0\u4f55\u5206\u6790\u65b9\u6cd5\uff0c\u7528\u4e8e\u8bc4\u4f30\u6570\u636e\u8868\u793a\u7684\u6709\u6548\u6027\u548c\u4f30\u8ba1\u6570\u636e\u96c6\u7684\u5185\u5728\u7ef4\u5ea6\uff0c\u7279\u522b\u9002\u7528\u4e8e\u7ef4\u5ea6\u7ea6\u7b80\u6280\u672f\u7684\u8bc4\u4f30\u3002", "motivation": "\u5f00\u53d1\u4e00\u79cd\u57fa\u4e8e\u66f2\u7387\u7684\u51e0\u4f55\u5206\u6790\u65b9\u6cd5\u6765\u91cf\u5316\u8bc4\u4f30\u6570\u636e\u8868\u793a\uff08\u5982\u7ef4\u5ea6\u7ea6\u7b80\u6280\u672f\u4ea7\u751f\u7684\u8868\u793a\uff09\u7684\u6709\u6548\u6027\uff0c\u5e76\u63a2\u7d22\u6570\u636e\u96c6\u7684\u672c\u8d28\u51e0\u4f55\u7279\u6027\u3002", "method": "\u5229\u7528\u65b0\u53d1\u5c55\u7684\u622a\u9762\u66f2\u7387\u62bd\u8c61\u6982\u5ff5\uff0c\u6784\u5efa\u79bb\u6563\u5ea6\u91cf\u7a7a\u95f4\u7684\u66f2\u7387\u51e0\u4f55\u5256\u9762\uff0c\u8be5\u65b9\u6cd5\u6355\u6349\u70b9\u4e09\u5143\u7ec4\u4e0e\u5176\u4ed6\u70b9\u4e4b\u95f4\u7684\u5ea6\u91cf\u5173\u7cfb\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0c\u8fd9\u79cd\u57fa\u4e8e\u66f2\u7387\u7684\u5206\u6790\u53ef\u7528\u4e8e\u4f30\u8ba1\u6570\u636e\u96c6\u7684\u5185\u5728\u7ef4\u5ea6\uff0c\u63a2\u7d22\u7ecf\u9a8c\u7f51\u7edc\u7684\u5927\u89c4\u6a21\u51e0\u4f55\u7ed3\u6784\uff0c\u5e76\u8bc4\u4f30\u7ef4\u5ea6\u7ea6\u7b80\u6280\u672f\u7684\u6709\u6548\u6027\u3002", "conclusion": "\u57fa\u4e8e\u66f2\u7387\u7684\u51e0\u4f55\u5256\u9762\u4e3a\u6570\u636e\u8868\u793a\u8d28\u91cf\u8bc4\u4f30\u63d0\u4f9b\u4e86\u6709\u6548\u7684\u91cf\u5316\u6307\u6807\uff0c\u5728\u6570\u636e\u5206\u6790\u548c\u7ef4\u5ea6\u7ea6\u7b80\u9886\u57df\u5177\u6709\u5e94\u7528\u4ef7\u503c\u3002", "relevance": 15.0}}
{"id": "2509.13608", "pdf": "https://arxiv.org/pdf/2509.13608", "abs": "https://arxiv.org/abs/2509.13608", "authors": ["Niruthiha Selvanayagam", "Ted Kurti"], "title": "Is GPT-4o mini Blinded by its Own Safety Filters? Exposing the Multimodal-to-Unimodal Bottleneck in Hate Speech Detection", "categories": ["cs.LG"], "comment": null, "summary": "As Large Multimodal Models (LMMs) become integral to daily digital life,\nunderstanding their safety architectures is a critical problem for AI\nAlignment. This paper presents a systematic analysis of OpenAI's GPT-4o mini, a\nglobally deployed model, on the difficult task of multimodal hate speech\ndetection. Using the Hateful Memes Challenge dataset, we conduct a multi-phase\ninvestigation on 500 samples to probe the model's reasoning and failure modes.\nOur central finding is the experimental identification of a \"Unimodal\nBottleneck,\" an architectural flaw where the model's advanced multimodal\nreasoning is systematically preempted by context-blind safety filters. A\nquantitative validation of 144 content policy refusals reveals that these\noverrides are triggered in equal measure by unimodal visual 50% and textual 50%\ncontent. We further demonstrate that this safety system is brittle, blocking\nnot only high-risk imagery but also benign, common meme formats, leading to\npredictable false positives. These findings expose a fundamental tension\nbetween capability and safety in state-of-the-art LMMs, highlighting the need\nfor more integrated, context-aware alignment strategies to ensure AI systems\ncan be deployed both safely and effectively.", "AI": {"tldr": "\u7814\u7a76\u53d1\u73b0GPT-4o mini\u5b58\u5728\"\u5355\u6a21\u6001\u74f6\u9888\"\u95ee\u9898\uff0c\u5176\u5148\u8fdb\u7684\u591a\u6a21\u6001\u63a8\u7406\u80fd\u529b\u88ab\u4e0a\u4e0b\u6587\u65e0\u5173\u7684\u5b89\u5168\u8fc7\u6ee4\u5668\u7cfb\u7edf\u6027\u5730\u8986\u76d6\uff0c\u5bfc\u81f4\u5728\u591a\u6a21\u6001\u4ec7\u6068\u8a00\u8bba\u68c0\u6d4b\u4e2d\u51fa\u73b0\u53ef\u9884\u6d4b\u7684\u8bef\u62a5\u3002", "motivation": "\u968f\u7740\u5927\u578b\u591a\u6a21\u6001\u6a21\u578b(LMMs)\u5728\u65e5\u5e38\u6570\u5b57\u751f\u6d3b\u4e2d\u7684\u666e\u53ca\uff0c\u7406\u89e3\u5176\u5b89\u5168\u67b6\u6784\u5bf9\u4e8eAI\u5bf9\u9f50\u81f3\u5173\u91cd\u8981\u3002\u672c\u7814\u7a76\u65e8\u5728\u7cfb\u7edf\u5206\u6790OpenAI GPT-4o mini\u5728\u4ec7\u6068\u8a00\u8bba\u68c0\u6d4b\u4efb\u52a1\u4e2d\u7684\u8868\u73b0\u548c\u5931\u8d25\u6a21\u5f0f\u3002", "method": "\u4f7f\u7528Hateful Memes Challenge\u6570\u636e\u96c6\uff0c\u5bf9500\u4e2a\u6837\u672c\u8fdb\u884c\u591a\u9636\u6bb5\u8c03\u67e5\uff0c\u5206\u6790\u6a21\u578b\u7684\u63a8\u7406\u8fc7\u7a0b\u548c\u5931\u8d25\u6a21\u5f0f\u3002\u901a\u8fc7\u5b9a\u91cf\u9a8c\u8bc1144\u4e2a\u5185\u5bb9\u7b56\u7565\u62d2\u7edd\u6848\u4f8b\u6765\u63a2\u7a76\u5b89\u5168\u8fc7\u6ee4\u5668\u7684\u89e6\u53d1\u673a\u5236\u3002", "result": "\u53d1\u73b050%\u7684\u62d2\u7edd\u7531\u89c6\u89c9\u5185\u5bb9\u89e6\u53d1\uff0c50%\u7531\u6587\u672c\u5185\u5bb9\u89e6\u53d1\uff0c\u5b89\u5168\u7cfb\u7edf\u8106\u5f31\uff0c\u4e0d\u4ec5\u963b\u6b62\u9ad8\u98ce\u9669\u56fe\u50cf\uff0c\u8fd8\u963b\u6b62\u826f\u6027\u7684\u5e38\u89c1meme\u683c\u5f0f\uff0c\u5bfc\u81f4\u53ef\u9884\u6d4b\u7684\u8bef\u62a5\u3002", "conclusion": "\u7814\u7a76\u63ed\u793a\u4e86\u6700\u5148\u8fdbLMMs\u4e2d\u80fd\u529b\u4e0e\u5b89\u5168\u4e4b\u95f4\u7684\u6839\u672c\u6027\u51b2\u7a81\uff0c\u5f3a\u8c03\u9700\u8981\u66f4\u96c6\u6210\u5316\u3001\u4e0a\u4e0b\u6587\u611f\u77e5\u7684\u5bf9\u9f50\u7b56\u7565\u6765\u786e\u4fddAI\u7cfb\u7edf\u65e2\u80fd\u5b89\u5168\u53c8\u80fd\u6709\u6548\u90e8\u7f72\u3002", "relevance": 75.0}}
{"id": "2509.13347", "pdf": "https://arxiv.org/pdf/2509.13347", "abs": "https://arxiv.org/abs/2509.13347", "authors": ["Zihao Wang", "Muyao Li", "Kaichen He", "Xiangyu Wang", "Zhancun Mu", "Anji Liu", "Yitao Liang"], "title": "OpenHA: A Series of Open-Source Hierarchical Agentic Models in Minecraft", "categories": ["cs.AI"], "comment": null, "summary": "The choice of action spaces is a critical yet unresolved challenge in\ndeveloping capable, end-to-end trainable agents. This paper first presents a\nlarge-scale, systematic comparison of prominent abstracted action spaces and\ntokenizers for Vision-Language-Action (VLA) or hierarchical agent models in the\nopen-ended Minecraft. Our analysis reveals that no single action space is\nuniversally optimal; instead, the most effective abstraction is highly\ntask-dependent, creating a dilemma for building generalist agents. To resolve\nthis, we introduce Chain of Action (CoA), a novel framework that unifies\nhigh-level planning and low-level control within a single, monolithic VLA\nmodel. CoA treats an abstracted action not as a command for a separate policy,\nbut as an intermediate reasoning step--akin to a chain of thought--that guides\nthe generation of the final, executable action. Furthermore, we demonstrate\nthat an All-in-One agent trained on a diverse mixture of action spaces using\nthe CoA paradigm learns a more robust and generalizable policy. This unified\nagent achieves a new state-of-the-art, improving the overall task success rate\nover strong, specialized baselines. To foster reproducible research, we release\nthe OpenHA (Open Hierarchical Agents) suite, which includes our comprehensive\nbenchmark of over 800 distinct tasks, curated datasets, source code, and all\npretrained model checkpoints at https://github.com/CraftJarvis/OpenHA", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86Chain of Action (CoA)\u6846\u67b6\uff0c\u7edf\u4e00\u4e86\u9ad8\u7ea7\u89c4\u5212\u548c\u4f4e\u7ea7\u63a7\u5236\u5728\u5355\u4e00VLA\u6a21\u578b\u4e2d\uff0c\u901a\u8fc7\u5c06\u62bd\u8c61\u52a8\u4f5c\u4f5c\u4e3a\u4e2d\u95f4\u63a8\u7406\u6b65\u9aa4\u6765\u6307\u5bfc\u6700\u7ec8\u53ef\u6267\u884c\u52a8\u4f5c\u7684\u751f\u6210\uff0c\u5728Minecraft\u4e2d\u5b9e\u73b0\u4e86\u65b0\u7684SOTA\u6027\u80fd\u3002", "motivation": "\u89e3\u51b3VLA\u548c\u5206\u5c42\u667a\u80fd\u4f53\u6a21\u578b\u4e2d\u52a8\u4f5c\u7a7a\u95f4\u9009\u62e9\u7684\u6311\u6218\uff0c\u73b0\u6709\u65b9\u6cd5\u6ca1\u6709\u666e\u904d\u6700\u4f18\u7684\u52a8\u4f5c\u7a7a\u95f4\u62bd\u8c61\uff0c\u6784\u5efa\u901a\u7528\u667a\u80fd\u4f53\u9762\u4e34\u56f0\u5883\u3002", "method": "\u63d0\u51faCoA\u6846\u67b6\uff0c\u5c06\u62bd\u8c61\u52a8\u4f5c\u89c6\u4e3a\u7c7b\u4f3c\u601d\u7ef4\u94fe\u7684\u4e2d\u95f4\u63a8\u7406\u6b65\u9aa4\uff1b\u8bad\u7ec3All-in-One\u667a\u80fd\u4f53\u5728\u591a\u6837\u5316\u52a8\u4f5c\u7a7a\u95f4\u6df7\u5408\u6570\u636e\u4e0a\u5b66\u4e60\uff1b\u53d1\u5e03OpenHA\u5957\u4ef6\u5305\u542b800+\u4efb\u52a1\u57fa\u51c6\u3002", "result": "\u7edf\u4e00\u667a\u80fd\u4f53\u5b9e\u73b0\u4e86\u65b0\u7684state-of-the-art\uff0c\u76f8\u6bd4\u4e13\u95e8\u7684\u57fa\u7ebf\u6a21\u578b\u63d0\u9ad8\u4e86\u6574\u4f53\u4efb\u52a1\u6210\u529f\u7387\uff0c\u5b66\u4e60\u5230\u4e86\u66f4\u9c81\u68d2\u548c\u53ef\u6cdb\u5316\u7684\u7b56\u7565\u3002", "conclusion": "CoA\u8303\u5f0f\u80fd\u591f\u6709\u6548\u89e3\u51b3\u52a8\u4f5c\u7a7a\u95f4\u9009\u62e9\u7684\u56f0\u5883\uff0c\u7edf\u4e00\u7684\u5355\u6a21\u578b\u65b9\u6cd5\u4f18\u4e8e\u4e13\u95e8\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u4e3a\u6784\u5efa\u901a\u7528\u667a\u80fd\u4f53\u63d0\u4f9b\u4e86\u65b0\u65b9\u5411\u3002", "relevance": 75.0}}
{"id": "2509.13696", "pdf": "https://arxiv.org/pdf/2509.13696", "abs": "https://arxiv.org/abs/2509.13696", "authors": ["Iyadh Ben Cheikh Larbi", "Ajay Madhavan Ravichandran", "Aljoscha Burchardt", "Roland Roller"], "title": "Integrating Text and Time-Series into (Large) Language Models to Predict Medical Outcomes", "categories": ["cs.CL"], "comment": "Presented and published at BioCreative IX", "summary": "Large language models (LLMs) excel at text generation, but their ability to\nhandle clinical classification tasks involving structured data, such as time\nseries, remains underexplored. In this work, we adapt instruction-tuned LLMs\nusing DSPy-based prompt optimization to process clinical notes and structured\nEHR inputs jointly. Our results show that this approach achieves performance on\npar with specialized multimodal systems while requiring less complexity and\noffering greater adaptability across tasks.", "AI": {"tldr": "\u4f7f\u7528DSPy\u63d0\u793a\u4f18\u5316\u6280\u672f\u5c06\u6307\u4ee4\u8c03\u4f18\u7684\u5927\u578b\u8bed\u8a00\u6a21\u578b\u9002\u914d\u4e8e\u4e34\u5e8a\u5206\u7c7b\u4efb\u52a1\uff0c\u80fd\u591f\u540c\u65f6\u5904\u7406\u4e34\u5e8a\u6587\u672c\u548c\u7ed3\u6784\u5316EHR\u6570\u636e\uff0c\u6027\u80fd\u5ab2\u7f8e\u4e13\u4e1a\u591a\u6a21\u6001\u7cfb\u7edf\u4f46\u66f4\u7b80\u5355\u7075\u6d3b", "motivation": "\u5927\u578b\u8bed\u8a00\u6a21\u578b\u5728\u6587\u672c\u751f\u6210\u65b9\u9762\u8868\u73b0\u51fa\u8272\uff0c\u4f46\u5728\u5904\u7406\u5305\u542b\u65f6\u95f4\u5e8f\u5217\u7b49\u7ed3\u6784\u5316\u6570\u636e\u7684\u4e34\u5e8a\u5206\u7c7b\u4efb\u52a1\u65b9\u9762\u4ecd\u6709\u5f85\u63a2\u7d22\uff0c\u7279\u522b\u662f\u5982\u4f55\u6709\u6548\u6574\u5408\u4e34\u5e8a\u7b14\u8bb0\u548c\u7ed3\u6784\u5316EHR\u6570\u636e", "method": "\u91c7\u7528DSPy-based\u63d0\u793a\u4f18\u5316\u6280\u672f\u5bf9\u6307\u4ee4\u8c03\u4f18\u7684LLMs\u8fdb\u884c\u9002\u914d\uff0c\u4f7f\u5176\u80fd\u591f\u8054\u5408\u5904\u7406\u4e34\u5e8a\u6587\u672c\u7b14\u8bb0\u548c\u7ed3\u6784\u5316\u7535\u5b50\u5065\u5eb7\u8bb0\u5f55(EHR)\u8f93\u5165", "result": "\u8be5\u65b9\u6cd5\u5728\u4e34\u5e8a\u5206\u7c7b\u4efb\u52a1\u4e0a\u8fbe\u5230\u4e86\u4e0e\u4e13\u4e1a\u591a\u6a21\u6001\u7cfb\u7edf\u76f8\u5f53\u7684\u6027\u80fd\u6c34\u5e73\uff0c\u540c\u65f6\u5177\u6709\u66f4\u4f4e\u7684\u590d\u6742\u5ea6\u548c\u66f4\u5f3a\u7684\u8de8\u4efb\u52a1\u9002\u5e94\u80fd\u529b", "conclusion": "\u901a\u8fc7\u63d0\u793a\u4f18\u5316\u6280\u672f\uff0c\u901a\u7528LLMs\u53ef\u4ee5\u6709\u6548\u5904\u7406\u4e34\u5e8a\u591a\u6a21\u6001\u6570\u636e\u5206\u7c7b\u4efb\u52a1\uff0c\u4e3a\u533b\u7597AI\u5e94\u7528\u63d0\u4f9b\u4e86\u66f4\u7b80\u5355\u7075\u6d3b\u7684\u89e3\u51b3\u65b9\u6848", "relevance": 75.0}}
{"id": "2509.13388", "pdf": "https://arxiv.org/pdf/2509.13388", "abs": "https://arxiv.org/abs/2509.13388", "authors": ["Yadvendra Gurjar", "Ruoni Wan", "Ehsan Farahbakhsh", "Rohitash Chandra"], "title": "Landcover classification and change detection using remote sensing and machine learning: a case study of Western Fiji", "categories": ["cs.CV", "cs.AI", "stat.AP"], "comment": null, "summary": "As a developing country, Fiji is facing rapid urbanisation, which is visible\nin the massive development projects that include housing, roads, and civil\nworks. In this study, we present machine learning and remote sensing frameworks\nto compare land use and land cover change from 2013 to 2024 in Nadi, Fiji. The\nultimate goal of this study is to provide technical support in land cover/land\nuse modelling and change detection. We used Landsat-8 satellite image for the\nstudy region and created our training dataset with labels for supervised\nmachine learning. We used Google Earth Engine and unsupervised machine learning\nvia k-means clustering to generate the land cover map. We used convolutional\nneural networks to classify the selected regions' land cover types. We present\na visualisation of change detection, highlighting urban area changes over time\nto monitor changes in the map.", "AI": {"tldr": "\u8be5\u7814\u7a76\u4f7f\u7528\u673a\u5668\u5b66\u4e60\u548c\u9065\u611f\u6280\u672f\u5206\u6790\u6590\u6d4eNadi\u5730\u533a2013-2024\u5e74\u7684\u571f\u5730\u5229\u7528\u53d8\u5316\uff0c\u901a\u8fc7Landsat-8\u536b\u661f\u5f71\u50cf\u3001k-means\u805a\u7c7b\u548c\u5377\u79ef\u795e\u7ecf\u7f51\u7edc\u8fdb\u884c\u571f\u5730\u8986\u76d6\u5206\u7c7b\u548c\u53d8\u5316\u68c0\u6d4b\u3002", "motivation": "\u6590\u6d4e\u4f5c\u4e3a\u53d1\u5c55\u4e2d\u56fd\u5bb6\u9762\u4e34\u5feb\u901f\u57ce\u5e02\u5316\uff0c\u9700\u8981\u6280\u672f\u624b\u6bb5\u6765\u76d1\u6d4b\u571f\u5730\u5229\u7528\u53d8\u5316\uff0c\u4e3a\u571f\u5730\u8986\u76d6\u5efa\u6a21\u548c\u53d8\u5316\u68c0\u6d4b\u63d0\u4f9b\u6280\u672f\u652f\u6301\u3002", "method": "\u4f7f\u7528Landsat-8\u536b\u661f\u5f71\u50cf\uff0c\u5728Google Earth Engine\u5e73\u53f0\u4e0a\u7ed3\u5408\u65e0\u76d1\u7763k-means\u805a\u7c7b\u548c\u6709\u76d1\u7763\u7684\u5377\u79ef\u795e\u7ecf\u7f51\u7edc\u8fdb\u884c\u571f\u5730\u8986\u76d6\u5206\u7c7b\uff0c\u751f\u6210\u8bad\u7ec3\u6570\u636e\u96c6\u5e76\u8fdb\u884c\u53d8\u5316\u68c0\u6d4b\u53ef\u89c6\u5316\u3002", "result": "\u7814\u7a76\u6210\u529f\u751f\u6210\u4e86\u571f\u5730\u8986\u76d6\u56fe\uff0c\u5e76\u901a\u8fc7\u53ef\u89c6\u5316\u5c55\u793a\u4e86\u57ce\u5e02\u533a\u57df\u968f\u65f6\u95f4\u7684\u53d8\u5316\u60c5\u51b5\uff0c\u5b9e\u73b0\u4e86\u5bf9\u571f\u5730\u5229\u7528\u53d8\u5316\u7684\u76d1\u6d4b\u3002", "conclusion": "\u8be5\u6846\u67b6\u4e3a\u53d1\u5c55\u4e2d\u56fd\u5bb6\u57ce\u5e02\u5316\u8fdb\u7a0b\u4e2d\u7684\u571f\u5730\u5229\u7528\u76d1\u6d4b\u63d0\u4f9b\u4e86\u6709\u6548\u7684\u6280\u672f\u89e3\u51b3\u65b9\u6848\u3002", "relevance": 5.0}}
{"id": "2509.13621", "pdf": "https://arxiv.org/pdf/2509.13621", "abs": "https://arxiv.org/abs/2509.13621", "authors": ["Antonin Sulc", "Thorsten Hellert", "Steven Hunt"], "title": "Unsupervised Anomaly Detection in ALS EPICS Event Logs", "categories": ["cs.LG"], "comment": "6 pages, 5 figures, The 20th International Conference on Accelerator\n  and Large Experimental Physics Control Systems", "summary": "This paper introduces an automated fault analysis framework for the Advanced\nLight Source (ALS) that processes real-time event logs from its EPICS control\nsystem. By treating log entries as natural language, we transform them into\ncontextual vector representations using semantic embedding techniques. A\nsequence-aware neural network, trained on normal operational data, assigns a\nreal-time anomaly score to each event. This method flags deviations from\nbaseline behavior, enabling operators to rapidly identify the critical event\nsequences that precede complex system failures.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u4e2a\u7528\u4e8e\u5148\u8fdb\u5149\u6e90(ALS)\u7684\u81ea\u52a8\u5316\u6545\u969c\u5206\u6790\u6846\u67b6\uff0c\u901a\u8fc7\u5c06EPICS\u63a7\u5236\u7cfb\u7edf\u7684\u4e8b\u4ef6\u65e5\u5fd7\u4f5c\u4e3a\u81ea\u7136\u8bed\u8a00\u5904\u7406\uff0c\u4f7f\u7528\u8bed\u4e49\u5d4c\u5165\u6280\u672f\u5c06\u5176\u8f6c\u6362\u4e3a\u4e0a\u4e0b\u6587\u5411\u91cf\u8868\u793a\uff0c\u5e76\u7528\u5e8f\u5217\u611f\u77e5\u795e\u7ecf\u7f51\u7edc\u8fdb\u884c\u5b9e\u65f6\u5f02\u5e38\u68c0\u6d4b\u3002", "motivation": "\u5f00\u53d1\u4e00\u4e2a\u80fd\u591f\u5b9e\u65f6\u5904\u7406\u63a7\u5236\u7cfb\u7edf\u4e8b\u4ef6\u65e5\u5fd7\u3001\u81ea\u52a8\u68c0\u6d4b\u5f02\u5e38\u5e76\u5e2e\u52a9\u64cd\u4f5c\u4eba\u5458\u5feb\u901f\u8bc6\u522b\u590d\u6742\u7cfb\u7edf\u6545\u969c\u524d\u5146\u7684\u81ea\u52a8\u5316\u6846\u67b6\uff0c\u4ee5\u63d0\u9ad8\u5927\u578b\u79d1\u5b66\u8bbe\u65bd\u7684\u8fd0\u884c\u53ef\u9760\u6027\u3002", "method": "\u5c06\u63a7\u5236\u7cfb\u7edf\u4e8b\u4ef6\u65e5\u5fd7\u89c6\u4e3a\u81ea\u7136\u8bed\u8a00\u6587\u672c\uff0c\u4f7f\u7528\u8bed\u4e49\u5d4c\u5165\u6280\u672f\u751f\u6210\u4e0a\u4e0b\u6587\u5411\u91cf\u8868\u793a\uff0c\u8bad\u7ec3\u5e8f\u5217\u611f\u77e5\u795e\u7ecf\u7f51\u7edc\u5728\u6b63\u5e38\u64cd\u4f5c\u6570\u636e\u4e0a\u5efa\u7acb\u57fa\u7ebf\uff0c\u4e3a\u6bcf\u4e2a\u4e8b\u4ef6\u5206\u914d\u5b9e\u65f6\u5f02\u5e38\u5206\u6570\u3002", "result": "\u8be5\u65b9\u6cd5\u80fd\u591f\u6709\u6548\u6807\u8bb0\u4e0e\u57fa\u7ebf\u884c\u4e3a\u7684\u504f\u5dee\uff0c\u4f7f\u64cd\u4f5c\u4eba\u5458\u80fd\u591f\u5feb\u901f\u8bc6\u522b\u5bfc\u81f4\u590d\u6742\u7cfb\u7edf\u6545\u969c\u7684\u5173\u952e\u4e8b\u4ef6\u5e8f\u5217\u3002", "conclusion": "\u63d0\u51fa\u7684\u57fa\u4e8e\u81ea\u7136\u8bed\u8a00\u5904\u7406\u548c\u5e8f\u5217\u795e\u7ecf\u7f51\u7edc\u7684\u6846\u67b6\u4e3a\u5927\u578b\u79d1\u5b66\u8bbe\u65bd\u7684\u5b9e\u65f6\u6545\u969c\u5206\u6790\u548c\u9884\u8b66\u63d0\u4f9b\u4e86\u4e00\u79cd\u6709\u6548\u7684\u81ea\u52a8\u5316\u89e3\u51b3\u65b9\u6848\u3002", "relevance": 15.0}}
{"id": "2509.13351", "pdf": "https://arxiv.org/pdf/2509.13351", "abs": "https://arxiv.org/abs/2509.13351", "authors": ["Pulkit Verma", "Ngoc La", "Anthony Favier", "Swaroop Mishra", "Julie A. Shah"], "title": "Teaching LLMs to Plan: Logical Chain-of-Thought Instruction Tuning for Symbolic Planning", "categories": ["cs.AI", "cs.CL"], "comment": null, "summary": "Large language models (LLMs) have demonstrated impressive capabilities across\ndiverse tasks, yet their ability to perform structured symbolic planning\nremains limited, particularly in domains requiring formal representations like\nthe Planning Domain Definition Language (PDDL). In this paper, we present a\nnovel instruction tuning framework, PDDL-Instruct, designed to enhance LLMs'\nsymbolic planning capabilities through logical chain-of-thought reasoning. Our\napproach focuses on teaching models to rigorously reason about action\napplicability, state transitions, and plan validity using explicit logical\ninference steps. By developing instruction prompts that guide models through\nthe precise logical reasoning required to determine when actions can be applied\nin a given state, we enable LLMs to self-correct their planning processes\nthrough structured reflection. The framework systematically builds verification\nskills by decomposing the planning process into explicit reasoning chains about\nprecondition satisfaction, effect application, and invariant preservation.\nExperimental results on multiple planning domains show that our\nchain-of-thought reasoning based instruction-tuned models are significantly\nbetter at planning, achieving planning accuracy of up to 94% on standard\nbenchmarks, representing a 66% absolute improvement over baseline models. This\nwork bridges the gap between the general reasoning capabilities of LLMs and the\nlogical precision required for automated planning, offering a promising\ndirection for developing better AI planning systems.", "AI": {"tldr": "PDDL-Instruct\uff1a\u901a\u8fc7\u903b\u8f91\u601d\u7ef4\u94fe\u63a8\u7406\u589e\u5f3aLLM\u7b26\u53f7\u89c4\u5212\u80fd\u529b\u7684\u6307\u4ee4\u8c03\u4f18\u6846\u67b6\uff0c\u5728\u6807\u51c6\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8fbe\u523094%\u7684\u89c4\u5212\u51c6\u786e\u7387\uff0c\u6bd4\u57fa\u7ebf\u6a21\u578b\u63d0\u534766%", "motivation": "\u5927\u578b\u8bed\u8a00\u6a21\u578b\u5728\u7ed3\u6784\u5316\u7b26\u53f7\u89c4\u5212\u65b9\u9762\u80fd\u529b\u6709\u9650\uff0c\u7279\u522b\u662f\u5728\u9700\u8981PDDL\u7b49\u5f62\u5f0f\u5316\u8868\u793a\u7684\u9886\u57df\u3002\u9700\u8981\u5f25\u5408LLM\u901a\u7528\u63a8\u7406\u80fd\u529b\u4e0e\u81ea\u52a8\u89c4\u5212\u6240\u9700\u903b\u8f91\u7cbe\u5ea6\u4e4b\u95f4\u7684\u5dee\u8ddd", "method": "\u5f00\u53d1\u6307\u4ee4\u63d0\u793a\u6846\u67b6\uff0c\u6559\u5bfc\u6a21\u578b\u901a\u8fc7\u663e\u5f0f\u903b\u8f91\u63a8\u7406\u6b65\u9aa4\u4e25\u683c\u63a8\u7406\u52a8\u4f5c\u9002\u7528\u6027\u3001\u72b6\u6001\u8f6c\u6362\u548c\u8ba1\u5212\u6709\u6548\u6027\u3002\u5c06\u89c4\u5212\u8fc7\u7a0b\u5206\u89e3\u4e3a\u5173\u4e8e\u524d\u63d0\u6761\u4ef6\u6ee1\u8db3\u3001\u6548\u679c\u5e94\u7528\u548c\u4e0d\u53d8\u6027\u4fdd\u6301\u7684\u663e\u5f0f\u63a8\u7406\u94fe", "result": "\u5728\u591a\u4e2a\u89c4\u5212\u9886\u57df\u7684\u5b9e\u9a8c\u7ed3\u679c\u663e\u793a\uff0c\u57fa\u4e8e\u601d\u7ef4\u94fe\u63a8\u7406\u7684\u6307\u4ee4\u8c03\u4f18\u6a21\u578b\u663e\u8457\u4f18\u4e8e\u57fa\u7ebf\uff0c\u5728\u6807\u51c6\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8fbe\u523094%\u7684\u89c4\u5212\u51c6\u786e\u7387\uff0c\u7edd\u5bf9\u6539\u8fdb66%", "conclusion": "\u8be5\u5de5\u4f5c\u4e3a\u5f00\u53d1\u66f4\u597d\u7684AI\u89c4\u5212\u7cfb\u7edf\u63d0\u4f9b\u4e86\u6709\u524d\u666f\u7684\u65b9\u5411\uff0c\u6210\u529f\u5f25\u5408\u4e86LLM\u901a\u7528\u63a8\u7406\u80fd\u529b\u4e0e\u81ea\u52a8\u89c4\u5212\u6240\u9700\u903b\u8f91\u7cbe\u5ea6\u4e4b\u95f4\u7684\u5dee\u8ddd", "relevance": 85.0}}
{"id": "2509.13702", "pdf": "https://arxiv.org/pdf/2509.13702", "abs": "https://arxiv.org/abs/2509.13702", "authors": ["Xiao Zheng"], "title": "DSCC-HS: A Dynamic Self-Reinforcing Framework for Hallucination Suppression in Large Language Models", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Large Language Model (LLM) hallucination is a significant barrier to their\nreliable deployment. Current methods like Retrieval-Augmented Generation (RAG)\nare often reactive. We introduce **Dynamic Self-reinforcing Calibration for\nHallucination Suppression (DSCC-HS)**, a novel, proactive framework that\nintervenes during autoregressive decoding. Inspired by dual-process cognitive\ntheory, DSCC-HS uses a compact proxy model, trained in adversarial roles as a\nFactual Alignment Proxy (FAP) and a Hallucination Detection Proxy (HDP). During\ninference, these proxies dynamically steer a large target model by injecting a\nreal-time steering vector, which is the difference between FAP and HDP logits,\nat each decoding step. This plug-and-play approach requires no modification to\nthe target model. Our experiments on TruthfulQA and BioGEN show DSCC-HS\nachieves state-of-the-art performance. On TruthfulQA, it reached a 99.2%\nFactual Consistency Rate (FCR). On the long-form BioGEN benchmark, it attained\nthe highest FActScore of 46.50. These results validate DSCC-HS as a principled\nand efficient solution for enhancing LLM factuality.", "AI": {"tldr": "DSCC-HS\u662f\u4e00\u4e2a\u4e3b\u52a8\u5f0f\u5e7b\u89c9\u6291\u5236\u6846\u67b6\uff0c\u901a\u8fc7\u5728\u81ea\u56de\u5f52\u89e3\u7801\u8fc7\u7a0b\u4e2d\u6ce8\u5165\u5b9e\u65f6\u5f15\u5bfc\u5411\u91cf\u6765\u52a8\u6001\u6821\u51c6LLM\u8f93\u51fa\uff0c\u65e0\u9700\u4fee\u6539\u76ee\u6807\u6a21\u578b\uff0c\u5728TruthfulQA\u548cBioGEN\u57fa\u51c6\u4e0a\u8fbe\u5230SOTA\u6027\u80fd\u3002", "motivation": "\u89e3\u51b3LLM\u5e7b\u89c9\u95ee\u9898\uff0c\u5f53\u524d\u65b9\u6cd5\u5982RAG\u591a\u4e3a\u88ab\u52a8\u5f0f\uff0c\u9700\u8981\u4e00\u79cd\u4e3b\u52a8\u5e72\u9884\u89e3\u7801\u8fc7\u7a0b\u7684\u65b9\u6cd5\u6765\u63d0\u5347\u4e8b\u5b9e\u51c6\u786e\u6027\u3002", "method": "\u57fa\u4e8e\u53cc\u8fc7\u7a0b\u8ba4\u77e5\u7406\u8bba\uff0c\u4f7f\u7528\u7d27\u51d1\u4ee3\u7406\u6a21\u578b\u8bad\u7ec3\u4e8b\u5b9e\u5bf9\u9f50\u4ee3\u7406(FAP)\u548c\u5e7b\u89c9\u68c0\u6d4b\u4ee3\u7406(HDP)\uff0c\u5728\u63a8\u7406\u65f6\u901a\u8fc7\u8ba1\u7b97\u4e24\u8005logits\u5dee\u5f02\u751f\u6210\u5b9e\u65f6\u5f15\u5bfc\u5411\u91cf\uff0c\u52a8\u6001\u5f15\u5bfc\u5927\u6a21\u578b\u89e3\u7801\u3002", "result": "\u5728TruthfulQA\u4e0a\u8fbe\u523099.2%\u4e8b\u5b9e\u4e00\u81f4\u6027\u7387\uff0c\u5728BioGEN\u957f\u6587\u672c\u57fa\u51c6\u4e0a\u83b7\u5f97\u6700\u9ad8FActScore 46.50\u3002", "conclusion": "DSCC-HS\u63d0\u4f9b\u4e86\u4e00\u4e2a\u539f\u5219\u6027\u4e14\u9ad8\u6548\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u80fd\u591f\u663e\u8457\u589e\u5f3aLLM\u7684\u4e8b\u5b9e\u6027\u3002", "relevance": 85.0}}
{"id": "2509.13396", "pdf": "https://arxiv.org/pdf/2509.13396", "abs": "https://arxiv.org/abs/2509.13396", "authors": ["Xinan Wang", "Di Shi", "Fengyu Wang"], "title": "Real-Time Detection and Tracking of Foreign Object Intrusions in Power Systems via Feature-Based Edge Intelligence", "categories": ["cs.CV", "cs.SY", "eess.SY"], "comment": "12 page Journal paper, accepted by IEEE Open Access Journal of Power\n  and Energy", "summary": "This paper presents a novel three-stage framework for real-time foreign\nobject intrusion (FOI) detection and tracking in power transmission systems.\nThe framework integrates: (1) a YOLOv7 segmentation model for fast and robust\nobject localization, (2) a ConvNeXt-based feature extractor trained with\ntriplet loss to generate discriminative embeddings, and (3) a feature-assisted\nIoU tracker that ensures resilient multi-object tracking under occlusion and\nmotion. To enable scalable field deployment, the pipeline is optimized for\ndeployment on low-cost edge hardware using mixed-precision inference. The\nsystem supports incremental updates by adding embeddings from previously unseen\nobjects into a reference database without requiring model retraining. Extensive\nexperiments on real-world surveillance and drone video datasets demonstrate the\nframework's high accuracy and robustness across diverse FOI scenarios. In\naddition, hardware benchmarks on NVIDIA Jetson devices confirm the framework's\npracticality and scalability for real-world edge applications.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u7528\u4e8e\u7535\u529b\u4f20\u8f93\u7cfb\u7edf\u5b9e\u65f6\u5f02\u7269\u5165\u4fb5\u68c0\u6d4b\u4e0e\u8ddf\u8e2a\u7684\u4e09\u9636\u6bb5\u6846\u67b6\uff0c\u7ed3\u5408YOLOv7\u5206\u5272\u3001ConvNeXt\u7279\u5f81\u63d0\u53d6\u548c\u7279\u5f81\u8f85\u52a9IoU\u8ddf\u8e2a\uff0c\u652f\u6301\u8fb9\u7f18\u90e8\u7f72\u548c\u589e\u91cf\u66f4\u65b0", "motivation": "\u7535\u529b\u4f20\u8f93\u7cfb\u7edf\u4e2d\u5f02\u7269\u5165\u4fb5\u68c0\u6d4b\u5bf9\u7535\u7f51\u5b89\u5168\u81f3\u5173\u91cd\u8981\uff0c\u9700\u8981\u5b9e\u65f6\u3001\u51c6\u786e\u4e14\u80fd\u5728\u8fb9\u7f18\u8bbe\u5907\u4e0a\u90e8\u7f72\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u540c\u65f6\u8981\u5904\u7406\u906e\u6321\u3001\u8fd0\u52a8\u7b49\u590d\u6742\u573a\u666f", "method": "\u4e09\u9636\u6bb5\u6846\u67b6\uff1a1) YOLOv7\u5206\u5272\u6a21\u578b\u8fdb\u884c\u76ee\u6807\u5b9a\u4f4d\uff1b2) ConvNeXt\u7279\u5f81\u63d0\u53d6\u5668\u914d\u5408\u4e09\u5143\u7ec4\u635f\u5931\u751f\u6210\u5224\u522b\u6027\u5d4c\u5165\uff1b3) \u7279\u5f81\u8f85\u52a9IoU\u8ddf\u8e2a\u5668\u5904\u7406\u906e\u6321\u548c\u591a\u76ee\u6807\u8ddf\u8e2a\u3002\u91c7\u7528\u6df7\u5408\u7cbe\u5ea6\u63a8\u7406\u4f18\u5316\u8fb9\u7f18\u90e8\u7f72", "result": "\u5728\u771f\u5b9e\u76d1\u63a7\u548c\u65e0\u4eba\u673a\u89c6\u9891\u6570\u636e\u96c6\u4e0a\u8868\u73b0\u51fa\u9ad8\u51c6\u786e\u6027\u548c\u9c81\u68d2\u6027\uff0c\u5728NVIDIA Jetson\u8bbe\u5907\u4e0a\u7684\u786c\u4ef6\u57fa\u51c6\u6d4b\u8bd5\u8bc1\u5b9e\u4e86\u6846\u67b6\u7684\u5b9e\u7528\u6027\u548c\u53ef\u6269\u5c55\u6027", "conclusion": "\u8be5\u6846\u67b6\u4e3a\u7535\u529b\u4f20\u8f93\u7cfb\u7edf\u63d0\u4f9b\u4e86\u4e00\u79cd\u6709\u6548\u7684\u5b9e\u65f6\u5f02\u7269\u5165\u4fb5\u68c0\u6d4b\u89e3\u51b3\u65b9\u6848\uff0c\u652f\u6301\u8fb9\u7f18\u90e8\u7f72\u548c\u589e\u91cf\u5b66\u4e60\uff0c\u5177\u6709\u5b9e\u9645\u5e94\u7528\u4ef7\u503c", "relevance": 15.0}}
{"id": "2509.13625", "pdf": "https://arxiv.org/pdf/2509.13625", "abs": "https://arxiv.org/abs/2509.13625", "authors": ["Bishnu Bhusal", "Manoj Acharya", "Ramneet Kaur", "Colin Samplawski", "Anirban Roy", "Adam D. Cobb", "Rohit Chadha", "Susmit Jha"], "title": "Privacy-Aware In-Context Learning for Large Language Models", "categories": ["cs.LG", "cs.CL", "cs.CR"], "comment": null, "summary": "Large language models (LLMs) have significantly transformed natural language\nunderstanding and generation, but they raise privacy concerns due to potential\nexposure of sensitive information. Studies have highlighted the risk of\ninformation leakage, where adversaries can extract sensitive information\nembedded in the prompts. In this work, we introduce a novel private prediction\nframework for generating high-quality synthetic text with strong privacy\nguarantees. Our approach leverages the Differential Privacy (DP) framework to\nensure worst-case theoretical bounds on information leakage without requiring\nany fine-tuning of the underlying models.The proposed method performs inference\non private records and aggregates the resulting per-token output distributions.\nThis enables the generation of longer and coherent synthetic text while\nmaintaining privacy guarantees. Additionally, we propose a simple blending\noperation that combines private and public inference to further enhance\nutility. Empirical evaluations demonstrate that our approach outperforms\nprevious state-of-the-art methods on in-context-learning (ICL) tasks, making it\na promising direction for privacy-preserving text generation while maintaining\nhigh utility.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u5dee\u5206\u9690\u79c1\u7684\u79c1\u6709\u9884\u6d4b\u6846\u67b6\uff0c\u7528\u4e8e\u751f\u6210\u9ad8\u8d28\u91cf\u5408\u6210\u6587\u672c\uff0c\u5728\u4fdd\u8bc1\u9690\u79c1\u7684\u540c\u65f6\u4fdd\u6301\u9ad8\u5b9e\u7528\u6027", "motivation": "\u5927\u578b\u8bed\u8a00\u6a21\u578b\u5b58\u5728\u9690\u79c1\u6cc4\u9732\u98ce\u9669\uff0c\u653b\u51fb\u8005\u53ef\u80fd\u4ece\u63d0\u793a\u4e2d\u63d0\u53d6\u654f\u611f\u4fe1\u606f\uff0c\u9700\u8981\u5728\u4e0d\u5fae\u8c03\u6a21\u578b\u7684\u60c5\u51b5\u4e0b\u63d0\u4f9b\u4e25\u683c\u9690\u79c1\u4fdd\u8bc1", "method": "\u5229\u7528\u5dee\u5206\u9690\u79c1\u6846\u67b6\uff0c\u5bf9\u79c1\u6709\u8bb0\u5f55\u8fdb\u884c\u63a8\u7406\u5e76\u805a\u5408\u6bcf\u4e2atoken\u7684\u8f93\u51fa\u5206\u5e03\uff0c\u7ed3\u5408\u79c1\u6709\u548c\u516c\u5171\u63a8\u7406\u7684\u6df7\u5408\u64cd\u4f5c\u6765\u589e\u5f3a\u5b9e\u7528\u6027", "result": "\u5728\u4e0a\u4e0b\u6587\u5b66\u4e60\u4efb\u52a1\u4e0a\u4f18\u4e8e\u73b0\u6709\u6700\u5148\u8fdb\u65b9\u6cd5\uff0c\u80fd\u591f\u751f\u6210\u957f\u4e14\u8fde\u8d2f\u7684\u5408\u6210\u6587\u672c\u540c\u65f6\u4fdd\u6301\u9690\u79c1\u4fdd\u8bc1", "conclusion": "\u8be5\u65b9\u6cd5\u4e3a\u9690\u79c1\u4fdd\u62a4\u6587\u672c\u751f\u6210\u63d0\u4f9b\u4e86\u4e00\u4e2a\u6709\u524d\u666f\u7684\u65b9\u5411\uff0c\u5728\u4fdd\u6301\u9ad8\u5b9e\u7528\u6027\u7684\u540c\u65f6\u63d0\u4f9b\u5f3a\u5927\u7684\u9690\u79c1\u4fdd\u969c", "relevance": 85.0}}
{"id": "2509.13352", "pdf": "https://arxiv.org/pdf/2509.13352", "abs": "https://arxiv.org/abs/2509.13352", "authors": ["Anis Koubaa", "Khaled Gabr"], "title": "Agentic UAVs: LLM-Driven Autonomy with Integrated Tool-Calling and Cognitive Reasoning", "categories": ["cs.AI", "cs.RO", "68T07, 68T40, 68T42", "I.2.9; I.2.11; I.2.8; I.2.10"], "comment": "14 pages, 1 figure", "summary": "Unmanned Aerial Vehicles (UAVs) are increasingly deployed in defense,\nsurveillance, and disaster response, yet most systems remain confined to SAE\nLevel 2--3 autonomy. Their reliance on rule-based control and narrow AI\nrestricts adaptability in dynamic, uncertain missions. Existing UAV frameworks\nlack context-aware reasoning, autonomous decision-making, and ecosystem-level\nintegration; critically, none leverage Large Language Model (LLM) agents with\ntool-calling for real-time knowledge access. This paper introduces the Agentic\nUAVs framework, a five-layer architecture (Perception, Reasoning, Action,\nIntegration, Learning) that augments UAVs with LLM-driven reasoning, database\nquerying, and third-party system interaction. A ROS2 and Gazebo-based prototype\nintegrates YOLOv11 object detection with GPT-4 reasoning and local Gemma-3\ndeployment. In simulated search-and-rescue scenarios, agentic UAVs achieved\nhigher detection confidence (0.79 vs. 0.72), improved person detection rates\n(91% vs. 75%), and markedly increased action recommendation (92% vs. 4.5%).\nThese results confirm that modest computational overhead enables qualitatively\nnew levels of autonomy and ecosystem integration.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86Agentic UAVs\u6846\u67b6\uff0c\u901a\u8fc7\u4e94\u5c42\u67b6\u6784\u5c06LLM\u9a71\u52a8\u7684\u63a8\u7406\u80fd\u529b\u96c6\u6210\u5230\u65e0\u4eba\u673a\u7cfb\u7edf\u4e2d\uff0c\u5728\u641c\u7d22\u6551\u63f4\u6a21\u62df\u4e2d\u663e\u8457\u63d0\u5347\u4e86\u68c0\u6d4b\u6027\u80fd\u548c\u81ea\u4e3b\u51b3\u7b56\u80fd\u529b", "motivation": "\u5f53\u524d\u65e0\u4eba\u673a\u7cfb\u7edf\u4e3b\u8981\u4f9d\u8d56\u57fa\u4e8e\u89c4\u5219\u7684\u63a7\u5236\u548c\u7a84AI\uff0c\u7f3a\u4e4f\u4e0a\u4e0b\u6587\u611f\u77e5\u63a8\u7406\u548c\u81ea\u4e3b\u51b3\u7b56\u80fd\u529b\uff0c\u65e0\u6cd5\u5728\u52a8\u6001\u4e0d\u786e\u5b9a\u4efb\u52a1\u4e2d\u6709\u6548\u9002\u5e94\u3002\u73b0\u6709\u6846\u67b6\u6ca1\u6709\u5229\u7528LLM\u4ee3\u7406\u8fdb\u884c\u5b9e\u65f6\u77e5\u8bc6\u8bbf\u95ee\u548c\u751f\u6001\u7cfb\u7edf\u96c6\u6210", "method": "\u63d0\u51fa\u4e94\u5c42\u67b6\u6784\uff08\u611f\u77e5\u3001\u63a8\u7406\u3001\u884c\u52a8\u3001\u96c6\u6210\u3001\u5b66\u4e60\uff09\uff0c\u96c6\u6210ROS2\u548cGazebo\u539f\u578b\u7cfb\u7edf\uff0c\u7ed3\u5408YOLOv11\u76ee\u6807\u68c0\u6d4b\u3001GPT-4\u63a8\u7406\u548c\u672c\u5730Gemma-3\u90e8\u7f72", "result": "\u5728\u6a21\u62df\u641c\u7d22\u6551\u63f4\u573a\u666f\u4e2d\uff0c\u68c0\u6d4b\u7f6e\u4fe1\u5ea6\u4ece0.72\u63d0\u5347\u52300.79\uff0c\u4eba\u5458\u68c0\u6d4b\u7387\u4ece75%\u63d0\u5347\u523091%\uff0c\u884c\u52a8\u63a8\u8350\u7387\u4ece4.5%\u5927\u5e45\u63d0\u5347\u523092%", "conclusion": "\u9002\u5ea6\u7684\u8ba1\u7b97\u5f00\u9500\u80fd\u591f\u5b9e\u73b0\u8d28\u7684\u81ea\u4e3b\u6027\u63d0\u5347\u548c\u751f\u6001\u7cfb\u7edf\u96c6\u6210\uff0cLLM\u9a71\u52a8\u7684\u65e0\u4eba\u673a\u7cfb\u7edf\u5177\u6709\u663e\u8457\u4f18\u52bf", "relevance": 65.0}}
{"id": "2509.13706", "pdf": "https://arxiv.org/pdf/2509.13706", "abs": "https://arxiv.org/abs/2509.13706", "authors": ["Peter Beidler", "Mark Nguyen", "Kevin Lybarger", "Ola Holmberg", "Eric Ford", "John Kang"], "title": "Automated Triaging and Transfer Learning of Incident Learning Safety Reports Using Large Language Representational Models", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "PURPOSE: Incident reports are an important tool for safety and quality\nimprovement in healthcare, but manual review is time-consuming and requires\nsubject matter expertise. Here we present a natural language processing (NLP)\nscreening tool to detect high-severity incident reports in radiation oncology\nacross two institutions.\n  METHODS AND MATERIALS: We used two text datasets to train and evaluate our\nNLP models: 7,094 reports from our institution (Inst.), and 571 from IAEA\nSAFRON (SF), all of which had severity scores labeled by clinical content\nexperts. We trained and evaluated two types of models: baseline support vector\nmachines (SVM) and BlueBERT which is a large language model pretrained on\nPubMed abstracts and hospitalized patient data. We assessed for\ngeneralizability of our model in two ways. First, we evaluated models trained\nusing Inst.-train on SF-test. Second, we trained a BlueBERT_TRANSFER model that\nwas first fine-tuned on Inst.-train then on SF-train before testing on SF-test\nset. To further analyze model performance, we also examined a subset of 59\nreports from our Inst. dataset, which were manually edited for clarity.\n  RESULTS Classification performance on the Inst. test achieved AUROC 0.82\nusing SVM and 0.81 using BlueBERT. Without cross-institution transfer learning,\nperformance on the SF test was limited to an AUROC of 0.42 using SVM and 0.56\nusing BlueBERT. BlueBERT_TRANSFER, which was fine-tuned on both datasets,\nimproved the performance on SF test to AUROC 0.78. Performance of SVM, and\nBlueBERT_TRANSFER models on the manually curated Inst. reports (AUROC 0.85 and\n0.74) was similar to human performance (AUROC 0.81).\n  CONCLUSION: In summary, we successfully developed cross-institution NLP\nmodels on incident report text from radiation oncology centers. These models\nwere able to detect high-severity reports similarly to humans on a curated\ndataset.", "AI": {"tldr": "\u5f00\u53d1\u4e86\u4e00\u4e2a\u57fa\u4e8eBlueBERT\u7684NLP\u5de5\u5177\uff0c\u7528\u4e8e\u81ea\u52a8\u68c0\u6d4b\u653e\u5c04\u80bf\u7624\u5b66\u4e2d\u7684\u9ad8\u4e25\u91cd\u6027\u4e8b\u4ef6\u62a5\u544a\uff0c\u901a\u8fc7\u8de8\u673a\u6784\u8fc1\u79fb\u5b66\u4e60\u5b9e\u73b0\u4e86\u4e0e\u4eba\u7c7b\u76f8\u5f53\u7684\u5206\u7c7b\u6027\u80fd\u3002", "motivation": "\u533b\u7597\u4e8b\u4ef6\u62a5\u544a\u7684\u624b\u52a8\u5ba1\u67e5\u8017\u65f6\u4e14\u9700\u8981\u4e13\u4e1a\u77e5\u8bc6\uff0c\u9700\u8981\u81ea\u52a8\u5316\u5de5\u5177\u6765\u9ad8\u6548\u8bc6\u522b\u9ad8\u4e25\u91cd\u6027\u4e8b\u4ef6\uff0c\u4ee5\u63d0\u5347\u533b\u7597\u5b89\u5168\u548c\u8d28\u91cf\u6539\u8fdb\u3002", "method": "\u4f7f\u7528\u652f\u6301\u5411\u91cf\u673a(SVM)\u548c\u57fa\u4e8ePubMed\u6570\u636e\u9884\u8bad\u7ec3\u7684BlueBERT\u6a21\u578b\uff0c\u5728\u4e24\u4e2a\u673a\u6784\u7684\u653e\u5c04\u80bf\u7624\u5b66\u4e8b\u4ef6\u62a5\u544a\u6570\u636e\u96c6\u4e0a\u8fdb\u884c\u8bad\u7ec3\u548c\u8bc4\u4f30\uff0c\u91c7\u7528\u8de8\u673a\u6784\u8fc1\u79fb\u5b66\u4e60\u7b56\u7565\u3002", "result": "BlueBERT_TRANSFER\u6a21\u578b\u5728\u8de8\u673a\u6784\u6d4b\u8bd5\u96c6\u4e0a\u8fbe\u5230AUROC 0.78\uff0c\u5728\u4eba\u5de5\u7f16\u8f91\u7684\u6570\u636e\u96c6\u4e0a\u8fbe\u5230AUROC 0.74\uff0c\u4e0e\u4eba\u7c7b\u6027\u80fd(AUROC 0.81)\u76f8\u5f53\u3002", "conclusion": "\u6210\u529f\u5f00\u53d1\u4e86\u8de8\u673a\u6784\u7684NLP\u6a21\u578b\uff0c\u80fd\u591f\u4ee5\u4e0e\u4eba\u7c7b\u76f8\u5f53\u7684\u6027\u80fd\u68c0\u6d4b\u9ad8\u4e25\u91cd\u6027\u4e8b\u4ef6\u62a5\u544a\uff0c\u8bc1\u660e\u4e86\u8fc1\u79fb\u5b66\u4e60\u5728\u533b\u7597\u6587\u672c\u5206\u6790\u4e2d\u7684\u6709\u6548\u6027\u3002", "relevance": 35.0}}
{"id": "2509.13399", "pdf": "https://arxiv.org/pdf/2509.13399", "abs": "https://arxiv.org/abs/2509.13399", "authors": ["Tianyu Chen", "Yasi Zhang", "Zhi Zhang", "Peiyu Yu", "Shu Wang", "Zhendong Wang", "Kevin Lin", "Xiaofei Wang", "Zhengyuan Yang", "Linjie Li", "Chung-Ching Lin", "Jianwen Xie", "Oscar Leong", "Lijuan Wang", "Ying Nian Wu", "Mingyuan Zhou"], "title": "EdiVal-Agent: An Object-Centric Framework for Automated, Scalable, Fine-Grained Evaluation of Multi-Turn Editing", "categories": ["cs.CV", "cs.AI", "cs.LG"], "comment": "Tianyu Chen and Yasi Zhang contributed equally; Oscar Leong, Lijuan\n  Wang, Ying Nian Wu, and Mingyuan Zhou advised equally", "summary": "Instruction-based image editing has advanced rapidly, yet reliable and\ninterpretable evaluation remains a bottleneck. Current protocols either (i)\ndepend on paired reference images -- resulting in limited coverage and\ninheriting biases from prior generative models -- or (ii) rely solely on\nzero-shot vision-language models (VLMs), whose prompt-based assessments of\ninstruction following, content consistency, and visual quality are often\nimprecise.\n  To address this, we introduce EdiVal-Agent, an automated, scalable, and\nfine-grained evaluation framework for multi-turn instruction-based editing from\nan object-centric perspective, supported by a suite of expert tools. Given an\nimage, EdiVal-Agent first decomposes it into semantically meaningful objects,\nthen synthesizes diverse, context-aware editing instructions. For evaluation,\nit integrates VLMs with open-vocabulary object detectors to assess instruction\nfollowing, uses semantic-level feature extractors to evaluate content\nconsistency, and leverages human preference models to judge visual quality. We\nshow that combining VLMs with object detectors yields stronger agreement with\nhuman judgments in instruction-following evaluation compared to using VLMs\nalone and CLIP-based metrics. Furthermore, the pipeline's modular design allows\nfuture tools to be seamlessly integrated, enhancing evaluation accuracy over\ntime.\n  Instantiating this pipeline, we build EdiVal-Bench, a multi-turn editing\nbenchmark covering 9 instruction types and 11 state-of-the-art editing models\nspanning autoregressive (AR) (including Nano Banana, GPT-Image-1),\nflow-matching, and diffusion paradigms. We demonstrate that EdiVal-Agent can be\nused to identify existing failure modes, thereby informing the development of\nthe next generation of editing models. Project page:\nhttps://tianyucodings.github.io/EdiVAL-page/.", "AI": {"tldr": "\u63d0\u51fa\u4e86EdiVal-Agent\uff0c\u4e00\u4e2a\u81ea\u52a8\u5316\u7684\u3001\u7ec6\u7c92\u5ea6\u7684\u591a\u8f6e\u6307\u4ee4\u56fe\u50cf\u7f16\u8f91\u8bc4\u4f30\u6846\u67b6\uff0c\u901a\u8fc7\u5bf9\u8c61\u4e2d\u5fc3\u89c6\u89d2\u7ed3\u5408VLM\u3001\u76ee\u6807\u68c0\u6d4b\u5668\u548c\u4eba\u7c7b\u504f\u597d\u6a21\u578b\u6765\u8bc4\u4f30\u6307\u4ee4\u9075\u5faa\u3001\u5185\u5bb9\u4e00\u81f4\u6027\u548c\u89c6\u89c9\u8d28\u91cf\u3002", "motivation": "\u5f53\u524d\u57fa\u4e8e\u6307\u4ee4\u7684\u56fe\u50cf\u7f16\u8f91\u8bc4\u4f30\u65b9\u6cd5\u5b58\u5728\u5c40\u9650\u6027\uff1a\u8981\u4e48\u4f9d\u8d56\u914d\u5bf9\u7684\u53c2\u8003\u56fe\u50cf\uff08\u8986\u76d6\u8303\u56f4\u6709\u9650\u4e14\u7ee7\u627f\u751f\u6210\u6a21\u578b\u504f\u89c1\uff09\uff0c\u8981\u4e48\u4ec5\u4f7f\u7528\u96f6\u6837\u672cVLM\uff08\u8bc4\u4f30\u4e0d\u7cbe\u786e\uff09\u3002\u9700\u8981\u66f4\u53ef\u9760\u548c\u53ef\u89e3\u91ca\u7684\u8bc4\u4f30\u6846\u67b6\u3002", "method": "1) \u5c06\u56fe\u50cf\u5206\u89e3\u4e3a\u8bed\u4e49\u5bf9\u8c61 2) \u5408\u6210\u591a\u6837\u5316\u7684\u4e0a\u4e0b\u6587\u611f\u77e5\u7f16\u8f91\u6307\u4ee4 3) \u7ed3\u5408VLM\u548c\u5f00\u653e\u8bcd\u6c47\u76ee\u6807\u68c0\u6d4b\u5668\u8bc4\u4f30\u6307\u4ee4\u9075\u5faa 4) \u4f7f\u7528\u8bed\u4e49\u7ea7\u7279\u5f81\u63d0\u53d6\u5668\u8bc4\u4f30\u5185\u5bb9\u4e00\u81f4\u6027 5) \u5229\u7528\u4eba\u7c7b\u504f\u597d\u6a21\u578b\u5224\u65ad\u89c6\u89c9\u8d28\u91cf", "result": "\u76f8\u6bd4\u5355\u72ec\u4f7f\u7528VLM\u548cCLIP\u6307\u6807\uff0cVLM\u4e0e\u76ee\u6807\u68c0\u6d4b\u5668\u7ed3\u5408\u5728\u6307\u4ee4\u9075\u5faa\u8bc4\u4f30\u4e2d\u4e0e\u4eba\u7c7b\u5224\u65ad\u6709\u66f4\u5f3a\u7684\u4e00\u81f4\u6027\u3002\u6a21\u5757\u5316\u8bbe\u8ba1\u5141\u8bb8\u672a\u6765\u5de5\u5177\u96c6\u6210\uff0c\u63d0\u9ad8\u8bc4\u4f30\u51c6\u786e\u6027\u3002", "conclusion": "EdiVal-Agent\u80fd\u591f\u8bc6\u522b\u73b0\u6709\u7f16\u8f91\u6a21\u578b\u7684\u5931\u8d25\u6a21\u5f0f\uff0c\u4e3a\u4e0b\u4e00\u4ee3\u7f16\u8f91\u6a21\u578b\u7684\u5f00\u53d1\u63d0\u4f9b\u4fe1\u606f\u3002\u6846\u67b6\u5177\u6709\u53ef\u6269\u5c55\u6027\u548c\u6a21\u5757\u5316\u7279\u6027\u3002", "relevance": 65.0}}
{"id": "2509.13633", "pdf": "https://arxiv.org/pdf/2509.13633", "abs": "https://arxiv.org/abs/2509.13633", "authors": ["Jeremy Oon", "Rakhi Manohar Mepparambath", "Ling Feng"], "title": "DeepLogit: A sequentially constrained explainable deep learning modeling approach for transport policy analysis", "categories": ["cs.LG", "cs.AI"], "comment": null, "summary": "Despite the significant progress of deep learning models in multitude of\napplications, their adaption in planning and policy related areas remains\nchallenging due to the black-box nature of these models. In this work, we\ndevelop a set of DeepLogit models that follow a novel sequentially constrained\napproach in estimating deep learning models for transport policy analysis. In\nthe first step of the proposed approach, we estimate a convolutional neural\nnetwork (CNN) model with only linear terms, which is equivalent of a\nlinear-in-parameter multinomial logit model. We then estimate other deep\nlearning models by constraining the parameters that need interpretability at\nthe values obtained in the linear-in-parameter CNN model and including higher\norder terms or by introducing advanced deep learning architectures like\nTransformers. Our approach can retain the interpretability of the selected\nparameters, yet provides significantly improved model accuracy than the\ndiscrete choice model. We demonstrate our approach on a transit route choice\nexample using real-world transit smart card data from Singapore. This study\nshows the potential for a unifying approach, where theory-based discrete choice\nmodel (DCM) and data-driven AI models can leverage each other's strengths in\ninterpretability and predictive power. With the availability of larger datasets\nand more complex constructions, such approach can lead to more accurate models\nusing discrete choice models while maintaining its applicability in planning\nand policy-related areas. Our code is available on\nhttps://github.com/jeremyoon/route-choice/ .", "AI": {"tldr": "\u63d0\u51faDeepLogit\u6a21\u578b\uff0c\u901a\u8fc7\u5e8f\u5217\u7ea6\u675f\u65b9\u6cd5\u7ed3\u5408\u6df1\u5ea6\u5b66\u4e60\u4e0e\u79bb\u6563\u9009\u62e9\u6a21\u578b\uff0c\u5728\u4fdd\u6301\u53ef\u89e3\u91ca\u6027\u7684\u540c\u65f6\u63d0\u5347\u4ea4\u901a\u653f\u7b56\u5206\u6790\u7684\u9884\u6d4b\u7cbe\u5ea6\u3002", "motivation": "\u6df1\u5ea6\u5b66\u4e60\u6a21\u578b\u5728\u89c4\u5212\u548c\u653f\u7b56\u9886\u57df\u7684\u5e94\u7528\u53d7\u9650\u4e8e\u5176\u9ed1\u76d2\u7279\u6027\uff0c\u9700\u8981\u4e00\u79cd\u65e2\u80fd\u4fdd\u6301\u53ef\u89e3\u91ca\u6027\u53c8\u80fd\u63d0\u9ad8\u9884\u6d4b\u51c6\u786e\u6027\u7684\u65b9\u6cd5\u3002", "method": "\u91c7\u7528\u5e8f\u5217\u7ea6\u675f\u65b9\u6cd5\uff1a\u9996\u5148\u4f30\u8ba1\u4ec5\u542b\u7ebf\u6027\u9879\u7684CNN\u6a21\u578b\uff08\u7b49\u4ef7\u4e8e\u7ebf\u6027\u53c2\u6570\u591a\u9879\u5f0flogit\u6a21\u578b\uff09\uff0c\u7136\u540e\u7ea6\u675f\u9700\u8981\u53ef\u89e3\u91ca\u7684\u53c2\u6570\u503c\uff0c\u5f15\u5165\u9ad8\u9636\u9879\u6216Transformer\u7b49\u5148\u8fdb\u67b6\u6784\u3002", "result": "\u5728\u771f\u5b9e\u4e16\u754c\u7684\u65b0\u52a0\u5761\u516c\u4ea4\u667a\u80fd\u5361\u6570\u636e\u4e0a\u9a8c\u8bc1\uff0c\u8be5\u65b9\u6cd5\u5728\u4fdd\u6301\u9009\u5b9a\u53c2\u6570\u53ef\u89e3\u91ca\u6027\u7684\u540c\u65f6\uff0c\u663e\u8457\u63d0\u9ad8\u4e86\u6a21\u578b\u51c6\u786e\u6027\u3002", "conclusion": "\u5c55\u793a\u4e86\u7406\u8bba\u9a71\u52a8\u7684\u79bb\u6563\u9009\u62e9\u6a21\u578b\u4e0e\u6570\u636e\u9a71\u52a8\u7684AI\u6a21\u578b\u76f8\u7ed3\u5408\u7684\u7edf\u4e00\u65b9\u6cd5\u6f5c\u529b\uff0c\u53ef\u5728\u4fdd\u6301\u89c4\u5212\u653f\u7b56\u5e94\u7528\u9002\u7528\u6027\u7684\u540c\u65f6\u83b7\u5f97\u66f4\u51c6\u786e\u6a21\u578b\u3002", "relevance": 35.0}}
{"id": "2509.13357", "pdf": "https://arxiv.org/pdf/2509.13357", "abs": "https://arxiv.org/abs/2509.13357", "authors": ["Yongchao Huang", "Hassan Raza"], "title": "Semantic Fusion with Fuzzy-Membership Features for Controllable Language Modelling", "categories": ["cs.AI"], "comment": "16 pages", "summary": "We propose semantic fusion, a lightweight scheme that augments a Transformer\nlanguage model (LM) with a parallel, fuzzy-membership feature channel that\nencodes token-level semantics. Each token is represented by a vector of\ninterpretable features (e.g. part-of-speech cues, shallow roles, boundary\nflags, sentiment polarity and strength) whose values are graded degrees from\ndifferentiable membership functions (e.g. power kernels). These per-token\nvectors form a sentence-level semantic matrix fused via a gated adapter into\nthe LM. Training uses standard next-token prediction, an auxiliary loss that\nreconstructs the semantic features from hidden states, and a lightweight\nuniformizer that regularizes adjective-class distributions. On a synthetic\ntwo-clause corpus with held-out adjectives for out-of-distribution (OOD)\ncontrol, semantic fusion improves perplexity and enables precise,\nuser-controllable generation of polarity and punctuation while maintaining\nmodel simplicity. This approach adds only small overhead, remains fully\ncompatible with tied input-output embeddings, and provides an interpretable\npathway for conditioned natural language generation.", "AI": {"tldr": "\u8bed\u4e49\u878d\u5408\uff1a\u4e00\u79cd\u8f7b\u91cf\u7ea7\u65b9\u6848\uff0c\u901a\u8fc7\u5e76\u884c\u6a21\u7cca\u6210\u5458\u7279\u5f81\u901a\u9053\u589e\u5f3aTransformer\u8bed\u8a00\u6a21\u578b\uff0c\u7f16\u7801token\u7ea7\u8bed\u4e49\u7279\u5f81\uff0c\u63d0\u5347\u751f\u6210\u53ef\u63a7\u6027\u548c\u53ef\u89e3\u91ca\u6027", "motivation": "\u73b0\u6709\u8bed\u8a00\u6a21\u578b\u7f3a\u4e4f\u5bf9token\u7ea7\u8bed\u4e49\u7279\u5f81\uff08\u5982\u8bcd\u6027\u3001\u60c5\u611f\u6781\u6027\u7b49\uff09\u7684\u663e\u5f0f\u7f16\u7801\uff0c\u9650\u5236\u4e86\u751f\u6210\u7684\u53ef\u63a7\u6027\u548c\u53ef\u89e3\u91ca\u6027\u3002\u4f5c\u8005\u5e0c\u671b\u5728\u4e0d\u663e\u8457\u589e\u52a0\u6a21\u578b\u590d\u6742\u5ea6\u7684\u60c5\u51b5\u4e0b\uff0c\u4e3a\u6a21\u578b\u6ce8\u5165\u53ef\u89e3\u91ca\u7684\u8bed\u4e49\u7279\u5f81\u901a\u9053", "method": "\u63d0\u51fa\u8bed\u4e49\u878d\u5408\u65b9\u6848\uff1a1\uff09\u4e3a\u6bcf\u4e2atoken\u6784\u5efa\u53ef\u89e3\u91ca\u7279\u5f81\u5411\u91cf\uff08\u8bcd\u6027\u3001\u60c5\u611f\u6781\u6027\u7b49\uff09\uff1b2\uff09\u4f7f\u7528\u53ef\u5fae\u5206\u6210\u5458\u51fd\u6570\u8ba1\u7b97\u7279\u5f81\u503c\uff1b3\uff09\u901a\u8fc7\u95e8\u63a7\u9002\u914d\u5668\u5c06\u8bed\u4e49\u77e9\u9635\u878d\u5408\u5230\u8bed\u8a00\u6a21\u578b\u4e2d\uff1b4\uff09\u4f7f\u7528\u6807\u51c6next-token\u9884\u6d4b+\u8f85\u52a9\u635f\u5931+\u8f7b\u91cf\u7ea7\u6b63\u5219\u5316\u8fdb\u884c\u8bad\u7ec3", "result": "\u5728\u5408\u6210\u53cc\u5b50\u53e5\u8bed\u6599\u5e93\u4e0a\uff0c\u8bed\u4e49\u878d\u5408\u964d\u4f4e\u4e86\u56f0\u60d1\u5ea6\uff0c\u5b9e\u73b0\u4e86\u7cbe\u786e\u7684\u7528\u6237\u53ef\u63a7\u751f\u6210\uff08\u6781\u6027\u548c\u6807\u70b9\u63a7\u5236\uff09\uff0c\u540c\u65f6\u4fdd\u6301\u6a21\u578b\u7b80\u6d01\u6027\uff0c\u4ec5\u589e\u52a0\u5c11\u91cf\u8ba1\u7b97\u5f00\u9500", "conclusion": "\u8bed\u4e49\u878d\u5408\u4e3a\u6761\u4ef6\u81ea\u7136\u8bed\u8a00\u751f\u6210\u63d0\u4f9b\u4e86\u53ef\u89e3\u91ca\u7684\u9014\u5f84\uff0c\u4fdd\u6301\u4e0e\u73b0\u6709\u5d4c\u5165\u65b9\u6848\u7684\u5b8c\u5168\u517c\u5bb9\u6027\uff0c\u662f\u589e\u5f3a\u8bed\u8a00\u6a21\u578b\u8bed\u4e49\u63a7\u5236\u80fd\u529b\u7684\u6709\u6548\u65b9\u6cd5", "relevance": 75.0}}
{"id": "2509.13723", "pdf": "https://arxiv.org/pdf/2509.13723", "abs": "https://arxiv.org/abs/2509.13723", "authors": ["Yaxin Gao", "Yao Lu", "Zongfei Zhang", "Jiaqi Nie", "Shanqing Yu", "Qi Xuan"], "title": "DSPC: Dual-Stage Progressive Compression Framework for Efficient Long-Context Reasoning", "categories": ["cs.CL"], "comment": null, "summary": "Large language models (LLMs) have achieved remarkable success in many natural\nlanguage processing (NLP) tasks. To achieve more accurate output, the prompts\nused to drive LLMs have become increasingly longer, which incurs higher\ncomputational costs. To address this prompt inflation problem, prompt\ncompression has been proposed. However, most existing methods require training\na small auxiliary model for compression, incurring a significant amount of\nadditional computation. To avoid this, we propose a two-stage, training-free\napproach, called Dual-Stage Progressive Compression (DSPC). In the\ncoarse-grained stage, semantic-related sentence filtering removes sentences\nwith low semantic value based on TF-IDF. In the fine-grained stage, token\nimportance is assessed using attention contribution, cross-model loss\ndifference, and positional importance, enabling the pruning of low-utility\ntokens while preserving semantics. We validate DSPC on LLaMA-3.1-8B-Instruct\nand GPT-3.5-Turbo under a constrained token budget and observe consistent\nimprovements. For instance, in the FewShot task of the Longbench dataset, DSPC\nachieves a performance of 49.17 by using only 3x fewer tokens, outperforming\nthe best state-of-the-art baseline LongLLMLingua by 7.76.", "AI": {"tldr": "\u63d0\u51faDSPC\u53cc\u9636\u6bb5\u8bad\u7ec3\u65e0\u5173\u7684\u63d0\u793a\u538b\u7f29\u65b9\u6cd5\uff0c\u901a\u8fc7\u8bed\u4e49\u7b5b\u9009\u548ctoken\u526a\u679d\u5728\u51cf\u5c113\u500dtoken\u7684\u60c5\u51b5\u4e0b\u63d0\u5347\u6027\u80fd7.76\u5206", "motivation": "\u89e3\u51b3LLM\u63d0\u793a\u8d8a\u6765\u8d8a\u957f\u5bfc\u81f4\u8ba1\u7b97\u6210\u672c\u589e\u52a0\u7684\u95ee\u9898\uff0c\u73b0\u6709\u65b9\u6cd5\u9700\u8981\u8bad\u7ec3\u8f85\u52a9\u6a21\u578b\u5e26\u6765\u989d\u5916\u8ba1\u7b97\u5f00\u9500", "method": "\u4e24\u9636\u6bb5\u65e0\u8bad\u7ec3\u65b9\u6cd5\uff1a\u7c97\u7c92\u5ea6\u9636\u6bb5\u57fa\u4e8eTF-IDF\u8fdb\u884c\u8bed\u4e49\u76f8\u5173\u53e5\u5b50\u8fc7\u6ee4\uff1b\u7ec6\u7c92\u5ea6\u9636\u6bb5\u4f7f\u7528\u6ce8\u610f\u529b\u8d21\u732e\u3001\u8de8\u6a21\u578b\u635f\u5931\u5dee\u5f02\u548c\u4f4d\u7f6e\u91cd\u8981\u6027\u8bc4\u4f30token\u91cd\u8981\u6027\u8fdb\u884c\u526a\u679d", "result": "\u5728LLaMA-3.1-8B-Instruct\u548cGPT-3.5-Turbo\u4e0a\u9a8c\u8bc1\uff0c\u5728Longbench FewShot\u4efb\u52a1\u4e2d\u4ec5\u75283\u500d\u5c11token\u8fbe\u523049.17\u6027\u80fd\uff0c\u8d85\u8d8a\u6700\u4f73\u57fa\u7ebfLongLLMLingua 7.76\u5206", "conclusion": "DSPC\u63d0\u4f9b\u4e86\u4e00\u79cd\u9ad8\u6548\u7684\u65e0\u8bad\u7ec3\u63d0\u793a\u538b\u7f29\u65b9\u6848\uff0c\u663e\u8457\u51cf\u5c11\u8ba1\u7b97\u6210\u672c\u540c\u65f6\u4fdd\u6301\u8bed\u4e49\u5b8c\u6574\u6027", "relevance": 85.0}}
{"id": "2509.13414", "pdf": "https://arxiv.org/pdf/2509.13414", "abs": "https://arxiv.org/abs/2509.13414", "authors": ["Nikhil Keetha", "Norman M\u00fcller", "Johannes Sch\u00f6nberger", "Lorenzo Porzi", "Yuchen Zhang", "Tobias Fischer", "Arno Knapitsch", "Duncan Zauss", "Ethan Weber", "Nelson Antunes", "Jonathon Luiten", "Manuel Lopez-Antequera", "Samuel Rota Bul\u00f2", "Christian Richardt", "Deva Ramanan", "Sebastian Scherer", "Peter Kontschieder"], "title": "MapAnything: Universal Feed-Forward Metric 3D Reconstruction", "categories": ["cs.CV", "cs.AI", "cs.LG", "cs.RO"], "comment": "Project Page: https://map-anything.github.io/", "summary": "We introduce MapAnything, a unified transformer-based feed-forward model that\ningests one or more images along with optional geometric inputs such as camera\nintrinsics, poses, depth, or partial reconstructions, and then directly\nregresses the metric 3D scene geometry and cameras. MapAnything leverages a\nfactored representation of multi-view scene geometry, i.e., a collection of\ndepth maps, local ray maps, camera poses, and a metric scale factor that\neffectively upgrades local reconstructions into a globally consistent metric\nframe. Standardizing the supervision and training across diverse datasets,\nalong with flexible input augmentation, enables MapAnything to address a broad\nrange of 3D vision tasks in a single feed-forward pass, including uncalibrated\nstructure-from-motion, calibrated multi-view stereo, monocular depth\nestimation, camera localization, depth completion, and more. We provide\nextensive experimental analyses and model ablations demonstrating that\nMapAnything outperforms or matches specialist feed-forward models while\noffering more efficient joint training behavior, thus paving the way toward a\nuniversal 3D reconstruction backbone.", "AI": {"tldr": "MapAnything\u662f\u4e00\u4e2a\u7edf\u4e00\u7684\u57fa\u4e8eTransformer\u7684\u524d\u9988\u6a21\u578b\uff0c\u80fd\u591f\u5904\u7406\u5355\u5f20\u6216\u591a\u5f20\u56fe\u50cf\u53ca\u53ef\u9009\u51e0\u4f55\u8f93\u5165\uff0c\u76f4\u63a5\u56de\u5f52\u5ea6\u91cf3D\u573a\u666f\u51e0\u4f55\u548c\u76f8\u673a\u53c2\u6570\uff0c\u5728\u591a\u79cd3D\u89c6\u89c9\u4efb\u52a1\u4e0a\u8868\u73b0\u4f18\u5f02\u3002", "motivation": "\u4e3a\u4e86\u89e3\u51b33D\u89c6\u89c9\u4efb\u52a1\u4e2d\u9700\u8981\u591a\u4e2a\u4e13\u95e8\u6a21\u578b\u7684\u95ee\u9898\uff0c\u7814\u7a76\u8005\u5e0c\u671b\u5f00\u53d1\u4e00\u4e2a\u7edf\u4e00\u7684\u6a21\u578b\u6765\u5904\u7406\u591a\u79cd3D\u91cd\u5efa\u4efb\u52a1\uff0c\u901a\u8fc7\u6807\u51c6\u5316\u76d1\u7763\u548c\u8bad\u7ec3\u6765\u5b9e\u73b0\u901a\u75283D\u91cd\u5efa\u9aa8\u5e72\u7f51\u7edc\u3002", "method": "\u4f7f\u7528\u57fa\u4e8eTransformer\u7684\u524d\u9988\u67b6\u6784\uff0c\u8f93\u5165\u56fe\u50cf\u548c\u53ef\u9009\u51e0\u4f55\u4fe1\u606f\uff0c\u91c7\u7528\u5206\u89e3\u7684\u591a\u89c6\u56fe\u573a\u666f\u51e0\u4f55\u8868\u793a\uff08\u6df1\u5ea6\u56fe\u3001\u5c40\u90e8\u5c04\u7ebf\u56fe\u3001\u76f8\u673a\u4f4d\u59ff\u548c\u5ea6\u91cf\u5c3a\u5ea6\u56e0\u5b50\uff09\uff0c\u901a\u8fc7\u7075\u6d3b\u7684\u8f93\u5165\u589e\u5f3a\u548c\u8de8\u6570\u636e\u96c6\u6807\u51c6\u5316\u8bad\u7ec3\u3002", "result": "MapAnything\u5728\u672a\u6807\u5b9a\u8fd0\u52a8\u6062\u590d\u7ed3\u6784\u3001\u6807\u5b9a\u591a\u89c6\u56fe\u7acb\u4f53\u3001\u5355\u76ee\u6df1\u5ea6\u4f30\u8ba1\u3001\u76f8\u673a\u5b9a\u4f4d\u3001\u6df1\u5ea6\u8865\u5168\u7b49\u4efb\u52a1\u4e0a\u8868\u73b0\u4f18\u4e8e\u6216\u5339\u914d\u4e13\u95e8\u7684\u524d\u9988\u6a21\u578b\uff0c\u540c\u65f6\u63d0\u4f9b\u66f4\u9ad8\u6548\u7684\u8054\u5408\u8bad\u7ec3\u6027\u80fd\u3002", "conclusion": "\u8be5\u5de5\u4f5c\u4e3a\u901a\u75283D\u91cd\u5efa\u9aa8\u5e72\u7f51\u7edc\u94fa\u5e73\u4e86\u9053\u8def\uff0c\u5c55\u793a\u4e86\u7edf\u4e00\u6a21\u578b\u5728\u591a\u79cd3D\u89c6\u89c9\u4efb\u52a1\u4e0a\u7684\u6f5c\u529b\u3002", "relevance": 40.0}}
{"id": "2509.13634", "pdf": "https://arxiv.org/pdf/2509.13634", "abs": "https://arxiv.org/abs/2509.13634", "authors": ["Md Bokhtiar Al Zami", "Md Raihan Uddin", "Dinh C. Nguyen"], "title": "Secure UAV-assisted Federated Learning: A Digital Twin-Driven Approach with Zero-Knowledge Proofs", "categories": ["cs.LG", "cs.CR"], "comment": "15 pages, under revision at IEEE Internet of Things Journal", "summary": "Federated learning (FL) has gained popularity as a privacy-preserving method\nof training machine learning models on decentralized networks. However to\nensure reliable operation of UAV-assisted FL systems, issues like as excessive\nenergy consumption, communication inefficiencies, and security vulnerabilities\nmust be solved. This paper proposes an innovative framework that integrates\nDigital Twin (DT) technology and Zero-Knowledge Federated Learning (zkFed) to\ntackle these challenges. UAVs act as mobile base stations, allowing scattered\ndevices to train FL models locally and upload model updates for aggregation. By\nincorporating DT technology, our approach enables real-time system monitoring\nand predictive maintenance, improving UAV network efficiency. Additionally,\nZero-Knowledge Proofs (ZKPs) strengthen security by allowing model verification\nwithout exposing sensitive data. To optimize energy efficiency and resource\nmanagement, we introduce a dynamic allocation strategy that adjusts UAV flight\npaths, transmission power, and processing rates based on network conditions.\nUsing block coordinate descent and convex optimization techniques, our method\nsignificantly reduces system energy consumption by up to 29.6% compared to\nconventional FL approaches. Simulation results demonstrate improved learning\nperformance, security, and scalability, positioning this framework as a\npromising solution for next-generation UAV-based intelligent networks.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u7ed3\u5408\u6570\u5b57\u5b6a\u751f\u548c\u96f6\u77e5\u8bc6\u8054\u90a6\u5b66\u4e60\u7684\u65b0\u578b\u6846\u67b6\uff0c\u7528\u4e8e\u4f18\u5316\u65e0\u4eba\u673a\u8f85\u52a9\u8054\u90a6\u5b66\u4e60\u7cfb\u7edf\u7684\u80fd\u6e90\u6548\u7387\u3001\u901a\u4fe1\u6548\u7387\u548c\u5b89\u5168\u6027\u3002", "motivation": "\u89e3\u51b3\u65e0\u4eba\u673a\u8f85\u52a9\u8054\u90a6\u5b66\u4e60\u7cfb\u7edf\u4e2d\u5b58\u5728\u7684\u9ad8\u80fd\u8017\u3001\u901a\u4fe1\u6548\u7387\u4f4e\u4e0b\u548c\u5b89\u5168\u6f0f\u6d1e\u7b49\u95ee\u9898\uff0c\u786e\u4fdd\u7cfb\u7edf\u7684\u53ef\u9760\u8fd0\u884c\u3002", "method": "\u96c6\u6210\u6570\u5b57\u5b6a\u751f\u6280\u672f\u5b9e\u73b0\u5b9e\u65f6\u7cfb\u7edf\u76d1\u63a7\u548c\u9884\u6d4b\u6027\u7ef4\u62a4\uff0c\u4f7f\u7528\u96f6\u77e5\u8bc6\u8bc1\u660e\u589e\u5f3a\u5b89\u5168\u6027\uff0c\u91c7\u7528\u52a8\u6001\u5206\u914d\u7b56\u7565\u4f18\u5316\u65e0\u4eba\u673a\u98de\u884c\u8def\u5f84\u3001\u4f20\u8f93\u529f\u7387\u548c\u5904\u7406\u901f\u7387\uff0c\u8fd0\u7528\u5757\u5750\u6807\u4e0b\u964d\u548c\u51f8\u4f18\u5316\u6280\u672f\u3002", "result": "\u76f8\u6bd4\u4f20\u7edf\u8054\u90a6\u5b66\u4e60\u65b9\u6cd5\uff0c\u7cfb\u7edf\u80fd\u8017\u964d\u4f4e\u9ad8\u8fbe29.6%\uff0c\u5b66\u4e60\u6027\u80fd\u3001\u5b89\u5168\u6027\u548c\u53ef\u6269\u5c55\u6027\u5747\u5f97\u5230\u663e\u8457\u63d0\u5347\u3002", "conclusion": "\u8be5\u6846\u67b6\u4e3a\u4e0b\u4e00\u4ee3\u65e0\u4eba\u673a\u667a\u80fd\u7f51\u7edc\u63d0\u4f9b\u4e86\u4e00\u4e2a\u6709\u524d\u666f\u7684\u89e3\u51b3\u65b9\u6848\u3002", "relevance": 25.0}}
{"id": "2509.13364", "pdf": "https://arxiv.org/pdf/2509.13364", "abs": "https://arxiv.org/abs/2509.13364", "authors": ["Zixi Li"], "title": "Asterisk Operator", "categories": ["cs.AI"], "comment": "Code available at: https://github.com/lizixi-0x2F/Asterisk-Games", "summary": "We propose the \\textbf{Asterisk Operator} ($\\ast$-operator), a novel unified\nframework for abstract reasoning based on Adjacency-Structured Parallel\nPropagation (ASPP). The operator formalizes structured reasoning tasks as\nlocal, parallel state evolution processes guided by implicit relational graphs.\nWe prove that the $\\ast$-operator maintains local computational constraints\nwhile achieving global reasoning capabilities, providing an efficient and\nconvergent computational paradigm for abstract reasoning problems. Through\nrigorous mathematical analysis and comprehensive experiments on ARC2 challenges\nand Conway's Game of Life, we demonstrate the operator's universality,\nconvergence properties, and superior performance. Our innovative\nEmbedding-Asterisk distillation method achieves 100\\% accuracy on ARC2\nvalidation with only 6M parameters, representing a significant breakthrough in\nneural-symbolic reasoning.\n  \\textbf{Keywords:} Abstract Reasoning, Adjacency Structure, Parallel\nPropagation, Asterisk Operator, Convergence, Universal Approximation", "AI": {"tldr": "\u63d0\u51fa\u4e86\u661f\u53f7\u7b97\u5b50(*-operator)\uff0c\u4e00\u4e2a\u57fa\u4e8e\u90bb\u63a5\u7ed3\u6784\u5e76\u884c\u4f20\u64ad(ASPP)\u7684\u7edf\u4e00\u62bd\u8c61\u63a8\u7406\u6846\u67b6\uff0c\u5c06\u7ed3\u6784\u5316\u63a8\u7406\u4efb\u52a1\u5f62\u5f0f\u5316\u4e3a\u7531\u9690\u5f0f\u5173\u7cfb\u56fe\u6307\u5bfc\u7684\u5c40\u90e8\u5e76\u884c\u72b6\u6001\u6f14\u5316\u8fc7\u7a0b\u3002", "motivation": "\u4e3a\u4e86\u89e3\u51b3\u62bd\u8c61\u63a8\u7406\u4efb\u52a1\u4e2d\u7684\u8ba1\u7b97\u6548\u7387\u548c\u5168\u5c40\u63a8\u7406\u80fd\u529b\u4e4b\u95f4\u7684\u5e73\u8861\u95ee\u9898\uff0c\u5f00\u53d1\u4e00\u4e2a\u65e2\u80fd\u4fdd\u6301\u5c40\u90e8\u8ba1\u7b97\u7ea6\u675f\u53c8\u80fd\u5b9e\u73b0\u5168\u5c40\u63a8\u7406\u7684\u7edf\u4e00\u6846\u67b6\u3002", "method": "\u57fa\u4e8e\u90bb\u63a5\u7ed3\u6784\u5e76\u884c\u4f20\u64ad(ASPP)\u7684\u661f\u53f7\u7b97\u5b50\uff0c\u5c06\u63a8\u7406\u4efb\u52a1\u5efa\u6a21\u4e3a\u5c40\u90e8\u5e76\u884c\u72b6\u6001\u6f14\u5316\u8fc7\u7a0b\uff0c\u5e76\u63d0\u51fa\u4e86Embedding-Asterisk\u84b8\u998f\u65b9\u6cd5\u3002", "result": "\u5728ARC2\u6311\u6218\u548c\u5eb7\u5a01\u751f\u547d\u6e38\u620f\u4e2d\u9a8c\u8bc1\u4e86\u7b97\u5b50\u7684\u666e\u9002\u6027\u3001\u6536\u655b\u6027\u548c\u4f18\u8d8a\u6027\u80fd\uff0c\u4f7f\u7528\u4ec56M\u53c2\u6570\u5728ARC2\u9a8c\u8bc1\u96c6\u4e0a\u8fbe\u5230100%\u51c6\u786e\u7387\u3002", "conclusion": "\u661f\u53f7\u7b97\u5b50\u4e3a\u795e\u7ecf\u7b26\u53f7\u63a8\u7406\u63d0\u4f9b\u4e86\u9ad8\u6548\u4e14\u6536\u655b\u7684\u8ba1\u7b97\u8303\u5f0f\uff0c\u5728\u62bd\u8c61\u63a8\u7406\u9886\u57df\u53d6\u5f97\u4e86\u663e\u8457\u7a81\u7834\u3002", "relevance": 65.0}}
{"id": "2509.13734", "pdf": "https://arxiv.org/pdf/2509.13734", "abs": "https://arxiv.org/abs/2509.13734", "authors": ["Yosuke Mikami", "Daiki Matsuoka", "Hitomi Yanaka"], "title": "Implementing a Logical Inference System for Japanese Comparatives", "categories": ["cs.CL"], "comment": "In Proceedings of the 5th Workshop on Natural Logic Meets Machine\n  Learning (NALOMA)", "summary": "Natural Language Inference (NLI) involving comparatives is challenging\nbecause it requires understanding quantities and comparative relations\nexpressed by sentences. While some approaches leverage Large Language Models\n(LLMs), we focus on logic-based approaches grounded in compositional semantics,\nwhich are promising for robust handling of numerical and logical expressions.\nPrevious studies along these lines have proposed logical inference systems for\nEnglish comparatives. However, it has been pointed out that there are several\nmorphological and semantic differences between Japanese and English\ncomparatives. These differences make it difficult to apply such systems\ndirectly to Japanese comparatives. To address this gap, this study proposes\nccg-jcomp, a logical inference system for Japanese comparatives based on\ncompositional semantics. We evaluate the proposed system on a Japanese NLI\ndataset containing comparative expressions. We demonstrate the effectiveness of\nour system by comparing its accuracy with that of existing LLMs.", "AI": {"tldr": "\u63d0\u51fa\u4e86ccg-jcomp\u7cfb\u7edf\uff0c\u8fd9\u662f\u4e00\u4e2a\u57fa\u4e8e\u7ec4\u5408\u8bed\u4e49\u7684\u65e5\u8bed\u6bd4\u8f83\u53e5\u903b\u8f91\u63a8\u7406\u7cfb\u7edf\uff0c\u7528\u4e8e\u5904\u7406\u65e5\u8bed\u81ea\u7136\u8bed\u8a00\u63a8\u7406\u4e2d\u7684\u6bd4\u8f83\u8868\u8fbe\uff0c\u5e76\u5728\u65e5\u8bedNLI\u6570\u636e\u96c6\u4e0a\u8bc4\u4f30\u5176\u6027\u80fd\u3002", "motivation": "\u65e5\u8bed\u548c\u82f1\u8bed\u6bd4\u8f83\u53e5\u5728\u5f62\u6001\u548c\u8bed\u4e49\u4e0a\u5b58\u5728\u663e\u8457\u5dee\u5f02\uff0c\u4f7f\u5f97\u73b0\u6709\u7684\u82f1\u8bed\u6bd4\u8f83\u53e5\u903b\u8f91\u63a8\u7406\u7cfb\u7edf\u96be\u4ee5\u76f4\u63a5\u5e94\u7528\u4e8e\u65e5\u8bed\u3002\u9700\u8981\u5f00\u53d1\u4e13\u95e8\u9488\u5bf9\u65e5\u8bed\u6bd4\u8f83\u53e5\u7684\u903b\u8f91\u63a8\u7406\u7cfb\u7edf\u3002", "method": "\u57fa\u4e8e\u7ec4\u5408\u8bed\u4e49\u6784\u5efa\u903b\u8f91\u63a8\u7406\u7cfb\u7edfccg-jcomp\uff0c\u4e13\u95e8\u5904\u7406\u65e5\u8bed\u6bd4\u8f83\u53e5\u7684\u6570\u503c\u548c\u903b\u8f91\u8868\u8fbe\u5f0f\uff0c\u5e76\u5728\u5305\u542b\u6bd4\u8f83\u8868\u8fbe\u7684\u65e5\u8bedNLI\u6570\u636e\u96c6\u4e0a\u8fdb\u884c\u8bc4\u4f30\u3002", "result": "\u7cfb\u7edf\u5728\u65e5\u8bedNLI\u6570\u636e\u96c6\u4e0a\u8868\u73b0\u51fa\u6709\u6548\u6027\uff0c\u901a\u8fc7\u4e0e\u73b0\u6709\u5927\u578b\u8bed\u8a00\u6a21\u578b\u7684\u51c6\u786e\u7387\u6bd4\u8f83\u8bc1\u660e\u4e86\u5176\u4f18\u52bf\u3002", "conclusion": "ccg-jcomp\u7cfb\u7edf\u4e3a\u65e5\u8bed\u6bd4\u8f83\u53e5\u7684\u903b\u8f91\u63a8\u7406\u63d0\u4f9b\u4e86\u6709\u6548\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u5c55\u793a\u4e86\u7ec4\u5408\u8bed\u4e49\u65b9\u6cd5\u5728\u5904\u7406\u8bed\u8a00\u7279\u5f02\u6027\u95ee\u9898\u4e0a\u7684\u4ef7\u503c\u3002", "relevance": 35.0}}
{"id": "2509.13474", "pdf": "https://arxiv.org/pdf/2509.13474", "abs": "https://arxiv.org/abs/2509.13474", "authors": ["Yujia Lin", "Nicholas Evans"], "title": "Semantic-Enhanced Cross-Modal Place Recognition for Robust Robot Localization", "categories": ["cs.CV"], "comment": null, "summary": "Ensuring accurate localization of robots in environments without GPS\ncapability is a challenging task. Visual Place Recognition (VPR) techniques can\npotentially achieve this goal, but existing RGB-based methods are sensitive to\nchanges in illumination, weather, and other seasonal changes. Existing\ncross-modal localization methods leverage the geometric properties of RGB\nimages and 3D LiDAR maps to reduce the sensitivity issues highlighted above.\nCurrently, state-of-the-art methods struggle in complex scenes, fine-grained or\nhigh-resolution matching, and situations where changes can occur in viewpoint.\nIn this work, we introduce a framework we call Semantic-Enhanced Cross-Modal\nPlace Recognition (SCM-PR) that combines high-level semantics utilizing RGB\nimages for robust localization in LiDAR maps. Our proposed method introduces: a\nVMamba backbone for feature extraction of RGB images; a Semantic-Aware Feature\nFusion (SAFF) module for using both place descriptors and segmentation masks;\nLiDAR descriptors that incorporate both semantics and geometry; and a\ncross-modal semantic attention mechanism in NetVLAD to improve matching.\nIncorporating the semantic information also was instrumental in designing a\nMulti-View Semantic-Geometric Matching and a Semantic Consistency Loss, both in\na contrastive learning framework. Our experimental work on the KITTI and\nKITTI-360 datasets show that SCM-PR achieves state-of-the-art performance\ncompared to other cross-modal place recognition methods.", "AI": {"tldr": "SCM-PR\u662f\u4e00\u4e2a\u7ed3\u5408RGB\u56fe\u50cf\u548cLiDAR\u5730\u56fe\u7684\u8de8\u6a21\u6001\u4f4d\u7f6e\u8bc6\u522b\u6846\u67b6\uff0c\u901a\u8fc7\u8bed\u4e49\u589e\u5f3a\u548c\u51e0\u4f55\u7279\u5f81\u878d\u5408\uff0c\u5728\u590d\u6742\u573a\u666f\u4e2d\u5b9e\u73b0\u9c81\u68d2\u7684\u673a\u5668\u4eba\u5b9a\u4f4d\u3002", "motivation": "\u89e3\u51b3RGB\u56fe\u50cf\u5728\u5149\u7167\u3001\u5929\u6c14\u548c\u5b63\u8282\u53d8\u5316\u4e0b\u7684\u654f\u611f\u6027\u95ee\u9898\uff0c\u4ee5\u53ca\u73b0\u6709\u8de8\u6a21\u6001\u5b9a\u4f4d\u65b9\u6cd5\u5728\u590d\u6742\u573a\u666f\u3001\u7ec6\u7c92\u5ea6\u5339\u914d\u548c\u89c6\u89d2\u53d8\u5316\u4e0b\u7684\u6027\u80fd\u4e0d\u8db3\u3002", "method": "\u63d0\u51faSCM-PR\u6846\u67b6\uff1a\u4f7f\u7528VMamba\u4e3b\u5e72\u7f51\u7edc\u63d0\u53d6RGB\u7279\u5f81\uff1b\u8bed\u4e49\u611f\u77e5\u7279\u5f81\u878d\u5408\u6a21\u5757\uff1b\u7ed3\u5408\u8bed\u4e49\u548c\u51e0\u4f55\u7684LiDAR\u63cf\u8ff0\u7b26\uff1b\u8de8\u6a21\u6001\u8bed\u4e49\u6ce8\u610f\u529b\u673a\u5236\uff1b\u591a\u89c6\u89d2\u8bed\u4e49-\u51e0\u4f55\u5339\u914d\u548c\u8bed\u4e49\u4e00\u81f4\u6027\u635f\u5931\u3002", "result": "\u5728KITTI\u548cKITTI-360\u6570\u636e\u96c6\u4e0a\u5b9e\u73b0\u4e86state-of-the-art\u6027\u80fd\uff0c\u76f8\u6bd4\u5176\u4ed6\u8de8\u6a21\u6001\u4f4d\u7f6e\u8bc6\u522b\u65b9\u6cd5\u8868\u73b0\u66f4\u4f18\u3002", "conclusion": "\u901a\u8fc7\u8bed\u4e49\u589e\u5f3a\u548c\u8de8\u6a21\u6001\u878d\u5408\uff0c\u663e\u8457\u63d0\u5347\u4e86\u5728\u590d\u6742\u73af\u5883\u4e0b\u7684\u673a\u5668\u4eba\u5b9a\u4f4d\u9c81\u68d2\u6027\u548c\u51c6\u786e\u6027\u3002", "relevance": 25.0}}
{"id": "2509.13636", "pdf": "https://arxiv.org/pdf/2509.13636", "abs": "https://arxiv.org/abs/2509.13636", "authors": ["Yasin Hasanpoor", "Bahram Tarvirdizadeh", "Khalil Alipour", "Mohammad Ghamari"], "title": "Multimodal signal fusion for stress detection using deep neural networks: a novel approach for converting 1D signals to unified 2D images", "categories": ["cs.LG"], "comment": "14 pages 7 images 2 tables", "summary": "This study introduces a novel method that transforms multimodal physiological\nsignalsphotoplethysmography (PPG), galvanic skin response (GSR), and\nacceleration (ACC) into 2D image matrices to enhance stress detection using\nconvolutional neural networks (CNNs). Unlike traditional approaches that\nprocess these signals separately or rely on fixed encodings, our technique\nfuses them into structured image representations that enable CNNs to capture\ntemporal and cross signal dependencies more effectively. This image based\ntransformation not only improves interpretability but also serves as a robust\nform of data augmentation. To further enhance generalization and model\nrobustness, we systematically reorganize the fused signals into multiple\nformats, combining them in a multi stage training pipeline. This approach\nsignificantly boosts classification performance. While demonstrated here in the\ncontext of stress detection, the proposed method is broadly applicable to any\ndomain involving multimodal physiological signals, paving the way for more\naccurate, personalized, and real time health monitoring through wearable\ntechnologies.", "AI": {"tldr": "\u8be5\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u79cd\u5c06\u591a\u6a21\u6001\u751f\u7406\u4fe1\u53f7\u8f6c\u6362\u4e3a2D\u56fe\u50cf\u77e9\u9635\u7684\u65b0\u65b9\u6cd5\uff0c\u4f7f\u7528CNN\u8fdb\u884c\u538b\u529b\u68c0\u6d4b\uff0c\u901a\u8fc7\u4fe1\u53f7\u878d\u5408\u548c\u56fe\u50cf\u8868\u793a\u63d0\u5347\u5206\u7c7b\u6027\u80fd\u3002", "motivation": "\u4f20\u7edf\u65b9\u6cd5\u5355\u72ec\u5904\u7406\u591a\u6a21\u6001\u751f\u7406\u4fe1\u53f7\u6216\u4f9d\u8d56\u56fa\u5b9a\u7f16\u7801\uff0c\u65e0\u6cd5\u6709\u6548\u6355\u6349\u4fe1\u53f7\u95f4\u7684\u65f6\u7a7a\u4f9d\u8d56\u5173\u7cfb\u3002\u7814\u7a76\u65e8\u5728\u901a\u8fc7\u56fe\u50cf\u5316\u8f6c\u6362\u63d0\u5347\u4fe1\u53f7\u5904\u7406\u7684\u6548\u7387\u548c\u53ef\u89e3\u91ca\u6027\u3002", "method": "\u5c06PPG\u3001GSR\u548cACC\u4e09\u79cd\u751f\u7406\u4fe1\u53f7\u878d\u5408\u4e3a\u7ed3\u6784\u53162D\u56fe\u50cf\u77e9\u9635\uff0c\u91c7\u7528\u591a\u9636\u6bb5\u8bad\u7ec3\u7ba1\u9053\u7cfb\u7edf\u91cd\u7ec4\u4fe1\u53f7\u683c\u5f0f\uff0c\u5229\u7528CNN\u6355\u6349\u65f6\u5e8f\u548c\u8de8\u4fe1\u53f7\u4f9d\u8d56\u5173\u7cfb\u3002", "result": "\u8be5\u65b9\u6cd5\u663e\u8457\u63d0\u5347\u4e86\u538b\u529b\u68c0\u6d4b\u7684\u5206\u7c7b\u6027\u80fd\uff0c\u540c\u65f6\u63d0\u9ad8\u4e86\u6a21\u578b\u7684\u6cdb\u5316\u80fd\u529b\u548c\u9c81\u68d2\u6027\uff0c\u53ef\u4f5c\u4e3a\u6709\u6548\u7684\u6570\u636e\u589e\u5f3a\u624b\u6bb5\u3002", "conclusion": "\u63d0\u51fa\u7684\u56fe\u50cf\u5316\u8f6c\u6362\u65b9\u6cd5\u4e0d\u4ec5\u9002\u7528\u4e8e\u538b\u529b\u68c0\u6d4b\uff0c\u8fd8\u53ef\u5e7f\u6cdb\u5e94\u7528\u4e8e\u4efb\u4f55\u6d89\u53ca\u591a\u6a21\u6001\u751f\u7406\u4fe1\u53f7\u7684\u9886\u57df\uff0c\u4e3a\u53ef\u7a7f\u6234\u6280\u672f\u7684\u5b9e\u65f6\u5065\u5eb7\u76d1\u6d4b\u63d0\u4f9b\u4e86\u65b0\u9014\u5f84\u3002", "relevance": 25.0}}
{"id": "2509.13368", "pdf": "https://arxiv.org/pdf/2509.13368", "abs": "https://arxiv.org/abs/2509.13368", "authors": ["Yuan Wei", "Xiaohan Shan", "Ran Miao", "Jianmin Li"], "title": "$Agent^2$: An Agent-Generates-Agent Framework for Reinforcement Learning Automation", "categories": ["cs.AI", "cs.LG"], "comment": "9 pages, 7 figures", "summary": "Reinforcement learning agent development traditionally requires extensive\nexpertise and lengthy iterations, often resulting in high failure rates and\nlimited accessibility. This paper introduces $Agent^2$, a novel\nagent-generates-agent framework that achieves fully automated RL agent design\nthrough intelligent LLM-driven generation. The system autonomously transforms\nnatural language task descriptions and environment code into comprehensive,\nhigh-performance reinforcement learning solutions without human intervention.\n$Agent^2$ features a revolutionary dual-agent architecture. The Generator Agent\nserves as an autonomous AI designer that analyzes tasks and generates\nexecutable RL agents, while the Target Agent is the resulting automatically\ngenerated RL agent. The framework decomposes RL development into two distinct\nstages: MDP modeling and algorithmic optimization, enabling more targeted and\neffective agent generation. Built on the Model Context Protocol, $Agent^2$\nprovides a unified framework that standardizes intelligent agent creation\nacross diverse environments and algorithms, while incorporating adaptive\ntraining management and intelligent feedback analysis for continuous\nimprovement. Extensive experiments on a wide range of benchmarks, including\nMuJoCo, MetaDrive, MPE, and SMAC, demonstrate that $Agent^2$ consistently\noutperforms manually designed solutions across all tasks, achieving up to 55%\nperformance improvement and substantial gains on average. By enabling truly\nend-to-end, closed-loop automation, this work establishes a new paradigm in\nwhich intelligent agents design and optimize other agents, marking a\nfundamental breakthrough for automated AI systems.", "AI": {"tldr": "Agent\u00b2\u662f\u4e00\u4e2a\u5b8c\u5168\u81ea\u52a8\u5316\u7684RL\u667a\u80fd\u4f53\u751f\u6210\u6846\u67b6\uff0c\u901a\u8fc7LLM\u9a71\u52a8\u7684\u53cc\u667a\u80fd\u4f53\u67b6\u6784\uff0c\u5c06\u81ea\u7136\u8bed\u8a00\u4efb\u52a1\u63cf\u8ff0\u81ea\u52a8\u8f6c\u6362\u4e3a\u9ad8\u6027\u80fd\u5f3a\u5316\u5b66\u4e60\u89e3\u51b3\u65b9\u6848\uff0c\u65e0\u9700\u4eba\u5de5\u5e72\u9884\u3002", "motivation": "\u4f20\u7edfRL\u667a\u80fd\u4f53\u5f00\u53d1\u9700\u8981\u5927\u91cf\u4e13\u4e1a\u77e5\u8bc6\u548c\u8fed\u4ee3\uff0c\u5931\u8d25\u7387\u9ad8\u4e14\u96be\u4ee5\u666e\u53ca\u3002\u4e3a\u4e86\u89e3\u51b3\u8fd9\u4e2a\u95ee\u9898\uff0c\u9700\u8981\u5b9e\u73b0\u5b8c\u5168\u81ea\u52a8\u5316\u7684RL\u667a\u80fd\u4f53\u8bbe\u8ba1\u3002", "method": "\u91c7\u7528\u53cc\u667a\u80fd\u4f53\u67b6\u6784\uff1a\u751f\u6210\u5668\u667a\u80fd\u4f53\u4f5c\u4e3a\u81ea\u4e3bAI\u8bbe\u8ba1\u5668\u5206\u6790\u4efb\u52a1\u5e76\u751f\u6210\u53ef\u6267\u884cRL\u667a\u80fd\u4f53\uff0c\u76ee\u6807\u667a\u80fd\u4f53\u662f\u81ea\u52a8\u751f\u6210\u7684RL\u667a\u80fd\u4f53\u3002\u6846\u67b6\u5c06RL\u5f00\u53d1\u5206\u89e3\u4e3aMDP\u5efa\u6a21\u548c\u7b97\u6cd5\u4f18\u5316\u4e24\u4e2a\u9636\u6bb5\uff0c\u57fa\u4e8e\u6a21\u578b\u4e0a\u4e0b\u6587\u534f\u8bae\u63d0\u4f9b\u7edf\u4e00\u6846\u67b6\u3002", "result": "\u5728MuJoCo\u3001MetaDrive\u3001MPE\u548cSMAC\u7b49\u591a\u4e2a\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0cAgent\u00b2\u5728\u6240\u6709\u4efb\u52a1\u4e0a\u90fd\u4f18\u4e8e\u4eba\u5de5\u8bbe\u8ba1\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u6027\u80fd\u63d0\u5347\u9ad8\u8fbe55%\uff0c\u5e73\u5747\u8868\u73b0\u4e5f\u6709\u663e\u8457\u63d0\u5347\u3002", "conclusion": "\u8fd9\u9879\u5de5\u4f5c\u5efa\u7acb\u4e86\u667a\u80fd\u4f53\u8bbe\u8ba1\u548c\u4f18\u5316\u5176\u4ed6\u667a\u80fd\u4f53\u7684\u65b0\u8303\u5f0f\uff0c\u5b9e\u73b0\u4e86\u771f\u6b63\u7aef\u5230\u7aef\u7684\u95ed\u73af\u81ea\u52a8\u5316\uff0c\u662f\u81ea\u52a8\u5316AI\u7cfb\u7edf\u7684\u6839\u672c\u6027\u7a81\u7834\u3002", "relevance": 85.0}}
{"id": "2509.13775", "pdf": "https://arxiv.org/pdf/2509.13775", "abs": "https://arxiv.org/abs/2509.13775", "authors": ["Vani Kanjirangat", "Ljiljana Dolamic", "Fabio Rinaldi"], "title": "Exploring Data and Parameter Efficient Strategies for Arabic Dialect Identifications", "categories": ["cs.CL", "cs.AI"], "comment": "4 main pages, 4 additional, 5 figures", "summary": "This paper discusses our exploration of different data-efficient and\nparameter-efficient approaches to Arabic Dialect Identification (ADI). In\nparticular, we investigate various soft-prompting strategies, including\nprefix-tuning, prompt-tuning, P-tuning, and P-tuning V2, as well as LoRA\nreparameterizations. For the data-efficient strategy, we analyze hard prompting\nwith zero-shot and few-shot inferences to analyze the dialect identification\ncapabilities of Large Language Models (LLMs). For the parameter-efficient PEFT\napproaches, we conducted our experiments using Arabic-specific encoder models\non several major datasets. We also analyzed the n-shot inferences on\nopen-source decoder-only models, a general multilingual model (Phi-3.5), and an\nArabic-specific one(SILMA). We observed that the LLMs generally struggle to\ndifferentiate the dialectal nuances in the few-shot or zero-shot setups. The\nsoft-prompted encoder variants perform better, while the LoRA-based fine-tuned\nmodels perform best, even surpassing full fine-tuning.", "AI": {"tldr": "\u672c\u6587\u63a2\u7d22\u4e86\u963f\u62c9\u4f2f\u8bed\u65b9\u8a00\u8bc6\u522b(ADI)\u7684\u6570\u636e\u9ad8\u6548\u548c\u53c2\u6570\u9ad8\u6548\u65b9\u6cd5\uff0c\u5305\u62ec\u8f6f\u63d0\u793a\u7b56\u7565\u3001LoRA\u91cd\u53c2\u6570\u5316\u4ee5\u53ca\u96f6\u6837\u672c/\u5c11\u6837\u672c\u63a8\u7406\uff0c\u53d1\u73b0LLM\u5728\u533a\u5206\u65b9\u8a00\u7ec6\u5fae\u5dee\u522b\u65b9\u9762\u8868\u73b0\u4e0d\u4f73\uff0c\u800cLoRA\u5fae\u8c03\u6a21\u578b\u8868\u73b0\u6700\u4f73\u3002", "motivation": "\u7814\u7a76\u5982\u4f55\u901a\u8fc7\u53c2\u6570\u9ad8\u6548\u5fae\u8c03(PEFT)\u548c\u6570\u636e\u9ad8\u6548\u65b9\u6cd5\u6765\u89e3\u51b3\u963f\u62c9\u4f2f\u8bed\u65b9\u8a00\u8bc6\u522b\u4efb\u52a1\uff0c\u7279\u522b\u662f\u5728\u8d44\u6e90\u6709\u9650\u7684\u60c5\u51b5\u4e0b\u63d0\u5347\u6a21\u578b\u6027\u80fd\u3002", "method": "\u4f7f\u7528\u591a\u79cd\u8f6f\u63d0\u793a\u7b56\u7565(prefix-tuning\u3001prompt-tuning\u3001P-tuning\u3001P-tuning V2)\u548cLoRA\u91cd\u53c2\u6570\u5316\uff1b\u5206\u6790\u96f6\u6837\u672c\u548c\u5c11\u6837\u672c\u63a8\u7406\uff1b\u5728\u963f\u62c9\u4f2f\u8bed\u4e13\u7528\u7f16\u7801\u5668\u6a21\u578b\u548c\u591a\u8bed\u8a00\u89e3\u7801\u5668\u6a21\u578b\u4e0a\u8fdb\u884c\u5b9e\u9a8c\u3002", "result": "LLM\u5728\u96f6\u6837\u672c\u548c\u5c11\u6837\u672c\u8bbe\u7f6e\u4e0b\u96be\u4ee5\u533a\u5206\u65b9\u8a00\u7ec6\u5fae\u5dee\u522b\uff1b\u8f6f\u63d0\u793a\u7f16\u7801\u5668\u53d8\u4f53\u8868\u73b0\u8f83\u597d\uff1b\u57fa\u4e8eLoRA\u5fae\u8c03\u7684\u6a21\u578b\u8868\u73b0\u6700\u4f73\uff0c\u751a\u81f3\u8d85\u8fc7\u5168\u53c2\u6570\u5fae\u8c03\u3002", "conclusion": "LoRA\u7b49\u53c2\u6570\u9ad8\u6548\u65b9\u6cd5\u5728\u963f\u62c9\u4f2f\u8bed\u65b9\u8a00\u8bc6\u522b\u4efb\u52a1\u4e2d\u8868\u73b0\u51fa\u8272\uff0c\u800cLLM\u5728\u5c11\u6837\u672c\u573a\u666f\u4e0b\u7684\u65b9\u8a00\u8bc6\u522b\u80fd\u529b\u6709\u9650\u3002", "relevance": 75.0}}
{"id": "2509.13482", "pdf": "https://arxiv.org/pdf/2509.13482", "abs": "https://arxiv.org/abs/2509.13482", "authors": ["Hao Xu", "Xiaolin Wu", "Xi Zhang"], "title": "Improving 3D Gaussian Splatting Compression by Scene-Adaptive Lattice Vector Quantization", "categories": ["cs.CV"], "comment": "Code available at https://github.com/hxu160/SALVQ", "summary": "3D Gaussian Splatting (3DGS) is rapidly gaining popularity for its\nphotorealistic rendering quality and real-time performance, but it generates\nmassive amounts of data. Hence compressing 3DGS data is necessary for the cost\neffectiveness of 3DGS models. Recently, several anchor-based neural compression\nmethods have been proposed, achieving good 3DGS compression performance.\nHowever, they all rely on uniform scalar quantization (USQ) due to its\nsimplicity. A tantalizing question is whether more sophisticated quantizers can\nimprove the current 3DGS compression methods with very little extra overhead\nand minimal change to the system. The answer is yes by replacing USQ with\nlattice vector quantization (LVQ). To better capture scene-specific\ncharacteristics, we optimize the lattice basis for each scene, improving LVQ's\nadaptability and R-D efficiency. This scene-adaptive LVQ (SALVQ) strikes a\nbalance between the R-D efficiency of vector quantization and the low\ncomplexity of USQ. SALVQ can be seamlessly integrated into existing 3DGS\ncompression architectures, enhancing their R-D performance with minimal\nmodifications and computational overhead. Moreover, by scaling the lattice\nbasis vectors, SALVQ can dynamically adjust lattice density, enabling a single\nmodel to accommodate multiple bit rate targets. This flexibility eliminates the\nneed to train separate models for different compression levels, significantly\nreducing training time and memory consumption.", "AI": {"tldr": "\u672c\u6587\u63d0\u51faSALVQ\u65b9\u6cd5\uff0c\u7528\u573a\u666f\u81ea\u9002\u5e94\u683c\u70b9\u5411\u91cf\u91cf\u5316\u66ff\u4ee3\u5747\u5300\u6807\u91cf\u91cf\u5316\uff0c\u63d0\u53473D\u9ad8\u65af\u6cfc\u6e85\u6570\u636e\u7684\u538b\u7f29\u6027\u80fd\uff0c\u5b9e\u73b0\u66f4\u597d\u7684\u7387\u5931\u771f\u6548\u7387\u548c\u7075\u6d3b\u6027\u3002", "motivation": "3D\u9ad8\u65af\u6cfc\u6e85\u6280\u672f\u867d\u7136\u6e32\u67d3\u8d28\u91cf\u9ad8\u4e14\u5b9e\u65f6\u6027\u597d\uff0c\u4f46\u6570\u636e\u91cf\u5de8\u5927\u9700\u8981\u538b\u7f29\u3002\u73b0\u6709\u65b9\u6cd5\u4f7f\u7528\u7b80\u5355\u7684\u5747\u5300\u6807\u91cf\u91cf\u5316\uff0c\u4f46\u66f4\u590d\u6742\u7684\u91cf\u5316\u5668\u53ef\u80fd\u5e26\u6765\u66f4\u597d\u7684\u538b\u7f29\u6548\u679c\u3002", "method": "\u91c7\u7528\u683c\u70b9\u5411\u91cf\u91cf\u5316(LVQ)\u66ff\u4ee3\u5747\u5300\u6807\u91cf\u91cf\u5316\uff0c\u5e76\u4e3a\u6bcf\u4e2a\u573a\u666f\u4f18\u5316\u683c\u70b9\u57fa\u5411\u91cf\uff0c\u63d0\u51fa\u573a\u666f\u81ea\u9002\u5e94LVQ(SALVQ)\u65b9\u6cd5\uff0c\u53ef\u65e0\u7f1d\u96c6\u6210\u5230\u73b0\u67093DGS\u538b\u7f29\u67b6\u6784\u4e2d\u3002", "result": "SALVQ\u5728\u7387\u5931\u771f\u6548\u7387\u4e0a\u4f18\u4e8e\u5747\u5300\u6807\u91cf\u91cf\u5316\uff0c\u901a\u8fc7\u7f29\u653e\u683c\u70b9\u57fa\u5411\u91cf\u53ef\u52a8\u6001\u8c03\u6574\u538b\u7f29\u7387\uff0c\u5355\u4e2a\u6a21\u578b\u652f\u6301\u591a\u6bd4\u7279\u7387\u76ee\u6807\uff0c\u51cf\u5c11\u8bad\u7ec3\u65f6\u95f4\u548c\u5185\u5b58\u6d88\u8017\u3002", "conclusion": "\u573a\u666f\u81ea\u9002\u5e94\u683c\u70b9\u5411\u91cf\u91cf\u5316\u662f\u63d0\u53473D\u9ad8\u65af\u6cfc\u6e85\u6570\u636e\u538b\u7f29\u6027\u80fd\u7684\u6709\u6548\u65b9\u6cd5\uff0c\u5728\u4fdd\u6301\u4f4e\u590d\u6742\u5ea6\u7684\u540c\u65f6\u63d0\u4f9b\u66f4\u597d\u7684\u538b\u7f29\u6548\u7387\u548c\u7075\u6d3b\u6027\u3002", "relevance": 25.0}}
{"id": "2509.13642", "pdf": "https://arxiv.org/pdf/2509.13642", "abs": "https://arxiv.org/abs/2509.13642", "authors": ["Zirun Guo", "Feng Zhang", "Kai Jia", "Tao Jin"], "title": "LLM-I: LLMs are Naturally Interleaved Multimodal Creators", "categories": ["cs.LG", "cs.CV"], "comment": null, "summary": "We propose LLM-Interleaved (LLM-I), a flexible and dynamic framework that\nreframes interleaved image-text generation as a tool-use problem. LLM-I is\ndesigned to overcome the \"one-tool\" bottleneck of current unified models, which\nare limited to synthetic imagery and struggle with tasks requiring factual\ngrounding or programmatic precision. Our framework empowers a central LLM or\nMLLM agent to intelligently orchestrate a diverse toolkit of specialized visual\ntools, including online image search, diffusion-based generation, code\nexecution, and image editing. The agent is trained to select and apply these\ntools proficiently via a Reinforcement Learning (RL) framework that features a\nhybrid reward system combining rule-based logic with judgments from LLM and\nMLLM evaluators. Trained on a diverse new dataset using four different model\nbackbones, LLM-I demonstrates state-of-the-art performance, outperforming\nexisting methods by a large margin across four benchmarks. We also introduce a\nnovel test-time scaling strategy that provides further performance gains.\nProject Page: https://github.com/ByteDance-BandAI/LLM-I.", "AI": {"tldr": "LLM-Interleaved\u662f\u4e00\u4e2a\u7075\u6d3b\u7684\u6846\u67b6\uff0c\u5c06\u4ea4\u9519\u56fe\u50cf-\u6587\u672c\u751f\u6210\u91cd\u65b0\u5b9a\u4e49\u4e3a\u5de5\u5177\u4f7f\u7528\u95ee\u9898\uff0c\u901a\u8fc7RL\u8bad\u7ec3LLM\u4ee3\u7406\u667a\u80fd\u534f\u8c03\u591a\u79cd\u89c6\u89c9\u5de5\u5177\uff0c\u5728\u591a\u4e2a\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u53d6\u5f97SOTA\u6027\u80fd\u3002", "motivation": "\u89e3\u51b3\u5f53\u524d\u7edf\u4e00\u6a21\u578b\u5728\u4ea4\u9519\u56fe\u50cf-\u6587\u672c\u751f\u6210\u4e2d\u7684\"\u5355\u4e00\u5de5\u5177\"\u74f6\u9888\u95ee\u9898\uff0c\u8fd9\u4e9b\u6a21\u578b\u5c40\u9650\u4e8e\u5408\u6210\u56fe\u50cf\uff0c\u96be\u4ee5\u5904\u7406\u9700\u8981\u4e8b\u5b9e\u57fa\u7840\u6216\u7a0b\u5e8f\u7cbe\u786e\u6027\u7684\u4efb\u52a1\u3002", "method": "\u8bbe\u8ba1\u4e00\u4e2a\u4e2d\u592eLLM/MLLM\u4ee3\u7406\uff0c\u901a\u8fc7\u5f3a\u5316\u5b66\u4e60\u6846\u67b6\u667a\u80fd\u534f\u8c03\u4e13\u7528\u89c6\u89c9\u5de5\u5177\uff08\u5728\u7ebf\u56fe\u50cf\u641c\u7d22\u3001\u6269\u6563\u751f\u6210\u3001\u4ee3\u7801\u6267\u884c\u3001\u56fe\u50cf\u7f16\u8f91\uff09\uff0c\u4f7f\u7528\u7ed3\u5408\u89c4\u5219\u903b\u8f91\u548cLLM/MLLM\u8bc4\u4f30\u7684\u6df7\u5408\u5956\u52b1\u7cfb\u7edf\u8fdb\u884c\u8bad\u7ec3\u3002", "result": "\u5728\u56db\u4e2a\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u5927\u5e45\u8d85\u8d8a\u73b0\u6709\u65b9\u6cd5\uff0c\u8fbe\u5230\u6700\u5148\u8fdb\u6027\u80fd\uff0c\u5e76\u5f15\u5165\u4e86\u65b0\u7684\u6d4b\u8bd5\u65f6\u6269\u5c55\u7b56\u7565\u8fdb\u4e00\u6b65\u63d0\u5347\u6027\u80fd\u3002", "conclusion": "LLM-Interleaved\u6846\u67b6\u6210\u529f\u89e3\u51b3\u4e86\u591a\u6a21\u6001\u751f\u6210\u4e2d\u7684\u5de5\u5177\u534f\u8c03\u95ee\u9898\uff0c\u4e3a\u590d\u6742\u89c6\u89c9-\u8bed\u8a00\u4efb\u52a1\u63d0\u4f9b\u4e86\u6709\u6548\u7684\u89e3\u51b3\u65b9\u6848\u3002", "relevance": 85.0}}
{"id": "2509.13379", "pdf": "https://arxiv.org/pdf/2509.13379", "abs": "https://arxiv.org/abs/2509.13379", "authors": ["Asif Azad", "Mohammad Sadat Hossain", "MD Sadik Hossain Shanto", "M Saifur Rahman", "Md Rizwan Pervez"], "title": "The Art of Saying \"Maybe\": A Conformal Lens for Uncertainty Benchmarking in VLMs", "categories": ["cs.AI", "cs.CV"], "comment": null, "summary": "Vision-Language Models (VLMs) have achieved remarkable progress in complex\nvisual understanding across scientific and reasoning tasks. While performance\nbenchmarking has advanced our understanding of these capabilities, the critical\ndimension of uncertainty quantification has received insufficient attention.\nTherefore, unlike prior conformal prediction studies that focused on limited\nsettings, we conduct a comprehensive uncertainty benchmarking study, evaluating\n16 state-of-the-art VLMs (open and closed-source) across 6 multimodal datasets\nwith 3 distinct scoring functions. Our findings demonstrate that larger models\nconsistently exhibit better uncertainty quantification; models that know more\nalso know better what they don't know. More certain models achieve higher\naccuracy, while mathematical and reasoning tasks elicit poorer uncertainty\nperformance across all models compared to other domains. This work establishes\na foundation for reliable uncertainty evaluation in multimodal systems.", "AI": {"tldr": "\u5bf916\u4e2a\u6700\u5148\u8fdb\u7684\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u57286\u4e2a\u591a\u6a21\u6001\u6570\u636e\u96c6\u4e0a\u8fdb\u884c\u5168\u9762\u7684\u4e0d\u786e\u5b9a\u6027\u57fa\u51c6\u6d4b\u8bd5\uff0c\u53d1\u73b0\u66f4\u5927\u6a21\u578b\u5177\u6709\u66f4\u597d\u7684\u4e0d\u786e\u5b9a\u6027\u91cf\u5316\u80fd\u529b\uff0c\u6570\u5b66\u548c\u63a8\u7406\u4efb\u52a1\u7684\u4e0d\u786e\u5b9a\u6027\u8868\u73b0\u8f83\u5dee\u3002", "motivation": "\u5f53\u524d\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u5728\u6027\u80fd\u57fa\u51c6\u6d4b\u8bd5\u65b9\u9762\u53d6\u5f97\u4e86\u8fdb\u5c55\uff0c\u4f46\u4e0d\u786e\u5b9a\u6027\u91cf\u5316\u8fd9\u4e00\u5173\u952e\u7ef4\u5ea6\u672a\u5f97\u5230\u8db3\u591f\u5173\u6ce8\uff0c\u9700\u8981\u7cfb\u7edf\u8bc4\u4f30VLMs\u7684\u4e0d\u786e\u5b9a\u6027\u8868\u73b0\u3002", "method": "\u8bc4\u4f3016\u4e2a\u5f00\u6e90\u548c\u95ed\u6e90\u7684\u6700\u5148\u8fdbVLMs\uff0c\u57286\u4e2a\u591a\u6a21\u6001\u6570\u636e\u96c6\u4e0a\u4f7f\u75283\u79cd\u4e0d\u540c\u7684\u8bc4\u5206\u51fd\u6570\u8fdb\u884c\u5168\u9762\u7684\u4e0d\u786e\u5b9a\u6027\u57fa\u51c6\u6d4b\u8bd5\u3002", "result": "\u53d1\u73b0\u66f4\u5927\u6a21\u578b\u59cb\u7ec8\u8868\u73b0\u51fa\u66f4\u597d\u7684\u4e0d\u786e\u5b9a\u6027\u91cf\u5316\u80fd\u529b\uff1b\u66f4\u786e\u5b9a\u7684\u6a21\u578b\u83b7\u5f97\u66f4\u9ad8\u51c6\u786e\u7387\uff1b\u6570\u5b66\u548c\u63a8\u7406\u4efb\u52a1\u5728\u6240\u6709\u6a21\u578b\u4e2d\u8868\u73b0\u51fa\u6bd4\u5176\u4ed6\u9886\u57df\u66f4\u5dee\u7684\u4e0d\u786e\u5b9a\u6027\u6027\u80fd\u3002", "conclusion": "\u8fd9\u9879\u5de5\u4f5c\u4e3a\u591a\u6a21\u6001\u7cfb\u7edf\u4e2d\u53ef\u9760\u7684\u4e0d\u786e\u5b9a\u6027\u8bc4\u4f30\u5960\u5b9a\u4e86\u57fa\u7840\uff0c\u5f3a\u8c03\u4e86\u6a21\u578b\u89c4\u6a21\u4e0e\u4e0d\u786e\u5b9a\u6027\u91cf\u5316\u80fd\u529b\u4e4b\u95f4\u7684\u6b63\u76f8\u5173\u5173\u7cfb\u3002", "relevance": 85.0}}
{"id": "2509.13790", "pdf": "https://arxiv.org/pdf/2509.13790", "abs": "https://arxiv.org/abs/2509.13790", "authors": ["Yangning Li", "Tingwei Lu", "Yinghui Li", "Yankai Chen", "Wei-Chieh Huang", "Wenhao Jiang", "Hui Wang", "Hai-Tao Zheng", "Philip S. Yu"], "title": "Teaching According to Talents! Instruction Tuning LLMs with Competence-Aware Curriculum Learning", "categories": ["cs.CL", "cs.AI"], "comment": "EMNLP 2025 Findings", "summary": "Efficient instruction tuning aims to enhance the ultimate performance of\nlarge language models (LLMs) trained on a given instruction dataset. Curriculum\nlearning as a typical data organization strategy has shown preliminary\neffectiveness in instruction tuning. However, current curriculum tuning methods\nsuffer from the curriculum rigidity, since they rely solely on static heuristic\ndifficulty metrics. These methods fail to adapt to the evolving capabilities of\nmodels during training, resulting in a fixed and potentially sub-optimal\nlearning trajectory. To address the issue, Competence-Aware Multi-Perspective\ncUrriculum inStruction tuning framework termed CAMPUS is proposed. CAMPUS\noffers several advantages: (1) Dynamic selection for sub-curriculum. (2)\nCompetency-aware adjustment to the curriculum schedule. (3) Multiple\ndifficulty-based scheduling. Extensive experiments prove the superior\nperformance of CAMPUS, compared to other state-of-the-art baselines for\nefficient instruction tuning.", "AI": {"tldr": "CAMPUS\u662f\u4e00\u4e2a\u52a8\u6001\u591a\u89c6\u89d2\u8bfe\u7a0b\u5b66\u4e60\u6846\u67b6\uff0c\u901a\u8fc7\u80fd\u529b\u611f\u77e5\u7684\u8bfe\u7a0b\u8c03\u5ea6\u548c\u52a8\u6001\u5b50\u8bfe\u7a0b\u9009\u62e9\uff0c\u89e3\u51b3\u4e86\u4f20\u7edf\u6307\u4ee4\u8c03\u4f18\u4e2d\u8bfe\u7a0b\u521a\u6027\u95ee\u9898\uff0c\u663e\u8457\u63d0\u5347\u4e86LLM\u7684\u6307\u4ee4\u8c03\u4f18\u6548\u7387\u3002", "motivation": "\u4f20\u7edf\u6307\u4ee4\u8c03\u4f18\u65b9\u6cd5\u4f9d\u8d56\u9759\u6001\u542f\u53d1\u5f0f\u96be\u5ea6\u5ea6\u91cf\uff0c\u5b58\u5728\u8bfe\u7a0b\u521a\u6027\u95ee\u9898\uff0c\u65e0\u6cd5\u9002\u5e94\u6a21\u578b\u5728\u8bad\u7ec3\u8fc7\u7a0b\u4e2d\u7684\u80fd\u529b\u6f14\u53d8\uff0c\u5bfc\u81f4\u5b66\u4e60\u8f68\u8ff9\u56fa\u5b9a\u4e14\u53ef\u80fd\u6b21\u4f18\u3002", "method": "\u63d0\u51faCAMPUS\u6846\u67b6\uff0c\u5305\u542b\uff1a1\uff09\u52a8\u6001\u5b50\u8bfe\u7a0b\u9009\u62e9\uff1b2\uff09\u80fd\u529b\u611f\u77e5\u7684\u8bfe\u7a0b\u8c03\u5ea6\u8c03\u6574\uff1b3\uff09\u591a\u96be\u5ea6\u57fa\u7840\u8c03\u5ea6\u7b56\u7565\u3002", "result": "\u5927\u91cf\u5b9e\u9a8c\u8bc1\u660eCAMPUS\u5728\u9ad8\u6548\u6307\u4ee4\u8c03\u4f18\u65b9\u9762\u4f18\u4e8e\u5176\u4ed6\u6700\u5148\u8fdb\u57fa\u7ebf\u65b9\u6cd5\u3002", "conclusion": "CAMPUS\u901a\u8fc7\u52a8\u6001\u9002\u5e94\u6a21\u578b\u80fd\u529b\u6f14\u53d8\u7684\u8bfe\u7a0b\u5b66\u4e60\u7b56\u7565\uff0c\u6709\u6548\u89e3\u51b3\u4e86\u6307\u4ee4\u8c03\u4f18\u4e2d\u7684\u8bfe\u7a0b\u521a\u6027\u95ee\u9898\uff0c\u63d0\u5347\u4e86\u8bad\u7ec3\u6548\u7387\u548c\u6700\u7ec8\u6027\u80fd\u3002", "relevance": 85.0}}
{"id": "2509.13484", "pdf": "https://arxiv.org/pdf/2509.13484", "abs": "https://arxiv.org/abs/2509.13484", "authors": ["Liu Liu", "Alexandra Kudaeva", "Marco Cipriano", "Fatimeh Al Ghannam", "Freya Tan", "Gerard de Melo", "Andres Sevtsuk"], "title": "MINGLE: VLMs for Semantically Complex Region Detection in Urban Scenes", "categories": ["cs.CV", "cs.CY"], "comment": "13 pages, 4 figures, under review at AAAI 2026", "summary": "Understanding group-level social interactions in public spaces is crucial for\nurban planning, informing the design of socially vibrant and inclusive\nenvironments. Detecting such interactions from images involves interpreting\nsubtle visual cues such as relations, proximity, and co-movement - semantically\ncomplex signals that go beyond traditional object detection. To address this\nchallenge, we introduce a social group region detection task, which requires\ninferring and spatially grounding visual regions defined by abstract\ninterpersonal relations. We propose MINGLE (Modeling INterpersonal Group-Level\nEngagement), a modular three-stage pipeline that integrates: (1) off-the-shelf\nhuman detection and depth estimation, (2) VLM-based reasoning to classify\npairwise social affiliation, and (3) a lightweight spatial aggregation\nalgorithm to localize socially connected groups. To support this task and\nencourage future research, we present a new dataset of 100K urban street-view\nimages annotated with bounding boxes and labels for both individuals and\nsocially interacting groups. The annotations combine human-created labels and\noutputs from the MINGLE pipeline, ensuring semantic richness and broad coverage\nof real-world scenarios.", "AI": {"tldr": "\u63d0\u51fa\u4e86MINGLE\u6846\u67b6\uff0c\u7528\u4e8e\u4ece\u8857\u666f\u56fe\u50cf\u4e2d\u68c0\u6d4b\u793e\u4ea4\u7fa4\u4f53\u533a\u57df\uff0c\u901a\u8fc7\u4e09\u9636\u6bb5\u6d41\u7a0b\u6574\u5408\u4eba\u4f53\u68c0\u6d4b\u3001\u6df1\u5ea6\u4f30\u8ba1\u3001VLM\u63a8\u7406\u548c\u7a7a\u95f4\u805a\u5408\u7b97\u6cd5\u3002", "motivation": "\u7406\u89e3\u516c\u5171\u573a\u6240\u7684\u7fa4\u4f53\u793e\u4ea4\u4e92\u52a8\u5bf9\u57ce\u5e02\u89c4\u5212\u81f3\u5173\u91cd\u8981\uff0c\u9700\u8981\u8d85\u8d8a\u4f20\u7edf\u7269\u4f53\u68c0\u6d4b\u7684\u590d\u6742\u8bed\u4e49\u4fe1\u53f7\u5206\u6790\u3002", "method": "\u4e09\u9636\u6bb5\u6a21\u5757\u5316\u6d41\u7a0b\uff1a1)\u73b0\u6210\u7684\u4eba\u4f53\u68c0\u6d4b\u548c\u6df1\u5ea6\u4f30\u8ba1 2)VLM\u63a8\u7406\u5206\u7c7b\u6210\u5bf9\u793e\u4ea4\u5173\u7cfb 3)\u8f7b\u91cf\u7ea7\u7a7a\u95f4\u805a\u5408\u7b97\u6cd5\u5b9a\u4f4d\u793e\u4ea4\u7fa4\u4f53", "result": "\u6784\u5efa\u4e86\u5305\u542b10\u4e07\u5f20\u8857\u666f\u56fe\u50cf\u7684\u65b0\u6570\u636e\u96c6\uff0c\u6807\u6ce8\u4e86\u4e2a\u4eba\u548c\u793e\u4ea4\u7fa4\u4f53\u7684\u8fb9\u754c\u6846\u548c\u6807\u7b7e\uff0c\u7ed3\u5408\u4eba\u5de5\u6807\u6ce8\u548cMINGLE\u8f93\u51fa\u3002", "conclusion": "\u63d0\u51fa\u4e86\u793e\u4ea4\u7fa4\u4f53\u533a\u57df\u68c0\u6d4b\u65b0\u4efb\u52a1\u548cMINGLE\u89e3\u51b3\u65b9\u6848\uff0c\u4e3a\u672a\u6765\u7814\u7a76\u63d0\u4f9b\u4e86\u4e30\u5bcc\u7684\u6570\u636e\u96c6\u548c\u57fa\u51c6\u3002", "relevance": 25.0}}
{"id": "2509.13648", "pdf": "https://arxiv.org/pdf/2509.13648", "abs": "https://arxiv.org/abs/2509.13648", "authors": ["Geon Lee", "Bhuvesh Kumar", "Clark Mingxuan Ju", "Tong Zhao", "Kijung Shin", "Neil Shah", "Liam Collins"], "title": "Sequential Data Augmentation for Generative Recommendation", "categories": ["cs.LG", "cs.IR"], "comment": null, "summary": "Generative recommendation plays a crucial role in personalized systems,\npredicting users' future interactions from their historical behavior sequences.\nA critical yet underexplored factor in training these models is data\naugmentation, the process of constructing training data from user interaction\nhistories. By shaping the training distribution, data augmentation directly and\noften substantially affects model generalization and performance. Nevertheless,\nin much of the existing work, this process is simplified, applied\ninconsistently, or treated as a minor design choice, without a systematic and\nprincipled understanding of its effects.\n  Motivated by our empirical finding that different augmentation strategies can\nyield large performance disparities, we conduct an in-depth analysis of how\nthey reshape training distributions and influence alignment with future targets\nand generalization to unseen inputs. To systematize this design space, we\npropose GenPAS, a generalized and principled framework that models augmentation\nas a stochastic sampling process over input-target pairs with three\nbias-controlled steps: sequence sampling, target sampling, and input sampling.\nThis formulation unifies widely used strategies as special cases and enables\nflexible control of the resulting training distribution. Our extensive\nexperiments on benchmark and industrial datasets demonstrate that GenPAS yields\nsuperior accuracy, data efficiency, and parameter efficiency compared to\nexisting strategies, providing practical guidance for principled training data\nconstruction in generative recommendation.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86GenPAS\u6846\u67b6\uff0c\u7cfb\u7edf\u5316\u5730\u7814\u7a76\u4e86\u751f\u6210\u5f0f\u63a8\u8350\u7cfb\u7edf\u4e2d\u6570\u636e\u589e\u5f3a\u7b56\u7565\u5bf9\u6a21\u578b\u6027\u80fd\u7684\u5f71\u54cd\uff0c\u901a\u8fc7\u4e09\u4e2a\u504f\u5dee\u63a7\u5236\u7684\u91c7\u6837\u6b65\u9aa4\u7edf\u4e00\u4e86\u73b0\u6709\u65b9\u6cd5\uff0c\u5e76\u5728\u591a\u4e2a\u6570\u636e\u96c6\u4e0a\u9a8c\u8bc1\u4e86\u5176\u4f18\u8d8a\u6027\u3002", "motivation": "\u73b0\u6709\u751f\u6210\u5f0f\u63a8\u8350\u7cfb\u7edf\u7684\u6570\u636e\u589e\u5f3a\u7b56\u7565\u7f3a\u4e4f\u7cfb\u7edf\u6027\u548c\u539f\u5219\u6027\u7406\u89e3\uff0c\u4e0d\u540c\u7b56\u7565\u4f1a\u5bfc\u81f4\u663e\u8457\u7684\u6027\u80fd\u5dee\u5f02\uff0c\u9700\u8981\u5efa\u7acb\u7edf\u4e00\u6846\u67b6\u6765\u6307\u5bfc\u8bad\u7ec3\u6570\u636e\u6784\u5efa\u3002", "method": "\u63d0\u51faGenPAS\u6846\u67b6\uff0c\u5c06\u6570\u636e\u589e\u5f3a\u5efa\u6a21\u4e3a\u5305\u542b\u4e09\u4e2a\u504f\u5dee\u63a7\u5236\u6b65\u9aa4\u7684\u968f\u673a\u91c7\u6837\u8fc7\u7a0b\uff1a\u5e8f\u5217\u91c7\u6837\u3001\u76ee\u6807\u91c7\u6837\u548c\u8f93\u5165\u91c7\u6837\uff0c\u7edf\u4e00\u73b0\u6709\u7b56\u7565\u5e76\u7075\u6d3b\u63a7\u5236\u8bad\u7ec3\u5206\u5e03\u3002", "result": "\u5728\u57fa\u51c6\u548c\u5de5\u4e1a\u6570\u636e\u96c6\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0cGenPAS\u5728\u51c6\u786e\u6027\u3001\u6570\u636e\u6548\u7387\u548c\u53c2\u6570\u6548\u7387\u65b9\u9762\u5747\u4f18\u4e8e\u73b0\u6709\u7b56\u7565\u3002", "conclusion": "GenPAS\u4e3a\u751f\u6210\u5f0f\u63a8\u8350\u7cfb\u7edf\u63d0\u4f9b\u4e86\u539f\u5219\u6027\u7684\u8bad\u7ec3\u6570\u636e\u6784\u5efa\u6307\u5bfc\uff0c\u7cfb\u7edf\u5316\u7684\u6570\u636e\u589e\u5f3a\u7b56\u7565\u80fd\u663e\u8457\u63d0\u5347\u6a21\u578b\u6027\u80fd\u3002", "relevance": 45.0}}
{"id": "2509.13389", "pdf": "https://arxiv.org/pdf/2509.13389", "abs": "https://arxiv.org/abs/2509.13389", "authors": ["Carlos N\u00fa\u00f1ez-Molina", "Vicen\u00e7 G\u00f3mez", "Hector Geffner"], "title": "From Next Token Prediction to (STRIPS) World Models -- Preliminary Results", "categories": ["cs.AI", "I.2.4; I.2.6; I.2.8"], "comment": "10 pages, 3 figures", "summary": "We consider the problem of learning propositional STRIPS world models from\naction traces alone, using a deep learning architecture (transformers) and\ngradient descent. The task is cast as a supervised next token prediction\nproblem where the tokens are the actions, and an action $a$ may follow an\naction sequence if the hidden effects of the previous actions do not make an\naction precondition of $a$ false. We show that a suitable transformer\narchitecture can faithfully represent propositional STRIPS world models, and\nthat the models can be learned from sets of random valid (positive) and invalid\n(negative) action sequences alone. A number of experiments are reported.", "AI": {"tldr": "\u4f7f\u7528Transformer\u67b6\u6784\u4ece\u52a8\u4f5c\u8f68\u8ff9\u4e2d\u5b66\u4e60\u547d\u9898STRIPS\u4e16\u754c\u6a21\u578b\uff0c\u901a\u8fc7\u76d1\u7763\u5f0f\u4e0b\u4e00\u4e2a\u52a8\u4f5c\u9884\u6d4b\u4efb\u52a1\uff0c\u80fd\u591f\u51c6\u786e\u8868\u793a\u548c\u5b66\u4e60\u4e16\u754c\u6a21\u578b\u3002", "motivation": "\u7814\u7a76\u5982\u4f55\u4ec5\u4ece\u52a8\u4f5c\u8f68\u8ff9\u4e2d\u5b66\u4e60\u4e16\u754c\u6a21\u578b\uff0c\u800c\u4e0d\u9700\u8981\u9884\u5148\u77e5\u9053\u72b6\u6001\u4fe1\u606f\uff0c\u8fd9\u5bf9\u4e8eAI\u7cfb\u7edf\u7406\u89e3\u73af\u5883\u548c\u89c4\u5212\u884c\u52a8\u5177\u6709\u91cd\u8981\u610f\u4e49\u3002", "method": "\u5c06\u4efb\u52a1\u5efa\u6a21\u4e3a\u76d1\u7763\u5f0f\u4e0b\u4e00\u4e2a\u52a8\u4f5c\u9884\u6d4b\u95ee\u9898\uff0c\u4f7f\u7528Transformer\u67b6\u6784\u5904\u7406\u52a8\u4f5c\u5e8f\u5217\uff0c\u901a\u8fc7\u6b63\u8d1f\u6837\u672c\uff08\u6709\u6548\u548c\u65e0\u6548\u52a8\u4f5c\u5e8f\u5217\uff09\u8fdb\u884c\u8bad\u7ec3\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\u5408\u9002\u7684Transformer\u67b6\u6784\u80fd\u591f\u5fe0\u5b9e\u8868\u793a\u547d\u9898STRIPS\u4e16\u754c\u6a21\u578b\uff0c\u5e76\u4e14\u4ec5\u4ece\u968f\u673a\u6709\u6548\u548c\u65e0\u6548\u52a8\u4f5c\u5e8f\u5217\u4e2d\u5c31\u80fd\u6210\u529f\u5b66\u4e60\u3002", "conclusion": "\u6df1\u5ea6\u5b66\u4e60\u65b9\u6cd5\u7279\u522b\u662fTransformer\u67b6\u6784\u80fd\u591f\u6709\u6548\u5b66\u4e60\u4e16\u754c\u6a21\u578b\uff0c\u4e3a\u4ece\u89c2\u5bdf\u4e2d\u5b66\u4e60\u73af\u5883\u52a8\u6001\u63d0\u4f9b\u4e86\u53ef\u884c\u9014\u5f84\u3002", "relevance": 65.0}}
{"id": "2509.13803", "pdf": "https://arxiv.org/pdf/2509.13803", "abs": "https://arxiv.org/abs/2509.13803", "authors": ["Laura Garc\u00eda-Sardi\u00f1a", "Hermenegildo Fabregat", "Daniel Deniz", "Rabih Zbib"], "title": "Measuring Gender Bias in Job Title Matching for Grammatical Gender Languages", "categories": ["cs.CL"], "comment": null, "summary": "This work sets the ground for studying how explicit grammatical gender\nassignment in job titles can affect the results of automatic job ranking\nsystems. We propose the usage of metrics for ranking comparison controlling for\ngender to evaluate gender bias in job title ranking systems, in particular RBO\n(Rank-Biased Overlap). We generate and share test sets for a job title matching\ntask in four grammatical gender languages, including occupations in masculine\nand feminine form and annotated by gender and matching relevance. We use the\nnew test sets and the proposed methodology to evaluate the gender bias of\nseveral out-of-the-box multilingual models to set as baselines, showing that\nall of them exhibit varying degrees of gender bias.", "AI": {"tldr": "\u672c\u6587\u7814\u7a76\u4e86\u8bed\u6cd5\u6027\u522b\u5bf9\u81ea\u52a8\u804c\u4f4d\u6392\u540d\u7cfb\u7edf\u7684\u5f71\u54cd\uff0c\u63d0\u51fa\u4e86\u57fa\u4e8eRBO\u7684\u6027\u522b\u504f\u89c1\u8bc4\u4f30\u65b9\u6cd5\uff0c\u5e76\u5728\u56db\u79cd\u8bed\u6cd5\u6027\u522b\u8bed\u8a00\u4e2d\u521b\u5efa\u4e86\u6d4b\u8bd5\u96c6\u6765\u8bc4\u4f30\u591a\u8bed\u8a00\u6a21\u578b\u7684\u6027\u522b\u504f\u89c1\u3002", "motivation": "\u7814\u7a76\u8bed\u6cd5\u6027\u522b\u5728\u804c\u4f4d\u540d\u79f0\u4e2d\u7684\u663e\u5f0f\u5206\u914d\u5982\u4f55\u5f71\u54cd\u81ea\u52a8\u804c\u4f4d\u6392\u540d\u7cfb\u7edf\u7684\u7ed3\u679c\uff0c\u65e8\u5728\u8bc4\u4f30\u548c\u91cf\u5316\u591a\u8bed\u8a00\u6a21\u578b\u4e2d\u7684\u6027\u522b\u504f\u89c1\u95ee\u9898\u3002", "method": "\u63d0\u51fa\u4f7f\u7528RBO\uff08Rank-Biased Overlap\uff09\u6307\u6807\u6765\u6bd4\u8f83\u63a7\u5236\u6027\u522b\u56e0\u7d20\u7684\u6392\u540d\u7ed3\u679c\uff0c\u521b\u5efa\u4e86\u5305\u542b\u9633\u6027\u548c\u9634\u6027\u5f62\u5f0f\u7684\u804c\u4f4d\u540d\u79f0\u6d4b\u8bd5\u96c6\uff0c\u6db5\u76d6\u56db\u79cd\u8bed\u6cd5\u6027\u522b\u8bed\u8a00\uff0c\u5e76\u8bc4\u4f30\u4e86\u591a\u4e2a\u73b0\u6210\u7684\u591a\u8bed\u8a00\u6a21\u578b\u3002", "result": "\u6240\u6709\u6d4b\u8bd5\u7684\u591a\u8bed\u8a00\u6a21\u578b\u90fd\u8868\u73b0\u51fa\u4e0d\u540c\u7a0b\u5ea6\u7684\u6027\u522b\u504f\u89c1\uff0c\u8bc1\u660e\u4e86\u73b0\u6709\u7cfb\u7edf\u5728\u8bed\u6cd5\u6027\u522b\u5904\u7406\u65b9\u9762\u5b58\u5728\u7cfb\u7edf\u6027\u504f\u5dee\u3002", "conclusion": "\u9700\u8981\u5f00\u53d1\u80fd\u591f\u66f4\u597d\u5730\u5904\u7406\u8bed\u6cd5\u6027\u522b\u5e76\u51cf\u5c11\u6027\u522b\u504f\u89c1\u7684\u804c\u4f4d\u6392\u540d\u7cfb\u7edf\uff0c\u672c\u6587\u63d0\u51fa\u7684\u65b9\u6cd5\u548c\u6d4b\u8bd5\u96c6\u4e3a\u6b64\u7c7b\u7814\u7a76\u63d0\u4f9b\u4e86\u57fa\u7840\u3002", "relevance": 45.0}}
{"id": "2509.13496", "pdf": "https://arxiv.org/pdf/2509.13496", "abs": "https://arxiv.org/abs/2509.13496", "authors": ["Rajatsubhra Chakraborty", "Xujun Che", "Depeng Xu", "Cori Faklaris", "Xi Niu", "Shuhan Yuan"], "title": "BiasMap: Leveraging Cross-Attentions to Discover and Mitigate Hidden Social Biases in Text-to-Image Generation", "categories": ["cs.CV", "cs.LG"], "comment": null, "summary": "Bias discovery is critical for black-box generative models, especiall\ntext-to-image (TTI) models. Existing works predominantly focus on output-level\ndemographic distributions, which do not necessarily guarantee concept\nrepresentations to be disentangled post-mitigation. We propose BiasMap, a\nmodel-agnostic framework for uncovering latent concept-level representational\nbiases in stable diffusion models. BiasMap leverages cross-attention\nattribution maps to reveal structural entanglements between demographics (e.g.,\ngender, race) and semantics (e.g., professions), going deeper into\nrepresentational bias during the image generation. Using attribution maps of\nthese concepts, we quantify the spatial demographics-semantics concept\nentanglement via Intersection over Union (IoU), offering a lens into bias that\nremains hidden in existing fairness discovery approaches. In addition, we\nfurther utilize BiasMap for bias mitigation through energy-guided diffusion\nsampling that directly modifies latent noise space and minimizes the expected\nSoftIoU during the denoising process. Our findings show that existing fairness\ninterventions may reduce the output distributional gap but often fail to\ndisentangle concept-level coupling, whereas our mitigation method can mitigate\nconcept entanglement in image generation while complementing distributional\nbias mitigation.", "AI": {"tldr": "BiasMap\u662f\u4e00\u4e2a\u6a21\u578b\u65e0\u5173\u7684\u6846\u67b6\uff0c\u7528\u4e8e\u53d1\u73b0\u7a33\u5b9a\u6269\u6563\u6a21\u578b\u4e2d\u7684\u6f5c\u5728\u6982\u5ff5\u7ea7\u8868\u5f81\u504f\u89c1\uff0c\u901a\u8fc7\u4ea4\u53c9\u6ce8\u610f\u529b\u5f52\u56e0\u56fe\u63ed\u793a\u4eba\u53e3\u7edf\u8ba1\u7279\u5f81\u4e0e\u8bed\u4e49\u6982\u5ff5\u7684\u7ed3\u6784\u6027\u7ea0\u7f20\uff0c\u5e76\u63d0\u51fa\u57fa\u4e8e\u80fd\u91cf\u5f15\u5bfc\u6269\u6563\u91c7\u6837\u7684\u504f\u89c1\u7f13\u89e3\u65b9\u6cd5\u3002", "motivation": "\u73b0\u6709\u7684\u504f\u89c1\u53d1\u73b0\u65b9\u6cd5\u4e3b\u8981\u5173\u6ce8\u8f93\u51fa\u5c42\u9762\u7684\u4eba\u53e3\u7edf\u8ba1\u5206\u5e03\uff0c\u65e0\u6cd5\u4fdd\u8bc1\u504f\u89c1\u7f13\u89e3\u540e\u6982\u5ff5\u8868\u5f81\u7684\u89e3\u8026\u3002\u9700\u8981\u66f4\u6df1\u5165\u5730\u63a2\u7d22\u751f\u6210\u6a21\u578b\u4e2d\u7684\u8868\u5f81\u5c42\u9762\u504f\u89c1\u3002", "method": "\u5229\u7528\u4ea4\u53c9\u6ce8\u610f\u529b\u5f52\u56e0\u56fe\u91cf\u5316\u4eba\u53e3\u7edf\u8ba1\u7279\u5f81\u4e0e\u8bed\u4e49\u6982\u5ff5\u7684\u7a7a\u95f4\u7ea0\u7f20\uff08\u901a\u8fc7IoU\u6307\u6807\uff09\uff0c\u5e76\u91c7\u7528\u80fd\u91cf\u5f15\u5bfc\u6269\u6563\u91c7\u6837\u5728\u53bb\u566a\u8fc7\u7a0b\u4e2d\u6700\u5c0f\u5316SoftIoU\u671f\u671b\u503c\u6765\u7f13\u89e3\u504f\u89c1\u3002", "result": "\u7814\u7a76\u53d1\u73b0\u73b0\u6709\u516c\u5e73\u6027\u5e72\u9884\u63aa\u65bd\u53ef\u80fd\u51cf\u5c11\u8f93\u51fa\u5206\u5e03\u5dee\u8ddd\uff0c\u4f46\u5f80\u5f80\u65e0\u6cd5\u89e3\u8026\u6982\u5ff5\u7ea7\u8026\u5408\uff0c\u800cBiasMap\u65b9\u6cd5\u80fd\u591f\u5728\u56fe\u50cf\u751f\u6210\u4e2d\u7f13\u89e3\u6982\u5ff5\u7ea0\u7f20\uff0c\u540c\u65f6\u8865\u5145\u5206\u5e03\u504f\u89c1\u7f13\u89e3\u3002", "conclusion": "BiasMap\u63d0\u4f9b\u4e86\u4e00\u4e2a\u6df1\u5165\u63a2\u7d22\u751f\u6210\u6a21\u578b\u4e2d\u9690\u85cf\u8868\u5f81\u504f\u89c1\u7684\u6846\u67b6\uff0c\u5176\u7f13\u89e3\u65b9\u6cd5\u80fd\u6709\u6548\u5904\u7406\u6982\u5ff5\u7ea7\u504f\u89c1\uff0c\u4e3a\u66f4\u5168\u9762\u7684\u504f\u89c1\u53d1\u73b0\u548c\u7f13\u89e3\u63d0\u4f9b\u4e86\u65b0\u89c6\u89d2\u3002", "relevance": 65.0}}
{"id": "2509.13651", "pdf": "https://arxiv.org/pdf/2509.13651", "abs": "https://arxiv.org/abs/2509.13651", "authors": ["Yongkang Du", "Jieyu Zhao", "Yijun Yang", "Tianyi Zhou"], "title": "Controllable Pareto Trade-off between Fairness and Accuracy", "categories": ["cs.LG"], "comment": null, "summary": "The fairness-accuracy trade-off is a key challenge in NLP tasks. Current work\nfocuses on finding a single \"optimal\" solution to balance the two objectives,\nwhich is limited considering the diverse solutions on the Pareto front. This\nwork intends to provide controllable trade-offs according to the user's\npreference of the two objectives, which is defined as a reference vector. To\nachieve this goal, we apply multi-objective optimization (MOO), which can find\nsolutions from various regions of the Pareto front. However, it is challenging\nto precisely control the trade-off due to the stochasticity of the training\nprocess and the high dimentional gradient vectors. Thus, we propose\nControllable Pareto Trade-off (CPT) that can effectively train models to\nperform different trade-offs according to users' preferences. CPT 1) stabilizes\nthe fairness update with a moving average of stochastic gradients to determine\nthe update direction, and 2) prunes the gradients by only keeping the gradients\nof the critical parameters. We evaluate CPT on hate speech detection and\noccupation classification tasks. Experiments show that CPT can achieve a\nhigher-quality set of solutions on the Pareto front than the baseline methods.\nIt also exhibits better controllability and can precisely follow the\nhuman-defined reference vectors.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u53ef\u63a7\u5e15\u7d2f\u6258\u6743\u8861(CPT)\u65b9\u6cd5\uff0c\u901a\u8fc7\u591a\u76ee\u6807\u4f18\u5316\u5b9e\u73b0NLP\u4efb\u52a1\u4e2d\u516c\u5e73\u6027\u4e0e\u51c6\u786e\u6027\u7684\u53ef\u63a7\u6743\u8861\uff0c\u4f7f\u7528\u79fb\u52a8\u5e73\u5747\u68af\u5ea6\u7a33\u5b9a\u6027\u548c\u5173\u952e\u53c2\u6570\u68af\u5ea6\u526a\u679d\u6765\u7cbe\u786e\u63a7\u5236\u7528\u6237\u504f\u597d\u7684\u6743\u8861\u65b9\u5411\u3002", "motivation": "\u5f53\u524dNLP\u4efb\u52a1\u4e2d\u7684\u516c\u5e73\u6027-\u51c6\u786e\u6027\u6743\u8861\u7814\u7a76\u4e3b\u8981\u5bfb\u627e\u5355\u4e00\"\u6700\u4f18\"\u89e3\u51b3\u65b9\u6848\uff0c\u4f46\u5ffd\u7565\u4e86\u5e15\u7d2f\u6258\u524d\u6cbf\u4e0a\u7684\u591a\u6837\u6027\u3002\u672c\u7814\u7a76\u65e8\u5728\u6839\u636e\u7528\u6237\u504f\u597d\u63d0\u4f9b\u53ef\u63a7\u7684\u6743\u8861\u89e3\u51b3\u65b9\u6848\u3002", "method": "\u91c7\u7528\u591a\u76ee\u6807\u4f18\u5316(MOO)\u65b9\u6cd5\uff0c\u63d0\u51faCPT\u6846\u67b6\uff1a1)\u4f7f\u7528\u968f\u673a\u68af\u5ea6\u7684\u79fb\u52a8\u5e73\u5747\u6765\u7a33\u5b9a\u516c\u5e73\u6027\u66f4\u65b0\u65b9\u5411\uff1b2)\u901a\u8fc7\u4ec5\u4fdd\u7559\u5173\u952e\u53c2\u6570\u7684\u68af\u5ea6\u6765\u8fdb\u884c\u68af\u5ea6\u526a\u679d\u3002\u5728\u4ec7\u6068\u8a00\u8bba\u68c0\u6d4b\u548c\u804c\u4e1a\u5206\u7c7b\u4efb\u52a1\u4e0a\u8fdb\u884c\u8bc4\u4f30\u3002", "result": "\u5b9e\u9a8c\u8868\u660eCPT\u80fd\u591f\u5728\u5e15\u7d2f\u6258\u524d\u6cbf\u4e0a\u83b7\u5f97\u6bd4\u57fa\u7ebf\u65b9\u6cd5\u66f4\u9ad8\u8d28\u91cf\u7684\u89e3\u51b3\u65b9\u6848\u96c6\u5408\uff0c\u5c55\u73b0\u51fa\u66f4\u597d\u7684\u53ef\u63a7\u6027\uff0c\u80fd\u591f\u7cbe\u786e\u9075\u5faa\u4eba\u5de5\u5b9a\u4e49\u7684\u53c2\u8003\u5411\u91cf\u3002", "conclusion": "CPT\u65b9\u6cd5\u6709\u6548\u5730\u89e3\u51b3\u4e86\u516c\u5e73\u6027-\u51c6\u786e\u6027\u6743\u8861\u7684\u53ef\u63a7\u6027\u95ee\u9898\uff0c\u4e3aNLP\u4efb\u52a1\u63d0\u4f9b\u4e86\u6839\u636e\u7528\u6237\u504f\u597d\u5b9a\u5236\u5316\u89e3\u51b3\u65b9\u6848\u7684\u80fd\u529b\u3002", "relevance": 75.0}}
{"id": "2509.13450", "pdf": "https://arxiv.org/pdf/2509.13450", "abs": "https://arxiv.org/abs/2509.13450", "authors": ["Vincent Siu", "Nicholas Crispino", "David Park", "Nathan W. Henry", "Zhun Wang", "Yang Liu", "Dawn Song", "Chenguang Wang"], "title": "SteeringControl: Holistic Evaluation of Alignment Steering in LLMs", "categories": ["cs.AI", "cs.CL", "cs.LG"], "comment": null, "summary": "We introduce SteeringControl, a benchmark for evaluating representation\nsteering methods across core alignment objectives--bias, harmful generation,\nand hallucination--and their effects on secondary behaviors such as sycophancy\nand commonsense morality. While prior alignment work often highlights\ntruthfulness or reasoning ability to demonstrate the side effects of\nrepresentation steering, we find there are many unexplored tradeoffs not yet\nunderstood in a systematic way. We collect a dataset of safety-relevant primary\nand secondary behaviors to evaluate steering effectiveness and behavioral\nentanglement centered around five popular steering methods. To enable this, we\ncraft a modular steering framework based on unique components that serve as the\nbuilding blocks of many existing methods. Our results on Qwen-2.5-7B and\nLlama-3.1-8B find that strong steering performance is dependent on the specific\ncombination of steering method, model, and targeted behavior, and that severe\nconcept entanglement can result from poor combinations of these three as well.\nWe release our code here:\nhttps://github.com/wang-research-lab/SteeringControl.git.", "AI": {"tldr": "SteeringControl\u662f\u4e00\u4e2a\u8bc4\u4f30\u8868\u793a\u5f15\u5bfc\u65b9\u6cd5\u7684\u57fa\u51c6\uff0c\u4e13\u6ce8\u4e8e\u504f\u89c1\u3001\u6709\u5bb3\u751f\u6210\u548c\u5e7b\u89c9\u7b49\u6838\u5fc3\u5bf9\u9f50\u76ee\u6807\uff0c\u4ee5\u53ca\u8fd9\u4e9b\u65b9\u6cd5\u5bf9\u6b21\u8981\u884c\u4e3a\uff08\u5982\u5949\u627f\u548c\u5e38\u8bc6\u9053\u5fb7\uff09\u7684\u5f71\u54cd\u3002", "motivation": "\u73b0\u6709\u7684\u5bf9\u9f50\u5de5\u4f5c\u901a\u5e38\u53ea\u5173\u6ce8\u771f\u5b9e\u6027\u6216\u63a8\u7406\u80fd\u529b\u6765\u5c55\u793a\u8868\u793a\u5f15\u5bfc\u7684\u526f\u4f5c\u7528\uff0c\u4f46\u8bb8\u591a\u6743\u8861\u5173\u7cfb\u5c1a\u672a\u88ab\u7cfb\u7edf\u6027\u5730\u7406\u89e3\u3002\u9700\u8981\u7cfb\u7edf\u8bc4\u4f30\u5f15\u5bfc\u65b9\u6cd5\u7684\u6709\u6548\u6027\u548c\u884c\u4e3a\u7ea0\u7f20\u95ee\u9898\u3002", "method": "\u6784\u5efa\u4e86\u4e00\u4e2a\u5305\u542b\u5b89\u5168\u76f8\u5173\u4e3b\u8981\u548c\u6b21\u8981\u884c\u4e3a\u7684\u6570\u636e\u96c6\uff0c\u57fa\u4e8e\u4e94\u4e2a\u6d41\u884c\u5f15\u5bfc\u65b9\u6cd5\u521b\u5efa\u6a21\u5757\u5316\u5f15\u5bfc\u6846\u67b6\uff0c\u5728Qwen-2.5-7B\u548cLlama-3.1-8B\u6a21\u578b\u4e0a\u8fdb\u884c\u8bc4\u4f30\u3002", "result": "\u53d1\u73b0\u5f3a\u5f15\u5bfc\u6027\u80fd\u53d6\u51b3\u4e8e\u5f15\u5bfc\u65b9\u6cd5\u3001\u6a21\u578b\u548c\u76ee\u6807\u884c\u4e3a\u7684\u7279\u5b9a\u7ec4\u5408\uff0c\u4e0d\u826f\u7ec4\u5408\u4f1a\u5bfc\u81f4\u4e25\u91cd\u7684\u6982\u5ff5\u7ea0\u7f20\u95ee\u9898\u3002", "conclusion": "\u8868\u793a\u5f15\u5bfc\u65b9\u6cd5\u7684\u6548\u679c\u5177\u6709\u9ad8\u5ea6\u60c5\u5883\u4f9d\u8d56\u6027\uff0c\u9700\u8981\u4ed4\u7ec6\u9009\u62e9\u65b9\u6cd5-\u6a21\u578b-\u76ee\u6807\u7684\u7ec4\u5408\u4ee5\u907f\u514d\u8d1f\u9762\u526f\u4f5c\u7528\u3002", "relevance": 85.0}}
{"id": "2509.13813", "pdf": "https://arxiv.org/pdf/2509.13813", "abs": "https://arxiv.org/abs/2509.13813", "authors": ["Edward Phillips", "Sean Wu", "Soheila Molaei", "Danielle Belgrave", "Anshul Thakur", "David Clifton"], "title": "Geometric Uncertainty for Detecting and Correcting Hallucinations in LLMs", "categories": ["cs.CL"], "comment": null, "summary": "Large language models demonstrate impressive results across diverse tasks but\nare still known to hallucinate, generating linguistically plausible but\nincorrect answers to questions. Uncertainty quantification has been proposed as\na strategy for hallucination detection, but no existing black-box approach\nprovides estimates for both global and local uncertainty. The former attributes\nuncertainty to a batch of responses, while the latter attributes uncertainty to\nindividual responses. Current local methods typically rely on white-box access\nto internal model states, whilst black-box methods only provide global\nuncertainty estimates. We introduce a geometric framework to address this,\nbased on archetypal analysis of batches of responses sampled with only\nblack-box model access. At the global level, we propose Geometric Volume, which\nmeasures the convex hull volume of archetypes derived from response embeddings.\nAt the local level, we propose Geometric Suspicion, which ranks responses by\nreliability and enables hallucination reduction through preferential response\nselection. Unlike prior dispersion methods which yield only a single global\nscore, our approach provides semantic boundary points which have utility for\nattributing reliability to individual responses. Experiments show that our\nframework performs comparably to or better than prior methods on short form\nquestion-answering datasets, and achieves superior results on medical datasets\nwhere hallucinations carry particularly critical risks. We also provide\ntheoretical justification by proving a link between convex hull volume and\nentropy.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u4e2a\u57fa\u4e8e\u51e0\u4f55\u6846\u67b6\u7684\u9ed1\u76d2\u4e0d\u786e\u5b9a\u6027\u91cf\u5316\u65b9\u6cd5\uff0c\u901a\u8fc7\u539f\u578b\u5206\u6790\u6765\u540c\u65f6\u4f30\u8ba1\u5168\u5c40\u548c\u5c40\u90e8\u4e0d\u786e\u5b9a\u6027\uff0c\u7528\u4e8e\u68c0\u6d4b\u5927\u8bed\u8a00\u6a21\u578b\u7684\u5e7b\u89c9\u95ee\u9898\u3002", "motivation": "\u5927\u8bed\u8a00\u6a21\u578b\u5b58\u5728\u5e7b\u89c9\u95ee\u9898\uff0c\u751f\u6210\u770b\u4f3c\u5408\u7406\u4f46\u9519\u8bef\u7684\u7b54\u6848\u3002\u73b0\u6709\u9ed1\u76d2\u65b9\u6cd5\u53ea\u80fd\u63d0\u4f9b\u5168\u5c40\u4e0d\u786e\u5b9a\u6027\u4f30\u8ba1\uff0c\u800c\u5c40\u90e8\u65b9\u6cd5\u9700\u8981\u767d\u76d2\u8bbf\u95ee\u6743\u9650\u3002\u9700\u8981\u4e00\u79cd\u9ed1\u76d2\u65b9\u6cd5\u540c\u65f6\u652f\u6301\u5168\u5c40\u548c\u5c40\u90e8\u4e0d\u786e\u5b9a\u6027\u91cf\u5316\u3002", "method": "\u57fa\u4e8e\u539f\u578b\u5206\u6790\u7684\u51e0\u4f55\u6846\u67b6\uff1a1) \u5168\u5c40\u5c42\u9762\u4f7f\u7528\u51e0\u4f55\u4f53\u79ef(Geometric Volume)\u6d4b\u91cf\u54cd\u5e94\u5d4c\u5165\u7684\u539f\u578b\u51f8\u5305\u4f53\u79ef\uff1b2) \u5c40\u90e8\u5c42\u9762\u4f7f\u7528\u51e0\u4f55\u6000\u7591\u5ea6(Geometric Suspicion)\u5bf9\u54cd\u5e94\u53ef\u9760\u6027\u8fdb\u884c\u6392\u540d\uff0c\u901a\u8fc7\u4f18\u5148\u9009\u62e9\u5b9e\u73b0\u5e7b\u89c9\u51cf\u5c11\u3002", "result": "\u5728\u77ed\u5f62\u5f0f\u95ee\u7b54\u6570\u636e\u96c6\u4e0a\u8868\u73b0\u4f18\u4e8e\u6216\u4e0e\u73b0\u6709\u65b9\u6cd5\u76f8\u5f53\uff0c\u5728\u533b\u7597\u6570\u636e\u96c6\u4e0a\u8868\u73b0\u66f4\u4f18\uff08\u5e7b\u89c9\u98ce\u9669\u66f4\u9ad8\uff09\u3002\u7406\u8bba\u8bc1\u660e\u51f8\u5305\u4f53\u79ef\u4e0e\u71b5\u4e4b\u95f4\u5b58\u5728\u8054\u7cfb\u3002", "conclusion": "\u63d0\u51fa\u7684\u51e0\u4f55\u6846\u67b6\u4e3a\u9ed1\u76d2\u4e0d\u786e\u5b9a\u6027\u91cf\u5316\u63d0\u4f9b\u4e86\u6709\u6548\u89e3\u51b3\u65b9\u6848\uff0c\u80fd\u591f\u540c\u65f6\u5904\u7406\u5168\u5c40\u548c\u5c40\u90e8\u4e0d\u786e\u5b9a\u6027\uff0c\u5728\u5173\u952e\u98ce\u9669\u9886\u57df\uff08\u5982\u533b\u7597\uff09\u7279\u522b\u6709\u4ef7\u503c\u3002", "relevance": 85.0}}
{"id": "2509.13504", "pdf": "https://arxiv.org/pdf/2509.13504", "abs": "https://arxiv.org/abs/2509.13504", "authors": ["Uriel Garcilazo-Cruz", "Joseph O. Okeme", "Rodrigo A. Vargas--Hern\u00e1ndez"], "title": "LivePyxel: Accelerating image annotations with a Python-integrated webcam live streaming", "categories": ["cs.CV"], "comment": "8 pages, 10 figures, SM, 5 pages, 4 figures", "summary": "The lack of flexible annotation tools has hindered the deployment of AI\nmodels in some scientific areas. Most existing image annotation software\nrequires users to upload a precollected dataset, which limits support for\non-demand pipelines and introduces unnecessary steps to acquire images. This\nconstraint is particularly problematic in laboratory environments, where\nreal-time data acquisition from instruments such as microscopes is increasingly\ncommon. In this work, we introduce \\texttt{LivePixel}, a Python-based graphical\nuser interface that integrates with imaging systems, such as webcams,\nmicroscopes, and others, to enable real-time image annotation. LivePyxel is\ndesigned to be easy to use through a simple interface that allows users to\nprecisely delimit areas for annotation using tools commonly found in commercial\ngraphics editing software. Of particular interest is the availability of\nB\\'ezier splines and binary masks, and the software's capacity to work with\nnon-destructive layers that enable high-performance editing. LivePyxel also\nintegrates a wide compatibility across video devices, and it's optimized for\nobject detection operations via the use of OpenCV in combination with\nhigh-performance libraries designed to handle matrix and linear algebra\noperations via Numpy effectively. LivePyxel facilitates seamless data\ncollection and labeling, accelerating the development of AI models in\nexperimental workflows. LivePyxel freely available at\nhttps://github.com/UGarCil/LivePyxel", "AI": {"tldr": "LivePyxel\u662f\u4e00\u4e2a\u57fa\u4e8ePython\u7684\u5b9e\u65f6\u56fe\u50cf\u6807\u6ce8\u5de5\u5177\uff0c\u53ef\u76f4\u63a5\u4e0e\u663e\u5fae\u955c\u7b49\u6210\u50cf\u8bbe\u5907\u96c6\u6210\uff0c\u63d0\u4f9b\u8d1d\u585e\u5c14\u66f2\u7ebf\u548c\u4e8c\u8fdb\u5236\u63a9\u7801\u7b49\u4e13\u4e1a\u6807\u6ce8\u529f\u80fd\uff0c\u652f\u6301\u975e\u7834\u574f\u6027\u56fe\u5c42\u7f16\u8f91\uff0c\u65e8\u5728\u52a0\u901f\u79d1\u5b66AI\u6a21\u578b\u7684\u5f00\u53d1\u3002", "motivation": "\u73b0\u6709\u56fe\u50cf\u6807\u6ce8\u5de5\u5177\u9700\u8981\u5148\u4e0a\u4f20\u9884\u6536\u96c6\u6570\u636e\u96c6\uff0c\u65e0\u6cd5\u652f\u6301\u5b9e\u65f6\u6570\u636e\u91c7\u96c6\u548c\u6309\u9700\u6807\u6ce8\u6d41\u7a0b\uff0c\u8fd9\u5728\u5b9e\u9a8c\u5ba4\u5b9e\u65f6\u4eea\u5668\u6570\u636e\u91c7\u96c6\u573a\u666f\u4e2d\u5c24\u4e3a\u4e0d\u4fbf\u3002", "method": "\u5f00\u53d1\u57fa\u4e8ePython\u7684\u56fe\u5f62\u7528\u6237\u754c\u9762\uff0c\u96c6\u6210OpenCV\u548cNumpy\u7b49\u9ad8\u6027\u80fd\u5e93\uff0c\u652f\u6301\u591a\u79cd\u89c6\u9891\u8bbe\u5907\uff0c\u63d0\u4f9b\u7c7b\u4f3c\u5546\u4e1a\u56fe\u5f62\u8f6f\u4ef6\u7684\u6807\u6ce8\u5de5\u5177\uff08\u8d1d\u585e\u5c14\u66f2\u7ebf\u3001\u4e8c\u8fdb\u5236\u63a9\u7801\uff09\u548c\u975e\u7834\u574f\u6027\u56fe\u5c42\u7f16\u8f91\u529f\u80fd\u3002", "result": "\u5b9e\u73b0\u4e86\u5b9e\u65f6\u56fe\u50cf\u6807\u6ce8\u5de5\u5177LivePyxel\uff0c\u53ef\u4e0e\u7f51\u7edc\u6444\u50cf\u5934\u3001\u663e\u5fae\u955c\u7b49\u6210\u50cf\u7cfb\u7edf\u76f4\u63a5\u96c6\u6210\uff0c\u652f\u6301\u9ad8\u6027\u80fd\u7684\u5bf9\u8c61\u68c0\u6d4b\u64cd\u4f5c\u3002", "conclusion": "LivePyxel\u7b80\u5316\u4e86\u6570\u636e\u6536\u96c6\u548c\u6807\u6ce8\u6d41\u7a0b\uff0c\u80fd\u591f\u52a0\u901f\u5b9e\u9a8c\u5de5\u4f5c\u6d41\u4e2dAI\u6a21\u578b\u7684\u5f00\u53d1\u3002", "relevance": 15.0}}
{"id": "2509.13686", "pdf": "https://arxiv.org/pdf/2509.13686", "abs": "https://arxiv.org/abs/2509.13686", "authors": ["Bingsheng Peng", "Shutao Zhang", "Xi Zheng", "Ye Xue", "Xinyu Qin", "Tsung-Hui Chang"], "title": "RF-LSCM: Pushing Radiance Fields to Multi-Domain Localized Statistical Channel Modeling for Cellular Network Optimization", "categories": ["cs.LG"], "comment": null, "summary": "Accurate localized wireless channel modeling is a cornerstone of cellular\nnetwork optimization, enabling reliable prediction of network performance\nduring parameter tuning. Localized statistical channel modeling (LSCM) is the\nstate-of-the-art channel modeling framework tailored for cellular network\noptimization. However, traditional LSCM methods, which infer the channel's\nAngular Power Spectrum (APS) from Reference Signal Received Power (RSRP)\nmeasurements, suffer from critical limitations: they are typically confined to\nsingle-cell, single-grid and single-carrier frequency analysis and fail to\ncapture complex cross-domain interactions. To overcome these challenges, we\npropose RF-LSCM, a novel framework that models the channel APS by jointly\nrepresenting large-scale signal attenuation and multipath components within a\nradiance field. RF-LSCM introduces a multi-domain LSCM formulation with a\nphysics-informed frequency-dependent Attenuation Model (FDAM) to facilitate the\ncross frequency generalization as well as a point-cloud-aided environment\nenhanced method to enable multi-cell and multi-grid channel modeling.\nFurthermore, to address the computational inefficiency of typical neural\nradiance fields, RF-LSCM leverages a low-rank tensor representation,\ncomplemented by a novel Hierarchical Tensor Angular Modeling (HiTAM) algorithm.\nThis efficient design significantly reduces GPU memory requirements and\ntraining time while preserving fine-grained accuracy. Extensive experiments on\nreal-world multi-cell datasets demonstrate that RF-LSCM significantly\noutperforms state-of-the-art methods, achieving up to a 30% reduction in mean\nabsolute error (MAE) for coverage prediction and a 22% MAE improvement by\neffectively fusing multi-frequency data.", "AI": {"tldr": "RF-LSCM\u662f\u4e00\u4e2a\u57fa\u4e8e\u795e\u7ecf\u8f90\u5c04\u573a\u7684\u65e0\u7ebf\u4fe1\u9053\u5efa\u6a21\u6846\u67b6\uff0c\u901a\u8fc7\u591a\u57df\u5efa\u6a21\u548c\u7269\u7406\u611f\u77e5\u7684\u9891\u7387\u76f8\u5173\u8870\u51cf\u6a21\u578b\uff0c\u663e\u8457\u63d0\u5347\u4e86\u591a\u5c0f\u533a\u591a\u9891\u6bb5\u4fe1\u9053\u9884\u6d4b\u7cbe\u5ea6", "motivation": "\u4f20\u7edf\u5c40\u90e8\u7edf\u8ba1\u4fe1\u9053\u5efa\u6a21\u65b9\u6cd5\u5c40\u9650\u4e8e\u5355\u5c0f\u533a\u3001\u5355\u7f51\u683c\u548c\u5355\u9891\u6bb5\u5206\u6790\uff0c\u65e0\u6cd5\u6355\u6349\u590d\u6742\u7684\u8de8\u57df\u4ea4\u4e92\uff0c\u9700\u8981\u65b0\u7684\u6846\u67b6\u6765\u514b\u670d\u8fd9\u4e9b\u9650\u5236", "method": "\u91c7\u7528\u795e\u7ecf\u8f90\u5c04\u573a\u8054\u5408\u5efa\u6a21\u5927\u5c3a\u5ea6\u4fe1\u53f7\u8870\u51cf\u548c\u591a\u5f84\u5206\u91cf\uff0c\u5f15\u5165\u7269\u7406\u611f\u77e5\u7684\u9891\u7387\u76f8\u5173\u8870\u51cf\u6a21\u578b(FDAM)\u548c\u70b9\u4e91\u8f85\u52a9\u73af\u5883\u589e\u5f3a\u65b9\u6cd5\uff0c\u4f7f\u7528\u4f4e\u79e9\u5f20\u91cf\u8868\u793a\u548c\u5206\u5c42\u5f20\u91cf\u89d2\u5ea6\u5efa\u6a21(HiTAM)\u7b97\u6cd5\u63d0\u9ad8\u8ba1\u7b97\u6548\u7387", "result": "\u5728\u771f\u5b9e\u591a\u5c0f\u533a\u6570\u636e\u96c6\u4e0a\uff0cRF-LSCM\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\uff0c\u8986\u76d6\u9884\u6d4b\u7684MAE\u964d\u4f4e30%\uff0c\u591a\u9891\u6570\u636e\u878d\u5408\u4f7fMAE\u63d0\u534722%", "conclusion": "RF-LSCM\u901a\u8fc7\u521b\u65b0\u7684\u591a\u57df\u5efa\u6a21\u548c\u9ad8\u6548\u5f20\u91cf\u8868\u793a\uff0c\u4e3a\u8702\u7a9d\u7f51\u7edc\u4f18\u5316\u63d0\u4f9b\u4e86\u66f4\u51c6\u786e\u548c\u53ef\u6269\u5c55\u7684\u4fe1\u9053\u5efa\u6a21\u89e3\u51b3\u65b9\u6848", "relevance": 15.0}}
{"id": "2509.13547", "pdf": "https://arxiv.org/pdf/2509.13547", "abs": "https://arxiv.org/abs/2509.13547", "authors": ["Harper Reed", "Michael Sugimura", "Angelo Zangari"], "title": "AI Agents with Human-Like Collaborative Tools: Adaptive Strategies for Enhanced Problem-Solving", "categories": ["cs.AI", "cs.HC"], "comment": "16 pages, 5 tables", "summary": "We investigate whether giving LLM agents the collaborative tools and autonomy\nthat humans naturally use for problem solving can improve their performance. We\nequip Claude Code agents with MCP-based social media and journaling tools and\nallow them to use these tools as they see fit. Across 34 Aider Polyglot Python\nprogramming challenges, collaborative tools substantially improve performance\non the hardest problems, delivering 15-40% lower cost, 12-27% fewer turns, and\n12-38% faster completion than baseline agents. Effects on the full challenge\nset are mixed, suggesting these tools act as performance enhancers when\nadditional reasoning scaffolding is most needed. Surprisingly, Different models\nnaturally adopted distinct collaborative strategies without explicit\ninstruction. Sonnet 3.7 engaged broadly across tools and benefited from\narticulation-based cognitive scaffolding. Sonnet 4 showed selective adoption,\nleaning on journal-based semantic search when problems were genuinely\ndifficult. This mirrors how human developers adjust collaboration based on\nexpertise and task complexity. Behavioral analysis shows agents prefer writing\nover reading by about 2-9x, indicating that structured articulation drives much\nof the improvement rather than information access alone. Overall, AI agents can\nsystematically benefit from human-inspired collaboration tools at the edge of\ntheir capabilities, pointing to adaptive collaborative interfaces as reasoning\nenhancers rather than universal efficiency boosts.", "AI": {"tldr": "\u4e3aLLM\u4ee3\u7406\u63d0\u4f9b\u4eba\u7c7b\u534f\u4f5c\u5de5\u5177\uff08\u793e\u4ea4\u5a92\u4f53\u548c\u65e5\u5fd7\u5de5\u5177\uff09\u53ef\u4ee5\u663e\u8457\u63d0\u5347\u5728\u56f0\u96be\u7f16\u7a0b\u95ee\u9898\u4e0a\u7684\u6027\u80fd\u8868\u73b0\uff0c\u964d\u4f4e\u6210\u672c\u3001\u51cf\u5c11\u4ea4\u4e92\u8f6e\u6b21\u5e76\u52a0\u5feb\u5b8c\u6210\u901f\u5ea6", "motivation": "\u7814\u7a76\u662f\u5426\u901a\u8fc7\u8d4b\u4e88LLM\u4ee3\u7406\u4eba\u7c7b\u81ea\u7136\u4f7f\u7528\u7684\u534f\u4f5c\u5de5\u5177\u548c\u81ea\u4e3b\u6743\uff0c\u80fd\u591f\u6539\u5584\u5b83\u4eec\u5728\u95ee\u9898\u89e3\u51b3\u4e2d\u7684\u8868\u73b0", "method": "\u4e3aClaude Code\u4ee3\u7406\u914d\u5907\u57fa\u4e8eMCP\u7684\u793e\u4ea4\u5a92\u4f53\u548c\u65e5\u5fd7\u5de5\u5177\uff0c\u8ba9\u5b83\u4eec\u81ea\u4e3b\u4f7f\u7528\u8fd9\u4e9b\u5de5\u5177\u6765\u89e3\u51b334\u4e2aAider Polyglot Python\u7f16\u7a0b\u6311\u6218", "result": "\u534f\u4f5c\u5de5\u5177\u5728\u6700\u5177\u6311\u6218\u6027\u7684\u95ee\u9898\u4e0a\u663e\u8457\u63d0\u5347\u6027\u80fd\uff1a\u6210\u672c\u964d\u4f4e15-40%\uff0c\u4ea4\u4e92\u8f6e\u6b21\u51cf\u5c1112-27%\uff0c\u5b8c\u6210\u901f\u5ea6\u52a0\u5feb12-38%\u3002\u4e0d\u540c\u6a21\u578b\u81ea\u7136\u91c7\u7528\u4e0d\u540c\u7684\u534f\u4f5c\u7b56\u7565\uff0c\u4ee3\u7406\u66f4\u503e\u5411\u4e8e\u5199\u4f5c\u800c\u975e\u9605\u8bfb\uff082-9\u500d\uff09", "conclusion": "AI\u4ee3\u7406\u53ef\u4ee5\u5728\u80fd\u529b\u8fb9\u754c\u5904\u7cfb\u7edf\u6027\u5730\u53d7\u76ca\u4e8e\u4eba\u7c7b\u542f\u53d1\u7684\u534f\u4f5c\u5de5\u5177\uff0c\u8868\u660e\u81ea\u9002\u5e94\u534f\u4f5c\u754c\u9762\u53ef\u4ee5\u4f5c\u4e3a\u63a8\u7406\u589e\u5f3a\u5668\u800c\u975e\u901a\u7528\u6548\u7387\u63d0\u5347\u5de5\u5177", "relevance": 75.0}}
{"id": "2509.13814", "pdf": "https://arxiv.org/pdf/2509.13814", "abs": "https://arxiv.org/abs/2509.13814", "authors": ["Kartik Shinde", "Laurent Besacier", "Ondrej Bojar", "Thibaut Thonet", "Tirthankar Ghosal"], "title": "Findings of the Third Automatic Minuting (AutoMin) Challenge", "categories": ["cs.CL"], "comment": "Automin 2025 Website: https://ufal.github.io/automin-2025/", "summary": "This paper presents the third edition of AutoMin, a shared task on automatic\nmeeting summarization into minutes. In 2025, AutoMin featured the main task of\nminuting, the creation of structured meeting minutes, as well as a new task:\nquestion answering (QA) based on meeting transcripts.\n  The minuting task covered two languages, English and Czech, and two domains:\nproject meetings and European Parliament sessions. The QA task focused solely\non project meetings and was available in two settings: monolingual QA in\nEnglish, and cross-lingual QA, where questions were asked and answered in Czech\nbased on English meetings.\n  Participation in 2025 was more limited compared to previous years, with only\none team joining the minuting task and two teams participating in QA. However,\nas organizers, we included multiple baseline systems to enable a comprehensive\nevaluation of current (2025) large language models (LLMs) on both tasks.", "AI": {"tldr": "AutoMin 2025\u662f\u4e00\u4e2a\u81ea\u52a8\u4f1a\u8bae\u7eaa\u8981\u751f\u6210\u548c\u95ee\u7b54\u7684\u5171\u4eab\u4efb\u52a1\uff0c\u5305\u542b\u82f1\u6587\u548c\u6377\u514b\u8bed\u7684\u9879\u76ee\u4f1a\u8bae\u548c\u6b27\u6d32\u8bae\u4f1a\u4f1a\u8bae\u7eaa\u8981\u751f\u6210\uff0c\u4ee5\u53ca\u57fa\u4e8e\u4f1a\u8bae\u8f6c\u5f55\u7684\u95ee\u7b54\u4efb\u52a1\u3002\u53c2\u4e0e\u5ea6\u8f83\u4f4e\u4f46\u5305\u542b\u4e86\u591a\u4e2aLLM\u57fa\u7ebf\u7cfb\u7edf\u8bc4\u4f30\u3002", "motivation": "\u63a8\u52a8\u81ea\u52a8\u4f1a\u8bae\u7eaa\u8981\u751f\u6210\u6280\u672f\u7684\u53d1\u5c55\uff0c\u7279\u522b\u662f\u5728\u591a\u8bed\u8a00\u548c\u8de8\u8bed\u8a00\u573a\u666f\u4e0b\uff0c\u8bc4\u4f30\u5f53\u524d\u5927\u578b\u8bed\u8a00\u6a21\u578b\u5728\u7ed3\u6784\u5316\u4f1a\u8bae\u7eaa\u8981\u521b\u5efa\u548c\u57fa\u4e8e\u4f1a\u8bae\u5185\u5bb9\u7684\u95ee\u7b54\u4efb\u52a1\u4e0a\u7684\u8868\u73b0\u3002", "method": "\u7ec4\u7ec7\u5171\u4eab\u4efb\u52a1\uff0c\u8bbe\u7f6e\u4e24\u4e2a\u4e3b\u8981\u4efb\u52a1\uff1a1\uff09\u7ed3\u6784\u5316\u4f1a\u8bae\u7eaa\u8981\u751f\u6210\uff08\u82f1\u6587\u548c\u6377\u514b\u8bed\u7684\u9879\u76ee\u4f1a\u8bae\u548c\u6b27\u6d32\u8bae\u4f1a\u4f1a\u8bae\uff092\uff09\u57fa\u4e8e\u4f1a\u8bae\u8f6c\u5f55\u7684\u95ee\u7b54\uff08\u5355\u8bed\u8a00\u82f1\u6587\u95ee\u7b54\u548c\u8de8\u8bed\u8a00\u6377\u514b\u95ee\u7b54\uff09\u3002\u4f7f\u7528\u591a\u4e2a\u57fa\u7ebf\u7cfb\u7edf\uff08\u5305\u62ec\u5f53\u524dLLMs\uff09\u8fdb\u884c\u7efc\u5408\u8bc4\u4f30\u3002", "result": "2025\u5e74\u53c2\u4e0e\u5ea6\u8f83\u4f4e\uff08\u7eaa\u8981\u4efb\u52a11\u4e2a\u56e2\u961f\uff0c\u95ee\u7b54\u4efb\u52a12\u4e2a\u56e2\u961f\uff09\uff0c\u4f46\u901a\u8fc7\u7ec4\u7ec7\u65b9\u63d0\u4f9b\u7684\u591a\u4e2a\u57fa\u7ebf\u7cfb\u7edf\uff0c\u80fd\u591f\u5168\u9762\u8bc4\u4f30\u5f53\u524dLLM\u5728\u4f1a\u8bae\u7eaa\u8981\u751f\u6210\u548c\u95ee\u7b54\u4efb\u52a1\u4e0a\u7684\u6027\u80fd\u8868\u73b0\u3002", "conclusion": "AutoMin 2025\u4e3a\u81ea\u52a8\u4f1a\u8bae\u7eaa\u8981\u751f\u6210\u548c\u95ee\u7b54\u4efb\u52a1\u63d0\u4f9b\u4e86\u6709\u4ef7\u503c\u7684\u57fa\u51c6\u6d4b\u8bd5\u5e73\u53f0\uff0c\u5c3d\u7ba1\u53c2\u4e0e\u56e2\u961f\u6570\u91cf\u6709\u9650\uff0c\u4f46\u901a\u8fc7\u57fa\u7ebf\u7cfb\u7edf\u7684\u5f15\u5165\u4ecd\u80fd\u6709\u6548\u8bc4\u4f30\u5f53\u524dLLM\u6280\u672f\u5728\u8fd9\u4e9b\u4efb\u52a1\u4e0a\u7684\u80fd\u529b\u3002", "relevance": 35.0}}
{"id": "2509.13506", "pdf": "https://arxiv.org/pdf/2509.13506", "abs": "https://arxiv.org/abs/2509.13506", "authors": ["Xingzi Xu", "Qi Li", "Shuwen Qiu", "Julien Han", "Karim Bouyarmane"], "title": "DEFT-VTON: Efficient Virtual Try-On with Consistent Generalised H-Transform", "categories": ["cs.CV"], "comment": "Published in 2025 CVPR Workshop", "summary": "Diffusion models enable high-quality virtual try-on (VTO) with their\nestablished image synthesis abilities. Despite the extensive end-to-end\ntraining of large pre-trained models involved in current VTO methods,\nreal-world applications often prioritize limited training and inference,\nserving, and deployment budgets for VTO. To solve this obstacle, we apply\nDoob's h-transform efficient fine-tuning (DEFT) for adapting large pre-trained\nunconditional models for downstream image-conditioned VTO abilities. DEFT\nfreezes the pre-trained model's parameters and trains a small h-transform\nnetwork to learn a conditional h-transform. The h-transform network allows\ntraining only 1.42 percent of the frozen parameters, compared to a baseline of\n5.52 percent in traditional parameter-efficient fine-tuning (PEFT).\n  To further improve DEFT's performance and decrease existing models' inference\ntime, we additionally propose an adaptive consistency loss. Consistency\ntraining distills slow but high-performing diffusion models into a fast one\nwhile retaining performance by enforcing consistencies along the inference\npath. Inspired by constrained optimization, instead of distillation, we combine\nthe consistency loss and the denoising score matching loss in a data-adaptive\nmanner for fine-tuning existing VTO models at a low cost. Empirical results\nshow the proposed DEFT-VTON method achieves state-of-the-art performance on VTO\ntasks, with as few as 15 denoising steps, while maintaining competitive\nresults.", "AI": {"tldr": "DEFT-VTON\uff1a\u57fa\u4e8eDoob's h-transform\u9ad8\u6548\u5fae\u8c03\u65b9\u6cd5\uff0c\u4ec5\u9700\u8bad\u7ec31.42%\u53c2\u6570\u5373\u53ef\u5c06\u65e0\u6761\u4ef6\u6269\u6563\u6a21\u578b\u9002\u914d\u5230\u865a\u62df\u8bd5\u7a7f\u4efb\u52a1\uff0c\u7ed3\u5408\u81ea\u9002\u5e94\u4e00\u81f4\u6027\u635f\u5931\u5b9e\u73b015\u6b65\u5feb\u901f\u63a8\u7406\u7684SOTA\u6027\u80fd", "motivation": "\u73b0\u6709\u865a\u62df\u8bd5\u7a7f\u65b9\u6cd5\u9700\u8981\u7aef\u5230\u7aef\u8bad\u7ec3\u5927\u578b\u9884\u8bad\u7ec3\u6a21\u578b\uff0c\u4f46\u5b9e\u9645\u5e94\u7528\u9700\u8981\u6709\u9650\u7684\u8bad\u7ec3\u548c\u63a8\u7406\u9884\u7b97\u3002\u9700\u8981\u4e00\u79cd\u53c2\u6570\u9ad8\u6548\u7684\u65b9\u6cd5\u6765\u9002\u914d\u9884\u8bad\u7ec3\u6a21\u578b\u5230\u4e0b\u6e38\u4efb\u52a1", "method": "1. DEFT\uff1a\u51bb\u7ed3\u9884\u8bad\u7ec3\u6a21\u578b\u53c2\u6570\uff0c\u8bad\u7ec3\u5c0f\u578bh-transform\u7f51\u7edc\u5b66\u4e60\u6761\u4ef6\u53d8\u6362\uff0c\u4ec5\u8bad\u7ec31.42%\u53c2\u6570\n2. \u81ea\u9002\u5e94\u4e00\u81f4\u6027\u635f\u5931\uff1a\u7ed3\u5408\u4e00\u81f4\u6027\u635f\u5931\u548c\u53bb\u566a\u5206\u6570\u5339\u914d\u635f\u5931\uff0c\u4ee5\u6570\u636e\u81ea\u9002\u5e94\u65b9\u5f0f\u5fae\u8c03\u6a21\u578b\n3. \u4ec5\u970015\u6b65\u53bb\u566a\u6b65\u9aa4\u5b9e\u73b0\u5feb\u901f\u63a8\u7406", "result": "\u5728\u865a\u62df\u8bd5\u7a7f\u4efb\u52a1\u4e0a\u8fbe\u5230state-of-the-art\u6027\u80fd\uff0c\u4ec5\u970015\u6b65\u63a8\u7406\u6b65\u9aa4\uff0c\u540c\u65f6\u4fdd\u6301\u7ade\u4e89\u6027\u7ed3\u679c\u3002\u76f8\u6bd4\u4f20\u7edf\u53c2\u6570\u9ad8\u6548\u5fae\u8c03\u65b9\u6cd5\uff085.52%\u53c2\u6570\uff09\uff0c\u4ec5\u9700\u8bad\u7ec31.42%\u53c2\u6570", "conclusion": "DEFT-VTON\u63d0\u4f9b\u4e86\u4e00\u79cd\u53c2\u6570\u9ad8\u6548\u7684\u65b9\u6cd5\u6765\u9002\u914d\u9884\u8bad\u7ec3\u6269\u6563\u6a21\u578b\u5230\u6761\u4ef6\u751f\u6210\u4efb\u52a1\uff0c\u5728\u4fdd\u6301\u6027\u80fd\u7684\u540c\u65f6\u663e\u8457\u51cf\u5c11\u8bad\u7ec3\u548c\u63a8\u7406\u6210\u672c", "relevance": 65.0}}
{"id": "2509.13717", "pdf": "https://arxiv.org/pdf/2509.13717", "abs": "https://arxiv.org/abs/2509.13717", "authors": ["Yifan Yu", "Cheuk Hin Ho", "Yangshuai Wang"], "title": "A Conformal Prediction Framework for Uncertainty Quantification in Physics-Informed Neural Networks", "categories": ["cs.LG", "cs.NA", "math.NA"], "comment": null, "summary": "Physics-Informed Neural Networks (PINNs) have emerged as a powerful framework\nfor solving PDEs, yet existing uncertainty quantification (UQ) approaches for\nPINNs generally lack rigorous statistical guarantees. In this work, we bridge\nthis gap by introducing a distribution-free conformal prediction (CP) framework\nfor UQ in PINNs. This framework calibrates prediction intervals by constructing\nnonconformity scores on a calibration set, thereby yielding distribution-free\nuncertainty estimates with rigorous finite-sample coverage guarantees for\nPINNs. To handle spatial heteroskedasticity, we further introduce local\nconformal quantile estimation, enabling spatially adaptive uncertainty bands\nwhile preserving theoretical guarantee. Through systematic evaluations on\ntypical PDEs (damped harmonic oscillator, Poisson, Allen-Cahn, and Helmholtz\nequations) and comprehensive testing across multiple uncertainty metrics, our\nresults demonstrate that the proposed framework achieves reliable calibration\nand locally adaptive uncertainty intervals, consistently outperforming\nheuristic UQ approaches. By bridging PINNs with distribution-free UQ, this work\nintroduces a general framework that not only enhances calibration and\nreliability, but also opens new avenues for uncertainty-aware modeling of\ncomplex PDE systems.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u4e2a\u57fa\u4e8e\u5171\u5f62\u9884\u6d4b\u7684\u5206\u5e03\u65e0\u5173\u4e0d\u786e\u5b9a\u6027\u91cf\u5316\u6846\u67b6\uff0c\u7528\u4e8e\u7269\u7406\u4fe1\u606f\u795e\u7ecf\u7f51\u7edc(PINNs)\uff0c\u4e3aPINNs\u63d0\u4f9b\u5177\u6709\u4e25\u683c\u7edf\u8ba1\u4fdd\u8bc1\u7684\u7a7a\u95f4\u81ea\u9002\u5e94\u4e0d\u786e\u5b9a\u6027\u533a\u95f4\u3002", "motivation": "\u73b0\u6709PINNs\u7684\u4e0d\u786e\u5b9a\u6027\u91cf\u5316\u65b9\u6cd5\u7f3a\u4e4f\u4e25\u683c\u7684\u7edf\u8ba1\u4fdd\u8bc1\uff0c\u9700\u8981\u4e00\u79cd\u80fd\u591f\u63d0\u4f9b\u6709\u9650\u6837\u672c\u8986\u76d6\u4fdd\u8bc1\u7684\u5206\u5e03\u65e0\u5173UQ\u6846\u67b6\u3002", "method": "\u5f15\u5165\u5206\u5e03\u65e0\u5173\u7684\u5171\u5f62\u9884\u6d4b\u6846\u67b6\uff0c\u901a\u8fc7\u5728\u6821\u51c6\u96c6\u4e0a\u6784\u5efa\u975e\u5171\u5f62\u6027\u5206\u6570\u6765\u6821\u51c6\u9884\u6d4b\u533a\u95f4\uff0c\u5e76\u8fdb\u4e00\u6b65\u63d0\u51fa\u5c40\u90e8\u5171\u5f62\u5206\u4f4d\u6570\u4f30\u8ba1\u6765\u5904\u7406\u7a7a\u95f4\u5f02\u65b9\u5dee\u6027\u3002", "result": "\u5728\u5178\u578bPDE\u7cfb\u7edf\u4e0a\u7684\u7cfb\u7edf\u8bc4\u4f30\u8868\u660e\uff0c\u8be5\u6846\u67b6\u5b9e\u73b0\u4e86\u53ef\u9760\u7684\u6821\u51c6\u548c\u5c40\u90e8\u81ea\u9002\u5e94\u4e0d\u786e\u5b9a\u6027\u533a\u95f4\uff0c\u5728\u591a\u4e2a\u4e0d\u786e\u5b9a\u6027\u6307\u6807\u4e0a\u4e00\u81f4\u4f18\u4e8e\u542f\u53d1\u5f0fUQ\u65b9\u6cd5\u3002", "conclusion": "\u8be5\u5de5\u4f5c\u901a\u8fc7\u5c06PINNs\u4e0e\u5206\u5e03\u65e0\u5173UQ\u76f8\u7ed3\u5408\uff0c\u4e0d\u4ec5\u63d0\u9ad8\u4e86\u6821\u51c6\u548c\u53ef\u9760\u6027\uff0c\u8fd8\u4e3a\u590d\u6742PDE\u7cfb\u7edf\u7684\u4e0d\u786e\u5b9a\u6027\u611f\u77e5\u5efa\u6a21\u5f00\u8f9f\u4e86\u65b0\u9014\u5f84\u3002", "relevance": 25.0}}
{"id": "2509.13570", "pdf": "https://arxiv.org/pdf/2509.13570", "abs": "https://arxiv.org/abs/2509.13570", "authors": ["Hannah Klawa", "Shraddha Rajpal", "Cigole Thomas"], "title": "Gen AI in Proof-based Math Courses: A Pilot Study", "categories": ["cs.AI", "math.HO", "Primary: 97U50, Secondary: 97U70, 97D40, 97D60, 97E50, 97H40"], "comment": "35 pages, 6 figures, Comments welcome!", "summary": "With the rapid rise of generative AI in higher education and the\nunreliability of current AI detection tools, developing policies that encourage\nstudent learning and critical thinking has become increasingly important. This\nstudy examines student use and perceptions of generative AI across three\nproof-based undergraduate mathematics courses: a first-semester abstract\nalgebra course, a topology course and a second-semester abstract algebra\ncourse. In each case, course policy permitted some use of generative AI.\nDrawing on survey responses and student interviews, we analyze how students\nengaged with AI tools, their perceptions of generative AI's usefulness and\nlimitations, and what implications these perceptions hold for teaching\nproof-based mathematics. We conclude by discussing future considerations for\nintegrating generative AI into proof-based mathematics instruction.", "AI": {"tldr": "\u8be5\u7814\u7a76\u8c03\u67e5\u4e86\u672c\u79d1\u751f\u5728\u8bc1\u660e\u7c7b\u6570\u5b66\u8bfe\u7a0b\u4e2d\u4f7f\u7528\u751f\u6210\u5f0fAI\u7684\u60c5\u51b5\uff0c\u5206\u6790\u4e86\u5b66\u751f\u7684\u4f7f\u7528\u6a21\u5f0f\u3001\u5bf9AI\u6709\u7528\u6027\u548c\u5c40\u9650\u6027\u7684\u770b\u6cd5\uff0c\u4ee5\u53ca\u5bf9\u6570\u5b66\u6559\u5b66\u7684\u5f71\u54cd\u3002", "motivation": "\u968f\u7740\u751f\u6210\u5f0fAI\u5728\u9ad8\u7b49\u6559\u80b2\u4e2d\u7684\u5feb\u901f\u666e\u53ca\u548c\u73b0\u6709AI\u68c0\u6d4b\u5de5\u5177\u7684\u4e0d\u53ef\u9760\u6027\uff0c\u9700\u8981\u5236\u5b9a\u9f13\u52b1\u5b66\u751f\u5b66\u4e60\u548c\u6279\u5224\u6027\u601d\u7ef4\u7684\u653f\u7b56\u3002\u7814\u7a76\u65e8\u5728\u4e86\u89e3\u5b66\u751f\u5728\u8bc1\u660e\u7c7b\u6570\u5b66\u8bfe\u7a0b\u4e2d\u5982\u4f55\u4f7f\u7528\u548c\u770b\u5f85\u751f\u6210\u5f0fAI\u3002", "method": "\u901a\u8fc7\u5bf9\u4e09\u95e8\u8bc1\u660e\u7c7b\u672c\u79d1\u6570\u5b66\u8bfe\u7a0b\uff08\u62bd\u8c61\u4ee3\u6570\u3001\u62d3\u6251\u5b66\uff09\u7684\u5b66\u751f\u8fdb\u884c\u95ee\u5377\u8c03\u67e5\u548c\u8bbf\u8c08\uff0c\u5206\u6790\u5b66\u751f\u5982\u4f55\u4e0eAI\u5de5\u5177\u4e92\u52a8\u53ca\u5176\u5bf9\u751f\u6210\u5f0fAI\u7684\u770b\u6cd5\u3002", "result": "\u7814\u7a76\u53d1\u73b0\u5b66\u751f\u5728\u5141\u8bb8\u4f7f\u7528AI\u7684\u8bfe\u7a0b\u653f\u7b56\u4e0b\u4f1a\u4f7f\u7528\u751f\u6210\u5f0fAI\u5de5\u5177\uff0c\u7814\u7a76\u5206\u6790\u4e86\u5b66\u751f\u7684\u4f7f\u7528\u6a21\u5f0f\u3001\u5bf9AI\u6709\u7528\u6027\u548c\u5c40\u9650\u6027\u7684\u8ba4\u77e5\u3002", "conclusion": "\u7814\u7a76\u8ba8\u8bba\u4e86\u5c06\u751f\u6210\u5f0fAI\u6574\u5408\u5230\u8bc1\u660e\u7c7b\u6570\u5b66\u6559\u5b66\u4e2d\u7684\u672a\u6765\u8003\u91cf\uff0c\u4e3a\u5236\u5b9a\u76f8\u5173\u6559\u5b66\u653f\u7b56\u63d0\u4f9b\u53c2\u8003\u3002", "relevance": 25.0}}
{"id": "2509.13835", "pdf": "https://arxiv.org/pdf/2509.13835", "abs": "https://arxiv.org/abs/2509.13835", "authors": ["Minh Duc Bui", "Carolin Holtermann", "Valentin Hofmann", "Anne Lauscher", "Katharina von der Wense"], "title": "Large Language Models Discriminate Against Speakers of German Dialects", "categories": ["cs.CL"], "comment": "Accepted to EMNLP 2025 Main", "summary": "Dialects represent a significant component of human culture and are found\nacross all regions of the world. In Germany, more than 40% of the population\nspeaks a regional dialect (Adler and Hansen, 2022). However, despite cultural\nimportance, individuals speaking dialects often face negative societal\nstereotypes. We examine whether such stereotypes are mirrored by large language\nmodels (LLMs). We draw on the sociolinguistic literature on dialect perception\nto analyze traits commonly associated with dialect speakers. Based on these\ntraits, we assess the dialect naming bias and dialect usage bias expressed by\nLLMs in two tasks: an association task and a decision task. To assess a model's\ndialect usage bias, we construct a novel evaluation corpus that pairs sentences\nfrom seven regional German dialects (e.g., Alemannic and Bavarian) with their\nstandard German counterparts. We find that: (1) in the association task, all\nevaluated LLMs exhibit significant dialect naming and dialect usage bias\nagainst German dialect speakers, reflected in negative adjective associations;\n(2) all models reproduce these dialect naming and dialect usage biases in their\ndecision making; and (3) contrary to prior work showing minimal bias with\nexplicit demographic mentions, we find that explicitly labeling linguistic\ndemographics--German dialect speakers--amplifies bias more than implicit cues\nlike dialect usage.", "AI": {"tldr": "\u8be5\u7814\u7a76\u5206\u6790\u4e86\u5927\u578b\u8bed\u8a00\u6a21\u578b\u5bf9\u5fb7\u56fd\u65b9\u8a00\u4f7f\u7528\u8005\u7684\u504f\u89c1\uff0c\u53d1\u73b0\u6240\u6709\u6d4b\u8bd5\u7684LLM\u90fd\u8868\u73b0\u51fa\u663e\u8457\u7684\u65b9\u8a00\u547d\u540d\u548c\u4f7f\u7528\u504f\u89c1\uff0c\u4e14\u5728\u51b3\u7b56\u4e2d\u590d\u5236\u8fd9\u4e9b\u504f\u89c1\uff0c\u660e\u786e\u6807\u6ce8\u65b9\u8a00\u4f7f\u7528\u8005\u8eab\u4efd\u4f1a\u653e\u5927\u504f\u89c1\u3002", "motivation": "\u7814\u7a76\u52a8\u673a\u662f\u8c03\u67e5\u793e\u4f1a\u8bed\u8a00\u5b66\u4e2d\u65b9\u8a00\u4f7f\u7528\u8005\u7684\u8d1f\u9762\u523b\u677f\u5370\u8c61\u662f\u5426\u5728\u5927\u578b\u8bed\u8a00\u6a21\u578b\u4e2d\u91cd\u73b0\uff0c\u5fb7\u56fd\u6709\u8d85\u8fc740%\u4eba\u53e3\u4f7f\u7528\u65b9\u8a00\u4f46\u9762\u4e34\u793e\u4f1a\u504f\u89c1\u3002", "method": "\u57fa\u4e8e\u793e\u4f1a\u8bed\u8a00\u5b66\u6587\u732e\u6784\u5efa\u8bc4\u4f30\u6846\u67b6\uff0c\u901a\u8fc7\u5173\u8054\u4efb\u52a1\u548c\u51b3\u7b56\u4efb\u52a1\u8bc4\u4f30\u65b9\u8a00\u547d\u540d\u504f\u89c1\u548c\u4f7f\u7528\u504f\u89c1\uff0c\u521b\u5efa\u4e86\u5305\u542b7\u79cd\u5fb7\u56fd\u65b9\u8a00\u4e0e\u6807\u51c6\u5fb7\u8bed\u5bf9\u7167\u7684\u65b0\u8bc4\u4f30\u8bed\u6599\u5e93\u3002", "result": "\u6240\u6709\u8bc4\u4f30\u7684LLM\u90fd\u8868\u73b0\u51fa\u663e\u8457\u7684\u65b9\u8a00\u504f\u89c1\uff0c\u4f53\u73b0\u5728\u8d1f\u9762\u5f62\u5bb9\u8bcd\u5173\u8054\u4e0a\uff1b\u6a21\u578b\u5728\u51b3\u7b56\u4e2d\u590d\u5236\u8fd9\u4e9b\u504f\u89c1\uff1b\u660e\u786e\u6807\u6ce8\u65b9\u8a00\u4f7f\u7528\u8005\u8eab\u4efd\u6bd4\u9690\u5f0f\u7ebf\u7d22\u66f4\u80fd\u653e\u5927\u504f\u89c1\u3002", "conclusion": "LLM\u786e\u5b9e\u53cd\u6620\u4e86\u793e\u4f1a\u4e2d\u5bf9\u65b9\u8a00\u4f7f\u7528\u8005\u7684\u523b\u677f\u5370\u8c61\u504f\u89c1\uff0c\u9700\u8981\u5f00\u53d1\u65b9\u6cd5\u6765\u51cf\u8f7b\u8fd9\u79cd\u8bed\u8a00\u504f\u89c1\uff0c\u7279\u522b\u662f\u5728\u591a\u8bed\u8a00\u548c\u591a\u65b9\u8a00\u73af\u5883\u4e2d\u3002", "relevance": 85.0}}
{"id": "2509.13507", "pdf": "https://arxiv.org/pdf/2509.13507", "abs": "https://arxiv.org/abs/2509.13507", "authors": ["Artem Savkin", "Thomas Lapotre", "Kevin Strauss", "Uzair Akbar", "Federico Tombari"], "title": "Adversarial Appearance Learning in Augmented Cityscapes for Pedestrian Recognition in Autonomous Driving", "categories": ["cs.CV"], "comment": null, "summary": "In the autonomous driving area synthetic data is crucial for cover specific\ntraffic scenarios which autonomous vehicle must handle. This data commonly\nintroduces domain gap between synthetic and real domains. In this paper we\ndeploy data augmentation to generate custom traffic scenarios with VRUs in\norder to improve pedestrian recognition. We provide a pipeline for augmentation\nof the Cityscapes dataset with virtual pedestrians. In order to improve\naugmentation realism of the pipeline we reveal a novel generative network\narchitecture for adversarial learning of the data-set lighting conditions. We\nalso evaluate our approach on the tasks of semantic and instance segmentation.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u6570\u636e\u589e\u5f3a\u6d41\u6c34\u7ebf\uff0c\u901a\u8fc7\u5728Cityscapes\u6570\u636e\u96c6\u4e2d\u6dfb\u52a0\u865a\u62df\u884c\u4eba\u6765\u6539\u5584\u81ea\u52a8\u9a7e\u9a76\u4e2d\u7684\u884c\u4eba\u8bc6\u522b\uff0c\u5e76\u5f15\u5165\u65b0\u7684\u751f\u6210\u5bf9\u6297\u7f51\u7edc\u67b6\u6784\u6765\u63d0\u5347\u5149\u7167\u6761\u4ef6\u7684\u771f\u5b9e\u6027\u3002", "motivation": "\u81ea\u52a8\u9a7e\u9a76\u9886\u57df\u9700\u8981\u5408\u6210\u6570\u636e\u6765\u8986\u76d6\u7279\u5b9a\u4ea4\u901a\u573a\u666f\uff0c\u4f46\u5408\u6210\u6570\u636e\u4e0e\u771f\u5b9e\u6570\u636e\u4e4b\u95f4\u5b58\u5728\u9886\u57df\u5dee\u8ddd\u3002\u4e3a\u4e86\u89e3\u51b3\u8fd9\u4e2a\u95ee\u9898\uff0c\u9700\u8981\u751f\u6210\u5305\u542b\u5f31\u52bf\u9053\u8def\u4f7f\u7528\u8005\uff08VRUs\uff09\u7684\u81ea\u5b9a\u4e49\u4ea4\u901a\u573a\u666f\u6765\u63d0\u5347\u884c\u4eba\u8bc6\u522b\u6027\u80fd\u3002", "method": "1) \u5f00\u53d1\u6570\u636e\u589e\u5f3a\u6d41\u6c34\u7ebf\uff0c\u5728Cityscapes\u6570\u636e\u96c6\u4e2d\u6dfb\u52a0\u865a\u62df\u884c\u4eba\n2) \u63d0\u51fa\u65b0\u9896\u7684\u751f\u6210\u5bf9\u6297\u7f51\u7edc\u67b6\u6784\uff0c\u7528\u4e8e\u5bf9\u6297\u5b66\u4e60\u6570\u636e\u96c6\u7684\u5149\u7167\u6761\u4ef6\uff0c\u63d0\u9ad8\u589e\u5f3a\u7684\u771f\u5b9e\u6027", "result": "\u5728\u8bed\u4e49\u5206\u5272\u548c\u5b9e\u4f8b\u5206\u5272\u4efb\u52a1\u4e0a\u8bc4\u4f30\u4e86\u8be5\u65b9\u6cd5\uff0c\u4f46\u5177\u4f53\u7ed3\u679c\u672a\u5728\u6458\u8981\u4e2d\u8be6\u7ec6\u8bf4\u660e", "conclusion": "\u8be5\u65b9\u6cd5\u901a\u8fc7\u6570\u636e\u589e\u5f3a\u548c\u5149\u7167\u6761\u4ef6\u5bf9\u6297\u5b66\u4e60\uff0c\u80fd\u591f\u751f\u6210\u66f4\u771f\u5b9e\u7684\u5408\u6210\u6570\u636e\uff0c\u6709\u52a9\u4e8e\u7f29\u5c0f\u5408\u6210\u4e0e\u771f\u5b9e\u6570\u636e\u4e4b\u95f4\u7684\u9886\u57df\u5dee\u8ddd\uff0c\u63d0\u5347\u884c\u4eba\u8bc6\u522b\u6027\u80fd", "relevance": 25.0}}
{"id": "2509.13725", "pdf": "https://arxiv.org/pdf/2509.13725", "abs": "https://arxiv.org/abs/2509.13725", "authors": ["Md Sabbir Ahmed", "Noah French", "Mark Rucker", "Zhiyuan Wang", "Taylor Myers-Brower", "Kaitlyn Petz", "Mehdi Boukhechba", "Bethany A. Teachman", "Laura E. Barnes"], "title": "WatchAnxiety: A Transfer Learning Approach for State Anxiety Prediction from Smartwatch Data", "categories": ["cs.LG", "cs.CY"], "comment": null, "summary": "Social anxiety is a common mental health condition linked to significant\nchallenges in academic, social, and occupational functioning. A core feature is\nelevated momentary (state) anxiety in social situations, yet little prior work\nhas measured or predicted fluctuations in this anxiety throughout the day.\nCapturing these intra-day dynamics is critical for designing real-time,\npersonalized interventions such as Just-In-Time Adaptive Interventions\n(JITAIs). To address this gap, we conducted a study with socially anxious\ncollege students (N=91; 72 after exclusions) using our custom smartwatch-based\nsystem over an average of 9.03 days (SD = 2.95). Participants received seven\necological momentary assessments (EMAs) per day to report state anxiety. We\ndeveloped a base model on over 10,000 days of external heart rate data,\ntransferred its representations to our dataset, and fine-tuned it to generate\nprobabilistic predictions. These were combined with trait-level measures in a\nmeta-learner. Our pipeline achieved 60.4% balanced accuracy in state anxiety\ndetection in our dataset. To evaluate generalizability, we applied the training\napproach to a separate hold-out set from the TILES-18 dataset-the same dataset\nused for pretraining. On 10,095 once-daily EMAs, our method achieved 59.1%\nbalanced accuracy, outperforming prior work by at least 7%.", "AI": {"tldr": "\u8be5\u7814\u7a76\u5f00\u53d1\u4e86\u4e00\u4e2a\u57fa\u4e8e\u667a\u80fd\u624b\u8868\u7684\u7cfb\u7edf\uff0c\u901a\u8fc7\u5fc3\u7387\u6570\u636e\u548c\u751f\u6001\u77ac\u65f6\u8bc4\u4f30\u6765\u9884\u6d4b\u793e\u4ea4\u7126\u8651\u60a3\u8005\u7684\u77ac\u65f6\u7126\u8651\u72b6\u6001\uff0c\u5728\u5185\u90e8\u548c\u5916\u90e8\u6570\u636e\u96c6\u4e0a\u5206\u522b\u8fbe\u523060.4%\u548c59.1%\u7684\u5e73\u8861\u51c6\u786e\u7387\u3002", "motivation": "\u793e\u4ea4\u7126\u8651\u662f\u4e00\u79cd\u5e38\u89c1\u5fc3\u7406\u5065\u5eb7\u95ee\u9898\uff0c\u4f46\u76ee\u524d\u7f3a\u4e4f\u5bf9\u65e5\u5e38\u7126\u8651\u6ce2\u52a8\u7684\u5b9e\u65f6\u76d1\u6d4b\u548c\u9884\u6d4b\u65b9\u6cd5\uff0c\u8fd9\u5bf9\u4e8e\u8bbe\u8ba1\u5b9e\u65f6\u4e2a\u6027\u5316\u5e72\u9884\u63aa\u65bd\u81f3\u5173\u91cd\u8981\u3002", "method": "\u4f7f\u7528\u5b9a\u5236\u667a\u80fd\u624b\u8868\u7cfb\u7edf\u6536\u96c691\u540d\u793e\u4ea4\u7126\u8651\u5927\u5b66\u751f\u7684\u5fc3\u7387\u6570\u636e\uff0c\u7ed3\u5408\u6bcf\u65e57\u6b21\u751f\u6001\u77ac\u65f6\u8bc4\u4f30\u3002\u57fa\u4e8e10,000\u591a\u5929\u5916\u90e8\u5fc3\u7387\u6570\u636e\u9884\u8bad\u7ec3\u57fa\u7840\u6a21\u578b\uff0c\u8fc1\u79fb\u5b66\u4e60\u5230\u76ee\u6807\u6570\u636e\u96c6\uff0c\u5e76\u7ed3\u5408\u7279\u8d28\u6c34\u5e73\u6d4b\u91cf\u6784\u5efa\u5143\u5b66\u4e60\u5668\u3002", "result": "\u5728\u5185\u90e8\u6570\u636e\u96c6\u4e0a\u8fbe\u523060.4%\u7684\u5e73\u8861\u51c6\u786e\u7387\uff0c\u5728\u5916\u90e8TILES-18\u6570\u636e\u96c6\u4e0a\u8fbe\u523059.1%\u7684\u5e73\u8861\u51c6\u786e\u7387\uff0c\u6bd4\u5148\u524d\u5de5\u4f5c\u63d0\u5347\u81f3\u5c117%\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u80fd\u591f\u6709\u6548\u9884\u6d4b\u793e\u4ea4\u7126\u8651\u60a3\u8005\u7684\u77ac\u65f6\u7126\u8651\u72b6\u6001\uff0c\u4e3a\u5b9e\u65f6\u4e2a\u6027\u5316\u5e72\u9884\u63d0\u4f9b\u4e86\u6280\u672f\u57fa\u7840\uff0c\u5177\u6709\u826f\u597d\u7684\u6cdb\u5316\u80fd\u529b\u3002", "relevance": 25.0}}
{"id": "2509.13588", "pdf": "https://arxiv.org/pdf/2509.13588", "abs": "https://arxiv.org/abs/2509.13588", "authors": ["Xuan Liu", "Haoyang Shang", "Haojian Jin"], "title": "Programmable Cognitive Bias in Social Agents", "categories": ["cs.AI", "cs.CE", "cs.CY"], "comment": null, "summary": "This paper introduces CoBRA, a novel toolkit for systematically specifying\nagent behavior in LLM-based social simulation. We found that conventional\napproaches that specify agent behaviors through implicit natural language\ndescriptions cannot yield consistent behaviors across models, and the produced\nagent behaviors do not capture the nuances of the descriptions. In contrast,\nCoBRA presents a new approach to program agents' cognitive biases explicitly,\nby grounding agents' expected behaviors using classic social science\nexperiments. CoBRA has two components: (1) Cognitive Bias Index that measures\nthe cognitive bias of a social agent, by quantifying the agent's reactions in a\nset of validated classical social science experiments; (2) Behavioral\nRegulation Engine that aligns the agent's behavior to demonstrate controlled\ncognitive bias. We evaluated CoBRA as an HCI toolkit through demonstration and\ntechnical benchmarks. Our results suggest that CoBRA can precisely program the\ncognitive bias demonstrated in a social agent in a model-agnostic manner.", "AI": {"tldr": "CoBRA\u662f\u4e00\u4e2a\u7528\u4e8e\u5728\u57fa\u4e8eLLM\u7684\u793e\u4f1a\u6a21\u62df\u4e2d\u7cfb\u7edf\u5316\u6307\u5b9a\u667a\u80fd\u4f53\u884c\u4e3a\u7684\u5de5\u5177\u5305\uff0c\u901a\u8fc7\u663e\u5f0f\u7f16\u7a0b\u8ba4\u77e5\u504f\u89c1\u6765\u89e3\u51b3\u4f20\u7edf\u81ea\u7136\u8bed\u8a00\u63cf\u8ff0\u65b9\u6cd5\u7684\u4e00\u81f4\u6027\u95ee\u9898\u3002", "motivation": "\u4f20\u7edf\u4f7f\u7528\u81ea\u7136\u8bed\u8a00\u9690\u5f0f\u63cf\u8ff0\u667a\u80fd\u4f53\u884c\u4e3a\u7684\u65b9\u6cd5\u65e0\u6cd5\u5728\u4e0d\u540c\u6a21\u578b\u95f4\u4ea7\u751f\u4e00\u81f4\u884c\u4e3a\uff0c\u4e14\u65e0\u6cd5\u51c6\u786e\u6355\u6349\u63cf\u8ff0\u7684\u7ec6\u5fae\u5dee\u522b\uff0c\u9700\u8981\u66f4\u7cfb\u7edf\u5316\u7684\u884c\u4e3a\u89c4\u8303\u65b9\u6cd5\u3002", "method": "CoBRA\u5305\u542b\u4e24\u4e2a\u7ec4\u4ef6\uff1a1)\u8ba4\u77e5\u504f\u89c1\u6307\u6570 - \u901a\u8fc7\u7ecf\u5178\u793e\u4f1a\u79d1\u5b66\u5b9e\u9a8c\u91cf\u5316\u667a\u80fd\u4f53\u53cd\u5e94\u6765\u6d4b\u91cf\u8ba4\u77e5\u504f\u89c1\uff1b2)\u884c\u4e3a\u8c03\u8282\u5f15\u64ce - \u901a\u8fc7\u8c03\u8282\u4f7f\u667a\u80fd\u4f53\u5c55\u73b0\u53d7\u63a7\u7684\u8ba4\u77e5\u504f\u89c1\u3002\u57fa\u4e8e\u7ecf\u5178\u793e\u4f1a\u5b9e\u9a8c\u6765\u663e\u5f0f\u7f16\u7a0b\u667a\u80fd\u4f53\u7684\u8ba4\u77e5\u504f\u89c1\u3002", "result": "\u8bc4\u4f30\u663e\u793aCoBRA\u80fd\u591f\u4ee5\u6a21\u578b\u65e0\u5173\u7684\u65b9\u5f0f\u7cbe\u786e\u7f16\u7a0b\u793e\u4f1a\u667a\u80fd\u4f53\u4e2d\u5c55\u73b0\u7684\u8ba4\u77e5\u504f\u89c1\uff0c\u5728\u6280\u672f\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8868\u73b0\u826f\u597d\u3002", "conclusion": "CoBRA\u63d0\u4f9b\u4e86\u4e00\u79cd\u7cfb\u7edf\u5316\u7684\u65b9\u6cd5\u6765\u663e\u5f0f\u7f16\u7a0bLLM\u667a\u80fd\u4f53\u7684\u8ba4\u77e5\u504f\u89c1\uff0c\u89e3\u51b3\u4e86\u4f20\u7edf\u81ea\u7136\u8bed\u8a00\u63cf\u8ff0\u65b9\u6cd5\u7684\u5c40\u9650\u6027\uff0c\u5b9e\u73b0\u4e86\u6a21\u578b\u65e0\u5173\u7684\u7cbe\u786e\u884c\u4e3a\u63a7\u5236\u3002", "relevance": 65.0}}
{"id": "2509.13869", "pdf": "https://arxiv.org/pdf/2509.13869", "abs": "https://arxiv.org/abs/2509.13869", "authors": ["Yang Liu", "Chenhui Chu"], "title": "Do LLMs Align Human Values Regarding Social Biases? Judging and Explaining Social Biases with LLMs", "categories": ["cs.CL"], "comment": "38 pages, 31 figures", "summary": "Large language models (LLMs) can lead to undesired consequences when\nmisaligned with human values, especially in scenarios involving complex and\nsensitive social biases. Previous studies have revealed the misalignment of\nLLMs with human values using expert-designed or agent-based emulated bias\nscenarios. However, it remains unclear whether the alignment of LLMs with human\nvalues differs across different types of scenarios (e.g., scenarios containing\nnegative vs. non-negative questions). In this study, we investigate the\nalignment of LLMs with human values regarding social biases (HVSB) in different\ntypes of bias scenarios. Through extensive analysis of 12 LLMs from four model\nfamilies and four datasets, we demonstrate that LLMs with large model parameter\nscales do not necessarily have lower misalignment rate and attack success rate.\nMoreover, LLMs show a certain degree of alignment preference for specific types\nof scenarios and the LLMs from the same model family tend to have higher\njudgment consistency. In addition, we study the understanding capacity of LLMs\nwith their explanations of HVSB. We find no significant differences in the\nunderstanding of HVSB across LLMs. We also find LLMs prefer their own generated\nexplanations. Additionally, we endow smaller language models (LMs) with the\nability to explain HVSB. The generation results show that the explanations\ngenerated by the fine-tuned smaller LMs are more readable, but have a\nrelatively lower model agreeability.", "AI": {"tldr": "\u8be5\u7814\u7a76\u8c03\u67e5\u4e86\u5927\u8bed\u8a00\u6a21\u578b\u5728\u4e0d\u540c\u7c7b\u578b\u504f\u89c1\u573a\u666f\u4e2d\u4e0e\u4eba\u7c7b\u4ef7\u503c\u89c2\u7684\u5bf9\u9f50\u60c5\u51b5\uff0c\u53d1\u73b0\u6a21\u578b\u53c2\u6570\u89c4\u6a21\u4e0e\u5bf9\u9f50\u7a0b\u5ea6\u65e0\u5fc5\u7136\u8054\u7cfb\uff0c\u6a21\u578b\u5bf9\u7279\u5b9a\u573a\u666f\u7c7b\u578b\u6709\u5bf9\u9f50\u504f\u597d\uff0c\u4e14\u540c\u5bb6\u65cf\u6a21\u578b\u5224\u65ad\u4e00\u81f4\u6027\u66f4\u9ad8\u3002", "motivation": "\u7814\u7a76\u5927\u8bed\u8a00\u6a21\u578b\u5728\u590d\u6742\u654f\u611f\u7684\u793e\u4f1a\u504f\u89c1\u573a\u666f\u4e2d\u4e0e\u4eba\u7c7b\u4ef7\u503c\u89c2\u7684\u5bf9\u9f50\u5dee\u5f02\uff0c\u7279\u522b\u662f\u4e0d\u540c\u7c7b\u578b\u573a\u666f\uff08\u5982\u5305\u542b\u8d1f\u9762\u4e0e\u975e\u8d1f\u9762\u95ee\u9898\uff09\u4e0b\u7684\u5bf9\u9f50\u8868\u73b0\u3002", "method": "\u901a\u8fc7\u5bf94\u4e2a\u6a21\u578b\u5bb6\u65cf\u768412\u4e2aLLM\u57284\u4e2a\u6570\u636e\u96c6\u4e0a\u8fdb\u884c\u5e7f\u6cdb\u5206\u6790\uff0c\u8bc4\u4f30\u6a21\u578b\u7684\u5bf9\u9f50\u7387\u3001\u653b\u51fb\u6210\u529f\u7387\u3001\u5224\u65ad\u4e00\u81f4\u6027\uff0c\u5e76\u7814\u7a76\u6a21\u578b\u5bf9HVSB\u7684\u7406\u89e3\u80fd\u529b\u3002", "result": "\u5927\u53c2\u6570\u6a21\u578b\u4e0d\u4e00\u5b9a\u6709\u66f4\u4f4e\u7684\u5bf9\u9f50\u9519\u8bef\u7387\u548c\u653b\u51fb\u6210\u529f\u7387\uff1b\u6a21\u578b\u5bf9\u7279\u5b9a\u573a\u666f\u7c7b\u578b\u6709\u5bf9\u9f50\u504f\u597d\uff1b\u540c\u5bb6\u65cf\u6a21\u578b\u5224\u65ad\u4e00\u81f4\u6027\u66f4\u9ad8\uff1b\u6a21\u578b\u5bf9HVSB\u7406\u89e3\u65e0\u663e\u8457\u5dee\u5f02\uff1b\u6a21\u578b\u504f\u597d\u81ea\u8eab\u751f\u6210\u7684\u89e3\u91ca\u3002", "conclusion": "LLM\u4e0e\u4eba\u7c7b\u4ef7\u503c\u89c2\u7684\u5bf9\u9f50\u5b58\u5728\u573a\u666f\u7c7b\u578b\u4f9d\u8d56\u6027\uff0c\u6a21\u578b\u89c4\u6a21\u4e0d\u662f\u5bf9\u9f50\u8d28\u91cf\u7684\u4fdd\u8bc1\uff0c\u9700\u8981\u66f4\u7ec6\u7c92\u5ea6\u7684\u5bf9\u9f50\u8bc4\u4f30\u65b9\u6cd5\u3002", "relevance": 85.0}}
{"id": "2509.13508", "pdf": "https://arxiv.org/pdf/2509.13508", "abs": "https://arxiv.org/abs/2509.13508", "authors": ["Maksim Penkin", "Andrey Krylov"], "title": "FunKAN: Functional Kolmogorov-Arnold Network for Medical Image Enhancement and Segmentation", "categories": ["cs.CV", "I.4.3; I.4.6"], "comment": "9 pages, 5 figures, submitted to the Fortieth AAAI Conference on\n  Artificial Intelligence (AAAI-26)", "summary": "Medical image enhancement and segmentation are critical yet challenging tasks\nin modern clinical practice, constrained by artifacts and complex anatomical\nvariations. Traditional deep learning approaches often rely on complex\narchitectures with limited interpretability. While Kolmogorov-Arnold networks\noffer interpretable solutions, their reliance on flattened feature\nrepresentations fundamentally disrupts the intrinsic spatial structure of\nimaging data. To address this issue we propose a Functional Kolmogorov-Arnold\nNetwork (FunKAN) -- a novel interpretable neural framework, designed\nspecifically for image processing, that formally generalizes the\nKolmogorov-Arnold representation theorem onto functional spaces and learns\ninner functions using Fourier decomposition over the basis Hermite functions.\nWe explore FunKAN on several medical image processing tasks, including Gibbs\nringing suppression in magnetic resonance images, benchmarking on IXI dataset.\nWe also propose U-FunKAN as state-of-the-art binary medical segmentation model\nwith benchmarks on three medical datasets: BUSI (ultrasound images), GlaS\n(histological structures) and CVC-ClinicDB (colonoscopy videos), detecting\nbreast cancer, glands and polyps, respectively. Experiments on those diverse\ndatasets demonstrate that our approach outperforms other KAN-based backbones in\nboth medical image enhancement (PSNR, TV) and segmentation (IoU, F1). Our work\nbridges the gap between theoretical function approximation and medical image\nanalysis, offering a robust, interpretable solution for clinical applications.", "AI": {"tldr": "FunKAN\uff1a\u4e00\u79cd\u57fa\u4e8e\u51fd\u6570\u7a7a\u95f4Kolmogorov-Arnold\u8868\u793a\u5b9a\u7406\u7684\u53ef\u89e3\u91ca\u795e\u7ecf\u7f51\u7edc\u6846\u67b6\uff0c\u4e13\u95e8\u7528\u4e8e\u533b\u5b66\u56fe\u50cf\u5904\u7406\uff0c\u5728\u56fe\u50cf\u589e\u5f3a\u548c\u5206\u5272\u4efb\u52a1\u4e2d\u8868\u73b0\u4f18\u5f02", "motivation": "\u4f20\u7edf\u6df1\u5ea6\u5b66\u4e60\u65b9\u6cd5\u5728\u533b\u5b66\u56fe\u50cf\u5904\u7406\u4e2d\u7f3a\u4e4f\u53ef\u89e3\u91ca\u6027\uff0c\u800c\u73b0\u6709KAN\u7f51\u7edc\u4f1a\u7834\u574f\u56fe\u50cf\u7684\u7a7a\u95f4\u7ed3\u6784\u4fe1\u606f\uff0c\u9700\u8981\u4e00\u79cd\u65e2\u80fd\u4fdd\u6301\u53ef\u89e3\u91ca\u6027\u53c8\u80fd\u5904\u7406\u7a7a\u95f4\u7ed3\u6784\u7684\u65b9\u6cd5", "method": "\u63d0\u51faFunctional Kolmogorov-Arnold Network (FunKAN)\uff0c\u5c06Kolmogorov-Arnold\u8868\u793a\u5b9a\u7406\u63a8\u5e7f\u5230\u51fd\u6570\u7a7a\u95f4\uff0c\u4f7f\u7528\u5085\u91cc\u53f6\u5206\u89e3\u548cHermite\u57fa\u51fd\u6570\u5b66\u4e60\u5185\u90e8\u51fd\u6570", "result": "\u5728IXI\u6570\u636e\u96c6\u4e0a\u5b9e\u73b0Gibbs ringing\u6291\u5236\uff0c\u5728BUSI\u3001GlaS\u3001CVC-ClinicDB\u4e09\u4e2a\u533b\u5b66\u6570\u636e\u96c6\u4e0a\u5b9e\u73b0\u6700\u5148\u8fdb\u7684\u4e8c\u503c\u5206\u5272\uff0c\u5728PSNR\u3001TV\u3001IoU\u3001F1\u7b49\u6307\u6807\u4e0a\u4f18\u4e8e\u5176\u4ed6KAN\u65b9\u6cd5", "conclusion": "FunKAN\u6210\u529f\u5c06\u7406\u8bba\u51fd\u6570\u903c\u8fd1\u4e0e\u533b\u5b66\u56fe\u50cf\u5206\u6790\u76f8\u7ed3\u5408\uff0c\u4e3a\u4e34\u5e8a\u5e94\u7528\u63d0\u4f9b\u4e86\u9c81\u68d2\u4e14\u53ef\u89e3\u91ca\u7684\u89e3\u51b3\u65b9\u6848", "relevance": 35.0}}
{"id": "2509.13735", "pdf": "https://arxiv.org/pdf/2509.13735", "abs": "https://arxiv.org/abs/2509.13735", "authors": ["Junzhi She", "Xunkai Li", "Rong-Hua Li", "Guoren Wang"], "title": "State Space Models over Directed Graphs", "categories": ["cs.LG", "cs.AI"], "comment": "currently undergoing review by IEEE Transactions on Big Data", "summary": "Directed graphs are ubiquitous across numerous domains, where the\ndirectionality of edges encodes critical causal dependencies. However, existing\nGNNs and graph Transformers tailored for directed graphs face two major\nchallenges: (1) effectively capturing long-range causal dependencies derived\nfrom directed edges; (2) balancing accuracy and training efficiency when\nprocessing large-scale graph datasets. In recent years, state space models\n(SSMs) have achieved substantial progress in causal sequence tasks, and their\nvariants designed for graphs have demonstrated state-of-the-art accuracy while\nmaintaining high efficiency across various graph learning benchmarks. However,\nexisting graph state space models are exclusively designed for undirected\ngraphs, which limits their performance in directed graph learning. To this end,\nwe propose an innovative approach DirEgo2Token which sequentializes directed\ngraphs via k-hop ego graphs. This marks the first systematic extension of state\nspace models to the field of directed graph learning. Building upon this, we\ndevelop DirGraphSSM, a novel directed graph neural network architecture that\nimplements state space models on directed graphs via the message-passing\nmechanism. Experimental results demonstrate that DirGraphSSM achieves\nstate-of-the-art performance on three representative directed graph learning\ntasks while attaining competitive performance on two additional tasks with\n1.5$\\times $ to 2$\\times $ training speed improvements compared to existing\nstate-of-the-art models.", "AI": {"tldr": "\u63d0\u51fa\u4e86DirGraphSSM\uff0c\u9996\u4e2a\u5c06\u72b6\u6001\u7a7a\u95f4\u6a21\u578b\u7cfb\u7edf\u6269\u5c55\u5230\u6709\u5411\u56fe\u5b66\u4e60\u7684\u67b6\u6784\uff0c\u901a\u8fc7k-hop ego\u56fe\u5e8f\u5217\u5316\u548c\u6d88\u606f\u4f20\u9012\u673a\u5236\uff0c\u5728\u4fdd\u6301\u9ad8\u6548\u8bad\u7ec3\u7684\u540c\u65f6\u5b9e\u73b0\u4e86SOTA\u6027\u80fd", "motivation": "\u73b0\u6709\u56fe\u795e\u7ecf\u7f51\u7edc\u548c\u56feTransformer\u5728\u5904\u7406\u6709\u5411\u56fe\u65f6\u9762\u4e34\u4e24\u5927\u6311\u6218\uff1a\u6709\u6548\u6355\u6349\u957f\u8ddd\u79bb\u56e0\u679c\u4f9d\u8d56\u5173\u7cfb\uff0c\u4ee5\u53ca\u5728\u5904\u7406\u5927\u89c4\u6a21\u56fe\u6570\u636e\u96c6\u65f6\u5e73\u8861\u51c6\u786e\u6027\u548c\u8bad\u7ec3\u6548\u7387\u3002\u73b0\u6709\u56fe\u72b6\u6001\u7a7a\u95f4\u6a21\u578b\u4ec5\u9002\u7528\u4e8e\u65e0\u5411\u56fe\uff0c\u9650\u5236\u4e86\u5176\u6027\u80fd", "method": "\u63d0\u51faDirEgo2Token\u65b9\u6cd5\u901a\u8fc7k-hop ego\u56fe\u5c06\u6709\u5411\u56fe\u5e8f\u5217\u5316\uff0c\u5e76\u5728\u6b64\u57fa\u7840\u4e0a\u5f00\u53d1DirGraphSSM\u67b6\u6784\uff0c\u901a\u8fc7\u6d88\u606f\u4f20\u9012\u673a\u5236\u5728\u6709\u5411\u56fe\u4e0a\u5b9e\u73b0\u72b6\u6001\u7a7a\u95f4\u6a21\u578b", "result": "\u5728\u4e09\u4e2a\u4ee3\u8868\u6027\u6709\u5411\u56fe\u5b66\u4e60\u4efb\u52a1\u4e0a\u8fbe\u5230SOTA\u6027\u80fd\uff0c\u5728\u53e6\u5916\u4e24\u4e2a\u4efb\u52a1\u4e0a\u83b7\u5f97\u7ade\u4e89\u6027\u6027\u80fd\uff0c\u8bad\u7ec3\u901f\u5ea6\u6bd4\u73b0\u6709SOTA\u6a21\u578b\u63d0\u53471.5-2\u500d", "conclusion": "DirGraphSSM\u6210\u529f\u5c06\u72b6\u6001\u7a7a\u95f4\u6a21\u578b\u6269\u5c55\u5230\u6709\u5411\u56fe\u5b66\u4e60\u9886\u57df\uff0c\u5728\u4fdd\u6301\u9ad8\u6548\u7387\u7684\u540c\u65f6\u663e\u8457\u63d0\u5347\u4e86\u6027\u80fd\uff0c\u4e3a\u6709\u5411\u56fe\u5b66\u4e60\u63d0\u4f9b\u4e86\u65b0\u7684\u89e3\u51b3\u65b9\u6848", "relevance": 75.0}}
{"id": "2509.13615", "pdf": "https://arxiv.org/pdf/2509.13615", "abs": "https://arxiv.org/abs/2509.13615", "authors": ["Zongru Wu", "Rui Mao", "Zhiyuan Tian", "Pengzhou Cheng", "Tianjie Ju", "Zheng Wu", "Lingzhong Dong", "Haiyue Sheng", "Zhuosheng Zhang", "Gongshen Liu"], "title": "See, Think, Act: Teaching Multimodal Agents to Effectively Interact with GUI by Identifying Toggles", "categories": ["cs.AI", "cs.CL", "cs.HC"], "comment": null, "summary": "The advent of multimodal agents facilitates effective interaction within\ngraphical user interface (GUI), especially in ubiquitous GUI control. However,\ntheir inability to reliably execute toggle control instructions remains a key\nbottleneck. To investigate this, we construct a state control benchmark with\nbinary toggle instructions from public datasets. Evaluations of existing agents\ndemonstrate their unreliability, particularly when the current toggle state\nalready matches the desired state. To address the challenge, we propose\nState-aware Reasoning (StaR), a training method that teaches agents to perceive\nthe current toggle state, analyze the desired state from the instruction, and\nact accordingly. Experiments on three multimodal agents demonstrate that StaR\ncan improve toggle instruction execution accuracy by over 30\\%. Further\nevaluations on three public benchmarks show that StaR also enhances general\ntask performance. Finally, evaluations on a dynamic environment highlight the\npotential of StaR for real-world applications. Code, benchmark, and\nStaR-enhanced agents are available at https://github.com/ZrW00/StaR.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86State-aware Reasoning (StaR)\u8bad\u7ec3\u65b9\u6cd5\uff0c\u89e3\u51b3\u591a\u6a21\u6001\u4ee3\u7406\u5728GUI\u5207\u6362\u63a7\u5236\u6307\u4ee4\u6267\u884c\u4e0d\u53ef\u9760\u7684\u95ee\u9898\uff0c\u7279\u522b\u662f\u5728\u5f53\u524d\u72b6\u6001\u4e0e\u671f\u671b\u72b6\u6001\u5339\u914d\u65f6\u7684\u9519\u8bef\u6267\u884c\u3002", "motivation": "\u591a\u6a21\u6001\u4ee3\u7406\u5728\u56fe\u5f62\u7528\u6237\u754c\u9762(GUI)\u63a7\u5236\u4e2d\u6267\u884c\u5207\u6362(toggle)\u6307\u4ee4\u65f6\u5b58\u5728\u4e0d\u53ef\u9760\u6027\uff0c\u7279\u522b\u662f\u5728\u5f53\u524d\u72b6\u6001\u5df2\u7ecf\u7b26\u5408\u671f\u671b\u72b6\u6001\u65f6\u5bb9\u6613\u51fa\u9519\uff0c\u8fd9\u9650\u5236\u4e86\u5176\u5728\u5b9e\u9645\u5e94\u7528\u4e2d\u7684\u6548\u679c\u3002", "method": "\u6784\u5efa\u4e86\u72b6\u6001\u63a7\u5236\u57fa\u51c6\u6d4b\u8bd5\uff0c\u63d0\u51faStaR\u8bad\u7ec3\u65b9\u6cd5\uff0c\u6559\u5bfc\u4ee3\u7406\u611f\u77e5\u5f53\u524d\u5207\u6362\u72b6\u6001\u3001\u5206\u6790\u6307\u4ee4\u4e2d\u7684\u671f\u671b\u72b6\u6001\uff0c\u5e76\u76f8\u5e94\u91c7\u53d6\u884c\u52a8\u3002", "result": "\u5728\u4e09\u4e2a\u591a\u6a21\u6001\u4ee3\u7406\u4e0a\u7684\u5b9e\u9a8c\u663e\u793a\uff0cStaR\u80fd\u5c06\u5207\u6362\u6307\u4ee4\u6267\u884c\u51c6\u786e\u7387\u63d0\u534730%\u4ee5\u4e0a\uff0c\u5728\u4e09\u4e2a\u516c\u5171\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u4e5f\u63d0\u5347\u4e86\u901a\u7528\u4efb\u52a1\u6027\u80fd\u3002", "conclusion": "StaR\u65b9\u6cd5\u6709\u6548\u89e3\u51b3\u4e86GUI\u5207\u6362\u63a7\u5236\u95ee\u9898\uff0c\u5728\u52a8\u6001\u73af\u5883\u4e2d\u663e\u793a\u51fa\u5b9e\u9645\u5e94\u7528\u6f5c\u529b\uff0c\u4e3a\u591a\u6a21\u6001\u4ee3\u7406\u7684\u53ef\u9760\u4ea4\u4e92\u63d0\u4f9b\u4e86\u89e3\u51b3\u65b9\u6848\u3002", "relevance": 35.0}}
{"id": "2509.13879", "pdf": "https://arxiv.org/pdf/2509.13879", "abs": "https://arxiv.org/abs/2509.13879", "authors": ["Mariano Barone", "Antonio Romano", "Giuseppe Riccio", "Marco Postiglione", "Vincenzo Moscato"], "title": "Combining Evidence and Reasoning for Biomedical Fact-Checking", "categories": ["cs.CL", "cs.AI", "cs.IR"], "comment": "Proceedings of the 48th International ACM SIGIR Conference on\n  Research and Development in Information Retrieval, 2025", "summary": "Misinformation in healthcare, from vaccine hesitancy to unproven treatments,\nposes risks to public health and trust in medical systems. While machine\nlearning and natural language processing have advanced automated fact-checking,\nvalidating biomedical claims remains uniquely challenging due to complex\nterminology, the need for domain expertise, and the critical importance of\ngrounding in scientific evidence. We introduce CER (Combining Evidence and\nReasoning), a novel framework for biomedical fact-checking that integrates\nscientific evidence retrieval, reasoning via large language models, and\nsupervised veracity prediction. By integrating the text-generation capabilities\nof large language models with advanced retrieval techniques for high-quality\nbiomedical scientific evidence, CER effectively mitigates the risk of\nhallucinations, ensuring that generated outputs are grounded in verifiable,\nevidence-based sources. Evaluations on expert-annotated datasets (HealthFC,\nBioASQ-7b, SciFact) demonstrate state-of-the-art performance and promising\ncross-dataset generalization. Code and data are released for transparency and\nreproducibility: https: //github.com/PRAISELab-PicusLab/CER.", "AI": {"tldr": "CER\u662f\u4e00\u4e2a\u7528\u4e8e\u751f\u7269\u533b\u5b66\u4e8b\u5b9e\u6838\u67e5\u7684\u65b0\u6846\u67b6\uff0c\u7ed3\u5408\u79d1\u5b66\u8bc1\u636e\u68c0\u7d22\u3001\u5927\u8bed\u8a00\u6a21\u578b\u63a8\u7406\u548c\u76d1\u7763\u771f\u5b9e\u6027\u9884\u6d4b\uff0c\u5728\u591a\u4e2a\u6570\u636e\u96c6\u4e0a\u8fbe\u5230\u6700\u5148\u8fdb\u6027\u80fd\u3002", "motivation": "\u533b\u7597\u9886\u57df\u7684\u9519\u8bef\u4fe1\u606f\uff08\u5982\u75ab\u82d7\u72b9\u8c6b\u548c\u672a\u7ecf\u8bc1\u5b9e\u7684\u6cbb\u7597\u65b9\u6cd5\uff09\u5bf9\u516c\u5171\u536b\u751f\u548c\u533b\u7597\u7cfb\u7edf\u4fe1\u4efb\u6784\u6210\u98ce\u9669\u3002\u751f\u7269\u533b\u5b66\u58f0\u660e\u9a8c\u8bc1\u5177\u6709\u72ec\u7279\u6311\u6218\u6027\uff0c\u9700\u8981\u590d\u6742\u672f\u8bed\u5904\u7406\u3001\u9886\u57df\u4e13\u4e1a\u77e5\u8bc6\u4ee5\u53ca\u57fa\u4e8e\u79d1\u5b66\u8bc1\u636e\u7684\u9a8c\u8bc1\u3002", "method": "CER\u6846\u67b6\u6574\u5408\u4e86\u79d1\u5b66\u8bc1\u636e\u68c0\u7d22\u3001\u5927\u8bed\u8a00\u6a21\u578b\u63a8\u7406\u548c\u76d1\u7763\u771f\u5b9e\u6027\u9884\u6d4b\u3002\u901a\u8fc7\u5c06\u5927\u8bed\u8a00\u6a21\u578b\u7684\u6587\u672c\u751f\u6210\u80fd\u529b\u4e0e\u9ad8\u8d28\u91cf\u751f\u7269\u533b\u5b66\u79d1\u5b66\u8bc1\u636e\u7684\u5148\u8fdb\u68c0\u7d22\u6280\u672f\u76f8\u7ed3\u5408\uff0c\u6709\u6548\u51cf\u8f7b\u5e7b\u89c9\u98ce\u9669\u3002", "result": "\u5728\u4e13\u5bb6\u6807\u6ce8\u7684\u6570\u636e\u96c6\uff08HealthFC\u3001BioASQ-7b\u3001SciFact\uff09\u4e0a\u8bc4\u4f30\u663e\u793a\u8fbe\u5230\u4e86\u6700\u5148\u8fdb\u7684\u6027\u80fd\uff0c\u5e76\u5c55\u73b0\u51fa\u6709\u524d\u666f\u7684\u8de8\u6570\u636e\u96c6\u6cdb\u5316\u80fd\u529b\u3002", "conclusion": "CER\u6846\u67b6\u901a\u8fc7\u7ed3\u5408\u8bc1\u636e\u68c0\u7d22\u548cLLM\u63a8\u7406\uff0c\u4e3a\u751f\u7269\u533b\u5b66\u4e8b\u5b9e\u6838\u67e5\u63d0\u4f9b\u4e86\u6709\u6548\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u4ee3\u7801\u548c\u6570\u636e\u5df2\u5f00\u6e90\u4ee5\u786e\u4fdd\u900f\u660e\u6027\u548c\u53ef\u91cd\u73b0\u6027\u3002", "relevance": 65.0}}
{"id": "2509.13515", "pdf": "https://arxiv.org/pdf/2509.13515", "abs": "https://arxiv.org/abs/2509.13515", "authors": ["Jiangbei Yue", "Shuonan Yang", "Tailin Chen", "Jianbo Jiao", "Zeyu Fu"], "title": "Multimodal Hate Detection Using Dual-Stream Graph Neural Networks", "categories": ["cs.CV"], "comment": null, "summary": "Hateful videos present serious risks to online safety and real-world\nwell-being, necessitating effective detection methods. Although multimodal\nclassification approaches integrating information from several modalities\noutperform unimodal ones, they typically neglect that even minimal hateful\ncontent defines a video's category. Specifically, they generally treat all\ncontent uniformly, instead of emphasizing the hateful components. Additionally,\nexisting multimodal methods cannot systematically capture structured\ninformation in videos, limiting the effectiveness of multimodal fusion. To\naddress these limitations, we propose a novel multimodal dual-stream graph\nneural network model. It constructs an instance graph by separating the given\nvideo into several instances to extract instance-level features. Then, a\ncomplementary weight graph assigns importance weights to these features,\nhighlighting hateful instances. Importance weights and instance features are\ncombined to generate video labels. Our model employs a graph-based framework to\nsystematically model structured relationships within and across modalities.\nExtensive experiments on public datasets show that our model is\nstate-of-the-art in hateful video classification and has strong explainability.\nCode is available:\nhttps://github.com/Multimodal-Intelligence-Lab-MIL/MultiHateGNN.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u9896\u7684\u591a\u6a21\u6001\u53cc\u6d41\u56fe\u795e\u7ecf\u7f51\u7edc\u6a21\u578b\uff0c\u7528\u4e8e\u4ec7\u6068\u89c6\u9891\u68c0\u6d4b\uff0c\u901a\u8fc7\u5206\u79bb\u89c6\u9891\u5b9e\u4f8b\u5e76\u5206\u914d\u91cd\u8981\u6027\u6743\u91cd\u6765\u7a81\u51fa\u4ec7\u6068\u5185\u5bb9\uff0c\u5728\u516c\u5f00\u6570\u636e\u96c6\u4e0a\u8fbe\u5230\u6700\u5148\u8fdb\u6027\u80fd\u3002", "motivation": "\u73b0\u6709\u591a\u6a21\u6001\u5206\u7c7b\u65b9\u6cd5\u901a\u5e38\u5ffd\u89c6\u4ec7\u6068\u5185\u5bb9\u7684\u5173\u952e\u6027\uff0c\u5bf9\u6240\u6709\u5185\u5bb9\u4e00\u89c6\u540c\u4ec1\uff0c\u4e14\u65e0\u6cd5\u7cfb\u7edf\u6355\u6349\u89c6\u9891\u4e2d\u7684\u7ed3\u6784\u5316\u4fe1\u606f\uff0c\u9650\u5236\u4e86\u591a\u6a21\u6001\u878d\u5408\u6548\u679c\u3002", "method": "\u6784\u5efa\u5b9e\u4f8b\u56fe\u5206\u79bb\u89c6\u9891\u4e3a\u591a\u4e2a\u5b9e\u4f8b\u63d0\u53d6\u7279\u5f81\uff0c\u901a\u8fc7\u4e92\u8865\u6743\u91cd\u56fe\u4e3a\u7279\u5f81\u5206\u914d\u91cd\u8981\u6027\u6743\u91cd\u4ee5\u7a81\u51fa\u4ec7\u6068\u5b9e\u4f8b\uff0c\u7ed3\u5408\u6743\u91cd\u548c\u7279\u5f81\u751f\u6210\u89c6\u9891\u6807\u7b7e\uff0c\u4f7f\u7528\u56fe\u6846\u67b6\u7cfb\u7edf\u5efa\u6a21\u6a21\u6001\u5185\u548c\u8de8\u6a21\u6001\u7684\u7ed3\u6784\u5173\u7cfb\u3002", "result": "\u5728\u516c\u5f00\u6570\u636e\u96c6\u4e0a\u7684\u5e7f\u6cdb\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u6a21\u578b\u5728\u4ec7\u6068\u89c6\u9891\u5206\u7c7b\u65b9\u9762\u8fbe\u5230\u6700\u5148\u8fdb\u6c34\u5e73\uff0c\u5e76\u5177\u6709\u5f88\u5f3a\u7684\u53ef\u89e3\u91ca\u6027\u3002", "conclusion": "\u63d0\u51fa\u7684\u591a\u6a21\u6001\u53cc\u6d41\u56fe\u795e\u7ecf\u7f51\u7edc\u6a21\u578b\u6709\u6548\u89e3\u51b3\u4e86\u4ec7\u6068\u89c6\u9891\u68c0\u6d4b\u4e2d\u7684\u5173\u952e\u95ee\u9898\uff0c\u901a\u8fc7\u5f3a\u8c03\u4ec7\u6068\u5185\u5bb9\u548c\u7cfb\u7edf\u5efa\u6a21\u7ed3\u6784\u5316\u5173\u7cfb\uff0c\u5b9e\u73b0\u4e86\u4f18\u8d8a\u7684\u5206\u7c7b\u6027\u80fd\u548c\u53ef\u89e3\u91ca\u6027\u3002", "relevance": 45.0}}
{"id": "2509.13739", "pdf": "https://arxiv.org/pdf/2509.13739", "abs": "https://arxiv.org/abs/2509.13739", "authors": ["Zihou Wu", "Yuecheng Li", "Tianchi Liao", "Jian Lou", "Chuan Chen"], "title": "ParaAegis: Parallel Protection for Flexible Privacy-preserved Federated Learning", "categories": ["cs.LG", "cs.DC"], "comment": "8 pages, 1 figure", "summary": "Federated learning (FL) faces a critical dilemma: existing protection\nmechanisms like differential privacy (DP) and homomorphic encryption (HE)\nenforce a rigid trade-off, forcing a choice between model utility and\ncomputational efficiency. This lack of flexibility hinders the practical\nimplementation. To address this, we introduce ParaAegis, a parallel protection\nframework designed to give practitioners flexible control over the\nprivacy-utility-efficiency balance. Our core innovation is a strategic model\npartitioning scheme. By applying lightweight DP to the less critical, low norm\nportion of the model while protecting the remainder with HE, we create a\ntunable system. A distributed voting mechanism ensures consensus on this\npartitioning. Theoretical analysis confirms the adjustments between efficiency\nand utility with the same privacy. Crucially, the experimental results\ndemonstrate that by adjusting the hyperparameters, our method enables flexible\nprioritization between model accuracy and training time.", "AI": {"tldr": "ParaAegis\u662f\u4e00\u4e2a\u8054\u90a6\u5b66\u4e60\u5e76\u884c\u4fdd\u62a4\u6846\u67b6\uff0c\u901a\u8fc7\u6a21\u578b\u5206\u533a\u7b56\u7565\u5728DP\u548cHE\u4e4b\u95f4\u5b9e\u73b0\u9690\u79c1-\u6548\u7528-\u6548\u7387\u7684\u7075\u6d3b\u6743\u8861\u63a7\u5236", "motivation": "\u89e3\u51b3\u8054\u90a6\u5b66\u4e60\u4e2d\u73b0\u6709\u4fdd\u62a4\u673a\u5236\uff08\u5982\u5dee\u5206\u9690\u79c1\u548c\u540c\u6001\u52a0\u5bc6\uff09\u5728\u6a21\u578b\u6548\u7528\u548c\u8ba1\u7b97\u6548\u7387\u4e4b\u95f4\u5b58\u5728\u7684\u521a\u6027\u6743\u8861\u95ee\u9898\uff0c\u63d0\u4f9b\u66f4\u7075\u6d3b\u5b9e\u7528\u7684\u4fdd\u62a4\u65b9\u6848", "method": "\u63d0\u51fa\u5e76\u884c\u4fdd\u62a4\u6846\u67b6\uff0c\u901a\u8fc7\u7b56\u7565\u6027\u6a21\u578b\u5206\u533a\uff1a\u5bf9\u4f4e\u8303\u6570\u90e8\u5206\u5e94\u7528\u8f7b\u91cf\u7ea7DP\uff0c\u5bf9\u5269\u4f59\u90e8\u5206\u4f7f\u7528HE\u4fdd\u62a4\uff0c\u5e76\u901a\u8fc7\u5206\u5e03\u5f0f\u6295\u7968\u673a\u5236\u786e\u4fdd\u5206\u533a\u5171\u8bc6", "result": "\u7406\u8bba\u5206\u6790\u786e\u8ba4\u4e86\u5728\u76f8\u540c\u9690\u79c1\u4fdd\u62a4\u4e0b\u6548\u7387\u4e0e\u6548\u7528\u7684\u53ef\u8c03\u8282\u6027\uff0c\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\u901a\u8fc7\u8c03\u6574\u8d85\u53c2\u6570\u53ef\u4ee5\u7075\u6d3b\u4f18\u5148\u8003\u8651\u6a21\u578b\u7cbe\u5ea6\u6216\u8bad\u7ec3\u65f6\u95f4", "conclusion": "ParaAegis\u4e3a\u8054\u90a6\u5b66\u4e60\u63d0\u4f9b\u4e86\u7075\u6d3b\u7684\u9690\u79c1-\u6548\u7528-\u6548\u7387\u5e73\u8861\u63a7\u5236\uff0c\u89e3\u51b3\u4e86\u73b0\u6709\u4fdd\u62a4\u673a\u5236\u7684\u521a\u6027\u6743\u8861\u9650\u5236", "relevance": 30.0}}
{"id": "2509.13704", "pdf": "https://arxiv.org/pdf/2509.13704", "abs": "https://arxiv.org/abs/2509.13704", "authors": ["Liangtao Lin", "Zhaomeng Zhu", "Tianwei Zhang", "Yonggang Wen"], "title": "InfraMind: A Novel Exploration-based GUI Agentic Framework for Mission-critical Industrial Management", "categories": ["cs.AI", "cs.SE"], "comment": null, "summary": "Mission-critical industrial infrastructure, such as data centers,\nincreasingly depends on complex management software. Its operations, however,\npose significant challenges due to the escalating system complexity,\nmulti-vendor integration, and a shortage of expert operators. While Robotic\nProcess Automation (RPA) offers partial automation through handcrafted scripts,\nit suffers from limited flexibility and high maintenance costs. Recent advances\nin Large Language Model (LLM)-based graphical user interface (GUI) agents have\nenabled more flexible automation, yet these general-purpose agents face five\ncritical challenges when applied to industrial management, including unfamiliar\nelement understanding, precision and efficiency, state localization, deployment\nconstraints, and safety requirements. To address these issues, we propose\nInfraMind, a novel exploration-based GUI agentic framework specifically\ntailored for industrial management systems. InfraMind integrates five\ninnovative modules to systematically resolve different challenges in industrial\nmanagement: (1) systematic search-based exploration with virtual machine\nsnapshots for autonomous understanding of complex GUIs; (2) memory-driven\nplanning to ensure high-precision and efficient task execution; (3) advanced\nstate identification for robust localization in hierarchical interfaces; (4)\nstructured knowledge distillation for efficient deployment with lightweight\nmodels; and (5) comprehensive, multi-layered safety mechanisms to safeguard\nsensitive operations. Extensive experiments on both open-source and commercial\nDCIM platforms demonstrate that our approach consistently outperforms existing\nframeworks in terms of task success rate and operational efficiency, providing\na rigorous and scalable solution for industrial management automation.", "AI": {"tldr": "InfraMind\u662f\u4e00\u4e2a\u4e13\u95e8\u4e3a\u5de5\u4e1a\u7ba1\u7406\u7cfb\u7edf\u8bbe\u8ba1\u7684\u57fa\u4e8e\u63a2\u7d22\u7684GUI\u4ee3\u7406\u6846\u67b6\uff0c\u901a\u8fc7\u4e94\u4e2a\u521b\u65b0\u6a21\u5757\u89e3\u51b3LLM\u5728\u5de5\u4e1a\u7ba1\u7406\u5e94\u7528\u4e2d\u7684\u6311\u6218\uff0c\u5728DCIM\u5e73\u53f0\u4e0a\u8868\u73b0\u51fa\u4f18\u8d8a\u7684\u4efb\u52a1\u6210\u529f\u7387\u548c\u64cd\u4f5c\u6548\u7387\u3002", "motivation": "\u5de5\u4e1a\u57fa\u7840\u8bbe\u65bd\u7ba1\u7406\u9762\u4e34\u7cfb\u7edf\u590d\u6742\u6027\u589e\u52a0\u3001\u591a\u4f9b\u5e94\u5546\u96c6\u6210\u548c\u4e13\u5bb6\u64cd\u4f5c\u5458\u77ed\u7f3a\u7b49\u6311\u6218\u3002\u73b0\u6709RPA\u65b9\u6cd5\u7075\u6d3b\u6027\u6709\u9650\u4e14\u7ef4\u62a4\u6210\u672c\u9ad8\uff0c\u901a\u7528LLM GUI\u4ee3\u7406\u5728\u5de5\u4e1a\u7ba1\u7406\u4e2d\u5b58\u5728\u5143\u7d20\u7406\u89e3\u3001\u7cbe\u5ea6\u6548\u7387\u3001\u72b6\u6001\u5b9a\u4f4d\u3001\u90e8\u7f72\u7ea6\u675f\u548c\u5b89\u5168\u8981\u6c42\u7b49\u4e94\u5927\u6311\u6218\u3002", "method": "\u63d0\u51faInfraMind\u6846\u67b6\uff0c\u5305\u542b\u4e94\u4e2a\u6838\u5fc3\u6a21\u5757\uff1a1)\u57fa\u4e8e\u7cfb\u7edf\u641c\u7d22\u63a2\u7d22\u548c\u865a\u62df\u673a\u5feb\u7167\u7684\u81ea\u4e3bGUI\u7406\u89e3\uff1b2)\u5185\u5b58\u9a71\u52a8\u89c4\u5212\u786e\u4fdd\u9ad8\u7cbe\u5ea6\u9ad8\u6548\u4efb\u52a1\u6267\u884c\uff1b3)\u9ad8\u7ea7\u72b6\u6001\u8bc6\u522b\u7528\u4e8e\u5c42\u6b21\u5316\u754c\u9762\u4e2d\u7684\u9c81\u68d2\u5b9a\u4f4d\uff1b4)\u7ed3\u6784\u5316\u77e5\u8bc6\u84b8\u998f\u5b9e\u73b0\u8f7b\u91cf\u6a21\u578b\u9ad8\u6548\u90e8\u7f72\uff1b5)\u591a\u5c42\u5b89\u5168\u673a\u5236\u4fdd\u62a4\u654f\u611f\u64cd\u4f5c\u3002", "result": "\u5728\u5f00\u6e90\u548c\u5546\u4e1aDCIM\u5e73\u53f0\u4e0a\u7684\u5e7f\u6cdb\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u5728\u4efb\u52a1\u6210\u529f\u7387\u548c\u64cd\u4f5c\u6548\u7387\u65b9\u9762\u6301\u7eed\u4f18\u4e8e\u73b0\u6709\u6846\u67b6\u3002", "conclusion": "InfraMind\u4e3a\u5de5\u4e1a\u7ba1\u7406\u81ea\u52a8\u5316\u63d0\u4f9b\u4e86\u4e00\u4e2a\u4e25\u8c28\u4e14\u53ef\u6269\u5c55\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u6709\u6548\u89e3\u51b3\u4e86LLM\u5728\u5de5\u4e1a\u73af\u5883\u5e94\u7528\u4e2d\u7684\u5173\u952e\u6311\u6218\u3002", "relevance": 75.0}}
{"id": "2509.13888", "pdf": "https://arxiv.org/pdf/2509.13888", "abs": "https://arxiv.org/abs/2509.13888", "authors": ["Mariano Barone", "Antonio Romano", "Giuseppe Riccio", "Marco Postiglione", "Vincenzo Moscato"], "title": "Combating Biomedical Misinformation through Multi-modal Claim Detection and Evidence-based Verification", "categories": ["cs.CL", "cs.AI", "cs.IR"], "comment": null, "summary": "Misinformation in healthcare, from vaccine hesitancy to unproven treatments,\nposes risks to public health and trust in medical systems. While machine\nlearning and natural language processing have advanced automated fact-checking,\nvalidating biomedical claims remains uniquely challenging due to complex\nterminology, the need for domain expertise, and the critical importance of\ngrounding in scientific evidence. We introduce CER (Combining Evidence and\nReasoning), a novel framework for biomedical fact-checking that integrates\nscientific evidence retrieval, reasoning via large language models, and\nsupervised veracity prediction. By integrating the text-generation capabilities\nof large language models with advanced retrieval techniques for high-quality\nbiomedical scientific evidence, CER effectively mitigates the risk of\nhallucinations, ensuring that generated outputs are grounded in verifiable,\nevidence-based sources. Evaluations on expert-annotated datasets (HealthFC,\nBioASQ-7b, SciFact) demonstrate state-of-the-art performance and promising\ncross-dataset generalization. Code and data are released for transparency and\nreproducibility: https://github.com/PRAISELab-PicusLab/CER", "AI": {"tldr": "CER\u662f\u4e00\u4e2a\u7528\u4e8e\u751f\u7269\u533b\u5b66\u4e8b\u5b9e\u6838\u67e5\u7684\u65b0\u6846\u67b6\uff0c\u7ed3\u5408\u4e86\u79d1\u5b66\u8bc1\u636e\u68c0\u7d22\u3001\u5927\u8bed\u8a00\u6a21\u578b\u63a8\u7406\u548c\u76d1\u7763\u771f\u5b9e\u6027\u9884\u6d4b\uff0c\u5728\u591a\u4e2a\u6570\u636e\u96c6\u4e0a\u8fbe\u5230\u6700\u5148\u8fdb\u6027\u80fd\u3002", "motivation": "\u533b\u7597\u9886\u57df\u7684\u9519\u8bef\u4fe1\u606f\uff08\u5982\u75ab\u82d7\u72b9\u8c6b\u548c\u672a\u7ecf\u8bc1\u5b9e\u7684\u6cbb\u7597\u65b9\u6cd5\uff09\u5bf9\u516c\u5171\u5065\u5eb7\u6784\u6210\u98ce\u9669\uff0c\u4f46\u751f\u7269\u533b\u5b66\u58f0\u660e\u9a8c\u8bc1\u5177\u6709\u72ec\u7279\u6311\u6218\u6027\uff0c\u9700\u8981\u590d\u6742\u7684\u672f\u8bed\u5904\u7406\u3001\u9886\u57df\u4e13\u4e1a\u77e5\u8bc6\u4ee5\u53ca\u57fa\u4e8e\u79d1\u5b66\u8bc1\u636e\u7684\u9a8c\u8bc1\u3002", "method": "CER\u6846\u67b6\u6574\u5408\u4e86\u79d1\u5b66\u8bc1\u636e\u68c0\u7d22\u3001\u5927\u8bed\u8a00\u6a21\u578b\u63a8\u7406\u548c\u76d1\u7763\u771f\u5b9e\u6027\u9884\u6d4b\u3002\u901a\u8fc7\u5c06\u5927\u8bed\u8a00\u6a21\u578b\u7684\u6587\u672c\u751f\u6210\u80fd\u529b\u4e0e\u9ad8\u8d28\u91cf\u751f\u7269\u533b\u5b66\u79d1\u5b66\u8bc1\u636e\u7684\u68c0\u7d22\u6280\u672f\u76f8\u7ed3\u5408\uff0c\u6709\u6548\u51cf\u5c11\u5e7b\u89c9\u98ce\u9669\u3002", "result": "\u5728\u4e13\u5bb6\u6807\u6ce8\u7684\u6570\u636e\u96c6\uff08HealthFC\u3001BioASQ-7b\u3001SciFact\uff09\u4e0a\u5c55\u793a\u4e86\u6700\u5148\u8fdb\u7684\u6027\u80fd\uff0c\u5e76\u663e\u793a\u51fa\u826f\u597d\u7684\u8de8\u6570\u636e\u96c6\u6cdb\u5316\u80fd\u529b\u3002", "conclusion": "CER\u6846\u67b6\u901a\u8fc7\u7ed3\u5408\u8bc1\u636e\u68c0\u7d22\u548cLLM\u63a8\u7406\uff0c\u4e3a\u751f\u7269\u533b\u5b66\u4e8b\u5b9e\u6838\u67e5\u63d0\u4f9b\u4e86\u6709\u6548\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u4ee3\u7801\u548c\u6570\u636e\u5df2\u5f00\u6e90\u4ee5\u786e\u4fdd\u900f\u660e\u5ea6\u548c\u53ef\u590d\u73b0\u6027\u3002", "relevance": 65.0}}
{"id": "2509.13525", "pdf": "https://arxiv.org/pdf/2509.13525", "abs": "https://arxiv.org/abs/2509.13525", "authors": ["Romain Hardy", "Tyler Berzin", "Pranav Rajpurkar"], "title": "ColonCrafter: A Depth Estimation Model for Colonoscopy Videos Using Diffusion Priors", "categories": ["cs.CV", "cs.AI", "cs.LG"], "comment": "12 pages, 8 figures", "summary": "Three-dimensional (3D) scene understanding in colonoscopy presents\nsignificant challenges that necessitate automated methods for accurate depth\nestimation. However, existing depth estimation models for endoscopy struggle\nwith temporal consistency across video sequences, limiting their applicability\nfor 3D reconstruction. We present ColonCrafter, a diffusion-based depth\nestimation model that generates temporally consistent depth maps from monocular\ncolonoscopy videos. Our approach learns robust geometric priors from synthetic\ncolonoscopy sequences to generate temporally consistent depth maps. We also\nintroduce a style transfer technique that preserves geometric structure while\nadapting real clinical videos to match our synthetic training domain.\nColonCrafter achieves state-of-the-art zero-shot performance on the C3VD\ndataset, outperforming both general-purpose and endoscopy-specific approaches.\nAlthough full trajectory 3D reconstruction remains a challenge, we demonstrate\nclinically relevant applications of ColonCrafter, including 3D point cloud\ngeneration and surface coverage assessment.", "AI": {"tldr": "ColonCrafter\u662f\u4e00\u4e2a\u57fa\u4e8e\u6269\u6563\u6a21\u578b\u7684\u6df1\u5ea6\u4f30\u8ba1\u65b9\u6cd5\uff0c\u4e13\u95e8\u7528\u4e8e\u7ed3\u80a0\u955c\u89c6\u9891\u7684\u65f6\u5e8f\u4e00\u81f4\u6df1\u5ea6\u56fe\u751f\u6210\uff0c\u5728C3VD\u6570\u636e\u96c6\u4e0a\u5b9e\u73b0\u4e86\u6700\u5148\u8fdb\u7684\u96f6\u6837\u672c\u6027\u80fd\u3002", "motivation": "\u7ed3\u80a0\u955c3D\u573a\u666f\u7406\u89e3\u9700\u8981\u81ea\u52a8\u5316\u6df1\u5ea6\u4f30\u8ba1\u65b9\u6cd5\uff0c\u4f46\u73b0\u6709\u5185\u7aa5\u955c\u6df1\u5ea6\u4f30\u8ba1\u6a21\u578b\u5728\u89c6\u9891\u5e8f\u5217\u4e2d\u7f3a\u4e4f\u65f6\u5e8f\u4e00\u81f4\u6027\uff0c\u9650\u5236\u4e863D\u91cd\u5efa\u7684\u5e94\u7528\u3002", "method": "\u4f7f\u7528\u57fa\u4e8e\u6269\u6563\u7684\u6df1\u5ea6\u4f30\u8ba1\u6a21\u578b\uff0c\u4ece\u5408\u6210\u7ed3\u80a0\u955c\u5e8f\u5217\u5b66\u4e60\u51e0\u4f55\u5148\u9a8c\u6765\u751f\u6210\u65f6\u5e8f\u4e00\u81f4\u7684\u6df1\u5ea6\u56fe\uff0c\u5e76\u5f15\u5165\u98ce\u683c\u8fc1\u79fb\u6280\u672f\u5c06\u771f\u5b9e\u4e34\u5e8a\u89c6\u9891\u9002\u914d\u5230\u5408\u6210\u8bad\u7ec3\u57df\u3002", "result": "\u5728C3VD\u6570\u636e\u96c6\u4e0a\u5b9e\u73b0\u4e86\u6700\u5148\u8fdb\u7684\u96f6\u6837\u672c\u6027\u80fd\uff0c\u4f18\u4e8e\u901a\u7528\u548c\u7279\u5b9a\u4e8e\u5185\u7aa5\u955c\u7684\u65b9\u6cd5\uff0c\u652f\u63013D\u70b9\u4e91\u751f\u6210\u548c\u8868\u9762\u8986\u76d6\u8bc4\u4f30\u7b49\u4e34\u5e8a\u5e94\u7528\u3002", "conclusion": "\u867d\u7136\u5b8c\u6574\u8f68\u8ff93D\u91cd\u5efa\u4ecd\u5177\u6311\u6218\u6027\uff0c\u4f46ColonCrafter\u5c55\u793a\u4e86\u5728\u4e34\u5e8a\u76f8\u5173\u5e94\u7528\u4e2d\u7684\u6f5c\u529b\uff0c\u5305\u62ec3D\u70b9\u4e91\u751f\u6210\u548c\u8868\u9762\u8986\u76d6\u8bc4\u4f30\u3002", "relevance": 30.0}}
{"id": "2509.13753", "pdf": "https://arxiv.org/pdf/2509.13753", "abs": "https://arxiv.org/abs/2509.13753", "authors": ["Hyotaek Jeon", "Hyunwook Lee", "Juwon Kim", "Sungahn Ko"], "title": "ST-LINK: Spatially-Aware Large Language Models for Spatio-Temporal Forecasting", "categories": ["cs.LG"], "comment": "11 pages, 4 figures, Accepted to CIKM 2025. Code:\n  https://github.com/HyoTaek98/ST_LINK", "summary": "Traffic forecasting represents a crucial problem within intelligent\ntransportation systems. In recent research, Large Language Models (LLMs) have\nemerged as a promising method, but their intrinsic design, tailored primarily\nfor sequential token processing, introduces notable challenges in effectively\ncapturing spatial dependencies. Specifically, the inherent limitations of LLMs\nin modeling spatial relationships and their architectural incompatibility with\ngraph-structured spatial data remain largely unaddressed. To overcome these\nlimitations, we introduce ST-LINK, a novel framework that enhances the\ncapability of Large Language Models to capture spatio-temporal dependencies.\nIts key components are Spatially-Enhanced Attention (SE-Attention) and the\nMemory Retrieval Feed-Forward Network (MRFFN). SE-Attention extends rotary\nposition embeddings to integrate spatial correlations as direct rotational\ntransformations within the attention mechanism. This approach maximizes spatial\nlearning while preserving the LLM's inherent sequential processing structure.\nMeanwhile, MRFFN dynamically retrieves and utilizes key historical patterns to\ncapture complex temporal dependencies and improve the stability of long-term\nforecasting. Comprehensive experiments on benchmark datasets demonstrate that\nST-LINK surpasses conventional deep learning and LLM approaches, and\neffectively captures both regular traffic patterns and abrupt changes.", "AI": {"tldr": "ST-LINK\u662f\u4e00\u4e2a\u589e\u5f3a\u5927\u8bed\u8a00\u6a21\u578b\u65f6\u7a7a\u4f9d\u8d56\u5efa\u6a21\u80fd\u529b\u7684\u65b0\u6846\u67b6\uff0c\u901a\u8fc7\u7a7a\u95f4\u589e\u5f3a\u6ce8\u610f\u529b\u548c\u8bb0\u5fc6\u68c0\u7d22\u524d\u9988\u7f51\u7edc\u6765\u6539\u8fdb\u4ea4\u901a\u9884\u6d4b\u4efb\u52a1", "motivation": "\u73b0\u6709LLM\u4e3b\u8981\u9488\u5bf9\u5e8f\u5217\u6807\u8bb0\u5904\u7406\u8bbe\u8ba1\uff0c\u5728\u6355\u6349\u7a7a\u95f4\u4f9d\u8d56\u5173\u7cfb\u65b9\u9762\u5b58\u5728\u56fa\u6709\u5c40\u9650\uff0c\u7279\u522b\u662f\u5728\u5904\u7406\u56fe\u7ed3\u6784\u7a7a\u95f4\u6570\u636e\u65f6\u5b58\u5728\u67b6\u6784\u4e0d\u517c\u5bb9\u95ee\u9898", "method": "\u63d0\u51faST-LINK\u6846\u67b6\uff0c\u5305\u542b\u7a7a\u95f4\u589e\u5f3a\u6ce8\u610f\u529b\uff08SE-Attention\uff09\u548c\u8bb0\u5fc6\u68c0\u7d22\u524d\u9988\u7f51\u7edc\uff08MRFFN\uff09\u3002SE-Attention\u6269\u5c55\u65cb\u8f6c\u4f4d\u7f6e\u7f16\u7801\u6765\u6574\u5408\u7a7a\u95f4\u76f8\u5173\u6027\uff0cMRFFN\u52a8\u6001\u68c0\u7d22\u5386\u53f2\u6a21\u5f0f\u6765\u6355\u6349\u590d\u6742\u65f6\u95f4\u4f9d\u8d56", "result": "\u5728\u57fa\u51c6\u6570\u636e\u96c6\u4e0a\u7684\u7efc\u5408\u5b9e\u9a8c\u8868\u660e\uff0cST-LINK\u8d85\u8d8a\u4e86\u4f20\u7edf\u6df1\u5ea6\u5b66\u4e60\u548cLLM\u65b9\u6cd5\uff0c\u6709\u6548\u6355\u6349\u4e86\u5e38\u89c4\u4ea4\u901a\u6a21\u5f0f\u548c\u7a81\u53d8", "conclusion": "\u8be5\u6846\u67b6\u6210\u529f\u89e3\u51b3\u4e86LLM\u5728\u7a7a\u95f4\u5efa\u6a21\u65b9\u9762\u7684\u5c40\u9650\u6027\uff0c\u4e3a\u4ea4\u901a\u9884\u6d4b\u7b49\u65f6\u7a7a\u4efb\u52a1\u63d0\u4f9b\u4e86\u6709\u6548\u7684\u89e3\u51b3\u65b9\u6848", "relevance": 65.0}}
{"id": "2509.13761", "pdf": "https://arxiv.org/pdf/2509.13761", "abs": "https://arxiv.org/abs/2509.13761", "authors": ["Qikai Chang", "Zhenrong Zhang", "Pengfei Hu", "Jiefeng Ma", "Yicheng Pan", "Jianshu Zhang", "Jun Du", "Quan Liu", "Jianqing Gao"], "title": "THOR: Tool-Integrated Hierarchical Optimization via RL for Mathematical Reasoning", "categories": ["cs.AI", "cs.CL"], "comment": "22 pages, 13 figures", "summary": "Large Language Models (LLMs) have made remarkable progress in mathematical\nreasoning, but still continue to struggle with high-precision tasks like\nnumerical computation and formal symbolic manipulation. Integrating external\ntools has emerged as a promising approach to bridge this gap. Despite recent\nadvances, existing methods struggle with three key challenges: constructing\ntool-integrated reasoning data, performing fine-grained optimization, and\nenhancing inference. To overcome these limitations, we propose THOR\n(Tool-Integrated Hierarchical Optimization via RL). First, we introduce TIRGen,\na multi-agent actor-critic-based pipeline for constructing high-quality\ndatasets of tool-integrated reasoning paths, aligning with the policy and\ngeneralizing well across diverse models. Second, to perform fine-grained\nhierarchical optimization, we introduce an RL strategy that jointly optimizes\nfor both trajectory-level problem solving and step-level code generation. This\nis motivated by our key insight that the success of an intermediate tool call\nis a strong predictor of the final answer's correctness. Finally, THOR\nincorporates a self-correction mechanism that leverages immediate tool feedback\nto dynamically revise erroneous reasoning paths during inference. Our approach\ndemonstrates strong generalization across diverse models, performing\neffectively in both reasoning and non-reasoning models. It further achieves\nstate-of-the-art performance for models of a similar scale on multiple\nmathematical benchmarks, while also delivering consistent improvements on code\nbenchmarks. Our code will be publicly available at\nhttps://github.com/JingMog/THOR.", "AI": {"tldr": "THOR\u662f\u4e00\u4e2a\u901a\u8fc7\u5f3a\u5316\u5b66\u4e60\u5b9e\u73b0\u5de5\u5177\u96c6\u6210\u5206\u5c42\u4f18\u5316\u7684\u6846\u67b6\uff0c\u7528\u4e8e\u63d0\u5347LLM\u5728\u6570\u5b66\u63a8\u7406\u548c\u4ee3\u7801\u751f\u6210\u4e2d\u7684\u6027\u80fd\uff0c\u901a\u8fc7\u591a\u667a\u80fd\u4f53\u6570\u636e\u751f\u6210\u3001\u5206\u5c42\u4f18\u5316\u548c\u81ea\u6821\u6b63\u673a\u5236\u5b9e\u73b0\u6700\u5148\u8fdb\u6548\u679c", "motivation": "\u5927\u578b\u8bed\u8a00\u6a21\u578b\u5728\u6570\u503c\u8ba1\u7b97\u548c\u7b26\u53f7\u64cd\u4f5c\u7b49\u9ad8\u7cbe\u5ea6\u4efb\u52a1\u4e0a\u4ecd\u7136\u5b58\u5728\u56f0\u96be\uff0c\u73b0\u6709\u5de5\u5177\u96c6\u6210\u65b9\u6cd5\u5728\u6570\u636e\u6784\u5efa\u3001\u7ec6\u7c92\u5ea6\u4f18\u5316\u548c\u63a8\u7406\u589e\u5f3a\u65b9\u9762\u9762\u4e34\u6311\u6218", "method": "\u63d0\u51faTHOR\u6846\u67b6\uff1a1) TIRGen\u591a\u667a\u80fd\u4f53actor-critic\u7ba1\u9053\u6784\u5efa\u9ad8\u8d28\u91cf\u5de5\u5177\u96c6\u6210\u63a8\u7406\u6570\u636e\u96c6\uff1b2) \u5206\u5c42RL\u7b56\u7565\u8054\u5408\u4f18\u5316\u8f68\u8ff9\u7ea7\u95ee\u9898\u89e3\u51b3\u548c\u6b65\u9aa4\u7ea7\u4ee3\u7801\u751f\u6210\uff1b3) \u63a8\u7406\u65f6\u5229\u7528\u5de5\u5177\u53cd\u9988\u8fdb\u884c\u52a8\u6001\u81ea\u6821\u6b63", "result": "\u5728\u591a\u4e2a\u6570\u5b66\u57fa\u51c6\u6d4b\u8bd5\u4e0a\u8fbe\u5230\u540c\u7c7b\u89c4\u6a21\u6a21\u578b\u7684\u6700\u5148\u8fdb\u6027\u80fd\uff0c\u5728\u4ee3\u7801\u57fa\u51c6\u6d4b\u8bd5\u4e0a\u4e5f\u6709\u6301\u7eed\u6539\u8fdb\uff0c\u5c55\u73b0\u51fa\u5bf9\u4e0d\u540c\u6a21\u578b\u7684\u5f3a\u6cdb\u5316\u80fd\u529b", "conclusion": "THOR\u901a\u8fc7\u5de5\u5177\u96c6\u6210\u548c\u5206\u5c42\u4f18\u5316\u6709\u6548\u63d0\u5347\u4e86LLM\u5728\u6570\u5b66\u63a8\u7406\u548c\u4ee3\u7801\u751f\u6210\u4efb\u52a1\u4e2d\u7684\u6027\u80fd\uff0c\u8bc1\u660e\u4e86\u4e2d\u95f4\u5de5\u5177\u8c03\u7528\u6210\u529f\u5bf9\u6700\u7ec8\u7b54\u6848\u6b63\u786e\u6027\u7684\u91cd\u8981\u9884\u6d4b\u4f5c\u7528", "relevance": 85.0}}
{"id": "2509.13905", "pdf": "https://arxiv.org/pdf/2509.13905", "abs": "https://arxiv.org/abs/2509.13905", "authors": ["Domenico Meconi", "Simone Stirpe", "Federico Martelli", "Leonardo Lavalle", "Roberto Navigli"], "title": "Do Large Language Models Understand Word Senses?", "categories": ["cs.CL", "cs.AI"], "comment": "20 pages, to be published in EMNLP2025", "summary": "Understanding the meaning of words in context is a fundamental capability for\nLarge Language Models (LLMs). Despite extensive evaluation efforts, the extent\nto which LLMs show evidence that they truly grasp word senses remains\nunderexplored. In this paper, we address this gap by evaluating both i) the\nWord Sense Disambiguation (WSD) capabilities of instruction-tuned LLMs,\ncomparing their performance to state-of-the-art systems specifically designed\nfor the task, and ii) the ability of two top-performing open- and closed-source\nLLMs to understand word senses in three generative settings: definition\ngeneration, free-form explanation, and example generation. Notably, we find\nthat, in the WSD task, leading models such as GPT-4o and DeepSeek-V3 achieve\nperformance on par with specialized WSD systems, while also demonstrating\ngreater robustness across domains and levels of difficulty. In the generation\ntasks, results reveal that LLMs can explain the meaning of words in context up\nto 98\\% accuracy, with the highest performance observed in the free-form\nexplanation task, which best aligns with their generative capabilities.", "AI": {"tldr": "\u8be5\u8bba\u6587\u8bc4\u4f30\u4e86\u6307\u4ee4\u8c03\u4f18LLM\u5728\u8bcd\u4e49\u6d88\u6b67(WSD)\u4efb\u52a1\u4e2d\u7684\u80fd\u529b\uff0c\u5e76\u4e0e\u4e13\u7528WSD\u7cfb\u7edf\u8fdb\u884c\u6bd4\u8f83\uff0c\u540c\u65f6\u6d4b\u8bd5\u4e86LLM\u5728\u5b9a\u4e49\u751f\u6210\u3001\u81ea\u7531\u89e3\u91ca\u548c\u793a\u4f8b\u751f\u6210\u4e09\u79cd\u751f\u6210\u4efb\u52a1\u4e2d\u7684\u8bcd\u4e49\u7406\u89e3\u80fd\u529b\u3002", "motivation": "\u5c3d\u7ba1LLM\u5728\u8bed\u8a00\u7406\u89e3\u65b9\u9762\u53d6\u5f97\u4e86\u663e\u8457\u8fdb\u5c55\uff0c\u4f46\u5176\u662f\u5426\u771f\u6b63\u638c\u63e1\u8bcd\u4e49\u7406\u89e3\u7684\u80fd\u529b\u5c1a\u672a\u5f97\u5230\u5145\u5206\u63a2\u7d22\u3002\u8bba\u6587\u65e8\u5728\u586b\u8865\u8fd9\u4e00\u7a7a\u767d\uff0c\u7cfb\u7edf\u8bc4\u4f30LLM\u5728\u8bcd\u4e49\u6d88\u6b67\u548c\u8bcd\u4e49\u89e3\u91ca\u65b9\u9762\u7684\u8868\u73b0\u3002", "method": "\u91c7\u7528\u4e24\u79cd\u8bc4\u4f30\u65b9\u6cd5\uff1a1) \u5c06\u6307\u4ee4\u8c03\u4f18LLM\u4e0e\u6700\u5148\u8fdb\u7684\u4e13\u7528WSD\u7cfb\u7edf\u5728\u8bcd\u4e49\u6d88\u6b67\u4efb\u52a1\u4e0a\u8fdb\u884c\u6027\u80fd\u6bd4\u8f83\uff1b2) \u6d4b\u8bd5\u9876\u7ea7\u5f00\u6e90\u548c\u95ed\u6e90LLM\u5728\u4e09\u79cd\u751f\u6210\u4efb\u52a1\uff08\u5b9a\u4e49\u751f\u6210\u3001\u81ea\u7531\u5f62\u5f0f\u89e3\u91ca\u3001\u793a\u4f8b\u751f\u6210\uff09\u4e2d\u7684\u8bcd\u4e49\u7406\u89e3\u80fd\u529b\u3002", "result": "\u7814\u7a76\u53d1\u73b0\uff1aGPT-4o\u548cDeepSeek-V3\u7b49\u9886\u5148\u6a21\u578b\u5728WSD\u4efb\u52a1\u4e2d\u8fbe\u5230\u4e0e\u4e13\u7528WSD\u7cfb\u7edf\u76f8\u5f53\u7684\u6027\u80fd\uff0c\u4e14\u5728\u8de8\u9886\u57df\u548c\u4e0d\u540c\u96be\u5ea6\u7ea7\u522b\u4e0a\u8868\u73b0\u51fa\u66f4\u5f3a\u7684\u9c81\u68d2\u6027\u3002\u5728\u751f\u6210\u4efb\u52a1\u4e2d\uff0cLLM\u80fd\u591f\u4ee5\u9ad8\u8fbe98%\u7684\u51c6\u786e\u7387\u89e3\u91ca\u4e0a\u4e0b\u6587\u4e2d\u7684\u8bcd\u4e49\uff0c\u5176\u4e2d\u81ea\u7531\u89e3\u91ca\u4efb\u52a1\u8868\u73b0\u6700\u4f73\u3002", "conclusion": "LLM\u4e0d\u4ec5\u80fd\u591f\u4e0e\u4e13\u7528\u7cfb\u7edf\u76f8\u5ab2\u7f8e\u5730\u6267\u884c\u8bcd\u4e49\u6d88\u6b67\u4efb\u52a1\uff0c\u8fd8\u5c55\u73b0\u51fa\u5f3a\u5927\u7684\u8bcd\u4e49\u89e3\u91ca\u751f\u6210\u80fd\u529b\uff0c\u7279\u522b\u662f\u5728\u81ea\u7531\u5f62\u5f0f\u7684\u89e3\u91ca\u4efb\u52a1\u4e2d\u8868\u73b0\u6700\u4e3a\u51fa\u8272\uff0c\u8fd9\u4e0e\u5176\u751f\u6210\u80fd\u529b\u9ad8\u5ea6\u5951\u5408\u3002", "relevance": 85.0}}
{"id": "2509.13536", "pdf": "https://arxiv.org/pdf/2509.13536", "abs": "https://arxiv.org/abs/2509.13536", "authors": ["Yinlong Bai", "Hongxin Zhang", "Sheng Zhong", "Junkai Niu", "Hai Li", "Yijia He", "Yi Zhou"], "title": "MemGS: Memory-Efficient Gaussian Splatting for Real-Time SLAM", "categories": ["cs.CV"], "comment": null, "summary": "Recent advancements in 3D Gaussian Splatting (3DGS) have made a significant\nimpact on rendering and reconstruction techniques. Current research\npredominantly focuses on improving rendering performance and reconstruction\nquality using high-performance desktop GPUs, largely overlooking applications\nfor embedded platforms like micro air vehicles (MAVs). These devices, with\ntheir limited computational resources and memory, often face a trade-off\nbetween system performance and reconstruction quality. In this paper, we\nimprove existing methods in terms of GPU memory usage while enhancing rendering\nquality. Specifically, to address redundant 3D Gaussian primitives in SLAM, we\npropose merging them in voxel space based on geometric similarity. This reduces\nGPU memory usage without impacting system runtime performance. Furthermore,\nrendering quality is improved by initializing 3D Gaussian primitives via\nPatch-Grid (PG) point sampling, enabling more accurate modeling of the entire\nscene. Quantitative and qualitative evaluations on publicly available datasets\ndemonstrate the effectiveness of our improvements.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u9488\u5bf9\u5d4c\u5165\u5f0f\u5e73\u53f0\u76843D\u9ad8\u65af\u6cfc\u6e85\u4f18\u5316\u65b9\u6cd5\uff0c\u901a\u8fc7\u4f53\u7d20\u7a7a\u95f4\u51e0\u4f55\u76f8\u4f3c\u6027\u5408\u5e76\u51cf\u5c11GPU\u5185\u5b58\u4f7f\u7528\uff0c\u540c\u65f6\u901a\u8fc7Patch-Grid\u70b9\u91c7\u6837\u63d0\u9ad8\u6e32\u67d3\u8d28\u91cf", "motivation": "\u5f53\u524d3DGS\u7814\u7a76\u4e3b\u8981\u5173\u6ce8\u9ad8\u6027\u80fd\u684c\u9762GPU\uff0c\u5ffd\u89c6\u4e86\u5d4c\u5165\u5f0f\u5e73\u53f0\uff08\u5982\u5fae\u578b\u98de\u884c\u5668\uff09\u7684\u8ba1\u7b97\u8d44\u6e90\u548c\u5185\u5b58\u9650\u5236\uff0c\u9700\u8981\u5728\u7cfb\u7edf\u6027\u80fd\u548c\u91cd\u5efa\u8d28\u91cf\u4e4b\u95f4\u8fdb\u884c\u6743\u8861", "method": "1) \u5728\u4f53\u7d20\u7a7a\u95f4\u4e2d\u57fa\u4e8e\u51e0\u4f55\u76f8\u4f3c\u6027\u5408\u5e76\u5197\u4f59\u76843D\u9ad8\u65af\u57fa\u5143\u4ee5\u51cf\u5c11GPU\u5185\u5b58\u4f7f\u7528\uff1b2) \u901a\u8fc7Patch-Grid\u70b9\u91c7\u6837\u521d\u59cb\u53163D\u9ad8\u65af\u57fa\u5143\u4ee5\u63d0\u9ad8\u6e32\u67d3\u7cbe\u5ea6", "result": "\u5728\u516c\u5f00\u6570\u636e\u96c6\u4e0a\u7684\u5b9a\u91cf\u548c\u5b9a\u6027\u8bc4\u4f30\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u6709\u6548\u51cf\u5c11\u4e86GPU\u5185\u5b58\u4f7f\u7528\uff0c\u540c\u65f6\u63d0\u5347\u4e86\u6e32\u67d3\u8d28\u91cf\uff0c\u4e14\u4e0d\u5f71\u54cd\u7cfb\u7edf\u8fd0\u884c\u65f6\u6027\u80fd", "conclusion": "\u8be5\u65b9\u6cd5\u4e3a\u5d4c\u5165\u5f0f\u5e73\u53f0\u4e0a\u76843D\u9ad8\u65af\u6cfc\u6e85\u5e94\u7528\u63d0\u4f9b\u4e86\u6709\u6548\u7684\u4f18\u5316\u65b9\u6848\uff0c\u5e73\u8861\u4e86\u5185\u5b58\u4f7f\u7528\u548c\u6e32\u67d3\u8d28\u91cf\u7684\u9700\u6c42", "relevance": 10.0}}
{"id": "2509.13763", "pdf": "https://arxiv.org/pdf/2509.13763", "abs": "https://arxiv.org/abs/2509.13763", "authors": ["Zongxin Shen", "Yanyong Huang", "Bin Wang", "Jinyuan Chang", "Shiyu Liu", "Tianrui Li"], "title": "Beyond Correlation: Causal Multi-View Unsupervised Feature Selection Learning", "categories": ["cs.LG"], "comment": null, "summary": "Multi-view unsupervised feature selection (MUFS) has recently received\nincreasing attention for its promising ability in dimensionality reduction on\nmulti-view unlabeled data. Existing MUFS methods typically select\ndiscriminative features by capturing correlations between features and\nclustering labels. However, an important yet underexplored question remains:\n\\textit{Are such correlations sufficiently reliable to guide feature\nselection?} In this paper, we analyze MUFS from a causal perspective by\nintroducing a novel structural causal model, which reveals that existing\nmethods may select irrelevant features because they overlook spurious\ncorrelations caused by confounders. Building on this causal perspective, we\npropose a novel MUFS method called CAusal multi-view Unsupervised feature\nSelection leArning (CAUSA). Specifically, we first employ a generalized\nunsupervised spectral regression model that identifies informative features by\ncapturing dependencies between features and consensus clustering labels. We\nthen introduce a causal regularization module that can adaptively separate\nconfounders from multi-view data and simultaneously learn view-shared sample\nweights to balance confounder distributions, thereby mitigating spurious\ncorrelations. Thereafter, integrating both into a unified learning framework\nenables CAUSA to select causally informative features. Comprehensive\nexperiments demonstrate that CAUSA outperforms several state-of-the-art\nmethods. To our knowledge, this is the first in-depth study of causal\nmulti-view feature selection in the unsupervised setting.", "AI": {"tldr": "\u672c\u6587\u4ece\u56e0\u679c\u89c6\u89d2\u5206\u6790\u591a\u89c6\u56fe\u65e0\u76d1\u7763\u7279\u5f81\u9009\u62e9\uff0c\u63d0\u51faCAUSA\u65b9\u6cd5\u901a\u8fc7\u56e0\u679c\u6b63\u5219\u5316\u6a21\u5757\u5206\u79bb\u6df7\u6742\u56e0\u5b50\u5e76\u5e73\u8861\u5206\u5e03\uff0c\u4ee5\u9009\u62e9\u56e0\u679c\u4fe1\u606f\u7279\u5f81\u3002", "motivation": "\u73b0\u6709MUFS\u65b9\u6cd5\u901a\u8fc7\u7279\u5f81\u4e0e\u805a\u7c7b\u6807\u7b7e\u7684\u76f8\u5173\u6027\u6765\u9009\u62e9\u5224\u522b\u6027\u7279\u5f81\uff0c\u4f46\u53ef\u80fd\u56e0\u5ffd\u7565\u6df7\u6742\u56e0\u5b50\u5bfc\u81f4\u7684\u4f2a\u76f8\u5173\u800c\u9009\u62e9\u65e0\u5173\u7279\u5f81\u3002\u672c\u6587\u4ece\u56e0\u679c\u89d2\u5ea6\u5206\u6790\u8fd9\u4e00\u95ee\u9898\u3002", "method": "\u63d0\u51faCAUSA\u65b9\u6cd5\uff1a1) \u5e7f\u4e49\u65e0\u76d1\u7763\u8c31\u56de\u5f52\u6a21\u578b\u6355\u6349\u7279\u5f81\u4e0e\u5171\u8bc6\u805a\u7c7b\u6807\u7b7e\u7684\u4f9d\u8d56\u5173\u7cfb\uff1b2) \u56e0\u679c\u6b63\u5219\u5316\u6a21\u5757\u81ea\u9002\u5e94\u5206\u79bb\u6df7\u6742\u56e0\u5b50\u5e76\u5b66\u4e60\u89c6\u56fe\u5171\u4eab\u6837\u672c\u6743\u91cd\u4ee5\u5e73\u8861\u6df7\u6742\u56e0\u5b50\u5206\u5e03\u3002", "result": "\u7efc\u5408\u5b9e\u9a8c\u8868\u660eCAUSA\u4f18\u4e8e\u591a\u4e2a\u6700\u5148\u8fdb\u65b9\u6cd5\uff0c\u662f\u9996\u4e2a\u5728\u65e0\u76d1\u7763\u8bbe\u7f6e\u4e0b\u6df1\u5165\u7814\u7a76\u56e0\u679c\u591a\u89c6\u56fe\u7279\u5f81\u9009\u62e9\u7684\u5de5\u4f5c\u3002", "conclusion": "\u4ece\u56e0\u679c\u89c6\u89d2\u5206\u6790MUFS\u95ee\u9898\uff0c\u63d0\u51fa\u7684CAUSA\u65b9\u6cd5\u80fd\u6709\u6548\u7f13\u89e3\u4f2a\u76f8\u5173\u95ee\u9898\uff0c\u9009\u62e9\u66f4\u5177\u56e0\u679c\u4fe1\u606f\u91cf\u7684\u7279\u5f81\u3002", "relevance": 20.0}}
{"id": "2509.13773", "pdf": "https://arxiv.org/pdf/2509.13773", "abs": "https://arxiv.org/abs/2509.13773", "authors": ["Zhipeng Bian", "Jieming Zhu", "Xuyang Xie", "Quanyu Dai", "Zhou Zhao", "Zhenhua Dong"], "title": "MIRA: Empowering One-Touch AI Services on Smartphones with MLLM-based Instruction Recommendation", "categories": ["cs.AI", "cs.IR", "I.2.7; I.2.10"], "comment": "Published in Proceedings of the 63rd Annual Meeting of the\n  Association for Computational Linguistics (Volume 6: Industry Track), ACL\n  2025. Official version: https://doi.org/10.18653/v1/2025.acl-industry.103", "summary": "The rapid advancement of generative AI technologies is driving the\nintegration of diverse AI-powered services into smartphones, transforming how\nusers interact with their devices. To simplify access to predefined AI\nservices, this paper introduces MIRA, a pioneering framework for task\ninstruction recommendation that enables intuitive one-touch AI tasking on\nsmartphones. With MIRA, users can long-press on images or text objects to\nreceive contextually relevant instruction recommendations for executing AI\ntasks. Our work introduces three key innovations: 1) A multimodal large\nlanguage model (MLLM)-based recommendation pipeline with structured reasoning\nto extract key entities, infer user intent, and generate precise instructions;\n2) A template-augmented reasoning mechanism that integrates high-level\nreasoning templates, enhancing task inference accuracy; 3) A prefix-tree-based\nconstrained decoding strategy that restricts outputs to predefined instruction\ncandidates, ensuring coherent and intent-aligned suggestions. Through\nevaluation using a real-world annotated datasets and a user study, MIRA has\ndemonstrated substantial improvements in the accuracy of instruction\nrecommendation. The encouraging results highlight MIRA's potential to\nrevolutionize the way users engage with AI services on their smartphones,\noffering a more seamless and efficient experience.", "AI": {"tldr": "MIRA\u662f\u4e00\u4e2a\u667a\u80fd\u624b\u673a\u4e0a\u7684\u4efb\u52a1\u6307\u4ee4\u63a8\u8350\u6846\u67b6\uff0c\u901a\u8fc7\u957f\u6309\u56fe\u50cf\u6216\u6587\u672c\u6765\u63a8\u8350\u4e0a\u4e0b\u6587\u76f8\u5173\u7684AI\u4efb\u52a1\u6307\u4ee4\uff0c\u4f7f\u7528MLLM\u8fdb\u884c\u591a\u6a21\u6001\u63a8\u7406\u548c\u7ea6\u675f\u89e3\u7801\u6765\u63d0\u9ad8\u63a8\u8350\u51c6\u786e\u6027\u3002", "motivation": "\u968f\u7740\u751f\u6210\u5f0fAI\u6280\u672f\u5728\u667a\u80fd\u624b\u673a\u4e2d\u7684\u96c6\u6210\uff0c\u9700\u8981\u7b80\u5316\u7528\u6237\u8bbf\u95ee\u9884\u5b9a\u4e49AI\u670d\u52a1\u7684\u65b9\u5f0f\uff0c\u63d0\u4f9b\u76f4\u89c2\u7684\u4e00\u952e\u5f0fAI\u4efb\u52a1\u6267\u884c\u4f53\u9a8c\u3002", "method": "1) \u57fa\u4e8e\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\u7684\u63a8\u8350\u6d41\u6c34\u7ebf\uff0c\u8fdb\u884c\u7ed3\u6784\u5316\u63a8\u7406\u63d0\u53d6\u5173\u952e\u5b9e\u4f53\u548c\u63a8\u65ad\u7528\u6237\u610f\u56fe\uff1b2) \u6a21\u677f\u589e\u5f3a\u63a8\u7406\u673a\u5236\u6574\u5408\u9ad8\u7ea7\u63a8\u7406\u6a21\u677f\uff1b3) \u57fa\u4e8e\u524d\u7f00\u6811\u7684\u7ea6\u675f\u89e3\u7801\u7b56\u7565\u9650\u5236\u8f93\u51fa\u5230\u9884\u5b9a\u4e49\u6307\u4ee4\u5019\u9009\u96c6", "result": "\u5728\u771f\u5b9e\u6807\u6ce8\u6570\u636e\u96c6\u548c\u7528\u6237\u7814\u7a76\u4e2d\uff0cMIRA\u5728\u6307\u4ee4\u63a8\u8350\u51c6\u786e\u6027\u65b9\u9762\u8868\u73b0\u51fa\u663e\u8457\u6539\u8fdb", "conclusion": "MIRA\u6709\u6f5c\u529b\u5f7b\u5e95\u6539\u53d8\u7528\u6237\u5728\u667a\u80fd\u624b\u673a\u4e0a\u4e0eAI\u670d\u52a1\u4ea4\u4e92\u7684\u65b9\u5f0f\uff0c\u63d0\u4f9b\u66f4\u65e0\u7f1d\u548c\u9ad8\u6548\u7684\u4f53\u9a8c", "relevance": 65.0}}
{"id": "2509.13930", "pdf": "https://arxiv.org/pdf/2509.13930", "abs": "https://arxiv.org/abs/2509.13930", "authors": ["Dayeon Ki", "Marine Carpuat", "Paul McNamee", "Daniel Khashabi", "Eugene Yang", "Dawn Lawrie", "Kevin Duh"], "title": "Linguistic Nepotism: Trading-off Quality for Language Preference in Multilingual RAG", "categories": ["cs.CL"], "comment": "33 pages, 20 figures", "summary": "Multilingual Retrieval-Augmented Generation (mRAG) systems enable language\nmodels to answer knowledge-intensive queries with citation-supported responses\nacross languages. While such systems have been proposed, an open questions is\nwhether the mixture of different document languages impacts generation and\ncitation in unintended ways. To investigate, we introduce a controlled\nmethodology using model internals to measure language preference while holding\nother factors such as document relevance constant. Across eight languages and\nsix open-weight models, we find that models preferentially cite English sources\nwhen queries are in English, with this bias amplified for lower-resource\nlanguages and for documents positioned mid-context. Crucially, we find that\nmodels sometimes trade-off document relevance for language preference,\nindicating that citation choices are not always driven by informativeness\nalone. Our findings shed light on how language models leverage multilingual\ncontext and influence citation behavior.", "AI": {"tldr": "\u8be5\u7814\u7a76\u53d1\u73b0\u591a\u8bed\u8a00\u68c0\u7d22\u589e\u5f3a\u751f\u6210(mRAG)\u7cfb\u7edf\u4e2d\u5b58\u5728\u8bed\u8a00\u504f\u597d\u504f\u89c1\uff0c\u6a21\u578b\u503e\u5411\u4e8e\u5f15\u7528\u82f1\u8bed\u6587\u6863\uff0c\u5373\u4f7f\u5176\u4ed6\u8bed\u8a00\u7684\u6587\u6863\u66f4\u76f8\u5173\uff0c\u8fd9\u79cd\u504f\u89c1\u5728\u4f4e\u8d44\u6e90\u8bed\u8a00\u548c\u4e2d\u95f4\u4f4d\u7f6e\u7684\u6587\u6863\u4e2d\u66f4\u52a0\u660e\u663e\u3002", "motivation": "\u7814\u7a76\u591a\u8bed\u8a00\u68c0\u7d22\u589e\u5f3a\u751f\u6210\u7cfb\u7edf\u4e2d\u4e0d\u540c\u6587\u6863\u8bed\u8a00\u7684\u6df7\u5408\u662f\u5426\u4f1a\u5f71\u54cd\u751f\u6210\u548c\u5f15\u7528\u7684\u884c\u4e3a\uff0c\u7279\u522b\u662f\u662f\u5426\u5b58\u5728\u8bed\u8a00\u504f\u597d\u504f\u89c1\u3002", "method": "\u4f7f\u7528\u6a21\u578b\u5185\u90e8\u673a\u5236\u7684\u63a7\u5236\u65b9\u6cd5\uff0c\u5728\u4fdd\u6301\u6587\u6863\u76f8\u5173\u6027\u7b49\u56e0\u7d20\u4e0d\u53d8\u7684\u60c5\u51b5\u4e0b\uff0c\u6d4b\u91cf\u8bed\u8a00\u504f\u597d\u3002\u7814\u7a76\u8986\u76d68\u79cd\u8bed\u8a00\u548c6\u4e2a\u5f00\u6e90\u6a21\u578b\u3002", "result": "\u53d1\u73b0\u6a21\u578b\u5728\u82f1\u8bed\u67e5\u8be2\u65f6\u4f18\u5148\u5f15\u7528\u82f1\u8bed\u6765\u6e90\uff0c\u8fd9\u79cd\u504f\u89c1\u5728\u4f4e\u8d44\u6e90\u8bed\u8a00\u548c\u4e2d\u95f4\u4f4d\u7f6e\u7684\u6587\u6863\u4e2d\u66f4\u52a0\u660e\u663e\u3002\u6a21\u578b\u6709\u65f6\u4f1a\u727a\u7272\u6587\u6863\u76f8\u5173\u6027\u6765\u9009\u62e9\u8bed\u8a00\u504f\u597d\u3002", "conclusion": "\u5f15\u7528\u9009\u62e9\u5e76\u4e0d\u603b\u662f\u7531\u4fe1\u606f\u6027\u9a71\u52a8\uff0c\u8bed\u8a00\u504f\u597d\u663e\u8457\u5f71\u54cd\u591a\u8bed\u8a00\u4e0a\u4e0b\u6587\u4e2d\u7684\u5f15\u7528\u884c\u4e3a\u3002", "relevance": 75.0}}
{"id": "2509.13577", "pdf": "https://arxiv.org/pdf/2509.13577", "abs": "https://arxiv.org/abs/2509.13577", "authors": ["Tongfei Guo", "Lili Su"], "title": "Dynamic Aware: Adaptive Multi-Mode Out-of-Distribution Detection for Trajectory Prediction in Autonomous Vehicles", "categories": ["cs.CV", "cs.LG", "cs.RO"], "comment": "8 pages, 7 figures", "summary": "Trajectory prediction is central to the safe and seamless operation of\nautonomous vehicles (AVs). In deployment, however, prediction models inevitably\nface distribution shifts between training data and real-world conditions, where\nrare or underrepresented traffic scenarios induce out-of-distribution (OOD)\ncases. While most prior OOD detection research in AVs has concentrated on\ncomputer vision tasks such as object detection and segmentation,\ntrajectory-level OOD detection remains largely underexplored. A recent study\nformulated this problem as a quickest change detection (QCD) task, providing\nformal guarantees on the trade-off between detection delay and false alarms\n[1]. Building on this foundation, we propose a new framework that introduces\nadaptive mechanisms to achieve robust detection in complex driving\nenvironments. Empirical analysis across multiple real-world datasets reveals\nthat prediction errors -- even on in-distribution samples -- exhibit\nmode-dependent distributions that evolve over time with dataset-specific\ndynamics. By explicitly modeling these error modes, our method achieves\nsubstantial improvements in both detection delay and false alarm rates.\nComprehensive experiments on established trajectory prediction benchmarks show\nthat our framework significantly outperforms prior UQ- and vision-based OOD\napproaches in both accuracy and computational efficiency, offering a practical\npath toward reliable, driving-aware autonomy.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u7528\u4e8e\u81ea\u52a8\u9a7e\u9a76\u8f66\u8f86\u8f68\u8ff9\u9884\u6d4b\u7684\u81ea\u9002\u5e94OOD\u68c0\u6d4b\u6846\u67b6\uff0c\u901a\u8fc7\u663e\u5f0f\u5efa\u6a21\u9884\u6d4b\u8bef\u5dee\u6a21\u5f0f\uff0c\u5728\u68c0\u6d4b\u5ef6\u8fdf\u548c\u8bef\u62a5\u7387\u65b9\u9762\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5", "motivation": "\u81ea\u52a8\u9a7e\u9a76\u8f66\u8f86\u5728\u90e8\u7f72\u4e2d\u9762\u4e34\u8bad\u7ec3\u6570\u636e\u4e0e\u771f\u5b9e\u4e16\u754c\u6761\u4ef6\u4e4b\u95f4\u7684\u5206\u5e03\u504f\u79fb\u95ee\u9898\uff0c\u73b0\u6709OOD\u68c0\u6d4b\u7814\u7a76\u4e3b\u8981\u96c6\u4e2d\u5728\u8ba1\u7b97\u673a\u89c6\u89c9\u4efb\u52a1\uff0c\u8f68\u8ff9\u7ea7\u522b\u7684OOD\u68c0\u6d4b\u7814\u7a76\u76f8\u5bf9\u4e0d\u8db3", "method": "\u57fa\u4e8e\u5feb\u901f\u53d8\u5316\u68c0\u6d4b(QCD)\u4efb\u52a1\uff0c\u5f15\u5165\u81ea\u9002\u5e94\u673a\u5236\u6765\u5efa\u6a21\u9884\u6d4b\u8bef\u5dee\u7684\u6a21\u5f0f\u4f9d\u8d56\u6027\u5206\u5e03\uff0c\u8fd9\u4e9b\u5206\u5e03\u968f\u65f6\u95f4\u6f14\u53d8\u5e76\u5177\u6709\u6570\u636e\u96c6\u7279\u5b9a\u7684\u52a8\u6001\u7279\u6027", "result": "\u5728\u591a\u4e2a\u771f\u5b9e\u4e16\u754c\u6570\u636e\u96c6\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u5728\u68c0\u6d4b\u5ef6\u8fdf\u548c\u8bef\u62a5\u7387\u65b9\u9762\u90fd\u53d6\u5f97\u4e86\u663e\u8457\u6539\u8fdb\uff0c\u5728\u51c6\u786e\u6027\u548c\u8ba1\u7b97\u6548\u7387\u4e0a\u5747\u4f18\u4e8e\u73b0\u6709\u7684UQ\u548c\u57fa\u4e8e\u89c6\u89c9\u7684OOD\u65b9\u6cd5", "conclusion": "\u8be5\u6846\u67b6\u4e3a\u53ef\u9760\u7684\u3001\u9a7e\u9a76\u611f\u77e5\u7684\u81ea\u4e3b\u6027\u63d0\u4f9b\u4e86\u4e00\u6761\u5b9e\u7528\u8def\u5f84\uff0c\u80fd\u591f\u6709\u6548\u5904\u7406\u590d\u6742\u9a7e\u9a76\u73af\u5883\u4e2d\u7684\u5206\u5e03\u504f\u79fb\u95ee\u9898", "relevance": 35.0}}
{"id": "2509.13783", "pdf": "https://arxiv.org/pdf/2509.13783", "abs": "https://arxiv.org/abs/2509.13783", "authors": ["Tianshuo Zhang", "Wenzhe Zhai", "Rui Yann", "Jia Gao", "He Cao", "Xianglei Xing"], "title": "Floating-Body Hydrodynamic Neural Networks", "categories": ["cs.LG"], "comment": null, "summary": "Fluid-structure interaction is common in engineering and natural systems,\nwhere floating-body motion is governed by added mass, drag, and background\nflows. Modeling these dissipative dynamics is difficult: black-box neural\nmodels regress state derivatives with limited interpretability and unstable\nlong-horizon predictions. We propose Floating-Body Hydrodynamic Neural Networks\n(FHNN), a physics-structured framework that predicts interpretable hydrodynamic\nparameters such as directional added masses, drag coefficients, and a\nstreamfunction-based flow, and couples them with analytic equations of motion.\nThis design constrains the hypothesis space, enhances interpretability, and\nstabilizes integration. On synthetic vortex datasets, FHNN achieves up to an\norder-of-magnitude lower error than Neural ODEs, recovers physically consistent\nflow fields. Compared with Hamiltonian and Lagrangian neural networks, FHNN\nmore effectively handles dissipative dynamics while preserving\ninterpretability, which bridges the gap between black-box learning and\ntransparent system identification.", "AI": {"tldr": "\u63d0\u51fa\u4e86Floating-Body Hydrodynamic Neural Networks (FHNN)\uff0c\u4e00\u79cd\u7269\u7406\u7ed3\u6784\u5316\u7684\u795e\u7ecf\u7f51\u7edc\u6846\u67b6\uff0c\u7528\u4e8e\u9884\u6d4b\u53ef\u89e3\u91ca\u7684\u6c34\u52a8\u529b\u53c2\u6570\u5e76\u8026\u5408\u89e3\u6790\u8fd0\u52a8\u65b9\u7a0b\uff0c\u5728\u8017\u6563\u52a8\u529b\u5b66\u5efa\u6a21\u65b9\u9762\u4f18\u4e8e\u4f20\u7edf\u65b9\u6cd5\u3002", "motivation": "\u6d41\u4f53-\u7ed3\u6784\u76f8\u4e92\u4f5c\u7528\u5728\u5de5\u7a0b\u548c\u81ea\u7136\u7cfb\u7edf\u4e2d\u5f88\u5e38\u89c1\uff0c\u4f46\u4f20\u7edf\u9ed1\u76d2\u795e\u7ecf\u7f51\u7edc\u6a21\u578b\u5728\u53ef\u89e3\u91ca\u6027\u548c\u957f\u671f\u9884\u6d4b\u7a33\u5b9a\u6027\u65b9\u9762\u5b58\u5728\u5c40\u9650\uff0c\u9700\u8981\u5f00\u53d1\u7269\u7406\u7ed3\u6784\u5316\u7684\u5efa\u6a21\u65b9\u6cd5\u3002", "method": "\u8bbe\u8ba1FHNN\u6846\u67b6\uff0c\u9884\u6d4b\u53ef\u89e3\u91ca\u7684\u6c34\u52a8\u529b\u53c2\u6570\uff08\u5982\u9644\u52a0\u8d28\u91cf\u3001\u963b\u529b\u7cfb\u6570\u548c\u57fa\u4e8e\u6d41\u51fd\u6570\u7684\u6d41\u52a8\uff09\uff0c\u5e76\u5c06\u5176\u4e0e\u89e3\u6790\u8fd0\u52a8\u65b9\u7a0b\u8026\u5408\uff0c\u901a\u8fc7\u7ea6\u675f\u5047\u8bbe\u7a7a\u95f4\u6765\u589e\u5f3a\u53ef\u89e3\u91ca\u6027\u548c\u7a33\u5b9a\u6027\u3002", "result": "\u5728\u5408\u6210\u6da1\u6d41\u6570\u636e\u96c6\u4e0a\uff0cFHNN\u6bd4\u795e\u7ecfODE\u7684\u8bef\u5dee\u4f4e\u4e00\u4e2a\u6570\u91cf\u7ea7\uff0c\u80fd\u6062\u590d\u7269\u7406\u4e00\u81f4\u7684\u6d41\u573a\uff0c\u76f8\u6bd4\u54c8\u5bc6\u987f\u548c\u62c9\u683c\u6717\u65e5\u795e\u7ecf\u7f51\u7edc\u80fd\u66f4\u6709\u6548\u5904\u7406\u8017\u6563\u52a8\u529b\u5b66\u3002", "conclusion": "FHNN\u5728\u4fdd\u6301\u53ef\u89e3\u91ca\u6027\u7684\u540c\u65f6\u6709\u6548\u5904\u7406\u8017\u6563\u52a8\u529b\u5b66\uff0c\u586b\u8865\u4e86\u9ed1\u76d2\u5b66\u4e60\u4e0e\u900f\u660e\u7cfb\u7edf\u8bc6\u522b\u4e4b\u95f4\u7684\u7a7a\u767d\u3002", "relevance": 15.0}}
{"id": "2509.13880", "pdf": "https://arxiv.org/pdf/2509.13880", "abs": "https://arxiv.org/abs/2509.13880", "authors": ["Mingwei Zhang", "Zhenhao Gu", "Liangda Fang", "Cunjing Ge", "Ziliang Chen", "Zhao-Rong Lai", "Quanlong Guan"], "title": "An Exhaustive DPLL Approach to Model Counting over Integer Linear Constraints with Simplification Techniques", "categories": ["cs.AI"], "comment": null, "summary": "Linear constraints are one of the most fundamental constraints in fields such\nas computer science, operations research and optimization. Many applications\nreduce to the task of model counting over integer linear constraints (MCILC).\nIn this paper, we design an exact approach to MCILC based on an exhaustive DPLL\narchitecture. To improve the efficiency, we integrate several effective\nsimplification techniques from mixed integer programming into the architecture.\nWe compare our approach to state-of-the-art MCILC counters and propositional\nmodel counters on 2840 random and 4131 application benchmarks. Experimental\nresults show that our approach significantly outperforms all exact methods in\nrandom benchmarks solving 1718 instances while the state-of-the-art approach\nonly computes 1470 instances. In addition, our approach is the only approach to\nsolve all 4131 application instances.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u4e2a\u57fa\u4e8eDPLL\u67b6\u6784\u7684\u7cbe\u786e\u65b9\u6cd5\u6765\u89e3\u51b3\u6574\u6570\u7ebf\u6027\u7ea6\u675f\u7684\u6a21\u578b\u8ba1\u6570\u95ee\u9898(MCILC)\uff0c\u96c6\u6210\u4e86\u6df7\u5408\u6574\u6570\u89c4\u5212\u4e2d\u7684\u7b80\u5316\u6280\u672f\uff0c\u5728\u968f\u673a\u548c\u5e94\u7528\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "motivation": "\u7ebf\u6027\u7ea6\u675f\u662f\u8ba1\u7b97\u673a\u79d1\u5b66\u3001\u8fd0\u7b79\u5b66\u548c\u4f18\u5316\u9886\u57df\u7684\u57fa\u7840\u7ea6\u675f\uff0c\u8bb8\u591a\u5e94\u7528\u95ee\u9898\u53ef\u5f52\u7ed3\u4e3a\u6574\u6570\u7ebf\u6027\u7ea6\u675f\u7684\u6a21\u578b\u8ba1\u6570\u95ee\u9898(MCILC)\uff0c\u9700\u8981\u9ad8\u6548\u7684\u7cbe\u786e\u6c42\u89e3\u65b9\u6cd5\u3002", "method": "\u57fa\u4e8e\u8be6\u5c3d\u7684DPLL\u67b6\u6784\u8bbe\u8ba1\u7cbe\u786e\u65b9\u6cd5\uff0c\u96c6\u6210\u6df7\u5408\u6574\u6570\u89c4\u5212\u4e2d\u7684\u6709\u6548\u7b80\u5316\u6280\u672f\u6765\u63d0\u9ad8\u6548\u7387\u3002", "result": "\u57282840\u4e2a\u968f\u673a\u57fa\u51c6\u548c4131\u4e2a\u5e94\u7528\u57fa\u51c6\u4e0a\u6d4b\u8bd5\uff0c\u65b0\u65b9\u6cd5\u5728\u968f\u673a\u57fa\u51c6\u4e0a\u89e3\u51b3\u4e861718\u4e2a\u5b9e\u4f8b\uff08\u73b0\u6709\u6700\u4f73\u65b9\u6cd5\u4e3a1470\u4e2a\uff09\uff0c\u5e76\u4e14\u662f\u552f\u4e00\u80fd\u89e3\u51b3\u6240\u67094131\u4e2a\u5e94\u7528\u5b9e\u4f8b\u7684\u65b9\u6cd5\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u5728MCILC\u95ee\u9898\u4e0a\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u7cbe\u786e\u65b9\u6cd5\uff0c\u7279\u522b\u662f\u5728\u5e94\u7528\u5b9e\u4f8b\u4e0a\u8868\u73b0\u51fa\u8272\u3002", "relevance": 20.0}}
{"id": "2509.13980", "pdf": "https://arxiv.org/pdf/2509.13980", "abs": "https://arxiv.org/abs/2509.13980", "authors": ["Sami Ul Haq", "Chinonso Cynthia Osuji", "Sheila Castilho", "Brian Davis"], "title": "Long-context Reference-based MT Quality Estimation", "categories": ["cs.CL", "cs.LG"], "comment": null, "summary": "In this paper, we present our submission to the Tenth Conference on Machine\nTranslation (WMT25) Shared Task on Automated Translation Quality Evaluation.\n  Our systems are built upon the COMET framework and trained to predict\nsegment-level Error Span Annotation (ESA) scores using augmented long-context\ndata.\n  To construct long-context training data, we concatenate in-domain,\nhuman-annotated sentences and compute a weighted average of their scores.\n  We integrate multiple human judgment datasets (MQM, SQM, and DA) by\nnormalising their scales and train multilingual regression models to predict\nquality scores from the source, hypothesis, and reference translations.\n  Experimental results show that incorporating long-context information\nimproves correlations with human judgments compared to models trained only on\nshort segments.", "AI": {"tldr": "\u57fa\u4e8eCOMET\u6846\u67b6\u6784\u5efa\u7684\u673a\u5668\u7ffb\u8bd1\u8d28\u91cf\u8bc4\u4f30\u7cfb\u7edf\uff0c\u901a\u8fc7\u957f\u4e0a\u4e0b\u6587\u6570\u636e\u589e\u5f3a\u548c\u591a\u79cd\u4eba\u5de5\u6807\u6ce8\u6570\u636e\u96c6\u6574\u5408\uff0c\u63d0\u5347\u4e86\u4e0e\u4eba\u5de5\u8bc4\u4f30\u7684\u76f8\u5173\u6027", "motivation": "\u89e3\u51b3\u4f20\u7edf\u673a\u5668\u7ffb\u8bd1\u8d28\u91cf\u8bc4\u4f30\u4e2d\u77ed\u7247\u6bb5\u8bad\u7ec3\u7684\u5c40\u9650\u6027\uff0c\u901a\u8fc7\u5229\u7528\u957f\u4e0a\u4e0b\u6587\u4fe1\u606f\u6765\u66f4\u597d\u5730\u9884\u6d4b\u7ffb\u8bd1\u8d28\u91cf", "method": "\u4f7f\u7528COMET\u6846\u67b6\uff0c\u901a\u8fc7\u62fc\u63a5\u9886\u57df\u5185\u4eba\u5de5\u6807\u6ce8\u53e5\u5b50\u6784\u5efa\u957f\u4e0a\u4e0b\u6587\u8bad\u7ec3\u6570\u636e\uff0c\u6574\u5408MQM\u3001SQM\u3001DA\u7b49\u591a\u79cd\u4eba\u5de5\u8bc4\u4f30\u6570\u636e\u96c6\uff0c\u8bad\u7ec3\u591a\u8bed\u8a00\u56de\u5f52\u6a21\u578b", "result": "\u5b9e\u9a8c\u7ed3\u679c\u663e\u793a\uff0c\u76f8\u6bd4\u4ec5\u4f7f\u7528\u77ed\u7247\u6bb5\u8bad\u7ec3\u7684\u6a21\u578b\uff0c\u6574\u5408\u957f\u4e0a\u4e0b\u6587\u4fe1\u606f\u663e\u8457\u63d0\u9ad8\u4e86\u4e0e\u4eba\u5de5\u8bc4\u4f30\u7684\u76f8\u5173\u6027", "conclusion": "\u957f\u4e0a\u4e0b\u6587\u4fe1\u606f\u5bf9\u4e8e\u673a\u5668\u7ffb\u8bd1\u8d28\u91cf\u8bc4\u4f30\u5177\u6709\u91cd\u8981\u4ef7\u503c\uff0c\u80fd\u591f\u6709\u6548\u63d0\u5347\u8bc4\u4f30\u6a21\u578b\u7684\u6027\u80fd", "relevance": 30.0}}
{"id": "2509.13586", "pdf": "https://arxiv.org/pdf/2509.13586", "abs": "https://arxiv.org/abs/2509.13586", "authors": ["Nathalie Neptune", "Josiane Mothe"], "title": "Annotating Satellite Images of Forests with Keywords from a Specialized Corpus in the Context of Change Detection", "categories": ["cs.CV", "cs.CL", "cs.IR", "cs.MM", "I.2; I.4; I.7; H.3"], "comment": null, "summary": "The Amazon rain forest is a vital ecosystem that plays a crucial role in\nregulating the Earth's climate and providing habitat for countless species.\nDeforestation in the Amazon is a major concern as it has a significant impact\non global carbon emissions and biodiversity. In this paper, we present a method\nfor detecting deforestation in the Amazon using image pairs from Earth\nobservation satellites. Our method leverages deep learning techniques to\ncompare the images of the same area at different dates and identify changes in\nthe forest cover. We also propose a visual semantic model that automatically\nannotates the detected changes with relevant keywords. The candidate annotation\nfor images are extracted from scientific documents related to the Amazon\nregion. We evaluate our approach on a dataset of Amazon image pairs and\ndemonstrate its effectiveness in detecting deforestation and generating\nrelevant annotations. Our method provides a useful tool for monitoring and\nstudying the impact of deforestation in the Amazon. While we focus on\nenvironment applications of our work by using images of deforestation in the\nAmazon rain forest to demonstrate the effectiveness of our proposed approach,\nit is generic enough to be applied to other domains.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u6df1\u5ea6\u5b66\u4e60\u7684\u65b9\u6cd5\uff0c\u901a\u8fc7\u536b\u661f\u56fe\u50cf\u5bf9\u68c0\u6d4b\u4e9a\u9a6c\u900a\u96e8\u6797\u780d\u4f10\uff0c\u5e76\u7ed3\u5408\u79d1\u5b66\u6587\u732e\u81ea\u52a8\u751f\u6210\u76f8\u5173\u6807\u6ce8", "motivation": "\u4e9a\u9a6c\u900a\u96e8\u6797\u780d\u4f10\u5bf9\u5168\u7403\u78b3\u6392\u653e\u548c\u751f\u7269\u591a\u6837\u6027\u6709\u91cd\u5927\u5f71\u54cd\uff0c\u9700\u8981\u6709\u6548\u7684\u76d1\u6d4b\u5de5\u5177\u6765\u68c0\u6d4b\u68ee\u6797\u8986\u76d6\u53d8\u5316", "method": "\u4f7f\u7528\u6df1\u5ea6\u5b66\u4e60\u6280\u672f\u6bd4\u8f83\u4e0d\u540c\u65f6\u95f4\u70b9\u7684\u536b\u661f\u56fe\u50cf\u5bf9\uff0c\u68c0\u6d4b\u68ee\u6797\u8986\u76d6\u53d8\u5316\uff0c\u5e76\u63d0\u51fa\u89c6\u89c9\u8bed\u4e49\u6a21\u578b\u4ece\u76f8\u5173\u79d1\u5b66\u6587\u732e\u4e2d\u81ea\u52a8\u63d0\u53d6\u5173\u952e\u8bcd\u8fdb\u884c\u6807\u6ce8", "result": "\u5728\u4e9a\u9a6c\u900a\u56fe\u50cf\u5bf9\u6570\u636e\u96c6\u4e0a\u8bc4\u4f30\u663e\u793a\uff0c\u8be5\u65b9\u6cd5\u80fd\u6709\u6548\u68c0\u6d4b\u780d\u4f10\u5e76\u751f\u6210\u76f8\u5173\u6807\u6ce8", "conclusion": "\u8be5\u65b9\u6cd5\u4e3a\u76d1\u6d4b\u4e9a\u9a6c\u900a\u780d\u4f10\u5f71\u54cd\u63d0\u4f9b\u4e86\u6709\u7528\u5de5\u5177\uff0c\u4e14\u5177\u6709\u901a\u7528\u6027\u53ef\u5e94\u7528\u4e8e\u5176\u4ed6\u9886\u57df", "relevance": 15.0}}
{"id": "2509.13805", "pdf": "https://arxiv.org/pdf/2509.13805", "abs": "https://arxiv.org/abs/2509.13805", "authors": ["Florian Wiesner", "Matthias Wessling", "Stephen Baek"], "title": "Towards a Physics Foundation Model", "categories": ["cs.LG", "cs.AI", "stat.ML"], "comment": null, "summary": "Foundation models have revolutionized natural language processing through a\n``train once, deploy anywhere'' paradigm, where a single pre-trained model\nadapts to countless downstream tasks without retraining. Access to a Physics\nFoundation Model (PFM) would be transformative -- democratizing access to\nhigh-fidelity simulations, accelerating scientific discovery, and eliminating\nthe need for specialized solver development. Yet current physics-aware machine\nlearning approaches remain fundamentally limited to single, narrow domains and\nrequire retraining for each new system. We present the General Physics\nTransformer (GPhyT), trained on 1.8 TB of diverse simulation data, that\ndemonstrates foundation model capabilities are achievable for physics. Our key\ninsight is that transformers can learn to infer governing dynamics from\ncontext, enabling a single model to simulate fluid-solid interactions, shock\nwaves, thermal convection, and multi-phase dynamics without being told the\nunderlying equations. GPhyT achieves three critical breakthroughs: (1) superior\nperformance across multiple physics domains, outperforming specialized\narchitectures by up to 29x, (2) zero-shot generalization to entirely unseen\nphysical systems through in-context learning, and (3) stable long-term\npredictions through 50-timestep rollouts. By establishing that a single model\ncan learn generalizable physical principles from data alone, this work opens\nthe path toward a universal PFM that could transform computational science and\nengineering.", "AI": {"tldr": "GPhyT\u662f\u4e00\u4e2a\u901a\u7528\u7269\u7406Transformer\uff0c\u57281.8TB\u591a\u6837\u5316\u6a21\u62df\u6570\u636e\u4e0a\u8bad\u7ec3\uff0c\u80fd\u591f\u5728\u591a\u4e2a\u7269\u7406\u9886\u57df\u5b9e\u73b0\u96f6\u6837\u672c\u6cdb\u5316\uff0c\u65e0\u9700\u91cd\u65b0\u8bad\u7ec3\u5373\u53ef\u6a21\u62df\u6d41\u4f53-\u56fa\u4f53\u76f8\u4e92\u4f5c\u7528\u3001\u51b2\u51fb\u6ce2\u3001\u70ed\u5bf9\u6d41\u7b49\u7269\u7406\u73b0\u8c61\u3002", "motivation": "\u5f53\u524d\u57fa\u4e8e\u7269\u7406\u7684\u673a\u5668\u5b66\u4e60\u65b9\u6cd5\u5c40\u9650\u4e8e\u5355\u4e00\u72ed\u7a84\u9886\u57df\uff0c\u9700\u8981\u4e3a\u6bcf\u4e2a\u65b0\u7cfb\u7edf\u91cd\u65b0\u8bad\u7ec3\u3002\u4f5c\u8005\u5e0c\u671b\u5f00\u53d1\u4e00\u4e2a\u7269\u7406\u57fa\u7840\u6a21\u578b\uff0c\u5b9e\u73b0\"\u4e00\u6b21\u8bad\u7ec3\uff0c\u968f\u5904\u90e8\u7f72\"\u7684\u8303\u5f0f\uff0c\u4ece\u800c\u6c11\u4e3b\u5316\u9ad8\u4fdd\u771f\u6a21\u62df\u7684\u8bbf\u95ee\uff0c\u52a0\u901f\u79d1\u5b66\u53d1\u73b0\u3002", "method": "\u4f7f\u7528Transformer\u67b6\u6784\uff0c\u57281.8TB\u591a\u6837\u5316\u7269\u7406\u6a21\u62df\u6570\u636e\u4e0a\u8fdb\u884c\u8bad\u7ec3\u3002\u5173\u952e\u6d1e\u5bdf\u662fTransformer\u53ef\u4ee5\u4ece\u4e0a\u4e0b\u6587\u4e2d\u5b66\u4e60\u63a8\u65ad\u63a7\u5236\u52a8\u529b\u5b66\uff0c\u65e0\u9700\u88ab\u544a\u77e5\u5e95\u5c42\u65b9\u7a0b\u3002", "result": "1) \u5728\u591a\u4e2a\u7269\u7406\u9886\u57df\u8868\u73b0\u4f18\u5f02\uff0c\u6bd4\u4e13\u95e8\u67b6\u6784\u6027\u80fd\u63d0\u5347\u9ad8\u8fbe29\u500d\uff1b2) \u901a\u8fc7\u4e0a\u4e0b\u6587\u5b66\u4e60\u5b9e\u73b0\u96f6\u6837\u672c\u6cdb\u5316\u5230\u5168\u65b0\u7269\u7406\u7cfb\u7edf\uff1b3) \u901a\u8fc750\u65f6\u95f4\u6b65\u5c55\u5f00\u5b9e\u73b0\u7a33\u5b9a\u7684\u957f\u671f\u9884\u6d4b\u3002", "conclusion": "\u8fd9\u9879\u5de5\u4f5c\u8bc1\u660e\u4e86\u5355\u4e2a\u6a21\u578b\u53ef\u4ee5\u4ece\u6570\u636e\u4e2d\u5b66\u4e60\u53ef\u6cdb\u5316\u7684\u7269\u7406\u539f\u7406\uff0c\u4e3a\u901a\u5411\u53ef\u80fd\u6539\u53d8\u8ba1\u7b97\u79d1\u5b66\u548c\u5de5\u7a0b\u7684\u901a\u7528\u7269\u7406\u57fa\u7840\u6a21\u578b\u5f00\u8f9f\u4e86\u9053\u8def\u3002", "relevance": 75.0}}
{"id": "2509.13968", "pdf": "https://arxiv.org/pdf/2509.13968", "abs": "https://arxiv.org/abs/2509.13968", "authors": ["Konstantinos Voudouris", "Andrew Barron", "Marta Halina", "Colin Klein", "Matishalin Patel"], "title": "Exploring Major Transitions in the Evolution of Biological Cognition With Artificial Neural Networks", "categories": ["cs.AI", "cs.CL", "cs.FL", "cs.LG"], "comment": null, "summary": "Transitional accounts of evolution emphasise a few changes that shape what is\nevolvable, with dramatic consequences for derived lineages. More recently it\nhas been proposed that cognition might also have evolved via a series of major\ntransitions that manipulate the structure of biological neural networks,\nfundamentally changing the flow of information. We used idealised models of\ninformation flow, artificial neural networks (ANNs), to evaluate whether\nchanges in information flow in a network can yield a transitional change in\ncognitive performance. We compared networks with feed-forward, recurrent and\nlaminated topologies, and tested their performance learning artificial grammars\nthat differed in complexity, controlling for network size and resources. We\ndocumented a qualitative expansion in the types of input that recurrent\nnetworks can process compared to feed-forward networks, and a related\nqualitative increase in performance for learning the most complex grammars. We\nalso noted how the difficulty in training recurrent networks poses a form of\ntransition barrier and contingent irreversibility -- other key features of\nevolutionary transitions. Not all changes in network topology confer a\nperformance advantage in this task set. Laminated networks did not outperform\nnon-laminated networks in grammar learning. Overall, our findings show how some\nchanges in information flow can yield transitions in cognitive performance.", "AI": {"tldr": "\u8be5\u7814\u7a76\u4f7f\u7528\u4eba\u5de5\u795e\u7ecf\u7f51\u7edc\u6a21\u578b\u63a2\u8ba8\u4fe1\u606f\u6d41\u7ed3\u6784\u53d8\u5316\u662f\u5426\u4f1a\u5bfc\u81f4\u8ba4\u77e5\u6027\u80fd\u7684\u8f6c\u53d8\u6027\u53d8\u5316\uff0c\u6bd4\u8f83\u4e86\u524d\u9988\u3001\u5faa\u73af\u548c\u5206\u5c42\u7f51\u7edc\u5728\u8bed\u6cd5\u5b66\u4e60\u4efb\u52a1\u4e2d\u7684\u8868\u73b0\u3002", "motivation": "\u7814\u7a76\u52a8\u673a\u662f\u9a8c\u8bc1\u8ba4\u77e5\u8fdb\u5316\u662f\u5426\u901a\u8fc7\u6539\u53d8\u751f\u7269\u795e\u7ecf\u7f51\u7edc\u4fe1\u606f\u6d41\u7ed3\u6784\u7684\u4e3b\u8981\u8f6c\u53d8\u6765\u5b9e\u73b0\uff0c\u501f\u9274\u8fdb\u5316\u751f\u7269\u5b66\u4e2d\u7684\u8f6c\u53d8\u7406\u8bba\u6765\u7406\u89e3\u8ba4\u77e5\u80fd\u529b\u7684\u8dc3\u8fc1\u3002", "method": "\u4f7f\u7528\u7406\u60f3\u5316\u4fe1\u606f\u6d41\u6a21\u578b\u548c\u4eba\u5de5\u795e\u7ecf\u7f51\u7edc\uff0c\u6bd4\u8f83\u524d\u9988\u3001\u5faa\u73af\u548c\u5206\u5c42\u62d3\u6251\u7ed3\u6784\uff0c\u5728\u7f51\u7edc\u89c4\u6a21\u548c\u8d44\u6e90\u63a7\u5236\u6761\u4ef6\u4e0b\u6d4b\u8bd5\u4e0d\u540c\u590d\u6742\u5ea6\u4eba\u5de5\u8bed\u6cd5\u7684\u5b66\u4e60\u6027\u80fd\u3002", "result": "\u53d1\u73b0\u5faa\u73af\u7f51\u7edc\u76f8\u6bd4\u524d\u9988\u7f51\u7edc\u80fd\u5904\u7406\u66f4\u591a\u7c7b\u578b\u7684\u8f93\u5165\uff0c\u5728\u6700\u590d\u6742\u8bed\u6cd5\u5b66\u4e60\u4e0a\u8868\u73b0\u6709\u8d28\u7684\u63d0\u5347\uff1b\u5faa\u73af\u7f51\u7edc\u8bad\u7ec3\u56f0\u96be\u6784\u6210\u8f6c\u53d8\u969c\u788d\uff1b\u5206\u5c42\u7f51\u7edc\u5728\u8bed\u6cd5\u5b66\u4e60\u4e2d\u672a\u8868\u73b0\u51fa\u4f18\u52bf\u3002", "conclusion": "\u67d0\u4e9b\u4fe1\u606f\u6d41\u7ed3\u6784\u53d8\u5316\u786e\u5b9e\u80fd\u5bfc\u81f4\u8ba4\u77e5\u6027\u80fd\u7684\u8f6c\u53d8\u6027\u53d8\u5316\uff0c\u8fd9\u652f\u6301\u4e86\u8ba4\u77e5\u8fdb\u5316\u53ef\u80fd\u901a\u8fc7\u795e\u7ecf\u7f51\u7edc\u7ed3\u6784\u8f6c\u53d8\u5b9e\u73b0\u7684\u7406\u8bba\u3002", "relevance": 45.0}}
{"id": "2509.13990", "pdf": "https://arxiv.org/pdf/2509.13990", "abs": "https://arxiv.org/abs/2509.13990", "authors": ["Colin Hong", "Xu Guo", "Anand Chaanan Singh", "Esha Choukse", "Dmitrii Ustiugov"], "title": "Slim-SC: Thought Pruning for Efficient Scaling with Self-Consistency", "categories": ["cs.CL", "cs.AI", "cs.LG", "I.2.7"], "comment": "Accepted by EMNLP 2025 (Oral), 9 pages", "summary": "Recently, Test-Time Scaling (TTS) has gained increasing attention for\nimproving LLM reasoning performance at test time without retraining the model.\nA notable TTS technique is Self-Consistency (SC), which generates multiple\nreasoning chains in parallel and selects the final answer via majority voting.\nWhile effective, the order-of-magnitude computational overhead limits its broad\ndeployment. Prior attempts to accelerate SC mainly rely on model-based\nconfidence scores or heuristics with limited empirical support. For the first\ntime, we theoretically and empirically analyze the inefficiencies of SC and\nreveal actionable opportunities for improvement. Building on these insights, we\npropose Slim-SC, a step-wise pruning strategy that identifies and removes\nredundant chains using inter-chain similarity at the thought level. Experiments\non three STEM reasoning datasets and two recent LLM architectures show that\nSlim-SC reduces inference latency and KVC usage by up to 45% and 26%,\nrespectively, with R1-Distill, while maintaining or improving accuracy, thus\noffering a simple yet efficient TTS alternative for SC.", "AI": {"tldr": "Slim-SC\u662f\u4e00\u79cd\u901a\u8fc7\u601d\u7ef4\u7ea7\u94fe\u95f4\u76f8\u4f3c\u6027\u8bc6\u522b\u548c\u79fb\u9664\u5197\u4f59\u94fe\u7684\u9010\u6b65\u526a\u679d\u7b56\u7565\uff0c\u663e\u8457\u964d\u4f4eSelf-Consistency\u7684\u8ba1\u7b97\u5f00\u9500\uff0c\u540c\u65f6\u4fdd\u6301\u6216\u63d0\u9ad8\u63a8\u7406\u51c6\u786e\u6027\u3002", "motivation": "Self-Consistency(SC)\u901a\u8fc7\u5e76\u884c\u751f\u6210\u591a\u4e2a\u63a8\u7406\u94fe\u5e76\u591a\u6570\u6295\u7968\u9009\u62e9\u6700\u7ec8\u7b54\u6848\u6765\u63d0\u5347LLM\u63a8\u7406\u6027\u80fd\uff0c\u4f46\u5176\u6570\u91cf\u7ea7\u8ba1\u7b97\u5f00\u9500\u9650\u5236\u4e86\u5e7f\u6cdb\u5e94\u7528\u3002\u73b0\u6709\u52a0\u901f\u65b9\u6cd5\u4e3b\u8981\u4f9d\u8d56\u6a21\u578b\u7f6e\u4fe1\u5ea6\u5206\u6570\u6216\u7ecf\u9a8c\u542f\u53d1\u5f0f\u65b9\u6cd5\uff0c\u7f3a\u4e4f\u7406\u8bba\u652f\u6301\u3002", "method": "\u63d0\u51faSlim-SC\u65b9\u6cd5\uff1a1\uff09\u7406\u8bba\u5206\u6790SC\u7684\u4f4e\u6548\u6027\u5e76\u8bc6\u522b\u6539\u8fdb\u673a\u4f1a\uff1b2\uff09\u5728\u601d\u7ef4\u5c42\u9762\u4f7f\u7528\u94fe\u95f4\u76f8\u4f3c\u6027\u8bc6\u522b\u5197\u4f59\u94fe\uff1b3\uff09\u91c7\u7528\u9010\u6b65\u526a\u679d\u7b56\u7565\u79fb\u9664\u5197\u4f59\u94fe", "result": "\u5728\u4e09\u4e2aSTEM\u63a8\u7406\u6570\u636e\u96c6\u548c\u4e24\u79cdLLM\u67b6\u6784\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff1a\u63a8\u7406\u5ef6\u8fdf\u964d\u4f4e\u9ad8\u8fbe45%\uff0cKVC\u4f7f\u7528\u51cf\u5c1126%\uff08\u4f7f\u7528R1-Distill\uff09\uff0c\u540c\u65f6\u4fdd\u6301\u6216\u63d0\u9ad8\u51c6\u786e\u6027", "conclusion": "Slim-SC\u4e3aSC\u63d0\u4f9b\u4e86\u4e00\u79cd\u7b80\u5355\u800c\u9ad8\u6548\u7684\u6d4b\u8bd5\u65f6\u7f29\u653e\u66ff\u4ee3\u65b9\u6848\uff0c\u663e\u8457\u964d\u4f4e\u4e86\u8ba1\u7b97\u5f00\u9500", "relevance": 85.0}}
{"id": "2509.13590", "pdf": "https://arxiv.org/pdf/2509.13590", "abs": "https://arxiv.org/abs/2509.13590", "authors": ["Samer Al-Hamadani"], "title": "Intelligent Healthcare Imaging Platform An VLM-Based Framework for Automated Medical Image Analysis and Clinical Report Generation", "categories": ["cs.CV", "cs.AI"], "comment": "32 pages, 14 figures, 6 tables", "summary": "The rapid advancement of artificial intelligence (AI) in healthcare imaging\nhas revolutionized diagnostic medicine and clinical decision-making processes.\nThis work presents an intelligent multimodal framework for medical image\nanalysis that leverages Vision-Language Models (VLMs) in healthcare\ndiagnostics. The framework integrates Google Gemini 2.5 Flash for automated\ntumor detection and clinical report generation across multiple imaging\nmodalities including CT, MRI, X-ray, and Ultrasound. The system combines visual\nfeature extraction with natural language processing to enable contextual image\ninterpretation, incorporating coordinate verification mechanisms and\nprobabilistic Gaussian modeling for anomaly distribution. Multi-layered\nvisualization techniques generate detailed medical illustrations, overlay\ncomparisons, and statistical representations to enhance clinical confidence,\nwith location measurement achieving 80 pixels average deviation. Result\nprocessing utilizes precise prompt engineering and textual analysis to extract\nstructured clinical information while maintaining interpretability.\nExperimental evaluations demonstrated high performance in anomaly detection\nacross multiple modalities. The system features a user-friendly Gradio\ninterface for clinical workflow integration and demonstrates zero-shot learning\ncapabilities to reduce dependence on large datasets. This framework represents\na significant advancement in automated diagnostic support and radiological\nworkflow efficiency, though clinical validation and multi-center evaluation are\nnecessary prior to widespread adoption.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u4e2a\u57fa\u4e8eVision-Language Models\u7684\u591a\u6a21\u6001\u533b\u7597\u5f71\u50cf\u5206\u6790\u6846\u67b6\uff0c\u96c6\u6210Google Gemini 2.5 Flash\u8fdb\u884c\u80bf\u7624\u68c0\u6d4b\u548c\u4e34\u5e8a\u62a5\u544a\u751f\u6210\uff0c\u652f\u6301CT\u3001MRI\u3001X-ray\u548c\u8d85\u58f0\u7b49\u591a\u79cd\u6210\u50cf\u6a21\u6001\u3002", "motivation": "\u533b\u7597\u5f71\u50cfAI\u7684\u5feb\u901f\u53d1\u5c55\u9700\u8981\u66f4\u667a\u80fd\u7684\u591a\u6a21\u6001\u5206\u6790\u6846\u67b6\uff0c\u4ee5\u63d0\u5347\u8bca\u65ad\u51c6\u786e\u6027\u548c\u4e34\u5e8a\u51b3\u7b56\u6548\u7387\uff0c\u51cf\u5c11\u5bf9\u5927\u578b\u6570\u636e\u96c6\u7684\u4f9d\u8d56\u3002", "method": "\u7ed3\u5408\u89c6\u89c9\u7279\u5f81\u63d0\u53d6\u548c\u81ea\u7136\u8bed\u8a00\u5904\u7406\uff0c\u91c7\u7528\u5750\u6807\u9a8c\u8bc1\u673a\u5236\u548c\u9ad8\u65af\u6982\u7387\u5efa\u6a21\u8fdb\u884c\u5f02\u5e38\u5206\u5e03\u5206\u6790\uff0c\u4f7f\u7528\u591a\u5c42\u53ef\u89c6\u5316\u6280\u672f\u548c\u7cbe\u786e\u7684\u63d0\u793a\u5de5\u7a0b\u3002", "result": "\u5728\u591a\u6a21\u6001\u5f02\u5e38\u68c0\u6d4b\u4e2d\u8868\u73b0\u4f18\u5f02\uff0c\u4f4d\u7f6e\u6d4b\u91cf\u5e73\u5747\u504f\u5dee80\u50cf\u7d20\uff0c\u5177\u5907\u96f6\u6837\u672c\u5b66\u4e60\u80fd\u529b\uff0c\u7528\u6237\u53cb\u597d\u7684Gradio\u754c\u9762\u4fbf\u4e8e\u4e34\u5e8a\u5de5\u4f5c\u6d41\u96c6\u6210\u3002", "conclusion": "\u8be5\u6846\u67b6\u5728\u81ea\u52a8\u5316\u8bca\u65ad\u652f\u6301\u548c\u653e\u5c04\u5de5\u4f5c\u6d41\u6548\u7387\u65b9\u9762\u6709\u663e\u8457\u8fdb\u5c55\uff0c\u4f46\u9700\u8981\u4e34\u5e8a\u9a8c\u8bc1\u548c\u591a\u4e2d\u5fc3\u8bc4\u4f30\u624d\u80fd\u5e7f\u6cdb\u5e94\u7528\u3002", "relevance": 45.0}}
{"id": "2509.13818", "pdf": "https://arxiv.org/pdf/2509.13818", "abs": "https://arxiv.org/abs/2509.13818", "authors": ["Zheng-an Wang", "Yanbo J. Wang", "Jiachi Zhang", "Qi Xu", "Yilun Zhao", "Jintao Li", "Yipeng Zhang", "Bo Yang", "Xinkai Gao", "Xiaofeng Cao", "Kai Xu", "Pengpeng Hao", "Xuan Yang", "Heng Fan"], "title": "Hybrid Quantum-Classical Neural Networks for Few-Shot Credit Risk Assessment", "categories": ["cs.LG", "quant-ph"], "comment": null, "summary": "Quantum Machine Learning (QML) offers a new paradigm for addressing complex\nfinancial problems intractable for classical methods. This work specifically\ntackles the challenge of few-shot credit risk assessment, a critical issue in\ninclusive finance where data scarcity and imbalance limit the effectiveness of\nconventional models. To address this, we design and implement a novel hybrid\nquantum-classical workflow. The methodology first employs an ensemble of\nclassical machine learning models (Logistic Regression, Random Forest, XGBoost)\nfor intelligent feature engineering and dimensionality reduction. Subsequently,\na Quantum Neural Network (QNN), trained via the parameter-shift rule, serves as\nthe core classifier. This framework was evaluated through numerical simulations\nand deployed on the Quafu Quantum Cloud Platform's ScQ-P21 superconducting\nprocessor. On a real-world credit dataset of 279 samples, our QNN achieved a\nrobust average AUC of 0.852 +/- 0.027 in simulations and yielded an impressive\nAUC of 0.88 in the hardware experiment. This performance surpasses a suite of\nclassical benchmarks, with a particularly strong result on the recall metric.\nThis study provides a pragmatic blueprint for applying quantum computing to\ndata-constrained financial scenarios in the NISQ era and offers valuable\nempirical evidence supporting its potential in high-stakes applications like\ninclusive finance.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u6df7\u5408\u91cf\u5b50-\u7ecf\u5178\u5de5\u4f5c\u6d41\uff0c\u7528\u4e8e\u5c11\u6837\u672c\u4fe1\u7528\u98ce\u9669\u8bc4\u4f30\uff0c\u5728\u91cf\u5b50\u795e\u7ecf\u7f51\u7edc\u4e0a\u5b9e\u73b0\u4e86\u4f18\u4e8e\u7ecf\u5178\u57fa\u51c6\u7684\u6027\u80fd\u3002", "motivation": "\u89e3\u51b3\u5305\u5bb9\u6027\u91d1\u878d\u4e2d\u6570\u636e\u7a00\u7f3a\u548c\u5931\u8861\u5bfc\u81f4\u7684\u4fe1\u7528\u98ce\u9669\u8bc4\u4f30\u96be\u9898\uff0c\u63a2\u7d22\u91cf\u5b50\u673a\u5668\u5b66\u4e60\u5728\u91d1\u878d\u9886\u57df\u7684\u5e94\u7528\u6f5c\u529b\u3002", "method": "\u91c7\u7528\u7ecf\u5178\u673a\u5668\u5b66\u4e60\u6a21\u578b\u8fdb\u884c\u7279\u5f81\u5de5\u7a0b\u548c\u964d\u7ef4\uff0c\u7136\u540e\u4f7f\u7528\u53c2\u6570\u4f4d\u79fb\u89c4\u5219\u8bad\u7ec3\u7684\u91cf\u5b50\u795e\u7ecf\u7f51\u7edc\u4f5c\u4e3a\u6838\u5fc3\u5206\u7c7b\u5668\uff0c\u5728Quafu\u91cf\u5b50\u4e91\u5e73\u53f0\u4e0a\u8fdb\u884c\u5b9e\u9a8c\u3002", "result": "\u5728279\u4e2a\u6837\u672c\u7684\u771f\u5b9e\u4fe1\u7528\u6570\u636e\u96c6\u4e0a\uff0c\u91cf\u5b50\u795e\u7ecf\u7f51\u7edc\u5728\u6a21\u62df\u4e2d\u83b7\u5f970.852\u7684\u5e73\u5747AUC\uff0c\u786c\u4ef6\u5b9e\u9a8c\u4e2d\u83b7\u5f970.88\u7684AUC\uff0c\u8d85\u8d8a\u4e86\u7ecf\u5178\u57fa\u51c6\u6a21\u578b\u3002", "conclusion": "\u4e3aNISQ\u65f6\u4ee3\u6570\u636e\u53d7\u9650\u7684\u91d1\u878d\u573a\u666f\u63d0\u4f9b\u4e86\u91cf\u5b50\u8ba1\u7b97\u5e94\u7528\u7684\u5b9e\u7528\u84dd\u56fe\uff0c\u8bc1\u660e\u4e86\u91cf\u5b50\u673a\u5668\u5b66\u4e60\u5728\u9ad8\u98ce\u9669\u91d1\u878d\u5e94\u7528\u4e2d\u7684\u6f5c\u529b\u3002", "relevance": 25.0}}
{"id": "2509.14030", "pdf": "https://arxiv.org/pdf/2509.14030", "abs": "https://arxiv.org/abs/2509.14030", "authors": ["Maosheng Qin", "Renyu Zhu", "Mingxuan Xia", "Chenkai Chen", "Zhen Zhu", "Minmin Lin", "Junbo Zhao", "Lu Xu", "Changjie Fan", "Runze Wu", "Haobo Wang"], "title": "CrowdAgent: Multi-Agent Managed Multi-Source Annotation System", "categories": ["cs.AI"], "comment": null, "summary": "High-quality annotated data is a cornerstone of modern Natural Language\nProcessing (NLP). While recent methods begin to leverage diverse annotation\nsources-including Large Language Models (LLMs), Small Language Models (SLMs),\nand human experts-they often focus narrowly on the labeling step itself. A\ncritical gap remains in the holistic process control required to manage these\nsources dynamically, addressing complex scheduling and quality-cost trade-offs\nin a unified manner. Inspired by real-world crowdsourcing companies, we\nintroduce CrowdAgent, a multi-agent system that provides end-to-end process\ncontrol by integrating task assignment, data annotation, and quality/cost\nmanagement. It implements a novel methodology that rationally assigns tasks,\nenabling LLMs, SLMs, and human experts to advance synergistically in a\ncollaborative annotation workflow. We demonstrate the effectiveness of\nCrowdAgent through extensive experiments on six diverse multimodal\nclassification tasks. The source code and video demo are available at\nhttps://github.com/QMMMS/CrowdAgent.", "AI": {"tldr": "CrowdAgent\u662f\u4e00\u4e2a\u591a\u667a\u80fd\u4f53\u7cfb\u7edf\uff0c\u901a\u8fc7\u6574\u5408\u4efb\u52a1\u5206\u914d\u3001\u6570\u636e\u6807\u6ce8\u548c\u8d28\u91cf/\u6210\u672c\u7ba1\u7406\uff0c\u4e3aLLMs\u3001SLMs\u548c\u4eba\u7c7b\u4e13\u5bb6\u63d0\u4f9b\u7aef\u5230\u7aef\u7684\u534f\u540c\u6807\u6ce8\u6d41\u7a0b\u63a7\u5236\u3002", "motivation": "\u5f53\u524dNLP\u65b9\u6cd5\u867d\u7136\u5f00\u59cb\u5229\u7528LLMs\u3001SLMs\u548c\u4eba\u7c7b\u4e13\u5bb6\u7b49\u591a\u79cd\u6807\u6ce8\u6765\u6e90\uff0c\u4f46\u4e3b\u8981\u5173\u6ce8\u6807\u6ce8\u6b65\u9aa4\u672c\u8eab\uff0c\u7f3a\u4e4f\u5bf9\u590d\u6742\u8c03\u5ea6\u548c\u8d28\u91cf-\u6210\u672c\u6743\u8861\u7684\u7edf\u4e00\u52a8\u6001\u7ba1\u7406\u3002", "method": "\u63d0\u51faCrowdAgent\u591a\u667a\u80fd\u4f53\u7cfb\u7edf\uff0c\u91c7\u7528\u65b0\u9896\u7684\u4efb\u52a1\u5206\u914d\u65b9\u6cd5\uff0c\u4f7fLLMs\u3001SLMs\u548c\u4eba\u7c7b\u4e13\u5bb6\u80fd\u591f\u5728\u534f\u540c\u6807\u6ce8\u5de5\u4f5c\u6d41\u4e2d\u534f\u540c\u63a8\u8fdb\u3002", "result": "\u5728\u516d\u4e2a\u4e0d\u540c\u7684\u591a\u6a21\u6001\u5206\u7c7b\u4efb\u52a1\u4e0a\u8fdb\u884c\u4e86\u5e7f\u6cdb\u5b9e\u9a8c\uff0c\u8bc1\u660e\u4e86CrowdAgent\u7684\u6709\u6548\u6027\u3002", "conclusion": "CrowdAgent\u4e3a\u7ba1\u7406\u591a\u6837\u5316\u6807\u6ce8\u6765\u6e90\u63d0\u4f9b\u4e86\u7aef\u5230\u7aef\u7684\u6d41\u7a0b\u63a7\u5236\u89e3\u51b3\u65b9\u6848\uff0c\u89e3\u51b3\u4e86\u590d\u6742\u8c03\u5ea6\u548c\u8d28\u91cf-\u6210\u672c\u6743\u8861\u95ee\u9898\u3002", "relevance": 65.0}}
{"id": "2509.14004", "pdf": "https://arxiv.org/pdf/2509.14004", "abs": "https://arxiv.org/abs/2509.14004", "authors": ["Minjia Mao", "Bowen Yin", "Yu Zhu", "Xiao Fang"], "title": "Early Stopping Chain-of-thoughts in Large Language Models", "categories": ["cs.CL"], "comment": null, "summary": "Reasoning large language models (LLMs) have demonstrated superior capacities\nin solving complicated problems by generating long chain-of-thoughts (CoT), but\nsuch a lengthy CoT incurs high inference costs. In this study, we introduce\nES-CoT, an inference-time method that shortens CoT generation by detecting\nanswer convergence and stopping early with minimal performance loss. At the end\nof each reasoning step, we prompt the LLM to output its current final answer,\ndenoted as a step answer. We then track the run length of consecutive identical\nstep answers as a measure of answer convergence. Once the run length exhibits a\nsharp increase and exceeds a minimum threshold, the generation is terminated.\nWe provide both empirical and theoretical support for this heuristic: step\nanswers steadily converge to the final answer, and large run-length jumps\nreliably mark this convergence. Experiments on five reasoning datasets across\nthree LLMs show that ES-CoT reduces the number of inference tokens by about\n41\\% on average while maintaining accuracy comparable to standard CoT. Further,\nES-CoT integrates seamlessly with self-consistency prompting and remains robust\nacross hyperparameter choices, highlighting it as a practical and effective\napproach for efficient reasoning.", "AI": {"tldr": "ES-CoT\u662f\u4e00\u79cd\u63a8\u7406\u65f6\u65b9\u6cd5\uff0c\u901a\u8fc7\u68c0\u6d4b\u7b54\u6848\u6536\u655b\u6027\u6765\u63d0\u524d\u505c\u6b62\u601d\u7ef4\u94fe\u751f\u6210\uff0c\u5728\u4fdd\u6301\u51c6\u786e\u6027\u7684\u540c\u65f6\u5e73\u5747\u51cf\u5c1141%\u7684\u63a8\u7406token\u4f7f\u7528", "motivation": "\u73b0\u6709\u7684\u601d\u7ef4\u94fe\u63a8\u7406\u65b9\u6cd5\u751f\u6210\u957f\u94fe\u601d\u7ef4\u8fc7\u7a0b\uff0c\u5bfc\u81f4\u9ad8\u6602\u7684\u63a8\u7406\u6210\u672c\u3002\u9700\u8981\u4e00\u79cd\u65b9\u6cd5\u80fd\u591f\u5728\u4fdd\u6301\u6027\u80fd\u7684\u540c\u65f6\u51cf\u5c11\u63a8\u7406\u5f00\u9500", "method": "\u5728\u6bcf\u4e2a\u63a8\u7406\u6b65\u9aa4\u7ed3\u675f\u65f6\u63d0\u793aLLM\u8f93\u51fa\u5f53\u524d\u6700\u7ec8\u7b54\u6848\uff08\u6b65\u9aa4\u7b54\u6848\uff09\uff0c\u8ddf\u8e2a\u8fde\u7eed\u76f8\u540c\u6b65\u9aa4\u7b54\u6848\u7684\u8fd0\u884c\u957f\u5ea6\u4f5c\u4e3a\u6536\u655b\u5ea6\u91cf\u3002\u5f53\u8fd0\u884c\u957f\u5ea6\u51fa\u73b0\u6025\u5267\u589e\u52a0\u5e76\u8d85\u8fc7\u6700\u5c0f\u9608\u503c\u65f6\u7ec8\u6b62\u751f\u6210", "result": "\u57285\u4e2a\u63a8\u7406\u6570\u636e\u96c6\u548c3\u4e2aLLM\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0cES-CoT\u5e73\u5747\u51cf\u5c11\u7ea641%\u7684\u63a8\u7406token\uff0c\u540c\u65f6\u4fdd\u6301\u4e0e\u6807\u51c6\u601d\u7ef4\u94fe\u76f8\u5f53\u7684\u51c6\u786e\u6027\u3002\u8be5\u65b9\u6cd5\u8fd8\u80fd\u4e0e\u81ea\u4e00\u81f4\u6027\u63d0\u793a\u65e0\u7f1d\u96c6\u6210", "conclusion": "ES-CoT\u662f\u4e00\u79cd\u5b9e\u7528\u6709\u6548\u7684\u63a8\u7406\u6548\u7387\u63d0\u5347\u65b9\u6cd5\uff0c\u901a\u8fc7\u68c0\u6d4b\u7b54\u6848\u6536\u655b\u6027\u5b9e\u73b0\u65e9\u671f\u505c\u6b62\uff0c\u5728\u4fdd\u6301\u6027\u80fd\u7684\u540c\u65f6\u663e\u8457\u964d\u4f4e\u63a8\u7406\u6210\u672c", "relevance": 85.0}}
{"id": "2509.13605", "pdf": "https://arxiv.org/pdf/2509.13605", "abs": "https://arxiv.org/abs/2509.13605", "authors": ["Ruochen Hou", "Gabriel I. Fernandez", "Alex Xu", "Dennis W. Hong"], "title": "A Generalization of CLAP from 3D Localization to Image Processing, A Connection With RANSAC & Hough Transforms", "categories": ["cs.CV", "cs.RO"], "comment": null, "summary": "In previous work, we introduced a 2D localization algorithm called CLAP,\nClustering to Localize Across $n$ Possibilities, which was used during our\nchampionship win in RoboCup 2024, an international autonomous humanoid soccer\ncompetition. CLAP is particularly recognized for its robustness against\noutliers, where clustering is employed to suppress noise and mitigate against\nerroneous feature matches. This clustering-based strategy provides an\nalternative to traditional outlier rejection schemes such as RANSAC, in which\ncandidates are validated by reprojection error across all data points. In this\npaper, CLAP is extended to a more general framework beyond 2D localization,\nspecifically to 3D localization and image stitching. We also show how CLAP,\nRANSAC, and Hough transforms are related. The generalization of CLAP is widely\napplicable to many different fields and can be a useful tool to deal with noise\nand uncertainty.", "AI": {"tldr": "CLAP\u7b97\u6cd5\u4ece2D\u5b9a\u4f4d\u6269\u5c55\u52303D\u5b9a\u4f4d\u548c\u56fe\u50cf\u62fc\u63a5\u7684\u901a\u7528\u6846\u67b6\uff0c\u901a\u8fc7\u805a\u7c7b\u65b9\u6cd5\u5904\u7406\u566a\u58f0\u548c\u5f02\u5e38\u503c\uff0c\u4e0eRANSAC\u548c\u970d\u592b\u53d8\u6362\u76f8\u5173", "motivation": "\u6269\u5c55CLAP\u7b97\u6cd5\u7684\u5e94\u7528\u8303\u56f4\uff0c\u4ece\u7279\u5b9a\u76842D\u5b9a\u4f4d\u95ee\u9898\u63a8\u5e7f\u5230\u66f4\u4e00\u822c\u76843D\u5b9a\u4f4d\u548c\u56fe\u50cf\u62fc\u63a5\u4efb\u52a1\uff0c\u63d0\u4f9b\u4e00\u79cd\u5904\u7406\u566a\u58f0\u548c\u4e0d\u786e\u5b9a\u6027\u7684\u901a\u7528\u5de5\u5177", "method": "\u57fa\u4e8e\u805a\u7c7b\u7684\u5b9a\u4f4d\u65b9\u6cd5\uff0c\u4f7f\u7528\u805a\u7c7b\u6765\u6291\u5236\u566a\u58f0\u548c\u7f13\u89e3\u9519\u8bef\u7279\u5f81\u5339\u914d\uff0c\u4f5c\u4e3a\u4f20\u7edf\u5f02\u5e38\u503c\u62d2\u7edd\u65b9\u6848\uff08\u5982RANSAC\uff09\u7684\u66ff\u4ee3\u65b9\u6cd5", "result": "\u6210\u529f\u5c06CLAP\u7b97\u6cd5\u6269\u5c55\u52303D\u5b9a\u4f4d\u548c\u56fe\u50cf\u62fc\u63a5\u9886\u57df\uff0c\u8bc1\u660e\u4e86\u8be5\u65b9\u6cd5\u5728\u4e0d\u540c\u5e94\u7528\u4e2d\u7684\u5e7f\u6cdb\u9002\u7528\u6027", "conclusion": "CLAP\u7684\u6cdb\u5316\u6846\u67b6\u53ef\u4ee5\u5e94\u7528\u4e8e\u8bb8\u591a\u4e0d\u540c\u9886\u57df\uff0c\u662f\u5904\u7406\u566a\u58f0\u548c\u4e0d\u786e\u5b9a\u6027\u7684\u6709\u7528\u5de5\u5177", "relevance": 15.0}}
{"id": "2509.13841", "pdf": "https://arxiv.org/pdf/2509.13841", "abs": "https://arxiv.org/abs/2509.13841", "authors": ["Qingqi Zhao", "Heng Xiao"], "title": "An End-to-End Differentiable, Graph Neural Network-Embedded Pore Network Model for Permeability Prediction", "categories": ["cs.LG", "physics.geo-ph"], "comment": "This preprint is also available at ESS Open Archive:\n  https://essopenarchive.org/users/960205/articles/1329010", "summary": "Accurate prediction of permeability in porous media is essential for modeling\nsubsurface flow. While pure data-driven models offer computational efficiency,\nthey often lack generalization across scales and do not incorporate explicit\nphysical constraints. Pore network models (PNMs), on the other hand, are\nphysics-based and efficient but rely on idealized geometric assumptions to\nestimate pore-scale hydraulic conductance, limiting their accuracy in complex\nstructures. To overcome these limitations, we present an end-to-end\ndifferentiable hybrid framework that embeds a graph neural network (GNN) into a\nPNM. In this framework, the analytical formulas used for conductance\ncalculations are replaced by GNN-based predictions derived from pore and throat\nfeatures. The predicted conductances are then passed to the PNM solver for\npermeability computation. In this way, the model avoids the idealized geometric\nassumptions of PNM while preserving the physics-based flow calculations. The\nGNN is trained without requiring labeled conductance data, which can number in\nthe thousands per pore network; instead, it learns conductance values by using\na single scalar permeability as the training target. This is made possible by\nbackpropagating gradients through both the GNN (via automatic differentiation)\nand the PNM solver (via a discrete adjoint method), enabling fully coupled,\nend-to-end training. The resulting model achieves high accuracy and generalizes\nwell across different scales, outperforming both pure data-driven and\ntraditional PNM approaches. Gradient-based sensitivity analysis further reveals\nphysically consistent feature influences, enhancing model interpretability.\nThis approach offers a scalable and physically informed framework for\npermeability prediction in complex porous media, reducing model uncertainty and\nimproving accuracy.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u7aef\u5230\u7aef\u53ef\u5fae\u5206\u6df7\u5408\u6846\u67b6\uff0c\u5c06\u56fe\u795e\u7ecf\u7f51\u7edc\u5d4c\u5165\u5b54\u9699\u7f51\u7edc\u6a21\u578b\u4e2d\uff0c\u7528\u4e8e\u591a\u5b54\u4ecb\u8d28\u6e17\u900f\u7387\u9884\u6d4b\uff0c\u907f\u514d\u4e86\u4f20\u7edf\u65b9\u6cd5\u7684\u7406\u60f3\u5316\u51e0\u4f55\u5047\u8bbe\uff0c\u540c\u65f6\u4fdd\u6301\u4e86\u7269\u7406\u57fa\u7840\u7684\u6d41\u52a8\u8ba1\u7b97\u3002", "motivation": "\u4f20\u7edf\u7eaf\u6570\u636e\u9a71\u52a8\u6a21\u578b\u7f3a\u4e4f\u8de8\u5c3a\u5ea6\u6cdb\u5316\u80fd\u529b\u4e14\u4e0d\u5305\u542b\u660e\u786e\u7269\u7406\u7ea6\u675f\uff0c\u800c\u5b54\u9699\u7f51\u7edc\u6a21\u578b\u867d\u7136\u57fa\u4e8e\u7269\u7406\u4f46\u4f9d\u8d56\u4e8e\u7406\u60f3\u5316\u51e0\u4f55\u5047\u8bbe\u6765\u4f30\u8ba1\u5b54\u9699\u5c3a\u5ea6\u6c34\u529b\u4f20\u5bfc\u5ea6\uff0c\u9650\u5236\u4e86\u5728\u590d\u6742\u7ed3\u6784\u4e2d\u7684\u51c6\u786e\u6027\u3002", "method": "\u4f7f\u7528\u56fe\u795e\u7ecf\u7f51\u7edc\u66ff\u4ee3\u5b54\u9699\u7f51\u7edc\u6a21\u578b\u4e2d\u7684\u89e3\u6790\u516c\u5f0f\u8fdb\u884c\u4f20\u5bfc\u5ea6\u8ba1\u7b97\uff0c\u901a\u8fc7\u7aef\u5230\u7aef\u8bad\u7ec3\uff08\u7ed3\u5408\u81ea\u52a8\u5fae\u5206\u548c\u79bb\u6563\u4f34\u968f\u65b9\u6cd5\uff09\u4ec5\u4f7f\u7528\u5355\u4e00\u6807\u91cf\u6e17\u900f\u7387\u4f5c\u4e3a\u8bad\u7ec3\u76ee\u6807\uff0c\u65e0\u9700\u6807\u8bb0\u7684\u4f20\u5bfc\u5ea6\u6570\u636e\u3002", "result": "\u8be5\u6a21\u578b\u5b9e\u73b0\u4e86\u9ad8\u7cbe\u5ea6\u548c\u826f\u597d\u7684\u8de8\u5c3a\u5ea6\u6cdb\u5316\u80fd\u529b\uff0c\u4f18\u4e8e\u7eaf\u6570\u636e\u9a71\u52a8\u548c\u4f20\u7edf\u5b54\u9699\u7f51\u7edc\u6a21\u578b\u65b9\u6cd5\uff0c\u68af\u5ea6\u654f\u611f\u6027\u5206\u6790\u663e\u793a\u51fa\u7269\u7406\u4e00\u81f4\u7684\u7279\u5f81\u5f71\u54cd\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u4e3a\u590d\u6742\u591a\u5b54\u4ecb\u8d28\u4e2d\u7684\u6e17\u900f\u7387\u9884\u6d4b\u63d0\u4f9b\u4e86\u4e00\u4e2a\u53ef\u6269\u5c55\u4e14\u7269\u7406\u4fe1\u606f\u5316\u7684\u6846\u67b6\uff0c\u51cf\u5c11\u4e86\u6a21\u578b\u4e0d\u786e\u5b9a\u6027\u5e76\u63d0\u9ad8\u4e86\u51c6\u786e\u6027\u3002", "relevance": 25.0}}
{"id": "2509.14195", "pdf": "https://arxiv.org/pdf/2509.14195", "abs": "https://arxiv.org/abs/2509.14195", "authors": ["Shalima Binta Manir", "Tim Oates"], "title": "Hierarchical Learning for Maze Navigation: Emergence of Mental Representations via Second-Order Learning", "categories": ["cs.AI", "cs.LG"], "comment": "8 pages, 3 figures", "summary": "Mental representation, characterized by structured internal models mirroring\nexternal environments, is fundamental to advanced cognition but remains\nchallenging to investigate empirically. Existing theory hypothesizes that\nsecond-order learning -- learning mechanisms that adapt first-order learning\n(i.e., learning about the task/domain) -- promotes the emergence of such\nenvironment-cognition isomorphism. In this paper, we empirically validate this\nhypothesis by proposing a hierarchical architecture comprising a Graph\nConvolutional Network (GCN) as a first-order learner and an MLP controller as a\nsecond-order learner. The GCN directly maps node-level features to predictions\nof optimal navigation paths, while the MLP dynamically adapts the GCN's\nparameters when confronting structurally novel maze environments. We\ndemonstrate that second-order learning is particularly effective when the\ncognitive system develops an internal mental map structurally isomorphic to the\nenvironment. Quantitative and qualitative results highlight significant\nperformance improvements and robust generalization on unseen maze tasks,\nproviding empirical support for the pivotal role of structured mental\nrepresentations in maximizing the effectiveness of second-order learning.", "AI": {"tldr": "\u8be5\u8bba\u6587\u901a\u8fc7\u5c42\u6b21\u67b6\u6784\u9a8c\u8bc1\u4e86\u4e8c\u9636\u5b66\u4e60\u4fc3\u8fdb\u73af\u5883-\u8ba4\u77e5\u540c\u6784\u6027\u5f62\u6210\u7684\u5047\u8bbe\uff0c\u5176\u4e2dGCN\u4f5c\u4e3a\u4e00\u9636\u5b66\u4e60\u5668\u8fdb\u884c\u8def\u5f84\u9884\u6d4b\uff0cMLP\u63a7\u5236\u5668\u4f5c\u4e3a\u4e8c\u9636\u5b66\u4e60\u5668\u52a8\u6001\u8c03\u6574GCN\u53c2\u6570\uff0c\u5728\u8ff7\u5bab\u4efb\u52a1\u4e2d\u5c55\u73b0\u4e86\u6027\u80fd\u63d0\u5347\u548c\u6cdb\u5316\u80fd\u529b\u3002", "motivation": "\u63a2\u7d22\u5fc3\u667a\u8868\u5f81\uff08\u5373\u5185\u90e8\u6a21\u578b\u4e0e\u5916\u90e8\u73af\u5883\u7684\u7ed3\u6784\u540c\u6784\uff09\u5982\u4f55\u901a\u8fc7\u4e8c\u9636\u5b66\u4e60\u673a\u5236\uff08\u8c03\u6574\u4e00\u9636\u5b66\u4e60\uff09\u6765\u4fc3\u8fdb\u9ad8\u7ea7\u8ba4\u77e5\uff0c\u5e76\u5bf9\u6b64\u5047\u8bbe\u8fdb\u884c\u5b9e\u8bc1\u9a8c\u8bc1\u3002", "method": "\u63d0\u51fa\u5206\u5c42\u67b6\u6784\uff1a\u4f7f\u7528\u56fe\u5377\u79ef\u7f51\u7edc\uff08GCN\uff09\u4f5c\u4e3a\u4e00\u9636\u5b66\u4e60\u5668\u76f4\u63a5\u9884\u6d4b\u6700\u4f18\u5bfc\u822a\u8def\u5f84\uff0cMLP\u63a7\u5236\u5668\u4f5c\u4e3a\u4e8c\u9636\u5b66\u4e60\u5668\u5728\u9047\u5230\u7ed3\u6784\u65b0\u9896\u7684\u8ff7\u5bab\u73af\u5883\u65f6\u52a8\u6001\u8c03\u6574GCN\u53c2\u6570\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0c\u5f53\u8ba4\u77e5\u7cfb\u7edf\u53d1\u5c55\u51fa\u4e0e\u73af\u5883\u7ed3\u6784\u540c\u6784\u7684\u5185\u90e8\u5fc3\u667a\u5730\u56fe\u65f6\uff0c\u4e8c\u9636\u5b66\u4e60\u7279\u522b\u6709\u6548\uff0c\u5728\u672a\u89c1\u8fc7\u7684\u8ff7\u5bab\u4efb\u52a1\u4e0a\u5b9e\u73b0\u4e86\u663e\u8457\u7684\u6027\u80fd\u63d0\u5347\u548c\u9c81\u68d2\u6cdb\u5316\u3002", "conclusion": "\u7814\u7a76\u4e3a\u7ed3\u6784\u5316\u5fc3\u667a\u8868\u5f81\u5728\u6700\u5927\u5316\u4e8c\u9636\u5b66\u4e60\u6548\u679c\u4e2d\u7684\u5173\u952e\u4f5c\u7528\u63d0\u4f9b\u4e86\u5b9e\u8bc1\u652f\u6301\uff0c\u9a8c\u8bc1\u4e86\u73af\u5883-\u8ba4\u77e5\u540c\u6784\u6027\u5047\u8bf4\u3002", "relevance": 40.0}}
{"id": "2509.14008", "pdf": "https://arxiv.org/pdf/2509.14008", "abs": "https://arxiv.org/abs/2509.14008", "authors": ["Hasan Abed Al Kader Hammoud", "Mohammad Zbeeb", "Bernard Ghanem"], "title": "Hala Technical Report: Building Arabic-Centric Instruction & Translation Models at Scale", "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": "Technical Report", "summary": "We present Hala, a family of Arabic-centric instruction and translation\nmodels built with our translate-and-tune pipeline. We first compress a strong\nAR$\\leftrightarrow$EN teacher to FP8 (yielding $\\sim$2$\\times$ higher\nthroughput with no quality loss) and use it to create high-fidelity bilingual\nsupervision. A lightweight language model LFM2-1.2B is then fine-tuned on this\ndata and used to translate high-quality English instruction sets into Arabic,\nproducing a million-scale corpus tailored to instruction following. We train\nHala models at 350M, 700M, 1.2B, and 9B parameters, and apply slerp merging to\nbalance Arabic specialization with base-model strengths. On Arabic-centric\nbenchmarks, Hala achieves state-of-the-art results within both the \"nano\"\n($\\leq$2B) and \"small\" (7-9B) categories, outperforming their bases. We release\nmodels, data, evaluation, and recipes to accelerate research in Arabic NLP.", "AI": {"tldr": "Hala\u662f\u4e00\u4e2a\u963f\u62c9\u4f2f\u8bed\u4e3a\u4e2d\u5fc3\u7684\u6307\u4ee4\u548c\u7ffb\u8bd1\u6a21\u578b\u5bb6\u65cf\uff0c\u901a\u8fc7\u7ffb\u8bd1-\u8c03\u4f18\u6d41\u7a0b\u6784\u5efa\uff0c\u5728\u963f\u62c9\u4f2f\u8bed\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u53d6\u5f97\u4e86\u6700\u5148\u8fdb\u7684\u7ed3\u679c\u3002", "motivation": "\u9488\u5bf9\u963f\u62c9\u4f2f\u8bedNLP\u9886\u57df\u7f3a\u4e4f\u9ad8\u8d28\u91cf\u6307\u4ee4\u6570\u636e\u96c6\u548c\u4e13\u95e8\u6a21\u578b\u7684\u95ee\u9898\uff0c\u5f00\u53d1\u80fd\u591f\u9ad8\u6548\u5904\u7406\u963f\u62c9\u4f2f\u8bed\u6307\u4ee4\u548c\u7ffb\u8bd1\u4efb\u52a1\u7684\u6a21\u578b\u5bb6\u65cf\u3002", "method": "\u4f7f\u7528FP8\u538b\u7f29\u7684AR-EN\u6559\u5e08\u6a21\u578b\u751f\u6210\u9ad8\u8d28\u91cf\u53cc\u8bed\u76d1\u7763\u6570\u636e\uff0c\u5728\u8f7b\u91cf\u7ea7LFM2-1.2B\u6a21\u578b\u4e0a\u8fdb\u884c\u5fae\u8c03\uff0c\u7ffb\u8bd1\u82f1\u6587\u6307\u4ee4\u96c6\u4e3a\u963f\u62c9\u4f2f\u8bed\uff0c\u521b\u5efa\u767e\u4e07\u7ea7\u6307\u4ee4\u8bed\u6599\u5e93\uff0c\u8bad\u7ec3\u4e0d\u540c\u53c2\u6570\u89c4\u6a21\u7684Hala\u6a21\u578b\uff0c\u5e76\u5e94\u7528slerp\u5408\u5e76\u6280\u672f\u5e73\u8861\u963f\u62c9\u4f2f\u8bed\u4e13\u4e1a\u5316\u548c\u57fa\u7840\u6a21\u578b\u4f18\u52bf\u3002", "result": "\u5728\u963f\u62c9\u4f2f\u8bed\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0cHala\u5728\"nano\"(\u22642B)\u548c\"small\"(7-9B)\u7c7b\u522b\u4e2d\u90fd\u53d6\u5f97\u4e86\u6700\u5148\u8fdb\u7684\u7ed3\u679c\uff0c\u8d85\u8d8a\u4e86\u5176\u57fa\u7840\u6a21\u578b\u3002", "conclusion": "Hala\u6a21\u578b\u4e3a\u963f\u62c9\u4f2f\u8bedNLP\u7814\u7a76\u63d0\u4f9b\u4e86\u6709\u6548\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u901a\u8fc7\u521b\u65b0\u7684\u7ffb\u8bd1-\u8c03\u4f18\u6d41\u7a0b\u548c\u6a21\u578b\u67b6\u6784\u4f18\u5316\uff0c\u663e\u8457\u63d0\u5347\u4e86\u963f\u62c9\u4f2f\u8bed\u6307\u4ee4\u8ddf\u968f\u548c\u7ffb\u8bd1\u4efb\u52a1\u7684\u6027\u80fd\u3002", "relevance": 65.0}}
{"id": "2509.13629", "pdf": "https://arxiv.org/pdf/2509.13629", "abs": "https://arxiv.org/abs/2509.13629", "authors": ["Yue He", "Min Liu", "Qinghao Liu", "Jiazheng Wang", "Yaonan Wang", "Hang Zhang", "Xiang Chen"], "title": "SAMIR, an efficient registration framework via robust feature learning from SAM", "categories": ["cs.CV"], "comment": null, "summary": "Image registration is a fundamental task in medical image analysis.\nDeformations are often closely related to the morphological characteristics of\ntissues, making accurate feature extraction crucial. Recent weakly supervised\nmethods improve registration by incorporating anatomical priors such as\nsegmentation masks or landmarks, either as inputs or in the loss function.\nHowever, such weak labels are often not readily available, limiting their\npractical use. Motivated by the strong representation learning ability of\nvisual foundation models, this paper introduces SAMIR, an efficient medical\nimage registration framework that utilizes the Segment Anything Model (SAM) to\nenhance feature extraction. SAM is pretrained on large-scale natural image\ndatasets and can learn robust, general-purpose visual representations. Rather\nthan using raw input images, we design a task-specific adaptation pipeline\nusing SAM's image encoder to extract structure-aware feature embeddings,\nenabling more accurate modeling of anatomical consistency and deformation\npatterns. We further design a lightweight 3D head to refine features within the\nembedding space, adapting to local deformations in medical images.\nAdditionally, we introduce a Hierarchical Feature Consistency Loss to guide\ncoarse-to-fine feature matching and improve anatomical alignment. Extensive\nexperiments demonstrate that SAMIR significantly outperforms state-of-the-art\nmethods on benchmark datasets for both intra-subject cardiac image registration\nand inter-subject abdomen CT image registration, achieving performance\nimprovements of 2.68% on ACDC and 6.44% on the abdomen dataset. The source code\nwill be publicly available on GitHub following the acceptance of this paper.", "AI": {"tldr": "SAMIR\u662f\u4e00\u4e2a\u5229\u7528Segment Anything Model (SAM)\u8fdb\u884c\u533b\u5b66\u56fe\u50cf\u914d\u51c6\u7684\u9ad8\u6548\u6846\u67b6\uff0c\u901a\u8fc7SAM\u63d0\u53d6\u7ed3\u6784\u611f\u77e5\u7279\u5f81\u5d4c\u5165\uff0c\u7ed3\u5408\u8f7b\u91cf\u7ea73D\u5934\u90e8\u548c\u5206\u5c42\u7279\u5f81\u4e00\u81f4\u6027\u635f\u5931\uff0c\u5728\u5fc3\u810f\u548c\u8179\u90e8CT\u56fe\u50cf\u914d\u51c6\u4efb\u52a1\u4e2d\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "motivation": "\u533b\u5b66\u56fe\u50cf\u914d\u51c6\u4e2d\u53d8\u5f62\u4e0e\u7ec4\u7ec7\u5f62\u6001\u7279\u5f81\u5bc6\u5207\u76f8\u5173\uff0c\u9700\u8981\u51c6\u786e\u7684\u7279\u5f81\u63d0\u53d6\u3002\u73b0\u6709\u7684\u5f31\u76d1\u7763\u65b9\u6cd5\u4f9d\u8d56\u5206\u5272\u63a9\u7801\u6216\u5730\u6807\u7b49\u89e3\u5256\u5148\u9a8c\uff0c\u4f46\u8fd9\u4e9b\u6807\u7b7e\u901a\u5e38\u96be\u4ee5\u83b7\u53d6\uff0c\u9650\u5236\u4e86\u5b9e\u9645\u5e94\u7528\u3002\u53d7\u89c6\u89c9\u57fa\u7840\u6a21\u578b\u5f3a\u5927\u8868\u793a\u5b66\u4e60\u80fd\u529b\u7684\u542f\u53d1\uff0c\u5229\u7528SAM\u6765\u589e\u5f3a\u7279\u5f81\u63d0\u53d6\u3002", "method": "\u8bbe\u8ba1\u4efb\u52a1\u7279\u5b9a\u7684\u9002\u5e94\u7ba1\u9053\uff0c\u4f7f\u7528SAM\u7684\u56fe\u50cf\u7f16\u7801\u5668\u63d0\u53d6\u7ed3\u6784\u611f\u77e5\u7279\u5f81\u5d4c\u5165\uff1b\u8bbe\u8ba1\u8f7b\u91cf\u7ea73D\u5934\u90e8\u5728\u5d4c\u5165\u7a7a\u95f4\u5185\u7ec6\u5316\u7279\u5f81\uff1b\u5f15\u5165\u5206\u5c42\u7279\u5f81\u4e00\u81f4\u6027\u635f\u5931\u6307\u5bfc\u4ece\u7c97\u5230\u7ec6\u7684\u7279\u5f81\u5339\u914d\u3002", "result": "\u5728\u5fc3\u810f\u56fe\u50cf\u914d\u51c6(ACDC\u6570\u636e\u96c6)\u4e0a\u6027\u80fd\u63d0\u53472.68%\uff0c\u5728\u8179\u90e8CT\u56fe\u50cf\u914d\u51c6\u4e0a\u6027\u80fd\u63d0\u53476.44%\uff0c\u663e\u8457\u4f18\u4e8e\u6700\u5148\u8fdb\u65b9\u6cd5\u3002", "conclusion": "SAMIR\u6846\u67b6\u6210\u529f\u5229\u7528\u9884\u8bad\u7ec3\u7684\u89c6\u89c9\u57fa\u7840\u6a21\u578bSAM\u6765\u589e\u5f3a\u533b\u5b66\u56fe\u50cf\u914d\u51c6\u7684\u7279\u5f81\u63d0\u53d6\u80fd\u529b\uff0c\u8bc1\u660e\u4e86\u57fa\u7840\u6a21\u578b\u5728\u533b\u5b66\u56fe\u50cf\u5206\u6790\u4efb\u52a1\u4e2d\u7684\u6709\u6548\u8fc1\u79fb\u548c\u5e94\u7528\u4ef7\u503c\u3002", "relevance": 35.0}}
{"id": "2509.13855", "pdf": "https://arxiv.org/pdf/2509.13855", "abs": "https://arxiv.org/abs/2509.13855", "authors": ["Shamsiiat Abdurakhmanova", "Alex Jung"], "title": "Graph-Regularized Learning of Gaussian Mixture Models", "categories": ["cs.LG", "cs.DC"], "comment": null, "summary": "We present a graph-regularized learning of Gaussian Mixture Models (GMMs) in\ndistributed settings with heterogeneous and limited local data. The method\nexploits a provided similarity graph to guide parameter sharing among nodes,\navoiding the transfer of raw data. The resulting model allows for flexible\naggregation of neighbors' parameters and outperforms both centralized and\nlocally trained GMMs in heterogeneous, low-sample regimes.", "AI": {"tldr": "\u5206\u5e03\u5f0f\u56fe\u6b63\u5219\u5316\u9ad8\u65af\u6df7\u5408\u6a21\u578b\u5b66\u4e60\uff0c\u5229\u7528\u76f8\u4f3c\u6027\u56fe\u6307\u5bfc\u8282\u70b9\u95f4\u53c2\u6570\u5171\u4eab\uff0c\u5728\u5f02\u6784\u5c0f\u6837\u672c\u573a\u666f\u4e0b\u4f18\u4e8e\u96c6\u4e2d\u5f0f\u548c\u672c\u5730\u8bad\u7ec3\u65b9\u6cd5", "motivation": "\u89e3\u51b3\u5206\u5e03\u5f0f\u73af\u5883\u4e2d\u6570\u636e\u5f02\u6784\u4e14\u6837\u672c\u6709\u9650\u7684\u60c5\u51b5\u4e0b\uff0c\u5982\u4f55\u6709\u6548\u5b66\u4e60\u9ad8\u65af\u6df7\u5408\u6a21\u578b\u800c\u4e0d\u9700\u8981\u4f20\u8f93\u539f\u59cb\u6570\u636e\u7684\u95ee\u9898", "method": "\u57fa\u4e8e\u56fe\u6b63\u5219\u5316\u7684\u9ad8\u65af\u6df7\u5408\u6a21\u578b\u5b66\u4e60\u65b9\u6cd5\uff0c\u5229\u7528\u8282\u70b9\u95f4\u7684\u76f8\u4f3c\u6027\u56fe\u6765\u6307\u5bfc\u53c2\u6570\u5171\u4eab\uff0c\u5b9e\u73b0\u7075\u6d3b\u7684\u90bb\u5c45\u53c2\u6570\u805a\u5408", "result": "\u5728\u5f02\u6784\u4f4e\u6837\u672c\u60c5\u51b5\u4e0b\uff0c\u8be5\u65b9\u6cd5\u6027\u80fd\u4f18\u4e8e\u96c6\u4e2d\u5f0f\u8bad\u7ec3\u548c\u672c\u5730\u8bad\u7ec3\u7684GMM\u6a21\u578b", "conclusion": "\u56fe\u6b63\u5219\u5316\u65b9\u6cd5\u4e3a\u5206\u5e03\u5f0f\u5f02\u6784\u6570\u636e\u5b66\u4e60\u63d0\u4f9b\u4e86\u6709\u6548\u7684\u53c2\u6570\u5171\u4eab\u673a\u5236\uff0c\u907f\u514d\u4e86\u539f\u59cb\u6570\u636e\u4f20\u8f93", "relevance": 25.0}}
{"id": "2010.01052", "pdf": "https://arxiv.org/pdf/2010.01052", "abs": "https://arxiv.org/abs/2010.01052", "authors": ["Jaume Banus", "Maxime Sermesant", "Oscar Camara", "Marco Lorenzi"], "title": "Joint data imputation and mechanistic modelling for simulating heart-brain interactions in incomplete datasets", "categories": ["cs.LG", "cs.AI", "stat.ML"], "comment": null, "summary": "The use of mechanistic models in clinical studies is limited by the lack of\nmulti-modal patients data representing different anatomical and physiological\nprocesses. For example, neuroimaging datasets do not provide a sufficient\nrepresentation of heart features for the modeling of cardiovascular factors in\nbrain disorders. To tackle this problem we introduce a probabilistic framework\nfor joint cardiac data imputation and personalisation of cardiovascular\nmechanistic models, with application to brain studies with incomplete heart\ndata. Our approach is based on a variational framework for the joint inference\nof an imputation model of cardiac information from the available features,\nalong with a Gaussian Process emulator that can faithfully reproduce\npersonalised cardiovascular dynamics. Experimental results on UK Biobank show\nthat our model allows accurate imputation of missing cardiac features in\ndatasets containing minimal heart information, e.g. systolic and diastolic\nblood pressures only, while jointly estimating the emulated parameters of the\nlumped model. This allows a novel exploration of the heart-brain joint\nrelationship through simulation of realistic cardiac dynamics corresponding to\ndifferent conditions of brain anatomy.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u4e2a\u6982\u7387\u6846\u67b6\uff0c\u7528\u4e8e\u8054\u5408\u5fc3\u810f\u6570\u636e\u586b\u8865\u548c\u5fc3\u8840\u7ba1\u673a\u5236\u6a21\u578b\u4e2a\u6027\u5316\uff0c\u7279\u522b\u9488\u5bf9\u8111\u7814\u7a76\u4e2d\u5fc3\u810f\u6570\u636e\u4e0d\u5b8c\u6574\u7684\u60c5\u51b5\u3002", "motivation": "\u89e3\u51b3\u4e34\u5e8a\u7814\u7a76\u4e2d\u591a\u6a21\u6001\u60a3\u8005\u6570\u636e\u7f3a\u4e4f\u7684\u95ee\u9898\uff0c\u7279\u522b\u662f\u795e\u7ecf\u5f71\u50cf\u6570\u636e\u96c6\u65e0\u6cd5\u5145\u5206\u4ee3\u8868\u5fc3\u810f\u7279\u5f81\u6765\u5efa\u6a21\u8111\u75be\u75c5\u4e2d\u7684\u5fc3\u8840\u7ba1\u56e0\u7d20\u3002", "method": "\u57fa\u4e8e\u53d8\u5206\u63a8\u65ad\u6846\u67b6\uff0c\u8054\u5408\u63a8\u65ad\u4ece\u53ef\u7528\u7279\u5f81\u4e2d\u586b\u8865\u5fc3\u810f\u4fe1\u606f\u7684\u6a21\u578b\uff0c\u4ee5\u53ca\u80fd\u591f\u5fe0\u5b9e\u518d\u73b0\u4e2a\u6027\u5316\u5fc3\u8840\u7ba1\u52a8\u6001\u7684\u9ad8\u65af\u8fc7\u7a0b\u4eff\u771f\u5668\u3002", "result": "\u5728UK Biobank\u6570\u636e\u96c6\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u80fd\u591f\u51c6\u786e\u586b\u8865\u4ec5\u5305\u542b\u6700\u5c0f\u5fc3\u810f\u4fe1\u606f\uff08\u5982\u6536\u7f29\u538b\u548c\u8212\u5f20\u538b\uff09\u7684\u6570\u636e\u96c6\u4e2d\u7f3a\u5931\u7684\u5fc3\u810f\u7279\u5f81\uff0c\u540c\u65f6\u8054\u5408\u4f30\u8ba1\u96c6\u603b\u6a21\u578b\u7684\u4eff\u771f\u53c2\u6570\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u901a\u8fc7\u6a21\u62df\u4e0e\u4e0d\u540c\u8111\u89e3\u5256\u6761\u4ef6\u76f8\u5bf9\u5e94\u7684\u771f\u5b9e\u5fc3\u810f\u52a8\u6001\uff0c\u4e3a\u63a2\u7d22\u5fc3\u8111\u8054\u5408\u5173\u7cfb\u63d0\u4f9b\u4e86\u65b0\u7684\u9014\u5f84\u3002", "relevance": 15.0}}
{"id": "2509.14023", "pdf": "https://arxiv.org/pdf/2509.14023", "abs": "https://arxiv.org/abs/2509.14023", "authors": ["Sami Ul Haq", "Sheila Castilho", "Yvette Graham"], "title": "Audio-Based Crowd-Sourced Evaluation of Machine Translation Quality", "categories": ["cs.CL", "cs.HC"], "comment": "Accepted at WMT2025 (ENNLP) for oral presented", "summary": "Machine Translation (MT) has achieved remarkable performance, with growing\ninterest in speech translation and multimodal approaches. However, despite\nthese advancements, MT quality assessment remains largely text centric,\ntypically relying on human experts who read and compare texts. Since many\nreal-world MT applications (e.g Google Translate Voice Mode, iFLYTEK\nTranslator) involve translation being spoken rather printed or read, a more\nnatural way to assess translation quality would be through speech as opposed\ntext-only evaluations. This study compares text-only and audio-based\nevaluations of 10 MT systems from the WMT General MT Shared Task, using\ncrowd-sourced judgments collected via Amazon Mechanical Turk. We additionally,\nperformed statistical significance testing and self-replication experiments to\ntest reliability and consistency of audio-based approach. Crowd-sourced\nassessments based on audio yield rankings largely consistent with text only\nevaluations but, in some cases, identify significant differences between\ntranslation systems. We attribute this to speech richer, more natural modality\nand propose incorporating speech-based assessments into future MT evaluation\nframeworks.", "AI": {"tldr": "\u8be5\u7814\u7a76\u6bd4\u8f83\u4e86\u6587\u672c\u548c\u97f3\u9891\u4e24\u79cd\u65b9\u5f0f\u5bf9\u673a\u5668\u7ffb\u8bd1\u7cfb\u7edf\u7684\u8bc4\u4f30\u6548\u679c\uff0c\u53d1\u73b0\u57fa\u4e8e\u97f3\u9891\u7684\u8bc4\u4f30\u80fd\u63d0\u4f9b\u66f4\u81ea\u7136\u7684\u8d28\u91cf\u5224\u65ad\uff0c\u5728\u67d0\u4e9b\u60c5\u51b5\u4e0b\u80fd\u8bc6\u522b\u51fa\u6587\u672c\u8bc4\u4f30\u65e0\u6cd5\u53d1\u73b0\u7684\u7cfb\u7edf\u5dee\u5f02\u3002", "motivation": "\u5f53\u524d\u673a\u5668\u7ffb\u8bd1\u8d28\u91cf\u8bc4\u4f30\u4e3b\u8981\u4f9d\u8d56\u6587\u672c\u65b9\u5f0f\uff0c\u4f46\u73b0\u5b9e\u5e94\u7528\u4e2d\u7ffb\u8bd1\u5e38\u4ee5\u8bed\u97f3\u5f62\u5f0f\u5448\u73b0\uff08\u5982\u8bed\u97f3\u7ffb\u8bd1\u5e94\u7528\uff09\uff0c\u9700\u8981\u63a2\u7d22\u66f4\u81ea\u7136\u7684\u8bed\u97f3\u8bc4\u4f30\u65b9\u5f0f\u3002", "method": "\u4f7f\u7528Amazon Mechanical Turk\u4f17\u5305\u5e73\u53f0\uff0c\u5bf9WMT General MT\u5171\u4eab\u4efb\u52a1\u768410\u4e2a\u7ffb\u8bd1\u7cfb\u7edf\u8fdb\u884c\u6587\u672c\u548c\u97f3\u9891\u4e24\u79cd\u65b9\u5f0f\u7684\u8bc4\u4f30\u6bd4\u8f83\uff0c\u5e76\u8fdb\u884c\u7edf\u8ba1\u663e\u8457\u6027\u68c0\u9a8c\u548c\u81ea\u590d\u5236\u5b9e\u9a8c\u9a8c\u8bc1\u53ef\u9760\u6027\u3002", "result": "\u97f3\u9891\u8bc4\u4f30\u5f97\u51fa\u7684\u6392\u540d\u4e0e\u6587\u672c\u8bc4\u4f30\u57fa\u672c\u4e00\u81f4\uff0c\u4f46\u5728\u67d0\u4e9b\u60c5\u51b5\u4e0b\u80fd\u8bc6\u522b\u51fa\u7ffb\u8bd1\u7cfb\u7edf\u95f4\u7684\u663e\u8457\u5dee\u5f02\uff0c\u8868\u660e\u8bed\u97f3\u6a21\u6001\u63d0\u4f9b\u4e86\u66f4\u4e30\u5bcc\u81ea\u7136\u7684\u8bc4\u4f30\u4fe1\u606f\u3002", "conclusion": "\u5efa\u8bae\u5c06\u57fa\u4e8e\u8bed\u97f3\u7684\u8bc4\u4f30\u7eb3\u5165\u672a\u6765\u673a\u5668\u7ffb\u8bd1\u8bc4\u4f30\u6846\u67b6\uff0c\u4ee5\u66f4\u597d\u5730\u53cd\u6620\u5b9e\u9645\u5e94\u7528\u573a\u666f\u4e2d\u7684\u7ffb\u8bd1\u8d28\u91cf\u3002", "relevance": 35.0}}
{"id": "2509.13631", "pdf": "https://arxiv.org/pdf/2509.13631", "abs": "https://arxiv.org/abs/2509.13631", "authors": ["Yuvraj Dutta", "Aaditya Sikder", "Basabdatta Palit"], "title": "Federated Learning for Deforestation Detection: A Distributed Approach with Satellite Imagery", "categories": ["cs.CV", "cs.DC", "14J60", "F.2.2; I.2.7"], "comment": "6 pages, 7 figures, accepted at IEEE INDISCON 2025", "summary": "Accurate identification of deforestation from satellite images is essential\nin order to understand the geographical situation of an area. This paper\nintroduces a new distributed approach to identify as well as locate\ndeforestation across different clients using Federated Learning (FL). Federated\nLearning enables distributed network clients to collaboratively train a model\nwhile maintaining data privacy and security of the active users. In our\nframework, a client corresponds to an edge satellite center responsible for\nlocal data processing. Moreover, FL provides an advantage over centralized\ntraining method which requires combining data, thereby compromising with data\nsecurity of the clients. Our framework leverages the FLOWER framework with RAY\nframework to execute the distributed learning workload. Furthermore, efficient\nclient spawning is ensured by RAY as it can select definite amount of users to\ncreate an emulation environment. Our FL framework uses YOLOS-small (a Vision\nTransformer variant), Faster R-CNN with a ResNet50 backbone, and Faster R-CNN\nwith a MobileNetV3 backbone models trained and tested on publicly available\ndatasets. Our approach provides us a different view for image\nsegmentation-based tasks on satellite imagery.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u8054\u90a6\u5b66\u4e60(FL)\u7684\u5206\u5e03\u5f0f\u65b9\u6cd5\uff0c\u5229\u7528\u536b\u661f\u56fe\u50cf\u8fdb\u884c\u68ee\u6797\u780d\u4f10\u8bc6\u522b\uff0c\u901a\u8fc7FLOWER\u548cRAY\u6846\u67b6\u5b9e\u73b0\u9690\u79c1\u4fdd\u62a4\u7684\u5206\u5e03\u5f0f\u8bad\u7ec3\uff0c\u6bd4\u8f83\u4e86YOLOS-small\u3001Faster R-CNN\u7b49\u6a21\u578b\u5728\u536b\u661f\u56fe\u50cf\u5206\u5272\u4efb\u52a1\u4e0a\u7684\u8868\u73b0\u3002", "motivation": "\u4f20\u7edf\u96c6\u4e2d\u5f0f\u8bad\u7ec3\u65b9\u6cd5\u9700\u8981\u5408\u5e76\u6570\u636e\uff0c\u4f1a\u635f\u5bb3\u5ba2\u6237\u7aef\u7684\u6570\u636e\u5b89\u5168\u548c\u9690\u79c1\u3002\u8054\u90a6\u5b66\u4e60\u80fd\u591f\u5728\u4fdd\u62a4\u6570\u636e\u9690\u79c1\u7684\u524d\u63d0\u4e0b\uff0c\u8ba9\u5206\u5e03\u5f0f\u7f51\u7edc\u5ba2\u6237\u7aef\u534f\u4f5c\u8bad\u7ec3\u6a21\u578b\uff0c\u7279\u522b\u9002\u5408\u536b\u661f\u56fe\u50cf\u5904\u7406\u8fd9\u79cd\u5bf9\u6570\u636e\u5b89\u5168\u8981\u6c42\u9ad8\u7684\u573a\u666f\u3002", "method": "\u4f7f\u7528FLOWER\u6846\u67b6\u548cRAY\u6846\u67b6\u6784\u5efa\u8054\u90a6\u5b66\u4e60\u7cfb\u7edf\uff0c\u5ba2\u6237\u7aef\u5bf9\u5e94\u8fb9\u7f18\u536b\u661f\u4e2d\u5fc3\u3002\u6bd4\u8f83\u4e86\u4e09\u79cd\u6a21\u578b\uff1aYOLOS-small\uff08Vision Transformer\u53d8\u4f53\uff09\u3001\u57fa\u4e8eResNet50\u7684Faster R-CNN\u3001\u57fa\u4e8eMobileNetV3\u7684Faster R-CNN\uff0c\u5728\u516c\u5f00\u6570\u636e\u96c6\u4e0a\u8fdb\u884c\u8bad\u7ec3\u548c\u6d4b\u8bd5\u3002", "result": "\u8be5\u65b9\u6cd5\u4e3a\u536b\u661f\u56fe\u50cf\u5206\u5272\u4efb\u52a1\u63d0\u4f9b\u4e86\u65b0\u7684\u89c6\u89d2\uff0c\u5b9e\u73b0\u4e86\u5728\u4fdd\u62a4\u6570\u636e\u9690\u79c1\u524d\u63d0\u4e0b\u7684\u5206\u5e03\u5f0f\u68ee\u6797\u780d\u4f10\u8bc6\u522b\u3002", "conclusion": "\u8054\u90a6\u5b66\u4e60\u4e3a\u536b\u661f\u56fe\u50cf\u5206\u6790\u63d0\u4f9b\u4e86\u4e00\u79cd\u6709\u6548\u7684\u9690\u79c1\u4fdd\u62a4\u89e3\u51b3\u65b9\u6848\uff0c\u7279\u522b\u662f\u5728\u9700\u8981\u591a\u5ba2\u6237\u7aef\u534f\u4f5c\u4f46\u53c8\u8981\u786e\u4fdd\u6570\u636e\u5b89\u5168\u7684\u573a\u666f\u4e2d\u5177\u6709\u4f18\u52bf\u3002", "relevance": 25.0}}
{"id": "2509.13866", "pdf": "https://arxiv.org/pdf/2509.13866", "abs": "https://arxiv.org/abs/2509.13866", "authors": ["Sitong Chen", "Shen Nie", "Jiacheng Sun", "Zijin Feng", "Zhenguo Li", "Ji-Rong Wen", "Chongxuan Li"], "title": "Masked Diffusion Models as Energy Minimization", "categories": ["cs.LG", "cs.AI"], "comment": null, "summary": "We present a systematic theoretical framework that interprets masked\ndiffusion models (MDMs) as solutions to energy minimization problems in\ndiscrete optimal transport. Specifically, we prove that three distinct energy\nformulations--kinetic, conditional kinetic, and geodesic energy--are\nmathematically equivalent under the structure of MDMs, and that MDMs minimize\nall three when the mask schedule satisfies a closed-form optimality condition.\nThis unification not only clarifies the theoretical foundations of MDMs, but\nalso motivates practical improvements in sampling. By parameterizing\ninterpolation schedules via Beta distributions, we reduce the schedule design\nspace to a tractable 2D search, enabling efficient post-training tuning without\nmodel modification. Experiments on synthetic and real-world benchmarks\ndemonstrate that our energy-inspired schedules outperform hand-crafted\nbaselines, particularly in low-step sampling settings.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u4e2a\u7406\u8bba\u6846\u67b6\uff0c\u5c06\u63a9\u7801\u6269\u6563\u6a21\u578b\u89e3\u91ca\u4e3a\u79bb\u6563\u6700\u4f18\u4f20\u8f93\u4e2d\u7684\u80fd\u91cf\u6700\u5c0f\u5316\u95ee\u9898\uff0c\u8bc1\u660e\u4e86\u4e09\u79cd\u80fd\u91cf\u516c\u5f0f\u7684\u6570\u5b66\u7b49\u4ef7\u6027\uff0c\u5e76\u901a\u8fc7Beta\u5206\u5e03\u53c2\u6570\u5316\u8c03\u5ea6\u5b9e\u73b0\u4e86\u9ad8\u6548\u7684\u540e\u8bad\u7ec3\u8c03\u4f18\u3002", "motivation": "\u7edf\u4e00\u63a9\u7801\u6269\u6563\u6a21\u578b\u7684\u7406\u8bba\u57fa\u7840\uff0c\u63ed\u793a\u5176\u4e0e\u80fd\u91cf\u6700\u5c0f\u5316\u95ee\u9898\u7684\u6570\u5b66\u8054\u7cfb\uff0c\u5e76\u4e3a\u5b9e\u9645\u91c7\u6837\u6539\u8fdb\u63d0\u4f9b\u7406\u8bba\u6307\u5bfc\u3002", "method": "\u5efa\u7acb\u7406\u8bba\u6846\u67b6\u8bc1\u660e\u4e09\u79cd\u80fd\u91cf\u516c\u5f0f\u7684\u7b49\u4ef7\u6027\uff0c\u4f7f\u7528Beta\u5206\u5e03\u53c2\u6570\u5316\u63d2\u503c\u8c03\u5ea6\uff0c\u5c06\u8c03\u5ea6\u8bbe\u8ba1\u7a7a\u95f4\u7b80\u5316\u4e3a2D\u641c\u7d22\uff0c\u5b9e\u73b0\u65e0\u9700\u6a21\u578b\u4fee\u6539\u7684\u540e\u8bad\u7ec3\u8c03\u4f18\u3002", "result": "\u5728\u5408\u6210\u548c\u771f\u5b9e\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0c\u80fd\u91cf\u542f\u53d1\u7684\u8c03\u5ea6\u65b9\u6848\u5728\u4f4e\u6b65\u91c7\u6837\u8bbe\u7f6e\u4e2d\u4f18\u4e8e\u624b\u5de5\u8bbe\u8ba1\u7684\u57fa\u7ebf\u65b9\u6cd5\u3002", "conclusion": "\u8be5\u5de5\u4f5c\u4e0d\u4ec5\u6f84\u6e05\u4e86MDMs\u7684\u7406\u8bba\u57fa\u7840\uff0c\u8fd8\u63d0\u4f9b\u4e86\u5b9e\u7528\u7684\u91c7\u6837\u6539\u8fdb\u65b9\u6cd5\uff0c\u901a\u8fc7\u7406\u8bba\u6307\u5bfc\u7684\u8c03\u5ea6\u8bbe\u8ba1\u63d0\u5347\u4e86\u6a21\u578b\u6027\u80fd\u3002", "relevance": 30.0}}
{"id": "2408.00208", "pdf": "https://arxiv.org/pdf/2408.00208", "abs": "https://arxiv.org/abs/2408.00208", "authors": ["SaeedReza Motamedian", "Sadra Mohaghegh", "Elham Babadi Oregani", "Mahrsa Amjadi", "Parnian Shobeiri", "Negin Cheraghi", "Niusha Solouki", "Nikoo Ahmadi", "Hossein Mohammad-Rahimi", "Yassine Bouchareb", "Arman Rahmim"], "title": "Prognosis of COVID-19 using Artificial Intelligence: A Systematic Review and Meta-analysis", "categories": ["physics.med-ph", "cs.AI", "cs.LG"], "comment": null, "summary": "Purpose: Artificial intelligence (AI) techniques have been extensively\nutilized for diagnosing and prognosis of several diseases in recent years. This\nstudy identifies, appraises and synthesizes published studies on the use of AI\nfor the prognosis of COVID-19. Method: Electronic search was performed using\nMedline, Google Scholar, Scopus, Embase, Cochrane and ProQuest. Studies that\nexamined machine learning or deep learning methods to determine the prognosis\nof COVID-19 using CT or chest X-ray images were included. Polled sensitivity,\nspecificity area under the curve and diagnostic odds ratio were calculated.\nResult: A total of 36 articles were included; various prognosis-related issues,\nincluding disease severity, mechanical ventilation or admission to the\nintensive care unit and mortality, were investigated. Several AI models and\narchitectures were employed, such as the Siamense model, support vector\nmachine, Random Forest , eXtreme Gradient Boosting, and convolutional neural\nnetworks. The models achieved 71%, 88% and 67% sensitivity for mortality,\nseverity assessment and need for ventilation, respectively. The specificity of\n69%, 89% and 89% were reported for the aforementioned variables. Conclusion:\nBased on the included articles, machine learning and deep learning methods used\nfor the prognosis of COVID-19 patients using radiomic features from CT or CXR\nimages can help clinicians manage patients and allocate resources more\neffectively. These studies also demonstrate that combining patient demographic,\nclinical data, laboratory tests and radiomic features improves model\nperformances.", "AI": {"tldr": "\u8be5\u8bba\u6587\u7cfb\u7edf\u7efc\u8ff0\u4e86AI\u6280\u672f\u5728COVID-19\u9884\u540e\u9884\u6d4b\u4e2d\u7684\u5e94\u7528\uff0c\u4e3b\u8981\u57fa\u4e8eCT\u548cX\u5149\u5f71\u50cf\u6570\u636e\uff0c\u5206\u6790\u4e8636\u9879\u7814\u7a76\uff0c\u8bc4\u4f30\u4e86\u591a\u79cd\u673a\u5668\u5b66\u4e60\u6a21\u578b\u5728\u6b7b\u4ea1\u7387\u3001\u75be\u75c5\u4e25\u91cd\u7a0b\u5ea6\u548c\u547c\u5438\u673a\u9700\u6c42\u9884\u6d4b\u65b9\u9762\u7684\u8868\u73b0\u3002", "motivation": "COVID-19\u5927\u6d41\u884c\u671f\u95f4\u9700\u8981\u6709\u6548\u7684\u9884\u540e\u9884\u6d4b\u5de5\u5177\u6765\u5e2e\u52a9\u4e34\u5e8a\u533b\u751f\u8fdb\u884c\u60a3\u8005\u7ba1\u7406\u548c\u8d44\u6e90\u5206\u914d\uff0cAI\u6280\u672f\u7279\u522b\u662f\u673a\u5668\u5b66\u4e60\u548c\u6df1\u5ea6\u5b66\u4e60\u65b9\u6cd5\u5728\u533b\u5b66\u5f71\u50cf\u5206\u6790\u65b9\u9762\u5c55\u73b0\u51fa\u5de8\u5927\u6f5c\u529b\u3002", "method": "\u7cfb\u7edf\u6027\u6587\u732e\u56de\u987e\uff0c\u68c0\u7d22\u4e86Medline\u3001Google Scholar\u3001Scopus\u7b49\u6570\u636e\u5e93\uff0c\u7eb3\u516536\u9879\u7814\u7a76\uff0c\u4f7f\u7528\u673a\u5668\u5b66\u4e60\uff08SVM\u3001Random Forest\u3001XGBoost\uff09\u548c\u6df1\u5ea6\u5b66\u4e60\uff08CNN\u3001Siamense\u6a21\u578b\uff09\u65b9\u6cd5\u5206\u6790CT\u548cX\u5149\u5f71\u50cf\u7684\u653e\u5c04\u7ec4\u5b66\u7279\u5f81\u3002", "result": "\u6a21\u578b\u5728\u6b7b\u4ea1\u7387\u9884\u6d4b\u654f\u611f\u602771%\u3001\u7279\u5f02\u602769%\uff1b\u75be\u75c5\u4e25\u91cd\u7a0b\u5ea6\u8bc4\u4f30\u654f\u611f\u602788%\u3001\u7279\u5f02\u602789%\uff1b\u547c\u5438\u673a\u9700\u6c42\u9884\u6d4b\u654f\u611f\u602767%\u3001\u7279\u5f02\u602789%\u3002\u7ed3\u5408\u60a3\u8005\u4eba\u53e3\u7edf\u8ba1\u5b66\u3001\u4e34\u5e8a\u6570\u636e\u548c\u5b9e\u9a8c\u5ba4\u68c0\u6d4b\u7ed3\u679c\u53ef\u8fdb\u4e00\u6b65\u63d0\u5347\u6a21\u578b\u6027\u80fd\u3002", "conclusion": "AI\u65b9\u6cd5\u57fa\u4e8e\u533b\u5b66\u5f71\u50cf\u7684\u653e\u5c04\u7ec4\u5b66\u7279\u5f81\u80fd\u591f\u6709\u6548\u9884\u6d4bCOVID-19\u60a3\u8005\u7684\u9884\u540e\uff0c\u6709\u52a9\u4e8e\u4e34\u5e8a\u51b3\u7b56\u548c\u8d44\u6e90\u4f18\u5316\u914d\u7f6e\uff0c\u591a\u6a21\u6001\u6570\u636e\u878d\u5408\u53ef\u663e\u8457\u6539\u5584\u9884\u6d4b\u51c6\u786e\u6027\u3002", "relevance": 25.0}}
{"id": "2509.14031", "pdf": "https://arxiv.org/pdf/2509.14031", "abs": "https://arxiv.org/abs/2509.14031", "authors": ["Pawe\u0142 M\u0105ka", "Yusuf Can Semerci", "Jan Scholtes", "Gerasimos Spanakis"], "title": "You Are What You Train: Effects of Data Composition on Training Context-aware Machine Translation Models", "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": "EMNLP 2025 main conference", "summary": "Achieving human-level translations requires leveraging context to ensure\ncoherence and handle complex phenomena like pronoun disambiguation. Sparsity of\ncontextually rich examples in the standard training data has been hypothesized\nas the reason for the difficulty of context utilization. In this work, we\nsystematically validate this claim in both single- and multilingual settings by\nconstructing training datasets with a controlled proportions of contextually\nrelevant examples. We demonstrate a strong association between training data\nsparsity and model performance confirming sparsity as a key bottleneck.\nImportantly, we reveal that improvements in one contextual phenomenon do no\ngeneralize to others. While we observe some cross-lingual transfer, it is not\nsignificantly higher between languages within the same sub-family. Finally, we\npropose and empirically evaluate two training strategies designed to leverage\nthe available data. These strategies improve context utilization, resulting in\naccuracy gains of up to 6 and 8 percentage points on the ctxPro evaluation in\nsingle- and multilingual settings respectively.", "AI": {"tldr": "\u8be5\u7814\u7a76\u9a8c\u8bc1\u4e86\u4e0a\u4e0b\u6587\u8bad\u7ec3\u6570\u636e\u7a00\u758f\u6027\u662f\u673a\u5668\u7ffb\u8bd1\u4e2d\u4e0a\u4e0b\u6587\u5229\u7528\u56f0\u96be\u7684\u4e3b\u8981\u539f\u56e0\uff0c\u53d1\u73b0\u4e0d\u540c\u4e0a\u4e0b\u6587\u73b0\u8c61\u4e4b\u95f4\u7f3a\u4e4f\u6cdb\u5316\u6027\uff0c\u5e76\u63d0\u51fa\u4e24\u79cd\u8bad\u7ec3\u7b56\u7565\u5728\u5355\u8bed\u548c\u591a\u8bed\u8bbe\u7f6e\u4e2d\u5206\u522b\u63d0\u53476%\u548c8%\u7684\u51c6\u786e\u7387", "motivation": "\u6807\u51c6\u8bad\u7ec3\u6570\u636e\u4e2d\u4e0a\u4e0b\u6587\u4e30\u5bcc\u6837\u672c\u7684\u7a00\u758f\u6027\u88ab\u8ba4\u4e3a\u662f\u673a\u5668\u7ffb\u8bd1\u96be\u4ee5\u6709\u6548\u5229\u7528\u4e0a\u4e0b\u6587\u7684\u4e3b\u8981\u539f\u56e0\uff0c\u7814\u7a76\u8005\u5e0c\u671b\u901a\u8fc7\u7cfb\u7edf\u9a8c\u8bc1\u8fd9\u4e00\u5047\u8bbe\u5e76\u63a2\u7d22\u89e3\u51b3\u65b9\u6848", "method": "\u6784\u5efa\u5177\u6709\u53d7\u63a7\u6bd4\u4f8b\u4e0a\u4e0b\u6587\u76f8\u5173\u6837\u672c\u7684\u8bad\u7ec3\u6570\u636e\u96c6\uff0c\u5728\u5355\u8bed\u548c\u591a\u8bed\u8bbe\u7f6e\u4e2d\u7cfb\u7edf\u9a8c\u8bc1\u6570\u636e\u7a00\u758f\u6027\u4e0e\u6a21\u578b\u6027\u80fd\u7684\u5173\u7cfb\uff0c\u5e76\u63d0\u51fa\u4e24\u79cd\u8bad\u7ec3\u7b56\u7565\u6765\u66f4\u597d\u5730\u5229\u7528\u53ef\u7528\u6570\u636e", "result": "\u7814\u7a76\u8bc1\u5b9e\u4e86\u8bad\u7ec3\u6570\u636e\u7a00\u758f\u6027\u4e0e\u6a21\u578b\u6027\u80fd\u4e4b\u95f4\u7684\u5f3a\u5173\u8054\uff0c\u53d1\u73b0\u4e0d\u540c\u4e0a\u4e0b\u6587\u73b0\u8c61\u7684\u6539\u8fdb\u65e0\u6cd5\u76f8\u4e92\u6cdb\u5316\uff0c\u8de8\u8bed\u8a00\u8fc1\u79fb\u6548\u679c\u6709\u9650\u3002\u63d0\u51fa\u7684\u8bad\u7ec3\u7b56\u7565\u5728ctxPro\u8bc4\u4f30\u4e2d\u5206\u522b\u5e26\u67656%\u548c8%\u7684\u51c6\u786e\u7387\u63d0\u5347", "conclusion": "\u4e0a\u4e0b\u6587\u6570\u636e\u7a00\u758f\u6027\u662f\u673a\u5668\u7ffb\u8bd1\u4e0a\u4e0b\u6587\u5229\u7528\u7684\u5173\u952e\u74f6\u9888\uff0c\u9700\u8981\u9488\u5bf9\u6027\u7684\u8bad\u7ec3\u7b56\u7565\u6765\u6539\u5584\uff0c\u4e14\u4e0d\u540c\u4e0a\u4e0b\u6587\u73b0\u8c61\u9700\u8981\u5206\u522b\u5904\u7406", "relevance": 45.0}}
{"id": "2509.13652", "pdf": "https://arxiv.org/pdf/2509.13652", "abs": "https://arxiv.org/abs/2509.13652", "authors": ["Yumin Li", "Dylan Campbell"], "title": "Gaussian Alignment for Relative Camera Pose Estimation via Single-View Reconstruction", "categories": ["cs.CV", "I.4.8; I.4.5"], "comment": "12 pages, 4 figures, accepted by AJCAI 2025", "summary": "Estimating metric relative camera pose from a pair of images is of great\nimportance for 3D reconstruction and localisation. However, conventional\ntwo-view pose estimation methods are not metric, with camera translation known\nonly up to a scale, and struggle with wide baselines and textureless or\nreflective surfaces. This paper introduces GARPS, a training-free framework\nthat casts this problem as the direct alignment of two independently\nreconstructed 3D scenes. GARPS leverages a metric monocular depth estimator and\na Gaussian scene reconstructor to obtain a metric 3D Gaussian Mixture Model\n(GMM) for each image. It then refines an initial pose from a feed-forward\ntwo-view pose estimator by optimising a differentiable GMM alignment objective.\nThis objective jointly considers geometric structure, view-independent colour,\nanisotropic covariance, and semantic feature consistency, and is robust to\nocclusions and texture-poor regions without requiring explicit 2D\ncorrespondences. Extensive experiments on the Real\\-Estate10K dataset\ndemonstrate that GARPS outperforms both classical and state-of-the-art\nlearning-based methods, including MASt3R. These results highlight the potential\nof bridging single-view perception with multi-view geometry to achieve robust\nand metric relative pose estimation.", "AI": {"tldr": "GARPS\u662f\u4e00\u4e2a\u65e0\u9700\u8bad\u7ec3\u7684\u4e24\u89c6\u56fe\u76f8\u673a\u4f4d\u59ff\u4f30\u8ba1\u6846\u67b6\uff0c\u901a\u8fc7\u76f4\u63a5\u5bf9\u9f50\u4e24\u4e2a\u72ec\u7acb\u91cd\u5efa\u76843D\u9ad8\u65af\u573a\u666f\u6765\u5b9e\u73b0\u5ea6\u91cf\u76f8\u5bf9\u4f4d\u59ff\u4f30\u8ba1\uff0c\u5728Real-Estate10K\u6570\u636e\u96c6\u4e0a\u8d85\u8d8a\u4e86\u73b0\u6709\u65b9\u6cd5\u3002", "motivation": "\u4f20\u7edf\u4e24\u89c6\u56fe\u4f4d\u59ff\u4f30\u8ba1\u65b9\u6cd5\u65e0\u6cd5\u63d0\u4f9b\u5ea6\u91cf\u5c3a\u5ea6\u4fe1\u606f\uff08\u76f8\u673a\u5e73\u79fb\u53ea\u6709\u5c3a\u5ea6\u672a\u77e5\uff09\uff0c\u4e14\u5728\u5bbd\u57fa\u7ebf\u548c\u7eb9\u7406\u7f3a\u5931\u533a\u57df\u8868\u73b0\u4e0d\u4f73\u3002\u9700\u8981\u4e00\u79cd\u80fd\u591f\u63d0\u4f9b\u5ea6\u91cf\u5c3a\u5ea6\u4fe1\u606f\u4e14\u5bf9\u7eb9\u7406\u7f3a\u5931\u533a\u57df\u9c81\u68d2\u7684\u65b9\u6cd5\u3002", "method": "\u4f7f\u7528\u5ea6\u91cf\u5355\u76ee\u6df1\u5ea6\u4f30\u8ba1\u5668\u548c\u9ad8\u65af\u573a\u666f\u91cd\u5efa\u5668\u4e3a\u6bcf\u5f20\u56fe\u50cf\u751f\u6210\u5ea6\u91cf3D\u9ad8\u65af\u6df7\u5408\u6a21\u578b(GMM)\uff0c\u7136\u540e\u901a\u8fc7\u4f18\u5316\u53ef\u5fae\u5206\u7684GMM\u5bf9\u9f50\u76ee\u6807\u6765\u7cbe\u5316\u521d\u59cb\u4f4d\u59ff\u4f30\u8ba1\uff0c\u8be5\u76ee\u6807\u7efc\u5408\u8003\u8651\u51e0\u4f55\u7ed3\u6784\u3001\u89c6\u89d2\u65e0\u5173\u989c\u8272\u3001\u5404\u5411\u5f02\u6027\u534f\u65b9\u5dee\u548c\u8bed\u4e49\u7279\u5f81\u4e00\u81f4\u6027\u3002", "result": "\u5728Real-Estate10K\u6570\u636e\u96c6\u4e0a\u7684\u5927\u91cf\u5b9e\u9a8c\u8868\u660e\uff0cGARPS\u8d85\u8d8a\u4e86\u4f20\u7edf\u65b9\u6cd5\u548c\u6700\u5148\u8fdb\u7684\u5b66\u4e60\u65b9\u6cd5\uff08\u5305\u62ecMASt3R\uff09\uff0c\u5b9e\u73b0\u4e86\u66f4\u597d\u7684\u5ea6\u91cf\u76f8\u5bf9\u4f4d\u59ff\u4f30\u8ba1\u6027\u80fd\u3002", "conclusion": "\u901a\u8fc7\u6865\u63a5\u5355\u89c6\u56fe\u611f\u77e5\u548c\u591a\u89c6\u56fe\u51e0\u4f55\uff0c\u53ef\u4ee5\u5b9e\u73b0\u9c81\u68d2\u4e14\u5ea6\u91cf\u7684\u76f8\u5bf9\u4f4d\u59ff\u4f30\u8ba1\uff0c\u5c55\u793a\u4e86\u8fd9\u79cd\u65b9\u6cd5\u7684\u6f5c\u529b\u3002", "relevance": 15.0}}
{"id": "2509.13895", "pdf": "https://arxiv.org/pdf/2509.13895", "abs": "https://arxiv.org/abs/2509.13895", "authors": ["Zhanting Zhou", "Jinshan Lai", "Fengchun Zhang", "Zeqin Wu", "Fengli Zhang"], "title": "FedSSG: Expectation-Gated and History-Aware Drift Alignment for Federated Learning", "categories": ["cs.LG", "cs.AI"], "comment": "4 page main text for conference", "summary": "Non-IID data and partial participation induce client drift and inconsistent\nlocal optima in federated learning, causing unstable convergence and accuracy\nloss. We present FedSSG, a stochastic sampling-guided, history-aware drift\nalignment method. FedSSG maintains a per-client drift memory that accumulates\nlocal model differences as a lightweight sketch of historical gradients;\ncrucially, it gates both the memory update and the local alignment term by a\nsmooth function of the observed/expected participation ratio (a\nphase-by-expectation signal derived from the server sampler). This\nstatistically grounded gate stays weak and smooth when sampling noise dominates\nearly, then strengthens once participation statistics stabilize, contracting\nthe local-global gap without extra communication. Across CIFAR-10/100 with\n100/500 clients and 2-15 percent participation, FedSSG consistently outperforms\nstrong drift-aware baselines and accelerates convergence; on our benchmarks it\nimproves test accuracy by up to a few points (e.g., about +0.9 on CIFAR-10 and\nabout +2.7 on CIFAR-100 on average over the top-2 baseline) and yields about\n4.5x faster target-accuracy convergence on average. The method adds only O(d)\nclient memory and a constant-time gate, and degrades gracefully to a mild\nregularizer under near-IID or uniform sampling. FedSSG shows that sampling\nstatistics can be turned into a principled, history-aware phase control to\nstabilize and speed up federated training.", "AI": {"tldr": "FedSSG\u662f\u4e00\u79cd\u8054\u90a6\u5b66\u4e60\u65b9\u6cd5\uff0c\u901a\u8fc7\u5386\u53f2\u611f\u77e5\u7684\u6f02\u79fb\u5bf9\u9f50\u673a\u5236\uff0c\u5229\u7528\u968f\u673a\u91c7\u6837\u6307\u5bfc\u6765\u89e3\u51b3\u975eIID\u6570\u636e\u548c\u90e8\u5206\u53c2\u4e0e\u5bfc\u81f4\u7684\u5ba2\u6237\u7aef\u6f02\u79fb\u95ee\u9898\uff0c\u63d0\u9ad8\u6536\u655b\u901f\u5ea6\u548c\u51c6\u786e\u6027\u3002", "motivation": "\u8054\u90a6\u5b66\u4e60\u4e2d\u975eIID\u6570\u636e\u548c\u90e8\u5206\u5ba2\u6237\u7aef\u53c2\u4e0e\u4f1a\u5bfc\u81f4\u5ba2\u6237\u7aef\u6f02\u79fb\u548c\u4e0d\u4e00\u81f4\u7684\u5c40\u90e8\u6700\u4f18\u89e3\uff0c\u9020\u6210\u6536\u655b\u4e0d\u7a33\u5b9a\u548c\u51c6\u786e\u7387\u4e0b\u964d\u3002\u9700\u8981\u4e00\u79cd\u65b9\u6cd5\u6765\u5bf9\u9f50\u5ba2\u6237\u7aef\u6f02\u79fb\u800c\u4e0d\u589e\u52a0\u989d\u5916\u901a\u4fe1\u5f00\u9500\u3002", "method": "FedSSG\u7ef4\u62a4\u6bcf\u4e2a\u5ba2\u6237\u7aef\u7684\u6f02\u79fb\u8bb0\u5fc6\uff0c\u7d2f\u79ef\u5c40\u90e8\u6a21\u578b\u5dee\u5f02\u4f5c\u4e3a\u5386\u53f2\u68af\u5ea6\u7684\u8f7b\u91cf\u7ea7\u8349\u56fe\u3002\u4f7f\u7528\u57fa\u4e8e\u53c2\u4e0e\u7387\u7684\u5e73\u6ed1\u95e8\u63a7\u51fd\u6570\u6765\u63a7\u5236\u8bb0\u5fc6\u66f4\u65b0\u548c\u5c40\u90e8\u5bf9\u9f50\u9879\uff0c\u65e9\u671f\u91c7\u6837\u566a\u58f0\u5927\u65f6\u95e8\u63a7\u8f83\u5f31\uff0c\u540e\u671f\u7edf\u8ba1\u7a33\u5b9a\u65f6\u52a0\u5f3a\u95e8\u63a7\u3002", "result": "\u5728CIFAR-10/100\u6570\u636e\u96c6\u4e0a\uff0c100/500\u4e2a\u5ba2\u6237\u7aef\uff0c2-15%\u53c2\u4e0e\u7387\u4e0b\uff0cFedSSG\u6bd4\u57fa\u7ebf\u65b9\u6cd5\u63d0\u5347\u6d4b\u8bd5\u51c6\u786e\u7387\u7ea60.9-2.7\u4e2a\u767e\u5206\u70b9\uff0c\u76ee\u6807\u51c6\u786e\u7387\u6536\u655b\u901f\u5ea6\u5e73\u5747\u52a0\u5feb\u7ea64.5\u500d\u3002", "conclusion": "FedSSG\u8bc1\u660e\u91c7\u6837\u7edf\u8ba1\u53ef\u4ee5\u8f6c\u5316\u4e3a\u6709\u539f\u5219\u7684\u5386\u53f2\u611f\u77e5\u76f8\u4f4d\u63a7\u5236\uff0c\u7a33\u5b9a\u5e76\u52a0\u901f\u8054\u90a6\u8bad\u7ec3\u3002\u65b9\u6cd5\u53ea\u589e\u52a0O(d)\u5ba2\u6237\u7aef\u5185\u5b58\u548c\u5e38\u6570\u65f6\u95f4\u95e8\u63a7\uff0c\u5728\u8fd1IID\u6216\u5747\u5300\u91c7\u6837\u4e0b\u4f18\u96c5\u9000\u5316\u4e3a\u6e29\u548c\u6b63\u5219\u5316\u5668\u3002", "relevance": 35.0}}
{"id": "2509.13328", "pdf": "https://arxiv.org/pdf/2509.13328", "abs": "https://arxiv.org/abs/2509.13328", "authors": ["Danish Rizvi", "David Boyle"], "title": "Dual Actor DDPG for Airborne STAR-RIS Assisted Communications", "categories": ["eess.SP", "cs.AI", "cs.IT", "cs.NI", "math.IT"], "comment": null, "summary": "This study departs from the prevailing assumption of independent Transmission\nand Reflection Coefficients (TRC) in Airborne Simultaneous Transmit and Reflect\nReconfigurable Intelligent Surface (STAR-RIS) research. Instead, we explore a\nnovel multi-user downlink communication system that leverages a UAV-mounted\nSTAR-RIS (Aerial-STAR) incorporating a coupled TRC phase shift model. Our key\ncontributions include the joint optimization of UAV trajectory, active\nbeamforming vectors at the base station, and passive RIS TRCs to enhance\ncommunication efficiency, while considering UAV energy constraints. We design\nthe TRC as a combination of discrete and continuous actions, and propose a\nnovel Dual Actor Deep Deterministic Policy Gradient (DA-DDPG) algorithm. The\nalgorithm relies on two separate actor networks for high-dimensional hybrid\naction space. We also propose a novel harmonic mean index (HFI)-based reward\nfunction to ensure communication fairness amongst users. For comprehensive\nanalysis, we study the impact of RIS size on UAV aerodynamics showing that it\nincreases drag and energy demand. Simulation results demonstrate that the\nproposed DA-DDPG algorithm outperforms conventional DDPG and DQN-based\nsolutions by 24% and 97%, respectively, in accumulated reward.\nThree-dimensional UAV trajectory optimization achieves 28% higher communication\nefficiency compared to two-dimensional and altitude optimization. The HFI based\nreward function provides 41% lower QoS denial rates as compared to other\nbenchmarks. The mobile Aerial-STAR system shows superior performance over fixed\ndeployed counterparts, with the coupled phase STAR-RIS outperforming dual\nTransmit/Reflect RIS and conventional RIS setups. These findings highlight the\npotential of Aerial-STAR systems and the effectiveness of our proposed DA-DDPG\napproach in optimizing their performance.", "AI": {"tldr": "\u8be5\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u65e0\u4eba\u673a\u8f7dSTAR-RIS\u7684\u901a\u4fe1\u7cfb\u7edf\uff0c\u91c7\u7528\u8026\u5408TRC\u76f8\u4f4d\u504f\u79fb\u6a21\u578b\uff0c\u5e76\u5f00\u53d1\u4e86DA-DDPG\u7b97\u6cd5\u6765\u8054\u5408\u4f18\u5316\u65e0\u4eba\u673a\u8f68\u8ff9\u3001\u6ce2\u675f\u6210\u5f62\u548cRIS\u914d\u7f6e\uff0c\u663e\u8457\u63d0\u5347\u4e86\u901a\u4fe1\u6548\u7387\u548c\u516c\u5e73\u6027\u3002", "motivation": "\u73b0\u6709STAR-RIS\u7814\u7a76\u901a\u5e38\u5047\u8bbe\u72ec\u7acb\u7684\u4f20\u8f93\u548c\u53cd\u5c04\u7cfb\u6570\uff0c\u4f46\u5b9e\u9645\u4e2d\u5b58\u5728\u8026\u5408\u5173\u7cfb\u3002\u672c\u7814\u7a76\u65e8\u5728\u63a2\u7d22\u66f4\u771f\u5b9e\u7684\u8026\u5408TRC\u6a21\u578b\uff0c\u5e76\u89e3\u51b3\u65e0\u4eba\u673a\u8f7dRIS\u7cfb\u7edf\u4e2d\u7684\u8054\u5408\u4f18\u5316\u95ee\u9898\u3002", "method": "\u63d0\u51faDA-DDPG\u7b97\u6cd5\uff0c\u4f7f\u7528\u53ccactor\u7f51\u7edc\u5904\u7406\u9ad8\u7ef4\u6df7\u5408\u52a8\u4f5c\u7a7a\u95f4\uff0c\u8bbe\u8ba1\u4e86\u57fa\u4e8e\u8c03\u548c\u5e73\u5747\u6307\u6570\u7684\u5956\u52b1\u51fd\u6570\u6765\u4fdd\u8bc1\u7528\u6237\u516c\u5e73\u6027\uff0c\u5e76\u8054\u5408\u4f18\u5316\u65e0\u4eba\u673a\u8f68\u8ff9\u3001\u57fa\u7ad9\u6ce2\u675f\u6210\u5f62\u548c\u88ab\u52a8RIS\u914d\u7f6e\u3002", "result": "DA-DDPG\u7b97\u6cd5\u6bd4\u4f20\u7edfDDPG\u548cDQN\u5206\u522b\u63d0\u534724%\u548c97%\u7684\u7d2f\u79ef\u5956\u52b1\uff1b3D\u8f68\u8ff9\u4f18\u5316\u6bd42D\u548c\u9ad8\u5ea6\u4f18\u5316\u63d0\u534728%\u901a\u4fe1\u6548\u7387\uff1bHFI\u5956\u52b1\u51fd\u6570\u964d\u4f4e41%QoS\u62d2\u7edd\u7387\uff1b\u79fb\u52a8Aerial-STAR\u7cfb\u7edf\u4f18\u4e8e\u56fa\u5b9a\u90e8\u7f72\u65b9\u6848\u3002", "conclusion": "Aerial-STAR\u7cfb\u7edf\u5177\u6709\u5de8\u5927\u6f5c\u529b\uff0c\u63d0\u51fa\u7684DA-DDPG\u65b9\u6cd5\u5728\u4f18\u5316\u6027\u80fd\u65b9\u9762\u975e\u5e38\u6709\u6548\uff0c\u8026\u5408\u76f8\u4f4dSTAR-RIS\u4f18\u4e8e\u4f20\u7edfRIS\u914d\u7f6e\u3002", "relevance": 15.0}}
{"id": "2509.14034", "pdf": "https://arxiv.org/pdf/2509.14034", "abs": "https://arxiv.org/abs/2509.14034", "authors": ["Zijie Lin", "Bryan Hooi"], "title": "Enhancing Multi-Agent Debate System Performance via Confidence Expression", "categories": ["cs.CL"], "comment": "EMNLP'25 Findings", "summary": "Generative Large Language Models (LLMs) have demonstrated remarkable\nperformance across a wide range of tasks. Recent research has introduced\nMulti-Agent Debate (MAD) systems, which leverage multiple LLMs to simulate\nhuman debate and thereby improve task performance. However, while some LLMs may\npossess superior knowledge or reasoning capabilities for specific tasks, they\noften struggle to clearly communicate this advantage during debates, in part\ndue to a lack of confidence expression. Moreover, inappropriate confidence\nexpression can cause agents in MAD systems to either stubbornly maintain\nincorrect beliefs or converge prematurely on suboptimal answers, ultimately\nreducing debate effectiveness and overall system performance. To address these\nchallenges, we propose incorporating confidence expression into MAD systems to\nallow LLMs to explicitly communicate their confidence levels. To validate this\napproach, we develop ConfMAD, a MAD framework that integrates confidence\nexpression throughout the debate process. Experimental results demonstrate the\neffectiveness of our method, and we further analyze how confidence influences\ndebate dynamics, offering insights into the design of confidence-aware MAD\nsystems.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86ConfMAD\u6846\u67b6\uff0c\u5728\u591a\u667a\u80fd\u4f53\u8fa9\u8bba\u7cfb\u7edf\u4e2d\u5f15\u5165\u7f6e\u4fe1\u5ea6\u8868\u8fbe\u673a\u5236\uff0c\u4ee5\u89e3\u51b3LLM\u5728\u8fa9\u8bba\u4e2d\u96be\u4ee5\u6e05\u6670\u4f20\u8fbe\u77e5\u8bc6\u4f18\u52bf\u548c\u907f\u514d\u9519\u8bef\u575a\u6301\u6216\u8fc7\u65e9\u6536\u655b\u7684\u95ee\u9898\u3002", "motivation": "\u73b0\u6709\u591a\u667a\u80fd\u4f53\u8fa9\u8bba\u7cfb\u7edf\u4e2d\uff0cLLM\u7f3a\u4e4f\u7f6e\u4fe1\u5ea6\u8868\u8fbe\u673a\u5236\uff0c\u5bfc\u81f4\u5373\u4f7f\u67d0\u4e9b\u6a21\u578b\u5177\u6709\u66f4\u597d\u7684\u77e5\u8bc6\u6216\u63a8\u7406\u80fd\u529b\uff0c\u4e5f\u65e0\u6cd5\u5728\u8fa9\u8bba\u4e2d\u6709\u6548\u4f20\u8fbe\u8fd9\u79cd\u4f18\u52bf\uff0c\u540c\u65f6\u4e0d\u6070\u5f53\u7684\u7f6e\u4fe1\u5ea6\u8868\u8fbe\u4f1a\u5bfc\u81f4\u667a\u80fd\u4f53\u56fa\u6267\u575a\u6301\u9519\u8bef\u4fe1\u5ff5\u6216\u8fc7\u65e9\u6536\u655b\u4e8e\u6b21\u4f18\u7b54\u6848\u3002", "method": "\u63d0\u51faConfMAD\u6846\u67b6\uff0c\u5728\u591a\u667a\u80fd\u4f53\u8fa9\u8bba\u8fc7\u7a0b\u4e2d\u96c6\u6210\u7f6e\u4fe1\u5ea6\u8868\u8fbe\u673a\u5236\uff0c\u8ba9LLM\u80fd\u591f\u660e\u786e\u4f20\u8fbe\u5176\u7f6e\u4fe1\u5ea6\u6c34\u5e73\uff0c\u4ece\u800c\u6539\u5584\u8fa9\u8bba\u52a8\u6001\u548c\u7cfb\u7edf\u6027\u80fd\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\u8be5\u65b9\u6cd5\u6709\u6548\uff0c\u5e76\u8fdb\u4e00\u6b65\u5206\u6790\u4e86\u7f6e\u4fe1\u5ea6\u5982\u4f55\u5f71\u54cd\u8fa9\u8bba\u52a8\u6001\uff0c\u4e3a\u8bbe\u8ba1\u7f6e\u4fe1\u5ea6\u611f\u77e5\u7684\u591a\u667a\u80fd\u4f53\u8fa9\u8bba\u7cfb\u7edf\u63d0\u4f9b\u4e86\u89c1\u89e3\u3002", "conclusion": "\u5728\u591a\u667a\u80fd\u4f53\u8fa9\u8bba\u7cfb\u7edf\u4e2d\u5f15\u5165\u7f6e\u4fe1\u5ea6\u8868\u8fbe\u673a\u5236\u80fd\u591f\u663e\u8457\u63d0\u5347\u8fa9\u8bba\u6548\u679c\u548c\u7cfb\u7edf\u6574\u4f53\u6027\u80fd\uff0c\u4e3aLLM\u534f\u4f5c\u7cfb\u7edf\u8bbe\u8ba1\u63d0\u4f9b\u4e86\u65b0\u7684\u601d\u8def\u3002", "relevance": 85.0}}
{"id": "2509.13662", "pdf": "https://arxiv.org/pdf/2509.13662", "abs": "https://arxiv.org/abs/2509.13662", "authors": ["Yulan Guo", "Longguang Wang", "Wendong Mao", "Xiaoyu Dong", "Yingqian Wang", "Li Liu", "Wei An"], "title": "Deep Lookup Network", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "Convolutional neural networks are constructed with massive operations with\ndifferent types and are highly computationally intensive. Among these\noperations, multiplication operation is higher in computational complexity and\nusually requires {more} energy consumption with longer inference time than\nother operations, which hinders the deployment of convolutional neural networks\non mobile devices. In many resource-limited edge devices, complicated\noperations can be calculated via lookup tables to reduce computational cost.\nMotivated by this, in this paper, we introduce a generic and efficient lookup\noperation which can be used as a basic operation for the construction of neural\nnetworks. Instead of calculating the multiplication of weights and activation\nvalues, simple yet efficient lookup operations are adopted to compute their\nresponses. To enable end-to-end optimization of the lookup operation, we\nconstruct the lookup tables in a differentiable manner and propose several\ntraining strategies to promote their convergence. By replacing computationally\nexpensive multiplication operations with our lookup operations, we develop\nlookup networks for the image classification, image super-resolution, and point\ncloud classification tasks. It is demonstrated that our lookup networks can\nbenefit from the lookup operations to achieve higher efficiency in terms of\nenergy consumption and inference speed while maintaining competitive\nperformance to vanilla convolutional networks. Extensive experiments show that\nour lookup networks produce state-of-the-art performance on different tasks\n(both classification and regression tasks) and different data types (both\nimages and point clouds).", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u7528\u67e5\u627e\u8868\u64cd\u4f5c\u66ff\u4ee3\u4f20\u7edf\u4e58\u6cd5\u8fd0\u7b97\u7684\u65b9\u6cd5\u6765\u6784\u5efa\u9ad8\u6548\u795e\u7ecf\u7f51\u7edc\uff0c\u5728\u56fe\u50cf\u5206\u7c7b\u3001\u8d85\u5206\u8fa8\u7387\u548c\u70b9\u4e91\u5206\u7c7b\u4efb\u52a1\u4e2d\u5b9e\u73b0\u4e86\u66f4\u9ad8\u7684\u80fd\u6548\u548c\u63a8\u7406\u901f\u5ea6\uff0c\u540c\u65f6\u4fdd\u6301\u4e0e\u539f\u59cb\u5377\u79ef\u7f51\u7edc\u76f8\u5f53\u7684\u6027\u80fd\u3002", "motivation": "\u5377\u79ef\u795e\u7ecf\u7f51\u7edc\u4e2d\u7684\u4e58\u6cd5\u8fd0\u7b97\u8ba1\u7b97\u590d\u6742\u5ea6\u9ad8\u3001\u80fd\u8017\u5927\uff0c\u963b\u788d\u4e86\u5728\u79fb\u52a8\u8bbe\u5907\u4e0a\u7684\u90e8\u7f72\u3002\u53d7\u8d44\u6e90\u53d7\u9650\u8fb9\u7f18\u8bbe\u5907\u4f7f\u7528\u67e5\u627e\u8868\u7b80\u5316\u8ba1\u7b97\u7684\u542f\u53d1\uff0c\u4f5c\u8005\u5e0c\u671b\u7528\u9ad8\u6548\u7684\u67e5\u627e\u64cd\u4f5c\u66ff\u4ee3\u4e58\u6cd5\u8fd0\u7b97\u6765\u6784\u5efa\u795e\u7ecf\u7f51\u7edc\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u901a\u7528\u7684\u53ef\u5fae\u5206\u67e5\u627e\u8868\u64cd\u4f5c\uff0c\u901a\u8fc7\u67e5\u627e\u8868\u8ba1\u7b97\u6743\u91cd\u548c\u6fc0\u6d3b\u503c\u7684\u54cd\u5e94\u800c\u975e\u76f4\u63a5\u76f8\u4e58\u3002\u8bbe\u8ba1\u4e86\u591a\u79cd\u8bad\u7ec3\u7b56\u7565\u6765\u4fc3\u8fdb\u67e5\u627e\u8868\u7684\u7aef\u5230\u7aef\u4f18\u5316\uff0c\u5e76\u5c06\u8be5\u65b9\u6cd5\u5e94\u7528\u4e8e\u56fe\u50cf\u5206\u7c7b\u3001\u8d85\u5206\u8fa8\u7387\u548c\u70b9\u4e91\u5206\u7c7b\u4efb\u52a1\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0c\u67e5\u627e\u7f51\u7edc\u5728\u80fd\u8017\u548c\u63a8\u7406\u901f\u5ea6\u65b9\u9762\u6548\u7387\u66f4\u9ad8\uff0c\u540c\u65f6\u5728\u5404\u79cd\u4efb\u52a1\uff08\u5206\u7c7b\u548c\u56de\u5f52\uff09\u548c\u6570\u636e\u7c7b\u578b\uff08\u56fe\u50cf\u548c\u70b9\u4e91\uff09\u4e0a\u90fd\u80fd\u8fbe\u5230\u4e0e\u539f\u59cb\u5377\u79ef\u7f51\u7edc\u7ade\u4e89\u7684\u6027\u80fd\uff0c\u751a\u81f3\u5728\u67d0\u4e9b\u4efb\u52a1\u4e0a\u5b9e\u73b0\u4e86\u6700\u5148\u8fdb\u7684\u8868\u73b0\u3002", "conclusion": "\u67e5\u627e\u8868\u64cd\u4f5c\u662f\u4e00\u79cd\u6709\u6548\u7684\u66ff\u4ee3\u4e58\u6cd5\u8fd0\u7b97\u7684\u65b9\u6cd5\uff0c\u80fd\u591f\u663e\u8457\u63d0\u5347\u795e\u7ecf\u7f51\u7edc\u5728\u8fb9\u7f18\u8bbe\u5907\u4e0a\u7684\u90e8\u7f72\u6548\u7387\uff0c\u540c\u65f6\u4fdd\u6301\u6a21\u578b\u6027\u80fd\u3002", "relevance": 35.0}}
{"id": "2509.13906", "pdf": "https://arxiv.org/pdf/2509.13906", "abs": "https://arxiv.org/abs/2509.13906", "authors": ["Afrin Dange", "Sunita Sarawagi"], "title": "TFMAdapter: Lightweight Instance-Level Adaptation of Foundation Models for Forecasting with Covariates", "categories": ["cs.LG", "I.2.6"], "comment": "Accepted at CIKM 2025", "summary": "Time Series Foundation Models (TSFMs) have recently achieved state-of-the-art\nperformance in univariate forecasting on new time series simply by conditioned\non a brief history of past values. Their success demonstrates that large-scale\npretraining across diverse domains can acquire the inductive bias to generalize\nfrom temporal patterns in a brief history. However, most TSFMs are unable to\nleverage covariates -- future-available exogenous variables critical for\naccurate forecasting in many applications -- due to their domain-specific\nnature and the lack of associated inductive bias. We propose TFMAdapter, a\nlightweight, instance-level adapter that augments TSFMs with covariate\ninformation without fine-tuning. Instead of retraining, TFMAdapter operates on\nthe limited history provided during a single model call, learning a\nnon-parametric cascade that combines covariates with univariate TSFM forecasts.\nHowever, such learning would require univariate forecasts at all steps in the\nhistory, requiring too many calls to the TSFM. To enable training on the full\nhistorical context while limiting TSFM invocations, TFMAdapter uses a two-stage\nmethod: (1) generating pseudo-forecasts with a simple regression model, and (2)\ntraining a Gaussian Process regressor to refine predictions using both pseudo-\nand TSFM forecasts alongside covariates. Extensive experiments on real-world\ndatasets demonstrate that TFMAdapter consistently outperforms both foundation\nmodels and supervised baselines, achieving a 24-27\\% improvement over base\nfoundation models with minimal data and computational overhead. Our results\nhighlight the potential of lightweight adapters to bridge the gap between\ngeneric foundation models and domain-specific forecasting needs.", "AI": {"tldr": "TFMAdapter\u662f\u4e00\u4e2a\u8f7b\u91cf\u7ea7\u9002\u914d\u5668\uff0c\u65e0\u9700\u5fae\u8c03\u5373\u53ef\u4e3a\u65f6\u95f4\u5e8f\u5217\u57fa\u7840\u6a21\u578b\u6dfb\u52a0\u534f\u53d8\u91cf\u4fe1\u606f\uff0c\u5728\u4fdd\u6301\u8ba1\u7b97\u6548\u7387\u7684\u540c\u65f6\u663e\u8457\u63d0\u5347\u9884\u6d4b\u6027\u80fd\u3002", "motivation": "\u5f53\u524d\u65f6\u95f4\u5e8f\u5217\u57fa\u7840\u6a21\u578b(TSFMs)\u5728\u591a\u53d8\u91cf\u9884\u6d4b\u4e2d\u65e0\u6cd5\u6709\u6548\u5229\u7528\u534f\u53d8\u91cf\u4fe1\u606f\uff0c\u56e0\u4e3a\u8fd9\u4e9b\u534f\u53d8\u91cf\u5177\u6709\u9886\u57df\u7279\u5b9a\u6027\u4e14\u7f3a\u4e4f\u76f8\u5173\u7684\u5f52\u7eb3\u504f\u7f6e\uff0c\u9650\u5236\u4e86\u6a21\u578b\u5728\u5b9e\u9645\u5e94\u7528\u4e2d\u7684\u51c6\u786e\u6027\u3002", "method": "\u63d0\u51faTFMAdapter\u4e24\u9636\u6bb5\u65b9\u6cd5\uff1a1)\u4f7f\u7528\u7b80\u5355\u56de\u5f52\u6a21\u578b\u751f\u6210\u4f2a\u9884\u6d4b\uff1b2)\u8bad\u7ec3\u9ad8\u65af\u8fc7\u7a0b\u56de\u5f52\u5668\uff0c\u7ed3\u5408\u4f2a\u9884\u6d4b\u3001TSFM\u9884\u6d4b\u548c\u534f\u53d8\u91cf\u6765\u7cbe\u70bc\u9884\u6d4b\u7ed3\u679c\u3002\u8be5\u65b9\u6cd5\u5728\u5355\u6b21\u6a21\u578b\u8c03\u7528\u4e2d\u64cd\u4f5c\uff0c\u907f\u514d\u5927\u91cfTSFM\u8c03\u7528\u3002", "result": "\u5728\u771f\u5b9e\u4e16\u754c\u6570\u636e\u96c6\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0cTFMAdapter\u6301\u7eed\u4f18\u4e8e\u57fa\u7840\u6a21\u578b\u548c\u76d1\u7763\u57fa\u7ebf\uff0c\u76f8\u6bd4\u57fa\u7840\u57fa\u7840\u6a21\u578b\u5b9e\u73b0\u4e8624-27%\u7684\u6027\u80fd\u63d0\u5347\uff0c\u4e14\u6570\u636e\u548c\u8ba1\u7b97\u5f00\u9500\u6700\u5c0f\u3002", "conclusion": "\u8f7b\u91cf\u7ea7\u9002\u914d\u5668\u6709\u6f5c\u529b\u5f25\u5408\u901a\u7528\u57fa\u7840\u6a21\u578b\u4e0e\u9886\u57df\u7279\u5b9a\u9884\u6d4b\u9700\u6c42\u4e4b\u95f4\u7684\u5dee\u8ddd\uff0c\u4e3a\u65f6\u95f4\u5e8f\u5217\u9884\u6d4b\u63d0\u4f9b\u9ad8\u6548\u89e3\u51b3\u65b9\u6848\u3002", "relevance": 65.0}}
{"id": "2509.13331", "pdf": "https://arxiv.org/pdf/2509.13331", "abs": "https://arxiv.org/abs/2509.13331", "authors": ["Reza Pirayeshshirazinezhad"], "title": "Explainable AI-Enhanced Supervisory Control for High-Precision Spacecraft Formation", "categories": ["astro-ph.IM", "cs.AI", "cs.RO", "cs.SY", "eess.SY"], "comment": null, "summary": "We use artificial intelligence (AI) and supervisory adaptive control systems\nto plan and optimize the mission of precise spacecraft formation. Machine\nlearning and robust control enhance the efficiency of spacecraft precision\nformation of the Virtual Telescope for X-ray Observation (VTXO) space mission.\nVTXO is a precise formation of two separate spacecraft making a virtual\ntelescope with a one-kilometer focal length. One spacecraft carries the lens\nand the other spacecraft holds the camera to observe high-energy space objects\nin the X-ray domain with 55 milli-arcsecond angular resolution accuracy. Timed\nautomata for supervisory control, Monte Carlo simulations for stability and\nrobustness evaluation, and integration of deep neural networks for optimal\nestimation of mission parameters, satisfy the high precision mission criteria.\nWe integrate deep neural networks with a constrained, non-convex dynamic\noptimization pipeline to predict optimal mission parameters, ensuring precision\nmission criteria are met. AI framework provides explainability by predicting\nthe resulting energy consumption and mission error for a given set of mission\nparameters. It allows for transparent, justifiable, and real-time trade-offs, a\ncapability not present in traditional adaptive controllers. The results show\nreductions in energy consumption and improved mission accuracy, demonstrating\nthe capability of the system to address dynamic uncertainties and disturbances.", "AI": {"tldr": "\u4f7f\u7528AI\u548c\u76d1\u7763\u81ea\u9002\u5e94\u63a7\u5236\u7cfb\u7edf\u4f18\u5316\u822a\u5929\u5668\u7cbe\u786e\u7f16\u961f\u4efb\u52a1\uff0c\u96c6\u6210\u6df1\u5ea6\u795e\u7ecf\u7f51\u7edc\u8fdb\u884c\u53c2\u6570\u9884\u6d4b\uff0c\u964d\u4f4e\u80fd\u8017\u5e76\u63d0\u9ad8\u4efb\u52a1\u7cbe\u5ea6", "motivation": "\u4e3aVTXO\u7a7a\u95f4\u4efb\u52a1\uff08\u4e24\u4e2a\u822a\u5929\u5668\u7ec4\u6210\u865a\u62df\u671b\u8fdc\u955c\uff09\u63d0\u4f9b\u9ad8\u7cbe\u5ea6\u7684\u7f16\u961f\u63a7\u5236\uff0c\u89e3\u51b3\u4f20\u7edf\u81ea\u9002\u5e94\u63a7\u5236\u5668\u65e0\u6cd5\u5b9e\u73b0\u7684\u5b9e\u65f6\u6743\u8861\u548c\u53ef\u89e3\u91ca\u6027\u95ee\u9898", "method": "\u7ed3\u5408\u5b9a\u65f6\u81ea\u52a8\u673a\u8fdb\u884c\u76d1\u7763\u63a7\u5236\u3001\u8499\u7279\u5361\u6d1b\u6a21\u62df\u8bc4\u4f30\u7a33\u5b9a\u6027\u3001\u6df1\u5ea6\u795e\u7ecf\u7f51\u7edc\u9884\u6d4b\u6700\u4f18\u4efb\u52a1\u53c2\u6570\uff0c\u96c6\u6210\u7ea6\u675f\u975e\u51f8\u52a8\u6001\u4f18\u5316\u6d41\u7a0b", "result": "\u7cfb\u7edf\u964d\u4f4e\u4e86\u80fd\u8017\u5e76\u63d0\u9ad8\u4e86\u4efb\u52a1\u7cbe\u5ea6\uff0c\u80fd\u591f\u6709\u6548\u5904\u7406\u52a8\u6001\u4e0d\u786e\u5b9a\u6027\u548c\u5e72\u6270", "conclusion": "AI\u6846\u67b6\u4e3a\u822a\u5929\u5668\u7cbe\u786e\u7f16\u961f\u4efb\u52a1\u63d0\u4f9b\u4e86\u53ef\u89e3\u91ca\u3001\u5b9e\u65f6\u6743\u8861\u7684\u4f18\u5316\u65b9\u6848\uff0c\u4f18\u4e8e\u4f20\u7edf\u63a7\u5236\u65b9\u6cd5", "relevance": 25.0}}
{"id": "2509.14036", "pdf": "https://arxiv.org/pdf/2509.14036", "abs": "https://arxiv.org/abs/2509.14036", "authors": ["Zekang Liu", "Wei Feng", "Fanhua Shang", "Lianyu Hu", "Jichao Feng", "Liqing Gao"], "title": "SSL-SSAW: Self-Supervised Learning with Sigmoid Self-Attention Weighting for Question-Based Sign Language Translation", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Sign Language Translation (SLT) bridges the communication gap between deaf\npeople and hearing people, where dialogue provides crucial contextual cues to\naid in translation. Building on this foundational concept, this paper proposes\nQuestion-based Sign Language Translation (QB-SLT), a novel task that explores\nthe efficient integration of dialogue. Unlike gloss (sign language\ntranscription) annotations, dialogue naturally occurs in communication and is\neasier to annotate. The key challenge lies in aligning multimodality features\nwhile leveraging the context of the question to improve translation. To address\nthis issue, we propose a cross-modality Self-supervised Learning with Sigmoid\nSelf-attention Weighting (SSL-SSAW) fusion method for sign language\ntranslation. Specifically, we employ contrastive learning to align\nmultimodality features in QB-SLT, then introduce a Sigmoid Self-attention\nWeighting (SSAW) module for adaptive feature extraction from question and sign\nlanguage sequences. Additionally, we leverage available question text through\nself-supervised learning to enhance representation and translation\ncapabilities. We evaluated our approach on newly constructed CSL-Daily-QA and\nPHOENIX-2014T-QA datasets, where SSL-SSAW achieved SOTA performance. Notably,\neasily accessible question assistance can achieve or even surpass the\nperformance of gloss assistance. Furthermore, visualization results demonstrate\nthe effectiveness of incorporating dialogue in improving translation quality.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u57fa\u4e8e\u95ee\u9898\u7684\u624b\u8bed\u7ffb\u8bd1(QB-SLT)\u65b0\u4efb\u52a1\uff0c\u901a\u8fc7\u8de8\u6a21\u6001\u81ea\u76d1\u7763\u5b66\u4e60\u548cSigmoid\u81ea\u6ce8\u610f\u529b\u52a0\u6743\u878d\u5408\u65b9\u6cd5\uff0c\u5229\u7528\u5bf9\u8bdd\u4e0a\u4e0b\u6587\u63d0\u5347\u624b\u8bed\u7ffb\u8bd1\u6027\u80fd\u3002", "motivation": "\u624b\u8bed\u7ffb\u8bd1\u4e2d\u5bf9\u8bdd\u63d0\u4f9b\u91cd\u8981\u4e0a\u4e0b\u6587\u7ebf\u7d22\uff0c\u4f46\u4f20\u7edfgloss\u6807\u6ce8\u6210\u672c\u9ad8\u3002\u5bf9\u8bdd\u81ea\u7136\u53d1\u751f\u4e14\u6613\u4e8e\u6807\u6ce8\uff0c\u63a2\u7d22\u5982\u4f55\u6709\u6548\u6574\u5408\u5bf9\u8bdd\u4fe1\u606f\u6765\u6539\u8fdb\u7ffb\u8bd1\u8d28\u91cf\u3002", "method": "\u63d0\u51faSSL-SSAW\u65b9\u6cd5\uff1a\u4f7f\u7528\u5bf9\u6bd4\u5b66\u4e60\u5bf9\u9f50\u591a\u6a21\u6001\u7279\u5f81\uff0cSigmoid\u81ea\u6ce8\u610f\u529b\u52a0\u6743\u6a21\u5757\u81ea\u9002\u5e94\u63d0\u53d6\u95ee\u9898\u548c\u624b\u8bed\u5e8f\u5217\u7279\u5f81\uff0c\u901a\u8fc7\u81ea\u76d1\u7763\u5b66\u4e60\u5229\u7528\u53ef\u7528\u95ee\u9898\u6587\u672c\u589e\u5f3a\u8868\u793a\u80fd\u529b\u3002", "result": "\u5728\u65b0\u5efa\u7684CSL-Daily-QA\u548cPHOENIX-2014T-QA\u6570\u636e\u96c6\u4e0a\u8fbe\u5230SOTA\u6027\u80fd\uff0c\u95ee\u9898\u8f85\u52a9\u80fd\u8fbe\u5230\u751a\u81f3\u8d85\u8d8agloss\u8f85\u52a9\u7684\u6548\u679c\uff0c\u53ef\u89c6\u5316\u7ed3\u679c\u8bc1\u5b9e\u5bf9\u8bdd\u6574\u5408\u7684\u6709\u6548\u6027\u3002", "conclusion": "\u57fa\u4e8e\u95ee\u9898\u7684\u5bf9\u8bdd\u6574\u5408\u662f\u624b\u8bed\u7ffb\u8bd1\u7684\u6709\u6548\u65b9\u6cd5\uff0c\u6613\u4e8e\u83b7\u53d6\u7684\u95ee\u9898\u8f85\u52a9\u53ef\u4ee5\u66ff\u4ee3\u4f20\u7edfgloss\u6807\u6ce8\uff0c\u4e3a\u624b\u8bed\u7ffb\u8bd1\u63d0\u4f9b\u4e86\u65b0\u7684\u7814\u7a76\u65b9\u5411\u3002", "relevance": 35.0}}
{"id": "2509.13676", "pdf": "https://arxiv.org/pdf/2509.13676", "abs": "https://arxiv.org/abs/2509.13676", "authors": ["Xiaobo Yang", "Xiaojin Gong"], "title": "Re-purposing SAM into Efficient Visual Projectors for MLLM-Based Referring Image Segmentation", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "Recently, Referring Image Segmentation (RIS) frameworks that pair the\nMultimodal Large Language Model (MLLM) with the Segment Anything Model (SAM)\nhave achieved impressive results. However, adapting MLLM to segmentation is\ncomputationally intensive, primarily due to visual token redundancy. We observe\nthat traditional patch-wise visual projectors struggle to strike a balance\nbetween reducing the number of visual tokens and preserving semantic clarity,\noften retaining overly long token sequences to avoid performance drops.\nInspired by text tokenizers, we propose a novel semantic visual projector that\nleverages semantic superpixels generated by SAM to identify \"visual words\" in\nan image. By compressing and projecting semantic superpixels as visual tokens,\nour approach adaptively shortens the token sequence according to scene\ncomplexity while minimizing semantic loss in compression. To mitigate loss of\ninformation, we propose a semantic superpixel positional embedding to\nstrengthen MLLM's awareness of superpixel geometry and position, alongside a\nsemantic superpixel aggregator to preserve both fine-grained details inside\nsuperpixels and global context outside. Experiments show that our method cuts\nvisual tokens by 93% without compromising performance, notably speeding up MLLM\ntraining and inference, and outperforming existing compressive visual\nprojectors on RIS.", "AI": {"tldr": "\u63d0\u51fa\u57fa\u4e8eSAM\u8bed\u4e49\u8d85\u50cf\u7d20\u7684\u89c6\u89c9\u6295\u5f71\u5668\uff0c\u5c06\u89c6\u89c9token\u51cf\u5c1193%\u800c\u4e0d\u635f\u5931\u6027\u80fd\uff0c\u663e\u8457\u52a0\u901fMLLM\u8bad\u7ec3\u548c\u63a8\u7406", "motivation": "\u4f20\u7edfpatch-wise\u89c6\u89c9\u6295\u5f71\u5668\u5728\u51cf\u5c11\u89c6\u89c9token\u6570\u91cf\u548c\u4fdd\u6301\u8bed\u4e49\u6e05\u6670\u5ea6\u4e4b\u95f4\u96be\u4ee5\u5e73\u8861\uff0c\u901a\u5e38\u4fdd\u7559\u8fc7\u957f\u7684token\u5e8f\u5217\u4ee5\u907f\u514d\u6027\u80fd\u4e0b\u964d", "method": "\u5229\u7528SAM\u751f\u6210\u7684\u8bed\u4e49\u8d85\u50cf\u7d20\u8bc6\u522b\"\u89c6\u89c9\u8bcd\u6c47\"\uff0c\u901a\u8fc7\u538b\u7f29\u548c\u6295\u5f71\u8bed\u4e49\u8d85\u50cf\u7d20\u4f5c\u4e3a\u89c6\u89c9token\uff0c\u6839\u636e\u573a\u666f\u590d\u6742\u5ea6\u81ea\u9002\u5e94\u7f29\u77edtoken\u5e8f\u5217\uff1b\u63d0\u51fa\u8bed\u4e49\u8d85\u50cf\u7d20\u4f4d\u7f6e\u5d4c\u5165\u548c\u805a\u5408\u5668\u6765\u4fdd\u6301\u51e0\u4f55\u4f4d\u7f6e\u4fe1\u606f\u548c\u7ec6\u8282", "result": "\u5b9e\u9a8c\u8868\u660e\u65b9\u6cd5\u5c06\u89c6\u89c9token\u51cf\u5c1193%\u800c\u4e0d\u5f71\u54cd\u6027\u80fd\uff0c\u663e\u8457\u52a0\u901fMLLM\u8bad\u7ec3\u548c\u63a8\u7406\uff0c\u5728RIS\u4efb\u52a1\u4e0a\u4f18\u4e8e\u73b0\u6709\u538b\u7f29\u89c6\u89c9\u6295\u5f71\u5668", "conclusion": "\u57fa\u4e8e\u8bed\u4e49\u8d85\u50cf\u7d20\u7684\u89c6\u89c9\u6295\u5f71\u65b9\u6cd5\u6709\u6548\u89e3\u51b3\u4e86\u89c6\u89c9token\u5197\u4f59\u95ee\u9898\uff0c\u5728\u4fdd\u6301\u6027\u80fd\u7684\u540c\u65f6\u5927\u5e45\u63d0\u5347\u6548\u7387", "relevance": 75.0}}
{"id": "2509.13908", "pdf": "https://arxiv.org/pdf/2509.13908", "abs": "https://arxiv.org/abs/2509.13908", "authors": ["Priyobrata Mondal", "Faizanuddin Ansari", "Swagatam Das"], "title": "APFEx: Adaptive Pareto Front Explorer for Intersectional Fairness", "categories": ["cs.LG"], "comment": null, "summary": "Ensuring fairness in machine learning models is critical, especially when\nbiases compound across intersecting protected attributes like race, gender, and\nage. While existing methods address fairness for single attributes, they fail\nto capture the nuanced, multiplicative biases faced by intersectional\nsubgroups. We introduce Adaptive Pareto Front Explorer (APFEx), the first\nframework to explicitly model intersectional fairness as a joint optimization\nproblem over the Cartesian product of sensitive attributes. APFEx combines\nthree key innovations- (1) an adaptive multi-objective optimizer that\ndynamically switches between Pareto cone projection, gradient weighting, and\nexploration strategies to navigate fairness-accuracy trade-offs, (2)\ndifferentiable intersectional fairness metrics enabling gradient-based\noptimization of non-smooth subgroup disparities, and (3) theoretical guarantees\nof convergence to Pareto-optimal solutions. Experiments on four real-world\ndatasets demonstrate APFEx's superiority, reducing fairness violations while\nmaintaining competitive accuracy. Our work bridges a critical gap in fair ML,\nproviding a scalable, model-agnostic solution for intersectional fairness.", "AI": {"tldr": "APFEx\u662f\u4e00\u4e2a\u9488\u5bf9\u4ea4\u53c9\u516c\u5e73\u6027\u7684\u591a\u76ee\u6807\u4f18\u5316\u6846\u67b6\uff0c\u901a\u8fc7\u81ea\u9002\u5e94\u4f18\u5316\u5668\u3001\u53ef\u5fae\u516c\u5e73\u6027\u6307\u6807\u548c\u7406\u8bba\u4fdd\u8bc1\u6765\u89e3\u51b3\u591a\u4e2a\u654f\u611f\u5c5e\u6027\u7ec4\u5408\u7684\u516c\u5e73\u6027\u95ee\u9898\u3002", "motivation": "\u73b0\u6709\u516c\u5e73\u6027\u65b9\u6cd5\u4e3b\u8981\u9488\u5bf9\u5355\u4e00\u654f\u611f\u5c5e\u6027\uff0c\u65e0\u6cd5\u5904\u7406\u4ea4\u53c9\u5c5e\u6027\uff08\u5982\u79cd\u65cf+\u6027\u522b+\u5e74\u9f84\uff09\u5e26\u6765\u7684\u590d\u5408\u504f\u89c1\u95ee\u9898\uff0c\u9700\u8981\u4e13\u95e8\u9488\u5bf9\u4ea4\u53c9\u5b50\u7ec4\u7684\u516c\u5e73\u6027\u4f18\u5316\u65b9\u6848\u3002", "method": "\u63d0\u51faAPFEx\u6846\u67b6\uff1a1\uff09\u81ea\u9002\u5e94\u591a\u76ee\u6807\u4f18\u5316\u5668\uff0c\u52a8\u6001\u5207\u6362Pareto\u9525\u6295\u5f71\u3001\u68af\u5ea6\u52a0\u6743\u548c\u63a2\u7d22\u7b56\u7565\uff1b2\uff09\u53ef\u5fae\u4ea4\u53c9\u516c\u5e73\u6027\u6307\u6807\uff0c\u652f\u6301\u57fa\u4e8e\u68af\u5ea6\u7684\u975e\u5e73\u6ed1\u5b50\u7ec4\u5dee\u5f02\u4f18\u5316\uff1b3\uff09\u63d0\u4f9b\u6536\u655b\u5230Pareto\u6700\u4f18\u89e3\u7684\u7406\u8bba\u4fdd\u8bc1\u3002", "result": "\u5728\u56db\u4e2a\u771f\u5b9e\u6570\u636e\u96c6\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0cAPFEx\u80fd\u591f\u663e\u8457\u51cf\u5c11\u516c\u5e73\u6027\u8fdd\u89c4\uff0c\u540c\u65f6\u4fdd\u6301\u6709\u7ade\u4e89\u529b\u7684\u51c6\u786e\u6027\uff0c\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "conclusion": "APFEx\u586b\u8865\u4e86\u516c\u5e73\u673a\u5668\u5b66\u4e60\u4e2d\u7684\u91cd\u8981\u7a7a\u767d\uff0c\u4e3a\u4ea4\u53c9\u516c\u5e73\u6027\u63d0\u4f9b\u4e86\u53ef\u6269\u5c55\u3001\u6a21\u578b\u65e0\u5173\u7684\u89e3\u51b3\u65b9\u6848\u3002", "relevance": 65.0}}
{"id": "2509.14128", "pdf": "https://arxiv.org/pdf/2509.14128", "abs": "https://arxiv.org/abs/2509.14128", "authors": ["Monica Sekoyan", "Nithin Rao Koluguri", "Nune Tadevosyan", "Piotr Zelasko", "Travis Bartley", "Nick Karpov", "Jagadeesh Balam", "Boris Ginsburg"], "title": "Canary-1B-v2 & Parakeet-TDT-0.6B-v3: Efficient and High-Performance Models for Multilingual ASR and AST", "categories": ["cs.CL", "eess.AS"], "comment": "Mini Version of it Submitted to ICASSP 2026", "summary": "This report introduces Canary-1B-v2, a fast, robust multilingual model for\nAutomatic Speech Recognition (ASR) and Speech-to-Text Translation (AST). Built\nwith a FastConformer encoder and Transformer decoder, it supports 25 languages\nprimarily European. The model was trained on 1.7M hours of total data samples,\nincluding Granary and NeMo ASR Set 3.0, with non-speech audio added to reduce\nhallucinations for ASR and AST. We describe its two-stage pre-training and\nfine-tuning process with dynamic data balancing, as well as experiments with an\nnGPT encoder. Results show nGPT scales well with massive data, while\nFastConformer excels after fine-tuning. For timestamps, Canary-1B-v2 uses the\nNeMo Forced Aligner (NFA) with an auxiliary CTC model, providing reliable\nsegment-level timestamps for ASR and AST. Evaluations show Canary-1B-v2\noutperforms Whisper-large-v3 on English ASR while being 10x faster, and\ndelivers competitive multilingual ASR and AST performance against larger models\nlike Seamless-M4T-v2-large and LLM-based systems. We also release\nParakeet-TDT-0.6B-v3, a successor to v2, offering multilingual ASR across the\nsame 25 languages with just 600M parameters.", "AI": {"tldr": "Canary-1B-v2\u662f\u4e00\u4e2a\u5feb\u901f\u3001\u9c81\u68d2\u7684\u591a\u8bed\u8a00\u8bed\u97f3\u8bc6\u522b\u548c\u8bed\u97f3\u8f6c\u6587\u672c\u7ffb\u8bd1\u6a21\u578b\uff0c\u57fa\u4e8eFastConformer\u7f16\u7801\u5668\u548cTransformer\u89e3\u7801\u5668\uff0c\u652f\u630125\u79cd\u6b27\u6d32\u8bed\u8a00\uff0c\u5728\u4fdd\u6301\u9ad8\u6027\u80fd\u7684\u540c\u65f6\u6bd4Whisper-large-v3\u5feb10\u500d\u3002", "motivation": "\u5f00\u53d1\u4e00\u4e2a\u9ad8\u6548\u3001\u5feb\u901f\u4e14\u51c6\u786e\u7684\u591a\u8bed\u8a00\u8bed\u97f3\u5904\u7406\u6a21\u578b\uff0c\u89e3\u51b3\u73b0\u6709\u5927\u578b\u6a21\u578b\u63a8\u7406\u901f\u5ea6\u6162\u7684\u95ee\u9898\uff0c\u540c\u65f6\u51cf\u5c11\u8bed\u97f3\u8bc6\u522b\u548c\u7ffb\u8bd1\u4e2d\u7684\u5e7b\u89c9\u73b0\u8c61\u3002", "method": "\u91c7\u7528\u4e24\u9636\u6bb5\u9884\u8bad\u7ec3\u548c\u5fae\u8c03\u6d41\u7a0b\uff0c\u4f7f\u7528FastConformer\u7f16\u7801\u5668\u548cTransformer\u89e3\u7801\u5668\u67b6\u6784\uff0c\u8bad\u7ec3\u6570\u636e\u5305\u62ec170\u4e07\u5c0f\u65f6\u7684\u591a\u8bed\u8a00\u8bed\u97f3\u6570\u636e\uff0c\u5e76\u6dfb\u52a0\u975e\u8bed\u97f3\u97f3\u9891\u4ee5\u51cf\u5c11\u5e7b\u89c9\u3002\u4f7f\u7528\u52a8\u6001\u6570\u636e\u5e73\u8861\u548cNeMo\u5f3a\u5236\u5bf9\u9f50\u5668\u8fdb\u884c\u65f6\u95f4\u6233\u6807\u6ce8\u3002", "result": "\u6a21\u578b\u5728\u82f1\u8bedASR\u4e0a\u8d85\u8d8aWhisper-large-v3\u4e14\u901f\u5ea6\u5feb10\u500d\uff0c\u5728\u591a\u8bed\u8a00ASR\u548cAST\u4efb\u52a1\u4e0a\u4e0eSeamless-M4T-v2-large\u7b49\u5927\u578b\u6a21\u578b\u7ade\u4e89\u6027\u80fd\uff0c\u540c\u65f6\u53d1\u5e03\u4e86\u66f4\u5c0f\u7684600M\u53c2\u6570\u7248\u672cParakeet-TDT-0.6B-v3\u3002", "conclusion": "Canary-1B-v2\u5c55\u793a\u4e86\u5728\u8bed\u97f3\u5904\u7406\u4efb\u52a1\u4e2d\u5b9e\u73b0\u9ad8\u6027\u80fd\u548c\u9ad8\u6548\u7387\u7684\u53ef\u884c\u6027\uff0cFastConformer\u67b6\u6784\u5728\u5fae\u8c03\u540e\u8868\u73b0\u4f18\u5f02\uff0cnGPT\u7f16\u7801\u5668\u5728\u5927\u89c4\u6a21\u6570\u636e\u4e0b\u5177\u6709\u826f\u597d\u7684\u6269\u5c55\u6027\u3002", "relevance": 45.0}}
{"id": "2509.13681", "pdf": "https://arxiv.org/pdf/2509.13681", "abs": "https://arxiv.org/abs/2509.13681", "authors": ["Hang Li", "Dianmo Sheng", "Qiankun Dong", "Zichun Wang", "Zhiwei Xu", "Tao Li"], "title": "FishBEV: Distortion-Resilient Bird's Eye View Segmentation with Surround-View Fisheye Cameras", "categories": ["cs.CV"], "comment": "8 pages, 4 figures", "summary": "As a cornerstone technique for autonomous driving, Bird's Eye View (BEV)\nsegmentation has recently achieved remarkable progress with pinhole cameras.\nHowever, it is non-trivial to extend the existing methods to fisheye cameras\nwith severe geometric distortion, ambiguous multi-view correspondences and\nunstable temporal dynamics, all of which significantly degrade BEV performance.\nTo address these challenges, we propose FishBEV, a novel BEV segmentation\nframework specifically tailored for fisheye cameras. This framework introduces\nthree complementary innovations, including a Distortion-Resilient Multi-scale\nExtraction (DRME) backbone that learns robust features under distortion while\npreserving scale consistency, an Uncertainty-aware Spatial Cross-Attention\n(U-SCA) mechanism that leverages uncertainty estimation for reliable cross-view\nalignment, a Distance-aware Temporal Self-Attention (D-TSA) module that\nadaptively balances near field details and far field context to ensure temporal\ncoherence. Extensive experiments on the Synwoodscapes dataset demonstrate that\nFishBEV consistently outperforms SOTA baselines, regarding the performance\nevaluation of FishBEV on the surround-view fisheye BEV segmentation tasks.", "AI": {"tldr": "FishBEV\uff1a\u4e13\u4e3a\u9c7c\u773c\u76f8\u673a\u8bbe\u8ba1\u7684BEV\u5206\u5272\u6846\u67b6\uff0c\u901a\u8fc7\u4e09\u4e2a\u521b\u65b0\u6a21\u5757\u89e3\u51b3\u9c7c\u773c\u76f8\u673a\u7684\u51e0\u4f55\u7578\u53d8\u3001\u591a\u89c6\u56fe\u5bf9\u5e94\u6a21\u7cca\u548c\u65f6\u95f4\u52a8\u6001\u4e0d\u7a33\u5b9a\u95ee\u9898\uff0c\u5728Synwoodscapes\u6570\u636e\u96c6\u4e0a\u8d85\u8d8a\u73b0\u6709\u6700\u4f73\u65b9\u6cd5", "motivation": "\u73b0\u6709\u7684BEV\u5206\u5272\u65b9\u6cd5\u4e3b\u8981\u9488\u5bf9\u9488\u5b54\u76f8\u673a\u8bbe\u8ba1\uff0c\u65e0\u6cd5\u6709\u6548\u5904\u7406\u9c7c\u773c\u76f8\u673a\u7684\u4e25\u91cd\u51e0\u4f55\u7578\u53d8\u3001\u591a\u89c6\u56fe\u5bf9\u5e94\u6a21\u7cca\u548c\u65f6\u95f4\u52a8\u6001\u4e0d\u7a33\u5b9a\u7b49\u95ee\u9898\uff0c\u5bfc\u81f4BEV\u6027\u80fd\u663e\u8457\u4e0b\u964d", "method": "\u63d0\u51faFishBEV\u6846\u67b6\uff0c\u5305\u542b\u4e09\u4e2a\u6838\u5fc3\u6a21\u5757\uff1a1) \u6297\u7578\u53d8\u591a\u5c3a\u5ea6\u63d0\u53d6(DRME)\u4e3b\u5e72\u7f51\u7edc\uff0c\u5728\u7578\u53d8\u4e0b\u5b66\u4e60\u9c81\u68d2\u7279\u5f81\u5e76\u4fdd\u6301\u5c3a\u5ea6\u4e00\u81f4\u6027\uff1b2) \u4e0d\u786e\u5b9a\u6027\u611f\u77e5\u7a7a\u95f4\u4ea4\u53c9\u6ce8\u610f\u529b(U-SCA)\u673a\u5236\uff0c\u5229\u7528\u4e0d\u786e\u5b9a\u6027\u4f30\u8ba1\u5b9e\u73b0\u53ef\u9760\u7684\u8de8\u89c6\u56fe\u5bf9\u9f50\uff1b3) \u8ddd\u79bb\u611f\u77e5\u65f6\u95f4\u81ea\u6ce8\u610f\u529b(D-TSA)\u6a21\u5757\uff0c\u81ea\u9002\u5e94\u5e73\u8861\u8fd1\u573a\u7ec6\u8282\u548c\u8fdc\u573a\u4e0a\u4e0b\u6587\u4ee5\u786e\u4fdd\u65f6\u95f4\u4e00\u81f4\u6027", "result": "\u5728Synwoodscapes\u6570\u636e\u96c6\u4e0a\u7684\u5927\u91cf\u5b9e\u9a8c\u8868\u660e\uff0cFishBEV\u5728\u73af\u89c6\u9c7c\u773cBEV\u5206\u5272\u4efb\u52a1\u4e0a\u6301\u7eed\u8d85\u8d8a\u6700\u5148\u8fdb\u7684\u57fa\u7ebf\u65b9\u6cd5", "conclusion": "FishBEV\u662f\u9488\u5bf9\u9c7c\u773c\u76f8\u673aBEV\u5206\u5272\u7684\u6709\u6548\u89e3\u51b3\u65b9\u6848\uff0c\u901a\u8fc7\u4e13\u95e8\u8bbe\u8ba1\u7684\u6a21\u5757\u6210\u529f\u89e3\u51b3\u4e86\u9c7c\u773c\u76f8\u673a\u7279\u6709\u7684\u6311\u6218", "relevance": 20.0}}
{"id": "2509.13914", "pdf": "https://arxiv.org/pdf/2509.13914", "abs": "https://arxiv.org/abs/2509.13914", "authors": ["Divya Thuremella", "Yi Yang", "Simon Wanna", "Lars Kunze", "Daniele De Martini"], "title": "Ensemble of Pre-Trained Models for Long-Tailed Trajectory Prediction", "categories": ["cs.LG", "cs.AI"], "comment": "Accepted 2025 IEEE International Conference on Intelligent\n  Transportation Systems (ITSC 2025)", "summary": "This work explores the application of ensemble modeling to the\nmultidimensional regression problem of trajectory prediction for vehicles in\nurban environments. As newer and bigger state-of-the-art prediction models for\nautonomous driving continue to emerge, an important open challenge is the\nproblem of how to combine the strengths of these big models without the need\nfor costly re-training. We show how, perhaps surprisingly, combining\nstate-of-the-art deep learning models out-of-the-box (without retraining or\nfine-tuning) with a simple confidence-weighted average method can enhance the\noverall prediction. Indeed, while combining trajectory prediction models is not\nstraightforward, this simple approach enhances performance by 10% over the best\nprediction model, especially in the long-tailed metrics. We show that this\nperformance improvement holds on both the NuScenes and Argoverse datasets, and\nthat these improvements are made across the dataset distribution. The code for\nour work is open source.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u7b80\u5355\u7684\u7f6e\u4fe1\u5ea6\u52a0\u6743\u5e73\u5747\u96c6\u6210\u65b9\u6cd5\uff0c\u65e0\u9700\u91cd\u65b0\u8bad\u7ec3\u5373\u53ef\u7ed3\u5408\u591a\u4e2a\u5148\u8fdb\u8f68\u8ff9\u9884\u6d4b\u6a21\u578b\u7684\u4f18\u52bf\uff0c\u5728NuScenes\u548cArgoverse\u6570\u636e\u96c6\u4e0a\u6027\u80fd\u63d0\u534710%", "motivation": "\u89e3\u51b3\u81ea\u52a8\u9a7e\u9a76\u9886\u57df\u4e2d\u5982\u4f55\u5728\u4e0d\u8fdb\u884c\u6602\u8d35\u91cd\u65b0\u8bad\u7ec3\u7684\u60c5\u51b5\u4e0b\uff0c\u7ed3\u5408\u591a\u4e2a\u5927\u578b\u5148\u8fdb\u8f68\u8ff9\u9884\u6d4b\u6a21\u578b\u4f18\u52bf\u7684\u6311\u6218", "method": "\u4f7f\u7528\u7f6e\u4fe1\u5ea6\u52a0\u6743\u5e73\u5747\u65b9\u6cd5\u5bf9\u591a\u4e2a\u6700\u5148\u8fdb\u7684\u6df1\u5ea6\u5b66\u4e60\u8f68\u8ff9\u9884\u6d4b\u6a21\u578b\u8fdb\u884c\u96c6\u6210\uff0c\u65e0\u9700\u91cd\u65b0\u8bad\u7ec3\u6216\u5fae\u8c03", "result": "\u5728NuScenes\u548cArgoverse\u6570\u636e\u96c6\u4e0a\uff0c\u8be5\u65b9\u6cd5\u6bd4\u6700\u4f73\u5355\u4e00\u6a21\u578b\u6027\u80fd\u63d0\u534710%\uff0c\u7279\u522b\u662f\u5728\u957f\u5c3e\u6307\u6807\u4e0a\u8868\u73b0\u4f18\u5f02\uff0c\u4e14\u6539\u8fdb\u5728\u6574\u4e2a\u6570\u636e\u5206\u5e03\u4e0a\u4fdd\u6301\u4e00\u81f4", "conclusion": "\u7b80\u5355\u7684\u7f6e\u4fe1\u5ea6\u52a0\u6743\u5e73\u5747\u96c6\u6210\u65b9\u6cd5\u80fd\u591f\u6709\u6548\u63d0\u5347\u8f68\u8ff9\u9884\u6d4b\u6027\u80fd\uff0c\u8bc1\u660e\u4e86\u96c6\u6210\u5b66\u4e60\u5728\u81ea\u52a8\u9a7e\u9a76\u9884\u6d4b\u4efb\u52a1\u4e2d\u7684\u4ef7\u503c", "relevance": 35.0}}
{"id": "2509.13342", "pdf": "https://arxiv.org/pdf/2509.13342", "abs": "https://arxiv.org/abs/2509.13342", "authors": ["Isaac Ronald Ward"], "title": "Real World Robotic Exploration using Deep Neural Networks Trained in Photorealistic Reconstructed Environments", "categories": ["cs.RO", "cs.AI"], "comment": "This report is submitted as partial fulfilment of the requirements\n  for the Honours Programme of the Department of Computer Science and Software\n  Engineering, The University of Western Australia, 2019", "summary": "In this work, an existing deep neural network approach for determining a\nrobot's pose from visual information (RGB images) is modified, improving its\nlocalization performance without impacting its ease of training. Explicitly,\nthe network's loss function is extended in a manner which intuitively combines\nthe positional and rotational error in order to increase robustness to\nperceptual aliasing. An improvement in the localization accuracy for indoor\nscenes is observed: with decreases of up to 9.64% and 2.99% in the median\npositional and rotational error respectively, when compared to the unmodified\nnetwork.\n  Additionally, photogrammetry data is used to produce a pose-labelled dataset\nwhich allows the above model to be trained on a local environment, resulting in\nlocalization accuracies of 0.11m & 0.89 degrees. This trained model forms the\nbasis of a navigation algorithm, which is tested in real-time on a TurtleBot (a\nwheeled robotic device). As such, this work introduces a full pipeline for\ncreating a robust navigational algorithm for any given real world indoor scene;\nthe only requirement being a collection of images from the scene, which can be\ncaptured in as little as 330 seconds of", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u6539\u8fdb\u7684\u89c6\u89c9\u5b9a\u4f4d\u65b9\u6cd5\uff0c\u901a\u8fc7\u4fee\u6539\u635f\u5931\u51fd\u6570\u7ed3\u5408\u4f4d\u7f6e\u548c\u65cb\u8f6c\u8bef\u5dee\uff0c\u63d0\u9ad8\u4e86\u673a\u5668\u4eba\u5728\u5ba4\u5185\u573a\u666f\u4e2d\u7684\u5b9a\u4f4d\u7cbe\u5ea6\uff0c\u5e76\u5efa\u7acb\u4e86\u5b8c\u6574\u7684\u5bfc\u822a\u7b97\u6cd5\u6d41\u7a0b", "motivation": "\u6539\u8fdb\u73b0\u6709\u89c6\u89c9\u5b9a\u4f4d\u795e\u7ecf\u7f51\u7edc\uff0c\u5728\u4e0d\u589e\u52a0\u8bad\u7ec3\u96be\u5ea6\u7684\u60c5\u51b5\u4e0b\u63d0\u9ad8\u5b9a\u4f4d\u6027\u80fd\uff0c\u7279\u522b\u662f\u89e3\u51b3\u611f\u77e5\u6df7\u6dc6\u95ee\u9898", "method": "\u6269\u5c55\u7f51\u7edc\u635f\u5931\u51fd\u6570\uff0c\u76f4\u89c2\u5730\u7ed3\u5408\u4f4d\u7f6e\u548c\u65cb\u8f6c\u8bef\u5dee\uff1b\u4f7f\u7528\u6444\u5f71\u6d4b\u91cf\u6570\u636e\u521b\u5efa\u59ff\u6001\u6807\u8bb0\u6570\u636e\u96c6\uff1b\u5728TurtleBot\u4e0a\u8fdb\u884c\u5b9e\u65f6\u6d4b\u8bd5", "result": "\u5ba4\u5185\u5b9a\u4f4d\u7cbe\u5ea6\u663e\u8457\u63d0\u5347\uff1a\u4e2d\u4f4d\u4f4d\u7f6e\u8bef\u5dee\u964d\u4f4e9.64%\uff0c\u65cb\u8f6c\u8bef\u5dee\u964d\u4f4e2.99%\uff1b\u6700\u7ec8\u8fbe\u52300.11\u7c73\u548c0.89\u5ea6\u7684\u5b9a\u4f4d\u7cbe\u5ea6", "conclusion": "\u63d0\u51fa\u4e86\u4e00\u4e2a\u5b8c\u6574\u7684\u9c81\u68d2\u5bfc\u822a\u7b97\u6cd5\u6d41\u7a0b\uff0c\u4ec5\u9700\u573a\u666f\u56fe\u50cf\u91c7\u96c6\uff08\u6700\u5feb330\u79d2\uff09\u5373\u53ef\u4e3a\u4efb\u4f55\u771f\u5b9e\u5ba4\u5185\u573a\u666f\u521b\u5efa\u5bfc\u822a\u7cfb\u7edf", "relevance": 15.0}}
{"id": "2509.14161", "pdf": "https://arxiv.org/pdf/2509.14161", "abs": "https://arxiv.org/abs/2509.14161", "authors": ["Brian Yan", "Injy Hamed", "Shuichiro Shimizu", "Vasista Lodagala", "William Chen", "Olga Iakovenko", "Bashar Talafha", "Amir Hussein", "Alexander Polok", "Kalvin Chang", "Dominik Klement", "Sara Althubaiti", "Puyuan Peng", "Matthew Wiesner", "Thamar Solorio", "Ahmed Ali", "Sanjeev Khudanpur", "Shinji Watanabe", "Chih-Chen Chen", "Zhen Wu", "Karim Benharrak", "Anuj Diwan", "Samuele Cornell", "Eunjung Yeo", "Kwanghee Choi", "Carlos Carvalho", "Karen Rosero"], "title": "CS-FLEURS: A Massively Multilingual and Code-Switched Speech Dataset", "categories": ["cs.CL", "cs.SD", "eess.AS"], "comment": null, "summary": "We present CS-FLEURS, a new dataset for developing and evaluating\ncode-switched speech recognition and translation systems beyond high-resourced\nlanguages. CS-FLEURS consists of 4 test sets which cover in total 113 unique\ncode-switched language pairs across 52 languages: 1) a 14 X-English language\npair set with real voices reading synthetically generated code-switched\nsentences, 2) a 16 X-English language pair set with generative text-to-speech\n3) a 60 {Arabic, Mandarin, Hindi, Spanish}-X language pair set with the\ngenerative text-to-speech, and 4) a 45 X-English lower-resourced language pair\ntest set with concatenative text-to-speech. Besides the four test sets,\nCS-FLEURS also provides a training set with 128 hours of generative\ntext-to-speech data across 16 X-English language pairs. Our hope is that\nCS-FLEURS helps to broaden the scope of future code-switched speech research.\nDataset link: https://huggingface.co/datasets/byan/cs-fleurs.", "AI": {"tldr": "CS-FLEURS\u662f\u4e00\u4e2a\u65b0\u7684\u4ee3\u7801\u5207\u6362\u8bed\u97f3\u8bc6\u522b\u548c\u7ffb\u8bd1\u6570\u636e\u96c6\uff0c\u5305\u542b4\u4e2a\u6d4b\u8bd5\u96c6\uff0c\u8986\u76d6113\u79cd\u8bed\u8a00\u5bf9\u768452\u79cd\u8bed\u8a00\uff0c\u65e8\u5728\u63a8\u52a8\u4f4e\u8d44\u6e90\u8bed\u8a00\u7684\u4ee3\u7801\u5207\u6362\u7814\u7a76\u3002", "motivation": "\u5f53\u524d\u4ee3\u7801\u5207\u6362\u8bed\u97f3\u7814\u7a76\u4e3b\u8981\u5c40\u9650\u4e8e\u9ad8\u8d44\u6e90\u8bed\u8a00\uff0c\u7f3a\u4e4f\u5bf9\u4f4e\u8d44\u6e90\u8bed\u8a00\u7684\u652f\u6301\u3002CS-FLEURS\u65e8\u5728\u586b\u8865\u8fd9\u4e00\u7a7a\u767d\uff0c\u4e3a\u5f00\u53d1\u66f4\u5e7f\u6cdb\u7684\u4ee3\u7801\u5207\u6362\u8bed\u97f3\u7cfb\u7edf\u63d0\u4f9b\u6570\u636e\u652f\u6301\u3002", "method": "\u6784\u5efa\u5305\u542b4\u4e2a\u6d4b\u8bd5\u96c6\u7684\u6570\u636e\u96c6\uff1a1\uff0914\u79cdX-\u82f1\u8bed\u8bed\u8a00\u5bf9\u7684\u771f\u5b9e\u8bed\u97f3\u6717\u8bfb\u5408\u6210\u4ee3\u7801\u5207\u6362\u53e5\u5b50\uff1b2\uff0916\u79cdX-\u82f1\u8bed\u8bed\u8a00\u5bf9\u7684\u751f\u6210\u5f0f\u6587\u672c\u8f6c\u8bed\u97f3\uff1b3\uff0960\u79cd{\u963f\u62c9\u4f2f\u8bed\u3001\u666e\u901a\u8bdd\u3001\u5370\u5730\u8bed\u3001\u897f\u73ed\u7259\u8bed}-X\u8bed\u8a00\u5bf9\u7684\u751f\u6210\u5f0f\u6587\u672c\u8f6c\u8bed\u97f3\uff1b4\uff0945\u79cdX-\u82f1\u8bed\u4f4e\u8d44\u6e90\u8bed\u8a00\u5bf9\u7684\u62fc\u63a5\u5f0f\u6587\u672c\u8f6c\u8bed\u97f3\u3002\u540c\u65f6\u63d0\u4f9b128\u5c0f\u65f6\u7684\u8bad\u7ec3\u6570\u636e\u3002", "result": "\u521b\u5efa\u4e86\u8986\u76d6113\u79cd\u72ec\u7279\u4ee3\u7801\u5207\u6362\u8bed\u8a00\u5bf9\u7684\u6570\u636e\u96c6\uff0c\u6db5\u76d652\u79cd\u8bed\u8a00\uff0c\u4e3a\u4ee3\u7801\u5207\u6362\u8bed\u97f3\u7814\u7a76\u63d0\u4f9b\u4e86\u5168\u9762\u7684\u57fa\u51c6\u6d4b\u8bd5\u8d44\u6e90\u3002", "conclusion": "CS-FLEURS\u6570\u636e\u96c6\u6709\u52a9\u4e8e\u6269\u5927\u4ee3\u7801\u5207\u6362\u8bed\u97f3\u7814\u7a76\u7684\u8303\u56f4\uff0c\u7279\u522b\u662f\u5728\u4f4e\u8d44\u6e90\u8bed\u8a00\u65b9\u9762\uff0c\u4e3a\u672a\u6765\u7814\u7a76\u63d0\u4f9b\u4e86\u91cd\u8981\u8d44\u6e90\u3002", "relevance": 25.0}}
{"id": "2509.13687", "pdf": "https://arxiv.org/pdf/2509.13687", "abs": "https://arxiv.org/abs/2509.13687", "authors": ["Kaniz Fatema", "Emad A. Mohammed", "Sukhjit Singh Sehra"], "title": "Taylor-Series Expanded Kolmogorov-Arnold Network for Medical Imaging Classification", "categories": ["cs.CV"], "comment": null, "summary": "Effective and interpretable classification of medical images is a challenge\nin computer-aided diagnosis, especially in resource-limited clinical settings.\nThis study introduces spline-based Kolmogorov-Arnold Networks (KANs) for\naccurate medical image classification with limited, diverse datasets. The\nmodels include SBTAYLOR-KAN, integrating B-splines with Taylor series;\nSBRBF-KAN, combining B-splines with Radial Basis Functions; and SBWAVELET-KAN,\nembedding B-splines in Morlet wavelet transforms. These approaches leverage\nspline-based function approximation to capture both local and global\nnonlinearities. The models were evaluated on brain MRI, chest X-rays,\ntuberculosis X-rays, and skin lesion images without preprocessing,\ndemonstrating the ability to learn directly from raw data. Extensive\nexperiments, including cross-dataset validation and data reduction analysis,\nshowed strong generalization and stability. SBTAYLOR-KAN achieved up to 98.93%\naccuracy, with a balanced F1-score, maintaining over 86% accuracy using only\n30% of the training data across three datasets. Despite class imbalance in the\nskin cancer dataset, experiments on both imbalanced and balanced versions\nshowed SBTAYLOR-KAN outperforming other models, achieving 68.22% accuracy.\nUnlike traditional CNNs, which require millions of parameters (e.g., ResNet50\nwith 24.18M), SBTAYLOR-KAN achieves comparable performance with just 2,872\ntrainable parameters, making it more suitable for constrained medical\nenvironments. Gradient-weighted Class Activation Mapping (Grad-CAM) was used\nfor interpretability, highlighting relevant regions in medical images. This\nframework provides a lightweight, interpretable, and generalizable solution for\nmedical image classification, addressing the challenges of limited datasets and\ndata-scarce scenarios in clinical AI applications.", "AI": {"tldr": "\u8be5\u7814\u7a76\u63d0\u51fa\u57fa\u4e8e\u6837\u6761\u7684Kolmogorov-Arnold\u7f51\u7edc(KANs)\u7528\u4e8e\u533b\u5b66\u56fe\u50cf\u5206\u7c7b\uff0c\u5305\u62ecSBTAYLOR-KAN\u3001SBRBF-KAN\u548cSBWAVELET-KAN\u4e09\u79cd\u53d8\u4f53\uff0c\u5728\u6709\u9650\u6570\u636e\u96c6\u4e0a\u5b9e\u73b0\u9ad8\u7cbe\u5ea6\u5206\u7c7b\uff0c\u53c2\u6570\u91cf\u8fdc\u5c11\u4e8e\u4f20\u7edfCNN\u6a21\u578b\u3002", "motivation": "\u89e3\u51b3\u8d44\u6e90\u53d7\u9650\u4e34\u5e8a\u73af\u5883\u4e2d\u533b\u5b66\u56fe\u50cf\u5206\u7c7b\u7684\u6311\u6218\uff0c\u7279\u522b\u662f\u5728\u6570\u636e\u96c6\u6709\u9650\u4e14\u591a\u6837\u7684\u60c5\u51b5\u4e0b\uff0c\u9700\u8981\u5f00\u53d1\u8f7b\u91cf\u7ea7\u3001\u53ef\u89e3\u91ca\u4e14\u6cdb\u5316\u80fd\u529b\u5f3a\u7684\u5206\u7c7b\u6a21\u578b\u3002", "method": "\u63d0\u51fa\u4e09\u79cd\u57fa\u4e8e\u6837\u6761\u7684KAN\u53d8\u4f53\uff1aSBTAYLOR-KAN\uff08B\u6837\u6761+\u6cf0\u52d2\u7ea7\u6570\uff09\u3001SBRBF-KAN\uff08B\u6837\u6761+\u5f84\u5411\u57fa\u51fd\u6570\uff09\u3001SBWAVELET-KAN\uff08B\u6837\u6761+Morlet\u5c0f\u6ce2\u53d8\u6362\uff09\uff0c\u5229\u7528\u6837\u6761\u51fd\u6570\u903c\u8fd1\u6355\u6349\u5c40\u90e8\u548c\u5168\u5c40\u975e\u7ebf\u6027\u7279\u5f81\u3002", "result": "\u5728\u8111\u90e8MRI\u3001\u80f8\u90e8X\u5149\u3001\u7ed3\u6838X\u5149\u548c\u76ae\u80a4\u75c5\u53d8\u56fe\u50cf\u4e0a\u8bc4\u4f30\uff0cSBTAYLOR-KAN\u8fbe\u523098.93%\u51c6\u786e\u7387\uff0c\u4ec5\u4f7f\u752830%\u8bad\u7ec3\u6570\u636e\u4ecd\u4fdd\u630186%\u4ee5\u4e0a\u51c6\u786e\u7387\u3002\u76f8\u6bd4ResNet50\u76842418\u4e07\u53c2\u6570\uff0cSBTAYLOR-KAN\u4ec5\u97002872\u4e2a\u53ef\u8bad\u7ec3\u53c2\u6570\u3002", "conclusion": "\u8be5\u6846\u67b6\u4e3a\u533b\u5b66\u56fe\u50cf\u5206\u7c7b\u63d0\u4f9b\u4e86\u8f7b\u91cf\u7ea7\u3001\u53ef\u89e3\u91ca\u4e14\u6cdb\u5316\u6027\u5f3a\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u7279\u522b\u9002\u5408\u6570\u636e\u7a00\u7f3a\u7684\u4e34\u5e8aAI\u5e94\u7528\u573a\u666f\u3002", "relevance": 35.0}}
{"id": "2509.13933", "pdf": "https://arxiv.org/pdf/2509.13933", "abs": "https://arxiv.org/abs/2509.13933", "authors": ["Qiyue Li", "Yingxin Liu", "Hang Qi", "Jieping Luo", "Zhizhang Liu", "Jingjin Wu"], "title": "Adaptive Client Selection via Q-Learning-based Whittle Index in Wireless Federated Learning", "categories": ["cs.LG", "cs.DC"], "comment": null, "summary": "We consider the client selection problem in wireless Federated Learning (FL),\nwith the objective of reducing the total required time to achieve a certain\nlevel of learning accuracy. Since the server cannot observe the clients'\ndynamic states that can change their computation and communication efficiency,\nwe formulate client selection as a restless multi-armed bandit problem. We\npropose a scalable and efficient approach called the Whittle Index Learning in\nFederated Q-learning (WILF-Q), which uses Q-learning to adaptively learn and\nupdate an approximated Whittle index associated with each client, and then\nselects the clients with the highest indices. Compared to existing approaches,\nWILF-Q does not require explicit knowledge of client state transitions or data\ndistributions, making it well-suited for deployment in practical FL settings.\nExperiment results demonstrate that WILF-Q significantly outperforms existing\nbaseline policies in terms of learning efficiency, providing a robust and\nefficient approach to client selection in wireless FL.", "AI": {"tldr": "WILF-Q\uff1a\u4e00\u79cd\u57fa\u4e8e\u5f3a\u5316\u5b66\u4e60\u7684\u65e0\u7ebf\u8054\u90a6\u5b66\u4e60\u5ba2\u6237\u7aef\u9009\u62e9\u65b9\u6cd5\uff0c\u901a\u8fc7Q\u5b66\u4e60\u8fd1\u4f3cWhittle\u6307\u6570\u6765\u4f18\u5316\u5b66\u4e60\u6548\u7387", "motivation": "\u89e3\u51b3\u65e0\u7ebf\u8054\u90a6\u5b66\u4e60\u4e2d\u5ba2\u6237\u7aef\u9009\u62e9\u95ee\u9898\uff0c\u76ee\u6807\u662f\u51cf\u5c11\u8fbe\u5230\u7279\u5b9a\u5b66\u4e60\u7cbe\u5ea6\u6240\u9700\u7684\u603b\u65f6\u95f4\u3002\u7531\u4e8e\u670d\u52a1\u5668\u65e0\u6cd5\u89c2\u5bdf\u5ba2\u6237\u7aef\u7684\u52a8\u6001\u72b6\u6001\u53d8\u5316\uff0c\u9700\u8981\u4e00\u79cd\u65e0\u9700\u663e\u5f0f\u72b6\u6001\u8f6c\u79fb\u77e5\u8bc6\u7684\u81ea\u9002\u5e94\u65b9\u6cd5", "method": "\u5c06\u5ba2\u6237\u7aef\u9009\u62e9\u5efa\u6a21\u4e3a\u591a\u81c2\u8001\u864e\u673a\u95ee\u9898\uff0c\u63d0\u51faWILF-Q\u65b9\u6cd5\uff1a\u4f7f\u7528Q\u5b66\u4e60\u81ea\u9002\u5e94\u5b66\u4e60\u548c\u66f4\u65b0\u6bcf\u4e2a\u5ba2\u6237\u7aef\u7684\u8fd1\u4f3cWhittle\u6307\u6570\uff0c\u7136\u540e\u9009\u62e9\u6307\u6570\u6700\u9ad8\u7684\u5ba2\u6237\u7aef", "result": "\u5b9e\u9a8c\u7ed3\u679c\u8868\u660eWILF-Q\u5728\u5b66\u4e60\u6548\u7387\u65b9\u9762\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u57fa\u7ebf\u7b56\u7565\uff0c\u4e3a\u65e0\u7ebfFL\u4e2d\u7684\u5ba2\u6237\u7aef\u9009\u62e9\u63d0\u4f9b\u4e86\u9c81\u68d2\u9ad8\u6548\u7684\u89e3\u51b3\u65b9\u6848", "conclusion": "WILF-Q\u4e0d\u9700\u8981\u5ba2\u6237\u7aef\u72b6\u6001\u8f6c\u79fb\u6216\u6570\u636e\u5206\u5e03\u7684\u663e\u5f0f\u77e5\u8bc6\uff0c\u9002\u5408\u5728\u5b9e\u9645FL\u8bbe\u7f6e\u4e2d\u90e8\u7f72\uff0c\u662f\u89e3\u51b3\u65e0\u7ebf\u8054\u90a6\u5b66\u4e60\u5ba2\u6237\u7aef\u9009\u62e9\u95ee\u9898\u7684\u6709\u6548\u65b9\u6cd5", "relevance": 25.0}}
{"id": "2509.13345", "pdf": "https://arxiv.org/pdf/2509.13345", "abs": "https://arxiv.org/abs/2509.13345", "authors": ["Zihao Li", "Weiwei Yi", "Jiahong Chen"], "title": "Accuracy Paradox in Large Language Models: Regulating Hallucination Risks in Generative AI", "categories": ["cs.CY", "cs.AI", "cs.CL", "cs.HC", "cs.LG"], "comment": null, "summary": "As Large Language Models (LLMs) permeate everyday decision-making, their\nepistemic and societal risks demand urgent scrutiny. Hallucinations, the\ngeneration of fabricated, misleading, oversimplified or untrustworthy outputs,\nhas emerged as imperative challenges. While regulatory, academic, and technical\ndiscourse position accuracy as the principal benchmark for mitigating such\nharms, this article contends that overreliance on accuracy misdiagnoses the\nproblem and has counterproductive effect: the accuracy paradox. Drawing on\ninterdisciplinary literatures, this article develops a taxonomy of\nhallucination types and shows the paradox along three intertwining dimensions:\noutputs, individuals and society. First, accuracy functions as a superficial\nproxy for reliability, incentivising the optimisation of rhetorical fluency and\nsurface-level correctness over epistemic trustworthiness. This encourages\npassive user trust in outputs that appear accurate but epistemically untenable.\nSecond, accuracy as a singular metric fails to detect harms that are not\nfactually false but are nonetheless misleading, value-laden, or socially\ndistorting, including consensus illusions, sycophantic alignment, and subtle\nmanipulation. Third, regulatory overemphasis on accuracy obscures the wider\nsocietal consequences of hallucination, including social sorting, privacy\nviolations, equity harms, epistemic convergence that marginalises dissent,\nreduces pluralism, and causes social deskilling. By examining the EU AI Act,\nGDPR, and DSA, the article argues that current regulations are not yet\nstructurally equipped to address these epistemic, relational, and systemic\nharms and exacerbated by the overreliance on accuracy. By exposing such\nconceptual and practical challenges, this article calls for a fundamental shift\ntowards pluralistic, context-aware, and manipulation-resilient approaches to AI\ntrustworthy governance.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\"\u51c6\u786e\u6027\u6096\u8bba\"\u6982\u5ff5\uff0c\u6307\u51fa\u8fc7\u5ea6\u4f9d\u8d56\u51c6\u786e\u6027\u4f5c\u4e3aLLM\u8bc4\u4f30\u6807\u51c6\u53cd\u800c\u4f1a\u52a0\u5267\u5e7b\u89c9\u95ee\u9898\uff0c\u9700\u8981\u8f6c\u5411\u66f4\u5168\u9762\u7684\u53ef\u4fe1AI\u6cbb\u7406\u6846\u67b6", "motivation": "\u968f\u7740LLM\u5728\u65e5\u5e38\u51b3\u7b56\u4e2d\u7684\u666e\u53ca\uff0c\u5176\u4ea7\u751f\u7684\u5e7b\u89c9\uff08\u865a\u5047\u3001\u8bef\u5bfc\u6027\u8f93\u51fa\uff09\u5e26\u6765\u4e86\u4e25\u91cd\u7684\u8ba4\u77e5\u548c\u793e\u4f1a\u98ce\u9669\u3002\u5f53\u524d\u76d1\u7ba1\u548c\u5b66\u672f\u8ba8\u8bba\u8fc7\u5ea6\u5f3a\u8c03\u51c6\u786e\u6027\u4f5c\u4e3a\u4e3b\u8981\u57fa\u51c6\uff0c\u4f46\u4f5c\u8005\u8ba4\u4e3a\u8fd9\u79cd\u5355\u4e00\u6807\u51c6\u4f1a\u8bef\u8bca\u95ee\u9898\u5e76\u4ea7\u751f\u53cd\u6548\u679c", "method": "\u901a\u8fc7\u8de8\u5b66\u79d1\u6587\u732e\u5206\u6790\uff0c\u5efa\u7acb\u4e86\u5e7b\u89c9\u7c7b\u578b\u7684\u5206\u7c7b\u6cd5\uff0c\u5e76\u4ece\u4e09\u4e2a\u7ef4\u5ea6\uff08\u8f93\u51fa\u3001\u4e2a\u4f53\u3001\u793e\u4f1a\uff09\u5206\u6790\u51c6\u786e\u6027\u6096\u8bba\uff0c\u540c\u65f6\u8003\u5bdf\u4e86\u6b27\u76dfAI\u6cd5\u6848\u3001GDPR\u548cDSA\u7b49\u73b0\u884c\u6cd5\u89c4\u7684\u5c40\u9650\u6027", "result": "\u53d1\u73b0\u51c6\u786e\u6027\u4f5c\u4e3a\u5355\u4e00\u6307\u6807\u5b58\u5728\u4e09\u4e2a\u95ee\u9898\uff1a1\uff09\u53ea\u662f\u53ef\u9760\u6027\u7684\u8868\u9762\u4ee3\u7406\uff0c\u9f13\u52b1\u4f18\u5316\u4fee\u8f9e\u6d41\u7545\u6027\u800c\u975e\u8ba4\u77e5\u53ef\u4fe1\u5ea6\uff1b2\uff09\u65e0\u6cd5\u68c0\u6d4b\u975e\u4e8b\u5b9e\u9519\u8bef\u4f46\u4ecd\u6709\u5371\u5bb3\u7684\u8f93\u51fa\uff1b3\uff09\u63a9\u76d6\u4e86\u5e7b\u89c9\u7684\u66f4\u5e7f\u6cdb\u793e\u4f1a\u540e\u679c", "conclusion": "\u9700\u8981\u4ece\u6839\u672c\u4e0a\u8f6c\u5411\u591a\u5143\u5316\u3001\u60c5\u5883\u611f\u77e5\u548c\u6297\u64cd\u7eb5\u7684AI\u53ef\u4fe1\u6cbb\u7406\u65b9\u6cd5\uff0c\u5f53\u524d\u6cd5\u89c4\u5728\u7ed3\u6784\u4e0a\u8fd8\u4e0d\u8db3\u4ee5\u5e94\u5bf9\u8fd9\u4e9b\u8ba4\u77e5\u3001\u5173\u7cfb\u548c\u7cfb\u7edf\u6027\u5371\u5bb3", "relevance": 85.0}}
{"id": "2509.14171", "pdf": "https://arxiv.org/pdf/2509.14171", "abs": "https://arxiv.org/abs/2509.14171", "authors": ["Yifan Liu", "Wenkuan Zhao", "Shanshan Zhong", "Jinghui Qin", "Mingfu Liang", "Zhongzhan Huang", "Wushao Wen"], "title": "AssoCiAm: A Benchmark for Evaluating Association Thinking while Circumventing Ambiguity", "categories": ["cs.CL"], "comment": null, "summary": "Recent advancements in multimodal large language models (MLLMs) have garnered\nsignificant attention, offering a promising pathway toward artificial general\nintelligence (AGI). Among the essential capabilities required for AGI,\ncreativity has emerged as a critical trait for MLLMs, with association serving\nas its foundation. Association reflects a model' s ability to think creatively,\nmaking it vital to evaluate and understand. While several frameworks have been\nproposed to assess associative ability, they often overlook the inherent\nambiguity in association tasks, which arises from the divergent nature of\nassociations and undermines the reliability of evaluations. To address this\nissue, we decompose ambiguity into two types-internal ambiguity and external\nambiguity-and introduce AssoCiAm, a benchmark designed to evaluate associative\nability while circumventing the ambiguity through a hybrid computational\nmethod. We then conduct extensive experiments on MLLMs, revealing a strong\npositive correlation between cognition and association. Additionally, we\nobserve that the presence of ambiguity in the evaluation process causes MLLMs'\nbehavior to become more random-like. Finally, we validate the effectiveness of\nour method in ensuring more accurate and reliable evaluations. See Project Page\nfor the data and codes.", "AI": {"tldr": "\u63d0\u51fa\u4e86AssoCiAm\u57fa\u51c6\u6d4b\u8bd5\uff0c\u901a\u8fc7\u6df7\u5408\u8ba1\u7b97\u65b9\u6cd5\u89e3\u51b3\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\u5173\u8054\u80fd\u529b\u8bc4\u4f30\u4e2d\u7684\u6a21\u7cca\u6027\u95ee\u9898\uff0c\u53d1\u73b0\u8ba4\u77e5\u4e0e\u5173\u8054\u80fd\u529b\u5b58\u5728\u5f3a\u6b63\u76f8\u5173\u5173\u7cfb\u3002", "motivation": "\u73b0\u6709\u7684\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\u5173\u8054\u80fd\u529b\u8bc4\u4f30\u6846\u67b6\u5f80\u5f80\u5ffd\u89c6\u4e86\u5173\u8054\u4efb\u52a1\u4e2d\u56fa\u6709\u7684\u6a21\u7cca\u6027\uff0c\u8fd9\u79cd\u6a21\u7cca\u6027\u6e90\u4e8e\u5173\u8054\u7684\u53d1\u6563\u6027\uff0c\u4f1a\u964d\u4f4e\u8bc4\u4f30\u7684\u53ef\u9760\u6027\u3002", "method": "\u5c06\u6a21\u7cca\u6027\u5206\u89e3\u4e3a\u5185\u90e8\u6a21\u7cca\u6027\u548c\u5916\u90e8\u6a21\u7cca\u6027\uff0c\u5f15\u5165AssoCiAm\u57fa\u51c6\u6d4b\u8bd5\uff0c\u91c7\u7528\u6df7\u5408\u8ba1\u7b97\u65b9\u6cd5\u6765\u89c4\u907f\u6a21\u7cca\u6027\uff0c\u5e76\u8fdb\u884c\u5e7f\u6cdb\u7684\u5b9e\u9a8c\u9a8c\u8bc1\u3002", "result": "\u5b9e\u9a8c\u53d1\u73b0\u8ba4\u77e5\u4e0e\u5173\u8054\u80fd\u529b\u5b58\u5728\u5f3a\u6b63\u76f8\u5173\u5173\u7cfb\uff0c\u8bc4\u4f30\u8fc7\u7a0b\u4e2d\u7684\u6a21\u7cca\u6027\u4f1a\u4f7fMLLMs\u7684\u884c\u4e3a\u66f4\u52a0\u968f\u673a\u5316\uff0c\u9a8c\u8bc1\u4e86\u6240\u63d0\u65b9\u6cd5\u80fd\u786e\u4fdd\u66f4\u51c6\u786e\u53ef\u9760\u7684\u8bc4\u4f30\u3002", "conclusion": "AssoCiAm\u57fa\u51c6\u6d4b\u8bd5\u80fd\u6709\u6548\u89e3\u51b3\u5173\u8054\u80fd\u529b\u8bc4\u4f30\u4e2d\u7684\u6a21\u7cca\u6027\u95ee\u9898\uff0c\u4e3aMLLMs\u7684\u521b\u9020\u529b\u8bc4\u4f30\u63d0\u4f9b\u4e86\u66f4\u53ef\u9760\u7684\u6846\u67b6\u3002", "relevance": 65.0}}
{"id": "2509.13711", "pdf": "https://arxiv.org/pdf/2509.13711", "abs": "https://arxiv.org/abs/2509.13711", "authors": ["Qiuyu Tang", "Joshua Krinsky", "Aparna Bharati"], "title": "StyleProtect: Safeguarding Artistic Identity in Fine-tuned Diffusion Models", "categories": ["cs.CV"], "comment": null, "summary": "The rapid advancement of generative models, particularly diffusion-based\napproaches, has inadvertently facilitated their potential for misuse. Such\nmodels enable malicious exploiters to replicate artistic styles that capture an\nartist's creative labor, personal vision, and years of dedication in an\ninexpensive manner. This has led to a rise in the need and exploration of\nmethods for protecting artworks against style mimicry. Although generic\ndiffusion models can easily mimic an artistic style, finetuning amplifies this\ncapability, enabling the model to internalize and reproduce the style with\nhigher fidelity and control. We hypothesize that certain cross-attention layers\nexhibit heightened sensitivity to artistic styles. Sensitivity is measured\nthrough activation strengths of attention layers in response to style and\ncontent representations, and assessing their correlations with features\nextracted from external models. Based on our findings, we introduce an\nefficient and lightweight protection strategy, StyleProtect, that achieves\neffective style defense against fine-tuned diffusion models by updating only\nselected cross-attention layers. Our experiments utilize a carefully curated\nartwork dataset based on WikiArt, comprising representative works from 30\nartists known for their distinctive and influential styles and cartoon\nanimations from the Anita dataset. The proposed method demonstrates promising\nperformance in safeguarding unique styles of artworks and anime from malicious\ndiffusion customization, while maintaining competitive imperceptibility.", "AI": {"tldr": "\u672c\u6587\u63d0\u51faStyleProtect\u65b9\u6cd5\uff0c\u901a\u8fc7\u9009\u62e9\u6027\u66f4\u65b0\u6269\u6563\u6a21\u578b\u4e2d\u7279\u5b9a\u7684cross-attention\u5c42\u6765\u6709\u6548\u9632\u5fa1\u9488\u5bf9\u827a\u672f\u98ce\u683c\u7684\u6076\u610f\u6a21\u4eff\u653b\u51fb\uff0c\u5728\u4fdd\u6301\u4e0d\u53ef\u611f\u77e5\u6027\u7684\u540c\u65f6\u4fdd\u62a4\u827a\u672f\u4f5c\u54c1\u7684\u72ec\u7279\u98ce\u683c\u3002", "motivation": "\u968f\u7740\u751f\u6210\u6a21\u578b\u7279\u522b\u662f\u6269\u6563\u6a21\u578b\u7684\u5feb\u901f\u53d1\u5c55\uff0c\u6076\u610f\u4f7f\u7528\u8005\u53ef\u4ee5\u4f4e\u6210\u672c\u5730\u590d\u5236\u827a\u672f\u5bb6\u7684\u72ec\u7279\u98ce\u683c\uff0c\u8fd9\u5bfc\u81f4\u4e86\u5bf9\u827a\u672f\u4f5c\u54c1\u98ce\u683c\u6a21\u4eff\u4fdd\u62a4\u65b9\u6cd5\u7684\u9700\u6c42\u589e\u52a0\u3002\u672c\u6587\u65e8\u5728\u89e3\u51b3\u6269\u6563\u6a21\u578b\u5fae\u8c03\u540e\u5bf9\u827a\u672f\u98ce\u683c\u7684\u9ad8\u5ea6\u6a21\u4eff\u95ee\u9898\u3002", "method": "\u7814\u7a76\u53d1\u73b0\u67d0\u4e9bcross-attention\u5c42\u5bf9\u827a\u672f\u98ce\u683c\u7279\u522b\u654f\u611f\uff0c\u57fa\u4e8e\u6b64\u63d0\u51fa\u4e86StyleProtect\u65b9\u6cd5\uff1a\u901a\u8fc7\u6d4b\u91cfattention\u5c42\u5bf9\u98ce\u683c\u548c\u5185\u5bb9\u8868\u793a\u7684\u6fc0\u6d3b\u5f3a\u5ea6\uff0c\u9009\u62e9\u6027\u5730\u66f4\u65b0\u7279\u5b9across-attention\u5c42\u6765\u5b9e\u73b0\u6709\u6548\u7684\u98ce\u683c\u9632\u5fa1\u3002", "result": "\u5b9e\u9a8c\u4f7f\u7528\u57fa\u4e8eWikiArt\u7684\u7cbe\u5fc3\u7b56\u5212\u6570\u636e\u96c6\uff08\u5305\u542b30\u4f4d\u77e5\u540d\u827a\u672f\u5bb6\u7684\u4ee3\u8868\u6027\u4f5c\u54c1\uff09\u548cAnita\u5361\u901a\u52a8\u753b\u6570\u636e\u96c6\uff0c\u8bc1\u660e\u8be5\u65b9\u6cd5\u5728\u4fdd\u62a4\u827a\u672f\u4f5c\u54c1\u548c\u52a8\u6f2b\u72ec\u7279\u98ce\u683c\u65b9\u9762\u8868\u73b0\u51fa\u8272\uff0c\u540c\u65f6\u4fdd\u6301\u7ade\u4e89\u6027\u7684\u4e0d\u53ef\u611f\u77e5\u6027\u3002", "conclusion": "StyleProtect\u63d0\u4f9b\u4e86\u4e00\u79cd\u9ad8\u6548\u8f7b\u91cf\u7684\u4fdd\u62a4\u7b56\u7565\uff0c\u80fd\u591f\u6709\u6548\u9632\u5fa1\u5fae\u8c03\u6269\u6563\u6a21\u578b\u7684\u98ce\u683c\u6a21\u4eff\u653b\u51fb\uff0c\u4e3a\u827a\u672f\u4f5c\u54c1\u7248\u6743\u4fdd\u62a4\u63d0\u4f9b\u4e86\u5b9e\u7528\u89e3\u51b3\u65b9\u6848\u3002", "relevance": 45.0}}
{"id": "2509.13952", "pdf": "https://arxiv.org/pdf/2509.13952", "abs": "https://arxiv.org/abs/2509.13952", "authors": ["Amin Lotfalian", "Mohammad Reza Banan", "Pooyan Broumand"], "title": "eXtended Physics Informed Neural Network Method for Fracture Mechanics Problems", "categories": ["cs.LG", "cs.NA", "math.NA"], "comment": null, "summary": "This paper presents eXtended Physics-Informed Neural Network (X-PINN), a\nnovel and robust framework for addressing fracture mechanics problems involving\nmultiple cracks in fractured media. To address this, an energy-based loss\nfunction, customized integration schemes, and domain decomposition procedures\nare proposed. Inspired by the Extended Finite Element Method (XFEM), the neural\nnetwork solution space is enriched with specialized functions that allow crack\nbody discontinuities and singularities at crack tips to be explicitly captured.\nFurthermore, a structured framework is introduced in which standard and\nenriched solution components are modeled using distinct neural networks,\nenabling flexible and effective simulations of complex multiple-crack problems\nin 1D and 2D domains, with convenient extensibility to 3D problems. Numerical\nexperiments are conducted to validate the effectiveness and robustness of the\nproposed method.", "AI": {"tldr": "\u63d0\u51fa\u4e86X-PINN\u6846\u67b6\uff0c\u901a\u8fc7\u80fd\u91cf\u635f\u5931\u51fd\u6570\u3001\u5b9a\u5236\u79ef\u5206\u65b9\u6848\u548c\u57df\u5206\u89e3\u65b9\u6cd5\u5904\u7406\u591a\u88c2\u7eb9\u65ad\u88c2\u529b\u5b66\u95ee\u9898\uff0c\u7ed3\u5408XFEM\u601d\u60f3\u7528\u7279\u6b8a\u51fd\u6570\u663e\u5f0f\u6355\u6349\u88c2\u7eb9\u4e0d\u8fde\u7eed\u6027\u548c\u5947\u5f02\u6027", "motivation": "\u89e3\u51b3\u65ad\u88c2\u4ecb\u8d28\u4e2d\u591a\u88c2\u7eb9\u95ee\u9898\u7684\u8ba1\u7b97\u6311\u6218\uff0c\u4f20\u7edf\u65b9\u6cd5\u5728\u5904\u7406\u590d\u6742\u88c2\u7eb9\u51e0\u4f55\u548c\u5947\u5f02\u6027\u65b9\u9762\u5b58\u5728\u56f0\u96be", "method": "\u6269\u5c55\u7269\u7406\u4fe1\u606f\u795e\u7ecf\u7f51\u7edc(X-PINN)\uff0c\u91c7\u7528\u57fa\u4e8e\u80fd\u91cf\u7684\u635f\u5931\u51fd\u6570\u3001\u5b9a\u5236\u79ef\u5206\u65b9\u6848\u3001\u57df\u5206\u89e3\uff0c\u5e76\u501f\u9274XFEM\u65b9\u6cd5\u7528\u7279\u6b8a\u51fd\u6570\u4e30\u5bcc\u795e\u7ecf\u7f51\u7edc\u89e3\u7a7a\u95f4", "result": "\u6570\u503c\u5b9e\u9a8c\u9a8c\u8bc1\u4e86\u8be5\u65b9\u6cd5\u57281D\u548c2D\u591a\u88c2\u7eb9\u95ee\u9898\u4e2d\u7684\u6709\u6548\u6027\u548c\u9c81\u68d2\u6027\uff0c\u53ef\u6269\u5c55\u52303D\u95ee\u9898", "conclusion": "X-PINN\u4e3a\u590d\u6742\u591a\u88c2\u7eb9\u65ad\u88c2\u529b\u5b66\u95ee\u9898\u63d0\u4f9b\u4e86\u7075\u6d3b\u6709\u6548\u7684\u8ba1\u7b97\u6846\u67b6", "relevance": 15.0}}
{"id": "2509.13349", "pdf": "https://arxiv.org/pdf/2509.13349", "abs": "https://arxiv.org/abs/2509.13349", "authors": ["Jed Guzelkabaagac", "Boris Petrovi\u0107"], "title": "Label-Efficient Grasp Joint Prediction with Point-JEPA", "categories": ["cs.RO", "cs.AI", "cs.LG"], "comment": "4 pages, 5 figures. Submitted to IROS 2025 Workshop", "summary": "We investigate whether 3D self-supervised pretraining with a Joint-Embedding\nPredictive Architecture (Point-JEPA) enables label-efficient grasp joint-angle\nprediction. Using point clouds tokenized from meshes and a ShapeNet-pretrained\nPoint-JEPA encoder, we train a lightweight multi-hypothesis head with\nwinner-takes-all and evaluate by top-logit selection. On DLR-Hand II with\nobject-level splits, Point-JEPA reduces RMSE by up to 26% in low-label regimes\nand reaches parity with full supervision. These results suggest JEPA-style\npretraining is a practical approach for data-efficient grasp learning.", "AI": {"tldr": "Point-JEPA\u81ea\u76d1\u7763\u9884\u8bad\u7ec3\u65b9\u6cd5\u5728\u4f4e\u6807\u7b7e\u6570\u636e\u60c5\u51b5\u4e0b\u663e\u8457\u63d0\u5347\u6293\u53d6\u5173\u8282\u89d2\u5ea6\u9884\u6d4b\u6027\u80fd\uff0c\u5728DLR-Hand II\u6570\u636e\u96c6\u4e0aRMSE\u964d\u4f4e26%\uff0c\u8fbe\u5230\u5168\u76d1\u7763\u6027\u80fd\u6c34\u5e73", "motivation": "\u7814\u7a763D\u81ea\u76d1\u7763\u9884\u8bad\u7ec3\uff08\u7279\u522b\u662fJoint-Embedding Predictive Architecture\uff09\u662f\u5426\u80fd\u591f\u5b9e\u73b0\u6807\u7b7e\u9ad8\u6548\u7684\u6293\u53d6\u5173\u8282\u89d2\u5ea6\u9884\u6d4b\uff0c\u89e3\u51b3\u673a\u5668\u4eba\u6293\u53d6\u5b66\u4e60\u4e2d\u6570\u636e\u6807\u6ce8\u6210\u672c\u9ad8\u7684\u95ee\u9898", "method": "\u4f7f\u7528\u4ece\u7f51\u683c\u6807\u8bb0\u5316\u7684\u70b9\u4e91\u6570\u636e\uff0c\u91c7\u7528ShapeNet\u9884\u8bad\u7ec3\u7684Point-JEPA\u7f16\u7801\u5668\uff0c\u8bad\u7ec3\u8f7b\u91cf\u7ea7\u591a\u5047\u8bbe\u5934\u90e8\uff0c\u4f7f\u7528winner-takes-all\u7b56\u7565\uff0c\u901a\u8fc7top-logit\u9009\u62e9\u8fdb\u884c\u8bc4\u4f30", "result": "\u5728DLR-Hand II\u6570\u636e\u96c6\u7684\u5bf9\u8c61\u7ea7\u5206\u5272\u4e0a\uff0cPoint-JEPA\u5728\u4f4e\u6807\u7b7e\u60c5\u51b5\u4e0b\u5c06RMSE\u964d\u4f4e\u4e86\u6700\u591a26%\uff0c\u5e76\u4e14\u8fbe\u5230\u4e86\u4e0e\u5168\u76d1\u7763\u65b9\u6cd5\u76f8\u5f53\u7684\u6027\u80fd\u6c34\u5e73", "conclusion": "JEPA\u98ce\u683c\u7684\u9884\u8bad\u7ec3\u662f\u6570\u636e\u9ad8\u6548\u6293\u53d6\u5b66\u4e60\u7684\u4e00\u79cd\u5b9e\u7528\u65b9\u6cd5\uff0c\u8bc1\u660e\u4e86\u81ea\u76d1\u7763\u9884\u8bad\u7ec3\u5728\u673a\u5668\u4eba\u6293\u53d6\u4efb\u52a1\u4e2d\u7684\u6709\u6548\u6027", "relevance": 45.0}}
{"id": "2509.14180", "pdf": "https://arxiv.org/pdf/2509.14180", "abs": "https://arxiv.org/abs/2509.14180", "authors": ["Akhil Theerthala"], "title": "Synthesizing Behaviorally-Grounded Reasoning Chains: A Data-Generation Framework for Personal Finance LLMs", "categories": ["cs.CL", "cs.AI", "cs.LG", "68T50", "I.2.7; J.4"], "comment": "24 pages, 11 figures. The paper presents a novel framework for\n  generating a personal finance dataset. The resulting fine-tuned model and\n  dataset are publicly available", "summary": "Personalized financial advice requires consideration of user goals,\nconstraints, risk tolerance, and jurisdiction. Prior LLM work has focused on\nsupport systems for investors and financial planners. Simultaneously, numerous\nrecent studies examine broader personal finance tasks, including budgeting,\ndebt management, retirement, and estate planning, through agentic pipelines\nthat incur high maintenance costs, yielding less than 25% of their expected\nfinancial returns. In this study, we introduce a novel and reproducible\nframework that integrates relevant financial context with behavioral finance\nstudies to construct supervision data for end-to-end advisors. Using this\nframework, we create a 19k sample reasoning dataset and conduct a comprehensive\nfine-tuning of the Qwen-3-8B model on the dataset. Through a held-out test\nsplit and a blind LLM-jury study, we demonstrate that through careful data\ncuration and behavioral integration, our 8B model achieves performance\ncomparable to significantly larger baselines (14-32B parameters) across factual\naccuracy, fluency, and personalization metrics while incurring 80% lower costs\nthan the larger counterparts.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u4e2a\u65b0\u9896\u7684\u91d1\u878d\u54a8\u8be2\u6846\u67b6\uff0c\u901a\u8fc7\u6574\u5408\u91d1\u878d\u80cc\u666f\u548c\u884c\u4e3a\u91d1\u878d\u5b66\u7814\u7a76\u6784\u5efa\u76d1\u7763\u6570\u636e\uff0c\u57288B\u53c2\u6570\u6a21\u578b\u4e0a\u5b9e\u73b0\u4e86\u4e0e14-32B\u5927\u6a21\u578b\u76f8\u5f53\u7684\u6027\u80fd\uff0c\u540c\u65f6\u6210\u672c\u964d\u4f4e80%\u3002", "motivation": "\u4e2a\u6027\u5316\u91d1\u878d\u54a8\u8be2\u9700\u8981\u8003\u8651\u7528\u6237\u76ee\u6807\u3001\u7ea6\u675f\u3001\u98ce\u9669\u627f\u53d7\u80fd\u529b\u548c\u53f8\u6cd5\u7ba1\u8f96\u533a\u3002\u73b0\u6709LLM\u5de5\u4f5c\u4e3b\u8981\u5173\u6ce8\u6295\u8d44\u8005\u652f\u6301\u7cfb\u7edf\uff0c\u800c\u4ee3\u7406\u5316\u7ba1\u9053\u7ef4\u62a4\u6210\u672c\u9ad8\u4e14\u8d22\u52a1\u56de\u62a5\u4f4e\u4e8e\u9884\u671f\u3002", "method": "\u6784\u5efa\u4e86\u4e00\u4e2a\u5305\u542b19k\u6837\u672c\u7684\u63a8\u7406\u6570\u636e\u96c6\uff0c\u6574\u5408\u76f8\u5173\u91d1\u878d\u80cc\u666f\u548c\u884c\u4e3a\u91d1\u878d\u5b66\u7814\u7a76\uff0c\u5bf9Qwen-3-8B\u6a21\u578b\u8fdb\u884c\u5168\u9762\u5fae\u8c03\u3002", "result": "\u901a\u8fc7\u4fdd\u7559\u6d4b\u8bd5\u96c6\u548c\u76f2\u5ba1LLM\u8bc4\u5ba1\u7814\u7a76\u663e\u793a\uff0c8B\u6a21\u578b\u5728\u4e8b\u5b9e\u51c6\u786e\u6027\u3001\u6d41\u7545\u6027\u548c\u4e2a\u6027\u5316\u6307\u6807\u4e0a\u4e0e14-32B\u5927\u6a21\u578b\u6027\u80fd\u76f8\u5f53\uff0c\u540c\u65f6\u6210\u672c\u964d\u4f4e80%\u3002", "conclusion": "\u901a\u8fc7\u7cbe\u5fc3\u6570\u636e\u7b56\u5212\u548c\u884c\u4e3a\u6574\u5408\uff0c\u5c0f\u6a21\u578b\u53ef\u4ee5\u5728\u91d1\u878d\u54a8\u8be2\u4efb\u52a1\u4e0a\u5b9e\u73b0\u4e0e\u5927\u6a21\u578b\u76f8\u5f53\u7684\u6027\u80fd\uff0c\u663e\u8457\u964d\u4f4e\u6210\u672c\u3002", "relevance": 65.0}}
{"id": "2509.13713", "pdf": "https://arxiv.org/pdf/2509.13713", "abs": "https://arxiv.org/abs/2509.13713", "authors": ["Tae-Wook Um", "Ki-Hyeon Kim", "Hyun-Duck Choi", "Hyo-Sung Ahn"], "title": "UM-Depth : Uncertainty Masked Self-Supervised Monocular Depth Estimation with Visual Odometry", "categories": ["cs.CV"], "comment": null, "summary": "Monocular depth estimation has been increasingly adopted in robotics and\nautonomous driving for its ability to infer scene geometry from a single\ncamera. In self-supervised monocular depth estimation frameworks, the network\njointly generates and exploits depth and pose estimates during training,\nthereby eliminating the need for depth labels. However, these methods remain\nchallenged by uncertainty in the input data, such as low-texture or dynamic\nregions, which can cause reduced depth accuracy. To address this, we introduce\nUM-Depth, a framework that combines motion- and uncertainty-aware refinement to\nenhance depth accuracy at dynamic object boundaries and in textureless regions.\nSpecifically, we develop a teacherstudent training strategy that embeds\nuncertainty estimation into both the training pipeline and network\narchitecture, thereby strengthening supervision where photometric signals are\nweak. Unlike prior motion-aware approaches that incur inference-time overhead\nand rely on additional labels or auxiliary networks for real-time generation,\nour method uses optical flow exclusively within the teacher network during\ntraining, which eliminating extra labeling demands and any runtime cost.\nExtensive experiments on the KITTI and Cityscapes datasets demonstrate the\neffectiveness of our uncertainty-aware refinement. Overall, UM-Depth achieves\nstate-of-the-art results in both self-supervised depth and pose estimation on\nthe KITTI datasets.", "AI": {"tldr": "UM-Depth\u662f\u4e00\u4e2a\u81ea\u76d1\u7763\u5355\u76ee\u6df1\u5ea6\u4f30\u8ba1\u6846\u67b6\uff0c\u901a\u8fc7\u8fd0\u52a8\u611f\u77e5\u548c\u4e0d\u786e\u5b9a\u6027\u611f\u77e5\u7684\u7ec6\u5316\u65b9\u6cd5\uff0c\u5728\u52a8\u6001\u7269\u4f53\u8fb9\u754c\u548c\u7eb9\u7406\u7f3a\u5931\u533a\u57df\u63d0\u9ad8\u6df1\u5ea6\u4f30\u8ba1\u7cbe\u5ea6\uff0c\u65e0\u9700\u989d\u5916\u6807\u7b7e\u6216\u8fd0\u884c\u65f6\u5f00\u9500\u3002", "motivation": "\u81ea\u76d1\u7763\u5355\u76ee\u6df1\u5ea6\u4f30\u8ba1\u65b9\u6cd5\u5728\u4f4e\u7eb9\u7406\u6216\u52a8\u6001\u533a\u57df\u5b58\u5728\u4e0d\u786e\u5b9a\u6027\uff0c\u5bfc\u81f4\u6df1\u5ea6\u7cbe\u5ea6\u964d\u4f4e\u3002\u9700\u8981\u4e00\u79cd\u65b9\u6cd5\u80fd\u591f\u5728\u8fd9\u4e9b\u6311\u6218\u6027\u533a\u57df\u589e\u5f3a\u6df1\u5ea6\u4f30\u8ba1\u51c6\u786e\u6027\uff0c\u540c\u65f6\u907f\u514d\u63a8\u7406\u65f6\u7684\u989d\u5916\u8ba1\u7b97\u5f00\u9500\u3002", "method": "\u63d0\u51faUM-Depth\u6846\u67b6\uff0c\u91c7\u7528\u5e08\u751f\u8bad\u7ec3\u7b56\u7565\uff0c\u5c06\u4e0d\u786e\u5b9a\u6027\u4f30\u8ba1\u5d4c\u5165\u8bad\u7ec3\u6d41\u7a0b\u548c\u7f51\u7edc\u67b6\u6784\u3002\u4ec5\u5728\u6559\u5e08\u7f51\u7edc\u8bad\u7ec3\u65f6\u4f7f\u7528\u5149\u6d41\uff0c\u907f\u514d\u63a8\u7406\u65f6\u989d\u5916\u8ba1\u7b97\u3002\u7ed3\u5408\u8fd0\u52a8\u611f\u77e5\u548c\u4e0d\u786e\u5b9a\u6027\u611f\u77e5\u7684\u7ec6\u5316\u65b9\u6cd5\u3002", "result": "\u5728KITTI\u548cCityscapes\u6570\u636e\u96c6\u4e0a\u7684\u5e7f\u6cdb\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u5728\u4e0d\u786e\u5b9a\u6027\u611f\u77e5\u7ec6\u5316\u65b9\u9762\u6709\u6548\u3002\u5728KITTI\u6570\u636e\u96c6\u4e0a\u5b9e\u73b0\u4e86\u81ea\u76d1\u7763\u6df1\u5ea6\u548c\u59ff\u6001\u4f30\u8ba1\u7684\u6700\u5148\u8fdb\u7ed3\u679c\u3002", "conclusion": "UM-Depth\u901a\u8fc7\u7ed3\u5408\u8fd0\u52a8\u611f\u77e5\u548c\u4e0d\u786e\u5b9a\u6027\u611f\u77e5\u7684\u7ec6\u5316\uff0c\u6210\u529f\u63d0\u9ad8\u4e86\u5728\u52a8\u6001\u7269\u4f53\u8fb9\u754c\u548c\u7eb9\u7406\u7f3a\u5931\u533a\u57df\u7684\u6df1\u5ea6\u4f30\u8ba1\u7cbe\u5ea6\uff0c\u4e14\u65e0\u9700\u989d\u5916\u6807\u7b7e\u6216\u8fd0\u884c\u65f6\u6210\u672c\u3002", "relevance": 25.0}}
{"id": "2509.13974", "pdf": "https://arxiv.org/pdf/2509.13974", "abs": "https://arxiv.org/abs/2509.13974", "authors": ["Amirhossein Shahbazinia", "Jonathan Dan", "Jose A. Miranda", "Giovanni Ansaloni", "David Atienza"], "title": "Personalization on a Budget: Minimally-Labeled Continual Learning for Resource-Efficient Seizure Detection", "categories": ["cs.LG", "eess.SP"], "comment": null, "summary": "Objective: Epilepsy, a prevalent neurological disease, demands careful\ndiagnosis and continuous care. Seizure detection remains challenging, as\ncurrent clinical practice relies on expert analysis of electroencephalography,\nwhich is a time-consuming process and requires specialized knowledge.\nAddressing this challenge, this paper explores automated epileptic seizure\ndetection using deep learning, focusing on personalized continual learning\nmodels that adapt to each patient's unique electroencephalography signal\nfeatures, which evolve over time. Methods: In this context, our approach\naddresses the challenge of integrating new data into existing models without\ncatastrophic forgetting, a common issue in static deep learning models. We\npropose EpiSMART, a continual learning framework for seizure detection that\nuses a size-constrained replay buffer and an informed sample selection strategy\nto incrementally adapt to patient-specific electroencephalography signals. By\nselectively retaining high-entropy and seizure-predicted samples, our method\npreserves critical past information while maintaining high performance with\nminimal memory and computational requirements. Results: Validation on the\nCHB-MIT dataset, shows that EpiSMART achieves a 21% improvement in the F1 score\nover a trained baseline without updates in all other patients. On average,\nEpiSMART requires only 6.46 minutes of labeled data and 6.28 updates per day,\nmaking it suitable for real-time deployment in wearable systems.\nConclusion:EpiSMART enables robust and personalized seizure detection under\nrealistic and resource-constrained conditions by effectively integrating new\ndata into existing models without degrading past knowledge. Significance: This\nframework advances automated seizure detection by providing a continual\nlearning approach that supports patient-specific adaptation and practical\ndeployment in wearable healthcare systems.", "AI": {"tldr": "EpiSMART\u662f\u4e00\u4e2a\u7528\u4e8e\u766b\u75eb\u53d1\u4f5c\u68c0\u6d4b\u7684\u6301\u7eed\u5b66\u4e60\u6846\u67b6\uff0c\u901a\u8fc7\u5927\u5c0f\u53d7\u9650\u7684\u56de\u653e\u7f13\u51b2\u533a\u548c\u667a\u80fd\u6837\u672c\u9009\u62e9\u7b56\u7565\uff0c\u5b9e\u73b0\u60a3\u8005\u7279\u5f02\u6027EEG\u4fe1\u53f7\u7684\u81ea\u9002\u5e94\u5b66\u4e60\uff0c\u907f\u514d\u707e\u96be\u6027\u9057\u5fd8\u3002", "motivation": "\u766b\u75eb\u8bca\u65ad\u4f9d\u8d56\u4e13\u5bb6\u5206\u6790EEG\u4fe1\u53f7\uff0c\u8017\u65f6\u4e14\u9700\u8981\u4e13\u4e1a\u77e5\u8bc6\u3002\u73b0\u6709\u6df1\u5ea6\u5b66\u4e60\u6a21\u578b\u96be\u4ee5\u9002\u5e94\u60a3\u8005EEG\u7279\u5f81\u7684\u65f6\u53d8\u7279\u6027\uff0c\u5b58\u5728\u707e\u96be\u6027\u9057\u5fd8\u95ee\u9898\u3002", "method": "\u63d0\u51faEpiSMART\u6846\u67b6\uff0c\u4f7f\u7528\u5927\u5c0f\u53d7\u9650\u7684\u56de\u653e\u7f13\u51b2\u533a\u548c\u57fa\u4e8e\u4fe1\u606f\u71b5\u7684\u6837\u672c\u9009\u62e9\u7b56\u7565\uff0c\u9009\u62e9\u6027\u4fdd\u7559\u9ad8\u71b5\u548c\u9884\u6d4b\u4e3a\u53d1\u4f5c\u7684\u6837\u672c\uff0c\u4ee5\u589e\u91cf\u65b9\u5f0f\u9002\u5e94\u60a3\u8005\u7279\u5f02\u6027EEG\u4fe1\u53f7\u3002", "result": "\u5728CHB-MIT\u6570\u636e\u96c6\u4e0a\u9a8c\u8bc1\uff0cEpiSMART\u76f8\u6bd4\u65e0\u66f4\u65b0\u7684\u57fa\u7ebf\u6a21\u578bF1\u5206\u6570\u63d0\u534721%\uff0c\u5e73\u5747\u6bcf\u5929\u4ec5\u97006.46\u5206\u949f\u6807\u8bb0\u6570\u636e\u548c6.28\u6b21\u66f4\u65b0\uff0c\u9002\u5408\u53ef\u7a7f\u6234\u8bbe\u5907\u5b9e\u65f6\u90e8\u7f72\u3002", "conclusion": "EpiSMART\u80fd\u591f\u5728\u8d44\u6e90\u53d7\u9650\u6761\u4ef6\u4e0b\u5b9e\u73b0\u9c81\u68d2\u7684\u4e2a\u6027\u5316\u766b\u75eb\u53d1\u4f5c\u68c0\u6d4b\uff0c\u6709\u6548\u6574\u5408\u65b0\u6570\u636e\u800c\u4e0d\u635f\u5bb3\u5df2\u6709\u77e5\u8bc6\u3002", "relevance": 30.0}}
{"id": "2509.14197", "pdf": "https://arxiv.org/pdf/2509.14197", "abs": "https://arxiv.org/abs/2509.14197", "authors": ["Vahid Ghafouri", "Robert McNeil", "Teodor Yankov", "Madeleine Sumption", "Luc Rocher", "Scott A. Hale", "Adam Mahdi"], "title": "Framing Migration: A Computational Analysis of UK Parliamentary Discourse", "categories": ["cs.CL", "cs.CY"], "comment": null, "summary": "We present a large-scale computational analysis of migration-related\ndiscourse in UK parliamentary debates spanning over 75 years and compare it\nwith US congressional discourse. Using open-weight LLMs, we annotate each\nstatement with high-level stances toward migrants and track the net tone toward\nmigrants across time and political parties. For the UK, we extend this with a\nsemi-automated framework for extracting fine-grained narrative frames to\ncapture nuances of migration discourse. Our findings show that, while US\ndiscourse has grown increasingly polarised, UK parliamentary attitudes remain\nrelatively aligned across parties, with a persistent ideological gap between\nLabour and the Conservatives, reaching its most negative level in 2025. The\nanalysis of narrative frames in the UK parliamentary statements reveals a shift\ntoward securitised narratives such as border control and illegal immigration,\nwhile longer-term integration-oriented frames such as social integration have\ndeclined. Moreover, discussions of national law about immigration have been\nreplaced over time by international law and human rights, revealing nuances in\ndiscourse trends. Taken together broadly, our findings demonstrate how LLMs can\nsupport scalable, fine-grained discourse analysis in political and historical\ncontexts.", "AI": {"tldr": "\u4f7f\u7528\u5f00\u6e90\u5927\u8bed\u8a00\u6a21\u578b\u5bf9\u82f1\u56fd\u8bae\u4f1a75\u5e74\u79fb\u6c11\u8fa9\u8bba\u8fdb\u884c\u5927\u89c4\u6a21\u8ba1\u7b97\u5206\u6790\uff0c\u53d1\u73b0\u4e0e\u7f8e\u56fd\u4e0d\u540c\uff0c\u82f1\u56fd\u653f\u515a\u95f4\u6001\u5ea6\u76f8\u5bf9\u4e00\u81f4\u4f46\u5b58\u5728\u610f\u8bc6\u5f62\u6001\u5dee\u8ddd\uff0c\u53d9\u4e8b\u6846\u67b6\u5411\u5b89\u5168\u5316\u8bae\u9898\u8f6c\u53d8\u3002", "motivation": "\u7814\u7a76\u65e8\u5728\u5229\u7528\u5927\u8bed\u8a00\u6a21\u578b\u8fdb\u884c\u5927\u89c4\u6a21\u653f\u6cbb\u8bdd\u8bed\u5206\u6790\uff0c\u6bd4\u8f83\u82f1\u7f8e\u79fb\u6c11\u653f\u7b56\u8fa9\u8bba\u7684\u957f\u671f\u8d8b\u52bf\u548c\u5dee\u5f02\uff0c\u63a2\u7d22LLM\u5728\u653f\u6cbb\u5386\u53f2\u8bed\u5883\u4e2d\u7684\u7ec6\u7c92\u5ea6\u5206\u6790\u80fd\u529b\u3002", "method": "\u4f7f\u7528\u5f00\u6e90\u6743\u91cdLLM\u5bf9\u8bae\u4f1a\u8fa9\u8bba\u58f0\u660e\u8fdb\u884c\u9ad8\u5c42\u7acb\u573a\u6807\u6ce8\uff0c\u91c7\u7528\u534a\u81ea\u52a8\u5316\u6846\u67b6\u63d0\u53d6\u7ec6\u7c92\u5ea6\u53d9\u4e8b\u6846\u67b6\uff0c\u8fdb\u884c\u8de8\u65f6\u95f4\u548c\u653f\u515a\u7684\u51c0\u6001\u5ea6\u8ffd\u8e2a\u3002", "result": "\u7f8e\u56fd\u8bdd\u8bed\u65e5\u76ca\u4e24\u6781\u5206\u5316\uff0c\u82f1\u56fd\u653f\u515a\u6001\u5ea6\u76f8\u5bf9\u4e00\u81f4\u4f46\u5de5\u515a\u4e0e\u4fdd\u5b88\u515a\u5b58\u5728\u6301\u7eed\u610f\u8bc6\u5f62\u6001\u5dee\u8ddd\uff1b\u82f1\u56fd\u53d9\u4e8b\u6846\u67b6\u5411\u8fb9\u5883\u7ba1\u63a7\u7b49\u5b89\u5168\u5316\u8bae\u9898\u8f6c\u53d8\uff0c\u793e\u4f1a\u878d\u5408\u7b49\u957f\u671f\u6574\u5408\u6846\u67b6\u51cf\u5c11\uff1b\u79fb\u6c11\u8ba8\u8bba\u4ece\u56fd\u5185\u6cd5\u8f6c\u5411\u56fd\u9645\u6cd5\u548c\u4eba\u6743\u3002", "conclusion": "\u5927\u8bed\u8a00\u6a21\u578b\u80fd\u591f\u652f\u6301\u653f\u6cbb\u5386\u53f2\u8bed\u5883\u4e2d\u53ef\u6269\u5c55\u7684\u7ec6\u7c92\u5ea6\u8bdd\u8bed\u5206\u6790\uff0c\u4e3a\u7406\u89e3\u957f\u671f\u653f\u7b56\u8fa9\u8bba\u8d8b\u52bf\u63d0\u4f9b\u4e86\u6709\u6548\u5de5\u5177\u3002", "relevance": 65.0}}
{"id": "2509.13722", "pdf": "https://arxiv.org/pdf/2509.13722", "abs": "https://arxiv.org/abs/2509.13722", "authors": ["Dingwei Zhang", "Dong Zhang", "Jinhui Tang"], "title": "Mitigating Query Selection Bias in Referring Video Object Segmentation", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "Recently, query-based methods have achieved remarkable performance in\nReferring Video Object Segmentation (RVOS) by using textual static object\nqueries to drive cross-modal alignment. However, these static queries are\neasily misled by distractors with similar appearance or motion, resulting in\n\\emph{query selection bias}. To address this issue, we propose Triple Query\nFormer (TQF), which factorizes the referring query into three specialized\ncomponents: an appearance query for static attributes, an intra-frame\ninteraction query for spatial relations, and an inter-frame motion query for\ntemporal association. Instead of relying solely on textual embeddings, our\nqueries are dynamically constructed by integrating both linguistic cues and\nvisual guidance. Furthermore, we introduce two motion-aware aggregation modules\nthat enhance object token representations: Intra-frame Interaction Aggregation\nincorporates position-aware interactions among objects within a single frame,\nwhile Inter-frame Motion Aggregation leverages trajectory-guided alignment\nacross frames to ensure temporal coherence. Extensive experiments on multiple\nRVOS benchmarks demonstrate the advantages of TQF and the effectiveness of our\nstructured query design and motion-aware aggregation modules.", "AI": {"tldr": "TQF\u5c06\u5f15\u7528\u67e5\u8be2\u5206\u89e3\u4e3a\u4e09\u4e2a\u4e13\u95e8\u7ec4\u4ef6\uff1a\u5916\u89c2\u67e5\u8be2\u3001\u5e27\u5185\u4ea4\u4e92\u67e5\u8be2\u548c\u5e27\u95f4\u8fd0\u52a8\u67e5\u8be2\uff0c\u901a\u8fc7\u52a8\u6001\u6784\u5efa\u67e5\u8be2\u548c\u8fd0\u52a8\u611f\u77e5\u805a\u5408\u6a21\u5757\u89e3\u51b3\u67e5\u8be2\u9009\u62e9\u504f\u5dee\u95ee\u9898\uff0c\u5728\u591a\u4e2aRVOS\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8868\u73b0\u51fa\u8272\u3002", "motivation": "\u73b0\u6709\u7684\u57fa\u4e8e\u67e5\u8be2\u7684\u5f15\u7528\u89c6\u9891\u5bf9\u8c61\u5206\u5272\u65b9\u6cd5\u4f7f\u7528\u9759\u6001\u6587\u672c\u67e5\u8be2\u8fdb\u884c\u8de8\u6a21\u6001\u5bf9\u9f50\uff0c\u4f46\u5bb9\u6613\u88ab\u5916\u89c2\u6216\u8fd0\u52a8\u76f8\u4f3c\u7684\u5e72\u6270\u7269\u8bef\u5bfc\uff0c\u5bfc\u81f4\u67e5\u8be2\u9009\u62e9\u504f\u5dee\u95ee\u9898\u3002", "method": "\u63d0\u51faTriple Query Former (TQF)\uff0c\u5c06\u5f15\u7528\u67e5\u8be2\u5206\u89e3\u4e3a\u4e09\u4e2a\u4e13\u95e8\u7ec4\u4ef6\uff1a\u5916\u89c2\u67e5\u8be2\uff08\u9759\u6001\u5c5e\u6027\uff09\u3001\u5e27\u5185\u4ea4\u4e92\u67e5\u8be2\uff08\u7a7a\u95f4\u5173\u7cfb\uff09\u548c\u5e27\u95f4\u8fd0\u52a8\u67e5\u8be2\uff08\u65f6\u95f4\u5173\u8054\uff09\u3002\u67e5\u8be2\u901a\u8fc7\u8bed\u8a00\u7ebf\u7d22\u548c\u89c6\u89c9\u6307\u5bfc\u52a8\u6001\u6784\u5efa\uff0c\u5e76\u5f15\u5165\u5e27\u5185\u4ea4\u4e92\u805a\u5408\u548c\u5e27\u95f4\u8fd0\u52a8\u805a\u5408\u4e24\u4e2a\u8fd0\u52a8\u611f\u77e5\u6a21\u5757\u6765\u589e\u5f3a\u5bf9\u8c61\u6807\u8bb0\u8868\u793a\u3002", "result": "\u5728\u591a\u4e2aRVOS\u57fa\u51c6\u6d4b\u8bd5\u4e0a\u7684\u5e7f\u6cdb\u5b9e\u9a8c\u8bc1\u660e\u4e86TQF\u7684\u4f18\u52bf\u4ee5\u53ca\u7ed3\u6784\u5316\u67e5\u8be2\u8bbe\u8ba1\u548c\u8fd0\u52a8\u611f\u77e5\u805a\u5408\u6a21\u5757\u7684\u6709\u6548\u6027\u3002", "conclusion": "\u901a\u8fc7\u5c06\u5f15\u7528\u67e5\u8be2\u5206\u89e3\u4e3a\u4e09\u4e2a\u4e13\u95e8\u7ec4\u4ef6\u5e76\u5f15\u5165\u8fd0\u52a8\u611f\u77e5\u805a\u5408\u673a\u5236\uff0cTQF\u6709\u6548\u89e3\u51b3\u4e86\u67e5\u8be2\u9009\u62e9\u504f\u5dee\u95ee\u9898\uff0c\u63d0\u5347\u4e86\u5f15\u7528\u89c6\u9891\u5bf9\u8c61\u5206\u5272\u7684\u6027\u80fd\u3002", "relevance": 35.0}}
{"id": "2509.14000", "pdf": "https://arxiv.org/pdf/2509.14000", "abs": "https://arxiv.org/abs/2509.14000", "authors": ["Ivana Kesi\u0107", "Alja\u017e Blatnik", "Carolina Fortuna", "Bla\u017e Bertalani\u010d"], "title": "Deep Temporal Graph Networks for Real-Time Correction of GNSS Jamming-Induced Deviations", "categories": ["cs.LG"], "comment": "20 pages, 4 figures", "summary": "Global Navigation Satellite Systems (GNSS) are increasingly disrupted by\nintentional jamming, degrading availability precisely when positioning and\ntiming must remain operational. We address this by reframing jamming mitigation\nas dynamic graph regression and introducing a receiver-centric deep temporal\ngraph network that predicts, and thus corrects, the receivers horizontal\ndeviation in real time. At each 1 Hz epoch, the satellite receiver environment\nis represented as a heterogeneous star graph (receiver center, tracked\nsatellites as leaves) with time varying attributes (e.g., SNR, azimuth,\nelevation, latitude/longitude). A single layer Heterogeneous Graph ConvLSTM\n(HeteroGCLSTM) aggregates one hop spatial context and temporal dynamics over a\nshort history to output the 2D deviation vector applied for on the fly\ncorrection.\n  We evaluate on datasets from two distinct receivers under three jammer\nprofiles, continuous wave (cw), triple tone (cw3), and wideband FM, each\nexercised at six power levels between -45 and -70 dBm, with 50 repetitions per\nscenario (prejam/jam/recovery). Against strong multivariate time series\nbaselines (MLP, uniform CNN, and Seq2Point CNN), our model consistently attains\nthe lowest mean absolute error (MAE). At -45 dBm, it achieves 3.64 cm\n(GP01/cw), 7.74 cm (GP01/cw3), 4.41 cm (ublox/cw), 4.84 cm (ublox/cw3), and\n4.82 cm (ublox/FM), improving to 1.65-2.08 cm by -60 to -70 dBm. On mixed mode\ndatasets pooling all powers, MAE is 3.78 cm (GP01) and 4.25 cm (ublox10),\noutperforming Seq2Point, MLP, and CNN. A split study shows superior data\nefficiency: with only 10\\% training data our approach remains well ahead of\nbaselines (20 cm vs. 36-42 cm).", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u52a8\u6001\u56fe\u56de\u5f52\u7684GNSS\u5e72\u6270\u6291\u5236\u65b9\u6cd5\uff0c\u4f7f\u7528\u5f02\u6784\u56fe\u5377\u79efLSTM\u7f51\u7edc\u5b9e\u65f6\u9884\u6d4b\u5e76\u4fee\u6b63\u63a5\u6536\u5668\u7684\u6c34\u5e73\u504f\u5dee\uff0c\u5728\u591a\u79cd\u5e72\u6270\u573a\u666f\u4e0b\u663e\u8457\u4f18\u4e8e\u4f20\u7edf\u65f6\u95f4\u5e8f\u5217\u57fa\u7ebf\u65b9\u6cd5\u3002", "motivation": "\u5168\u7403\u5bfc\u822a\u536b\u661f\u7cfb\u7edf(GNSS)\u9762\u4e34\u65e5\u76ca\u4e25\u91cd\u7684\u6545\u610f\u5e72\u6270\u95ee\u9898\uff0c\u5728\u9700\u8981\u7cbe\u786e\u5b9a\u4f4d\u548c\u8ba1\u65f6\u65f6\u5bfc\u81f4\u7cfb\u7edf\u53ef\u7528\u6027\u4e0b\u964d\u3002\u4f20\u7edf\u65b9\u6cd5\u96be\u4ee5\u6709\u6548\u5904\u7406\u8fd9\u79cd\u52a8\u6001\u5e72\u6270\u73af\u5883\u3002", "method": "\u5c06\u5e72\u6270\u6291\u5236\u95ee\u9898\u91cd\u6784\u4e3a\u52a8\u6001\u56fe\u56de\u5f52\u4efb\u52a1\uff0c\u6784\u5efa\u63a5\u6536\u5668\u4e3a\u4e2d\u5fc3\u7684\u5f02\u6784\u661f\u5f62\u56fe\uff08\u63a5\u6536\u5668\u4e3a\u4e2d\u5fc3\u8282\u70b9\uff0c\u536b\u661f\u4e3a\u53f6\u8282\u70b9\uff09\uff0c\u4f7f\u7528\u5355\u5c42\u5f02\u6784\u56fe\u5377\u79efLSTM(HeteroGCLSTM)\u805a\u5408\u7a7a\u95f4\u4e0a\u4e0b\u6587\u548c\u65f6\u95f4\u52a8\u6001\u4fe1\u606f\uff0c\u5b9e\u65f6\u8f93\u51fa2D\u504f\u5dee\u5411\u91cf\u8fdb\u884c\u5728\u7ebf\u4fee\u6b63\u3002", "result": "\u5728\u4e24\u79cd\u4e0d\u540c\u63a5\u6536\u5668\u548c\u4e09\u79cd\u5e72\u6270\u6a21\u5f0f\u4e0b\uff08\u8fde\u7eed\u6ce2\u3001\u4e09\u97f3\u8c03\u3001\u5bbd\u5e26FM\uff09\uff0c\u8be5\u6a21\u578b\u5728\u6240\u6709\u529f\u7387\u6c34\u5e73\uff08-45\u81f3-70 dBm\uff09\u4e0b\u5747\u83b7\u5f97\u6700\u4f4e\u5e73\u5747\u7edd\u5bf9\u8bef\u5dee(MAE)\uff0c\u5728-45 dBm\u65f6\u8fbe\u52303.64-7.74 cm\u7cbe\u5ea6\uff0c\u5728-60\u81f3-70 dBm\u65f6\u63d0\u5347\u81f31.65-2.08 cm\u3002\u6570\u636e\u6548\u7387\u7814\u7a76\u663e\u793a\u4ec5\u752810%\u8bad\u7ec3\u6570\u636e\u4ecd\u4f18\u4e8e\u57fa\u7ebf\u65b9\u6cd5\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u901a\u8fc7\u56fe\u795e\u7ecf\u7f51\u7edc\u6709\u6548\u5904\u7406GNSS\u5e72\u6270\u95ee\u9898\uff0c\u5728\u7cbe\u5ea6\u548c\u6570\u636e\u6548\u7387\u65b9\u9762\u5747\u663e\u8457\u4f18\u4e8e\u4f20\u7edf\u65f6\u95f4\u5e8f\u5217\u65b9\u6cd5\uff0c\u4e3a\u5b9e\u65f6\u5e72\u6270\u6291\u5236\u63d0\u4f9b\u4e86\u6709\u6548\u89e3\u51b3\u65b9\u6848\u3002", "relevance": 25.0}}
{"id": "2509.13355", "pdf": "https://arxiv.org/pdf/2509.13355", "abs": "https://arxiv.org/abs/2509.13355", "authors": ["Dietmar Offenhuber"], "title": "Synthetic Data and the Shifting Ground of Truth", "categories": ["cs.CY", "cs.AI", "cs.LG"], "comment": "Talk presented at the Society for the Social Studies of Science (4S)\n  2025 meeting in Seattle, Sept. 3, 2025", "summary": "The emergence of synthetic data for privacy protection, training data\ngeneration, or simply convenient access to quasi-realistic data in any shape or\nvolume complicates the concept of ground truth. Synthetic data mimic real-world\nobservations, but do not refer to external features. This lack of a\nrepresentational relationship, however, not prevent researchers from using\nsynthetic data as training data for AI models and ground truth repositories. It\nis claimed that the lack of data realism is not merely an acceptable tradeoff,\nbut often leads to better model performance than realistic data: compensate for\nknown biases, prevent overfitting and support generalization, and make the\nmodels more robust in dealing with unexpected outliers. Indeed, injecting noisy\nand outright implausible data into training sets can be beneficial for the\nmodel. This greatly complicates usual assumptions based on which\nrepresentational accuracy determines data fidelity (garbage in - garbage out).\nFurthermore, ground truth becomes a self-referential affair, in which the\nlabels used as a ground truth repository are themselves synthetic products of a\ngenerative model and as such not connected to real-world observations. My paper\nexamines how ML researchers and practitioners bootstrap ground truth under such\nparadoxical circumstances without relying on the stable ground of\nrepresentation and real-world reference. It will also reflect on the broader\nimplications of a shift from a representational to what could be described as a\nmimetic or iconic concept of data.", "AI": {"tldr": "\u8bba\u6587\u63a2\u8ba8\u5408\u6210\u6570\u636e\u5982\u4f55\u6311\u6218\u4f20\u7edf\"\u5783\u573e\u8fdb\u5783\u573e\u51fa\"\u5047\u8bbe\uff0c\u5206\u6790\u5728\u6ca1\u6709\u771f\u5b9e\u4e16\u754c\u53c2\u7167\u7684\u60c5\u51b5\u4e0b\u5982\u4f55\u6784\u5efaground truth\uff0c\u5e76\u8ba8\u8bba\u4ece\u8868\u5f81\u6027\u6570\u636e\u5411\u6a21\u4eff\u6027\u6570\u636e\u6982\u5ff5\u7684\u8f6c\u53d8\u3002", "motivation": "\u7814\u7a76\u5408\u6210\u6570\u636e\u5728AI\u8bad\u7ec3\u4e2d\u7684\u4f7f\u7528\u5982\u4f55\u98a0\u8986\u4f20\u7edf\u7684\u6570\u636e\u4fdd\u771f\u5ea6\u6982\u5ff5\uff0c\u63a2\u7d22\u5728\u6ca1\u6709\u771f\u5b9e\u4e16\u754c\u53c2\u7167\u7684\u60c5\u51b5\u4e0b\u5982\u4f55\u5efa\u7acb\u6709\u6548\u7684ground truth\u7cfb\u7edf\u3002", "method": "\u901a\u8fc7\u7406\u8bba\u5206\u6790\u63a2\u8ba8\u5408\u6210\u6570\u636e\u7684\u7279\u6027\u53ca\u5176\u5bf9\u673a\u5668\u5b66\u4e60\u57fa\u7840\u5047\u8bbe\u7684\u5f71\u54cd\uff0c\u8003\u5bdf\u7814\u7a76\u4eba\u5458\u5728\u5b9e\u8df5\u4e2d\u5982\u4f55\u5e94\u5bf9ground truth\u7684\u81ea\u6307\u6027\u6096\u8bba\u3002", "result": "\u53d1\u73b0\u5408\u6210\u6570\u636e\u867d\u7136\u7f3a\u4e4f\u771f\u5b9e\u4e16\u754c\u53c2\u7167\uff0c\u4f46\u53ef\u80fd\u901a\u8fc7\u8865\u507f\u5df2\u77e5\u504f\u5dee\u3001\u9632\u6b62\u8fc7\u62df\u5408\u548c\u652f\u6301\u6cdb\u5316\u6765\u63d0\u5347\u6a21\u578b\u6027\u80fd\uff0c\u751a\u81f3\u6ce8\u5165\u566a\u58f0\u548c\u4e0d\u53ef\u4fe1\u6570\u636e\u4e5f\u53ef\u80fd\u6709\u76ca\u3002", "conclusion": "\u4f20\u7edf\u57fa\u4e8e\u8868\u5f81\u51c6\u786e\u6027\u7684\u6570\u636e\u4fdd\u771f\u5ea6\u5047\u8bbe\u9700\u8981\u91cd\u65b0\u5ba1\u89c6\uff0cground truth\u6b63\u5728\u4ece\u8868\u5f81\u6027\u6982\u5ff5\u8f6c\u5411\u6a21\u4eff\u6027\u6982\u5ff5\uff0c\u8fd9\u5bf9AI\u7814\u7a76\u548c\u5b9e\u8df5\u5177\u6709\u6df1\u8fdc\u5f71\u54cd\u3002", "relevance": 65.0}}
{"id": "2509.14233", "pdf": "https://arxiv.org/pdf/2509.14233", "abs": "https://arxiv.org/abs/2509.14233", "authors": ["Alejandro Hern\u00e1ndez-Cano", "Alexander H\u00e4gele", "Allen Hao Huang", "Angelika Romanou", "Antoni-Joan Solergibert", "Barna Pasztor", "Bettina Messmer", "Dhia Garbaya", "Eduard Frank \u010eurech", "Ido Hakimi", "Juan Garc\u00eda Giraldo", "Mete Ismayilzada", "Negar Foroutan", "Skander Moalla", "Tiancheng Chen", "Vinko Sabol\u010dec", "Yixuan Xu", "Michael Aerni", "Badr AlKhamissi", "Ines Altemir Marinas", "Mohammad Hossein Amani", "Matin Ansaripour", "Ilia Badanin", "Harold Benoit", "Emanuela Boros", "Nicholas Browning", "Fabian B\u00f6sch", "Maximilian B\u00f6ther", "Niklas Canova", "Camille Challier", "Clement Charmillot", "Jonathan Coles", "Jan Deriu", "Arnout Devos", "Lukas Drescher", "Daniil Dzenhaliou", "Maud Ehrmann", "Dongyang Fan", "Simin Fan", "Silin Gao", "Miguel Gila", "Mar\u00eda Grandury", "Diba Hashemi", "Alexander Hoyle", "Jiaming Jiang", "Mark Klein", "Andrei Kucharavy", "Anastasiia Kucherenko", "Frederike L\u00fcbeck", "Roman Machacek", "Theofilos Manitaras", "Andreas Marfurt", "Kyle Matoba", "Simon Matrenok", "Henrique Mendonc\u00e7a", "Fawzi Roberto Mohamed", "Syrielle Montariol", "Luca Mouchel", "Sven Najem-Meyer", "Jingwei Ni", "Gennaro Oliva", "Matteo Pagliardini", "Elia Palme", "Andrei Panferov", "L\u00e9o Paoletti", "Marco Passerini", "Ivan Pavlov", "Auguste Poiroux", "Kaustubh Ponkshe", "Nathan Ranchin", "Javi Rando", "Mathieu Sauser", "Jakhongir Saydaliev", "Muhammad Ali Sayfiddinov", "Marian Schneider", "Stefano Schuppli", "Marco Scialanga", "Andrei Semenov", "Kumar Shridhar", "Raghav Singhal", "Anna Sotnikova", "Alexander Sternfeld", "Ayush Kumar Tarun", "Paul Teiletche", "Jannis Vamvas", "Xiaozhe Yao", "Hao Zhao Alexander Ilic", "Ana Klimovic", "Andreas Krause", "Caglar Gulcehre", "David Rosenthal", "Elliott Ash", "Florian Tram\u00e8r", "Joost VandeVondele", "Livio Veraldi", "Martin Rajman", "Thomas Schulthess", "Torsten Hoefler", "Antoine Bosselut", "Martin Jaggi", "Imanol Schlag"], "title": "Apertus: Democratizing Open and Compliant LLMs for Global Language Environments", "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": null, "summary": "We present Apertus, a fully open suite of large language models (LLMs)\ndesigned to address two systemic shortcomings in today's open model ecosystem:\ndata compliance and multilingual representation. Unlike many prior models that\nrelease weights without reproducible data pipelines or regard for content-owner\nrights, Apertus models are pretrained exclusively on openly available data,\nretroactively respecting robots.txt exclusions and filtering for\nnon-permissive, toxic, and personally identifiable content. To mitigate risks\nof memorization, we adopt the Goldfish objective during pretraining, strongly\nsuppressing verbatim recall of data while retaining downstream task\nperformance. The Apertus models also expand multilingual coverage, training on\n15T tokens from over 1800 languages, with ~40% of pretraining data allocated to\nnon-English content. Released at 8B and 70B scales, Apertus approaches\nstate-of-the-art results among fully open models on multilingual benchmarks,\nrivalling or surpassing open-weight counterparts. Beyond model weights, we\nrelease all scientific artifacts from our development cycle with a permissive\nlicense, including data preparation scripts, checkpoints, evaluation suites,\nand training code, enabling transparent audit and extension.", "AI": {"tldr": "Apertus\u662f\u4e00\u4e2a\u5b8c\u5168\u5f00\u6e90\u7684LLM\u5957\u4ef6\uff0c\u4e13\u6ce8\u4e8e\u6570\u636e\u5408\u89c4\u6027\u548c\u591a\u8bed\u8a00\u8868\u793a\uff0c\u4f7f\u7528\u5f00\u653e\u53ef\u7528\u6570\u636e\u8bad\u7ec3\uff0c\u91c7\u7528Goldfish\u76ee\u6807\u6291\u5236\u8bb0\u5fc6\u5316\uff0c\u652f\u63011800\u591a\u79cd\u8bed\u8a00\uff0c\u57288B\u548c70B\u89c4\u6a21\u4e0a\u8fbe\u5230\u5148\u8fdb\u7684\u591a\u8bed\u8a00\u6027\u80fd\u3002", "motivation": "\u89e3\u51b3\u5f53\u524d\u5f00\u6e90\u6a21\u578b\u751f\u6001\u7cfb\u7edf\u4e2d\u6570\u636e\u5408\u89c4\u6027\u548c\u591a\u8bed\u8a00\u8868\u793a\u4e0d\u8db3\u7684\u95ee\u9898\uff0c\u63d0\u4f9b\u5b8c\u5168\u900f\u660e\u4e14\u5408\u89c4\u7684\u6a21\u578b\u5f00\u53d1\u6d41\u7a0b\u3002", "method": "\u4f7f\u7528\u5b8c\u5168\u5f00\u653e\u53ef\u7528\u6570\u636e\u8fdb\u884c\u9884\u8bad\u7ec3\uff0c\u9075\u5faarobots.txt\u6392\u9664\u89c4\u5219\uff0c\u8fc7\u6ee4\u975e\u8bb8\u53ef\u3001\u6709\u6bd2\u548c\u4e2a\u4eba\u8eab\u4efd\u4fe1\u606f\u5185\u5bb9\uff0c\u91c7\u7528Goldfish\u76ee\u6807\u6291\u5236\u8bb0\u5fc6\u5316\uff0c\u5728\u591a\u8bed\u8a00\u6570\u636e\u4e0a\u8bad\u7ec3\uff0815T tokens\uff0c1800+\u8bed\u8a00\uff0c40%\u975e\u82f1\u8bed\u5185\u5bb9\uff09\u3002", "result": "Apertus\u6a21\u578b\u5728\u591a\u8bed\u8a00\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8fbe\u5230\u6216\u8d85\u8d8a\u540c\u7c7b\u5f00\u6e90\u6743\u91cd\u6a21\u578b\u7684\u5148\u8fdb\u6c34\u5e73\uff0c\u540c\u65f6\u4fdd\u6301\u4e86\u6570\u636e\u5408\u89c4\u6027\u548c\u900f\u660e\u5ea6\u3002", "conclusion": "Apertus\u63d0\u4f9b\u4e86\u4e00\u4e2a\u5b8c\u5168\u5f00\u6e90\u3001\u6570\u636e\u5408\u89c4\u3001\u591a\u8bed\u8a00\u80fd\u529b\u5f3a\u7684LLM\u5957\u4ef6\uff0c\u4e3a\u900f\u660e\u5ba1\u8ba1\u548c\u6269\u5c55\u63d0\u4f9b\u4e86\u5b8c\u6574\u7684\u79d1\u5b66\u5de5\u4ef6\u3002", "relevance": 85.0}}
{"id": "2509.13747", "pdf": "https://arxiv.org/pdf/2509.13747", "abs": "https://arxiv.org/abs/2509.13747", "authors": ["Ming Dai", "Wenxuan Cheng", "Jiang-Jiang Liu", "Lingfeng Yang", "Zhenhua Feng", "Wankou Yang", "Jingdong Wang"], "title": "Improving Generalized Visual Grounding with Instance-aware Joint Learning", "categories": ["cs.CV"], "comment": "Accepted by IEEE Transactions on Pattern Analysis and Machine\n  Intelligence (TPAMI) in September 2025", "summary": "Generalized visual grounding tasks, including Generalized Referring\nExpression Comprehension (GREC) and Segmentation (GRES), extend the classical\nvisual grounding paradigm by accommodating multi-target and non-target\nscenarios. Specifically, GREC focuses on accurately identifying all referential\nobjects at the coarse bounding box level, while GRES aims for achieve\nfine-grained pixel-level perception. However, existing approaches typically\ntreat these tasks independently, overlooking the benefits of jointly training\nGREC and GRES to ensure consistent multi-granularity predictions and streamline\nthe overall process. Moreover, current methods often treat GRES as a semantic\nsegmentation task, neglecting the crucial role of instance-aware capabilities\nand the necessity of ensuring consistent predictions between instance-level\nboxes and masks. To address these limitations, we propose InstanceVG, a\nmulti-task generalized visual grounding framework equipped with instance-aware\ncapabilities, which leverages instance queries to unify the joint and\nconsistency predictions of instance-level boxes and masks. To the best of our\nknowledge, InstanceVG is the first framework to simultaneously tackle both GREC\nand GRES while incorporating instance-aware capabilities into generalized\nvisual grounding. To instantiate the framework, we assign each instance query a\nprior reference point, which also serves as an additional basis for target\nmatching. This design facilitates consistent predictions of points, boxes, and\nmasks for the same instance. Extensive experiments obtained on ten datasets\nacross four tasks demonstrate that InstanceVG achieves state-of-the-art\nperformance, significantly surpassing the existing methods in various\nevaluation metrics. The code and model will be publicly available at\nhttps://github.com/Dmmm1997/InstanceVG.", "AI": {"tldr": "InstanceVG\u662f\u4e00\u4e2a\u591a\u4efb\u52a1\u5e7f\u4e49\u89c6\u89c9\u5b9a\u4f4d\u6846\u67b6\uff0c\u9996\u6b21\u540c\u65f6\u5904\u7406GREC\u548cGRES\u4efb\u52a1\uff0c\u901a\u8fc7\u5b9e\u4f8b\u67e5\u8be2\u5b9e\u73b0\u5b9e\u4f8b\u7ea7\u6846\u548c\u63a9\u7801\u7684\u4e00\u81f4\u9884\u6d4b\uff0c\u5728\u591a\u4e2a\u6570\u636e\u96c6\u4e0a\u8fbe\u5230SOTA\u6027\u80fd\u3002", "motivation": "\u73b0\u6709\u65b9\u6cd5\u901a\u5e38\u72ec\u7acb\u5904\u7406\u5e7f\u4e49\u6307\u4ee3\u7406\u89e3(GREC)\u548c\u5206\u5272(GRES)\u4efb\u52a1\uff0c\u5ffd\u89c6\u4e86\u8054\u5408\u8bad\u7ec3\u7684\u4f18\u52bf\uff0c\u4e14GRES\u65b9\u6cd5\u7f3a\u4e4f\u5b9e\u4f8b\u611f\u77e5\u80fd\u529b\uff0c\u65e0\u6cd5\u4fdd\u8bc1\u5b9e\u4f8b\u7ea7\u6846\u548c\u63a9\u7801\u9884\u6d4b\u7684\u4e00\u81f4\u6027\u3002", "method": "\u63d0\u51faInstanceVG\u6846\u67b6\uff0c\u4f7f\u7528\u5b9e\u4f8b\u67e5\u8be2\u7edf\u4e00\u5b9e\u4f8b\u7ea7\u6846\u548c\u63a9\u7801\u7684\u8054\u5408\u4e00\u81f4\u6027\u9884\u6d4b\uff0c\u4e3a\u6bcf\u4e2a\u5b9e\u4f8b\u67e5\u8be2\u5206\u914d\u5148\u9a8c\u53c2\u8003\u70b9\uff0c\u4fc3\u8fdb\u70b9\u3001\u6846\u3001\u63a9\u7801\u7684\u4e00\u81f4\u9884\u6d4b\u3002", "result": "\u572810\u4e2a\u6570\u636e\u96c64\u4e2a\u4efb\u52a1\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0cInstanceVG\u5728\u5404\u9879\u8bc4\u4f30\u6307\u6807\u4e0a\u663e\u8457\u8d85\u8d8a\u73b0\u6709\u65b9\u6cd5\uff0c\u8fbe\u5230\u6700\u5148\u8fdb\u6027\u80fd\u3002", "conclusion": "InstanceVG\u662f\u9996\u4e2a\u540c\u65f6\u5904\u7406GREC\u548cGRES\u5e76\u878d\u5165\u5b9e\u4f8b\u611f\u77e5\u80fd\u529b\u7684\u5e7f\u4e49\u89c6\u89c9\u5b9a\u4f4d\u6846\u67b6\uff0c\u901a\u8fc7\u5b9e\u4f8b\u67e5\u8be2\u673a\u5236\u5b9e\u73b0\u4e86\u591a\u7c92\u5ea6\u9884\u6d4b\u7684\u4e00\u81f4\u6027\u3002", "relevance": 35.0}}
{"id": "2509.14024", "pdf": "https://arxiv.org/pdf/2509.14024", "abs": "https://arxiv.org/abs/2509.14024", "authors": ["Raouf Kerkouche", "Henrik Zunker", "Mario Fritz", "Martin J. K\u00fchn"], "title": "Differentially private federated learning for localized control of infectious disease dynamics", "categories": ["cs.LG", "68T07, 68P27, 92-08, 92-10"], "comment": "18 pages, 6 figures", "summary": "In times of epidemics, swift reaction is necessary to mitigate epidemic\nspreading. For this reaction, localized approaches have several advantages,\nlimiting necessary resources and reducing the impact of interventions on a\nlarger scale. However, training a separate machine learning (ML) model on a\nlocal scale is often not feasible due to limited available data. Centralizing\nthe data is also challenging because of its high sensitivity and privacy\nconstraints. In this study, we consider a localized strategy based on the\nGerman counties and communities managed by the related local health authorities\n(LHA). For the preservation of privacy to not oppose the availability of\ndetailed situational data, we propose a privacy-preserving forecasting method\nthat can assist public health experts and decision makers. ML methods with\nfederated learning (FL) train a shared model without centralizing raw data.\nConsidering the counties, communities or LHAs as clients and finding a balance\nbetween utility and privacy, we study a FL framework with client-level\ndifferential privacy (DP). We train a shared multilayer perceptron on sliding\nwindows of recent case counts to forecast the number of cases, while clients\nexchange only norm-clipped updates and the server aggregated updates with DP\nnoise. We evaluate the approach on COVID-19 data on county-level during two\nphases. As expected, very strict privacy yields unstable, unusable forecasts.\nAt a moderately strong level, the DP model closely approaches the non-DP model:\n$R^2= 0.94$ (vs. 0.95) and mean absolute percentage error (MAPE) of 26 % in\nNovember 2020; $R^2= 0.88$ (vs. 0.93) and MAPE of 21 % in March 2022. Overall,\nclient-level DP-FL can deliver useful county-level predictions with strong\nprivacy guarantees, and viable privacy budgets depend on epidemic phase,\nallowing privacy-compliant collaboration among health authorities for local\nforecasting.", "AI": {"tldr": "\u8be5\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u8054\u90a6\u5b66\u4e60\u548c\u5dee\u5206\u9690\u79c1\u7684\u9690\u79c1\u4fdd\u62a4\u6d41\u884c\u75c5\u9884\u6d4b\u65b9\u6cd5\uff0c\u5728\u5fb7\u56fd\u53bf\u7ea7\u5c42\u9762\u8fdb\u884cCOVID-19\u75c5\u4f8b\u9884\u6d4b\uff0c\u5728\u4fdd\u6301\u6570\u636e\u9690\u79c1\u7684\u540c\u65f6\u5b9e\u73b0\u4e86\u63a5\u8fd1\u975e\u9690\u79c1\u4fdd\u62a4\u6a21\u578b\u7684\u9884\u6d4b\u6027\u80fd\u3002", "motivation": "\u5728\u6d41\u884c\u75c5\u7206\u53d1\u65f6\u9700\u8981\u5feb\u901f\u53cd\u5e94\uff0c\u4f46\u672c\u5730\u5316\u673a\u5668\u5b66\u4e60\u6a21\u578b\u8bad\u7ec3\u9762\u4e34\u6570\u636e\u4e0d\u8db3\u95ee\u9898\uff0c\u800c\u96c6\u4e2d\u5316\u6570\u636e\u53c8\u5b58\u5728\u9690\u79c1\u654f\u611f\u6027\u6311\u6218\u3002\u9700\u8981\u4e00\u79cd\u65e2\u80fd\u4fdd\u62a4\u9690\u79c1\u53c8\u80fd\u8fdb\u884c\u6709\u6548\u9884\u6d4b\u7684\u65b9\u6cd5\u3002", "method": "\u91c7\u7528\u8054\u90a6\u5b66\u4e60\u6846\u67b6\uff0c\u5c06\u5404\u53bf/\u793e\u533a\u4f5c\u4e3a\u5ba2\u6237\u7aef\uff0c\u4f7f\u7528\u591a\u5c42\u611f\u77e5\u673a\u5728\u6ed1\u52a8\u7a97\u53e3\u4e0a\u8fdb\u884c\u75c5\u4f8b\u9884\u6d4b\u3002\u901a\u8fc7\u5ba2\u6237\u7aef\u7ea7\u5dee\u5206\u9690\u79c1\uff0c\u4ec5\u4ea4\u6362\u7ecf\u8fc7\u8303\u6570\u88c1\u526a\u7684\u66f4\u65b0\uff0c\u5e76\u5728\u670d\u52a1\u5668\u805a\u5408\u65f6\u6dfb\u52a0DP\u566a\u58f0\u3002", "result": "\u5728\u9002\u5ea6\u9690\u79c1\u4fdd\u62a4\u6c34\u5e73\u4e0b\uff0cDP\u6a21\u578b\u63a5\u8fd1\u975eDP\u6a21\u578b\u6027\u80fd\uff1a2020\u5e7411\u6708R\u00b2=0.94\uff08vs. 0.95\uff09\uff0cMAPE=26%\uff1b2022\u5e743\u6708R\u00b2=0.88\uff08vs. 0.93\uff09\uff0cMAPE=21%\u3002\u4e25\u683c\u9690\u79c1\u4fdd\u62a4\u4f1a\u5bfc\u81f4\u9884\u6d4b\u4e0d\u7a33\u5b9a\u3002", "conclusion": "\u5ba2\u6237\u7aef\u7ea7DP-FL\u80fd\u591f\u63d0\u4f9b\u6709\u7528\u7684\u53bf\u7ea7\u9884\u6d4b\u5e76\u4fdd\u6301\u5f3a\u9690\u79c1\u4fdd\u8bc1\uff0c\u53ef\u884c\u7684\u9690\u79c1\u9884\u7b97\u53d6\u51b3\u4e8e\u6d41\u884c\u75c5\u9636\u6bb5\uff0c\u652f\u6301\u536b\u751f\u5f53\u5c40\u8fdb\u884c\u9690\u79c1\u5408\u89c4\u7684\u672c\u5730\u9884\u6d4b\u534f\u4f5c\u3002", "relevance": 25.0}}
{"id": "2509.13359", "pdf": "https://arxiv.org/pdf/2509.13359", "abs": "https://arxiv.org/abs/2509.13359", "authors": ["Benjamin J. Walker", "Beatriz Navarro Lameda", "Ruth A. Reynolds"], "title": "Evaluating undergraduate mathematics examinations in the era of generative AI: a curriculum-level case study", "categories": ["cs.CY", "cs.AI"], "comment": null, "summary": "Generative artificial intelligence (GenAI) tools such as OpenAI's ChatGPT are\ntransforming the educational landscape, prompting reconsideration of\ntraditional assessment practices. In parallel, universities are exploring\nalternatives to in-person, closed-book examinations, raising concerns about\nacademic integrity and pedagogical alignment in uninvigilated settings. This\nstudy investigates whether traditional closed-book mathematics examinations\nretain their pedagogical relevance when hypothetically administered in\nuninvigilated, open-book settings with GenAI access. Adopting an empirical\napproach, we generate, transcribe, and blind-mark GenAI submissions to eight\nundergraduate mathematics examinations at a Russel Group university, spanning\nthe entirety of the first-year curriculum. By combining independent GenAI\nresponses to individual questions, we enable a meaningful evaluation of GenAI\nperformance, both at the level of modules and across the first-year curriculum.\nWe find that GenAI attainment is at the level of a first-class degree, though\ncurrent performance can vary between modules. Further, we find that GenAI\nperformance is remarkably consistent when viewed across the entire curriculum,\nsignificantly more so than that of students in invigilated examinations. Our\nfindings evidence the need for redesigning assessments in mathematics for\nunsupervised settings, and highlight the potential reduction in pedagogical\nvalue of current standards in the era of generative artificial intelligence.", "AI": {"tldr": "\u8be5\u7814\u7a76\u8bc4\u4f30\u4e86\u751f\u6210\u5f0fAI\u5728\u65e0\u76d1\u8003\u5f00\u653e\u4e66\u6570\u5b66\u8003\u8bd5\u4e2d\u7684\u8868\u73b0\uff0c\u53d1\u73b0AI\u80fd\u8fbe\u5230\u4e00\u7b49\u5b66\u4f4d\u6c34\u5e73\u4e14\u8868\u73b0\u6bd4\u5b66\u751f\u66f4\u7a33\u5b9a\uff0c\u8868\u660e\u9700\u8981\u91cd\u65b0\u8bbe\u8ba1\u6570\u5b66\u8bc4\u4f30\u65b9\u5f0f\u3002", "motivation": "\u968f\u7740\u751f\u6210\u5f0fAI\u5de5\u5177\u5728\u6559\u80b2\u9886\u57df\u7684\u5e94\u7528\uff0c\u9700\u8981\u91cd\u65b0\u8bc4\u4f30\u4f20\u7edf\u95ed\u5377\u8003\u8bd5\u5728\u65e0\u76d1\u8003\u5f00\u653e\u4e66\u73af\u5883\u4e2d\u7684\u6559\u5b66\u76f8\u5173\u6027\uff0c\u7279\u522b\u662f\u5bf9\u5b66\u672f\u8bda\u4fe1\u548c\u6559\u5b66\u4e00\u81f4\u6027\u7684\u62c5\u5fe7\u3002", "method": "\u7814\u7a76\u751f\u6210\u3001\u8f6c\u5f55\u5e76\u76f2\u8bc4\u751f\u6210\u5f0fAI\u5bf98\u95e8\u672c\u79d1\u6570\u5b66\u8003\u8bd5\u7684\u4f5c\u7b54\uff0c\u6db5\u76d6\u7b2c\u4e00\u5e74\u5168\u90e8\u8bfe\u7a0b\uff0c\u901a\u8fc7\u7ec4\u5408AI\u5bf9\u5355\u4e2a\u95ee\u9898\u7684\u72ec\u7acb\u56de\u7b54\u6765\u8bc4\u4f30\u5176\u8868\u73b0\u3002", "result": "\u751f\u6210\u5f0fAI\u8fbe\u5230\u4e00\u7b49\u5b66\u4f4d\u6c34\u5e73\uff0c\u8868\u73b0\u6bd4\u76d1\u8003\u73af\u5883\u4e2d\u7684\u5b66\u751f\u66f4\u52a0\u7a33\u5b9a\uff0c\u4f46\u4e0d\u540c\u6a21\u5757\u95f4\u8868\u73b0\u5b58\u5728\u5dee\u5f02\u3002", "conclusion": "\u9700\u8981\u5728\u65e0\u76d1\u7763\u73af\u5883\u4e2d\u91cd\u65b0\u8bbe\u8ba1\u6570\u5b66\u8bc4\u4f30\u65b9\u5f0f\uff0c\u5f53\u524d\u6807\u51c6\u5728\u751f\u6210\u5f0fAI\u65f6\u4ee3\u53ef\u80fd\u964d\u4f4e\u6559\u5b66\u4ef7\u503c\u3002", "relevance": 45.0}}
{"id": "2509.12577", "pdf": "https://arxiv.org/pdf/2509.12577", "abs": "https://arxiv.org/abs/2509.12577", "authors": ["Elinor Poole-Dayan", "Deb Roy", "Jad Kabbara"], "title": "An AI-Powered Framework for Analyzing Collective Idea Evolution in Deliberative Assemblies", "categories": ["cs.CY", "cs.CL"], "comment": null, "summary": "In an era of increasing societal fragmentation, political polarization, and\nerosion of public trust in institutions, representative deliberative assemblies\nare emerging as a promising democratic forum for developing effective policy\noutcomes on complex global issues. Despite theoretical attention, there remains\nlimited empirical work that systematically traces how specific ideas evolve,\nare prioritized, or are discarded during deliberation to form policy\nrecommendations. Addressing these gaps, this work poses two central questions:\n(1) How might we trace the evolution and distillation of ideas into concrete\nrecommendations within deliberative assemblies? (2) How does the deliberative\nprocess shape delegate perspectives and influence voting dynamics over the\ncourse of the assembly? To address these questions, we develop LLM-based\nmethodologies for empirically analyzing transcripts from a tech-enhanced\nin-person deliberative assembly. The framework identifies and visualizes the\nspace of expressed suggestions. We also empirically reconstruct each delegate's\nevolving perspective throughout the assembly. Our methods contribute novel\nempirical insights into deliberative processes and demonstrate how LLMs can\nsurface high-resolution dynamics otherwise invisible in traditional assembly\noutputs.", "AI": {"tldr": "\u8be5\u7814\u7a76\u5f00\u53d1\u4e86\u57fa\u4e8eLLM\u7684\u65b9\u6cd5\u6765\u5206\u6790\u5ba1\u8bae\u5927\u4f1a\u7684\u8f6c\u5f55\u6587\u672c\uff0c\u8ffd\u8e2a\u89c2\u70b9\u6f14\u53d8\u548c\u6295\u7968\u52a8\u6001\uff0c\u4e3a\u6c11\u4e3b\u5ba1\u8bae\u8fc7\u7a0b\u63d0\u4f9b\u65b0\u7684\u5b9e\u8bc1\u5206\u6790\u5de5\u5177\u3002", "motivation": "\u5728\u653f\u6cbb\u6781\u5316\u548c\u793e\u4f1a\u5206\u88c2\u52a0\u5267\u7684\u80cc\u666f\u4e0b\uff0c\u4ee3\u8868\u6027\u5ba1\u8bae\u5927\u4f1a\u4f5c\u4e3a\u6c11\u4e3b\u8bba\u575b\u65e5\u76ca\u91cd\u8981\uff0c\u4f46\u7f3a\u4e4f\u7cfb\u7edf\u8ffd\u8e2a\u89c2\u70b9\u6f14\u53d8\u548c\u4f18\u5148\u6392\u5e8f\u7684\u5b9e\u8bc1\u7814\u7a76\u65b9\u6cd5\u3002", "method": "\u5f00\u53d1\u57fa\u4e8eLLM\u7684\u65b9\u6cd5\u8bba\uff0c\u5206\u6790\u6280\u672f\u589e\u5f3a\u7684\u9762\u5bf9\u9762\u5ba1\u8bae\u5927\u4f1a\u8f6c\u5f55\u6587\u672c\uff0c\u8bc6\u522b\u548c\u53ef\u89c6\u5316\u8868\u8fbe\u5efa\u8bae\u7684\u7a7a\u95f4\uff0c\u91cd\u6784\u4ee3\u8868\u89c2\u70b9\u6f14\u53d8\u8fc7\u7a0b\u3002", "result": "\u8be5\u65b9\u6cd5\u80fd\u591f\u63ed\u793a\u4f20\u7edf\u5927\u4f1a\u8f93\u51fa\u4e2d\u4e0d\u53ef\u89c1\u7684\u9ad8\u5206\u8fa8\u7387\u52a8\u6001\uff0c\u4e3a\u5ba1\u8bae\u8fc7\u7a0b\u63d0\u4f9b\u65b0\u9896\u7684\u5b9e\u8bc1\u89c1\u89e3\u3002", "conclusion": "LLM\u53ef\u4ee5\u6709\u6548\u5730\u7528\u4e8e\u5206\u6790\u590d\u6742\u7684\u6c11\u4e3b\u5ba1\u8bae\u8fc7\u7a0b\uff0c\u63ed\u793a\u89c2\u70b9\u5f62\u6210\u548c\u51b3\u7b56\u52a8\u6001\u7684\u5fae\u89c2\u673a\u5236\u3002", "relevance": 35.0}}
{"id": "2509.13754", "pdf": "https://arxiv.org/pdf/2509.13754", "abs": "https://arxiv.org/abs/2509.13754", "authors": ["Hao Yin", "Xin Man", "Feiyu Chen", "Jie Shao", "Heng Tao Shen"], "title": "Cross-modal Full-mode Fine-grained Alignment for Text-to-Image Person Retrieval", "categories": ["cs.CV"], "comment": null, "summary": "Text-to-Image Person Retrieval (TIPR) is a cross-modal matching task that\naims to retrieve the most relevant person images based on a given text query.\nThe key challenge in TIPR lies in achieving effective alignment between textual\nand visual modalities within a common latent space. To address this challenge,\nprior approaches incorporate attention mechanisms for implicit cross-modal\nlocal alignment. However, they lack the ability to verify whether all local\nfeatures are correctly aligned. Moreover, existing methods primarily focus on\nhard negative samples during model updates, with the goal of refining\ndistinctions between positive and negative pairs, often neglecting incorrectly\nmatched positive pairs. To alleviate these issues, we propose FMFA, a\ncross-modal Full-Mode Fine-grained Alignment framework, which enhances global\nmatching through explicit fine-grained alignment and existing implicit\nrelational reasoning -- hence the term ``full-mode\" -- without requiring\nadditional supervision. Specifically, we design an Adaptive Similarity\nDistribution Matching (A-SDM) module to rectify unmatched positive sample\npairs. A-SDM adaptively pulls the unmatched positive pairs closer in the joint\nembedding space, thereby achieving more precise global alignment. Additionally,\nwe introduce an Explicit Fine-grained Alignment (EFA) module, which makes up\nfor the lack of verification capability of implicit relational reasoning. EFA\nstrengthens explicit cross-modal fine-grained interactions by sparsifying the\nsimilarity matrix and employs a hard coding method for local alignment. Our\nproposed method is evaluated on three public datasets, achieving\nstate-of-the-art performance among all global matching methods. Our code is\navailable at https://github.com/yinhao1102/FMFA.", "AI": {"tldr": "FMFA\u662f\u4e00\u4e2a\u8de8\u6a21\u6001\u5168\u6a21\u5f0f\u7ec6\u7c92\u5ea6\u5bf9\u9f50\u6846\u67b6\uff0c\u901a\u8fc7\u663e\u5f0f\u7ec6\u7c92\u5ea6\u5bf9\u9f50\u548c\u9690\u5f0f\u5173\u7cfb\u63a8\u7406\u6765\u6539\u8fdb\u6587\u672c-\u56fe\u50cf\u4eba\u7269\u68c0\u7d22\u4efb\u52a1\uff0c\u5728\u4e09\u4e2a\u516c\u5f00\u6570\u636e\u96c6\u4e0a\u8fbe\u5230\u4e86\u6700\u5148\u8fdb\u7684\u6027\u80fd\u3002", "motivation": "\u89e3\u51b3\u6587\u672c-\u56fe\u50cf\u4eba\u7269\u68c0\u7d22\u4e2d\u8de8\u6a21\u6001\u5bf9\u9f50\u7684\u6311\u6218\uff0c\u73b0\u6709\u65b9\u6cd5\u7f3a\u4e4f\u9a8c\u8bc1\u5c40\u90e8\u7279\u5f81\u662f\u5426\u6b63\u786e\u5bf9\u9f50\u7684\u80fd\u529b\uff0c\u4e14\u4e3b\u8981\u5173\u6ce8\u56f0\u96be\u8d1f\u6837\u672c\u800c\u5ffd\u7565\u4e86\u9519\u8bef\u5339\u914d\u7684\u6b63\u6837\u672c\u5bf9\u3002", "method": "\u63d0\u51fa\u81ea\u9002\u5e94\u76f8\u4f3c\u5ea6\u5206\u5e03\u5339\u914d(A-SDM)\u6a21\u5757\u6765\u4fee\u6b63\u672a\u5339\u914d\u7684\u6b63\u6837\u672c\u5bf9\uff0c\u4ee5\u53ca\u663e\u5f0f\u7ec6\u7c92\u5ea6\u5bf9\u9f50(EFA)\u6a21\u5757\u6765\u52a0\u5f3a\u663e\u5f0f\u8de8\u6a21\u6001\u7ec6\u7c92\u5ea6\u4ea4\u4e92\uff0c\u901a\u8fc7\u7a00\u758f\u5316\u76f8\u4f3c\u5ea6\u77e9\u9635\u548c\u786c\u7f16\u7801\u65b9\u6cd5\u5b9e\u73b0\u5c40\u90e8\u5bf9\u9f50\u3002", "result": "\u5728\u4e09\u4e2a\u516c\u5f00\u6570\u636e\u96c6\u4e0a\u5b9e\u73b0\u4e86\u6240\u6709\u5168\u5c40\u5339\u914d\u65b9\u6cd5\u4e2d\u7684\u6700\u5148\u8fdb\u6027\u80fd\u3002", "conclusion": "FMFA\u6846\u67b6\u901a\u8fc7\u5168\u6a21\u5f0f\u5bf9\u9f50\u7b56\u7565\u6709\u6548\u63d0\u5347\u4e86\u8de8\u6a21\u6001\u68c0\u7d22\u7684\u7cbe\u5ea6\uff0c\u65e0\u9700\u989d\u5916\u76d1\u7763\u5373\u53ef\u5b9e\u73b0\u66f4\u597d\u7684\u5168\u5c40\u548c\u5c40\u90e8\u5bf9\u9f50\u3002", "relevance": 35.0}}
{"id": "2509.14029", "pdf": "https://arxiv.org/pdf/2509.14029", "abs": "https://arxiv.org/abs/2509.14029", "authors": ["Samuel Tovey", "Julian Ho\u00dfbach", "Sandro Kuppel", "Tobias Ensslen", "Jan C. Behrends", "Christian Holm"], "title": "Deep Learning-Driven Peptide Classification in Biological Nanopores", "categories": ["cs.LG", "eess.SP", "physics.comp-ph", "q-bio.BM"], "comment": "29 pages (incl. references) 7 figures", "summary": "A device capable of performing real time classification of proteins in a\nclinical setting would allow for inexpensive and rapid disease diagnosis. One\nsuch candidate for this technology are nanopore devices. These devices work by\nmeasuring a current signal that arises when a protein or peptide enters a\nnanometer-length-scale pore. Should this current be uniquely related to the\nstructure of the peptide and its interactions with the pore, the signals can be\nused to perform identification. While such a method would allow for real time\nidentification of peptides and proteins in a clinical setting, to date, the\ncomplexities of these signals limit their accuracy. In this work, we tackle the\nissue of classification by converting the current signals into scaleogram\nimages via wavelet transforms, capturing amplitude, frequency, and time\ninformation in a modality well-suited to machine learning algorithms. When\ntested on 42 peptides, our method achieved a classification accuracy of\n~$81\\,\\%$, setting a new state-of-the-art in the field and taking a step toward\npractical peptide/protein diagnostics at the point of care. In addition, we\ndemonstrate model transfer techniques that will be critical when deploying\nthese models into real hardware, paving the way to a new method for real-time\ndisease diagnosis.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u4f7f\u7528\u5c0f\u6ce2\u53d8\u6362\u5c06\u86cb\u767d\u8d28\u7eb3\u7c73\u5b54\u7535\u6d41\u4fe1\u53f7\u8f6c\u6362\u4e3a\u5c3a\u5ea6\u56fe\u56fe\u50cf\uff0c\u7136\u540e\u5229\u7528\u673a\u5668\u5b66\u4e60\u7b97\u6cd5\u8fdb\u884c\u86cb\u767d\u8d28\u5206\u7c7b\u7684\u65b0\u65b9\u6cd5\uff0c\u572842\u79cd\u80bd\u4e0a\u8fbe\u5230\u4e8681%\u7684\u5206\u7c7b\u51c6\u786e\u7387\u3002", "motivation": "\u5f00\u53d1\u80fd\u591f\u5728\u4e34\u5e8a\u73af\u5883\u4e2d\u5b9e\u65f6\u5206\u7c7b\u86cb\u767d\u8d28\u7684\u8bbe\u5907\uff0c\u5b9e\u73b0\u5ec9\u4ef7\u5feb\u901f\u7684\u75be\u75c5\u8bca\u65ad\u3002\u7eb3\u7c73\u5b54\u8bbe\u5907\u901a\u8fc7\u6d4b\u91cf\u86cb\u767d\u8d28\u8fdb\u5165\u7eb3\u7c73\u5b54\u65f6\u4ea7\u751f\u7684\u7535\u6d41\u4fe1\u53f7\u6765\u5de5\u4f5c\uff0c\u4f46\u73b0\u6709\u65b9\u6cd5\u7684\u4fe1\u53f7\u590d\u6742\u6027\u9650\u5236\u4e86\u51c6\u786e\u6027\u3002", "method": "\u5c06\u7535\u6d41\u4fe1\u53f7\u901a\u8fc7\u5c0f\u6ce2\u53d8\u6362\u8f6c\u6362\u4e3a\u5c3a\u5ea6\u56fe\u56fe\u50cf\uff0c\u6355\u6349\u632f\u5e45\u3001\u9891\u7387\u548c\u65f6\u95f4\u4fe1\u606f\uff0c\u4f7f\u7528\u673a\u5668\u5b66\u4e60\u7b97\u6cd5\u8fdb\u884c\u5206\u7c7b\u3002\u8fd8\u5c55\u793a\u4e86\u6a21\u578b\u8fc1\u79fb\u6280\u672f\u4ee5\u4fbf\u5728\u5b9e\u9645\u786c\u4ef6\u4e2d\u90e8\u7f72\u3002", "result": "\u572842\u79cd\u80bd\u4e0a\u5b9e\u73b0\u4e86\u7ea681%\u7684\u5206\u7c7b\u51c6\u786e\u7387\uff0c\u521b\u4e0b\u4e86\u8be5\u9886\u57df\u7684\u65b0\u6700\u5148\u8fdb\u6c34\u5e73\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u4e3a\u5b9e\u65f6\u75be\u75c5\u8bca\u65ad\u63d0\u4f9b\u4e86\u4e00\u79cd\u65b0\u9014\u5f84\uff0c\u671d\u7740\u5b9e\u9645\u4e34\u5e8a\u5e94\u7528\u8fc8\u51fa\u4e86\u91cd\u8981\u4e00\u6b65\u3002", "relevance": 25.0}}
{"id": "2509.13365", "pdf": "https://arxiv.org/pdf/2509.13365", "abs": "https://arxiv.org/abs/2509.13365", "authors": ["Brian D. Earp", "Haotian Yuan", "Julian Koplin", "Sebastian Porsdam Mann"], "title": "The Provenance Problem: LLMs and the Breakdown of Citation Norms", "categories": ["cs.CY", "cs.AI"], "comment": "9 pages", "summary": "The increasing use of generative AI in scientific writing raises urgent\nquestions about attribution and intellectual credit. When a researcher employs\nChatGPT to draft a manuscript, the resulting text may echo ideas from sources\nthe author has never encountered. If an AI system reproduces insights from, for\nexample, an obscure 1975 paper without citation, does this constitute\nplagiarism? We argue that such cases exemplify the 'provenance problem': a\nsystematic breakdown in the chain of scholarly credit. Unlike conventional\nplagiarism, this phenomenon does not involve intent to deceive (researchers may\ndisclose AI use and act in good faith) yet still benefit from the uncredited\nintellectual contributions of others. This dynamic creates a novel category of\nattributional harm that current ethical and professional frameworks fail to\naddress. As generative AI becomes embedded across disciplines, the risk that\nsignificant ideas will circulate without recognition threatens both the\nreputational economy of science and the demands of epistemic justice. This\nPerspective analyzes how AI challenges established norms of authorship,\nintroduces conceptual tools for understanding the provenance problem, and\nproposes strategies to preserve integrity and fairness in scholarly\ncommunication.", "AI": {"tldr": "\u8bba\u6587\u63a2\u8ba8\u4e86\u751f\u6210\u5f0fAI\u5728\u79d1\u7814\u5199\u4f5c\u4e2d\u5f15\u53d1\u7684\u5b66\u672f\u6eaf\u6e90\u95ee\u9898\uff0c\u5f53\u7814\u7a76\u8005\u4f7f\u7528ChatGPT\u7b49\u5de5\u5177\u65f6\u53ef\u80fd\u65e0\u610f\u4e2d\u590d\u5236\u672a\u5f15\u7528\u7684\u524d\u4eba\u89c2\u70b9\uff0c\u5f62\u6210\u65b0\u578b\u7684\u5b66\u672f\u4e0d\u7aef\u95ee\u9898", "motivation": "\u968f\u7740\u751f\u6210\u5f0fAI\u5728\u79d1\u7814\u5199\u4f5c\u4e2d\u7684\u5e7f\u6cdb\u5e94\u7528\uff0c\u4f20\u7edf\u7684\u5b66\u672f\u5f15\u7528\u548c\u77e5\u8bc6\u4ea7\u6743\u4f53\u7cfb\u9762\u4e34\u6311\u6218\uff0c\u9700\u8981\u89e3\u51b3AI\u751f\u6210\u5185\u5bb9\u4e2d\u53ef\u80fd\u5b58\u5728\u7684\u65e0\u610f\u8bc6\u6284\u88ad\u548c\u5b66\u672f\u6eaf\u6e90\u65ad\u88c2\u95ee\u9898", "method": "\u91c7\u7528\u6982\u5ff5\u5206\u6790\u548c\u4f26\u7406\u6846\u67b6\u6784\u5efa\u7684\u65b9\u6cd5\uff0c\u63d0\u51fa\u4e86\"\u6eaf\u6e90\u95ee\u9898\"\u7684\u7406\u8bba\u6982\u5ff5\uff0c\u5206\u6790AI\u5bf9\u5b66\u672f\u4f5c\u8005\u8eab\u4efd\u89c4\u8303\u7684\u6311\u6218", "result": "\u8bc6\u522b\u51fa\u4e00\u79cd\u65b0\u578b\u7684\u5b66\u672f attributional harm\uff08\u5f52\u5c5e\u4f24\u5bb3\uff09\uff0c\u5373\u7814\u7a76\u8005\u53ef\u80fd\u65e0\u610f\u4e2d\u901a\u8fc7AI\u5de5\u5177\u53d7\u76ca\u4e8e\u4ed6\u4eba\u7684\u667a\u529b\u8d21\u732e\u800c\u65e0\u9700\u5f15\u7528", "conclusion": "\u9700\u8981\u5efa\u7acb\u65b0\u7684\u4f26\u7406\u548c\u4e13\u4e1a\u6846\u67b6\u6765\u5e94\u5bf9\u751f\u6210\u5f0fAI\u5e26\u6765\u7684\u5b66\u672f\u6eaf\u6e90\u6311\u6218\uff0c\u4fdd\u62a4\u5b66\u672f\u8bda\u4fe1\u548c\u516c\u5e73\u6027", "relevance": 35.0}}
{"id": "2509.13756", "pdf": "https://arxiv.org/pdf/2509.13756", "abs": "https://arxiv.org/abs/2509.13756", "authors": ["Yuqi Yang", "Dongliang Chang", "Yuanchen Fang", "Yi-Zhe SonG", "Zhanyu Ma", "Jun Guo"], "title": "Controllable-Continuous Color Editing in Diffusion Model via Color Mapping", "categories": ["cs.CV"], "comment": null, "summary": "In recent years, text-driven image editing has made significant progress.\nHowever, due to the inherent ambiguity and discreteness of natural language,\ncolor editing still faces challenges such as insufficient precision and\ndifficulty in achieving continuous control. Although linearly interpolating the\nembedding vectors of different textual descriptions can guide the model to\ngenerate a sequence of images with varying colors, this approach lacks precise\ncontrol over the range of color changes in the output images. Moreover, the\nrelationship between the interpolation coefficient and the resulting image\ncolor is unknown and uncontrollable. To address these issues, we introduce a\ncolor mapping module that explicitly models the correspondence between the text\nembedding space and image RGB values. This module predicts the corresponding\nembedding vector based on a given RGB value, enabling precise color control of\nthe generated images while maintaining semantic consistency. Users can specify\na target RGB range to generate images with continuous color variations within\nthe desired range, thereby achieving finer-grained, continuous, and\ncontrollable color editing. Experimental results demonstrate that our method\nperforms well in terms of color continuity and controllability.", "AI": {"tldr": "\u63d0\u51fa\u989c\u8272\u6620\u5c04\u6a21\u5757\uff0c\u901a\u8fc7\u5efa\u7acb\u6587\u672c\u5d4c\u5165\u7a7a\u95f4\u4e0eRGB\u503c\u7684\u5bf9\u5e94\u5173\u7cfb\uff0c\u5b9e\u73b0\u7cbe\u786e\u53ef\u63a7\u7684\u8fde\u7eed\u989c\u8272\u7f16\u8f91", "motivation": "\u89e3\u51b3\u6587\u672c\u9a71\u52a8\u56fe\u50cf\u7f16\u8f91\u4e2d\u989c\u8272\u63a7\u5236\u7684\u4e0d\u8db3\uff0c\u5305\u62ec\u7cbe\u5ea6\u4e0d\u591f\u548c\u96be\u4ee5\u5b9e\u73b0\u8fde\u7eed\u63a7\u5236\u7684\u95ee\u9898", "method": "\u5f15\u5165\u989c\u8272\u6620\u5c04\u6a21\u5757\uff0c\u663e\u5f0f\u5efa\u6a21\u6587\u672c\u5d4c\u5165\u7a7a\u95f4\u4e0e\u56fe\u50cfRGB\u503c\u7684\u5bf9\u5e94\u5173\u7cfb\uff0c\u901a\u8fc7\u7ed9\u5b9aRGB\u503c\u9884\u6d4b\u5bf9\u5e94\u5d4c\u5165\u5411\u91cf", "result": "\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\u8be5\u65b9\u6cd5\u5728\u989c\u8272\u8fde\u7eed\u6027\u548c\u53ef\u63a7\u6027\u65b9\u9762\u8868\u73b0\u826f\u597d", "conclusion": "\u8be5\u65b9\u6cd5\u80fd\u591f\u5b9e\u73b0\u66f4\u7ec6\u7c92\u5ea6\u3001\u8fde\u7eed\u4e14\u53ef\u63a7\u7684\u989c\u8272\u7f16\u8f91\uff0c\u540c\u65f6\u4fdd\u6301\u8bed\u4e49\u4e00\u81f4\u6027", "relevance": 20.0}}
{"id": "2509.14061", "pdf": "https://arxiv.org/pdf/2509.14061", "abs": "https://arxiv.org/abs/2509.14061", "authors": ["Chiara De Luca", "Elisa Donati"], "title": "Queen Detection in Beehives via Environmental Sensor Fusion for Low-Power Edge Computing", "categories": ["cs.LG", "cs.AI"], "comment": null, "summary": "Queen bee presence is essential for the health and stability of honeybee\ncolonies, yet current monitoring methods rely on manual inspections that are\nlabor-intensive, disruptive, and impractical for large-scale beekeeping. While\nrecent audio-based approaches have shown promise, they often require high power\nconsumption, complex preprocessing, and are susceptible to ambient noise. To\novercome these limitations, we propose a lightweight, multimodal system for\nqueen detection based on environmental sensor fusion-specifically, temperature,\nhumidity, and pressure differentials between the inside and outside of the\nhive. Our approach employs quantized decision tree inference on a commercial\nSTM32 microcontroller, enabling real-time, low-power edge computing without\ncompromising accuracy. We show that our system achieves over 99% queen\ndetection accuracy using only environmental inputs, with audio features\noffering no significant performance gain. This work presents a scalable and\nsustainable solution for non-invasive hive monitoring, paving the way for\nautonomous, precision beekeeping using off-the-shelf, energy-efficient\nhardware.", "AI": {"tldr": "\u63d0\u51fa\u57fa\u4e8e\u73af\u5883\u4f20\u611f\u5668\u878d\u5408\u7684\u8f7b\u91cf\u7ea7\u8702\u738b\u68c0\u6d4b\u7cfb\u7edf\uff0c\u4f7f\u7528\u6e29\u6e7f\u5ea6\u538b\u529b\u5dee\u5206\u7279\u5f81\uff0c\u5728STM32\u5fae\u63a7\u5236\u5668\u4e0a\u5b9e\u73b0\u5b9e\u65f6\u4f4e\u529f\u8017\u8fb9\u7f18\u8ba1\u7b97\uff0c\u51c6\u786e\u7387\u8d85\u8fc799%", "motivation": "\u4f20\u7edf\u8702\u738b\u68c0\u6d4b\u65b9\u6cd5\u4f9d\u8d56\u4eba\u5de5\u68c0\u67e5\uff0c\u52b3\u52a8\u5bc6\u96c6\u4e14\u5e72\u6270\u8702\u7fa4\uff1b\u73b0\u6709\u97f3\u9891\u65b9\u6cd5\u529f\u8017\u9ad8\u3001\u9884\u5904\u7406\u590d\u6742\u4e14\u6613\u53d7\u73af\u5883\u566a\u58f0\u5f71\u54cd\uff0c\u9700\u8981\u66f4\u53ef\u6301\u7eed\u7684\u89e3\u51b3\u65b9\u6848", "method": "\u4f7f\u7528\u8702\u7bb1\u5185\u5916\u6e29\u5ea6\u3001\u6e7f\u5ea6\u548c\u538b\u529b\u5dee\u5206\u4f5c\u4e3a\u73af\u5883\u4f20\u611f\u5668\u7279\u5f81\uff0c\u5728\u5546\u7528STM32\u5fae\u63a7\u5236\u5668\u4e0a\u90e8\u7f72\u91cf\u5316\u51b3\u7b56\u6811\u63a8\u7406\u5b9e\u73b0\u8fb9\u7f18\u8ba1\u7b97", "result": "\u4ec5\u4f7f\u7528\u73af\u5883\u8f93\u5165\u5373\u53ef\u5b9e\u73b0\u8d85\u8fc799%\u7684\u8702\u738b\u68c0\u6d4b\u51c6\u786e\u7387\uff0c\u97f3\u9891\u7279\u5f81\u672a\u5e26\u6765\u663e\u8457\u6027\u80fd\u63d0\u5347", "conclusion": "\u8be5\u7cfb\u7edf\u4e3a\u65e0\u521b\u8702\u7bb1\u76d1\u6d4b\u63d0\u4f9b\u4e86\u53ef\u6269\u5c55\u7684\u53ef\u6301\u7eed\u89e3\u51b3\u65b9\u6848\uff0c\u4e3a\u4f7f\u7528\u73b0\u6210\u8282\u80fd\u786c\u4ef6\u7684\u81ea\u4e3b\u7cbe\u51c6\u517b\u8702\u94fa\u5e73\u9053\u8def", "relevance": 5.0}}
{"id": "2509.13372", "pdf": "https://arxiv.org/pdf/2509.13372", "abs": "https://arxiv.org/abs/2509.13372", "authors": ["Prahlad G Menon"], "title": "Generative AI Pipeline for Interactive Prompt-driven 2D-to-3D Vascular Reconstruction for Fontan Geometries from Contrast-Enhanced X-Ray Fluoroscopy Imaging", "categories": ["eess.IV", "cs.AI", "cs.CV", "cs.ET", "q-bio.QM", "92C50, 68T07, 76D05, 65D18, 92C55", "I.4.6; I.4.8; J.3; I.2.10; I.4.9"], "comment": null, "summary": "Fontan palliation for univentricular congenital heart disease progresses to\nhemodynamic failure with complex flow patterns poorly characterized by\nconventional 2D imaging. Current assessment relies on fluoroscopic angiography,\nproviding limited 3D geometric information essential for computational fluid\ndynamics (CFD) analysis and surgical planning.\n  A multi-step AI pipeline was developed utilizing Google's Gemini 2.5 Flash\n(2.5B parameters) for systematic, iterative processing of fluoroscopic\nangiograms through transformer-based neural architecture. The pipeline\nencompasses medical image preprocessing, vascular segmentation, contrast\nenhancement, artifact removal, and virtual hemodynamic flow visualization\nwithin 2D projections. Final views were processed through Tencent's\nHunyuan3D-2mini (384M parameters) for stereolithography file generation.\n  The pipeline successfully generated geometrically optimized 2D projections\nfrom single-view angiograms after 16 processing steps using a custom web\ninterface. Initial iterations contained hallucinated vascular features\nrequiring iterative refinement to achieve anatomically faithful\nrepresentations. Final projections demonstrated accurate preservation of\ncomplex Fontan geometry with enhanced contrast suitable for 3D conversion.\nAI-generated virtual flow visualization identified stagnation zones in central\nconnections and flow patterns in branch arteries. Complete processing required\nunder 15 minutes with second-level API response times.\n  This approach demonstrates clinical feasibility of generating CFD-suitable\ngeometries from routine angiographic data, enabling 3D generation and rapid\nvirtual flow visualization for cursory insights prior to full CFD simulation.\nWhile requiring refinement cycles for accuracy, this establishes foundation for\ndemocratizing advanced geometric and hemodynamic analysis using readily\navailable imaging data.", "AI": {"tldr": "\u5f00\u53d1\u4e86\u4e00\u4e2a\u57fa\u4e8eGemini 2.5 Flash\u548cHunyuan3D-2mini\u7684\u591a\u6b65\u9aa4AI\u6d41\u6c34\u7ebf\uff0c\u4ece\u5355\u89c6\u89d2\u8840\u7ba1\u9020\u5f71\u751f\u6210CFD\u9002\u7528\u76843D\u51e0\u4f55\u6a21\u578b\uff0c\u7528\u4e8eFontan\u59d1\u606f\u624b\u672f\u7684\u8840\u6d41\u52a8\u529b\u5b66\u5206\u6790", "motivation": "\u4f20\u7edf2D\u6210\u50cf\u65e0\u6cd5\u5145\u5206\u8868\u5f81\u5355\u5fc3\u5ba4\u5148\u5929\u6027\u5fc3\u810f\u75c5Fontan\u59d1\u606f\u624b\u672f\u540e\u7684\u590d\u6742\u8840\u6d41\u6a21\u5f0f\uff0c\u5f53\u524d\u4f9d\u8d56\u8367\u5149\u8840\u7ba1\u9020\u5f71\u7684\u65b9\u6cd5\u53ea\u80fd\u63d0\u4f9b\u6709\u9650\u76843D\u51e0\u4f55\u4fe1\u606f\uff0c\u9650\u5236\u4e86\u8ba1\u7b97\u6d41\u4f53\u52a8\u529b\u5b66\u5206\u6790\u548c\u624b\u672f\u89c4\u5212", "method": "\u4f7f\u7528Google Gemini 2.5 Flash\uff0825\u4ebf\u53c2\u6570\uff09\u8fdb\u884c\u533b\u5b66\u56fe\u50cf\u9884\u5904\u7406\u3001\u8840\u7ba1\u5206\u5272\u3001\u5bf9\u6bd4\u5ea6\u589e\u5f3a\u3001\u4f2a\u5f71\u53bb\u9664\u548c\u865a\u62df\u8840\u6d41\u53ef\u89c6\u5316\uff0c\u7136\u540e\u901a\u8fc7\u817e\u8bafHunyuan3D-2mini\uff083.84\u4ebf\u53c2\u6570\uff09\u751f\u6210\u7acb\u4f53\u5149\u523b\u6587\u4ef6", "result": "\u6d41\u6c34\u7ebf\u7ecf\u8fc716\u4e2a\u5904\u7406\u6b65\u9aa4\u6210\u529f\u751f\u6210\u51e0\u4f55\u4f18\u5316\u76842D\u6295\u5f71\uff0c\u6700\u7ec8\u6295\u5f71\u51c6\u786e\u4fdd\u7559\u4e86\u590d\u6742\u7684Fontan\u51e0\u4f55\u7ed3\u6784\uff0c\u865a\u62df\u8840\u6d41\u53ef\u89c6\u5316\u8bc6\u522b\u51fa\u4e2d\u5fc3\u8fde\u63a5\u5904\u7684\u505c\u6ede\u533a\u548c\u5206\u652f\u52a8\u8109\u7684\u8840\u6d41\u6a21\u5f0f\uff0c\u6574\u4e2a\u5904\u7406\u8fc7\u7a0b\u572815\u5206\u949f\u5185\u5b8c\u6210", "conclusion": "\u8be5\u65b9\u6cd5\u5c55\u793a\u4e86\u4ece\u5e38\u89c4\u8840\u7ba1\u9020\u5f71\u6570\u636e\u751f\u6210CFD\u9002\u7528\u51e0\u4f55\u6a21\u578b\u7684\u4e34\u5e8a\u53ef\u884c\u6027\uff0c\u4e3a\u4f7f\u7528\u73b0\u6210\u6210\u50cf\u6570\u636e\u8fdb\u884c\u9ad8\u7ea7\u51e0\u4f55\u548c\u8840\u6d41\u52a8\u529b\u5b66\u5206\u6790\u7684\u666e\u53ca\u5960\u5b9a\u4e86\u57fa\u7840", "relevance": 25.0}}
{"id": "2509.13760", "pdf": "https://arxiv.org/pdf/2509.13760", "abs": "https://arxiv.org/abs/2509.13760", "authors": ["Jinwoo Jeon", "JunHyeok Oh", "Hayeong Lee", "Byung-Jun Lee"], "title": "Iterative Prompt Refinement for Safer Text-to-Image Generation", "categories": ["cs.CV"], "comment": null, "summary": "Text-to-Image (T2I) models have made remarkable progress in generating images\nfrom text prompts, but their output quality and safety still depend heavily on\nhow prompts are phrased. Existing safety methods typically refine prompts using\nlarge language models (LLMs), but they overlook the images produced, which can\nresult in unsafe outputs or unnecessary changes to already safe prompts. To\naddress this, we propose an iterative prompt refinement algorithm that uses\nVision Language Models (VLMs) to analyze both the input prompts and the\ngenerated images. By leveraging visual feedback, our method refines prompts\nmore effectively, improving safety while maintaining user intent and\nreliability comparable to existing LLM-based approaches. Additionally, we\nintroduce a new dataset labeled with both textual and visual safety signals\nusing off-the-shelf multi-modal LLM, enabling supervised fine-tuning.\nExperimental results demonstrate that our approach produces safer outputs\nwithout compromising alignment with user intent, offering a practical solution\nfor generating safer T2I content. Our code is available at\nhttps://github.com/ku-dmlab/IPR. \\textbf{\\textcolor{red}WARNING: This paper\ncontains examples of harmful or inappropriate images generated by models.", "AI": {"tldr": "\u63d0\u51fa\u57fa\u4e8e\u89c6\u89c9\u53cd\u9988\u7684\u8fed\u4ee3\u63d0\u793a\u8bcd\u4f18\u5316\u65b9\u6cd5\uff0c\u4f7f\u7528VLM\u5206\u6790\u6587\u672c\u63d0\u793a\u548c\u751f\u6210\u56fe\u50cf\uff0c\u63d0\u5347\u6587\u672c\u5230\u56fe\u50cf\u6a21\u578b\u7684\u5b89\u5168\u6027\u540c\u65f6\u4fdd\u6301\u7528\u6237\u610f\u56fe", "motivation": "\u73b0\u6709\u57fa\u4e8eLLM\u7684\u5b89\u5168\u65b9\u6cd5\u53ea\u5173\u6ce8\u6587\u672c\u63d0\u793a\u800c\u5ffd\u7565\u751f\u6210\u56fe\u50cf\uff0c\u53ef\u80fd\u5bfc\u81f4\u4e0d\u5b89\u5168\u8f93\u51fa\u6216\u5bf9\u5b89\u5168\u63d0\u793a\u7684\u4e0d\u5fc5\u8981\u4fee\u6539", "method": "\u8fed\u4ee3\u63d0\u793a\u8bcd\u4f18\u5316\u7b97\u6cd5\uff0c\u5229\u7528\u89c6\u89c9\u8bed\u8a00\u6a21\u578b(VLM)\u5206\u6790\u8f93\u5165\u63d0\u793a\u548c\u751f\u6210\u56fe\u50cf\uff0c\u901a\u8fc7\u89c6\u89c9\u53cd\u9988\u66f4\u6709\u6548\u5730\u4f18\u5316\u63d0\u793a", "result": "\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\u8be5\u65b9\u6cd5\u80fd\u751f\u6210\u66f4\u5b89\u5168\u7684\u8f93\u51fa\uff0c\u540c\u65f6\u4fdd\u6301\u4e0e\u7528\u6237\u610f\u56fe\u7684\u5bf9\u9f50\uff0c\u5b89\u5168\u6027\u4f18\u4e8e\u73b0\u6709LLM\u65b9\u6cd5", "conclusion": "\u8be5\u65b9\u6cd5\u4e3a\u751f\u6210\u66f4\u5b89\u5168\u7684T2I\u5185\u5bb9\u63d0\u4f9b\u4e86\u5b9e\u7528\u89e3\u51b3\u65b9\u6848\uff0c\u7ed3\u5408\u6587\u672c\u548c\u89c6\u89c9\u5b89\u5168\u4fe1\u53f7\u8fdb\u884c\u76d1\u7763\u5fae\u8c03", "relevance": 65.0}}
{"id": "2509.14077", "pdf": "https://arxiv.org/pdf/2509.14077", "abs": "https://arxiv.org/abs/2509.14077", "authors": ["Yuhao Wang", "Enlu Zhou"], "title": "Online Bayesian Risk-Averse Reinforcement Learning", "categories": ["cs.LG"], "comment": null, "summary": "In this paper, we study the Bayesian risk-averse formulation in reinforcement\nlearning (RL). To address the epistemic uncertainty due to a lack of data, we\nadopt the Bayesian Risk Markov Decision Process (BRMDP) to account for the\nparameter uncertainty of the unknown underlying model. We derive the asymptotic\nnormality that characterizes the difference between the Bayesian risk value\nfunction and the original value function under the true unknown distribution.\nThe results indicate that the Bayesian risk-averse approach tends to\npessimistically underestimate the original value function. This discrepancy\nincreases with stronger risk aversion and decreases as more data become\navailable. We then utilize this adaptive property in the setting of online RL\nas well as online contextual multi-arm bandits (CMAB), a special case of online\nRL. We provide two procedures using posterior sampling for both the general RL\nproblem and the CMAB problem. We establish a sub-linear regret bound, with the\nregret defined as the conventional regret for both the RL and CMAB settings.\nAdditionally, we establish a sub-linear regret bound for the CMAB setting with\nthe regret defined as the Bayesian risk regret. Finally, we conduct numerical\nexperiments to demonstrate the effectiveness of the proposed algorithm in\naddressing epistemic uncertainty and verifying the theoretical properties.", "AI": {"tldr": "\u8be5\u8bba\u6587\u7814\u7a76\u5f3a\u5316\u5b66\u4e60\u4e2d\u7684\u8d1d\u53f6\u65af\u98ce\u9669\u89c4\u907f\u65b9\u6cd5\uff0c\u901a\u8fc7BRMDP\u5904\u7406\u6a21\u578b\u53c2\u6570\u4e0d\u786e\u5b9a\u6027\uff0c\u8bc1\u660e\u4e86\u8d1d\u53f6\u65af\u98ce\u9669\u4ef7\u503c\u51fd\u6570\u4e0e\u771f\u5b9e\u4ef7\u503c\u51fd\u6570\u4e4b\u95f4\u7684\u6e10\u8fd1\u6b63\u6001\u6027\u5dee\u5f02\uff0c\u5e76\u63d0\u51fa\u4e86\u5728\u7ebfRL\u548cCMAB\u7684\u540e\u9a8c\u91c7\u6837\u7b97\u6cd5\uff0c\u83b7\u5f97\u4e86\u6b21\u7ebf\u6027\u9057\u61be\u754c\u3002", "motivation": "\u89e3\u51b3\u5f3a\u5316\u5b66\u4e60\u4e2d\u7531\u4e8e\u6570\u636e\u4e0d\u8db3\u5bfc\u81f4\u7684\u8ba4\u77e5\u4e0d\u786e\u5b9a\u6027\uff0c\u901a\u8fc7\u8d1d\u53f6\u65af\u98ce\u9669\u89c4\u907f\u65b9\u6cd5\u6765\u5904\u7406\u6a21\u578b\u53c2\u6570\u7684\u4e0d\u786e\u5b9a\u6027\uff0c\u63d0\u4f9b\u66f4\u7a33\u5065\u7684\u51b3\u7b56\u7b56\u7565\u3002", "method": "\u91c7\u7528\u8d1d\u53f6\u65af\u98ce\u9669\u9a6c\u5c14\u53ef\u592b\u51b3\u7b56\u8fc7\u7a0b(BRMDP)\uff0c\u63a8\u5bfc\u4ef7\u503c\u51fd\u6570\u7684\u6e10\u8fd1\u6b63\u6001\u6027\uff0c\u63d0\u51fa\u57fa\u4e8e\u540e\u9a8c\u91c7\u6837\u7684\u5728\u7ebfRL\u548c\u4e0a\u4e0b\u6587\u591a\u81c2\u8001\u864e\u673a(CMAB)\u7b97\u6cd5\u3002", "result": "\u7406\u8bba\u8bc1\u660e\u8d1d\u53f6\u65af\u98ce\u9669\u89c4\u907f\u65b9\u6cd5\u4f1a\u60b2\u89c2\u4f4e\u4f30\u539f\u59cb\u4ef7\u503c\u51fd\u6570\uff0c\u5dee\u5f02\u968f\u98ce\u9669\u89c4\u907f\u5f3a\u5ea6\u589e\u52a0\u800c\u589e\u5927\uff0c\u968f\u6570\u636e\u589e\u591a\u800c\u51cf\u5c0f\u3002\u7b97\u6cd5\u5728\u5728\u7ebfRL\u548cCMAB\u4e2d\u5b9e\u73b0\u4e86\u6b21\u7ebf\u6027\u9057\u61be\u754c\u3002", "conclusion": "\u8d1d\u53f6\u65af\u98ce\u9669\u89c4\u907f\u65b9\u6cd5\u80fd\u6709\u6548\u5904\u7406\u8ba4\u77e5\u4e0d\u786e\u5b9a\u6027\uff0c\u6570\u503c\u5b9e\u9a8c\u9a8c\u8bc1\u4e86\u7406\u8bba\u6027\u8d28\u548c\u7b97\u6cd5\u6709\u6548\u6027\u3002", "relevance": 40.0}}
{"id": "2509.13762", "pdf": "https://arxiv.org/pdf/2509.13762", "abs": "https://arxiv.org/abs/2509.13762", "authors": ["Kai Chen", "Jin Xiao", "Leheng Zhang", "Kexuan Shi", "Shuhang Gu"], "title": "Task-Aware Image Signal Processor for Advanced Visual Perception", "categories": ["cs.CV"], "comment": null, "summary": "In recent years, there has been a growing trend in computer vision towards\nexploiting RAW sensor data, which preserves richer information compared to\nconventional low-bit RGB images. Early studies mainly focused on enhancing\nvisual quality, while more recent efforts aim to leverage the abundant\ninformation in RAW data to improve the performance of visual perception tasks\nsuch as object detection and segmentation. However, existing approaches still\nface two key limitations: large-scale ISP networks impose heavy computational\noverhead, while methods based on tuning traditional ISP pipelines are\nrestricted by limited representational capacity.To address these issues, we\npropose Task-Aware Image Signal Processing (TA-ISP), a compact RAW-to-RGB\nframework that produces task-oriented representations for pretrained vision\nmodels. Instead of heavy dense convolutional pipelines, TA-ISP predicts a small\nset of lightweight, multi-scale modulation operators that act at global,\nregional, and pixel scales to reshape image statistics across different spatial\nextents. This factorized control significantly expands the range of spatially\nvarying transforms that can be represented while keeping memory usage,\ncomputation, and latency tightly constrained. Evaluated on several RAW-domain\ndetection and segmentation benchmarks under both daytime and nighttime\nconditions, TA-ISP consistently improves downstream accuracy while markedly\nreducing parameter count and inference time, making it well suited for\ndeployment on resource-constrained devices.", "AI": {"tldr": "TA-ISP\u662f\u4e00\u4e2a\u7d27\u51d1\u7684RAW-to-RGB\u6846\u67b6\uff0c\u901a\u8fc7\u9884\u6d4b\u8f7b\u91cf\u7ea7\u591a\u5c3a\u5ea6\u8c03\u5236\u7b97\u5b50\u6765\u751f\u6210\u9762\u5411\u4efb\u52a1\u7684\u56fe\u50cf\u8868\u793a\uff0c\u663e\u8457\u964d\u4f4e\u8ba1\u7b97\u5f00\u9500\u540c\u65f6\u63d0\u5347\u4e0b\u6e38\u89c6\u89c9\u4efb\u52a1\u6027\u80fd\u3002", "motivation": "\u73b0\u6709RAW\u6570\u636e\u5904\u7406\u65b9\u6cd5\u5b58\u5728\u4e24\u4e2a\u5173\u952e\u9650\u5236\uff1a\u5927\u89c4\u6a21ISP\u7f51\u7edc\u8ba1\u7b97\u5f00\u9500\u5927\uff0c\u800c\u57fa\u4e8e\u4f20\u7edfISP\u6d41\u6c34\u7ebf\u8c03\u4f18\u7684\u65b9\u6cd5\u8868\u793a\u80fd\u529b\u6709\u9650\u3002\u9700\u8981\u4e00\u79cd\u65e2\u80fd\u4fdd\u6301\u4e30\u5bccRAW\u4fe1\u606f\u53c8\u80fd\u9ad8\u6548\u5904\u7406\u7684\u65b9\u6cd5\u3002", "method": "\u63d0\u51fa\u4efb\u52a1\u611f\u77e5\u56fe\u50cf\u4fe1\u53f7\u5904\u7406(TA-ISP)\u6846\u67b6\uff0c\u4f7f\u7528\u8f7b\u91cf\u7ea7\u591a\u5c3a\u5ea6\u8c03\u5236\u7b97\u5b50\uff08\u5168\u5c40\u3001\u533a\u57df\u3001\u50cf\u7d20\u5c3a\u5ea6\uff09\u6765\u91cd\u5851\u56fe\u50cf\u7edf\u8ba1\u7279\u5f81\uff0c\u66ff\u4ee3\u4f20\u7edf\u7684\u5bc6\u96c6\u5377\u79ef\u6d41\u6c34\u7ebf\u3002", "result": "\u5728\u591a\u4e2aRAW\u57df\u68c0\u6d4b\u548c\u5206\u5272\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0cTA-ISP\u5728\u767d\u5929\u548c\u591c\u95f4\u6761\u4ef6\u4e0b\u90fd\u80fd\u6301\u7eed\u63d0\u5347\u4e0b\u6e38\u4efb\u52a1\u51c6\u786e\u7387\uff0c\u540c\u65f6\u663e\u8457\u51cf\u5c11\u53c2\u6570\u6570\u91cf\u548c\u63a8\u7406\u65f6\u95f4\u3002", "conclusion": "TA-ISP\u4e3a\u8d44\u6e90\u53d7\u9650\u8bbe\u5907\u63d0\u4f9b\u4e86\u4e00\u79cd\u9ad8\u6548\u7684RAW\u6570\u636e\u5904\u7406\u89e3\u51b3\u65b9\u6848\uff0c\u901a\u8fc7\u56e0\u5b50\u5316\u63a7\u5236\u6269\u5c55\u4e86\u7a7a\u95f4\u53d8\u5316\u53d8\u6362\u7684\u8868\u793a\u8303\u56f4\u3002", "relevance": 25.0}}
{"id": "2509.14078", "pdf": "https://arxiv.org/pdf/2509.14078", "abs": "https://arxiv.org/abs/2509.14078", "authors": ["Robiul Islam", "Dmitry I. Ignatov", "Karl Kaberg", "Roman Nabatchikov"], "title": "Exploring the Relationship between Brain Hemisphere States and Frequency Bands through Deep Learning Optimization Techniques", "categories": ["cs.LG"], "comment": null, "summary": "This study investigates classifier performance across EEG frequency bands\nusing various optimizers and evaluates efficient class prediction for the left\nand right hemispheres. Three neural network architectures - a deep dense\nnetwork, a shallow three-layer network, and a convolutional neural network\n(CNN) - are implemented and compared using the TensorFlow and PyTorch\nframeworks. Results indicate that the Adagrad and RMSprop optimizers\nconsistently perform well across different frequency bands, with Adadelta\nexhibiting robust performance in cross-model evaluations. Specifically, Adagrad\nexcels in the beta band, while RMSprop achieves superior performance in the\ngamma band. Conversely, SGD and FTRL exhibit inconsistent performance. Among\nthe models, the CNN demonstrates the second highest accuracy, particularly in\ncapturing spatial features of EEG data. The deep dense network shows\ncompetitive performance in learning complex patterns, whereas the shallow\nthree-layer network, sometimes being less accurate, provides computational\nefficiency. SHAP (Shapley Additive Explanations) plots are employed to identify\nefficient class prediction, revealing nuanced contributions of EEG frequency\nbands to model accuracy. Overall, the study highlights the importance of\noptimizer selection, model architecture, and EEG frequency band analysis in\nenhancing classifier performance and understanding feature importance in\nneuroimaging-based classification tasks.", "AI": {"tldr": "\u8be5\u7814\u7a76\u6bd4\u8f83\u4e86\u4e0d\u540c\u4f18\u5316\u5668\u548c\u795e\u7ecf\u7f51\u7edc\u67b6\u6784\u5728EEG\u9891\u6bb5\u5206\u7c7b\u4efb\u52a1\u4e2d\u7684\u6027\u80fd\uff0c\u53d1\u73b0Adagrad\u548cRMSprop\u4f18\u5316\u5668\u8868\u73b0\u6700\u4f73\uff0cCNN\u5728\u7a7a\u95f4\u7279\u5f81\u63d0\u53d6\u65b9\u9762\u8868\u73b0\u51fa\u8272\uff0cSHAP\u5206\u6790\u63ed\u793a\u4e86EEG\u9891\u6bb5\u5bf9\u5206\u7c7b\u51c6\u786e\u6027\u7684\u8d21\u732e\u3002", "motivation": "\u7814\u7a76\u65e8\u5728\u63a2\u7d22\u4e0d\u540c\u4f18\u5316\u5668\u548c\u795e\u7ecf\u7f51\u7edc\u67b6\u6784\u5728\u8111\u7535\u56fe(EEG)\u9891\u6bb5\u5206\u7c7b\u4efb\u52a1\u4e2d\u7684\u6027\u80fd\u5dee\u5f02\uff0c\u4ee5\u63d0\u9ad8\u795e\u7ecf\u5f71\u50cf\u5206\u7c7b\u4efb\u52a1\u7684\u51c6\u786e\u6027\u548c\u53ef\u89e3\u91ca\u6027\u3002", "method": "\u4f7f\u7528\u4e09\u79cd\u795e\u7ecf\u7f51\u7edc\u67b6\u6784\uff08\u6df1\u5ea6\u5bc6\u96c6\u7f51\u7edc\u3001\u6d45\u5c42\u4e09\u5c42\u7f51\u7edc\u548cCNN\uff09\uff0c\u5728TensorFlow\u548cPyTorch\u6846\u67b6\u4e0b\u6bd4\u8f83\u591a\u79cd\u4f18\u5316\u5668\uff08Adagrad\u3001RMSprop\u3001Adadelta\u3001SGD\u3001FTRL\uff09\u5728\u4e0d\u540cEEG\u9891\u6bb5\u7684\u6027\u80fd\uff0c\u5e76\u91c7\u7528SHAP\u8fdb\u884c\u7279\u5f81\u91cd\u8981\u6027\u5206\u6790\u3002", "result": "Adagrad\u548cRMSprop\u4f18\u5316\u5668\u5728\u4e0d\u540c\u9891\u6bb5\u8868\u73b0\u7a33\u5b9a\uff0cAdagrad\u5728beta\u9891\u6bb5\u8868\u73b0\u6700\u4f73\uff0cRMSprop\u5728gamma\u9891\u6bb5\u6700\u4f18\u3002CNN\u6a21\u578b\u51c6\u786e\u7387\u7b2c\u4e8c\u9ad8\uff0c\u64c5\u957f\u6355\u6349EEG\u7a7a\u95f4\u7279\u5f81\u3002\u6df1\u5ea6\u7f51\u7edc\u5728\u5b66\u4e60\u590d\u6742\u6a21\u5f0f\u65b9\u9762\u6709\u7ade\u4e89\u529b\uff0c\u6d45\u5c42\u7f51\u7edc\u8ba1\u7b97\u6548\u7387\u9ad8\u4f46\u51c6\u786e\u7387\u8f83\u4f4e\u3002", "conclusion": "\u4f18\u5316\u5668\u9009\u62e9\u3001\u6a21\u578b\u67b6\u6784\u548cEEG\u9891\u6bb5\u5206\u6790\u5bf9\u63d0\u9ad8\u5206\u7c7b\u5668\u6027\u80fd\u81f3\u5173\u91cd\u8981\uff0cSHAP\u5206\u6790\u6709\u52a9\u4e8e\u7406\u89e3EEG\u9891\u6bb5\u5bf9\u6a21\u578b\u51c6\u786e\u6027\u7684\u8d21\u732e\u673a\u5236\u3002", "relevance": 25.0}}
