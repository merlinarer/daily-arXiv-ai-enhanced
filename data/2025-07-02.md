<div id=toc></div>

# Table of Contents

- [cs.CL](#cs.CL) [Total: 48]
- [cs.CV](#cs.CV) [Total: 119]
- [cs.AI](#cs.AI) [Total: 75]
- [cs.LG](#cs.LG) [Total: 120]


<div id='cs.CL'></div>

# cs.CL [[Back]](#toc)

### [1] [Question Decomposition for Retrieval-Augmented Generation](https://arxiv.org/abs/2507.00355)
*Paul J. L. Ammann,Jonas Golde,Alan Akbik*

Main category: cs.CL

Relevance: 90.0

TL;DR: 提出了一种改进的RAG方法，通过问题分解和重新排序解决多跳问题，显著提升了检索和答案准确性。


<details>
  <summary>Details</summary>
Motivation: 多跳问题中，相关信息分散在不同文档，标准RAG难以有效检索。

Method: 1) LLM分解问题为子问题；2) 为每个子问题检索段落；3) 合并候选池并重新排序。

Result: 在MultiHop-RAG和HotpotQA上，检索效果（MRR@10）提升36.7%，答案准确率（F1）提升11.6%。

Conclusion: 问题分解与重新排序结合，无需额外训练即可显著提升多跳问题的RAG性能。

Abstract: Grounding large language models (LLMs) in verifiable external sources is a
well-established strategy for generating reliable answers. Retrieval-augmented
generation (RAG) is one such approach, particularly effective for tasks like
question answering: it retrieves passages that are semantically related to the
question and then conditions the model on this evidence. However, multi-hop
questions, such as "Which company among NVIDIA, Apple, and Google made the
biggest profit in 2023?," challenge RAG because relevant facts are often
distributed across multiple documents rather than co-occurring in one source,
making it difficult for standard RAG to retrieve sufficient information. To
address this, we propose a RAG pipeline that incorporates question
decomposition: (i) an LLM decomposes the original query into sub-questions,
(ii) passages are retrieved for each sub-question, and (iii) the merged
candidate pool is reranked to improve the coverage and precision of the
retrieved evidence. We show that question decomposition effectively assembles
complementary documents, while reranking reduces noise and promotes the most
relevant passages before answer generation. Although reranking itself is
standard, we show that pairing an off-the-shelf cross-encoder reranker with
LLM-driven question decomposition bridges the retrieval gap on multi-hop
questions and provides a practical, drop-in enhancement, without any extra
training or specialized indexing. We evaluate our approach on the MultiHop-RAG
and HotpotQA, showing gains in retrieval (MRR@10: +36.7%) and answer accuracy
(F1: +11.6%) over standard RAG baselines.

</details>


### [2] [SAFER: Probing Safety in Reward Models with Sparse Autoencoder](https://arxiv.org/abs/2507.00665)
*Sihang Li,Wei Shi,Ziyuan Xie,Tao Liang,Guojun Ma,Xiang Wang*

Main category: cs.CL

Relevance: 90.0

TL;DR: SAFER框架通过稀疏自编码器（SAEs）解释和改进奖励模型，揭示人类可理解的特征，并设计数据投毒和去噪策略，以增强LLM的安全性对齐。


<details>
  <summary>Details</summary>
Motivation: 奖励模型在RLHF中对齐LLM与人类价值观，但其内部机制不透明，需要解释和改进。

Method: 利用稀疏自编码器（SAEs）分析奖励模型激活，识别安全相关特征，并通过数据投毒和去噪策略优化模型。

Result: SAFER能精确调整安全性对齐，且不影响通用聊天性能。

Conclusion: SAFER为高风险LLM对齐任务中的奖励模型解释、审计和优化提供了新方法。

Abstract: Reinforcement learning from human feedback (RLHF) is a key paradigm for
aligning large language models (LLMs) with human values, yet the reward models
at its core remain largely opaque. In this work, we present sparse Autoencoder
For Enhanced Reward model (\textbf{SAFER}), a novel framework for interpreting
and improving reward models through mechanistic analysis. Leveraging Sparse
Autoencoders (SAEs), we uncover human-interpretable features in reward model
activations, enabling insight into safety-relevant decision-making. We apply
SAFER to safety-oriented preference datasets and quantify the salience of
individual features by activation differences between chosen and rejected
responses. Using these feature-level signals, we design targeted data poisoning
and denoising strategies. Experiments show that SAFER can precisely degrade or
enhance safety alignment with minimal data modification, without sacrificing
general chat performance. Our approach contributes to interpreting, auditing
and refining reward models in high-stakes LLM alignment tasks. Our codes are
available at https://github.com/xzy-101/SAFER-code. \textit{This paper
discusses topics related to large language model safety and may include
discussions or examples that highlight potential risks or unsafe outcomes.}

</details>


### [3] [Prompting as Scientific Inquiry](https://arxiv.org/abs/2507.00163)
*Ari Holtzman,Chenhao Tan*

Main category: cs.CL

Relevance: 85.0

TL;DR: 论文主张将提示（prompting）视为研究大型语言模型（LLMs）的科学方法，而非权宜之计。


<details>
  <summary>Details</summary>
Motivation: 当前提示方法常被视为‘炼金术’，缺乏科学性。作者认为应将提示视为行为科学，以更科学的方式研究LLMs。

Method: 通过将LLMs视为复杂且不透明的‘生物体’，提示成为研究其行为的科学工具，而非编程的替代。

Result: 提示是LLMs科学研究的关键组成部分，不应被低估。

Conclusion: 提示是LLMs研究的核心方法，应被视为行为科学的一部分。

Abstract: Prompting is the primary method by which we study and control large language
models. It is also one of the most powerful: nearly every major capability
attributed to LLMs-few-shot learning, chain-of-thought, constitutional AI-was
first unlocked through prompting. Yet prompting is rarely treated as science
and is frequently frowned upon as alchemy. We argue that this is a category
error. If we treat LLMs as a new kind of complex and opaque organism that is
trained rather than programmed, then prompting is not a workaround: it is
behavioral science. Mechanistic interpretability peers into the neural
substrate, prompting probes the model in its native interface: language. We
contend that prompting is not inferior, but rather a key component in the
science of LLMs.

</details>


### [4] [Two-Stage Reasoning-Infused Learning: Improving Classification with LLM-Generated Reasoning](https://arxiv.org/abs/2507.00214)
*Mads Henrichsen,Rasmus Krebs*

Main category: cs.CL

Relevance: 85.0

TL;DR: 论文提出了一种两阶段方法，利用LLM生成推理增强文本分类，显著提升了情感分类任务的准确性。


<details>
  <summary>Details</summary>
Motivation: 传统分类模型直接映射输入到标签，缺乏显式推理，可能限制性能、鲁棒性和可解释性。

Method: 1) 微调Llama-3.2-1B-Instruct生成推理文本；2) 使用生成的推理增强下游生成模型的训练数据。

Result: 生成模型（输出推理和情感）比基线模型（仅输出情感）准确率提升8.7个百分点。

Conclusion: LLM生成的推理能丰富训练数据，提升下游任务性能并提供显式解释。

Abstract: Standard classification models often map inputs directly to labels without
explicit reasoning, potentially limiting their performance, robustness, and
interpretability. This paper introduces a novel two-stage approach to enhance
text classification by leveraging Large Language Model (LLM)-generated
reasonings. In the first stage, we fine-tune a Llama-3.2-1B-Instruct model
(henceforth Llama-R-Gen) on a general-purpose reasoning dataset
(syvai/reasoning-gen) to generate textual reasoning (R) given a question and
its answer. In the second stage, this generally trained Llama-R-Gen is used
offline to create an augmented training dataset for a downstream generative
model. This downstream model, based on Llama-3.2-1B-Instruct, takes only the
input text (Q) and is trained to output the generated reasoning (R) immediately
followed by the predicted emotion (A). We demonstrate this methodology on the
dair-ai/emotion dataset for emotion classification. Our experiments show that
the generative model trained to output reasoning and the emotion (Classifier
Q->RA) achieves a significant improvement of 8.7 percentage points in accuracy
(for emotion prediction) compared to a baseline generative model trained solely
to output the emotion (Classifier Q->A), highlighting the strong generalization
capabilities of the reasoning generation and the benefit of explicit reasoning
training. This work underscores the potential of LLM-generated reasonings for
creating richer training datasets, thereby improving the performance of diverse
downstream NLP tasks and providing explicit explanations.

</details>


### [5] [Linearly Decoding Refused Knowledge in Aligned Language Models](https://arxiv.org/abs/2507.00239)
*Aryan Shrivastava,Ari Holtzman*

Main category: cs.CL

Relevance: 85.0

TL;DR: 研究发现，通过线性探针可以从语言模型的隐藏状态中解码出被拒绝的有害信息，表明指令调优并未完全消除这些信息，而是抑制了其直接表达。


<details>
  <summary>Details</summary>
Motivation: 研究动机是探索指令调优和强化学习对齐的语言模型是否真正消除了有害信息，还是仅仅抑制了其直接表达。

Method: 方法包括使用线性探针训练在语言模型的隐藏状态上，以解码被拒绝的信息，并分析其在指令调优模型中的可转移性和影响。

Result: 结果显示，大量被拒绝的信息是线性可解码的，且这些信息在指令调优模型中仍然存在并被间接使用。

Conclusion: 结论是指令调优并未完全消除或重新定位有害信息，而是抑制了其直接表达，使其仍可通过线性探针访问并影响下游行为。

Abstract: Most commonly used language models (LMs) are instruction-tuned and aligned
using a combination of fine-tuning and reinforcement learning, causing them to
refuse users requests deemed harmful by the model. However, jailbreak prompts
can often bypass these refusal mechanisms and elicit harmful responses. In this
work, we study the extent to which information accessed via jailbreak prompts
is decodable using linear probes trained on LM hidden states. We show that a
great deal of initially refused information is linearly decodable. For example,
across models, the response of a jailbroken LM for the average IQ of a country
can be predicted by a linear probe with Pearson correlations exceeding $0.8$.
Surprisingly, we find that probes trained on base models (which do not refuse)
sometimes transfer to their instruction-tuned versions and are capable of
revealing information that jailbreaks decode generatively, suggesting that the
internal representations of many refused properties persist from base LMs
through instruction-tuning. Importantly, we show that this information is not
merely "leftover" in instruction-tuned models, but is actively used by them: we
find that probe-predicted values correlate with LM generated pairwise
comparisons, indicating that the information decoded by our probes align with
suppressed generative behavior that may be expressed more subtly in other
downstream tasks. Overall, our results suggest that instruction-tuning does not
wholly eliminate or even relocate harmful information in representation
space-they merely suppress its direct expression, leaving it both linearly
accessible and indirectly influential in downstream behavior.

</details>


### [6] [Impact of Fine-Tuning Methods on Memorization in Large Language Models](https://arxiv.org/abs/2507.00258)
*Jie Hou,Chuxiong Wu,Lannan Luo,Qiang Zeng*

Main category: cs.CL

Relevance: 85.0

TL;DR: 论文研究了不同微调方法对LLMs隐私风险的影响，发现基于提示的微调比基于参数的微调更隐私安全。


<details>
  <summary>Details</summary>
Motivation: 随着预训练大语言模型（LLMs）能力的提升，微调方法多样化，但其隐私风险（如记忆化导致的隐私泄露）研究较少。本文旨在填补这一空白。

Method: 通过分类流行的微调方法，并基于成员推理攻击（MIAs）评估其对记忆化的影响。

Result: 基于提示的微调在性能上与基于参数的微调相当，但对MIAs的脆弱性更低，且不受模型规模影响。

Conclusion: 基于参数的微调更容易泄露隐私信息，而基于提示的微调是一种更隐私安全的替代方案。

Abstract: As the capabilities of pre-trained large language models (LLMs) continue to
advance, the "pre-train and fine-tune" paradigm has become increasingly
mainstream, leading to the development of various fine-tuning methods. However,
the privacy risks arising from memorization during fine-tuning have received
relatively little attention. To address this gap, we categorize popular
fine-tuning approaches and assess their impact on memorization through the lens
of membership inference attacks (MIAs). Our results show that, compared to
parameter-based fine-tuning, prompt-based fine-tuning achieves competitive
performance while exhibiting lower vulnerability to MIAs. Furthermore,
prompt-based methods maintain low memorization regardless of model scale. These
findings suggest that parameter-based fine-tuning is more prone to leaking
private information, whereas prompt-based fine-tuning serves as a more
privacy-preserving option.

</details>


### [7] [Failure by Interference: Language Models Make Balanced Parentheses Errors When Faulty Mechanisms Overshadow Sound Ones](https://arxiv.org/abs/2507.00322)
*Daking Rai,Samuel Miller,Kevin Moran,Ziyu Yao*

Main category: cs.CL

Relevance: 85.0

TL;DR: 研究发现语言模型在简单语法任务（如生成平衡括号）上表现不佳，原因是部分组件（如注意力头和FF神经元）引入噪声。提出RASteer方法，通过增强可靠组件的作用显著提升性能。


<details>
  <summary>Details</summary>
Motivation: 探究语言模型在简单语法任务中表现不佳的机制，并提出改进方法。

Method: 分析模型组件的行为，提出RASteer方法，通过增强可靠组件的作用优化模型。

Result: RASteer显著提升平衡括号任务的准确率（0%到100%），并在算术推理任务中实现约20%的性能提升。

Conclusion: RASteer通过优化模型组件的作用，有效提升语言模型在特定任务中的性能。

Abstract: Despite remarkable advances in coding capabilities, language models (LMs)
still struggle with simple syntactic tasks such as generating balanced
parentheses. In this study, we investigate the underlying mechanisms behind the
persistence of these errors across LMs of varying sizes (124M-7B) to both
understand and mitigate the errors. Our study reveals that LMs rely on a number
of components (attention heads and FF neurons) that independently make their
own predictions. While some components reliably promote correct answers across
a generalized range of inputs (i.e., implementing "sound mechanisms''), others
are less reliable and introduce noise by promoting incorrect tokens (i.e.,
implementing "faulty mechanisms''). Errors occur when the faulty mechanisms
overshadow the sound ones and dominantly affect the predictions. Motivated by
this insight, we introduce RASteer, a steering method to systematically
identify and increase the contribution of reliable components for improving
model performance. RASteer substantially improves performance on balanced
parentheses tasks, boosting accuracy of some models from $0$% to around $100$%
without impairing the models' general coding ability. We further demonstrate
its broader applicability in arithmetic reasoning tasks, achieving performance
gains of up to around $20$%.

</details>


### [8] [Modeling Data Diversity for Joint Instance and Verbalizer Selection in Cold-Start Scenarios](https://arxiv.org/abs/2507.00330)
*Mohna Chakraborty,Adithya Kulkarni,Qi Li*

Main category: cs.CL

Relevance: 85.0

TL;DR: COLDSELECT是一种联合选择verbalizer和实例的方法，通过建模数据多样性优化冷启动场景下的性能。


<details>
  <summary>Details</summary>
Motivation: 现有基于提示的方法对模板、verbalizer和少样本实例选择敏感，尤其在冷启动场景下表现不佳。现有研究忽略了实例与verbalizer之间的依赖关系。

Method: COLDSELECT将PLM词汇和$h_{[MASK]}$嵌入映射到共享空间，应用降维和聚类以实现高效多样的选择。

Result: 在八个基准测试中，COLDSELECT在减少不确定性和增强泛化能力方面优于基线方法。

Conclusion: COLDSELECT通过联合优化verbalizer和实例选择，显著提升了冷启动场景下的性能。

Abstract: Prompt-based methods leverage the knowledge of pre-trained language models
(PLMs) trained with a masked language modeling (MLM) objective; however, these
methods are sensitive to template, verbalizer, and few-shot instance selection,
particularly in cold-start settings with no labeled data. Existing studies
overlook the dependency between instances and verbalizers, where instance-label
probabilities depend on verbalizer token proximity in the embedding space. To
address this, we propose COLDSELECT, a joint verbalizer and instance selection
approach that models data diversity. COLDSELECT maps PLM vocabulary and
$h_{[MASK]}$ embeddings into a shared space, applying dimensionality reduction
and clustering to ensure efficient and diverse selection. By optimizing for
minimal uncertainty and maximal diversity, COLDSELECT captures data
relationships effectively. Experiments on eight benchmarks demonstrate
COLDSELECT's superiority in reducing uncertainty and enhancing generalization,
outperforming baselines in verbalizer and few-shot instance selection for
cold-start scenarios.

</details>


### [9] [Pitfalls of Evaluating Language Models with Open Benchmarks](https://arxiv.org/abs/2507.00460)
*Md. Najib Hasan,Mohammad Fakhruddin Babar,Souvika Sarkar,Monowar Hasan,Santu Karmaker*

Main category: cs.CL

Relevance: 85.0

TL;DR: 研究发现开放大语言模型基准测试存在漏洞，通过微调小模型在公开测试集上作弊，揭示了当前评估方法的不完善。


<details>
  <summary>Details</summary>
Motivation: 揭示开放基准测试（如HELM和BIG-bench）的潜在漏洞，以促进更稳健的语言模型评估方法。

Method: 通过微调小模型（BART、T5、GPT-2）在公开测试集上构造作弊模型，测试其在HELM基准上的表现。

Result: 作弊模型在HELM上排名靠前，但泛化能力差，实用性有限。

Conclusion: 需结合私有或动态基准测试，重新评估当前基准实践以确保评估的稳健性和可信度。

Abstract: Open Large Language Model (LLM) benchmarks, such as HELM and BIG-bench, offer
standardized, transparent protocols that facilitate the fair comparison,
reproducibility, and iterative advancement of Language Models (LMs). However,
their openness also introduces critical and underexplored pitfalls. This study
exposes these weaknesses by systematically constructing ``cheating'' models --
smaller variants of BART, T5, and GPT-2 fine-tuned directly on public test sets
-- which achieve top rankings on a prominent open, holistic benchmark (HELM)
despite poor generalization and limited practical utility. Our findings
underscore three key insights: \ca high leaderboard performance on open
benchmarks may not always reflect real-world effectiveness; \cb private or
dynamic benchmarks must complement open evaluations to safeguard integrity; and
\cc a fundamental reevaluation of current benchmarking practices is essential
to ensure robust and trustworthy LM assessments.

</details>


### [10] [TeamCMU at Touché: Adversarial Co-Evolution for Advertisement Integration and Detection in Conversational Search](https://arxiv.org/abs/2507.00509)
*To Eun Kim,João Coelho,Gbemileke Onilude,Jai Singh*

Main category: cs.CL

Relevance: 85.0

TL;DR: 提出了一种模块化管道，用于在基于RAG的对话系统中管理广告，包括广告重写器和广告分类器，通过合成数据和对抗性优化实现无缝广告集成和检测。


<details>
  <summary>Details</summary>
Motivation: 随着生成式搜索引擎的普及，广告与信息内容的界限模糊，引发透明度和信任问题，需要解决方案来平衡商业机会和用户体验。

Method: 采用模块化管道，包括广告重写器和广告分类器，利用合成数据和课程学习训练分类器，并通过监督微调和最佳候选选择优化广告集成。

Result: 广告分类器在检测多样化广告策略上表现优异，分类器引导的优化显著提升了广告的隐蔽性。

Conclusion: 研究提出了对抗性协同进化框架，为广告感知生成系统和鲁棒分类器的开发提供了新思路。

Abstract: As conversational search engines increasingly adopt generation-based
paradigms powered by Large Language Models (LLMs) and Retrieval-Augmented
Generation (RAG), the integration of advertisements into generated responses
presents both commercial opportunities and challenges for user experience.
Unlike traditional search, where advertisements are clearly delineated,
generative systems blur the boundary between informational content and
promotional material, raising concerns around transparency and trust. In this
work, we propose a modular pipeline for advertisement management in RAG-based
conversational systems, consisting of an ad-rewriter for seamless ad
integration and a robust ad-classifier for detection. We leverage synthetic
data to train high-performing classifiers, which are then used to guide two
complementary ad-integration strategies: supervised fine-tuning of the
ad-rewriter and a best-of-N sampling approach that selects the least detectable
ad-integrated response among multiple candidates. Our evaluation focuses on two
core questions: the effectiveness of ad classifiers in detecting diverse ad
integration strategies, and the training methods that best support coherent,
minimally intrusive ad insertion. Experimental results show that our
ad-classifier, trained on synthetic advertisement data inspired by marketing
strategies and enhanced through curriculum learning, achieves robust detection
performance. Additionally, we demonstrate that classifier-guided optimization,
through both fine-tuning and best-of-N sampling, significantly improves ad
stealth, enabling more seamless integration. These findings contribute an
adversarial co-evolution framework for developing more sophisticated ad-aware
generative search systems and robust ad classifiers.

</details>


### [11] [TUM-MiKaNi at SemEval-2025 Task 3: Towards Multilingual and Knowledge-Aware Non-factual Hallucination Identification](https://arxiv.org/abs/2507.00579)
*Miriam Anschütz,Ekaterina Gikalo,Niklas Herbster,Georg Groh*

Main category: cs.CL

Relevance: 85.0

TL;DR: 该论文提出了一种多语言幻觉识别系统，结合检索式事实验证和基于BERT的模型，用于改善LLM输出的可信度。


<details>
  <summary>Details</summary>
Motivation: 解决LLM在多语言环境中的幻觉问题，提升其可信度和实用性。

Method: 采用两阶段流程：基于Wikipedia的检索式事实验证和基于BERT的模型微调，识别常见幻觉模式。

Result: 在SemEval-2025 Task-3中表现优异，八种语言进入前十，支持更多语言。

Conclusion: 多语言幻觉识别系统有助于提升LLM输出的质量和未来应用价值。

Abstract: Hallucinations are one of the major problems of LLMs, hindering their
trustworthiness and deployment to wider use cases. However, most of the
research on hallucinations focuses on English data, neglecting the multilingual
nature of LLMs. This paper describes our submission to the SemEval-2025 Task-3
- Mu-SHROOM, the Multilingual Shared-task on Hallucinations and Related
Observable Overgeneration Mistakes. We propose a two-part pipeline that
combines retrieval-based fact verification against Wikipedia with a BERT-based
system fine-tuned to identify common hallucination patterns. Our system
achieves competitive results across all languages, reaching top-10 results in
eight languages, including English. Moreover, it supports multiple languages
beyond the fourteen covered by the shared task. This multilingual hallucination
identifier can help to improve LLM outputs and their usefulness in the future.

</details>


### [12] [Transferable Modeling Strategies for Low-Resource LLM Tasks: A Prompt and Alignment-Based](https://arxiv.org/abs/2507.00601)
*Shuangquan Lyu,Yingnan Deng,Guiran Liu,Zhen Qi,Ruotong Wang*

Main category: cs.CL

Relevance: 85.0

TL;DR: 提出了一种结合知识转移模块和参数高效微调策略的统一框架，用于提升大语言模型在低资源语言场景下的适应能力。


<details>
  <summary>Details</summary>
Motivation: 解决大语言模型在低资源语言场景下迁移和适应能力不足的问题。

Method: 引入知识对齐损失和软提示调优，结合轻量级适应模块、冻结策略和提示注入。

Result: 在跨语言任务（如MLQA、XQuAD和PAWS-X）上表现优于现有方法，尤其在数据稀缺条件下优势明显。

Conclusion: 该方法具有强通用性和可扩展性，适合复杂语义建模和多语言处理任务。

Abstract: This paper addresses the limited transfer and adaptation capabilities of
large language models in low-resource language scenarios. It proposes a unified
framework that combines a knowledge transfer module with parameter-efficient
fine-tuning strategies. The method introduces knowledge alignment loss and soft
prompt tuning to guide the model in effectively absorbing the structural
features of target languages or tasks under minimal annotation. This enhances
both generalization performance and training stability. The framework includes
lightweight adaptation modules to reduce computational costs. During training,
it integrates freezing strategies and prompt injection to preserve the model's
original knowledge while enabling quick adaptation to new tasks. The study also
conducts stability analysis experiments and synthetic pseudo-data transfer
experiments to systematically evaluate the method's applicability and
robustness across different low-resource tasks. Experimental results show that
compared with existing multilingual pre-trained models and mainstream transfer
methods, the proposed approach achieves higher performance and stability on
cross-lingual tasks such as MLQA, XQuAD, and PAWS-X. It demonstrates
particularly strong advantages under extremely data-scarce conditions. The
proposed method offers strong generality and scalability. It enhances
task-specific adaptability while preserving the general capabilities of large
language models. This makes it well-suited for complex semantic modeling and
multilingual processing tasks.

</details>


### [13] [Mixture of Reasonings: Teach Large Language Models to Reason with Adaptive Strategies](https://arxiv.org/abs/2507.00606)
*Tao Xiong,Xavier Hu,Wenyan Fan,Shengyu Zhang*

Main category: cs.CL

Relevance: 85.0

TL;DR: MoR框架通过嵌入多样化推理策略，使LLM无需外部提示工程即可自适应任务推理，显著提升性能。


<details>
  <summary>Details</summary>
Motivation: 现有LLM依赖手工设计的任务特定提示，限制了适应性和效率，MoR旨在解决这一问题。

Method: MoR分为两阶段：1) 用GPT-4o生成推理链模板；2) 构建监督微调数据集。

Result: MoR150在CoT提示下提升2.2%，基线对比提升13.5%。

Conclusion: MoR提供了一种通用解决方案，适用于多样化任务的鲁棒推理。

Abstract: Large language models (LLMs) excel in complex tasks through advanced
prompting techniques like Chain-of-Thought (CoT) and Tree-of-Thought (ToT), but
their reliance on manually crafted, task-specific prompts limits adaptability
and efficiency. We introduce Mixture of Reasoning (MoR), a training framework
that embeds diverse reasoning strategies into LLMs for autonomous,
task-adaptive reasoning without external prompt engineering. MoR has two
phases: Thought Generation, creating reasoning chain templates with models like
GPT-4o, and SFT Dataset Construction, pairing templates with benchmark datasets
for supervised fine-tuning.Our experiments show that MoR significantly enhances
performance, with MoR150 achieving 0.730 (2.2% improvement) using CoT prompting
and 0.734 (13.5% improvement) compared to baselines. MoR eliminates the need
for task-specific prompts, offering a generalizable solution for robust
reasoning across diverse tasks.

</details>


### [14] [LitBench: A Benchmark and Dataset for Reliable Evaluation of Creative Writing](https://arxiv.org/abs/2507.00769)
*Daniel Fein,Sebastian Russo,Violet Xiang,Kabir Jolly,Rafael Rafailov,Nick Haber*

Main category: cs.CL

Relevance: 85.0

TL;DR: LitBench是一个用于评估LLM生成创意写作的标准化基准和数据集，通过人类偏好标签和在线研究验证了其有效性。


<details>
  <summary>Details</summary>
Motivation: 由于创意写作缺乏明确的标准，现有的零样本LLM评估方法可靠性存疑，因此需要更稳健的评估工具。

Method: 引入LitBench，包含人类标注的故事比较数据集，并训练Bradley-Terry和生成式奖励模型，通过在线研究验证其效果。

Result: Claude-3.7-Sonnet是最强的零样本评估模型（73%与人类一致），训练后的奖励模型达到78%准确率。

Conclusion: LitBench为创意写作系统的自动评估和优化提供了可靠资源。

Abstract: Evaluating creative writing generated by large language models (LLMs) remains
challenging because open-ended narratives lack ground truths. Without
performant automated evaluation methods, off-the-shelf (OTS) language models
are employed as zero-shot judges, yet their reliability is unclear in this
context. In pursuit of robust evaluation for creative writing, we introduce
LitBench, the first standardized benchmark and paired dataset for creative
writing verification, comprising a held-out test set of 2,480 debiased,
human-labeled story comparisons drawn from Reddit and a 43,827-pair training
corpus of human preference labels. Using LitBench, we (i) benchmark zero-shot
LLM judges, (ii) train Bradley Terry and generative reward models, and (iii)
conduct an online human study to validate reward model rankings on newly
LLM-generated stories. Our benchmark identifies Claude-3.7-Sonnet as the
strongest off-the-shelf judge, reaching 73% agreement with human preferences;
among trained reward models, Bradley-Terry and Generative reward models both
attain an accuracy of 78%, outperforming all off-the-shelf judges. An online
human study further confirms that our trained reward models consistently align
with human preferences in novel LLM-generated stories. We release LitBench and
reward models at
https://huggingface.co/collections/SAA-Lab/litbench-68267b5da3aafe58f9e43461,
providing a vetted resource for reliable, automated evaluation and optimization
of creative writing systems.

</details>


### [15] [Many LLMs Are More Utilitarian Than One](https://arxiv.org/abs/2507.00814)
*Anita Keshmirian,Razan Baltaji,Babak Hemmatian,Hadi Asghari,Lav R. Varshney*

Main category: cs.CL

Relevance: 85.0

TL;DR: 研究探讨多智能体LLM在道德判断中的集体行为，发现其表面类似人类群体决策，但驱动机制不同。


<details>
  <summary>Details</summary>
Motivation: 理解LLM在多智能体系统中的集体道德判断行为，以促进AI对齐和社会推理。

Method: 测试六种LLM模型在独立和群体条件下的道德困境反应，比较与人类行为的异同。

Result: LLM群体更倾向于接受道德违规行为，但驱动机制与人类不同（降低规范敏感度或增强公正性）。

Conclusion: LLM集体行为表面类似人类，但内在机制差异显著，对AI对齐和多智能体设计有重要启示。

Abstract: Moral judgment is integral to large language model (LLM) alignment and social
reasoning. As multi-agent systems gain prominence, it becomes crucial to
understand how LLMs function collectively during collaboration, compared to
individual agents. In human moral judgment, group deliberation leads to a
utilitarian boost: a tendency to endorse norm violations that maximize benefits
for the greatest number of people despite harms. We study whether a similar
dynamic emerges in multi-agent LLM systems. We tested six models on
well-established sets of moral dilemmas across two conditions: (1) Solo, where
models reasoned independently, and (2) Group, where they engaged in multi-turn
discussions in pairs or triads. In personal moral dilemmas, where agents must
decide to directly harm one individual to maximize the utility for others, all
models found moral violations to be more acceptable when part of a group than
individually, similar to human experiments. Some models endorsed actions that
maximized overall well-being, even if they benefited strangers over familiar
individuals. Others became more willing to violate moral norms in groups.
However, while human groups show a similar action bias, the mechanism for their
utilitarian boost differs from LLMs. Whereas the human shift comes from
heightened sensitivity to decision outcomes, LLM groups show either reduced
norm sensitivity or enhanced impartiality. This suggests that while the surface
behavior of LLM collectives mimics human group reasoning, the underlying
drivers differ. We discuss the implications for AI alignment, multi-agent
design, and artificial moral reasoning.

</details>


### [16] [Stylometry recognizes human and LLM-generated texts in short samples](https://arxiv.org/abs/2507.00838)
*Karol Przystalski,Jan K. Argasiński,Iwona Grabska-Gradzińska,Jeremi K. Ochab*

Main category: cs.CL

Relevance: 85.0

TL;DR: 该论文探讨了如何使用风格计量学（stylometry）区分LLM生成文本与人类文本，涉及模型归属、知识产权和伦理AI问题。通过构建基准数据集并应用树模型分类，结果显示在多类和二分类任务中性能优异，最高准确率达0.98。


<details>
  <summary>Details</summary>
Motivation: 研究动机是解决LLM生成文本的归属问题及伦理AI使用，通过风格计量学分析LLM的写作模式。

Method: 方法包括构建基于Wikipedia的基准数据集（人类文本、LLM生成文本、摘要和改写文本），使用树模型（决策树和LightGBM）结合风格计量特征（词汇、语法、标点等）进行分类。

Result: 结果显示多类分类的Matthews相关系数达0.87，二分类准确率在0.79到1之间，Wikipedia与GPT-4的平衡数据集准确率达0.98。

Conclusion: 结论表明，对于特定文本类型，可以区分机器与人类生成文本，尤其在LLM日益复杂的背景下具有重要意义。

Abstract: The paper explores stylometry as a method to distinguish between texts
created by Large Language Models (LLMs) and humans, addressing issues of model
attribution, intellectual property, and ethical AI use. Stylometry has been
used extensively to characterise the style and attribute authorship of texts.
By applying it to LLM-generated texts, we identify their emergent writing
patterns. The paper involves creating a benchmark dataset based on Wikipedia,
with (a) human-written term summaries, (b) texts generated purely by LLMs
(GPT-3.5/4, LLaMa 2/3, Orca, and Falcon), (c) processed through multiple text
summarisation methods (T5, BART, Gensim, and Sumy), and (d) rephrasing methods
(Dipper, T5). The 10-sentence long texts were classified by tree-based models
(decision trees and LightGBM) using human-designed (StyloMetrix) and
n-gram-based (our own pipeline) stylometric features that encode lexical,
grammatical, syntactic, and punctuation patterns. The cross-validated results
reached a performance of up to .87 Matthews correlation coefficient in the
multiclass scenario with 7 classes, and accuracy between .79 and 1. in binary
classification, with the particular example of Wikipedia and GPT-4 reaching up
to .98 accuracy on a balanced dataset. Shapley Additive Explanations pinpointed
features characteristic of the encyclopaedic text type, individual overused
words, as well as a greater grammatical standardisation of LLMs with respect to
human-written texts. These results show -- crucially, in the context of the
increasingly sophisticated LLMs -- that it is possible to distinguish machine-
from human-generated texts at least for a well-defined text type.

</details>


### [17] [Scaling Laws Are Unreliable for Downstream Tasks: A Reality Check](https://arxiv.org/abs/2507.00885)
*Nicholas Lourie,Michael Y. Hu,Kyunghyun Cho*

Main category: cs.CL

Relevance: 85.0

TL;DR: 论文分析了现有下游扩展定律的数据，发现线性扩展定律仅在少数情况下适用（39%），且实验设置的微小变化可能完全改变扩展趋势。


<details>
  <summary>Details</summary>
Motivation: 探讨下游扩展定律是否能预测更大规模的任务性能，以及线性扩展趋势的适用性。

Method: 对现有下游扩展定律数据进行元分析。

Result: 仅39%的情况下线性扩展定律适用，实验设置的微小变化可能完全改变扩展趋势。

Conclusion: 需要理解扩展定律成功的条件，并接受偏离线性趋势的情况。

Abstract: Downstream scaling laws aim to predict task performance at larger scales from
pretraining losses at smaller scales. Whether this prediction should be
possible is unclear: some works demonstrate that task performance follows clear
linear scaling trends under transformation, whereas others point out
fundamental challenges to downstream scaling laws, such as emergence and
inverse scaling. In this work, we conduct a meta-analysis of existing data on
downstream scaling laws, finding that close fit to linear scaling laws only
occurs in a minority of cases: 39% of the time. Furthermore, seemingly benign
changes to the experimental setting can completely change the scaling trend.
Our analysis underscores the need to understand the conditions under which
scaling laws succeed. To fully model the relationship between pretraining loss
and downstream task performance, we must embrace the cases in which scaling
behavior deviates from linear trends.

</details>


### [18] [Discourse Heuristics For Paradoxically Moral Self-Correction](https://arxiv.org/abs/2507.00985)
*Guangliang Liu,Zimo Qi,Xitong Zhang,Kristen Marie Johnson*

Main category: cs.CL

Relevance: 85.0

TL;DR: 该论文探讨了大型语言模型（LLMs）的道德自我修正能力，揭示了其表面性和不一致性，并提出了一种基于启发式数据集的改进方法。


<details>
  <summary>Details</summary>
Motivation: 研究旨在解决LLMs在道德自我修正中的两个主要悖论：表面性和诊断能力不足。

Method: 通过分析微调语料库中的话语结构，揭示了启发式捷径的存在，并提出了基于这些启发式的改进方案。

Result: 研究发现道德自我修正依赖启发式捷径，导致修正与诊断能力的不一致。

Conclusion: 论文提出了一种改进方法，并指出了该能力在情境学习和模型规模上的泛化挑战。

Abstract: Moral self-correction has emerged as a promising approach for aligning the
output of Large Language Models (LLMs) with human moral values. However, moral
self-correction techniques are subject to two primary paradoxes. First, despite
empirical and theoretical evidence to support the effectiveness of
self-correction, this LLM capability only operates at a superficial level.
Second, while LLMs possess the capability of self-diagnosing immoral aspects of
their output, they struggle to identify the cause of this moral inconsistency
during their self-correction process. To better understand and address these
paradoxes, we analyze the discourse constructions in fine-tuning corpora
designed to enhance moral self-correction, uncovering the existence of the
heuristics underlying effective constructions. We demonstrate that moral
self-correction relies on discourse constructions that reflect heuristic
shortcuts, and that the presence of these heuristic shortcuts during
self-correction leads to inconsistency when attempting to enhance both
self-correction and self-diagnosis capabilities jointly. Based on our findings,
we propose a solution to improve moral self-correction by leveraging the
heuristics of curated datasets. We also highlight the generalization challenges
of this capability, particularly in terms of learning from situated context and
model scales.

</details>


### [19] [Should We Still Pretrain Encoders with Masked Language Modeling?](https://arxiv.org/abs/2507.00994)
*Hippolyte Gisserot-Boukhlef,Nicolas Boizard,Manuel Faysse,Duarte M. Alves,Emmanuel Malherbe,André F. T. Martins,Céline Hudelot,Pierre Colombo*

Main category: cs.CL

Relevance: 85.0

TL;DR: 论文探讨了CLM和MLM在文本表示任务中的表现差异，发现CLM更高效且稳定，提出了一种结合两者的双阶段训练策略。


<details>
  <summary>Details</summary>
Motivation: 研究CLM和MLM在文本表示任务中的表现差异，以确定哪种目标更优，并探索高效训练方法。

Method: 通过大规模实验（30个模型，15,000次评估），比较CLM和MLM的表现，并提出双阶段训练策略。

Result: CLM更高效且稳定，双阶段训练策略在固定计算预算下表现最优。

Conclusion: 双阶段训练策略结合CLM和MLM的优势，尤其适合从现有LLM生态初始化。

Abstract: Learning high-quality text representations is fundamental to a wide range of
NLP tasks. While encoder pretraining has traditionally relied on Masked
Language Modeling (MLM), recent evidence suggests that decoder models
pretrained with Causal Language Modeling (CLM) can be effectively repurposed as
encoders, often surpassing traditional encoders on text representation
benchmarks. However, it remains unclear whether these gains reflect an inherent
advantage of the CLM objective or arise from confounding factors such as model
and data scale. In this paper, we address this question through a series of
large-scale, carefully controlled pretraining ablations, training a total of 30
models ranging from 210 million to 1 billion parameters, and conducting over
15,000 fine-tuning and evaluation runs. We find that while training with MLM
generally yields better performance across text representation tasks,
CLM-trained models are more data-efficient and demonstrate improved fine-tuning
stability. Building on these findings, we experimentally show that a biphasic
training strategy that sequentially applies CLM and then MLM, achieves optimal
performance under a fixed computational training budget. Moreover, we
demonstrate that this strategy becomes more appealing when initializing from
readily available pretrained CLM models (from the existing LLM ecosystem),
reducing the computational burden needed to train best-in-class encoder models.
We release all project artifacts at https://hf.co/MLMvsCLM to foster further
research.

</details>


### [20] [MassTool: A Multi-Task Search-Based Tool Retrieval Framework for Large Language Models](https://arxiv.org/abs/2507.00487)
*Jianghao Lin,Xinyuan Wang,Xinyi Dai,Menghui Zhu,Bo Chen,Ruiming Tang,Yong Yu,Weinan Zhang*

Main category: cs.IR

Relevance: 85.0

TL;DR: MassTool是一个多任务搜索框架，旨在通过双塔架构提升工具检索精度，包括工具使用检测和查询中心图卷积网络，结合搜索式用户意图建模和自适应知识转移。


<details>
  <summary>Details</summary>
Motivation: 现有工具检索方法主要优化工具表示，忽略了精确查询理解的重要性，MassTool旨在填补这一空白。

Method: 采用双塔架构：工具使用检测塔和工具检索塔（QC-GCN），结合SUIM和AdaKT模块进行多任务学习。

Result: 实验证明MassTool显著提高了检索准确性。

Conclusion: MassTool通过联合优化多任务学习，实现了精确的查询理解和工具检索。

Abstract: Tool retrieval is a critical component in enabling large language models
(LLMs) to interact effectively with external tools. It aims to precisely filter
the massive tools into a small set of candidates for the downstream
tool-augmented LLMs. However, most existing approaches primarily focus on
optimizing tool representations, often neglecting the importance of precise
query comprehension. To address this gap, we introduce MassTool, a multi-task
search-based framework designed to enhance both query representation and tool
retrieval accuracy. MassTool employs a two-tower architecture: a tool usage
detection tower that predicts the need for function calls, and a tool retrieval
tower that leverages a query-centric graph convolution network (QC-GCN) for
effective query-tool matching. It also incorporates search-based user intent
modeling (SUIM) to handle diverse and out-of-distribution queries, alongside an
adaptive knowledge transfer (AdaKT) module for efficient multi-task learning.
By jointly optimizing tool usage detection loss, list-wise retrieval loss, and
contrastive regularization loss, MassTool establishes a robust dual-step
sequential decision-making pipeline for precise query understanding. Extensive
experiments demonstrate its effectiveness in improving retrieval accuracy. Our
code is available at https://github.com/wxydada/MassTool.

</details>


### [21] [Table Understanding and (Multimodal) LLMs: A Cross-Domain Case Study on Scientific vs. Non-Scientific Data](https://arxiv.org/abs/2507.00152)
*Ekaterina Borisova,Fabio Barth,Nils Feldhus,Raia Abu Ahmad,Malte Ostendorff,Pedro Ortiz Suarez,Georg Rehm,Sebastian Möller*

Main category: cs.CL

Relevance: 75.0

TL;DR: 该论文研究了文本和多模态LLM在表格理解任务中的表现，发现LLM在科学表格处理中存在显著挑战。


<details>
  <summary>Details</summary>
Motivation: 尽管LLM在下游任务中表现优异，但其处理表格数据的效率尚未充分探索，尤其是在跨领域和跨模态场景下。

Method: 通过跨领域和跨模态评估，比较LLM在科学与非科学表格中的表现，并分析其在不同格式（图像、文本等）下的鲁棒性。同时进行可解释性分析。

Result: LLM在表格模态间表现鲁棒，但在科学表格处理中面临显著挑战。

Conclusion: 论文提出了TableEval基准，为未来研究提供了工具，并揭示了LLM在科学表格处理中的局限性。

Abstract: Tables are among the most widely used tools for representing structured data
in research, business, medicine, and education. Although LLMs demonstrate
strong performance in downstream tasks, their efficiency in processing tabular
data remains underexplored. In this paper, we investigate the effectiveness of
both text-based and multimodal LLMs on table understanding tasks through a
cross-domain and cross-modality evaluation. Specifically, we compare their
performance on tables from scientific vs. non-scientific contexts and examine
their robustness on tables represented as images vs. text. Additionally, we
conduct an interpretability analysis to measure context usage and input
relevance. We also introduce the TableEval benchmark, comprising 3017 tables
from scholarly publications, Wikipedia, and financial reports, where each table
is provided in five different formats: Image, Dictionary, HTML, XML, and LaTeX.
Our findings indicate that while LLMs maintain robustness across table
modalities, they face significant challenges when processing scientific tables.

</details>


### [22] [LineRetriever: Planning-Aware Observation Reduction for Web Agents](https://arxiv.org/abs/2507.00210)
*Imene Kerboua,Sahar Omidi Shayegan,Megh Thakkar,Xing Han Lù,Massimo Caccia,Véronique Eglin,Alexandre Aussem,Jérémy Espinas,Alexandre Lacoste*

Main category: cs.CL

Relevance: 75.0

TL;DR: 提出了一种名为LineRetriever的新方法，用于在网页导航任务中优化检索方法，以支持自适应规划。


<details>
  <summary>Details</summary>
Motivation: 当前的方法在处理网页导航任务时，由于上下文限制，无法有效保留页面状态和动作历史的关键信息，尤其是对于自适应规划任务。

Method: 引入LineRetriever，利用语言模型识别和检索与未来导航步骤最相关的观察行，而不仅仅是语义相似性。

Result: 实验表明，LineRetriever能在减少观察大小的同时，保持性能。

Conclusion: LineRetriever为网页导航任务中的自适应规划提供了一种有效的检索优化方法。

Abstract: While large language models have demonstrated impressive capabilities in web
navigation tasks, the extensive context of web pages, often represented as DOM
or Accessibility Tree (AxTree) structures, frequently exceeds model context
limits. Current approaches like bottom-up truncation or embedding-based
retrieval lose critical information about page state and action history. This
is particularly problematic for adaptive planning in web agents, where
understanding the current state is essential for determining future actions. We
hypothesize that embedding models lack sufficient capacity to capture
plan-relevant information, especially when retrieving content that supports
future action prediction. This raises a fundamental question: how can retrieval
methods be optimized for adaptive planning in web navigation tasks? In
response, we introduce \textit{LineRetriever}, a novel approach that leverages
a language model to identify and retrieve observation lines most relevant to
future navigation steps. Unlike traditional retrieval methods that focus solely
on semantic similarity, \textit{LineRetriever} explicitly considers the
planning horizon, prioritizing elements that contribute to action prediction.
Our experiments demonstrate that \textit{LineRetriever} can reduce the size of
the observation at each step for the web agent while maintaining consistent
performance within the context limitations.

</details>


### [23] [Causal Prompting for Implicit Sentiment Analysis with Large Language Models](https://arxiv.org/abs/2507.00389)
*Jing Ren,Wenhao Zhou,Bowen Li,Mujie Liu,Nguyen Linh Dan Le,Jiade Cen,Liping Chen,Ziqi Xu,Xiwei Xu,Xiaodong Li*

Main category: cs.CL

Relevance: 75.0

TL;DR: CAPITAL是一个因果提示框架，通过将前门调整引入思维链推理，解决了现有方法依赖多数投票而忽视因果有效性的问题，提升了隐式情感分析的准确性和鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 现有基于提示的LLM方法在隐式情感分析中依赖多数投票，忽视了因果有效性，容易受内部偏见和虚假相关性的影响。

Method: CAPITAL将总体因果效应分解为输入提示对推理链的影响和推理链对最终输出的影响，利用编码器聚类和NWGM近似进行估计，并通过对比学习对齐编码器表示与LLM推理空间。

Result: 在三个LLM的基准数据集上，CAPITAL在准确性和鲁棒性上均优于基线方法，尤其在对抗条件下表现突出。

Conclusion: CAPITAL为将因果推理整合到LLM提示中提供了原则性方法，并展示了其在偏见感知情感推理中的优势。

Abstract: Implicit Sentiment Analysis (ISA) aims to infer sentiment that is implied
rather than explicitly stated, requiring models to perform deeper reasoning
over subtle contextual cues. While recent prompting-based methods using Large
Language Models (LLMs) have shown promise in ISA, they often rely on majority
voting over chain-of-thought (CoT) reasoning paths without evaluating their
causal validity, making them susceptible to internal biases and spurious
correlations. To address this challenge, we propose CAPITAL, a causal prompting
framework that incorporates front-door adjustment into CoT reasoning. CAPITAL
decomposes the overall causal effect into two components: the influence of the
input prompt on the reasoning chains, and the impact of those chains on the
final output. These components are estimated using encoder-based clustering and
the NWGM approximation, with a contrastive learning objective used to better
align the encoder's representation with the LLM's reasoning space. Experiments
on benchmark ISA datasets with three LLMs demonstrate that CAPITAL consistently
outperforms strong prompting baselines in both accuracy and robustness,
particularly under adversarial conditions. This work offers a principled
approach to integrating causal inference into LLM prompting and highlights its
benefits for bias-aware sentiment reasoning. The source code and case study are
available at: https://github.com/whZ62/CAPITAL.

</details>


### [24] [ProxAnn: Use-Oriented Evaluations of Topic Models and Document Clustering](https://arxiv.org/abs/2507.00828)
*Alexander Hoyle,Lorena Calvo-Bartolomé,Jordan Boyd-Graber,Philip Resnik*

Main category: cs.CL

Relevance: 75.0

TL;DR: 提出了一种可扩展的人类评估协议及其自动化近似方法，用于评估主题模型和文档聚类，利用LLM代理替代人工标注。


<details>
  <summary>Details</summary>
Motivation: 解决现有评估方法中自动化指标与人类偏好不一致或专家标注难以扩展的问题。

Method: 设计了一种评估协议，通过人工或LLM代理推断类别并应用于其他文档，收集标注数据验证自动化代理。

Result: 发现最佳LLM代理在统计上与人类标注者无差异，可作为自动化评估的合理替代。

Conclusion: LLM代理在主题模型和文档聚类评估中可有效替代人工标注。

Abstract: Topic model and document-clustering evaluations either use automated metrics
that align poorly with human preferences or require expert labels that are
intractable to scale. We design a scalable human evaluation protocol and a
corresponding automated approximation that reflect practitioners' real-world
usage of models. Annotators -- or an LLM-based proxy -- review text items
assigned to a topic or cluster, infer a category for the group, then apply that
category to other documents. Using this protocol, we collect extensive
crowdworker annotations of outputs from a diverse set of topic models on two
datasets. We then use these annotations to validate automated proxies, finding
that the best LLM proxies are statistically indistinguishable from a human
annotator and can therefore serve as a reasonable substitute in automated
evaluations. Package, web interface, and data are at
https://github.com/ahoho/proxann

</details>


### [25] [Mathematics Isn't Culture-Free: Probing Cultural Gaps via Entity and Scenario Perturbations](https://arxiv.org/abs/2507.00883)
*Aditya Tomar,Nihar Ranjan Sahoo,Ashish Mittal,Rudra Murthy,Pushpak Bhattacharyya*

Main category: cs.CL

Relevance: 75.0

TL;DR: 论文研究了数学问题中的文化偏见对LLM性能的影响，通过创建不同文化背景的GSM8K变体，评估了六种LLM的表现。


<details>
  <summary>Details</summary>
Motivation: 现有数学基准（如GSM8K）主要基于西方文化背景，可能影响LLM在不同文化中的表现。作者旨在评估LLM对文化变化的鲁棒性。

Method: 创建了非洲、印度、中国、韩国和日本的文化适应版GSM8K测试集，使用提示转换和人工验证。评估了六种LLM（8B-72B参数）在五种提示策略下的表现。

Result: LLM在原始美国中心数据集上表现最佳，在文化适应版本上表现较差。具备推理能力的模型对文化变化的适应性更强。

Conclusion: 文化背景影响LLM在数学任务中的表现，推理能力有助于缓解这种影响。

Abstract: Although mathematics is often considered culturally neutral, the way
mathematical problems are presented can carry implicit cultural context.
Existing benchmarks like GSM8K are predominantly rooted in Western norms,
including names, currencies, and everyday scenarios. In this work, we create
culturally adapted variants of the GSM8K test set for five regions Africa,
India, China, Korea, and Japan using prompt-based transformations followed by
manual verification. We evaluate six large language models (LLMs), ranging from
8B to 72B parameters, across five prompting strategies to assess their
robustness to cultural variation in math problem presentation. Our findings
reveal a consistent performance gap: models perform best on the original
US-centric dataset and comparatively worse on culturally adapted versions.
However, models with reasoning capabilities are more resilient to these shifts,
suggesting that deeper reasoning helps bridge cultural presentation gaps in
mathematical tasks

</details>


### [26] [Towards Style Alignment in Cross-Cultural Translation](https://arxiv.org/abs/2507.00216)
*Shreya Havaldar,Adam Stein,Eric Wong,Lyle Ungar*

Main category: cs.CL

Relevance: 70.0

TL;DR: 论文提出RASTA方法，通过检索增强风格对齐，解决LLM在跨文化翻译中风格偏差的问题。


<details>
  <summary>Details</summary>
Motivation: 文化差异导致LLM在翻译风格时出现偏差，例如礼貌表达丢失，尤其在非西方语言中表现更差。

Method: 提出RASTA方法，利用学习到的风格概念，增强LLM翻译的文化适应性。

Result: RASTA有效减少了风格偏差，提升了翻译的文化对齐性。

Conclusion: RASTA为LLM跨文化翻译提供了一种有效的风格对齐解决方案。

Abstract: Successful communication depends on the speaker's intended style (i.e., what
the speaker is trying to convey) aligning with the listener's interpreted style
(i.e., what the listener perceives). However, cultural differences often lead
to misalignment between the two; for example, politeness is often lost in
translation. We characterize the ways that LLMs fail to translate style -
biasing translations towards neutrality and performing worse in non-Western
languages. We mitigate these failures with RASTA (Retrieval-Augmented STylistic
Alignment), a method that leverages learned stylistic concepts to encourage LLM
translation to appropriately convey cultural communication norms and align
style.

</details>


### [27] [EfficientXLang: Towards Improving Token Efficiency Through Cross-Lingual Reasoning](https://arxiv.org/abs/2507.00246)
*Sanchit Ahuja,Praneetha Vaddamanu,Barun Patra*

Main category: cs.CL

Relevance: 70.0

TL;DR: 研究发现，非英语语言在推理任务中不仅减少token使用，还能保持准确性，且效果优于英语。


<details>
  <summary>Details</summary>
Motivation: 探索英语是否是最高效的推理语言，并研究多语言模型在推理任务中的表现。

Method: 评估三个开源LRM（DeepSeek R1, Qwen 2.5, Qwen 3）在四个数学数据集和七种语言上的表现。

Result: 非英语语言在推理任务中更高效且准确，且效果不受翻译影响。

Conclusion: 多语言推理具有潜力，强调多语言基础的重要性。

Abstract: Despite recent advances in Language Reasoning Models (LRMs), most research
focuses solely on English, even though many models are pretrained on
multilingual data. In this work, we investigate: Is English the most
token-efficient language for reasoning? We evaluate three open-source RLMs:
DeepSeek R1, Qwen 2.5 and Qwen 3, across four math datasets and seven
typologically diverse languages. We find that reasoning in non-English
languages not only reduces token usage, but also preserves accuracy. These
gains persist even after translating the reasoning traces into English,
suggesting genuine shifts in reasoning behavior rather than surface-level
linguistic effects. The extent of improvement, however, depends on the models
multilingual strength. Our findings motivate a broader view of reasoning in
language models, highlighting the potential of multilingual reasoning and the
importance of strong multilingual foundations. The code for our work can be
found: https://github.com/microsoft/EfficientXLang.

</details>


### [28] [Beyond Sociodemographic Prompting: Using Supervision to Align LLMs with Human Response Distributions](https://arxiv.org/abs/2507.00439)
*Gauri Kambhatla,Sanjana Gautam,Angela Zhang,Alex Liu,Ravi Srinivasan,Junyi Jessy Li,Matthew Lease*

Main category: cs.CL

Relevance: 70.0

TL;DR: 论文提出了一种简单监督方法，显著提升了语言模型与不同人口群体的对齐能力，并通过多数据集和多模型评估提供了实用基准。


<details>
  <summary>Details</summary>
Motivation: 准确预测不同人群对主观问题的回答具有重要价值，但现有方法在多样性和对齐性上存在不足。

Method: 采用相对简单的监督方法，通过多数据集和多模型评估，研究语言模型与不同人口群体的对齐性。

Result: 方法显著提升了模型的对齐能力，并提供了对不同群体对齐性的详细分析。

Conclusion: 方法的简单性和通用性便于实际应用，同时为未来研究提供了有用的基准和指导。

Abstract: The ability to accurately predict how different population groups would
answer subjective questions would have great value. In this work, we show that
use of relatively simple supervision can greatly improve language model
alignment with diverse population groups, as measured over three datasets
spanning various topics. Beyond evaluating average performance, we also report
how alignment varies across specific groups. The simplicity and generality of
our approach promotes easy adoption, while our broad findings provide useful
guidance for when to use or not use our approach in practice. By conducting
evaluation over many LLMs and prompting strategies, along with open-sourcing
our work, we provide a useful benchmark to stimulate future research.

</details>


### [29] [AI Analyst: Framework and Comprehensive Evaluation of Large Language Models for Financial Time Series Report Generation](https://arxiv.org/abs/2507.00718)
*Elizabeth Fons,Elena Kochkina,Rachneet Kaur,Zhen Zeng,Berowne Hlavaty,Charese Smiley,Svitlana Vyetrenko,Manuela Veloso*

Main category: cs.CL

Relevance: 70.0

TL;DR: 该论文探讨了大型语言模型（LLMs）从时间序列数据生成财务报告的潜力，提出了一种包含提示工程、模型选择和评估的框架，并引入了一种自动化高亮系统来分类报告中的信息。


<details>
  <summary>Details</summary>
Motivation: 研究LLMs在财务报告生成中的应用，以评估其事实基础和推理能力。

Method: 提出了一个框架，结合提示工程、模型选择和评估，并引入自动化高亮系统分类信息。

Result: 实验表明，LLMs能够生成连贯且信息丰富的财务报告。

Conclusion: LLMs在财务报告生成中具有潜力，但需进一步评估其事实基础和推理能力。

Abstract: This paper explores the potential of large language models (LLMs) to generate
financial reports from time series data. We propose a framework encompassing
prompt engineering, model selection, and evaluation. We introduce an automated
highlighting system to categorize information within the generated reports,
differentiating between insights derived directly from time series data,
stemming from financial reasoning, and those reliant on external knowledge.
This approach aids in evaluating the factual grounding and reasoning
capabilities of the models. Our experiments, utilizing both data from the real
stock market indices and synthetic time series, demonstrate the capability of
LLMs to produce coherent and informative financial reports.

</details>


### [30] [TransLaw: Benchmarking Large Language Models in Multi-Agent Simulation of the Collaborative Translation](https://arxiv.org/abs/2507.00875)
*Xi Xuan,King-kui Sin,Yufei Zhou,Chunyu Kit*

Main category: cs.CL

Relevance: 70.0

TL;DR: TransLaw是一个基于多代理LLM的框架，用于翻译香港法律判决，通过三个专业代理（翻译、注释、校对）协作，提高法律翻译的准确性、风格和结构连贯性。


<details>
  <summary>Details</summary>
Motivation: 探索LLM在法律翻译中的潜力，尤其是处理复杂法律术语和文化细微差别的能力。

Method: 采用多代理框架（Translator, Annotator, Proofreader），支持可定制的LLM配置，并与13种开源和商业LLM进行比较评估。

Result: TransLaw在语义准确性、结构连贯性和风格保真度上优于GPT-4o，但在复杂术语和风格自然度上仍落后于人类专家。

Conclusion: TransLaw展示了LLM在法律翻译中的潜力，同时指出了与人类专家的差距。

Abstract: Multi-agent systems empowered by large language models (LLMs) have
demonstrated remarkable capabilities in a wide range of downstream
applications, including machine translation. However, the potential of LLMs in
translating Hong Kong legal judgments remains uncertain due to challenges such
as intricate legal terminology, culturally embedded nuances, and strict
linguistic structures. In this work, we introduce TransLaw, a novel multi-agent
framework implemented for real-world Hong Kong case law translation. It employs
three specialized agents, namely, Translator, Annotator, and Proofreader, to
collaboratively produce translations for high accuracy in legal meaning,
appropriateness in style, and adequate coherence and cohesion in structure.
This framework supports customizable LLM configurations and achieves tremendous
cost reduction compared to professional human translation services. We
evaluated its performance using 13 open-source and commercial LLMs as agents
and obtained interesting findings, including that it surpasses GPT-4o in legal
semantic accuracy, structural coherence, and stylistic fidelity, yet trails
human experts in contextualizing complex terminology and stylistic naturalness.
Our platform website is available at CityUHK, and our bilingual judgment corpus
used for the evaluation is available at Hugging Face.

</details>


### [31] [La Leaderboard: A Large Language Model Leaderboard for Spanish Varieties and Languages of Spain and Latin America](https://arxiv.org/abs/2507.00999)
*María Grandury,Javier Aula-Blasco,Júlia Falcão,Clémentine Fourrier,Miguel González,Gonzalo Martínez,Gonzalo Santamaría,Rodrigo Agerri,Nuria Aldama,Luis Chiruzzo,Javier Conde,Helena Gómez,Marta Guerrero,Guido Ivetta,Natalia López,Flor Miriam Plaza-del-Arco,María Teresa Martín-Valdivia,Helena Montoro,Carmen Muñoz,Pedro Reviriego,Leire Rosado,Alejandro Vaca,María Estrella Vallecillo-Rodríguez,Jorge Vallego,Irune Zubiaga*

Main category: cs.CL

Relevance: 70.0

TL;DR: La Leaderboard是首个针对西班牙语及其变体的开源LLM评估排行榜，旨在推动西班牙语社区的LLM发展。


<details>
  <summary>Details</summary>
Motivation: 激励开发能够代表西班牙语社区语言和文化多样性的LLM。

Method: 结合66个数据集，评估50个模型，提供方法论指导，减少few-shot示例以降低环境影响。

Result: 展示了50个模型在多种西班牙语变体中的评估结果。

Conclusion: La Leaderboard为西班牙语社区的LLM开发设立了评估标准，并鼓励其他语言的类似项目。

Abstract: Leaderboards showcase the current capabilities and limitations of Large
Language Models (LLMs). To motivate the development of LLMs that represent the
linguistic and cultural diversity of the Spanish-speaking community, we present
La Leaderboard, the first open-source leaderboard to evaluate generative LLMs
in languages and language varieties of Spain and Latin America. La Leaderboard
is a community-driven project that aims to establish an evaluation standard for
everyone interested in developing LLMs for the Spanish-speaking community. This
initial version combines 66 datasets in Basque, Catalan, Galician, and
different Spanish varieties, showcasing the evaluation results of 50 models. To
encourage community-driven development of leaderboards in other languages, we
explain our methodology, including guidance on selecting the most suitable
evaluation setup for each downstream task. In particular, we provide a
rationale for using fewer few-shot examples than typically found in the
literature, aiming to reduce environmental impact and facilitate access to
reproducible results for a broader research community.

</details>


### [32] [SciArena: An Open Evaluation Platform for Foundation Models in Scientific Literature Tasks](https://arxiv.org/abs/2507.01001)
*Yilun Zhao,Kaiyan Zhang,Tiansheng Hu,Sihong Wu,Ronan Le Bras,Taira Anderson,Jonathan Bragg,Joseph Chee Chang,Jesse Dodge,Matt Latzke,Yixin Liu,Charles McGrady,Xiangru Tang,Zihang Wang,Chen Zhao,Hannaneh Hajishirzi,Doug Downey,Arman Cohan*

Main category: cs.CL

Relevance: 70.0

TL;DR: SciArena是一个开放协作的平台，用于评估基础模型在科学文献任务上的表现，通过社区投票方式收集数据，并发布了一个元评估基准SciArena-Eval。


<details>
  <summary>Details</summary>
Motivation: 传统科学文献评估基准缺乏开放性和社区参与，SciArena旨在通过集体智慧提供更真实的模型性能评估。

Method: 采用类似Chatbot Arena的社区投票方法，收集研究者对23个基础模型的13,000多票评估数据。

Result: 数据表明问题多样且符合实际需求，研究者评估一致性高。模型排名榜单提供了有价值的见解。

Conclusion: SciArena为科学文献任务提供了社区驱动的评估方法，并揭示了自动化评估系统的挑战。

Abstract: We present SciArena, an open and collaborative platform for evaluating
foundation models on scientific literature tasks. Unlike traditional benchmarks
for scientific literature understanding and synthesis, SciArena engages the
research community directly, following the Chatbot Arena evaluation approach of
community voting on model comparisons. By leveraging collective intelligence,
SciArena offers a community-driven evaluation of model performance on
open-ended scientific tasks that demand literature-grounded, long-form
responses. The platform currently supports 23 open-source and proprietary
foundation models and has collected over 13,000 votes from trusted researchers
across diverse scientific domains. We analyze the data collected so far and
confirm that the submitted questions are diverse, aligned with real-world
literature needs, and that participating researchers demonstrate strong
self-consistency and inter-annotator agreement in their evaluations. We discuss
the results and insights based on the model ranking leaderboard. To further
promote research in building model-based automated evaluation systems for
literature tasks, we release SciArena-Eval, a meta-evaluation benchmark based
on our collected preference data. The benchmark measures the accuracy of models
in judging answer quality by comparing their pairwise assessments with human
votes. Our experiments highlight the benchmark's challenges and emphasize the
need for more reliable automated evaluation methods.

</details>


### [33] [Natural language processing for African languages](https://arxiv.org/abs/2507.00297)
*David Ifeoluwa Adelani*

Main category: cs.CL

Relevance: 60.0

TL;DR: 该论文研究了低资源非洲语言在NLP中的挑战，通过构建高质量语料库和标注数据集，探索了多语言预训练语言模型（PLM）的适应性和性能。


<details>
  <summary>Details</summary>
Motivation: 解决非洲低资源语言在NLP中的代表性不足问题，并分析数据质量对语义表示的影响。

Method: 分析了公开语料库的噪声，构建高质量语料库，并通过监督、弱监督和迁移学习评估多语言PLM的适应性。

Result: 证明了数据质量对词嵌入的重要性，并展示了多语言PLM在低资源语言中的潜力。

Conclusion: 通过高质量数据和多语言PLM的适应性，可以提升低资源语言的NLP性能。

Abstract: Recent advances in word embeddings and language models use large-scale,
unlabelled data and self-supervised learning to boost NLP performance.
Multilingual models, often trained on web-sourced data like Wikipedia, face
challenges: few low-resource languages are included, their data is often noisy,
and lack of labeled datasets makes it hard to evaluate performance outside
high-resource languages like English. In this dissertation, we focus on
languages spoken in Sub-Saharan Africa where all the indigenous languages in
this region can be regarded as low-resourced in terms of the availability of
labelled data for NLP tasks and unlabelled data found on the web. We analyse
the noise in the publicly available corpora, and curate a high-quality corpus,
demonstrating that the quality of semantic representations learned in word
embeddings does not only depend on the amount of data but on the quality of
pre-training data. We demonstrate empirically the limitations of word
embeddings, and the opportunities the multilingual pre-trained language model
(PLM) offers especially for languages unseen during pre-training and
low-resource scenarios. We further study how to adapt and specialize
multilingual PLMs to unseen African languages using a small amount of
monolingual texts. To address the under-representation of the African languages
in NLP research, we developed large scale human-annotated labelled datasets for
21 African languages in two impactful NLP tasks: named entity recognition and
machine translation. We conduct an extensive empirical evaluation using
state-of-the-art methods across supervised, weakly-supervised, and transfer
learning settings.

</details>


### [34] [NIRANTAR: Continual Learning with New Languages and Domains on Real-world Speech Data](https://arxiv.org/abs/2507.00534)
*Tahir Javed,Kaushal Bhogale,Mitesh M. Khapra*

Main category: cs.CL

Relevance: 40.0

TL;DR: Nirantar是一个用于评估多语言和多领域ASR中持续学习（CL）的综合框架，基于真实世界数据，涵盖22种语言和208个印度地区。


<details>
  <summary>Details</summary>
Motivation: 研究旨在解决持续学习在多语言和多领域ASR中的挑战，提供更贴近真实场景的评估方法。

Method: 利用增量收集的3250小时语音数据（包括1720小时新数据），评估语言增量（LIL）、领域增量（DIL）和新型语言-领域增量（LIDIL）场景。

Result: 现有方法在动态、非均匀的语言和领域变化中表现不一致，表明需要更鲁棒的CL策略。

Conclusion: Nirantar为CL研究提供了理想的测试平台，并揭示了当前方法的局限性。

Abstract: We introduce Nirantar, a comprehensive framework for evaluating continual
learning (CL) in multilingual and multi-domain ASR. Designed to reflect
real-world CL challenges, Nirantar leverages data collected incrementally
across 22 languages and 208 districts in India through natural episodes. This
enables evaluation across Language-Incremental (LIL), Domain-Incremental (DIL),
and the novel Language-Incremental Domain-Incremental Learning (LIDIL)
scenarios. Unlike prior work that relies on simulated episodes, Nirantar
presents dynamic, non-uniform language and domain shifts, making it an ideal
testbed for CL research. With 3250 hours of human-transcribed speech, including
1720 hours newly introduced in this work, our framework enables systematic
benchmarking of CL methods. We evaluate existing approaches and demonstrate
that no single method performs consistently well, underscoring the need for
more robust CL strategies.

</details>


### [35] [Capsule Network-Based Semantic Intent Modeling for Human-Computer Interaction](https://arxiv.org/abs/2507.00540)
*Shixiao Wang,Yifan Zhuang,Runsheng Zhang,Zhijun Song*

Main category: cs.CL

Relevance: 40.0

TL;DR: 提出了一种基于胶囊网络的用户语义意图建模算法，通过动态路由机制和卷积特征提取模块提升意图识别的准确性和性能。


<details>
  <summary>Details</summary>
Motivation: 解决人机交互中意图识别准确性不足的问题。

Method: 使用胶囊网络和动态路由机制构建语义特征表示，结合卷积特征提取模块和基于边界的损失函数优化。

Result: 在公开数据集上表现优于传统方法和深度学习结构，验证了方法的稳定性和有效性。

Conclusion: 提出了一种结构化建模方法，改善了复杂语义条件下的意图识别。

Abstract: This paper proposes a user semantic intent modeling algorithm based on
Capsule Networks to address the problem of insufficient accuracy in intent
recognition for human-computer interaction. The method represents semantic
features in input text through a vectorized capsule structure. It uses a
dynamic routing mechanism to transfer information across multiple capsule
layers. This helps capture hierarchical relationships and part-whole structures
between semantic entities more effectively. The model uses a convolutional
feature extraction module as the low-level encoder. After generating initial
semantic capsules, it forms high-level abstract intent representations through
an iterative routing process. To further enhance performance, a margin-based
mechanism is introduced into the loss function. This improves the model's
ability to distinguish between intent classes. Experiments are conducted using
a public natural language understanding dataset. Multiple mainstream models are
used for comparison. Results show that the proposed model outperforms
traditional methods and other deep learning structures in terms of accuracy,
F1-score, and intent detection rate. The study also analyzes the effect of the
number of dynamic routing iterations on model performance. A convergence curve
of the loss function during training is provided. These results verify the
stability and effectiveness of the proposed method in semantic modeling.
Overall, this study presents a new structured modeling approach to improve
intent recognition under complex semantic conditions.

</details>


### [36] [Methodological Rigour in Algorithm Application: An Illustration of Topic Modelling Algorithm](https://arxiv.org/abs/2507.00547)
*Malmi Amadoru*

Main category: cs.CL

Relevance: 40.0

TL;DR: 论文提供了关于主题建模算法的方法论严谨性指南，适用于新手研究人员和审稿人。


<details>
  <summary>Details</summary>
Motivation: 由于高级计算算法的不透明性和应用缺乏透明度，研究信任可能受到影响，因此需要方法论严谨性的指导。

Method: 通过结构主题建模算法的应用示例和一套指南，讨论如何确保主题建模研究的严谨性。

Result: 提出了一套适用于主题建模的指南，并可调整应用于其他算法。

Conclusion: 论文为文献贡献了主题建模的方法论严谨性指南，并参与了计算密集型理论构建研究的讨论。

Abstract: The rise of advanced computational algorithms has opened new avenues for
computationally intensive research approaches to theory development. However,
the opacity of these algorithms and lack of transparency and rigour in their
application pose methodological challenges, potentially undermining trust in
research. The discourse on methodological rigour in this new genre of research
is still emerging. Against this backdrop, I attempt to offer guidance on
methodological rigour, particularly in the context of topic modelling
algorithms. By illustrating the application of the structural topic modelling
algorithm and presenting a set of guidelines, I discuss how to ensure rigour in
topic modelling studies. Although the guidelines are for the application of
topic modelling algorithms, they can be applied to other algorithms with
context-specific adjustments. The guidelines are helpful, especially for novice
researchers applying topic modelling, and editors and reviewers handling topic
modelling manuscripts. I contribute to the literature on topic modelling and
join the emerging dialogue on methodological rigour in computationally
intensive theory construction research.

</details>


### [37] [Contrasting Cognitive Styles in Vision-Language Models: Holistic Attention in Japanese Versus Analytical Focus in English](https://arxiv.org/abs/2507.00700)
*Ahmed Sabir,Azinovič Gasper,Mengsay Loem,Rajesh Sharma*

Main category: cs.CL

Relevance: 40.0

TL;DR: 研究探讨了不同语言训练的视觉语言模型（VLMs）是否表现出文化相关的注意力模式，发现模型输出可能隐含文化认知。


<details>
  <summary>Details</summary>
Motivation: 探索视觉语言模型是否能够反映文化背景对信息处理方式的影响，特别是东亚（整体性）与西方（分析性）的差异。

Method: 通过比较日语和英语训练的VLMs生成的图像描述，分析其是否表现出整体性或分析性倾向。

Result: VLMs不仅内化了语言结构，还再现了训练数据中的文化行为，表明文化认知可能影响模型输出。

Conclusion: 文化背景可能通过训练数据隐式地影响视觉语言模型的输出行为。

Abstract: Cross-cultural research in perception and cognition has shown that
individuals from different cultural backgrounds process visual information in
distinct ways. East Asians, for example, tend to adopt a holistic perspective,
attending to contextual relationships, whereas Westerners often employ an
analytical approach, focusing on individual objects and their attributes. In
this study, we investigate whether Vision-Language Models (VLMs) trained
predominantly on different languages, specifically Japanese and English,
exhibit similar culturally grounded attentional patterns. Using comparative
analysis of image descriptions, we examine whether these models reflect
differences in holistic versus analytic tendencies. Our findings suggest that
VLMs not only internalize the structural properties of language but also
reproduce cultural behaviors embedded in the training data, indicating that
cultural cognition may implicitly shape model outputs.

</details>


### [38] [Generative AI and the future of scientometrics: current topics and future questions](https://arxiv.org/abs/2507.00783)
*Benedetto Lepori,Jens Peter Andersen,Karsten Donnay*

Main category: cs.CL

Relevance: 40.0

TL;DR: 本文回顾了GenAI在科学计量学中的应用，并探讨了其对领域的影响。GenAI在语言生成任务中表现良好，但在需要稳定语义或领域知识的任务中存在局限。建议系统比较不同GenAI模型在特定任务中的性能，并强调需通过实证和理论反思应对知识生产模式的变化。


<details>
  <summary>Details</summary>
Motivation: 探讨GenAI在科学计量学中的应用潜力及其对领域的影响，特别是在语言生成和语义理解方面的表现。

Method: 通过文献综述和实验分析，评估GenAI在科学计量学任务（如主题标注、引用分析等）中的表现。

Result: GenAI在语言生成任务中表现良好，但在需要稳定语义或领域知识的任务中存在局限。

Conclusion: 建议系统比较不同GenAI模型的性能，并强调需通过实证和理论反思应对知识生产模式的变化。

Abstract: The aim of this paper is to review the use of GenAI in scientometrics, and to
begin a debate on the broader implications for the field. First, we provide an
introduction on GenAI's generative and probabilistic nature as rooted in
distributional linguistics. And we relate this to the debate on the extent to
which GenAI might be able to mimic human 'reasoning'. Second, we leverage this
distinction for a critical engagement with recent experiments using GenAI in
scientometrics, including topic labelling, the analysis of citation contexts,
predictive applications, scholars' profiling, and research assessment. GenAI
shows promise in tasks where language generation dominates, such as labelling,
but faces limitations in tasks that require stable semantics, pragmatic
reasoning, or structured domain knowledge. However, these results might become
quickly outdated. Our recommendation is, therefore, to always strive to
systematically compare the performance of different GenAI models for specific
tasks. Third, we inquire whether, by generating large amounts of scientific
language, GenAI might have a fundamental impact on our field by affecting
textual characteristics used to measure science, such as authors, words, and
references. We argue that careful empirical work and theoretical reflection
will be essential to remain capable of interpreting the evolving patterns of
knowledge production.

</details>


### [39] [MemeCMD: An Automatically Generated Chinese Multi-turn Dialogue Dataset with Contextually Retrieved Memes](https://arxiv.org/abs/2507.00891)
*Yuheng Wang,Xianhe Tang,Pufeng Huang*

Main category: cs.CL

Relevance: 40.0

TL;DR: 论文介绍了MemeCMD，一个自动生成的中文多轮对话数据集，结合了上下文检索的模因，用于提升多模态对话AI的表达能力。


<details>
  <summary>Details</summary>
Motivation: 现有对话数据集多为纯文本或手动标注，缺乏多模态交互的表达力和上下文细微差别。

Method: 结合大规模MLLM标注的模因库和双代理自动生成的对话，引入检索框架和自适应阈值确保模因的上下文相关性和自然使用。

Result: 实验证明该方法能生成上下文恰当且多样的模因对话，为多模态对话AI提供了可扩展且隐私保护的资源。

Conclusion: MemeCMD为多模态对话AI的发展提供了新的数据集和方法。

Abstract: Memes are widely used in online social interactions, providing vivid,
intuitive, and often humorous means to express intentions and emotions.
Existing dialogue datasets are predominantly limited to either manually
annotated or pure-text conversations, lacking the expressiveness and contextual
nuance that multimodal interactions provide.To address these challenges, we
introduce MemeCMD, an automatically generated Chinese Multi-turn Dialogue
dataset with contextually retrieved memes. Our dataset combines a large-scale,
MLLM-annotated meme library with dialogues auto-generated by dual agents across
diverse scenarios. We introduce a retrieval framework and adaptive threshold to
ensure contextually relevant, naturally spaced meme usage. Experiments
demonstrate the effectiveness of our approach in generating contextually
appropriate and diverse meme-incorporated dialogues, offering a scalable and
privacy-preserving resource for advancing multimodal conversational AI.

</details>


### [40] [Leveraging Large Language Models for Spontaneous Speech-Based Suicide Risk Detection](https://arxiv.org/abs/2507.00693)
*Yifan Gao,Jiao Fu,Long Guo,Hong Liu*

Main category: cs.SD

Relevance: 40.0

TL;DR: 论文提出了一种基于大语言模型（LLM）的方法，结合声学和语义特征，用于通过语音识别青少年自杀风险，并在SW1挑战赛中取得74%的准确率。


<details>
  <summary>Details</summary>
Motivation: 早期识别自杀风险对预防自杀行为至关重要，语音作为一种非侵入性且易于获取的指标，具有研究价值。

Method: 利用LLM进行特征提取，结合传统声学和语义特征，构建自杀风险识别模型。

Result: 在SW1挑战赛中，该方法以74%的准确率排名第一。

Conclusion: LLM在语音分析中具有潜力，可用于自杀风险评估。

Abstract: Early identification of suicide risk is crucial for preventing suicidal
behaviors. As a result, the identification and study of patterns and markers
related to suicide risk have become a key focus of current research. In this
paper, we present the results of our work in the 1st SpeechWellness Challenge
(SW1), which aims to explore speech as a non-invasive and easily accessible
mental health indicator for identifying adolescents at risk of suicide.Our
approach leverages large language model (LLM) as the primary tool for feature
extraction, alongside conventional acoustic and semantic features. The proposed
method achieves an accuracy of 74\% on the test set, ranking first in the SW1
challenge. These findings demonstrate the potential of LLM-based methods for
analyzing speech in the context of suicide risk assessment.

</details>


### [41] [Multi-interaction TTS toward professional recording reproduction](https://arxiv.org/abs/2507.00808)
*Hiroki Kanagawa,Kenichi Fujita,Aya Watanabe,Yusuke Ijima*

Main category: cs.SD

Relevance: 40.0

TL;DR: 提出了一种支持多步交互的TTS方法，允许用户直观快速地调整合成语音的风格。


<details>
  <summary>Details</summary>
Motivation: 现有TTS系统缺乏类似语音导演与演员之间的迭代反馈机制，导致合成语音风格难以精细调整。

Method: 通过建模TTS模型与用户之间的交互关系，模拟语音导演与演员的协作过程。

Result: 实验表明，该方法支持用户根据需求迭代调整语音风格，具备多步交互能力。

Conclusion: 该方法填补了TTS领域在迭代风格调整上的空白，提升了用户体验。

Abstract: Voice directors often iteratively refine voice actors' performances by
providing feedback to achieve the desired outcome. While this iterative
feedback-based refinement process is important in actual recordings, it has
been overlooked in text-to-speech synthesis (TTS). As a result, fine-grained
style refinement after the initial synthesis is not possible, even though the
synthesized speech often deviates from the user's intended style. To address
this issue, we propose a TTS method with multi-step interaction that allows
users to intuitively and rapidly refine synthetized speech. Our approach models
the interaction between the TTS model and its user to emulate the relationship
between voice actors and voice directors. Experiments show that the proposed
model with its corresponding dataset enable iterative style refinements in
accordance with users' directions, thus demonstrating its multi-interaction
capability. Sample audios are available: https://ntt-hilab-gensp.
github.io/ssw13multiinteraction_tts/

</details>


### [42] [Verifiable Natural Language to Linear Temporal Logic Translation: A Benchmark Dataset and Evaluation Suite](https://arxiv.org/abs/2507.00877)
*William H English,Chase Walker,Dominic Simon,Sumit Kumar Jha,Rickard Ewetz*

Main category: eess.SY

Relevance: 40.0

TL;DR: 论文介绍了VLTL-Bench，一个用于评估自然语言到线性时序逻辑（NL-to-LTL）翻译系统可验证性的新基准。


<details>
  <summary>Details</summary>
Motivation: 现有NL-to-TL翻译系统在现有基准上表现近乎完美，但忽略了系统在新场景中接地原子命题的能力，这是验证公式的关键。

Method: 提出VLTL-Bench，包含三个独特状态空间、多样化的自然语言和形式化规范，以及验证时序逻辑表达式的样本轨迹。

Result: 基准支持端到端评估，并提供每个子步骤的基准真值，以促进方法改进。

Conclusion: VLTL-Bench旨在推动可验证NL-to-LTL翻译方法的发展。

Abstract: Empirical evaluation of state-of-the-art natural-language (NL) to
temporal-logic (TL) translation systems reveals near-perfect performance on
existing benchmarks. However, current studies measure only the accuracy of the
translation of NL logic into formal TL, ignoring a system's capacity to ground
atomic propositions into new scenarios or environments. This is a critical
feature, necessary for the verification of resulting formulas in a concrete
state space. Consequently, most NL-to-TL translation frameworks propose their
own bespoke dataset in which the correct grounding is known a-priori, inflating
performance metrics and neglecting the need for extensible, domain-general
systems. In this paper, we introduce the Verifiable Linear Temporal Logic
Benchmark ( VLTL-Bench), a unifying benchmark that measures verification and
verifiability of automated NL-to-LTL translation. The dataset consists of three
unique state spaces and thousands of diverse natural language specifications
and corresponding formal specifications in temporal logic. Moreover, the
benchmark contains sample traces to validate the temporal logic expressions.
While the benchmark directly supports end-to-end evaluation, we observe that
many frameworks decompose the process into i) lifting, ii) grounding, iii)
translation, and iv) verification. The benchmark provides ground truths after
each of these steps to enable researches to improve and evaluate different
substeps of the overall problem. To encourage methodologically sound advances
in verifiable NL-to-LTL translation approaches, we release VLTL-Bench here:
https://www.kaggle.com/datasets/dubascudes/vltl bench.

</details>


### [43] [A Diagrammatic Calculus for a Functional Model of Natural Language Semantics](https://arxiv.org/abs/2507.00782)
*Matthieu Pierre Boyer*

Main category: cs.CL

Relevance: 30.0

TL;DR: 论文提出了一种基于函数式编程的自然语言语义方法，通过类型和效应系统以及图示演算提高表达效率。


<details>
  <summary>Details</summary>
Motivation: 传统指称语义的表达性有限，作者希望通过函数式编程方法提升自然语言语义的表达能力。

Method: 构建了基于类别的类型和效应系统，并开发了图示演算模型，用于高效计算句子的指称。

Result: 该方法能够更高效地处理自然语言语义，提升表达性。

Conclusion: 函数式编程方法为自然语言语义提供了新的表达和计算途径。

Abstract: In this paper, we study a functional programming approach to natural language
semantics, allowing us to increase the expressivity of a more traditional
denotation style. We will formalize a category based type and effect system,
and construct a diagrammatic calculus to model parsing and handling of effects,
and use it to efficiently compute the denotations for sentences.

</details>


### [44] [Beat and Downbeat Tracking in Performance MIDI Using an End-to-End Transformer Architecture](https://arxiv.org/abs/2507.00466)
*Sebastian Murgul,Michael Heizmann*

Main category: cs.SD

Relevance: 30.0

TL;DR: 论文提出了一种基于Transformer的端到端模型，用于MIDI音乐中的节拍和强拍跟踪，通过新颖的数据预处理技术提升了准确性和泛化能力，并在多个数据集上验证了其优越性。


<details>
  <summary>Details</summary>
Motivation: 现有方法主要关注基于音频的节拍跟踪，而MIDI音乐中的节拍跟踪是一个重要但具有挑战性的任务，本文旨在填补这一空白。

Method: 采用编码器-解码器架构的Transformer模型，结合动态数据增强和优化的标记化策略，实现MIDI输入到节拍标注的序列到序列翻译。

Result: 模型在A-MAPS、ASAP、GuitarSet和Leduc数据集上表现优于现有的隐马尔可夫模型和深度学习方法，F1分数具有竞争力。

Conclusion: Transformer架构在符号音乐节拍跟踪中具有潜力，未来可进一步与自动音乐转录系统集成。

Abstract: Beat tracking in musical performance MIDI is a challenging and important task
for notation-level music transcription and rhythmical analysis, yet existing
methods primarily focus on audio-based approaches. This paper proposes an
end-to-end transformer-based model for beat and downbeat tracking in
performance MIDI, leveraging an encoder-decoder architecture for
sequence-to-sequence translation of MIDI input to beat annotations. Our
approach introduces novel data preprocessing techniques, including dynamic
augmentation and optimized tokenization strategies, to improve accuracy and
generalizability across different datasets. We conduct extensive experiments
using the A-MAPS, ASAP, GuitarSet, and Leduc datasets, comparing our model
against state-of-the-art hidden Markov models (HMMs) and deep learning-based
beat tracking methods. The results demonstrate that our model outperforms
existing symbolic music beat tracking approaches, achieving competitive
F1-scores across various musical styles and instruments. Our findings highlight
the potential of transformer architectures for symbolic beat tracking and
suggest future integration with automatic music transcription systems for
enhanced music analysis and score generation.

</details>


### [45] [The Algebraic Structure of Morphosyntax](https://arxiv.org/abs/2507.00244)
*Isabella Senturia,Matilde Marcolli*

Main category: cs.CL

Relevance: 20.0

TL;DR: 论文提出了一个数学模型，描述形态学与句法学的接口，通过代数操作和形态学树的概念重新解释分布式形态学中的操作。


<details>
  <summary>Details</summary>
Motivation: 研究动机是形式化形态学与句法学之间的接口，以更好地理解语言结构的生成过程。

Method: 方法包括使用代数结构（如magma和operad）建模形态学树，并通过形态学余积和操作对应关系描述形态句法树的形成。

Result: 结果表明，形态学与句法学的接口可以通过代数操作和形态学余积来形式化，并重新解释分布式形态学中的操作。

Conclusion: 结论是提出的数学模型为形态句法接口提供了新的理论框架，并支持形态学与句法学边界的灵活性。

Abstract: Within the context of the mathematical formulation of Merge and the Strong
Minimalist Thesis, we present a mathematical model of the morphology-syntax
interface. In this setting, morphology has compositional properties responsible
for word formation, organized into a magma of morphological trees. However,
unlike syntax, we do not have movement within morphology. A coproduct
decomposition exists, but it requires extending the set of morphological trees
beyond those which are generated solely by the magma, to a larger set of
possible morphological inputs to syntactic trees. These participate in the
formation of morphosyntactic trees as an algebra over an operad, and a
correspondence between algebras over an operad. The process of structure
formation for morphosyntactic trees can then be described in terms of this
operadic correspondence that pairs syntactic and morphological data and the
morphology coproduct. We reinterpret in this setting certain operations of
Distributed Morphology as transformation that allow for flexibility in moving
the boundary between syntax and morphology within the morphosyntactic objects.

</details>


### [46] [Gregorian melody, modality, and memory: Segmenting chant with Bayesian nonparametrics](https://arxiv.org/abs/2507.00380)
*Vojtěch Lanz,Jan Hajič jr*

Main category: cs.CL

Relevance: 20.0

TL;DR: 论文研究了格里高利圣歌的旋律分割，使用无监督的分层Pitman-Yor语言模型寻找最优分割，发现其在模式分类中表现优异，但结果不支持传统的'centonisation'理论。


<details>
  <summary>Details</summary>
Motivation: 探索格里高利圣歌旋律的分割方式，验证'centonisation'理论的有效性，并研究记忆效率与模式分类的关系。

Method: 采用无监督的分层Pitman-Yor语言模型对圣歌旋律进行分割，并分析其在模式分类中的表现。

Result: 找到的分割在模式分类中达到最优性能，但不符合'centonisation'理论。发现旋律的开头和结尾更公式化，与表演实践相关。

Conclusion: 即使记忆最优的分割也不支持'centonisation'理论，表明该理论可能不适用于格里高利圣歌。

Abstract: The idea that Gregorian melodies are constructed from some vocabulary of
segments has long been a part of chant scholarship. This so-called
"centonisation" theory has received much musicological criticism, but frequent
re-use of certain melodic segments has been observed in chant melodies, and the
intractable number of possible segmentations allowed the option that some
undiscovered segmentation exists that will yet prove the value of
centonisation, and recent empirical results have shown that segmentations can
outperform music-theoretical features in mode classification. Inspired by the
fact that Gregorian chant was memorised, we search for an optimal unsupervised
segmentation of chant melody using nested hierarchical Pitman-Yor language
models. The segmentation we find achieves state-of-the-art performance in mode
classification. Modeling a monk memorising the melodies from one liturgical
manuscript, we then find empirical evidence for the link between mode
classification and memory efficiency, and observe more formulaic areas at the
beginnings and ends of melodies corresponding to the practical role of modality
in performance. However, the resulting segmentations themselves indicate that
even such a memory-optimal segmentation is not what is understood as
centonisation.

</details>


### [47] [The Cognate Data Bottleneck in Language Phylogenetics](https://arxiv.org/abs/2507.00911)
*Luise Häuser,Alexandros Stamatakis*

Main category: cs.CL

Relevance: 20.0

TL;DR: 论文探讨了利用计算系统发育方法处理同源数据的挑战，指出当前缺乏自动生成大规模同源数据集的方法，并验证了从BabelNet提取的数据集在系统发育推断中的不一致性。


<details>
  <summary>Details</summary>
Motivation: 研究动机在于解决历史语言学中计算系统发育方法因缺乏大规模同源数据集而难以应用的问题。

Method: 方法包括从BabelNet自动提取同源数据集，并通过系统发育推断验证其与金标准树的不一致性。

Result: 结果显示提取的数据集在系统发育推断中表现不佳，且难以从其他多语言资源中获取更合适的特征矩阵。

Conclusion: 结论指出，目前无法将需要大数据集的系统发育分析方法应用于同源数据，其应用前景仍存疑问。

Abstract: To fully exploit the potential of computational phylogenetic methods for
cognate data one needs to leverage specific (complex) models an machine
learning-based techniques. However, both approaches require datasets that are
substantially larger than the manually collected cognate data currently
available. To the best of our knowledge, there exists no feasible approach to
automatically generate larger cognate datasets. We substantiate this claim by
automatically extracting datasets from BabelNet, a large multilingual
encyclopedic dictionary. We demonstrate that phylogenetic inferences on the
respective character matrices yield trees that are largely inconsistent with
the established gold standard ground truth trees. We also discuss why we
consider it as being unlikely to be able to extract more suitable character
matrices from other multilingual resources. Phylogenetic data analysis
approaches that require larger datasets can therefore not be applied to cognate
data. Thus, it remains an open question how, and if these computational
approaches can be applied in historical linguistics.

</details>


### [48] [Safe Low Bandwidth SPV: A Formal Treatment of Simplified Payment Verification Protocols and Security Bounds](https://arxiv.org/abs/2507.00740)
*Craig S Wright*

Main category: cs.CR

Relevance: 20.0

TL;DR: 本文对比特币白皮书中的简化支付验证（SPV）进行了完整的规范描述、协议定义和数学证明，证明了其在数字现金系统中的安全性和最优性。


<details>
  <summary>Details</summary>
Motivation: 澄清SPV协议的误解，证明其在有限对抗假设下的安全性，并为实现提供蓝图。

Method: 基于符号自动机、默克尔成员关系和证明链支配谓词重建SPV协议，并通过概率和博弈论分析验证其安全性。

Result: 证明了SPV在部分连接、敌对中继网络和对抗传播延迟下的活跃性和安全性，并提出了低带宽优化。

Conclusion: SPV是安全且最优的，适用于需要可扩展和可验证交易包含的数字现金系统。

Abstract: This paper presents a complete formal specification, protocol description,
and mathematical proof structure for Simplified Payment Verification (SPV) as
originally defined in the Bitcoin whitepaper \cite{nakamoto2008}. In stark
contrast to the misrepresentations proliferated by popular implementations, we
show that SPV is not only secure under bounded adversarial assumptions but
strictly optimal for digital cash systems requiring scalable and verifiable
transaction inclusion. We reconstruct the SPV protocol from first principles,
grounding its verification model in symbolic automata, Merkle membership
relations, and chain-of-proof dominance predicates. Through rigorous
probabilistic and game-theoretic analysis, we derive the economic bounds within
which the protocol operates securely and verify its liveness and safety
properties under partial connectivity, hostile relay networks, and adversarial
propagation delay. Our specification further introduces low-bandwidth
optimisations such as adaptive polling and compressed header synchronisation
while preserving correctness. This document serves both as a blueprint for
secure SPV implementation and a rebuttal of common misconceptions surrounding
non-validating clients.

</details>


<div id='cs.CV'></div>

# cs.CV [[Back]](#toc)

### [49] [Beyond Low-Rank Tuning: Model Prior-Guided Rank Allocation for Effective Transfer in Low-Data and Large-Gap Regimes](https://arxiv.org/abs/2507.00327)
*Chuyan Zhang,Kefan Wang,Yun Gu*

Main category: cs.CV

Relevance: 85.0

TL;DR: SR-LoRA是一种基于稳定秩的低秩自适应方法，通过层间秩分配提升模型在领域差距大的任务中的适应性，同时保持高效性。


<details>
  <summary>Details</summary>
Motivation: 固定低秩结构的LoRA在领域差距大的任务中适应性不足，现有动态调整方法计算成本高。

Method: 利用预训练权重矩阵的稳定秩作为层间秩分配的先验，实现高效且自适应的低秩调整。

Result: 在领域差距大的少样本任务中，SR-LoRA性能优于现有自适应LoRA变体，平衡了性能与效率。

Conclusion: SR-LoRA通过稳定秩指导的秩分配，提升了低秩自适应的适应性和效率。

Abstract: Low-Rank Adaptation (LoRA) has proven effective in reducing computational
costs while maintaining performance comparable to fully fine-tuned foundation
models across various tasks. However, its fixed low-rank structure restricts
its adaptability in scenarios with substantial domain gaps, where higher ranks
are often required to capture domain-specific complexities. Current adaptive
LoRA methods attempt to overcome this limitation by dynamically expanding or
selectively allocating ranks, but these approaches frequently depend on
computationally intensive techniques such as iterative pruning, rank searches,
or additional regularization. To address these challenges, we introduce Stable
Rank-Guided Low-Rank Adaptation (SR-LoRA), a novel framework that utilizes the
stable rank of pre-trained weight matrices as a natural prior for layer-wise
rank allocation. By leveraging the stable rank, which reflects the intrinsic
dimensionality of the weights, SR-LoRA enables a principled and efficient
redistribution of ranks across layers, enhancing adaptability without incurring
additional search costs. Empirical evaluations on few-shot tasks with
significant domain gaps show that SR-LoRA consistently outperforms recent
adaptive LoRA variants, achieving a superior trade-off between performance and
efficiency. Our code is available at
https://github.com/EndoluminalSurgicalVision-IMR/SR-LoRA.

</details>


### [50] [LLaVA-SP: Enhancing Visual Representation with Visual Spatial Tokens for MLLMs](https://arxiv.org/abs/2507.00505)
*Haoran Lou,Chunxiao Fan,Ziyan Liu,Yuexin Wu,Xinxiang Wang*

Main category: cs.CV

Relevance: 85.0

TL;DR: LLaVA-SP通过添加六个空间视觉标记增强多模态大语言模型的视觉表示，提出两种变体并显著提升性能。


<details>
  <summary>Details</summary>
Motivation: CLIP-ViT在捕捉局部图像特征时表现不佳，影响多模态大语言模型的详细理解能力。

Method: 提出LLaVA-SP，添加空间视觉标记，使用卷积核和交叉注意力机制增强视觉表示，并设计两种变体（Cropping和Pooling）。

Result: 在多种多模态基准测试中性能显著提升，优于LLaVA-1.5，推理延迟几乎相同。

Conclusion: LLaVA-SP通过简单但有效的改进显著提升了多模态大语言模型的视觉理解能力。

Abstract: The architecture of multimodal large language models (MLLMs) commonly
connects a vision encoder, often based on CLIP-ViT, to a large language model.
While CLIP-ViT works well for capturing global image features, it struggles to
model local relationships between adjacent patches, leading to weaker visual
representation, which in turn affects the detailed understanding ability of
MLLMs. To solve this, we propose LLaVA-SP, which \textbf{ only adds six spatial
visual tokens} to the original visual tokens to enhance the visual
representation. Our approach offers three key advantages: 1)We propose a novel
Projector, which uses convolutional kernels to derive visual spatial tokens
from ViT patch features, simulating two visual spatial ordering approaches:
``from central region to global" and ``from abstract to specific". Then, a
cross-attention mechanism is applied to fuse fine-grained visual information,
enriching the overall visual representation. 2) We present two model variants:
LLaVA-SP-Cropping, which focuses on detail features through progressive
cropping, and LLaVA-SP-Pooling, which captures global semantics through
adaptive pooling, enabling the model to handle diverse visual understanding
tasks. 3) Extensive experiments show that LLaVA-SP, fine-tuned with LoRA,
achieves significant performance improvements across various multimodal
benchmarks, outperforming the state-of-the-art LLaVA-1.5 model in multiple
tasks with nearly identical inference latency. The code and models are
available at
\href{https://github.com/CnFaker/LLaVA-SP}{\texttt{https://github.com/CnFaker/LLaVA-SP}}.

</details>


### [51] [Rectifying Magnitude Neglect in Linear Attention](https://arxiv.org/abs/2507.00698)
*Qihang Fan,Huaibo Huang,Yuang Ai,ran He*

Main category: cs.CV

Relevance: 85.0

TL;DR: 论文提出了一种名为MALA的线性注意力机制，通过引入查询向量的幅度信息，解决了传统线性注意力性能下降的问题，并在多个任务中表现优异。


<details>
  <summary>Details</summary>
Motivation: 线性注意力虽能实现线性复杂度，但其性能显著低于标准Softmax注意力。论文旨在分析并解决这一问题。

Method: 基于线性注意力的公式分析，发现其忽略了查询向量的幅度信息，导致注意力分布无法动态调整。提出MALA，通过修改线性注意力的计算方式，引入查询向量的幅度信息。

Result: MALA在图像分类、目标检测、实例分割、语义分割、自然语言处理、语音识别和图像生成等多个任务中表现优异。

Conclusion: MALA通过引入查询向量的幅度信息，显著提升了线性注意力的性能，同时保持了线性复杂度。

Abstract: As the core operator of Transformers, Softmax Attention exhibits excellent
global modeling capabilities. However, its quadratic complexity limits its
applicability to vision tasks. In contrast, Linear Attention shares a similar
formulation with Softmax Attention while achieving linear complexity, enabling
efficient global information modeling. Nevertheless, Linear Attention suffers
from a significant performance degradation compared to standard Softmax
Attention. In this paper, we analyze the underlying causes of this issue based
on the formulation of Linear Attention. We find that, unlike Softmax Attention,
Linear Attention entirely disregards the magnitude information of the Query.
This prevents the attention score distribution from dynamically adapting as the
Query scales. As a result, despite its structural similarity to Softmax
Attention, Linear Attention exhibits a significantly different attention score
distribution. Based on this observation, we propose Magnitude-Aware Linear
Attention (MALA), which modifies the computation of Linear Attention to fully
incorporate the Query's magnitude. This adjustment allows MALA to generate an
attention score distribution that closely resembles Softmax Attention while
exhibiting a more well-balanced structure. We evaluate the effectiveness of
MALA on multiple tasks, including image classification, object detection,
instance segmentation, semantic segmentation, natural language processing,
speech recognition, and image generation. Our MALA achieves strong results on
all of these tasks. Code will be available at https://github.com/qhfan/MALA

</details>


### [52] [CaughtCheating: Is Your MLLM a Good Cheating Detective? Exploring the Boundary of Visual Perception and Reasoning](https://arxiv.org/abs/2507.00045)
*Ming Li,Chenguang Wang,Yijun Liang,Xiyao Wang,Yuhang Zhou,Xiyang Wu,Yuqing Zhang,Ruiyi Zhang,Tianyi Zhou*

Main category: cs.CV

Relevance: 75.0

TL;DR: 论文探讨了多模态大语言模型（MLLMs）在复杂视觉推理任务中的表现，发现其在某些场景（如CaughtCheating）中表现极差，并提出此类任务对提升模型能力的重要性。


<details>
  <summary>Details</summary>
Motivation: 现有MLLMs在简单任务上表现优异，但在复杂场景（如侦探推理）中能力不足，需开发更具挑战性的测试任务。

Method: 通过设计CaughtCheating任务（基于社交媒体请求的视觉推理场景），对GPT-o3进行实验和分析。

Result: GPT-o3在CaughtCheating任务中表现接近零，揭示了MLLMs在复杂视觉推理中的局限性。

Conclusion: CaughtCheating任务为提升MLLMs的视觉感知和推理能力提供了重要方向，具有实际应用价值。

Abstract: Recent agentic Multi-Modal Large Language Models (MLLMs) such as GPT-o3 have
achieved near-ceiling scores on various existing benchmarks, motivating a
demand for more challenging test tasks. These MLLMs have been reported to excel
in a few expert-level tasks for humans, e.g., GeoGuesser, reflecting their
potential as a detective who can notice minuscule cues in an image and weave
them into coherent, situational explanations, leading to a reliable answer. But
can they match the performance of excellent human detectives? To answer this
question, we investigate some hard scenarios where GPT-o3 can still handle, and
find a common scenario where o3's performance drops to nearly zero, which we
name CaughtCheating. It is inspired by the social media requests that ask
others to detect suspicious clues from photos shared by the poster's partner.
We conduct extensive experiments and analysis to understand why existing MLLMs
lack sufficient capability to solve this kind of task. CaughtCheating provides
a class of challenging visual perception and reasoning tasks with great value
and practical usage. Success in these tasks paves the way for MLLMs to acquire
human-level detective perception and reasoning capabilities.

</details>


### [53] [Not All Attention Heads Are What You Need: Refining CLIP's Image Representation with Attention Ablation](https://arxiv.org/abs/2507.00537)
*Feng Lin,Marco Chen,Haokui Zhang,Xiaotian Yu,Guangming Lu,Rong Xiao*

Main category: cs.CV

Relevance: 75.0

TL;DR: 本文提出了一种名为注意力消融技术（AAT）的方法，通过抑制CLIP图像编码器中特定注意力头的贡献来提升下游任务性能。实验表明，AAT在跨模态检索等任务中显著提升了性能。


<details>
  <summary>Details</summary>
Motivation: CLIP在多种应用中表现优异，但某些注意力头可能对最终表示产生负面影响。通过消融这些头，可以提升模型性能。

Method: 提出AAT方法，通过操纵注意力权重来抑制特定头的贡献，并结合两种策略适应不同场景。

Result: AAT在跨模态检索任务中最高提升11.1%的召回率，且不增加推理成本。

Conclusion: AAT能有效优化大规模视觉语言模型，提升性能而不增加计算负担。

Abstract: This paper studies the role of attention heads in CLIP's image encoder. While
CLIP has exhibited robust performance across diverse applications, we
hypothesize that certain attention heads negatively affect final
representations and that ablating them can improve performance in downstream
tasks. To capitalize on this insight, we propose a simple yet effective method,
called Attention Ablation Technique (AAT), to suppress the contribution of
specific heads by manipulating attention weights. By integrating two
alternative strategies tailored for different application scenarios, AAT
systematically identifies and ablates detrimental attention heads to enhance
representation quality. Experiments demonstrate that AAT consistently improves
downstream task performance across various domains, boosting recall rate by up
to 11.1% on CLIP-family models for cross-modal retrieval. The results highlight
the potential of AAT to effectively refine large-scale vision-language models
with virtually no increase in inference cost.

</details>


### [54] [Improving the Reasoning of Multi-Image Grounding in MLLMs via Reinforcement Learning](https://arxiv.org/abs/2507.00748)
*Bob Zhang,Haoran Li,Tao Zhang,Cilin Yan,Jiayin Cai,Xiaolong Jiang,Yanbin Hao*

Main category: cs.CV

Relevance: 75.0

TL;DR: 该论文提出了一种基于强化学习的后训练策略，用于提升多模态大语言模型（MLLMs）在多图像任务中的推理性能。通过合成高质量思维链数据、监督微调和规则强化学习，模型在多图像基准测试中表现显著提升。


<details>
  <summary>Details</summary>
Motivation: 多模态大语言模型在单图像任务中表现优异，但在多图像任务中推理和泛化能力不足。本文旨在通过强化学习改进其多图像推理性能。

Method: 1. 合成高质量思维链数据用于冷启动初始化；2. 使用低秩适应（LoRA）进行监督微调；3. 通过拒绝采样和规则强化学习优化推理路径。

Result: 在MIG-Bench上提升9.04%，在多个域外基准测试上提升4.98%，在BLINK和MMIU子集上分别提升3.1%和2.4%。

Conclusion: 该方法有效提升了MLLMs在多图像任务中的推理和泛化能力，展示了强化学习在模型优化中的潜力。

Abstract: Recently, Multimodal Large Language Models (MLLMs) excel at visual grounding
in single-image scenarios with textual references. However, their performance
degrades when handling real-world applications involving complex multi-image
compositions and multimodal instructions, which reveals limitations in
cross-image reasoning and generalization. To address these challenges, we adopt
a Reinforcement Learning (RL) based post-training strategy to improve the
reasoning performance of MLLMs in multi-image grounding tasks. Our approach
begins with synthesizing high-quality chain-of-thought (CoT) data for
cold-start initialization, followed by supervised fine-tuning (SFT) using
low-rank adaptation (LoRA). The cold-start training stage enables the model to
identify correct solutions. Subsequently, we perform rejection sampling using
the merged SFT model to curate high-quality RL data and leverage rule-based RL
to guide the model toward optimal reasoning paths. Extensive experimental
results demonstrate the effectiveness of our approach, achieving +9.04\%
improvements on MIG-Bench and +4.98\% improvements on several out-of-domain
reasoning grounding benchmarks over the SFT baseline. Furthermore, our approach
exhibits strong generalization in multi-image perception, with gains of +3.1\%
and +2.4\% over the base model on subsets of the BLINK and MMIU benchmarks,
respectively.

</details>


### [55] [Language-Unlocked ViT (LUViT): Empowering Self-Supervised Vision Transformers with LLMs](https://arxiv.org/abs/2507.00754)
*Selim Kuzucu,Muhammad Ferjad Naeem,Anna Kukleva,Federico Tombari,Bernt Schiele*

Main category: cs.CV

Relevance: 75.0

TL;DR: LUViT bridges the modality mismatch between LLMs and ViTs through a synergistic pre-training strategy, improving vision task performance.


<details>
  <summary>Details</summary>
Motivation: The challenge of modality mismatch between text-centric LLMs and vision-centric ViTs limits their combined potential.

Method: LUViT co-adapts ViT and LLM blocks using MAE for ViT pre-training and LoRA layers for LLM alignment.

Result: LUViT significantly enhances performance on downstream vision tasks.

Conclusion: LUViT provides an effective way to leverage LLM knowledge for visual understanding.

Abstract: The integration of Large Language Model (LLMs) blocks with Vision
Transformers (ViTs) holds immense promise for vision-only tasks by leveraging
the rich semantic knowledge and reasoning capabilities of LLMs. However, a
fundamental challenge lies in the inherent modality mismatch between
text-centric pretraining of LLMs and vision-centric training of ViTs. Direct
fusion often fails to fully exploit the LLM's potential and suffers from
unstable finetuning. As a result, LLM blocks are kept frozen while only the
vision components are learned. As a remedy to these challenges, we introduce
Language-Unlocked Vision Transformers (LUViT), a novel approach that bridges
this modality mismatch through a synergistic pre-training strategy. LUViT
co-adapts a ViT backbone and an LLM fusion block by (1) employing Masked
Auto-Encoding (MAE) to pre-train the ViT for richer visual representations, and
(2) concurrently training Low-Rank Adaptation (LoRA) layers within the LLM
block using the MAE objective. This joint optimization guides the ViT to
produce LLM-aligned features and the LLM to effectively interpret visual
information. We demonstrate through extensive experiments that LUViT
significantly improves performance on various downstream vision tasks,
showcasing a more effective and efficient pathway to harness LLM knowledge for
visual understanding.

</details>


### [56] [Moment Sampling in Video LLMs for Long-Form Video QA](https://arxiv.org/abs/2507.00033)
*Mustafa Chasmai,Gauri Jagatap,Gouthaman KV,Grant Van Horn,Subhransu Maji,Andrea Fanelli*

Main category: cs.CV

Relevance: 70.0

TL;DR: 提出了一种名为“moment sampling”的新方法，通过文本到视频时刻检索模型指导帧采样，提升长视频问答性能。


<details>
  <summary>Details</summary>
Motivation: 现有视频大语言模型在长视频推理中表现不佳，传统帧采样方法易丢失关键帧或包含冗余信息。

Method: 采用轻量级时刻检索模型，根据问题上下文选择最相关帧。

Result: 在四个长视频问答数据集和四种先进视频大语言模型上验证了方法的有效性。

Conclusion: “moment sampling”显著提升了长视频问答的性能，减少了冗余计算。

Abstract: Recent advancements in video large language models (Video LLMs) have
significantly advanced the field of video question answering (VideoQA). While
existing methods perform well on short videos, they often struggle with
long-range reasoning in longer videos. To scale Video LLMs for longer video
content, frame sub-sampling (selecting frames at regular intervals) is commonly
used. However, this approach is suboptimal, often leading to the loss of
crucial frames or the inclusion of redundant information from multiple similar
frames. Missing key frames impairs the model's ability to answer questions
accurately, while redundant frames lead the model to focus on irrelevant video
segments and increase computational resource consumption. In this paper, we
investigate the use of a general-purpose text-to-video moment retrieval model
to guide the frame sampling process. We propose "moment sampling", a novel,
model-agnostic approach that enables the model to select the most relevant
frames according to the context of the question. Specifically, we employ a
lightweight moment retrieval model to prioritize frame selection. By focusing
on the frames most pertinent to the given question, our method enhances
long-form VideoQA performance in Video LLMs. Through extensive experiments on
four long-form VideoQA datasets, using four state-of-the-art Video LLMs, we
demonstrate the effectiveness of the proposed approach.

</details>


### [57] [VSF-Med:A Vulnerability Scoring Framework for Medical Vision-Language Models](https://arxiv.org/abs/2507.00052)
*Binesh Sadanandan,Vahid Behzadan*

Main category: cs.CV

Relevance: 70.0

TL;DR: VSF-Med是一个端到端的医疗视觉语言模型（VLM）漏洞评分框架，包含文本提示攻击模板、视觉扰动和八维评分标准，用于系统评估医疗VLM的安全性。


<details>
  <summary>Details</summary>
Motivation: 医疗VLM在临床应用中缺乏系统性的安全评估，VSF-Med旨在填补这一空白，提供可复现的基准测试。

Method: VSF-Med结合了文本攻击模板、视觉扰动和LLM评分，生成30,000多个对抗样本，并通过z-score标准化计算综合风险指标。

Result: 实验显示，主流VLM在攻击持续性、提示注入和安全性绕过方面存在显著漏洞，如Llama-3.2-11B-Vision-Instruct的攻击持续性漏洞增加1.29σ。

Conclusion: VSF-Med为医疗VLM的安全性提供了标准化评估工具，揭示了现有模型的潜在风险。

Abstract: Vision Language Models (VLMs) hold great promise for streamlining
labour-intensive medical imaging workflows, yet systematic security evaluations
in clinical settings remain scarce. We introduce VSF--Med, an end-to-end
vulnerability-scoring framework for medical VLMs that unites three novel
components: (i) a rich library of sophisticated text-prompt attack templates
targeting emerging threat vectors; (ii) imperceptible visual perturbations
calibrated by structural similarity (SSIM) thresholds to preserve clinical
realism; and (iii) an eight-dimensional rubric evaluated by two independent
judge LLMs, whose raw scores are consolidated via z-score normalization to
yield a 0--32 composite risk metric. Built entirely on publicly available
datasets and accompanied by open-source code, VSF--Med synthesizes over 30,000
adversarial variants from 5,000 radiology images and enables reproducible
benchmarking of any medical VLM with a single command. Our consolidated
analysis reports mean z-score shifts of $0.90\sigma$ for
persistence-of-attack-effects, $0.74\sigma$ for prompt-injection effectiveness,
and $0.63\sigma$ for safety-bypass success across state-of-the-art VLMs.
Notably, Llama-3.2-11B-Vision-Instruct exhibits a peak vulnerability increase
of $1.29\sigma$ for persistence-of-attack-effects, while GPT-4o shows increases
of $0.69\sigma$ for that same vector and $0.28\sigma$ for prompt-injection
attacks.

</details>


### [58] [Out-of-Distribution Detection with Adaptive Top-K Logits Integration](https://arxiv.org/abs/2507.00368)
*Hikaru Shijo,Yutaka Yoshihama,Kenichi Yadani,Norifumi Murata*

Main category: cs.CV

Relevance: 70.0

TL;DR: 论文提出了一种名为ATLI的新方法，用于检测分布外（OOD）数据，通过自适应选择有效的top-k logits并结合最大logit，显著降低了误报率。


<details>
  <summary>Details</summary>
Motivation: 神经网络对分布外数据容易产生过度自信的预测，影响模型安全性。现有方法MaxLogit仅使用最大logit，作者发现其他logits也有助于OOD检测。

Method: 提出ATLI方法，自适应选择模型特定的有效top-k logits，并将其与最大logit结合，用于OOD检测。

Result: 在ImageNet-1K基准测试中，ATLI将FPR95降低了6.73%（相比MaxLogit）和2.67%（相比其他SOTA方法）。

Conclusion: ATLI通过利用更多logit信息，显著提升了OOD检测性能，为模型安全性提供了新思路。

Abstract: Neural networks often make overconfident predictions from out-of-distribution
(OOD) samples. Detection of OOD data is therefore crucial to improve the safety
of machine learning. The simplest and most powerful method for OOD detection is
MaxLogit, which uses the model's maximum logit to provide an OOD score. We have
discovered that, in addition to the maximum logit, some other logits are also
useful for OOD detection. Based on this finding, we propose a new method called
ATLI (Adaptive Top-k Logits Integration), which adaptively determines effective
top-k logits that are specific to each model and combines the maximum logit
with the other top-k logits. In this study we evaluate our proposed method
using ImageNet-1K benchmark. Extensive experiments showed our proposed method
to reduce the false positive rate (FPR95) by 6.73% compared to the MaxLogit
approach, and decreased FPR95 by an additional 2.67% compared to other
state-of-the-art methods.

</details>


### [59] [Bisecle: Binding and Separation in Continual Learning for Video Language Understanding](https://arxiv.org/abs/2507.00469)
*Yue Tan,Xiaoqian Hu,Hao Xue,Celso De Melo,Flora D. Salim*

Main category: cs.CV

Relevance: 70.0

TL;DR: 论文提出Bisecle方法，用于视频语言持续学习，通过多方向监督模块和对比提示学习方案解决灾难性遗忘和更新冲突问题。


<details>
  <summary>Details</summary>
Motivation: 现实世界视频是持续演化的数据流，现有持续学习框架在大规模多模态基础模型中面临灾难性遗忘和更新冲突的挑战。

Method: 受海马体快速绑定和模式分离机制启发，提出多方向监督模块和对比提示学习方案。

Result: 在多个VideoQA基准测试中，Bisecle有效缓解遗忘并增强跨任务泛化能力。

Conclusion: Bisecle通过结合绑定和分离机制，提升了视频理解任务中的持续学习能力。

Abstract: Frontier vision-language models (VLMs) have made remarkable improvements in
video understanding tasks. However, real-world videos typically exist as
continuously evolving data streams (e.g., dynamic scenes captured by wearable
glasses), necessitating models to continually adapt to shifting data
distributions and novel scenarios. Considering the prohibitive computational
costs of fine-tuning models on new tasks, usually, a small subset of parameters
is updated while the bulk of the model remains frozen. This poses new
challenges to existing continual learning frameworks in the context of large
multimodal foundation models, i.e., catastrophic forgetting and update
conflict. While the foundation models struggle with parameter-efficient
continual learning, the hippocampus in the human brain has evolved highly
efficient mechanisms for memory formation and consolidation. Inspired by the
rapid Binding and pattern separation mechanisms in the hippocampus, in this
work, we propose Bisecle for video-language continual learning, where a
multi-directional supervision module is used to capture more cross-modal
relationships and a contrastive prompt learning scheme is designed to isolate
task-specific knowledge to facilitate efficient memory storage. Binding and
separation processes further strengthen the ability of VLMs to retain complex
experiences, enabling robust and efficient continual learning in video
understanding tasks. We perform a thorough evaluation of the proposed Bisecle,
demonstrating its ability to mitigate forgetting and enhance cross-task
generalization on several VideoQA benchmarks.

</details>


### [60] [ONLY: One-Layer Intervention Sufficiently Mitigates Hallucinations in Large Vision-Language Models](https://arxiv.org/abs/2507.00898)
*Zifu Wan,Ce Zhang,Silong Yong,Martin Q. Ma,Simon Stepputtis,Louis-Philippe Morency,Deva Ramanan,Katia Sycara,Yaqi Xie*

Main category: cs.CV

Relevance: 70.0

TL;DR: 论文提出了一种名为ONLY的训练免费解码方法，通过单次查询和单层干预减少视觉语言模型（LVLM）的幻觉问题，提升实时部署效率。


<details>
  <summary>Details</summary>
Motivation: 解决现有LVLM在多模态任务中存在的幻觉问题，同时避免现有对比解码方法因多次查询导致的实时性不足。

Method: 提出ONLY方法，利用文本到视觉熵比选择性放大关键文本信息，仅需单次查询和单层干预。

Result: 在多个基准测试中，ONLY方法表现优于现有技术，且实现简单、计算成本低。

Conclusion: ONLY是一种高效、低成本的解决方案，适用于实时应用中的LVLM解码。

Abstract: Recent Large Vision-Language Models (LVLMs) have introduced a new paradigm
for understanding and reasoning about image input through textual responses.
Although they have achieved remarkable performance across a range of
multi-modal tasks, they face the persistent challenge of hallucination, which
introduces practical weaknesses and raises concerns about their reliable
deployment in real-world applications. Existing work has explored contrastive
decoding approaches to mitigate this issue, where the output of the original
LVLM is compared and contrasted with that of a perturbed version. However,
these methods require two or more queries that slow down LVLM response
generation, making them less suitable for real-time applications. To overcome
this limitation, we propose ONLY, a training-free decoding approach that
requires only a single query and a one-layer intervention during decoding,
enabling efficient real-time deployment. Specifically, we enhance textual
outputs by selectively amplifying crucial textual information using a
text-to-visual entropy ratio for each token. Extensive experimental results
demonstrate that our proposed ONLY consistently outperforms state-of-the-art
methods across various benchmarks while requiring minimal implementation effort
and computational cost. Code is available at https://github.com/zifuwan/ONLY.

</details>


### [61] [CAVALRY-V: A Large-Scale Generator Framework for Adversarial Attacks on Video MLLMs](https://arxiv.org/abs/2507.00817)
*Jiaming Zhang,Rui Hu,Qing Guo,Wei Yang Bryan Lim*

Main category: cs.CV

Relevance: 70.0

TL;DR: CAVALRY-V是一种针对视频多模态大语言模型（V-MLLMs）的对抗攻击框架，通过双目标语义-视觉损失函数和高效的两阶段生成器，显著提升了攻击效果。


<details>
  <summary>Details</summary>
Motivation: 探索V-MLLMs在对抗攻击下的脆弱性，填补现有研究的空白。

Method: 提出双目标语义-视觉损失函数和两阶段生成器框架，结合预训练和微调。

Result: 在视频理解基准测试中，CAVALRY-V显著优于现有攻击方法，平均提升22.8%。

Conclusion: CAVALRY-V为多模态系统的对抗研究提供了基础性方法。

Abstract: Video Multimodal Large Language Models (V-MLLMs) have shown impressive
capabilities in temporal reasoning and cross-modal understanding, yet their
vulnerability to adversarial attacks remains underexplored due to unique
challenges: complex cross-modal reasoning mechanisms, temporal dependencies,
and computational constraints. We present CAVALRY-V (Cross-modal
Language-Vision Adversarial Yielding for Videos), a novel framework that
directly targets the critical interface between visual perception and language
generation in V-MLLMs. Our approach introduces two key innovations: (1) a
dual-objective semantic-visual loss function that simultaneously disrupts the
model's text generation logits and visual representations to undermine
cross-modal integration, and (2) a computationally efficient two-stage
generator framework that combines large-scale pre-training for cross-model
transferability with specialized fine-tuning for spatiotemporal coherence.
Empirical evaluation on comprehensive video understanding benchmarks
demonstrates that CAVALRY-V significantly outperforms existing attack methods,
achieving 22.8% average improvement over the best baseline attacks on both
commercial systems (GPT-4.1, Gemini 2.0) and open-source models (QwenVL-2.5,
InternVL-2.5, Llava-Video, Aria, MiniCPM-o-2.6). Our framework achieves
flexibility through implicit temporal coherence modeling rather than explicit
regularization, enabling significant performance improvements even on image
understanding (34.4% average gain). This capability demonstrates CAVALRY-V's
potential as a foundational approach for adversarial research across multimodal
systems.

</details>


### [62] [Is Visual in-Context Learning for Compositional Medical Tasks within Reach?](https://arxiv.org/abs/2507.00868)
*Simon Reiß,Zdravko Marinov,Alexander Jaus,Constantin Seibold,M. Saquib Sarfraz,Erik Rodner,Rainer Stiefelhagen*

Main category: cs.CV

Relevance: 65.0

TL;DR: 探索视觉上下文学习的潜力，使单一模型能处理多任务并在测试时适应新任务，无需重新训练。


<details>
  <summary>Details</summary>
Motivation: 研究如何通过上下文学习使模型适应任务序列而非单个任务，以解决复杂多步任务。

Method: 分析视觉上下文学习架构的局限，提出合成组合任务生成引擎，研究不同掩码训练目标。

Result: 为多模态医疗任务序列提供重要见解，并指出需解决的挑战。

Conclusion: 视觉上下文学习在复杂任务中具有潜力，但仍需进一步研究。

Abstract: In this paper, we explore the potential of visual in-context learning to
enable a single model to handle multiple tasks and adapt to new tasks during
test time without re-training. Unlike previous approaches, our focus is on
training in-context learners to adapt to sequences of tasks, rather than
individual tasks. Our goal is to solve complex tasks that involve multiple
intermediate steps using a single model, allowing users to define entire vision
pipelines flexibly at test time. To achieve this, we first examine the
properties and limitations of visual in-context learning architectures, with a
particular focus on the role of codebooks. We then introduce a novel method for
training in-context learners using a synthetic compositional task generation
engine. This engine bootstraps task sequences from arbitrary segmentation
datasets, enabling the training of visual in-context learners for compositional
tasks. Additionally, we investigate different masking-based training objectives
to gather insights into how to train models better for solving complex,
compositional tasks. Our exploration not only provides important insights
especially for multi-modal medical task sequences but also highlights
challenges that need to be addressed.

</details>


### [63] [AdaDeDup: Adaptive Hybrid Data Pruning for Efficient Large-Scale Object Detection Training](https://arxiv.org/abs/2507.00049)
*Feiyang Kang,Nadine Chang,Maying Shen,Marc T. Law,Rafid Mahmood,Ruoxi Jia,Jose M. Alvarez*

Main category: cs.CV

Relevance: 60.0

TL;DR: AdaDeDup是一种混合框架，结合密度修剪和模型反馈，自适应地优化数据修剪，显著提升大规模模型训练的数据效率。


<details>
  <summary>Details</summary>
Motivation: 解决大规模数据集的计算负担和冗余问题，现有方法在任务相关性和计算效率上存在不足。

Method: AdaDeDup通过密度修剪和模型反馈自适应调整修剪阈值，保留关键数据并减少冗余。

Result: 在Waymo等基准测试中，AdaDeDup显著优于基线方法，减少性能下降，并在修剪20%数据时接近原始性能。

Conclusion: AdaDeDup有效提升数据效率，适用于大规模模型训练。

Abstract: The computational burden and inherent redundancy of large-scale datasets
challenge the training of contemporary machine learning models. Data pruning
offers a solution by selecting smaller, informative subsets, yet existing
methods struggle: density-based approaches can be task-agnostic, while
model-based techniques may introduce redundancy or prove computationally
prohibitive. We introduce Adaptive De-Duplication (AdaDeDup), a novel hybrid
framework that synergistically integrates density-based pruning with
model-informed feedback in a cluster-adaptive manner. AdaDeDup first partitions
data and applies an initial density-based pruning. It then employs a proxy
model to evaluate the impact of this initial pruning within each cluster by
comparing losses on kept versus pruned samples. This task-aware signal
adaptively adjusts cluster-specific pruning thresholds, enabling more
aggressive pruning in redundant clusters while preserving critical data in
informative ones. Extensive experiments on large-scale object detection
benchmarks (Waymo, COCO, nuScenes) using standard models (BEVFormer, Faster
R-CNN) demonstrate AdaDeDup's advantages. It significantly outperforms
prominent baselines, substantially reduces performance degradation (e.g., over
54% versus random sampling on Waymo), and achieves near-original model
performance while pruning 20% of data, highlighting its efficacy in enhancing
data efficiency for large-scale model training. Code is open-sourced.

</details>


### [64] [MANTA: Cross-Modal Semantic Alignment and Information-Theoretic Optimization for Long-form Multimodal Understanding](https://arxiv.org/abs/2507.00068)
*Ziqi Zhong,Daniel Tang*

Main category: cs.CV

Relevance: 60.0

TL;DR: MANTA是一个多模态学习框架，通过文本对齐统一视觉和听觉输入，优化语义对齐、时间同步、内容表示和信息检索，显著提升长视频问答和跨模态理解任务性能。


<details>
  <summary>Details</summary>
Motivation: 当前多模态学习方法通常将模态分开处理，导致表示和推理不一致。MANTA旨在通过文本对齐统一多模态输入，解决语义对齐、时间同步、多尺度理解和稀疏信息检索等挑战。

Method: MANTA采用信息论优化实现跨模态语义对齐，自适应时间同步处理不同信息密度，分层内容表示支持多尺度理解，上下文感知检索稀疏信息。通过数学框架证明其在标记约束下的最优性。

Result: 在长视频问答任务中，MANTA将最先进模型的准确率提升22.6%，对超过30分钟的视频提升27.3%。在时间推理和跨模态理解任务中分别提升23.8%和25.1%。

Conclusion: MANTA通过新颖的密度估计技术最小化冗余并保留稀有信号，为多模态表示的统一提供了新基础。

Abstract: While multi-modal learning has advanced significantly, current approaches
often treat modalities separately, creating inconsistencies in representation
and reasoning. We introduce MANTA (Multi-modal Abstraction and Normalization
via Textual Alignment), a theoretically-grounded framework that unifies visual
and auditory inputs into a structured textual space for seamless processing
with large language models. MANTA addresses four key challenges: (1) semantic
alignment across modalities with information-theoretic optimization, (2)
adaptive temporal synchronization for varying information densities, (3)
hierarchical content representation for multi-scale understanding, and (4)
context-aware retrieval of sparse information from long sequences. We formalize
our approach within a rigorous mathematical framework, proving its optimality
for context selection under token constraints. Extensive experiments on the
challenging task of Long Video Question Answering show that MANTA improves
state-of-the-art models by up to 22.6% in overall accuracy, with particularly
significant gains (27.3%) on videos exceeding 30 minutes. Additionally, we
demonstrate MANTA's superiority on temporal reasoning tasks (23.8% improvement)
and cross-modal understanding (25.1% improvement). Our framework introduces
novel density estimation techniques for redundancy minimization while
preserving rare signals, establishing new foundations for unifying multimodal
representations through structured text.

</details>


### [65] [Populate-A-Scene: Affordance-Aware Human Video Generation](https://arxiv.org/abs/2507.00334)
*Mengyi Shan,Zecheng He,Haoyu Ma,Felix Juefei-Xu,Peizhao Zhang,Tingbo Hou,Ching-Yao Chuang*

Main category: cs.CV

Relevance: 60.0

TL;DR: 论文探讨了如何将视频生成模型重新用作交互式世界模拟器，通过微调模型使其能够根据场景图像和动作提示预测人与环境的交互。


<details>
  <summary>Details</summary>
Motivation: 研究动机在于探索文本到视频模型的潜在能力，尤其是其在感知场景可操作性方面的潜力，而无需依赖显式的条件（如边界框或人体姿态）。

Method: 方法包括微调预训练的视频生成模型，使其能够根据单张场景图像和动作提示插入人物并确保行为、外观和场景的协调性。通过分析交叉注意力热图，揭示了模型固有的可操作性感知能力。

Result: 结果表明，模型能够在不使用标记可操作性数据集的情况下，成功预测并生成符合场景可操作性的人类行为视频。

Conclusion: 结论指出，预训练的视频生成模型具有潜在的可操作性感知能力，可以用于交互式世界模拟，且无需依赖显式条件或标记数据。

Abstract: Can a video generation model be repurposed as an interactive world simulator?
We explore the affordance perception potential of text-to-video models by
teaching them to predict human-environment interaction. Given a scene image and
a prompt describing human actions, we fine-tune the model to insert a person
into the scene, while ensuring coherent behavior, appearance, harmonization,
and scene affordance. Unlike prior work, we infer human affordance for video
generation (i.e., where to insert a person and how they should behave) from a
single scene image, without explicit conditions like bounding boxes or body
poses. An in-depth study of cross-attention heatmaps demonstrates that we can
uncover the inherent affordance perception of a pre-trained video model without
labeled affordance datasets.

</details>


### [66] [Unleashing the Potential of All Test Samples: Mean-Shift Guided Test-Time Adaptation](https://arxiv.org/abs/2507.00462)
*Jizhou Han,Chenhao Ding,SongLin Dong,Yuhang He,Xinyuan Gao,Yihong Gong*

Main category: cs.CV

Relevance: 60.0

TL;DR: MS-TTA是一种无需训练的测试时适应方法，通过kNN Mean-Shift增强CLIP特征表示，提升分布外泛化能力。


<details>
  <summary>Details</summary>
Motivation: 解决视觉语言模型（如CLIP）在测试时分布偏移问题，现有方法仅依赖高置信度样本，忽略了低置信度样本的潜力。

Method: 提出MS-TTA，利用单步kNN Mean-Shift在CLIP特征空间外优化所有测试样本的特征表示，增强特征紧凑性和类别可分性。

Result: 在OOD和跨数据集基准测试中，MS-TTA表现优于现有训练无关的TTA方法，实现稳定适应。

Conclusion: MS-TTA无需额外训练即可显著提升CLIP在分布偏移下的性能。

Abstract: Visual-language models (VLMs) like CLIP exhibit strong generalization but
struggle with distribution shifts at test time. Existing training-free
test-time adaptation (TTA) methods operate strictly within CLIP's original
feature space, relying on high-confidence samples while overlooking the
potential of low-confidence ones. We propose MS-TTA, a training-free approach
that enhances feature representations beyond CLIP's space using a single-step
k-nearest neighbors (kNN) Mean-Shift. By refining all test samples, MS-TTA
improves feature compactness and class separability, leading to more stable
adaptation. Additionally, a cache of refined embeddings further enhances
inference by providing Mean Shift enhanced logits. Extensive evaluations on OOD
and cross-dataset benchmarks demonstrate that MS-TTA consistently outperforms
state-of-the-art training-free TTA methods, achieving robust adaptation without
requiring additional training.

</details>


### [67] [Visual Anagrams Reveal Hidden Differences in Holistic Shape Processing Across Vision Models](https://arxiv.org/abs/2507.00493)
*Fenil R. Doshi,Thomas Fel,Talia Konkle,George Alvarez*

Main category: cs.CV

Relevance: 60.0

TL;DR: 论文提出了一种新的形状评估方法Configural Shape Score (CSS)，用于衡量模型对全局配置形状的敏感性，发现自监督和语言对齐的Transformer模型表现最佳。


<details>
  <summary>Details</summary>
Motivation: 当前视觉模型主要依赖局部纹理线索，导致特征脆弱且非组合性。研究旨在评估模型对全局配置形状的敏感性，而非将形状与纹理对立。

Method: 通过Object-Anagram对测量模型的Configural Shape Score (CSS)，并分析不同模型的表现，包括卷积、Transformer和混合模型。

Result: 自监督和语言对齐的Transformer模型（如DINOv2、SigLIP2和EVA-CLIP）在CSS中表现最佳，依赖长程交互。

Conclusion: 真正鲁棒且类人的视觉系统需整合局部纹理和全局配置形状，而非在二者间做人为选择。

Abstract: Humans are able to recognize objects based on both local texture cues and the
configuration of object parts, yet contemporary vision models primarily harvest
local texture cues, yielding brittle, non-compositional features. Work on
shape-vs-texture bias has pitted shape and texture representations in
opposition, measuring shape relative to texture, ignoring the possibility that
models (and humans) can simultaneously rely on both types of cues, and
obscuring the absolute quality of both types of representation. We therefore
recast shape evaluation as a matter of absolute configural competence,
operationalized by the Configural Shape Score (CSS), which (i) measures the
ability to recognize both images in Object-Anagram pairs that preserve local
texture while permuting global part arrangement to depict different object
categories. Across 86 convolutional, transformer, and hybrid models, CSS (ii)
uncovers a broad spectrum of configural sensitivity with fully self-supervised
and language-aligned transformers -- exemplified by DINOv2, SigLIP2 and
EVA-CLIP -- occupying the top end of the CSS spectrum. Mechanistic probes
reveal that (iii) high-CSS networks depend on long-range interactions:
radius-controlled attention masks abolish performance showing a distinctive
U-shaped integration profile, and representational-similarity analyses expose a
mid-depth transition from local to global coding. A BagNet control remains at
chance (iv), ruling out "border-hacking" strategies. Finally, (v) we show that
configural shape score also predicts other shape-dependent evals. Overall, we
propose that the path toward truly robust, generalizable, and human-like vision
systems may not lie in forcing an artificial choice between shape and texture,
but rather in architectural and learning frameworks that seamlessly integrate
both local-texture and global configural shape.

</details>


### [68] [Laplace-Mamba: Laplace Frequency Prior-Guided Mamba-CNN Fusion Network for Image Dehazing](https://arxiv.org/abs/2507.00501)
*Yongzhen Wang,Liangliang Chen,Bingwen Hu,Heng Liu,Xiao-Ping Zhang,Mingqiang Wei*

Main category: cs.CV

Relevance: 60.0

TL;DR: 论文提出Laplace-Mamba框架，结合Laplace频率先验与Mamba-CNN混合架构，用于高效图像去雾。通过分解图像为低频和高频分量，分别处理全局和局部细节，显著提升恢复质量和效率。


<details>
  <summary>Details</summary>
Motivation: 解决基于SSM的方法在重建局部结构和处理高维数据时的局限性，提出更高效的图像去雾方法。

Method: 结合Laplace分解与Mamba-CNN混合架构，低频用SSM建模全局，高频用CNN优化局部细节。

Result: 在多个基准测试中，方法在恢复质量和效率上均优于现有技术。

Conclusion: Laplace-Mamba框架有效解决了SSM的局限性，提升了图像去雾性能。

Abstract: Recent progress in image restoration has underscored Spatial State Models
(SSMs) as powerful tools for modeling long-range dependencies, owing to their
appealing linear complexity and computational efficiency. However, SSM-based
approaches exhibit limitations in reconstructing localized structures and tend
to be less effective when handling high-dimensional data, frequently resulting
in suboptimal recovery of fine image features. To tackle these challenges, we
introduce Laplace-Mamba, a novel framework that integrates Laplace frequency
prior with a hybrid Mamba-CNN architecture for efficient image dehazing.
Leveraging the Laplace decomposition, the image is disentangled into
low-frequency components capturing global texture and high-frequency components
representing edges and fine details. This decomposition enables specialized
processing via dual parallel pathways: the low-frequency branch employs SSMs
for global context modeling, while the high-frequency branch utilizes CNNs to
refine local structural details, effectively addressing diverse haze scenarios.
Notably, the Laplace transformation facilitates information-preserving
downsampling of low-frequency components in accordance with the Nyquist theory,
thereby significantly improving computational efficiency. Extensive evaluations
across multiple benchmarks demonstrate that our method outperforms
state-of-the-art approaches in both restoration quality and efficiency. The
source code and pretrained models are available at
https://github.com/yz-wang/Laplace-Mamba.

</details>


### [69] [Out-of-distribution detection in 3D applications: a review](https://arxiv.org/abs/2507.00570)
*Zizhao Li,Xueyang Kang,Joseph West,Kourosh Khoshelham*

Main category: cs.CV

Relevance: 60.0

TL;DR: 本文全面综述了OOD检测在可信赖AI中的重要性，涵盖用例、数据集、评估方法、方法论比较及未来研究方向。


<details>
  <summary>Details</summary>
Motivation: 解决现实世界中训练数据未涵盖的物体识别问题，提升AI系统的可靠性和安全性。

Method: 综述了OOD检测的方法论，包括模型结构、不确定性指标、分布距离分类和校准技术。

Result: 提供了理论和实践见解，展示了3D视觉集成等新兴研究方向。

Conclusion: OOD检测是可信赖AI的关键方向，未来研究需关注对抗鲁棒性和失败识别。

Abstract: The ability to detect objects that are not prevalent in the training set is a
critical capability in many 3D applications, including autonomous driving.
Machine learning methods for object recognition often assume that all object
categories encountered during inference belong to a closed set of classes
present in the training data. This assumption limits generalization to the real
world, as objects not seen during training may be misclassified or entirely
ignored. As part of reliable AI, OOD detection identifies inputs that deviate
significantly from the training distribution. This paper provides a
comprehensive overview of OOD detection within the broader scope of trustworthy
and uncertain AI. We begin with key use cases across diverse domains, introduce
benchmark datasets spanning multiple modalities, and discuss evaluation
metrics. Next, we present a comparative analysis of OOD detection
methodologies, exploring model structures, uncertainty indicators, and
distributional distance taxonomies, alongside uncertainty calibration
techniques. Finally, we highlight promising research directions, including
adversarially robust OOD detection and failure identification, particularly
relevant to 3D applications. The paper offers both theoretical and practical
insights into OOD detection, showcasing emerging research opportunities such as
3D vision integration. These insights help new researchers navigate the field
more effectively, contributing to the development of reliable, safe, and robust
AI systems.

</details>


### [70] [UPRE: Zero-Shot Domain Adaptation for Object Detection via Unified Prompt and Representation Enhancement](https://arxiv.org/abs/2507.00721)
*Xiao Zhang,Fei Wei,Yong Wang,Wenda Zhao,Feiyi Li,Xiangxiang Chu*

Main category: cs.CV

Relevance: 60.0

TL;DR: 论文提出UPRE框架，通过联合优化文本提示和视觉表示，解决零样本领域适应中的任务与视觉语言模型不对齐问题。


<details>
  <summary>Details</summary>
Motivation: 零样本领域适应（ZSDA）因目标领域缺乏图像而具有挑战性，现有方法依赖手工提示且忽视任务与模型的对齐问题。

Method: UPRE框架结合多视图领域提示和视觉表示增强模块，并采用多级增强策略（如相对领域距离和正负分离）。

Result: 在九个基准数据集上的实验表明，UPRE在ZSDA检测场景中表现优异。

Conclusion: UPRE通过联合优化提示和表示，显著提升了零样本领域适应的性能。

Abstract: Zero-shot domain adaptation (ZSDA) presents substantial challenges due to the
lack of images in the target domain. Previous approaches leverage
Vision-Language Models (VLMs) to tackle this challenge, exploiting their
zero-shot learning capabilities. However, these methods primarily address
domain distribution shifts and overlook the misalignment between the detection
task and VLMs, which rely on manually crafted prompts. To overcome these
limitations, we propose the unified prompt and representation enhancement
(UPRE) framework, which jointly optimizes both textual prompts and visual
representations. Specifically, our approach introduces a multi-view domain
prompt that combines linguistic domain priors with detection-specific
knowledge, and a visual representation enhancement module that produces domain
style variations. Furthermore, we introduce multi-level enhancement strategies,
including relative domain distance and positive-negative separation, which
align multi-modal representations at the image level and capture diverse visual
representations at the instance level, respectively. Extensive experiments
conducted on nine benchmark datasets demonstrate the superior performance of
our framework in ZSDA detection scenarios. Code is available at
https://github.com/AMAP-ML/UPRE.

</details>


### [71] [Holmes: Towards Effective and Harmless Model Ownership Verification to Personalized Large Vision Models via Decoupling Common Features](https://arxiv.org/abs/2507.00724)
*Linghui Zhu,Yiming Li,Haiqin Weng,Yan Liu,Tianwei Zhang,Shu-Tao Xia,Zhi Wang*

Main category: cs.CV

Relevance: 60.0

TL;DR: 论文提出了一种针对个性化视觉模型的无害所有权验证方法，通过解耦相似特征来防御模型窃取攻击。


<details>
  <summary>Details</summary>
Motivation: 现有防御方法对微调模型效果不佳或引入新风险，需开发更有效且安全的验证方法。

Method: 分三阶段：创建保留共同特征的影子模型，训练元分类器识别特定特征，通过假设检验验证所有权。

Result: 在基准数据集上验证了方法能有效检测多种模型窃取类型。

Conclusion: 该方法为个性化模型提供了一种安全且高效的所有权验证方案。

Abstract: Large vision models achieve remarkable performance in various downstream
tasks, primarily by personalizing pre-trained models through fine-tuning with
private and valuable local data, which makes the personalized model a valuable
intellectual property for its owner. Similar to the era of traditional DNNs,
model stealing attacks also pose significant risks to these personalized
models. However, in this paper, we reveal that most existing defense methods
(developed for traditional DNNs), typically designed for models trained from
scratch, either introduce additional security risks, are prone to misjudgment,
or are even ineffective for fine-tuned models. To alleviate these problems,
this paper proposes a harmless model ownership verification method for
personalized models by decoupling similar common features. In general, our
method consists of three main stages. In the first stage, we create shadow
models that retain common features of the victim model while disrupting
dataset-specific features. We represent the dataset-specific features of the
victim model by the output differences between the shadow and victim models.
After that, a meta-classifier is trained to identify stolen models by
determining whether suspicious models contain the dataset-specific features of
the victim. In the third stage, we conduct model ownership verification by
hypothesis test to mitigate randomness and enhance robustness. Extensive
experiments on benchmark datasets verify the effectiveness of the proposed
method in detecting different types of model stealing simultaneously.

</details>


### [72] [OptiPrune: Boosting Prompt-Image Consistency with Attention-Guided Noise and Dynamic Token Selection](https://arxiv.org/abs/2507.00789)
*Ziji Lu*

Main category: cs.CV

Relevance: 60.0

TL;DR: OptiPrune是一个统一框架，通过分布感知的初始噪声优化和基于相似性的token剪枝，同时解决文本到图像扩散模型中的语义对齐和效率问题。


<details>
  <summary>Details</summary>
Motivation: 现有方法在文本到图像扩散模型中要么计算开销大，要么语义保真度低，OptiPrune旨在同时解决这两个问题。

Method: 1) 基于注意力分数的分布感知噪声优化模块；2) 硬件高效的token剪枝策略，包括相似性选择和随机性注入。

Result: 在Animal-Animal等基准数据集上，OptiPrune实现了最先进的提示-图像一致性，并显著降低计算成本。

Conclusion: OptiPrune在保持语义对齐的同时提高了效率，适用于资源受限的硬件。

Abstract: Text-to-image diffusion models often struggle to achieve accurate semantic
alignment between generated images and text prompts while maintaining
efficiency for deployment on resource-constrained hardware. Existing approaches
either incur substantial computational overhead through noise optimization or
compromise semantic fidelity by aggressively pruning tokens. In this work, we
propose OptiPrune, a unified framework that combines distribution-aware initial
noise optimization with similarity-based token pruning to address both
challenges simultaneously. Specifically, (1) we introduce a distribution-aware
noise optimization module guided by attention scores to steer the initial
latent noise toward semantically meaningful regions, mitigating issues such as
subject neglect and feature entanglement; (2) we design a hardware-efficient
token pruning strategy that selects representative base tokens via patch-wise
similarity, injects randomness to enhance generalization, and recovers pruned
tokens using maximum similarity copying before attention operations. Our method
preserves the Gaussian prior during noise optimization and enables efficient
inference without sacrificing alignment quality. Experiments on benchmark
datasets, including Animal-Animal, demonstrate that OptiPrune achieves
state-of-the-art prompt-image consistency with significantly reduced
computational cost.

</details>


### [73] [GLM-4.1V-Thinking: Towards Versatile Multimodal Reasoning with Scalable Reinforcement Learning](https://arxiv.org/abs/2507.01006)
*Wenyi Hong,Wenmeng Yu,Xiaotao Gu,Guo Wang,Guobing Gan,Haomiao Tang,Jiale Cheng,Ji Qi,Junhui Ji,Lihang Pan,Shuaiqi Duan,Weihan Wang,Yan Wang,Yean Cheng,Zehai He,Zhe Su,Zhen Yang,Ziyang Pan,Aohan Zeng,Baoxu Wang,Boyan Shi,Changyu Pang,Chenhui Zhang,Da Yin,Fan Yang,Guoqing Chen,Jiazheng Xu,Jiali Chen,Jing Chen,Jinhao Chen,Jinghao Lin,Jinjiang Wang,Junjie Chen,Leqi Lei,Leyi Pan,Mingzhi Zhang,Qinkai Zheng,Sheng Yang,Shi Zhong,Shiyu Huang,Shuyuan Zhao,Siyan Xue,Shangqin Tu,Shengbiao Meng,Tianshu Zhang,Tianwei Luo,Tianxiang Hao,Tianle Gong,Wenkai Li,Wei Jia,Xin Lyu,Xuancheng Huang,Yanling Wang,Yadong Xue,Yanfeng Wang,Yifan An,Yifan Du,Yiming Shi,Yiheng Huang,Yilin Niu,Yuan Wang,Yuanchang Yue,Yuchen Li,Yutao Zhang,Yuxuan Zhang,Zhanxiao Du,Zhenyu Hou,Zhao Xue,Zhengxiao Du,Zihan Wang,Peng Zhang,Debing Liu,Bin Xu,Juanzi Li,Minlie Huang,Yuxiao Dong,Jie Tang*

Main category: cs.CV

Relevance: 60.0

TL;DR: GLM-4.1V-Thinking是一个视觉语言模型（VLM），通过强化学习与课程采样（RLCS）提升多模态推理能力，在多个任务上表现优异，甚至超越更大规模的模型。


<details>
  <summary>Details</summary>
Motivation: 推动通用多模态推理的发展，通过大规模预训练和强化学习优化模型性能。

Method: 结合大规模预训练和RLCS框架，提升模型在STEM、视频理解、编码等任务上的能力。

Result: 在28个公共基准测试中表现优异，超越同类规模模型，部分任务甚至优于更大规模或闭源模型。

Conclusion: GLM-4.1V-Thinking展示了强大的多模态推理能力，为研究提供了开源资源。

Abstract: We present GLM-4.1V-Thinking, a vision-language model (VLM) designed to
advance general-purpose multimodal reasoning. In this report, we share our key
findings in the development of the reasoning-centric training framework. We
first develop a capable vision foundation model with significant potential
through large-scale pre-training, which arguably sets the upper bound for the
final performance. Reinforcement Learning with Curriculum Sampling (RLCS) then
unlocks the full potential of the model, leading to comprehensive capability
enhancement across a diverse range of tasks, including STEM problem solving,
video understanding, content recognition, coding, grounding, GUI-based agents,
and long document understanding, among others. To facilitate research in this
field, we open-source GLM-4.1V-9B-Thinking, which achieves state-of-the-art
performance among models of comparable size. In a comprehensive evaluation
across 28 public benchmarks, our model outperforms Qwen2.5-VL-7B on nearly all
tasks and achieves comparable or even superior performance on 18 benchmarks
relative to the significantly larger Qwen2.5-VL-72B. Notably,
GLM-4.1V-9B-Thinking also demonstrates competitive or superior performance
compared to closed-source models such as GPT-4o on challenging tasks including
long document understanding and STEM reasoning, further underscoring its strong
capabilities. Code, models and more information are released at
https://github.com/THUDM/GLM-4.1V-Thinking.

</details>


### [74] [Evo-0: Vision-Language-Action Model with Implicit Spatial Understanding](https://arxiv.org/abs/2507.00416)
*Tao Lin,Gen Li,Yilei Zhong,Yanwen Zou,Bo Zhao*

Main category: cs.RO

Relevance: 60.0

TL;DR: 提出了一种通过视觉几何基础模型隐式注入3D几何特征的模块，显著提升了VLA模型在空间理解任务中的性能。


<details>
  <summary>Details</summary>
Motivation: 解决现有VLM在空间理解能力上的不足，避免依赖额外传感器或有缺陷的估计。

Method: 设计了一个即插即用的模块，利用现成的视觉几何基础模型隐式注入3D几何特征。

Result: 在五种空间挑战任务中，方法显著提升了VLA模型的性能。

Conclusion: 该方法有效增强了VLA模型的空间理解能力，无需额外传感器。

Abstract: Vision-Language-Action (VLA) models have emerged as a promising framework for
enabling generalist robots capable of perceiving, reasoning, and acting in the
real world. These models usually build upon pretrained Vision-Language Models
(VLMs), which excel at semantic understanding due to large-scale text
pretraining. However, VLMs typically lack precise spatial understanding
capabilities, as they are primarily tuned on 2D image-text pairs without 3D
supervision. To address this limitation, recent approaches have incorporated
explicit 3D inputs such as point clouds or depth maps, but this necessitates
additional depth sensors or defective estimation. In contrast, our work
introduces a plug-and-play module that implicitly injects 3D geometry features
into VLA models by leveraging an off-the-shelf visual geometry foundation
models. We design five spatially challenging tasks that require precise spatial
understanding ability to validate effectiveness of our method. Extensive
evaluations show that our method significantly improves the performance of
state-of-the-art VLA models across diverse scenarios.

</details>


### [75] [Box-QAymo: Box-Referring VQA Dataset for Autonomous Driving](https://arxiv.org/abs/2507.00525)
*Djamahl Etchegaray,Yuxia Fu,Zi Huang,Yadan Luo*

Main category: cs.CV

Relevance: 50.0

TL;DR: Box-QAymo是一个用于评估和微调视觉语言模型（VLMs）在用户指定对象上的时空推理能力的基准数据集，通过边界框标注实现直观查询。


<details>
  <summary>Details</summary>
Motivation: 现有驾驶导向的VQA数据集局限于全场景描述或路径点预测，无法评估VLMs对用户驱动的局部查询的响应能力。

Method: 提出Box-QAymo数据集，包含层次化评估协议，从基础能力测试到属性预测、运动理解和时空推理。

Result: 当前VLMs在感知问题上的表现显著受限，显示出现实世界性能的差距。

Conclusion: 该工作为开发更鲁棒和可解释的自动驾驶系统提供了基础。

Abstract: Interpretable communication is essential for safe and trustworthy autonomous
driving, yet current vision-language models (VLMs) often operate under
idealized assumptions and struggle to capture user intent in real-world
scenarios. Existing driving-oriented VQA datasets are limited to full-scene
descriptions or waypoint prediction, preventing the assessment of whether VLMs
can respond to localized user-driven queries. We introduce Box-QAymo, a
box-referring dataset and benchmark designed to both evaluate and finetune VLMs
on spatial and temporal reasoning over user-specified objects. Users express
intent by drawing bounding boxes, offering a fast and intuitive interface for
focused queries in complex scenes. Specifically, we propose a hierarchical
evaluation protocol that begins with binary sanity-check questions to assess
basic model capacities, and progresses to (1) attribute prediction for
box-referred objects, (2) motion understanding of target instances, and (3)
spatiotemporal motion reasoning over inter-object dynamics across frames. To
support this, we crowd-sourced fine-grained object classes and visual
attributes that reflect the complexity drivers encounter, and extract object
trajectories to construct temporally grounded QA pairs. Rigorous quality
control through negative sampling, temporal consistency checks, and
difficulty-aware balancing guarantee dataset robustness and diversity. Our
comprehensive evaluation reveals significant limitations in current VLMs when
queried about perception questions, highlighting the gap in achieving
real-world performance. This work provides a foundation for developing more
robust and interpretable autonomous driving systems that can communicate
effectively with users under real-world conditions. Project page and dataset
are available at https://djamahl99.github.io/qaymo-pages/.

</details>


### [76] [A Unified Transformer-Based Framework with Pretraining For Whole Body Grasping Motion Generation](https://arxiv.org/abs/2507.00676)
*Edward Effendy,Kuan-Wei Tseng,Rei Kawakami*

Main category: cs.CV

Relevance: 50.0

TL;DR: 提出了一种基于Transformer的全身抓取框架，结合姿态生成和运动填充，实现稳定且真实的物体交互。


<details>
  <summary>Details</summary>
Motivation: 解决全身抓取任务中姿态生成和运动连续性的挑战，并克服手-物体交互数据稀缺的问题。

Method: 采用三阶段流程：抓取姿态生成、时间填充和LiftUp Transformer。通过广义预训练在多样化运动数据上学习鲁棒的时空表示。

Result: 在GRAB数据集上表现优于现有方法，具有更好的连贯性、稳定性和视觉真实性。

Conclusion: 模块化设计易于扩展到其他人体运动应用。

Abstract: Accepted in the ICIP 2025
  We present a novel transformer-based framework for whole-body grasping that
addresses both pose generation and motion infilling, enabling realistic and
stable object interactions. Our pipeline comprises three stages: Grasp Pose
Generation for full-body grasp generation, Temporal Infilling for smooth motion
continuity, and a LiftUp Transformer that refines downsampled joints back to
high-resolution markers. To overcome the scarcity of hand-object interaction
data, we introduce a data-efficient Generalized Pretraining stage on large,
diverse motion datasets, yielding robust spatio-temporal representations
transferable to grasping tasks. Experiments on the GRAB dataset show that our
method outperforms state-of-the-art baselines in terms of coherence, stability,
and visual realism. The modular design also supports easy adaptation to other
human-motion applications.

</details>


### [77] [Diffusion-Based Image Augmentation for Semantic Segmentation in Outdoor Robotics](https://arxiv.org/abs/2507.00153)
*Peter Mortimer,Mirko Maehlisch*

Main category: cs.CV

Relevance: 40.0

TL;DR: 提出了一种基于扩散模型的图像增强方法，用于改善自动驾驶车辆在雪地环境中的感知性能。


<details>
  <summary>Details</summary>
Motivation: 解决学习型感知算法在分布外和代表性不足环境（如雪地）中性能下降的问题。

Method: 利用公开可用的视觉基础模型进行扩散式图像增强，并通过开放词汇语义分割模型过滤幻觉。

Result: 能够更准确地表示部署环境中的语义分布，提升模型在目标环境中的性能。

Conclusion: 扩散式图像增强可扩展至其他环境（如沙地、火山地形），具有广泛应用潜力。

Abstract: The performance of leaning-based perception algorithms suffer when deployed
in out-of-distribution and underrepresented environments. Outdoor robots are
particularly susceptible to rapid changes in visual scene appearance due to
dynamic lighting, seasonality and weather effects that lead to scenes
underrepresented in the training data of the learning-based perception system.
In this conceptual paper, we focus on preparing our autonomous vehicle for
deployment in snow-filled environments. We propose a novel method for
diffusion-based image augmentation to more closely represent the deployment
environment in our training data. Diffusion-based image augmentations rely on
the public availability of vision foundation models learned on internet-scale
datasets. The diffusion-based image augmentations allow us to take control over
the semantic distribution of the ground surfaces in the training data and to
fine-tune our model for its deployment environment. We employ open vocabulary
semantic segmentation models to filter out augmentation candidates that contain
hallucinations. We believe that diffusion-based image augmentations can be
extended to many other environments apart from snow surfaces, like sandy
environments and volcanic terrains.

</details>


### [78] [FreeLong++: Training-Free Long Video Generation via Multi-band SpectralFusion](https://arxiv.org/abs/2507.00162)
*Yu Lu,Yi Yang*

Main category: cs.CV

Relevance: 40.0

TL;DR: FreeLong和FreeLong++是无需训练的框架，通过平衡长视频特征频率分布，提升长视频生成的质量。


<details>
  <summary>Details</summary>
Motivation: 解决现有视频生成模型在生成长视频时的时间一致性和视觉保真度下降问题。

Method: 提出FreeLong框架，结合全局低频和局部高频特征；FreeLong++扩展为多分支架构，实现多频段融合。

Result: 在长视频生成任务中表现优于现有方法，支持多提示生成和可控视频生成。

Conclusion: FreeLong++能有效提升长视频生成质量，无需额外训练即可应用于现有模型。

Abstract: Recent advances in video generation models have enabled high-quality short
video generation from text prompts. However, extending these models to longer
videos remains a significant challenge, primarily due to degraded temporal
consistency and visual fidelity. Our preliminary observations show that naively
applying short-video generation models to longer sequences leads to noticeable
quality degradation. Further analysis identifies a systematic trend where
high-frequency components become increasingly distorted as video length grows,
an issue we term high-frequency distortion. To address this, we propose
FreeLong, a training-free framework designed to balance the frequency
distribution of long video features during the denoising process. FreeLong
achieves this by blending global low-frequency features, which capture holistic
semantics across the full video, with local high-frequency features extracted
from short temporal windows to preserve fine details. Building on this,
FreeLong++ extends FreeLong dual-branch design into a multi-branch architecture
with multiple attention branches, each operating at a distinct temporal scale.
By arranging multiple window sizes from global to local, FreeLong++ enables
multi-band frequency fusion from low to high frequencies, ensuring both
semantic continuity and fine-grained motion dynamics across longer video
sequences. Without any additional training, FreeLong++ can be plugged into
existing video generation models (e.g. Wan2.1 and LTX-Video) to produce longer
videos with substantially improved temporal consistency and visual fidelity. We
demonstrate that our approach outperforms previous methods on longer video
generation tasks (e.g. 4x and 8x of native length). It also supports coherent
multi-prompt video generation with smooth scene transitions and enables
controllable video generation using long depth or pose sequences.

</details>


### [79] [CGEarthEye:A High-Resolution Remote Sensing Vision Foundation Model Based on the Jilin-1 Satellite Constellation](https://arxiv.org/abs/2507.00356)
*Zhiwei Yi,Xin Cheng,Jingyu Ma,Ruifei Zhu,Junwei Tian,Yuanxiu Zhou,Xinge Zhao,Hongzhe Li*

Main category: cs.CV

Relevance: 40.0

TL;DR: 论文提出CGEarthEye，一个针对吉林一号卫星特性的高分辨率遥感视觉基础模型框架，包含五个不同参数规模的骨干网络，总计21亿参数。通过构建首个1500万规模的多时序自监督学习数据集JLSSD，结合多种对比策略进行预训练，在10个基准数据集上实现SOTA性能。


<details>
  <summary>Details</summary>
Motivation: 解决超高分辨率光学遥感影像获取渠道有限的问题，推动高分辨率遥感视觉基础模型的发展。

Method: 提出CGEarthEye框架，包含五个骨干网络，构建JLSSD数据集，结合季节性对比、增强对比和掩码补丁对比策略进行预训练。

Result: 在10个基准数据集上实现SOTA性能，特征可视化、模型收敛性、参数效率和实际应用表现优异。

Conclusion: CGEarthEye的卓越表征能力将促进吉林一号数据在传统地球观测应用中的更广泛和高效应用。

Abstract: Deep learning methods have significantly advanced the development of
intelligent rinterpretation in remote sensing (RS), with foundational model
research based on large-scale pre-training paradigms rapidly reshaping various
domains of Earth Observation (EO). However, compared to the open accessibility
and high spatiotemporal coverage of medium-resolution data, the limited
acquisition channels for ultra-high-resolution optical RS imagery have
constrained the progress of high-resolution remote sensing vision foundation
models (RSVFM). As the world's largest sub-meter-level commercial RS satellite
constellation, the Jilin-1 constellation possesses abundant sub-meter-level
image resources. This study proposes CGEarthEye, a RSVFM framework specifically
designed for Jilin-1 satellite characteristics, comprising five backbones with
different parameter scales with totaling 2.1 billion parameters. To enhance the
representational capacity of the foundation model, we developed JLSSD, the
first 15-million-scale multi-temporal self-supervised learning (SSL) dataset
featuring global coverage with quarterly temporal sampling within a single
year, constructed through multi-level representation clustering and sampling
strategies. The framework integrates seasonal contrast, augmentation-based
contrast, and masked patch token contrastive strategies for pre-training.
Comprehensive evaluations across 10 benchmark datasets covering four typical RS
tasks demonstrate that the CGEarthEye consistently achieves state-of-the-art
(SOTA) performance. Further analysis reveals CGEarthEye's superior
characteristics in feature visualization, model convergence, parameter
efficiency, and practical mapping applications. This study anticipates that the
exceptional representation capabilities of CGEarthEye will facilitate broader
and more efficient applications of Jilin-1 data in traditional EO application.

</details>


### [80] [MedDiff-FT: Data-Efficient Diffusion Model Fine-tuning with Structural Guidance for Controllable Medical Image Synthesis](https://arxiv.org/abs/2507.00377)
*Jianhao Xie,Ziang Zhang,Zhenyu Weng,Yuesheng Zhu,Guibo Luo*

Main category: cs.CV

Relevance: 40.0

TL;DR: MedDiff-FT是一种可控的医学图像生成方法，通过微调扩散基础模型，以数据高效的方式生成具有结构依赖性和领域特异性的医学图像。


<details>
  <summary>Details</summary>
Motivation: 解决医学图像分割中高质量训练数据稀缺的问题，以及现有扩散模型对大规模数据集和高图像质量的依赖限制。

Method: 提出MedDiff-FT方法，包括动态自适应引导掩码、轻量级随机掩码生成器和自动化质量评估协议。

Result: 在五个医学分割数据集上，MedDiff-FT的合成图像-掩码对将SOTA方法的Dice分数平均提高了1%。

Conclusion: MedDiff-FT在生成质量、多样性和计算效率之间取得了平衡，为医学数据增强提供了实用解决方案。

Abstract: Recent advancements in deep learning for medical image segmentation are often
limited by the scarcity of high-quality training data.While diffusion models
provide a potential solution by generating synthetic images, their
effectiveness in medical imaging remains constrained due to their reliance on
large-scale medical datasets and the need for higher image quality. To address
these challenges, we present MedDiff-FT, a controllable medical image
generation method that fine-tunes a diffusion foundation model to produce
medical images with structural dependency and domain specificity in a
data-efficient manner. During inference, a dynamic adaptive guiding mask
enforces spatial constraints to ensure anatomically coherent synthesis, while a
lightweight stochastic mask generator enhances diversity through hierarchical
randomness injection. Additionally, an automated quality assessment protocol
filters suboptimal outputs using feature-space metrics, followed by mask
corrosion to refine fidelity. Evaluated on five medical segmentation
datasets,MedDiff-FT's synthetic image-mask pairs improve SOTA method's
segmentation performance by an average of 1% in Dice score. The framework
effectively balances generation quality, diversity, and computational
efficiency, offering a practical solution for medical data augmentation. The
code is available at https://github.com/JianhaoXie1/MedDiff-FT.

</details>


### [81] [Few-shot Classification as Multi-instance Verification: Effective Backbone-agnostic Transfer across Domains](https://arxiv.org/abs/2507.00401)
*Xin Xu,Eibe Frank,Geoffrey Holmes*

Main category: cs.CV

Relevance: 40.0

TL;DR: 论文提出了一种名为“MIV-head”的新方法，用于解决在无法微调主干网络的情况下进行跨域少样本学习的问题。该方法通过将少样本分类任务转化为多实例验证任务，实现了高效的域适应。


<details>
  <summary>Details</summary>
Motivation: 在实际应用中，微调主干网络通常不可行，导致生成的嵌入质量低且静态。论文旨在解决这一问题，提出一种不依赖主干网络微调的高效少样本学习方法。

Method: 将少样本分类任务表示为多实例验证任务，并设计了一个与主干网络无关的“MIV-head”分类头。该方法在目标域的少样本数据上进行训练，无需微调主干网络。

Result: 在跨域少样本图像分类任务中，MIV-head表现优于部分微调方法，且适应成本更低。传统分类头方法在准确性上表现较差。

Conclusion: MIV-head是一种高效且无需微调主干网络的少样本学习方法，适用于实际应用场景。

Abstract: We investigate cross-domain few-shot learning under the constraint that
fine-tuning of backbones (i.e., feature extractors) is impossible or infeasible
-- a scenario that is increasingly common in practical use cases. Handling the
low-quality and static embeddings produced by frozen, "black-box" backbones
leads to a problem representation of few-shot classification as a series of
multiple instance verification (MIV) tasks. Inspired by this representation, we
introduce a novel approach to few-shot domain adaptation, named the "MIV-head",
akin to a classification head that is agnostic to any pretrained backbone and
computationally efficient. The core components designed for the MIV-head, when
trained on few-shot data from a target domain, collectively yield strong
performance on test data from that domain. Importantly, it does so without
fine-tuning the backbone, and within the "meta-testing" phase. Experimenting
under various settings and on an extension of the Meta-dataset benchmark for
cross-domain few-shot image classification, using representative off-the-shelf
convolutional neural network and vision transformer backbones pretrained on
ImageNet1K, we show that the MIV-head achieves highly competitive accuracy when
compared to state-of-the-art "adapter" (or partially fine-tuning) methods
applied to the same backbones, while incurring substantially lower adaptation
cost. We also find well-known "classification head" approaches lag far behind
in terms of accuracy. Ablation study empirically justifies the core components
of our approach. We share our code at https://github.com/xxweka/MIV-head.

</details>


### [82] [Latent Posterior-Mean Rectified Flow for Higher-Fidelity Perceptual Face Restoration](https://arxiv.org/abs/2507.00447)
*Xin Luo,Menglin Zhang,Yunwei Lan,Tianyu Zhang,Rui Li,Chang Liu,Dong Liu*

Main category: cs.CV

Relevance: 40.0

TL;DR: Latent-PMRF改进PMRF，通过在VAE的潜在空间中重新定义源分布，以更好地与人类感知对齐，显著提升了盲脸恢复的性能和效率。


<details>
  <summary>Details</summary>
Motivation: 解决PMRF在像素空间中建模时与人类感知对齐不足的问题，提出在潜在空间中优化以实现更好的感知-失真权衡。

Method: 将PMRF重新定义为VAE潜在空间中的流模型，利用VAE的重构误差限制最小失真，并设计新的VAE架构。

Result: Latent-PMRF在盲脸恢复中表现优越，实现了5.79倍的FID加速，显著提升了PD权衡。

Conclusion: 潜在空间优化和VAE设计对提升感知-失真权衡至关重要，Latent-PMRF为相关任务提供了高效解决方案。

Abstract: The Perception-Distortion tradeoff (PD-tradeoff) theory suggests that face
restoration algorithms must balance perceptual quality and fidelity. To achieve
minimal distortion while maintaining perfect perceptual quality, Posterior-Mean
Rectified Flow (PMRF) proposes a flow based approach where source distribution
is minimum distortion estimations. Although PMRF is shown to be effective, its
pixel-space modeling approach limits its ability to align with human
perception, where human perception is defined as how humans distinguish between
two image distributions. In this work, we propose Latent-PMRF, which
reformulates PMRF in the latent space of a variational autoencoder (VAE),
facilitating better alignment with human perception during optimization. By
defining the source distribution on latent representations of minimum
distortion estimation, we bound the minimum distortion by the VAE's
reconstruction error. Moreover, we reveal the design of VAE is crucial, and our
proposed VAE significantly outperforms existing VAEs in both reconstruction and
restoration. Extensive experiments on blind face restoration demonstrate the
superiority of Latent-PMRF, offering an improved PD-tradeoff compared to
existing methods, along with remarkable convergence efficiency, achieving a
5.79X speedup over PMRF in terms of FID. Our code will be available as
open-source.

</details>


### [83] [ATSTrack: Enhancing Visual-Language Tracking by Aligning Temporal and Spatial Scales](https://arxiv.org/abs/2507.00454)
*Yihao Zhen,Qiang Wang,Yu Qiao,Liangqiong Qu,Huijie Fan*

Main category: cs.CV

Relevance: 40.0

TL;DR: 论文提出了一种名为ATSTrack的视觉语言跟踪器，通过对齐视觉和语言输入的时间和空间尺度差异，改进了特征修改效果。


<details>
  <summary>Details</summary>
Motivation: 视觉语言跟踪中的主要挑战是视觉输入与语言描述之间的不对齐，尤其是目标移动导致的时空尺度差异。

Method: ATSTrack将语言描述分解为具有不同属性的短语，并细粒度地修改其特征，同时引入视觉语言令牌以减少空间尺度差异的影响。

Result: 实验表明，ATSTrack的性能与现有方法相当。

Conclusion: ATSTrack通过时空尺度对齐解决了视觉语言跟踪中的关键问题。

Abstract: A main challenge of Visual-Language Tracking (VLT) is the misalignment
between visual inputs and language descriptions caused by target movement.
Previous trackers have explored many effective feature modification methods to
preserve more aligned features. However, an important yet unexplored factor
ultimately hinders their capability, which is the inherent differences in the
temporal and spatial scale of information between visual and language inputs.
To address this issue, we propose a novel visual-language tracker that enhances
the effect of feature modification by \textbf{A}ligning \textbf{T}emporal and
\textbf{S}patial scale of different input components, named as
\textbf{ATSTrack}. Specifically, we decompose each language description into
phrases with different attributes based on their temporal and spatial
correspondence with visual inputs, and modify their features in a fine-grained
manner. Moreover, we introduce a Visual-Language token that comprises modified
linguistic information from the previous frame to guide the model to extract
visual features that are more relevant to language description, thereby
reducing the impact caused by the differences in spatial scale. Experimental
results show that our proposed ATSTrack achieves performance comparable to
existing methods. Our code will be released.

</details>


### [84] [ADAptation: Reconstruction-based Unsupervised Active Learning for Breast Ultrasound Diagnosis](https://arxiv.org/abs/2507.00474)
*Yaofei Duan,Yuhao Huang,Xin Yang,Luyi Han,Xinyu Xie,Zhiyuan Zhu,Ping He,Ka-Hou Chan,Ligang Cui,Sio-Kei Im,Dong Ni,Tao Tan*

Main category: cs.CV

Relevance: 40.0

TL;DR: 提出了一种名为ADAptation的无监督主动学习框架，用于解决深度学习模型在跨域分布变化时的性能下降问题，通过扩散模型和对比学习网络优化样本选择。


<details>
  <summary>Details</summary>
Motivation: 解决深度学习模型在训练和测试域分布变化时的性能下降问题，减少标注成本。

Method: 利用扩散模型进行域间分布对齐，结合对比学习网络和双评分机制选择样本。

Result: 在四个乳腺超声数据集上超越现有主动学习方法，验证了方法的有效性和泛化能力。

Conclusion: ADAptation框架在临床域适应中表现出色，为跨域问题提供了高效解决方案。

Abstract: Deep learning-based diagnostic models often suffer performance drops due to
distribution shifts between training (source) and test (target) domains.
Collecting and labeling sufficient target domain data for model retraining
represents an optimal solution, yet is limited by time and scarce resources.
Active learning (AL) offers an efficient approach to reduce annotation costs
while maintaining performance, but struggles to handle the challenge posed by
distribution variations across different datasets. In this study, we propose a
novel unsupervised Active learning framework for Domain Adaptation, named
ADAptation, which efficiently selects informative samples from multi-domain
data pools under limited annotation budget. As a fundamental step, our method
first utilizes the distribution homogenization capabilities of diffusion models
to bridge cross-dataset gaps by translating target images into source-domain
style. We then introduce two key innovations: (a) a hypersphere-constrained
contrastive learning network for compact feature clustering, and (b) a
dual-scoring mechanism that quantifies and balances sample uncertainty and
representativeness. Extensive experiments on four breast ultrasound datasets
(three public and one in-house/multi-center) across five common deep
classifiers demonstrate that our method surpasses existing strong AL-based
competitors, validating its effectiveness and generalization for clinical
domain adaptation. The code is available at the anonymized link:
https://github.com/miccai25-966/ADAptation.

</details>


### [85] [Just Noticeable Difference for Large Multimodal Models](https://arxiv.org/abs/2507.00490)
*Zijian Chen,Yuan Tian,Yuze Sun,Wei Sun,Zicheng Zhang,Weisi Lin,Guangtao Zhai,Wenjun Zhang*

Main category: cs.CV

Relevance: 40.0

TL;DR: 论文提出了LMM-JND概念，用于量化大型多模态模型（LMMs）的视觉盲区，并构建了VPA-JND数据集，揭示了当前LMMs在视觉任务中的不足。


<details>
  <summary>Details</summary>
Motivation: 研究大型多模态模型（LMMs）在视觉感知任务中的盲区，以解决潜在的安全问题和响应效率不足。

Method: 提出LMM-JND概念及其确定流程，构建包含21.5k参考图像和489k刺激的大规模数据集VPA-JND，分析多种LMM家族的视觉表现。

Result: 发现包括GPT-4o和InternVL2.5在内的先进LMMs在基本视觉任务中表现显著低于人类水平，并揭示了视觉和语言主干设计的相关性。

Conclusion: LMM-JND为研究LMMs提供了新视角，其可预测性对安全性至关重要。

Abstract: Just noticeable difference (JND), the minimum change that the human visual
system (HVS) can perceive, has been studied for decades. Although recent work
has extended this line of research into machine vision, there has been a
scarcity of studies systematically exploring its perceptual boundaries across
multiple tasks and stimulus types, particularly in the current era of rapidly
advancing large multimodal models (LMMs), where studying the multifaceted
capabilities of models has become a mainstream focus. Moreover, the perceptual
defects of LMMs are not investigated thoroughly, resulting in potential
security issues and suboptimal response efficiency. In this paper, we take an
initial attempt and demonstrate that there exist significant visual blind spots
in current LMMs. To systemically quantify this characteristic, we propose a new
concept, {\bf LMM-JND}, together with its determination pipeline. Targeting
uncovering the behavior commonalities in HVS-aligned visual perception tasks,
we delve into several LMM families and construct a large-scale dataset, named
VPA-JND, which contains 21.5k reference images with over 489k stimuli across 12
distortion types, to facilitate LMM-JND studies. VPA-JND exposes areas where
state-of-the-art LMMs, including GPT-4o and the InternVL2.5 series, struggle
with basic comparison queries and fall significantly short of human-level
visual performance. We further explore the effects of vision and language
backbones and find a notable correlation between their design philosophy that
may instruct the future refinement of LMMs for their visual acuity. Together,
our research underscores the significance of LMM-JND as a unique perspective
for studying LMMs, and predictable LMM-JND is crucial for security concerns.
This work will be available at https://github.com/zijianchen98/LMM-JND.

</details>


### [86] [ExPaMoE: An Expandable Parallel Mixture of Experts for Continual Test-Time Adaptation](https://arxiv.org/abs/2507.00502)
*JianChao Zhao,Songlin Dong*

Main category: cs.CV

Relevance: 40.0

TL;DR: ExPaMoE提出了一种基于可扩展并行混合专家架构的框架，用于持续测试时间适应（CTTA），通过解耦领域通用和特定知识，动态扩展专家池，并在多个基准测试中表现优异。


<details>
  <summary>Details</summary>
Motivation: 现有CTTA方法依赖共享模型参数，易受特征纠缠和灾难性遗忘影响，特别是在大或非平稳领域偏移时。

Method: ExPaMoE采用双分支专家设计，通过令牌引导特征分离，并利用频谱感知在线领域鉴别器（SODD）实时检测分布变化。

Result: 在CIFAR-10C、CIFAR-100C、ImageNet-C等标准基准测试中表现优异，并引入ImageNet++作为新的大规模CTTA基准。

Conclusion: ExPaMoE在鲁棒性、可扩展性和抗遗忘性方面显著优于现有方法。

Abstract: Continual Test-Time Adaptation (CTTA) aims to enable models to adapt
on-the-fly to a stream of unlabeled data under evolving distribution shifts.
However, existing CTTA methods typically rely on shared model parameters across
all domains, making them vulnerable to feature entanglement and catastrophic
forgetting in the presence of large or non-stationary domain shifts. To address
this limitation, we propose \textbf{ExPaMoE}, a novel framework based on an
\emph{Expandable Parallel Mixture-of-Experts} architecture. ExPaMoE decouples
domain-general and domain-specific knowledge via a dual-branch expert design
with token-guided feature separation, and dynamically expands its expert pool
based on a \emph{Spectral-Aware Online Domain Discriminator} (SODD) that
detects distribution changes in real-time using frequency-domain cues.
Extensive experiments demonstrate the superiority of ExPaMoE across diverse
CTTA scenarios. We evaluate our method on standard benchmarks including
CIFAR-10C, CIFAR-100C, ImageNet-C, and Cityscapes-to-ACDC for semantic
segmentation. Additionally, we introduce \textbf{ImageNet++}, a large-scale and
realistic CTTA benchmark built from multiple ImageNet-derived datasets, to
better reflect long-term adaptation under complex domain evolution. ExPaMoE
consistently outperforms prior arts, showing strong robustness, scalability,
and resistance to forgetting.

</details>


### [87] [SCING:Towards More Efficient and Robust Person Re-Identification through Selective Cross-modal Prompt Tuning](https://arxiv.org/abs/2507.00506)
*Yunfei Xie,Yuxuan Cheng,Juncheng Wu,Haoyu Zhang,Yuyin Zhou,Shoudong Han*

Main category: cs.CV

Relevance: 40.0

TL;DR: 论文提出了一种名为SCING的框架，通过选择性视觉提示融合和扰动驱动一致性对齐，优化了跨模态对齐和鲁棒性，避免了复杂适配器设计。


<details>
  <summary>Details</summary>
Motivation: 现有方法在视觉语言预训练模型适应ReID任务时，常因忽略跨模态交互而导致高计算成本或对齐效果不佳。

Method: 提出SCING框架，包含选择性视觉提示融合（SVIP）和扰动驱动一致性对齐（PDCA）两个创新模块。

Result: 在多个基准测试中表现优异，平衡了性能与计算开销。

Conclusion: SCING框架简单高效，无需复杂适配器即可实现高性能。

Abstract: Recent advancements in adapting vision-language pre-training models like CLIP
for person re-identification (ReID) tasks often rely on complex adapter design
or modality-specific tuning while neglecting cross-modal interaction, leading
to high computational costs or suboptimal alignment. To address these
limitations, we propose a simple yet effective framework named Selective
Cross-modal Prompt Tuning (SCING) that enhances cross-modal alignment and
robustness against real-world perturbations. Our method introduces two key
innovations: Firstly, we proposed Selective Visual Prompt Fusion (SVIP), a
lightweight module that dynamically injects discriminative visual features into
text prompts via a cross-modal gating mechanism. Moreover, the proposed
Perturbation-Driven Consistency Alignment (PDCA) is a dual-path training
strategy that enforces invariant feature alignment under random image
perturbations by regularizing consistency between original and augmented
cross-modal embeddings. Extensive experiments are conducted on several popular
benchmarks covering Market1501, DukeMTMC-ReID, Occluded-Duke, Occluded-REID,
and P-DukeMTMC, which demonstrate the impressive performance of the proposed
method. Notably, our framework eliminates heavy adapters while maintaining
efficient inference, achieving an optimal trade-off between performance and
computational overhead. The code will be released upon acceptance.

</details>


### [88] [Zero-shot Skeleton-based Action Recognition with Prototype-guided Feature Alignment](https://arxiv.org/abs/2507.00566)
*Kai Zhou,Shuhai Zhang,Zeng You,Jinwu Hu,Mingkui Tan,Fei Liu*

Main category: cs.CV

Relevance: 40.0

TL;DR: 论文提出了一种原型引导的特征对齐范式（PGFA），用于零样本骨架动作识别，通过端到端跨模态对比训练框架和改进的文本特征对齐策略，显著提升了性能。


<details>
  <summary>Details</summary>
Motivation: 零样本骨架动作识别因从已知动作泛化到未知动作的困难而极具挑战性。现有方法因骨架特征区分度不足和对齐偏差问题效果受限。

Method: 提出PGFA方法，包括端到端跨模态对比训练框架和原型引导的文本特征对齐策略。

Result: 在NTU-60、NTU-120和PKU-MMD数据集上，PGFA比SMIE方法分别提升了22.96%、12.53%和18.54%的准确率。

Conclusion: PGFA通过改进骨架-文本对齐和减少分布差异，显著提升了零样本骨架动作识别的性能。

Abstract: Zero-shot skeleton-based action recognition aims to classify unseen
skeleton-based human actions without prior exposure to such categories during
training. This task is extremely challenging due to the difficulty in
generalizing from known to unknown actions. Previous studies typically use
two-stage training: pre-training skeleton encoders on seen action categories
using cross-entropy loss and then aligning pre-extracted skeleton and text
features, enabling knowledge transfer to unseen classes through skeleton-text
alignment and language models' generalization. However, their efficacy is
hindered by 1) insufficient discrimination for skeleton features, as the fixed
skeleton encoder fails to capture necessary alignment information for effective
skeleton-text alignment; 2) the neglect of alignment bias between skeleton and
unseen text features during testing. To this end, we propose a prototype-guided
feature alignment paradigm for zero-shot skeleton-based action recognition,
termed PGFA. Specifically, we develop an end-to-end cross-modal contrastive
training framework to improve skeleton-text alignment, ensuring sufficient
discrimination for skeleton features. Additionally, we introduce a
prototype-guided text feature alignment strategy to mitigate the adverse impact
of the distribution discrepancy during testing. We provide a theoretical
analysis to support our prototype-guided text feature alignment strategy and
empirically evaluate our overall PGFA on three well-known datasets. Compared
with the top competitor SMIE method, our PGFA achieves absolute accuracy
improvements of 22.96%, 12.53%, and 18.54% on the NTU-60, NTU-120, and PKU-MMD
datasets, respectively.

</details>


### [89] [AI-Generated Video Detection via Perceptual Straightening](https://arxiv.org/abs/2507.00583)
*Christian Internò,Robert Geirhos,Markus Olhofer,Sunny Liu,Barbara Hammer,David Klindt*

Main category: cs.CV

Relevance: 40.0

TL;DR: ReStraV利用预训练的视觉Transformer（DINOv2）分析视频的神经表示几何特性，通过量化时间曲率和步长距离来区分AI生成视频与真实视频，实现了高效且高性能的检测。


<details>
  <summary>Details</summary>
Motivation: 生成式AI的快速发展使得合成视频逼真度极高，现有检测方法难以捕捉时间不一致性，亟需新的解决方案。

Method: 基于‘感知直线化’假设，利用DINOv2量化视频表示域的时间曲率和步长距离，训练轻量级分类器。

Result: 在VidProM基准上达到97.17%准确率和98.63% AUROC，显著优于现有方法。

Conclusion: ReStraV为AI生成视频检测提供了基于神经表示几何的新思路，具有高效和低成本的优势。

Abstract: The rapid advancement of generative AI enables highly realistic synthetic
videos, posing significant challenges for content authentication and raising
urgent concerns about misuse. Existing detection methods often struggle with
generalization and capturing subtle temporal inconsistencies. We propose
ReStraV(Representation Straightening Video), a novel approach to distinguish
natural from AI-generated videos. Inspired by the "perceptual straightening"
hypothesis -- which suggests real-world video trajectories become more straight
in neural representation domain -- we analyze deviations from this expected
geometric property. Using a pre-trained self-supervised vision transformer
(DINOv2), we quantify the temporal curvature and stepwise distance in the
model's representation domain. We aggregate statistics of these measures for
each video and train a classifier. Our analysis shows that AI-generated videos
exhibit significantly different curvature and distance patterns compared to
real videos. A lightweight classifier achieves state-of-the-art detection
performance (e.g., 97.17% accuracy and 98.63% AUROC on the VidProM benchmark),
substantially outperforming existing image- and video-based methods. ReStraV is
computationally efficient, it is offering a low-cost and effective detection
solution. This work provides new insights into using neural representation
geometry for AI-generated video detection.

</details>


### [90] [World4Drive: End-to-End Autonomous Driving via Intention-aware Physical Latent World Model](https://arxiv.org/abs/2507.00603)
*Yupeng Zheng,Pengxuan Yang,Zebin Xing,Qichao Zhang,Yuhang Zheng,Yinfeng Gao,Pengfei Li,Teng Zhang,Zhongpu Xia,Peng Jia,Dongbin Zhao*

Main category: cs.CV

Relevance: 40.0

TL;DR: World4Drive是一个端到端自动驾驶框架，利用视觉基础模型构建潜在世界模型，实现无感知标注的规划轨迹生成与评估。


<details>
  <summary>Details</summary>
Motivation: 解决端到端自动驾驶依赖昂贵感知监督的问题，通过自监督学习构建信息丰富的驾驶世界模型。

Method: 使用视觉基础模型提取场景特征，生成多模态规划轨迹，并通过世界模型选择器评估最佳轨迹。

Result: 在nuScenes和NavSim基准测试中表现优异，L2误差降低18.1%，碰撞率减少46.7%，训练收敛速度提高3.75倍。

Conclusion: World4Drive展示了无感知标注的端到端规划的可行性，性能显著优于现有方法。

Abstract: End-to-end autonomous driving directly generates planning trajectories from
raw sensor data, yet it typically relies on costly perception supervision to
extract scene information. A critical research challenge arises: constructing
an informative driving world model to enable perception annotation-free,
end-to-end planning via self-supervised learning. In this paper, we present
World4Drive, an end-to-end autonomous driving framework that employs vision
foundation models to build latent world models for generating and evaluating
multi-modal planning trajectories. Specifically, World4Drive first extracts
scene features, including driving intention and world latent representations
enriched with spatial-semantic priors provided by vision foundation models. It
then generates multi-modal planning trajectories based on current scene
features and driving intentions and predicts multiple intention-driven future
states within the latent space. Finally, it introduces a world model selector
module to evaluate and select the best trajectory. We achieve perception
annotation-free, end-to-end planning through self-supervised alignment between
actual future observations and predicted observations reconstructed from the
latent space. World4Drive achieves state-of-the-art performance without manual
perception annotations on both the open-loop nuScenes and closed-loop NavSim
benchmarks, demonstrating an 18.1\% relative reduction in L2 error, 46.7% lower
collision rate, and 3.75 faster training convergence. Codes will be accessed at
https://github.com/ucaszyp/World4Drive.

</details>


### [91] [De-Simplifying Pseudo Labels to Enhancing Domain Adaptive Object Detection](https://arxiv.org/abs/2507.00608)
*Zehua Fu,Chenguang Liu,Yuyu Chen,Jiaqi Zhou,Qingjie Liu,Yunhong Wang*

Main category: cs.CV

Relevance: 40.0

TL;DR: 论文提出DeSimPL方法，通过减少训练中简单样本的比例，提升自标注检测器的性能。


<details>
  <summary>Details</summary>
Motivation: 解决自标注检测器因简单样本比例高（简单标签偏差）而性能不足的问题。

Method: 提出DeSimPL方法，包括实例级记忆库、对抗样本引入和自适应加权损失。

Result: 在四个基准测试中验证了DeSimPL能显著提升自标注检测器的性能。

Conclusion: DeSimPL有效解决了简单标签偏差问题，提升了自标注检测器的性能。

Abstract: Despite its significant success, object detection in traffic and
transportation scenarios requires time-consuming and laborious efforts in
acquiring high-quality labeled data. Therefore, Unsupervised Domain Adaptation
(UDA) for object detection has recently gained increasing research attention.
UDA for object detection has been dominated by domain alignment methods, which
achieve top performance. Recently, self-labeling methods have gained popularity
due to their simplicity and efficiency. In this paper, we investigate the
limitations that prevent self-labeling detectors from achieving commensurate
performance with domain alignment methods. Specifically, we identify the high
proportion of simple samples during training, i.e., the simple-label bias, as
the central cause. We propose a novel approach called De-Simplifying Pseudo
Labels (DeSimPL) to mitigate the issue. DeSimPL utilizes an instance-level
memory bank to implement an innovative pseudo label updating strategy. Then,
adversarial samples are introduced during training to enhance the proportion.
Furthermore, we propose an adaptive weighted loss to avoid the model suffering
from an abundance of false positive pseudo labels in the late training period.
Experimental results demonstrate that DeSimPL effectively reduces the
proportion of simple samples during training, leading to a significant
performance improvement for self-labeling detectors. Extensive experiments
conducted on four benchmarks validate our analysis and conclusions.

</details>


### [92] [Cage-Based Deformation for Transferable and Undefendable Point Cloud Attack](https://arxiv.org/abs/2507.00690)
*Keke Tang,Ziyong Du,Weilong Peng,Xiaofei Wang,Peican Zhu,Ligang Liu,Zhihong Tian*

Main category: cs.CV

Relevance: 40.0

TL;DR: 提出了一种基于笼形变形的对抗攻击框架CageAttack，用于生成自然的对抗点云，平衡了可转移性、不可防御性和合理性。


<details>
  <summary>Details</summary>
Motivation: 现有对抗攻击方法在点云上施加严格的几何约束或导致不自然的变形，限制了其效果。

Method: 通过构建目标对象的笼形结构，平滑地变形点云，确保变形自然且合理。

Result: 在多个3D深度学习分类器和数据集上，CageAttack在可转移性、不可防御性和合理性方面优于现有方法。

Conclusion: CageAttack提供了一种有效的对抗攻击方法，适用于点云数据。

Abstract: Adversarial attacks on point clouds often impose strict geometric constraints
to preserve plausibility; however, such constraints inherently limit
transferability and undefendability. While deformation offers an alternative,
existing unstructured approaches may introduce unnatural distortions, making
adversarial point clouds conspicuous and undermining their plausibility. In
this paper, we propose CageAttack, a cage-based deformation framework that
produces natural adversarial point clouds. It first constructs a cage around
the target object, providing a structured basis for smooth, natural-looking
deformation. Perturbations are then applied to the cage vertices, which
seamlessly propagate to the point cloud, ensuring that the resulting
deformations remain intrinsic to the object and preserve plausibility.
Extensive experiments on seven 3D deep neural network classifiers across three
datasets show that CageAttack achieves a superior balance among
transferability, undefendability, and plausibility, outperforming
state-of-the-art methods. Codes will be made public upon acceptance.

</details>


### [93] [BEV-VAE: Multi-view Image Generation with Spatial Consistency for Autonomous Driving](https://arxiv.org/abs/2507.00707)
*Zeming Chen,Hang Zhao*

Main category: cs.CV

Relevance: 40.0

TL;DR: BEV-VAE提出了一种基于BEV（鸟瞰图）潜在空间的多视角图像生成方法，通过变分自编码器和潜在扩散变换器实现3D一致的场景生成。


<details>
  <summary>Details</summary>
Motivation: 现有方法将多视角图像生成视为2D任务，缺乏显式3D建模，而结构化表示对自动驾驶场景生成至关重要。

Method: BEV-VAE首先训练多视角图像变分自编码器，构建紧凑的BEV潜在空间，然后使用潜在扩散变换器生成场景。

Result: 在nuScenes和Argoverse 2数据集上，BEV-VAE在3D一致重建和生成方面表现优异。

Conclusion: BEV-VAE为自动驾驶中的多视角图像生成提供了3D一致且可控的解决方案。

Abstract: Multi-view image generation in autonomous driving demands consistent 3D scene
understanding across camera views. Most existing methods treat this problem as
a 2D image set generation task, lacking explicit 3D modeling. However, we argue
that a structured representation is crucial for scene generation, especially
for autonomous driving applications. This paper proposes BEV-VAE for consistent
and controllable view synthesis. BEV-VAE first trains a multi-view image
variational autoencoder for a compact and unified BEV latent space and then
generates the scene with a latent diffusion transformer. BEV-VAE supports
arbitrary view generation given camera configurations, and optionally 3D
layouts. Experiments on nuScenes and Argoverse 2 (AV2) show strong performance
in both 3D consistent reconstruction and generation. The code is available at:
https://github.com/Czm369/bev-vae.

</details>


### [94] [Towards Open-World Human Action Segmentation Using Graph Convolutional Networks](https://arxiv.org/abs/2507.00756)
*Hao Xing,Kai Zhe Boey,Gordon Cheng*

Main category: cs.CV

Relevance: 40.0

TL;DR: 该论文提出了一种用于开放世界动作分割的结构化框架，包含三个创新点：增强的金字塔图卷积网络、基于Mixup的训练方法和时间聚类损失。实验表明，该框架在开放集分割和分布外检测方面表现优异。


<details>
  <summary>Details</summary>
Motivation: 解决现有方法在开放世界场景中难以泛化到新动作的问题，避免依赖手动标注的动态多样性人类活动。

Method: 1) 使用EPGCN进行时空特征上采样；2) 基于Mixup合成分布外数据；3) 引入时间聚类损失。

Result: 在Bimanual Actions和H2O数据集上，开放集分割和分布外检测性能分别提升16.9%和34.6%。

Conclusion: 该框架有效解决了开放世界动作分割问题，显著优于现有方法。

Abstract: Human-object interaction segmentation is a fundamental task of daily activity
understanding, which plays a crucial role in applications such as assistive
robotics, healthcare, and autonomous systems. Most existing learning-based
methods excel in closed-world action segmentation, they struggle to generalize
to open-world scenarios where novel actions emerge. Collecting exhaustive
action categories for training is impractical due to the dynamic diversity of
human activities, necessitating models that detect and segment
out-of-distribution actions without manual annotation. To address this issue,
we formally define the open-world action segmentation problem and propose a
structured framework for detecting and segmenting unseen actions. Our framework
introduces three key innovations: 1) an Enhanced Pyramid Graph Convolutional
Network (EPGCN) with a novel decoder module for robust spatiotemporal feature
upsampling. 2) Mixup-based training to synthesize out-of-distribution data,
eliminating reliance on manual annotations. 3) A novel Temporal Clustering loss
that groups in-distribution actions while distancing out-of-distribution
samples.
  We evaluate our framework on two challenging human-object interaction
recognition datasets: Bimanual Actions and 2 Hands and Object (H2O) datasets.
Experimental results demonstrate significant improvements over state-of-the-art
action segmentation models across multiple open-set evaluation metrics,
achieving 16.9% and 34.6% relative gains in open-set segmentation (F1@50) and
out-of-distribution detection performances (AUROC), respectively. Additionally,
we conduct an in-depth ablation study to assess the impact of each proposed
component, identifying the optimal framework configuration for open-world
action segmentation.

</details>


### [95] [LD-RPS: Zero-Shot Unified Image Restoration via Latent Diffusion Recurrent Posterior Sampling](https://arxiv.org/abs/2507.00790)
*Huaqiu Li,Yong Wang,Tongwen Huang,Hailang Huang,Haoqian Wang,Xiangxiang Chu*

Main category: cs.CV

Relevance: 40.0

TL;DR: 提出了一种基于预训练潜在扩散模型的统一图像恢复方法，无需配对数据集，通过循环后验采样实现多任务通用性。


<details>
  <summary>Details</summary>
Motivation: 解决现有方法在通用性和数据集依赖上的局限性。

Method: 利用预训练扩散模型和轻量级模块进行语义先验提取和循环细化。

Result: 在实验中优于现有方法，验证了有效性和鲁棒性。

Conclusion: 该方法为统一图像恢复提供了高效且通用的解决方案。

Abstract: Unified image restoration is a significantly challenging task in low-level
vision. Existing methods either make tailored designs for specific tasks,
limiting their generalizability across various types of degradation, or rely on
training with paired datasets, thereby suffering from closed-set constraints.
To address these issues, we propose a novel, dataset-free, and unified approach
through recurrent posterior sampling utilizing a pretrained latent diffusion
model. Our method incorporates the multimodal understanding model to provide
sematic priors for the generative model under a task-blind condition.
Furthermore, it utilizes a lightweight module to align the degraded input with
the generated preference of the diffusion model, and employs recurrent
refinement for posterior sampling. Extensive experiments demonstrate that our
method outperforms state-of-the-art methods, validating its effectiveness and
robustness. Our code and data will be available at
https://github.com/AMAP-ML/LD-RPS.

</details>


### [96] [TRACE: Temporally Reliable Anatomically-Conditioned 3D CT Generation with Enhanced Efficiency](https://arxiv.org/abs/2507.00802)
*Minye Shao,Xingyu Miao,Haoran Duan,Zeyu Wang,Jingkun Chen,Yawen Huang,Xian Wu,Jingjing Deng,Yang Long,Yefeng Zheng*

Main category: cs.CV

Relevance: 40.0

TL;DR: TRACE是一个基于2D多模态条件扩散的框架，用于生成具有时空对齐的3D医学图像，解决了现有方法在解剖保真度、轴向长度和计算成本上的限制。


<details>
  <summary>Details</summary>
Motivation: 当前3D医学图像生成方法存在解剖保真度低、轴向长度受限和计算成本高的问题，限制了其在资源有限地区的应用。

Method: TRACE通过将2D切片建模为视频帧对，结合分割先验和放射学报告进行解剖对齐，并利用光流保持时间一致性。推理时采用重叠帧策略生成灵活长度的序列。

Result: 实验表明，TRACE在计算效率和保持解剖保真度与时空一致性之间取得了平衡。

Conclusion: TRACE为3D医学图像生成提供了一种高效且可靠的解决方案。

Abstract: 3D medical image generation is essential for data augmentation and patient
privacy, calling for reliable and efficient models suited for clinical
practice. However, current methods suffer from limited anatomical fidelity,
restricted axial length, and substantial computational cost, placing them
beyond reach for regions with limited resources and infrastructure. We
introduce TRACE, a framework that generates 3D medical images with
spatiotemporal alignment using a 2D multimodal-conditioned diffusion approach.
TRACE models sequential 2D slices as video frame pairs, combining segmentation
priors and radiology reports for anatomical alignment, incorporating optical
flow to sustain temporal coherence. During inference, an overlapping-frame
strategy links frame pairs into a flexible length sequence, reconstructed into
a spatiotemporally and anatomically aligned 3D volume. Experimental results
demonstrate that TRACE effectively balances computational efficiency with
preserving anatomical fidelity and spatiotemporal consistency. Code is
available at: https://github.com/VinyehShaw/TRACE.

</details>


### [97] [High-Frequency Semantics and Geometric Priors for End-to-End Detection Transformers in Challenging UAV Imagery](https://arxiv.org/abs/2507.00825)
*Hongxing Peng,Lide Chen,Hui Zhu,Yan Chen*

Main category: cs.CV

Relevance: 40.0

TL;DR: HEGS-DETR是一个针对无人机图像检测的改进Transformer框架，通过高频增强语义网络、高效小目标金字塔策略等模块，显著提升了小目标和密集场景的检测性能。


<details>
  <summary>Details</summary>
Motivation: 无人机图像检测面临小目标、高密度分布和复杂背景等挑战，现有方法依赖手工组件且泛化能力有限，因此需要一种更高效的端到端解决方案。

Method: 提出HEGS-DETR框架，包括高频增强语义网络（HFESNet）、高效小目标金字塔（ESOP）、选择性查询重收集（SQR）和几何感知位置编码（GAPE）模块。

Result: 在VisDrone数据集上，HEGS-DETR比基线模型AP50提升5.1%，AP提升3.8%，同时保持实时速度和减少参数数量。

Conclusion: HEGS-DETR通过针对性设计有效解决了无人机图像检测的挑战，显著提升了性能。

Abstract: Unmanned Aerial Vehicle-based Object Detection (UAV-OD) faces substantial
challenges, including small target sizes, high-density distributions, and
cluttered backgrounds in UAV imagery. Current algorithms often depend on
hand-crafted components like anchor boxes, which demand fine-tuning and exhibit
limited generalization, and Non-Maximum Suppression (NMS), which is
threshold-sensitive and prone to misclassifying dense objects. These generic
architectures thus struggle to adapt to aerial imaging characteristics,
resulting in performance limitations. Moreover, emerging end-to-end frameworks
have yet to effectively mitigate these aerial-specific challenges.To address
these issues, we propose HEGS-DETR, a comprehensively enhanced, real-time
Detection Transformer framework tailored for UAVs. First, we introduce the
High-Frequency Enhanced Semantics Network (HFESNet) as a novel backbone.
HFESNet preserves critical high-frequency spatial details to extract robust
semantic features, thereby improving discriminative capability for small and
occluded targets in complex backgrounds. Second, our Efficient Small Object
Pyramid (ESOP) strategy strategically fuses high-resolution feature maps with
minimal computational overhead, significantly boosting small object detection.
Finally, the proposed Selective Query Recollection (SQR) and Geometry-Aware
Positional Encoding (GAPE) modules enhance the detector's decoder stability and
localization accuracy, effectively optimizing bounding boxes and providing
explicit spatial priors for dense scenes. Experiments on the VisDrone dataset
demonstrate that HEGS-DETR achieves a 5.1\% AP$_{50}$ and 3.8\% AP increase
over the baseline, while maintaining real-time speed and reducing parameter
count by 4M.

</details>


### [98] [UAVD-Mamba: Deformable Token Fusion Vision Mamba for Multimodal UAV Detection](https://arxiv.org/abs/2507.00849)
*Wei Li,Jiaman Tang,Yang Li,Beihao Xia,Ligang Tan,Hongmao Qin*

Main category: cs.CV

Relevance: 40.0

TL;DR: 提出了一种基于Mamba架构的多模态无人机目标检测框架UAVD-Mamba，通过改进几何适应性和多模态特征互补性，显著提升了检测性能。


<details>
  <summary>Details</summary>
Motivation: 无人机目标检测在遮挡、小目标和形状不规则等挑战下需要更鲁棒和高效的方法。Mamba在多模态图像融合中表现出潜力，因此被用于此任务。

Method: 设计了Deformable Token Mamba Block（DTMB）生成可变形令牌，并分别处理RGB和红外模态。通过堆叠DTMB和多尺度特征融合模块（DNM）提升检测能力。

Result: 在DroneVehicle数据集上，mAP指标比基线方法OAFA提高了3.6%。

Conclusion: UAVD-Mamba通过结合Mamba架构和多模态特征融合，显著提升了无人机目标检测的性能。

Abstract: Unmanned Aerial Vehicle (UAV) object detection has been widely used in
traffic management, agriculture, emergency rescue, etc. However, it faces
significant challenges, including occlusions, small object sizes, and irregular
shapes. These challenges highlight the necessity for a robust and efficient
multimodal UAV object detection method. Mamba has demonstrated considerable
potential in multimodal image fusion. Leveraging this, we propose UAVD-Mamba, a
multimodal UAV object detection framework based on Mamba architectures. To
improve geometric adaptability, we propose the Deformable Token Mamba Block
(DTMB) to generate deformable tokens by incorporating adaptive patches from
deformable convolutions alongside normal patches from normal convolutions,
which serve as the inputs to the Mamba Block. To optimize the multimodal
feature complementarity, we design two separate DTMBs for the RGB and infrared
(IR) modalities, with the outputs from both DTMBs integrated into the Mamba
Block for feature extraction and into the Fusion Mamba Block for feature
fusion. Additionally, to improve multiscale object detection, especially for
small objects, we stack four DTMBs at different scales to produce multiscale
feature representations, which are then sent to the Detection Neck for Mamba
(DNM). The DNM module, inspired by the YOLO series, includes modifications to
the SPPF and C3K2 of YOLOv11 to better handle the multiscale features. In
particular, we employ cross-enhanced spatial attention before the DTMB and
cross-channel attention after the Fusion Mamba Block to extract more
discriminative features. Experimental results on the DroneVehicle dataset show
that our method outperforms the baseline OAFA method by 3.6% in the mAP metric.
Codes will be released at https://github.com/GreatPlum-hnu/UAVD-Mamba.git.

</details>


### [99] [GaussianVLM: Scene-centric 3D Vision-Language Models using Language-aligned Gaussian Splats for Embodied Reasoning and Beyond](https://arxiv.org/abs/2507.00886)
*Anna-Maria Halacheva,Jan-Nico Zaech,Xi Wang,Danda Pani Paudel,Luc Van Gool*

Main category: cs.CV

Relevance: 40.0

TL;DR: 提出了一种基于3D高斯飞溅的场景中心3D视觉语言模型（VLM），通过语言和任务感知的场景表示，直接嵌入语言特征到3D场景中，解决了现有方法对物体检测器的依赖问题。


<details>
  <summary>Details</summary>
Motivation: 当前多模态语言模型在3D场景理解中依赖物体检测器，导致处理瓶颈和分类灵活性受限。

Method: 采用语言和任务感知的场景表示，将语言特征嵌入3D高斯飞溅场景中，并通过双稀疏器生成紧凑的任务相关令牌。

Result: 该模型在域外设置下将现有3D VLM性能提升了五倍。

Conclusion: 提出的方法显著提升了3D场景理解的性能，展示了强大的泛化能力。

Abstract: As multimodal language models advance, their application to 3D scene
understanding is a fast-growing frontier, driving the development of 3D
Vision-Language Models (VLMs). Current methods show strong dependence on object
detectors, introducing processing bottlenecks and limitations in taxonomic
flexibility. To address these limitations, we propose a scene-centric 3D VLM
for 3D Gaussian splat scenes that employs language- and task-aware scene
representations. Our approach directly embeds rich linguistic features into the
3D scene representation by associating language with each Gaussian primitive,
achieving early modality alignment. To process the resulting dense
representations, we introduce a dual sparsifier that distills them into
compact, task-relevant tokens via task-guided and location-guided pathways,
producing sparse, task-aware global and local scene tokens. Notably, we present
the first Gaussian splatting-based VLM, leveraging photorealistic 3D
representations derived from standard RGB images, demonstrating strong
generalization: it improves performance of prior 3D VLM five folds, in
out-of-the-domain settings.

</details>


### [100] [UniGlyph: Unified Segmentation-Conditioned Diffusion for Precise Visual Text Synthesis](https://arxiv.org/abs/2507.00992)
*Yuanrui Wang,Cong Han,YafeiLi,Zhipeng Jin,Xiawei Li,SiNan Du,Wen Tao,Yi Yang,shuanglong li,Chun Yuan,Liu Lin*

Main category: cs.CV

Relevance: 40.0

TL;DR: 提出了一种基于分割引导的文本到图像生成框架，通过像素级视觉文本掩码作为统一条件输入，解决了现有方法在视觉文本渲染中的问题。


<details>
  <summary>Details</summary>
Motivation: 现有方法在视觉文本渲染中存在字形模糊、语义漂移和风格控制有限等问题，且多分支设计增加了模型复杂度和降低了灵活性。

Method: 采用分割引导框架，包括微调的双语分割模型用于精确提取文本掩码，以及优化的扩散模型结合自适应字形条件和区域特定损失。

Result: 在AnyText基准测试中表现最佳，显著优于现有方法，并在新提出的GlyphMM-benchmark和MiniText-benchmark中大幅领先。

Conclusion: 该方法在视觉文本渲染中表现出色，特别是在小文本渲染和复杂布局保留方面，验证了其强大的泛化能力和部署准备度。

Abstract: Text-to-image generation has greatly advanced content creation, yet
accurately rendering visual text remains a key challenge due to blurred glyphs,
semantic drift, and limited style control. Existing methods often rely on
pre-rendered glyph images as conditions, but these struggle to retain original
font styles and color cues, necessitating complex multi-branch designs that
increase model overhead and reduce flexibility. To address these issues, we
propose a segmentation-guided framework that uses pixel-level visual text masks
-- rich in glyph shape, color, and spatial detail -- as unified conditional
inputs. Our method introduces two core components: (1) a fine-tuned bilingual
segmentation model for precise text mask extraction, and (2) a streamlined
diffusion model augmented with adaptive glyph conditioning and a
region-specific loss to preserve textual fidelity in both content and style.
Our approach achieves state-of-the-art performance on the AnyText benchmark,
significantly surpassing prior methods in both Chinese and English settings. To
enable more rigorous evaluation, we also introduce two new benchmarks:
GlyphMM-benchmark for testing layout and glyph consistency in complex
typesetting, and MiniText-benchmark for assessing generation quality in
small-scale text regions. Experimental results show that our model outperforms
existing methods by a large margin in both scenarios, particularly excelling at
small text rendering and complex layout preservation, validating its strong
generalization and deployment readiness.

</details>


### [101] [Towards 3D Semantic Image Synthesis for Medical Imaging](https://arxiv.org/abs/2507.00206)
*Wenwu Tang,Khaled Seyam,Bin Yang*

Main category: eess.IV

Relevance: 40.0

TL;DR: 论文提出Med-LSDM，一种直接在3D领域中操作的潜在语义扩散模型，用于生成合成医学图像以解决数据隐私和可用性问题。


<details>
  <summary>Details</summary>
Motivation: 医学领域数据获取困难且隐私保护严格，限制了机器学习在医学影像中的应用。

Method: Med-LSDM在预训练的VQ-GAN潜在空间中应用扩散模型，生成3D语义图像，降低计算复杂度。

Result: 模型在Duke Breast数据集上表现优异，3D-FID得分为0.0054，Dice分数接近真实数据。

Conclusion: Med-LSDM生成的合成数据与真实数据差距小，适用于数据增强。

Abstract: In the medical domain, acquiring large datasets is challenging due to both
accessibility issues and stringent privacy regulations. Consequently, data
availability and privacy protection are major obstacles to applying machine
learning in medical imaging. To address this, our study proposes the Med-LSDM
(Latent Semantic Diffusion Model), which operates directly in the 3D domain and
leverages de-identified semantic maps to generate synthetic data as a method of
privacy preservation and data augmentation. Unlike many existing methods that
focus on generating 2D slices, Med-LSDM is designed specifically for 3D
semantic image synthesis, making it well-suited for applications requiring full
volumetric data. Med-LSDM incorporates a guiding mechanism that controls the 3D
image generation process by applying a diffusion model within the latent space
of a pre-trained VQ-GAN. By operating in the compressed latent space, the model
significantly reduces computational complexity while still preserving critical
3D spatial details. Our approach demonstrates strong performance in 3D semantic
medical image synthesis, achieving a 3D-FID score of 0.0054 on the conditional
Duke Breast dataset and similar Dice scores (0.70964) to those of real images
(0.71496). These results demonstrate that the synthetic data from our model
have a small domain gap with real data and are useful for data augmentation.

</details>


### [102] [Bridging Classical and Learning-based Iterative Registration through Deep Equilibrium Models](https://arxiv.org/abs/2507.00582)
*Yi Zhang,Yidong Zhao,Qian Tao*

Main category: eess.IV

Relevance: 40.0

TL;DR: 论文提出DEQReg，一种基于深度平衡模型（DEQ）的医学图像配准框架，解决了传统优化方法与学习展开方法在理论和实践上的问题。


<details>
  <summary>Details</summary>
Motivation: 传统优化方法收敛但计算复杂，学习展开方法缺乏理论保证且内存消耗大。DEQReg旨在结合两者的优势。

Method: DEQReg将配准问题转化为平衡寻求问题，利用DEQ模型实现恒定内存消耗和理论上的无限迭代。

Result: 在脑MRI和肺CT数据集上，DEQReg性能与现有方法相当，但内存消耗显著降低，且能稳定收敛。

Conclusion: DEQReg填补了传统优化与学习展开方法之间的空白，提供了更高效稳定的配准解决方案。

Abstract: Deformable medical image registration is traditionally formulated as an
optimization problem. While classical methods solve this problem iteratively,
recent learning-based approaches use recurrent neural networks (RNNs) to mimic
this process by unrolling the prediction of deformation fields in a fixed
number of steps. However, classical methods typically converge after sufficient
iterations, but learning-based unrolling methods lack a theoretical convergence
guarantee and show instability empirically. In addition, unrolling methods have
a practical bottleneck at training time: GPU memory usage grows linearly with
the unrolling steps due to backpropagation through time (BPTT). To address both
theoretical and practical challenges, we propose DEQReg, a novel registration
framework based on Deep Equilibrium Models (DEQ), which formulates registration
as an equilibrium-seeking problem, establishing a natural connection between
classical optimization and learning-based unrolling methods. DEQReg maintains
constant memory usage, enabling theoretically unlimited iteration steps.
Through extensive evaluation on the public brain MRI and lung CT datasets, we
show that DEQReg can achieve competitive registration performance, while
substantially reducing memory consumption compared to state-of-the-art
unrolling methods. We also reveal an intriguing phenomenon: the performance of
existing unrolling methods first increases slightly then degrades irreversibly
when the inference steps go beyond the training configuration. In contrast,
DEQReg achieves stable convergence with its inbuilt equilibrium-seeking
mechanism, bridging the gap between classical optimization-based and modern
learning-based registration methods.

</details>


### [103] [VQ-VLA: Improving Vision-Language-Action Models via Scaling Vector-Quantized Action Tokenizers](https://arxiv.org/abs/2507.01016)
*Yating Wang,Haoyi Zhu,Mingyu Liu,Jiange Yang,Hao-Shu Fang,Tong He*

Main category: cs.RO

Relevance: 40.0

TL;DR: 提出了一种基于向量量化的动作标记器，利用大规模动作轨迹数据集，显著提升推理速度和动作输出的连贯性，并能零样本适应多种下游任务。


<details>
  <summary>Details</summary>
Motivation: 解决动作轨迹数据的稀疏性和域差距问题，利用合成数据提升模型在真实场景中的性能。

Method: 基于向量量化的动作标记器，利用大规模合成动作轨迹数据进行训练，并在模拟和真实机器人平台上验证。

Result: 随着合成数据量的增加，下游任务性能显著提升，在长时程场景中成功率提高30%。

Conclusion: 该动作标记器为实时智能系统提供了一种高效可靠的解决方案。

Abstract: In this paper, we introduce an innovative vector quantization based action
tokenizer built upon the largest-scale action trajectory dataset to date,
leveraging over 100 times more data than previous approaches. This extensive
dataset enables our tokenizer to capture rich spatiotemporal dynamics,
resulting in a model that not only accelerates inference but also generates
smoother and more coherent action outputs. Once trained, the tokenizer can be
seamlessly adapted to a wide range of downstream tasks in a zero-shot manner,
from short-horizon reactive behaviors to long-horizon planning. A key finding
of our work is that the domain gap between synthetic and real action
trajectories is marginal, allowing us to effectively utilize a vast amount of
synthetic data during training without compromising real-world performance. To
validate our approach, we conducted extensive experiments in both simulated
environments and on real robotic platforms. The results demonstrate that as the
volume of synthetic trajectory data increases, the performance of our tokenizer
on downstream tasks improves significantly-most notably, achieving up to a 30%
higher success rate on two real-world tasks in long-horizon scenarios. These
findings highlight the potential of our action tokenizer as a robust and
scalable solution for real-time embodied intelligence systems, paving the way
for more efficient and reliable robotic control in diverse application
domains.Project website: https://xiaoxiao0406.github.io/vqvla.github.io

</details>


### [104] [Catastrophic Forgetting Mitigation via Discrepancy-Weighted Experience Replay](https://arxiv.org/abs/2507.00042)
*Xinrun Xu,Jianwen Yang,Qiuhong Zhang,Zhanbiao Lian,Zhiming Ding,Shan Jiang*

Main category: cs.CV

Relevance: 30.0

TL;DR: 论文提出ER-EMU算法，通过自适应经验回放解决边缘模型在动态交通环境中因灾难性遗忘导致的知识丢失问题。


<details>
  <summary>Details</summary>
Motivation: 动态交通环境中的周期性变化（如昼夜、高峰时段）导致模型在适应新数据分布时遗忘旧知识，现有方法未能有效利用历史数据。

Method: ER-EMU结合FIFO经验缓冲区和基于域距离度量的经验选择算法（DDM-ES），利用MK-MMD量化域差异，优先选择与当前目标域差异大的历史数据。

Result: 在Bellevue交通视频数据集上的实验表明，ER-EMU显著提升了多种云边协同目标检测框架的性能。

Conclusion: ER-EMU通过自适应经验回放有效缓解灾难性遗忘，提升模型在动态环境中的适应能力。

Abstract: Continually adapting edge models in cloud-edge collaborative object detection
for traffic monitoring suffers from catastrophic forgetting, where models lose
previously learned knowledge when adapting to new data distributions. This is
especially problematic in dynamic traffic environments characterised by
periodic variations (e.g., day/night, peak hours), where past knowledge remains
valuable. Existing approaches like experience replay and visual prompts offer
some mitigation, but struggle to effectively prioritize and leverage historical
data for optimal knowledge retention and adaptation. Specifically, simply
storing and replaying all historical data can be inefficient, while treating
all historical experiences as equally important overlooks their varying
relevance to the current domain. This paper proposes ER-EMU, an edge model
update algorithm based on adaptive experience replay, to address these
limitations. ER-EMU utilizes a limited-size experience buffer managed using a
First-In-First-Out (FIFO) principle, and a novel Domain Distance Metric-based
Experience Selection (DDM-ES) algorithm. DDM-ES employs the multi-kernel
maximum mean discrepancy (MK-MMD) to quantify the dissimilarity between target
domains, prioritizing the selection of historical data that is most dissimilar
to the current target domain. This ensures training diversity and facilitates
the retention of knowledge from a wider range of past experiences, while also
preventing overfitting to the new domain. The experience buffer is also updated
using a simple random sampling strategy to maintain a balanced representation
of previous domains. Experiments on the Bellevue traffic video dataset,
involving repeated day/night cycles, demonstrate that ER-EMU consistently
improves the performance of several state-of-the-art cloud-edge collaborative
object detection frameworks.

</details>


### [105] [MR-CLIP: Efficient Metadata-Guided Learning of MRI Contrast Representations](https://arxiv.org/abs/2507.00043)
*Mehmet Yigit Avci,Pedro Borges,Paul Wright,Mehmet Yigitsoy,Sebastien Ourselin,Jorge Cardoso*

Main category: cs.CV

Relevance: 30.0

TL;DR: MR-CLIP是一个多模态对比学习框架，用于学习MRI图像的对比感知表示，无需依赖手动标签。


<details>
  <summary>Details</summary>
Motivation: 解决MRI图像对比识别中因DICOM元数据不完整、噪声或不一致而导致的挑战，以支持高级临床应用。

Method: 提出MR-CLIP框架，通过对比学习对齐MRI图像与DICOM元数据，学习对比感知表示。

Result: 在跨模态检索和对比分类任务中表现优异，展示了其可扩展性和临床应用潜力。

Conclusion: MR-CLIP为MRI图像提供了一种无需手动标签的对比感知表示学习方法，具有广泛的应用前景。

Abstract: Accurate interpretation of Magnetic Resonance Imaging scans in clinical
systems is based on a precise understanding of image contrast. This contrast is
primarily governed by acquisition parameters, such as echo time and repetition
time, which are stored in the DICOM metadata. To simplify contrast
identification, broad labels such as T1-weighted or T2-weighted are commonly
used, but these offer only a coarse approximation of the underlying acquisition
settings. In many real-world datasets, such labels are entirely missing,
leaving raw acquisition parameters as the only indicators of contrast. Adding
to this challenge, the available metadata is often incomplete, noisy, or
inconsistent. The lack of reliable and standardized metadata complicates tasks
such as image interpretation, retrieval, and integration into clinical
workflows. Furthermore, robust contrast-aware representations are essential to
enable more advanced clinical applications, such as achieving
modality-invariant representations and data harmonization. To address these
challenges, we propose MR-CLIP, a multimodal contrastive learning framework
that aligns MR images with their DICOM metadata to learn contrast-aware
representations, without relying on manual labels. Trained on a diverse
clinical dataset that spans various scanners and protocols, MR-CLIP captures
contrast variations across acquisitions and within scans, enabling
anatomy-invariant representations. We demonstrate its effectiveness in
cross-modal retrieval and contrast classification, highlighting its scalability
and potential for further clinical applications. The code and weights are
publicly available at https://github.com/myigitavci/MR-CLIP.

</details>


### [106] [SelvaBox: A high-resolution dataset for tropical tree crown detection](https://arxiv.org/abs/2507.00170)
*Hugo Baudchon,Arthur Ouaknine,Martin Weiss,Mélisande Teng,Thomas R. Walla,Antoine Caron-Guay,Christopher Pal,Etienne Laliberté*

Main category: cs.CV

Relevance: 30.0

TL;DR: 介绍了SelvaBox，一个用于热带树冠检测的最大开放数据集，包含83,000个标记树冠，显著提升了检测精度和零样本性能。


<details>
  <summary>Details</summary>
Motivation: 热带树冠检测对研究生态系统至关重要，但现有数据集稀缺，阻碍了模型开发。

Method: 使用高分辨率无人机图像构建SelvaBox数据集，并通过多分辨率管道进行模型训练和评估。

Result: 高分辨率输入提升检测精度；SelvaBox训练的模型在零样本检测中表现优异，联合训练后排名领先。

Conclusion: SelvaBox为热带树冠检测提供了重要资源，显著推动了该领域的研究。

Abstract: Detecting individual tree crowns in tropical forests is essential to study
these complex and crucial ecosystems impacted by human interventions and
climate change. However, tropical crowns vary widely in size, structure, and
pattern and are largely overlapping and intertwined, requiring advanced remote
sensing methods applied to high-resolution imagery. Despite growing interest in
tropical tree crown detection, annotated datasets remain scarce, hindering
robust model development. We introduce SelvaBox, the largest open-access
dataset for tropical tree crown detection in high-resolution drone imagery. It
spans three countries and contains more than 83,000 manually labeled crowns -
an order of magnitude larger than all previous tropical forest datasets
combined. Extensive benchmarks on SelvaBox reveal two key findings: (1)
higher-resolution inputs consistently boost detection accuracy; and (2) models
trained exclusively on SelvaBox achieve competitive zero-shot detection
performance on unseen tropical tree crown datasets, matching or exceeding
competing methods. Furthermore, jointly training on SelvaBox and three other
datasets at resolutions from 3 to 10 cm per pixel within a unified
multi-resolution pipeline yields a detector ranking first or second across all
evaluated datasets. Our dataset, code, and pre-trained weights are made public.

</details>


### [107] [Developing Lightweight DNN Models With Limited Data For Real-Time Sign Language Recognition](https://arxiv.org/abs/2507.00248)
*Nikita Nikitin,Eugene Fomin*

Main category: cs.CV

Relevance: 30.0

TL;DR: 提出了一种基于轻量级DNN的实时手语识别框架，解决了数据稀缺、高计算成本和帧率差异等问题，实现了低延迟高精度的分类。


<details>
  <summary>Details</summary>
Motivation: 解决手语识别中的数据稀缺、高计算成本和帧率差异等挑战。

Method: 通过编码手语特定参数（如手形、手掌方向、动作和位置）为向量化输入，并利用MediaPipe提取关键点，设计了一个优化至10MB以下的DNN架构。

Result: 在边缘设备上实现了343种手语的分类，延迟低于10ms，准确率达92%。

Conclusion: 该框架在实时手语识别中表现出色，已集成到实际应用中。

Abstract: We present a novel framework for real-time sign language recognition using
lightweight DNNs trained on limited data. Our system addresses key challenges
in sign language recognition, including data scarcity, high computational
costs, and discrepancies in frame rates between training and inference
environments. By encoding sign language specific parameters, such as handshape,
palm orientation, movement, and location into vectorized inputs, and leveraging
MediaPipe for landmark extraction, we achieve highly separable input data
representations. Our DNN architecture, optimized for sub 10MB deployment,
enables accurate classification of 343 signs with less than 10ms latency on
edge devices. The data annotation platform 'slait data' facilitates structured
labeling and vector extraction. Our model achieved 92% accuracy in isolated
sign recognition and has been integrated into the 'slait ai' web application,
where it demonstrates stable inference.

</details>


### [108] [GazeTarget360: Towards Gaze Target Estimation in 360-Degree for Robot Perception](https://arxiv.org/abs/2507.00253)
*Zhuangzhuang Dai,Vincent Gbouna Zakka,Luis J. Manso,Chen Li*

Main category: cs.CV

Relevance: 30.0

TL;DR: 提出了一种名为GazeTarget360的系统，用于从图像中估计360度视线目标，适用于广义视觉场景。


<details>
  <summary>Details</summary>
Motivation: 机器人理解人类视线目标是实现人机交互中注意力估计和运动预测等下游任务的关键。现有方法在视线目标定位上存在局限，如无法有效处理背景信息或预测视线离开相机的情况。

Method: 系统整合了眼神接触检测器、预训练视觉编码器和多尺度融合解码器的条件推理引擎。

Result: 交叉验证结果表明，GazeTarget360能在未见场景中准确预测视线目标，是首个高效且可部署的实时相机镜头预测系统。

Conclusion: GazeTarget360为解决广义视觉场景中的视线目标预测问题提供了高效且可靠的解决方案。

Abstract: Enabling robots to understand human gaze target is a crucial step to allow
capabilities in downstream tasks, for example, attention estimation and
movement anticipation in real-world human-robot interactions. Prior works have
addressed the in-frame target localization problem with data-driven approaches
by carefully removing out-of-frame samples. Vision-based gaze estimation
methods, such as OpenFace, do not effectively absorb background information in
images and cannot predict gaze target in situations where subjects look away
from the camera. In this work, we propose a system to address the problem of
360-degree gaze target estimation from an image in generalized visual scenes.
The system, named GazeTarget360, integrates conditional inference engines of an
eye-contact detector, a pre-trained vision encoder, and a multi-scale-fusion
decoder. Cross validation results show that GazeTarget360 can produce accurate
and reliable gaze target predictions in unseen scenarios. This makes a
first-of-its-kind system to predict gaze targets from realistic camera footage
which is highly efficient and deployable. Our source code is made publicly
available at: https://github.com/zdai257/DisengageNet.

</details>


### [109] [Self-Supervised Multiview Xray Matching](https://arxiv.org/abs/2507.00287)
*Mohamad Dabboussi,Malo Huard,Yann Gousseau,Pietro Gori*

Main category: cs.CV

Relevance: 30.0

TL;DR: 提出了一种自监督方法，通过生成多视角X射线之间的对应矩阵，无需人工标注，提升了多视角骨折检测的性能。


<details>
  <summary>Details</summary>
Motivation: 当前AI方法在多视角X射线分析中难以建立稳健的对应关系，影响了临床诊断的准确性。

Method: 利用数字重建放射影像（DRR）自动生成多视角对应矩阵，结合基于Transformer的训练方法。

Result: 在合成和真实X射线数据集上的实验表明，该方法提升了多视角骨折分类的性能。

Conclusion: 自监督学习多视角对应关系可作为预训练策略，提升真实数据的多视角分析能力。

Abstract: Accurate interpretation of multi-view radiographs is crucial for diagnosing
fractures, muscular injuries, and other anomalies. While significant advances
have been made in AI-based analysis of single images, current methods often
struggle to establish robust correspondences between different X-ray views, an
essential capability for precise clinical evaluations. In this work, we present
a novel self-supervised pipeline that eliminates the need for manual annotation
by automatically generating a many-to-many correspondence matrix between
synthetic X-ray views. This is achieved using digitally reconstructed
radiographs (DRR), which are automatically derived from unannotated CT volumes.
Our approach incorporates a transformer-based training phase to accurately
predict correspondences across two or more X-ray views. Furthermore, we
demonstrate that learning correspondences among synthetic X-ray views can be
leveraged as a pretraining strategy to enhance automatic multi-view fracture
detection on real data. Extensive evaluations on both synthetic and real X-ray
datasets show that incorporating correspondences improves performance in
multi-view fracture classification.

</details>


### [110] [Reducing Variability of Multiple Instance Learning Methods for Digital Pathology](https://arxiv.org/abs/2507.00292)
*Ali Mammadov,Loïc Le Folgoc,Guillaume Hocquet,Pietro Gori*

Main category: cs.CV

Relevance: 30.0

TL;DR: 提出了一种多保真度模型融合策略，用于减少多实例学习（MIL）方法在数字病理学中的性能波动。


<details>
  <summary>Details</summary>
Motivation: 数字病理学中的全切片图像（WSIs）分类面临性能波动大的问题，影响方法比较的可靠性。

Method: 通过训练多个模型并基于验证分数融合最稳定和有前景的模型，以减少性能波动。

Result: 在多个数据集和MIL方法上验证了该策略的有效性，显著降低了性能波动。

Conclusion: 该方法提升了MIL方法的稳定性和可复现性，同时保持计算效率。

Abstract: Digital pathology has revolutionized the field by enabling the digitization
of tissue samples into whole slide images (WSIs). However, the high resolution
and large size of WSIs present significant challenges when it comes to applying
Deep Learning models. As a solution, WSIs are often divided into smaller
patches with a global label (\textit{i.e., diagnostic}) per slide, instead of a
(too) costly pixel-wise annotation. By treating each slide as a bag of patches,
Multiple Instance Learning (MIL) methods have emerged as a suitable solution
for WSI classification. A major drawback of MIL methods is their high
variability in performance across different runs, which can reach up to 10-15
AUC points on the test set, making it difficult to compare different MIL
methods reliably. This variability mainly comes from three factors: i) weight
initialization, ii) batch (shuffling) ordering, iii) and learning rate. To
address that, we introduce a Multi-Fidelity, Model Fusion strategy for MIL
methods. We first train multiple models for a few epochs and average the most
stable and promising ones based on validation scores. This approach can be
applied to any existing MIL model to reduce performance variability. It also
simplifies hyperparameter tuning and improves reproducibility while maintaining
computational efficiency. We extensively validate our approach on WSI
classification tasks using 2 different datasets, 3 initialization strategies
and 5 MIL methods, for a total of more than 2000 experiments.

</details>


### [111] [Training for X-Ray Vision: Amodal Segmentation, Amodal Content Completion, and View-Invariant Object Representation from Multi-Camera Video](https://arxiv.org/abs/2507.00339)
*Alexander Moore,Amar Saini,Kylie Cancilla,Doug Poland,Carmen Carrano*

Main category: cs.CV

Relevance: 30.0

TL;DR: 论文介绍了MOVi-MC-AC数据集，这是最大的amodal分割和首个amodal内容数据集，支持多摄像头场景下的物体识别与跟踪。


<details>
  <summary>Details</summary>
Motivation: 现有数据缺乏多摄像头共享场景的维度，限制了amodal分割和内容完成的研究。

Method: 通过模拟多摄像头视频场景，提供一致的物体ID和amodal内容标签。

Result: 数据集包含580万物体实例，首次提供真实amodal内容标签。

Conclusion: MOVi-MC-AC为计算机视觉领域提供了新的复杂性和规模。

Abstract: Amodal segmentation and amodal content completion require using object priors
to estimate occluded masks and features of objects in complex scenes. Until
now, no data has provided an additional dimension for object context: the
possibility of multiple cameras sharing a view of a scene. We introduce
MOVi-MC-AC: Multiple Object Video with Multi-Cameras and Amodal Content, the
largest amodal segmentation and first amodal content dataset to date. Cluttered
scenes of generic household objects are simulated in multi-camera video.
MOVi-MC-AC contributes to the growing literature of object detection, tracking,
and segmentation by including two new contributions to the deep learning for
computer vision world. Multiple Camera (MC) settings where objects can be
identified and tracked between various unique camera perspectives are rare in
both synthetic and real-world video. We introduce a new complexity to synthetic
video by providing consistent object ids for detections and segmentations
between both frames and multiple cameras each with unique features and motion
patterns on a single scene. Amodal Content (AC) is a reconstructive task in
which models predict the appearance of target objects through occlusions. In
the amodal segmentation literature, some datasets have been released with
amodal detection, tracking, and segmentation labels. While other methods rely
on slow cut-and-paste schemes to generate amodal content pseudo-labels, they do
not account for natural occlusions present in the modal masks. MOVi-MC-AC
provides labels for ~5.8 million object instances, setting a new maximum in the
amodal dataset literature, along with being the first to provide ground-truth
amodal content. The full dataset is available at
https://huggingface.co/datasets/Amar-S/MOVi-MC-AC ,

</details>


### [112] [GDGS: 3D Gaussian Splatting Via Geometry-Guided Initialization And Dynamic Density Control](https://arxiv.org/abs/2507.00363)
*Xingjun Wang,Lianlei Shan*

Main category: cs.CV

Relevance: 30.0

TL;DR: 提出了一种改进3D高斯泼溅（3DGS）的方法，解决了初始化、优化和密度控制的挑战，实现了高保真实时渲染。


<details>
  <summary>Details</summary>
Motivation: 3DGS依赖准确的初始化且缺乏有效的密度控制机制，限制了其性能。本文旨在解决这些问题。

Method: 1. 几何引导的初始化预测高斯参数；2. 表面对齐优化策略；3. 动态自适应密度控制机制。

Result: 方法在复杂场景中实现了高保真实时渲染，视觉质量显著提升，优于或媲美现有技术。

Conclusion: 提出的方法有效解决了3DGS的挑战，提升了渲染质量和效率。

Abstract: We propose a method to enhance 3D Gaussian Splatting (3DGS)~\cite{Kerbl2023},
addressing challenges in initialization, optimization, and density control.
Gaussian Splatting is an alternative for rendering realistic images while
supporting real-time performance, and it has gained popularity due to its
explicit 3D Gaussian representation. However, 3DGS heavily depends on accurate
initialization and faces difficulties in optimizing unstructured Gaussian
distributions into ordered surfaces, with limited adaptive density control
mechanism proposed so far. Our first key contribution is a geometry-guided
initialization to predict Gaussian parameters, ensuring precise placement and
faster convergence. We then introduce a surface-aligned optimization strategy
to refine Gaussian placement, improving geometric accuracy and aligning with
the surface normals of the scene. Finally, we present a dynamic adaptive
density control mechanism that adjusts Gaussian density based on regional
complexity, for visual fidelity. These innovations enable our method to achieve
high-fidelity real-time rendering and significant improvements in visual
quality, even in complex scenes. Our method demonstrates comparable or superior
results to state-of-the-art methods, rendering high-fidelity images in real
time.

</details>


### [113] [An Improved U-Net Model for Offline handwriting signature denoising](https://arxiv.org/abs/2507.00365)
*Wanghui Xiao*

Main category: cs.CV

Relevance: 30.0

TL;DR: 该论文提出了一种基于改进U-net结构的签名手写去噪模型，通过离散小波变换和PCA变换增强去噪能力，显著优于传统方法。


<details>
  <summary>Details</summary>
Motivation: 手写签名在身份识别中具有重要作用，但样本常受干扰信息影响，影响识别效果。研究旨在提升签名识别系统的鲁棒性。

Method: 采用改进的U-net结构，结合离散小波变换和PCA变换，增强模型去噪能力。

Result: 实验表明，该模型在去噪效果上显著优于传统方法，提高了签名图像的清晰度和可读性。

Conclusion: 该模型为签名分析与识别提供了更可靠的技术支持。

Abstract: Handwriting signatures, as an important means of identity recognition, are
widely used in multiple fields such as financial transactions, commercial
contracts and personal affairs due to their legal effect and uniqueness. In
forensic science appraisals, the analysis of offline handwriting signatures
requires the appraiser to provide a certain number of signature samples, which
are usually derived from various historical contracts or archival materials.
However, the provided handwriting samples are often mixed with a large amount
of interfering information, which brings severe challenges to handwriting
identification work. This study proposes a signature handwriting denoising
model based on the improved U-net structure, aiming to enhance the robustness
of the signature recognition system. By introducing discrete wavelet transform
and PCA transform, the model's ability to suppress noise has been enhanced. The
experimental results show that this modelis significantly superior to the
traditional methods in denoising effect, can effectively improve the clarity
and readability of the signed images, and provide more reliable technical
support for signature analysis and recognition.

</details>


### [114] [PlantSegNeRF: A few-shot, cross-dataset method for plant 3D instance point cloud reconstruction via joint-channel NeRF with multi-view image instance matching](https://arxiv.org/abs/2507.00371)
*Xin Yang,Ruiming Du,Hanyang Huang,Jiayang Xie,Pengyao Xie,Leisen Fang,Ziyue Guo,Nanjun Jiang,Yu Jiang,Haiyan Cen*

Main category: cs.CV

Relevance: 30.0

TL;DR: 提出了一种名为PlantSegNeRF的新方法，用于从多视角RGB图像序列生成高精度植物器官点云，显著提升了分割精度和通用性。


<details>
  <summary>Details</summary>
Motivation: 现有植物点云器官分割技术在分辨率、精度和跨物种通用性方面存在局限，需要一种更高效的方法。

Method: 结合2D实例分割、实例匹配模块和实例NeRF，生成包含颜色、密度、语义和实例信息的隐式场景，最终转换为高精度点云。

Result: 在语义分割和实例分割任务中，PlantSegNeRF显著优于现有方法，平均提升16.1%至38.2%的指标。

Conclusion: PlantSegNeRF为植物表型研究提供了高质量3D数据，支持大规模模型开发。

Abstract: Organ segmentation of plant point clouds is a prerequisite for the
high-resolution and accurate extraction of organ-level phenotypic traits.
Although the fast development of deep learning has boosted much research on
segmentation of plant point clouds, the existing techniques for organ
segmentation still face limitations in resolution, segmentation accuracy, and
generalizability across various plant species. In this study, we proposed a
novel approach called plant segmentation neural radiance fields (PlantSegNeRF),
aiming to directly generate high-precision instance point clouds from
multi-view RGB image sequences for a wide range of plant species. PlantSegNeRF
performed 2D instance segmentation on the multi-view images to generate
instance masks for each organ with a corresponding ID. The multi-view instance
IDs corresponding to the same plant organ were then matched and refined using a
specially designed instance matching module. The instance NeRF was developed to
render an implicit scene, containing color, density, semantic and instance
information. The implicit scene was ultimately converted into high-precision
plant instance point clouds based on the volume density. The results proved
that in semantic segmentation of point clouds, PlantSegNeRF outperformed the
commonly used methods, demonstrating an average improvement of 16.1%, 18.3%,
17.8%, and 24.2% in precision, recall, F1-score, and IoU compared to the
second-best results on structurally complex datasets. More importantly,
PlantSegNeRF exhibited significant advantages in plant point cloud instance
segmentation tasks. Across all plant datasets, it achieved average improvements
of 11.7%, 38.2%, 32.2% and 25.3% in mPrec, mRec, mCov, mWCov, respectively.
This study extends the organ-level plant phenotyping and provides a
high-throughput way to supply high-quality 3D data for the development of
large-scale models in plant science.

</details>


### [115] [Efficient Depth- and Spatially-Varying Image Simulation for Defocus Deblur](https://arxiv.org/abs/2507.00372)
*Xinge Yang,Chuong Nguyen,Wenbin Wang,Kaizhang Kang,Wolfgang Heidrich,Xiaoxing Li*

Main category: cs.CV

Relevance: 30.0

TL;DR: 论文提出了一种高效且可扩展的数据集合成方法，用于解决大光圈相机因浅景深导致的模糊问题，无需依赖真实数据微调。


<details>
  <summary>Details</summary>
Motivation: 解决固定焦距相机（如智能眼镜）因光学像差和散焦特性导致的深度学习模型在真实场景中表现不佳的问题。

Method: 提出一种同时建模深度相关散焦和空间变化光学像差的数据集合成方法，解决计算复杂性和高质量RGB-D数据集稀缺问题。

Result: 实验表明，用低分辨率合成图像训练的网络能有效泛化到高分辨率（12MP）真实场景图像。

Conclusion: 该方法为固定焦距相机的图像处理提供了一种高效且可扩展的解决方案。

Abstract: Modern cameras with large apertures often suffer from a shallow depth of
field, resulting in blurry images of objects outside the focal plane. This
limitation is particularly problematic for fixed-focus cameras, such as those
used in smart glasses, where adding autofocus mechanisms is challenging due to
form factor and power constraints. Due to unmatched optical aberrations and
defocus properties unique to each camera system, deep learning models trained
on existing open-source datasets often face domain gaps and do not perform well
in real-world settings. In this paper, we propose an efficient and scalable
dataset synthesis approach that does not rely on fine-tuning with real-world
data. Our method simultaneously models depth-dependent defocus and spatially
varying optical aberrations, addressing both computational complexity and the
scarcity of high-quality RGB-D datasets. Experimental results demonstrate that
a network trained on our low resolution synthetic images generalizes
effectively to high resolution (12MP) real-world images across diverse scenes.

</details>


### [116] [Customizable ROI-Based Deep Image Compression](https://arxiv.org/abs/2507.00373)
*Ian Jin,Fanxin Xia,Feng Ding,Xinfeng Zhang,Meiqin Liu,Yao Zhao,Weisi Lin,Lili Meng*

Main category: cs.CV

Relevance: 30.0

TL;DR: 该论文提出了一种可定制的基于ROI的深度图像压缩方法，通过文本控制ROI定义和灵活的掩码机制，满足不同用户的需求。


<details>
  <summary>Details</summary>
Motivation: 随着用户多样性增加，现有的ROI图像压缩方法无法灵活定制ROI和平衡ROI与非ROI的重建质量。

Method: 1. 文本控制掩码获取（TMA）模块；2. 可定制值分配（CVA）机制；3. 潜在掩码注意力（LMA）模块。

Result: 实验证明该方法能有效支持ROI定制和重建质量平衡。

Conclusion: 该方法为ROI图像压缩提供了灵活性和可扩展性。

Abstract: Region of Interest (ROI)-based image compression optimizes bit allocation by
prioritizing ROI for higher-quality reconstruction. However, as the users
(including human clients and downstream machine tasks) become more diverse,
ROI-based image compression needs to be customizable to support various
preferences. For example, different users may define distinct ROI or require
different quality trade-offs between ROI and non-ROI. Existing ROI-based image
compression schemes predefine the ROI, making it unchangeable, and lack
effective mechanisms to balance reconstruction quality between ROI and non-ROI.
This work proposes a paradigm for customizable ROI-based deep image
compression. First, we develop a Text-controlled Mask Acquisition (TMA) module,
which allows users to easily customize their ROI for compression by just
inputting the corresponding semantic \emph{text}. It makes the encoder
controlled by text. Second, we design a Customizable Value Assign (CVA)
mechanism, which masks the non-ROI with a changeable extent decided by users
instead of a constant one to manage the reconstruction quality trade-off
between ROI and non-ROI. Finally, we present a Latent Mask Attention (LMA)
module, where the latent spatial prior of the mask and the latent
Rate-Distortion Optimization (RDO) prior of the image are extracted and fused
in the latent space, and further used to optimize the latent representation of
the source image. Experimental results demonstrate that our proposed
customizable ROI-based deep image compression paradigm effectively addresses
the needs of customization for ROI definition and mask acquisition as well as
the reconstruction quality trade-off management between the ROI and non-ROI.

</details>


### [117] [Learning Dense Feature Matching via Lifting Single 2D Image to 3D Space](https://arxiv.org/abs/2507.00392)
*Yingping Liang,Yutao Hu,Wenqi Shao,Ying Fu*

Main category: cs.CV

Relevance: 30.0

TL;DR: 提出了一种名为Lift to Match (L2M)的两阶段框架，通过将2D图像提升到3D空间，利用大规模单视图图像实现鲁棒的特征匹配。


<details>
  <summary>Details</summary>
Motivation: 现有特征匹配方法依赖稀缺且干净的多视图图像，限制了其在多样化场景中的泛化能力。传统特征编码器基于单视图2D图像训练，难以捕捉3D感知的对应关系。

Method: 1. 第一阶段：通过多视图图像合成和3D特征高斯表示学习3D感知特征编码器。2. 第二阶段：结合新视图渲染策略和大规模合成数据训练特征解码器。

Result: 在零样本评估基准上表现出优异的泛化能力。

Conclusion: L2M框架通过3D感知特征编码和大规模合成数据，实现了跨领域的鲁棒特征匹配。

Abstract: Feature matching plays a fundamental role in many computer vision tasks, yet
existing methods heavily rely on scarce and clean multi-view image collections,
which constrains their generalization to diverse and challenging scenarios.
Moreover, conventional feature encoders are typically trained on single-view 2D
images, limiting their capacity to capture 3D-aware correspondences. In this
paper, we propose a novel two-stage framework that lifts 2D images to 3D space,
named as \textbf{Lift to Match (L2M)}, taking full advantage of large-scale and
diverse single-view images. To be specific, in the first stage, we learn a
3D-aware feature encoder using a combination of multi-view image synthesis and
3D feature Gaussian representation, which injects 3D geometry knowledge into
the encoder. In the second stage, a novel-view rendering strategy, combined
with large-scale synthetic data generation from single-view images, is employed
to learn a feature decoder for robust feature matching, thus achieving
generalization across diverse domains. Extensive experiments demonstrate that
our method achieves superior generalization across zero-shot evaluation
benchmarks, highlighting the effectiveness of the proposed framework for robust
feature matching.

</details>


### [118] [DiGA3D: Coarse-to-Fine Diffusional Propagation of Geometry and Appearance for Versatile 3D Inpainting](https://arxiv.org/abs/2507.00429)
*Jingyi Pan,Dan Xu,Qiong Luo*

Main category: cs.CV

Relevance: 30.0

TL;DR: DiGA3D是一个统一的3D修复管道，利用扩散模型在多视图中传播一致的外观和几何信息，解决了单参考视图不鲁棒、外观和几何不一致的问题。


<details>
  <summary>Details</summary>
Motivation: 解决文本引导的3D修复中多任务统一框架的挑战，包括单参考视图不鲁棒、外观和几何不一致。

Method: DiGA3D通过多参考视图选择、注意力特征传播（AFP）机制和纹理-几何评分蒸馏采样（TG-SDS）损失，实现一致的外观和几何传播。

Result: 实验表明DiGA3D在多种3D修复任务中有效。

Conclusion: DiGA3D提供了一种鲁棒且一致的3D修复方法，适用于多任务场景。

Abstract: Developing a unified pipeline that enables users to remove, re-texture, or
replace objects in a versatile manner is crucial for text-guided 3D inpainting.
However, there are still challenges in performing multiple 3D inpainting tasks
within a unified framework: 1) Single reference inpainting methods lack
robustness when dealing with views that are far from the reference view. 2)
Appearance inconsistency arises when independently inpainting multi-view images
with 2D diffusion priors; 3) Geometry inconsistency limits performance when
there are significant geometric changes in the inpainting regions. To tackle
these challenges, we introduce DiGA3D, a novel and versatile 3D inpainting
pipeline that leverages diffusion models to propagate consistent appearance and
geometry in a coarse-to-fine manner. First, DiGA3D develops a robust strategy
for selecting multiple reference views to reduce errors during propagation.
Next, DiGA3D designs an Attention Feature Propagation (AFP) mechanism that
propagates attention features from the selected reference views to other views
via diffusion models to maintain appearance consistency. Furthermore, DiGA3D
introduces a Texture-Geometry Score Distillation Sampling (TG-SDS) loss to
further improve the geometric consistency of inpainted 3D scenes. Extensive
experiments on multiple 3D inpainting tasks demonstrate the effectiveness of
our method. The project page is available at https://rorisis.github.io/DiGA3D/.

</details>


### [119] [MFH: Marrying Frequency Domain with Handwritten Mathematical Expression Recognition](https://arxiv.org/abs/2507.00430)
*Huanxin Yang,Qiwen Wang*

Main category: cs.CV

Relevance: 30.0

TL;DR: 论文提出了一种结合频域分析的手写数学表达式识别方法（MFH），利用离散余弦变换（DCT）提升识别性能。


<details>
  <summary>Details</summary>
Motivation: 解决手写数学表达式识别中因复杂公式结构和字符布局带来的序列预测挑战。

Method: 提出MFH方法，将频域信息（DCT）融入HMER任务，强调频域信息对公式结构分析的辅助作用。

Result: 在CROHME 2014/2016/2019测试集上分别达到61.66%/62.07%/63.72%的准确率。

Conclusion: 频域信息能显著提升HMER性能，MFH方法具有普适性。

Abstract: Handwritten mathematical expression recognition (HMER) suffers from complex
formula structures and character layouts in sequence prediction. In this paper,
we incorporate frequency domain analysis into HMER and propose a method that
marries frequency domain with HMER (MFH), leveraging the discrete cosine
transform (DCT). We emphasize the structural analysis assistance of frequency
information for recognizing mathematical formulas. When implemented on various
baseline models, our network exhibits a consistent performance enhancement,
demonstrating the efficacy of frequency domain information. Experiments show
that our MFH-CoMER achieves noteworthy accuracyrates of 61.66%/62.07%/63.72% on
the CROHME 2014/2016/2019 test sets. The source code is available at
https://github.com/Hryxyhe/MFH.

</details>


### [120] [ARIG: Autoregressive Interactive Head Generation for Real-time Conversations](https://arxiv.org/abs/2507.00472)
*Ying Guo,Xi Liu,Cheng Zhen,Pengfei Yan,Xiaoming Wei*

Main category: cs.CV

Relevance: 30.0

TL;DR: 提出了一种基于自回归（AR）的帧级框架ARIG，用于实时生成更真实的交互式头部运动。通过扩散过程建模运动分布，并结合交互行为理解（IBU）和对话状态理解（CSU）提升交互真实感。


<details>
  <summary>Details</summary>
Motivation: 面对面对话是常见的人类活动，但现有方法在实时性和真实感上存在局限，需要改进。

Method: 采用非向量量化的自回归过程建模运动预测，结合扩散过程提升准确性。通过双轨双模态信号和上下文特征理解交互行为和对话状态。

Result: 实验验证了模型在实时性和交互真实感上的有效性。

Conclusion: ARIG框架在实时交互式头部生成任务中表现出色，解决了现有方法的局限性。

Abstract: Face-to-face communication, as a common human activity, motivates the
research on interactive head generation. A virtual agent can generate motion
responses with both listening and speaking capabilities based on the audio or
motion signals of the other user and itself. However, previous clip-wise
generation paradigm or explicit listener/speaker generator-switching methods
have limitations in future signal acquisition, contextual behavioral
understanding, and switching smoothness, making it challenging to be real-time
and realistic. In this paper, we propose an autoregressive (AR) based
frame-wise framework called ARIG to realize the real-time generation with
better interaction realism. To achieve real-time generation, we model motion
prediction as a non-vector-quantized AR process. Unlike discrete codebook-index
prediction, we represent motion distribution using diffusion procedure,
achieving more accurate predictions in continuous space. To improve interaction
realism, we emphasize interactive behavior understanding (IBU) and detailed
conversational state understanding (CSU). In IBU, based on dual-track
dual-modal signals, we summarize short-range behaviors through
bidirectional-integrated learning and perform contextual understanding over
long ranges. In CSU, we use voice activity signals and context features of IBU
to understand the various states (interruption, feedback, pause, etc.) that
exist in actual conversations. These serve as conditions for the final
progressive motion prediction. Extensive experiments have verified the
effectiveness of our model.

</details>


### [121] [Topology-Constrained Learning for Efficient Laparoscopic Liver Landmark Detection](https://arxiv.org/abs/2507.00519)
*Ruize Cui,Jiaan Zhang,Jialun Pei,Kai Wang,Pheng-Ann Heng,Jing Qin*

Main category: cs.CV

Relevance: 30.0

TL;DR: TopoNet是一个用于腹腔镜肝脏标志物检测的新型拓扑约束学习框架，结合RGB-D特征和拓扑约束损失函数，显著提高了检测精度。


<details>
  <summary>Details</summary>
Motivation: 腹腔镜肝脏手术中，标志物的自动检测面临管状结构和动态变形的挑战，需要一种能够同时捕捉纹理信息和拓扑结构的方法。

Method: 采用snake-CNN双路径编码器捕捉RGB纹理和深度拓扑结构，提出边界感知拓扑融合模块（BTF）和拓扑约束损失函数。

Result: 在L3D和P2ILF数据集上表现出色，精度和计算复杂度均优。

Conclusion: TopoNet在临床应用中具有潜力，代码已开源。

Abstract: Liver landmarks provide crucial anatomical guidance to the surgeon during
laparoscopic liver surgery to minimize surgical risk. However, the tubular
structural properties of landmarks and dynamic intraoperative deformations pose
significant challenges for automatic landmark detection. In this study, we
introduce TopoNet, a novel topology-constrained learning framework for
laparoscopic liver landmark detection. Our framework adopts a snake-CNN
dual-path encoder to simultaneously capture detailed RGB texture information
and depth-informed topological structures. Meanwhile, we propose a
boundary-aware topology fusion (BTF) module, which adaptively merges RGB-D
features to enhance edge perception while preserving global topology.
Additionally, a topological constraint loss function is embedded, which
contains a center-line constraint loss and a topological persistence loss to
ensure homotopy equivalence between predictions and labels. Extensive
experiments on L3D and P2ILF datasets demonstrate that TopoNet achieves
outstanding accuracy and computational complexity, highlighting the potential
for clinical applications in laparoscopic liver surgery. Our code will be
available at https://github.com/cuiruize/TopoNet.

</details>


### [122] [LOD-GS: Level-of-Detail-Sensitive 3D Gaussian Splatting for Detail Conserved Anti-Aliasing](https://arxiv.org/abs/2507.00554)
*Zhenya Yang,Bingchen Gong,Kai Chen,Qi Dou*

Main category: cs.CV

Relevance: 30.0

TL;DR: 论文提出LOD-GS框架，通过动态预测每个3D高斯图元的滤波强度，解决3D高斯渲染中的锯齿问题。


<details>
  <summary>Details</summary>
Motivation: 现有方法对采样率不敏感，导致渲染质量不足或过度平滑，需要一种更灵活的滤波方法。

Method: 引入一组基函数，以采样率为输入建模外观变化，实现采样率敏感的滤波，并通过端到端优化基函数参数。

Result: 在公开数据集和新合成数据集上，LOD-GS实现了SOTA渲染质量并有效消除锯齿。

Conclusion: LOD-GS通过动态滤波和端到端优化，显著提升了3D高斯渲染的质量和效率。

Abstract: Despite the advancements in quality and efficiency achieved by 3D Gaussian
Splatting (3DGS) in 3D scene rendering, aliasing artifacts remain a persistent
challenge. Existing approaches primarily rely on low-pass filtering to mitigate
aliasing. However, these methods are not sensitive to the sampling rate, often
resulting in under-filtering and over-smoothing renderings. To address this
limitation, we propose LOD-GS, a Level-of-Detail-sensitive filtering framework
for Gaussian Splatting, which dynamically predicts the optimal filtering
strength for each 3D Gaussian primitive. Specifically, we introduce a set of
basis functions to each Gaussian, which take the sampling rate as input to
model appearance variations, enabling sampling-rate-sensitive filtering. These
basis function parameters are jointly optimized with the 3D Gaussian in an
end-to-end manner. The sampling rate is influenced by both focal length and
camera distance. However, existing methods and datasets rely solely on
down-sampling to simulate focal length changes for anti-aliasing evaluation,
overlooking the impact of camera distance. To enable a more comprehensive
assessment, we introduce a new synthetic dataset featuring objects rendered at
varying camera distances. Extensive experiments on both public datasets and our
newly collected dataset demonstrate that our method achieves SOTA rendering
quality while effectively eliminating aliasing. The code and dataset have been
open-sourced.

</details>


### [123] [Similarity Memory Prior is All You Need for Medical Image Segmentation](https://arxiv.org/abs/2507.00585)
*Tang Hao,Guo ZhiQing,Wang LieJun,Liu Chao*

Main category: cs.CV

Relevance: 30.0

TL;DR: 论文提出了一种用于医学图像分割的相似性记忆先验网络（Sim-MPNet），通过动态记忆权重损失注意力和双相似性全局内部增强模块，显著提升了分割性能。


<details>
  <summary>Details</summary>
Motivation: 受猕猴初级视觉皮层中“祖母细胞”直接识别复杂形状的启发，研究这些细胞在医学图像分割中的潜在价值。

Method: 设计了Sim-MPNet，包括动态记忆权重损失注意力（DMW-LA）和双相似性全局内部增强模块（DS-GIM），通过相似性记忆先验和动态更新策略优化特征提取。

Result: 在四个公开数据集上的实验表明，Sim-MPNet优于其他最先进方法。

Conclusion: Sim-MPNet通过创新的记忆先验和动态更新策略，有效提升了医学图像分割的性能。

Abstract: In recent years, it has been found that "grandmother cells" in the primary
visual cortex (V1) of macaques can directly recognize visual input with complex
shapes. This inspires us to examine the value of these cells in promoting the
research of medical image segmentation. In this paper, we design a Similarity
Memory Prior Network (Sim-MPNet) for medical image segmentation. Specifically,
we propose a Dynamic Memory Weights-Loss Attention (DMW-LA), which matches and
remembers the category features of specific lesions or organs in medical images
through the similarity memory prior in the prototype memory bank, thus helping
the network to learn subtle texture changes between categories. DMW-LA also
dynamically updates the similarity memory prior in reverse through Weight-Loss
Dynamic (W-LD) update strategy, effectively assisting the network directly
extract category features. In addition, we propose the Double-Similarity Global
Internal Enhancement Module (DS-GIM) to deeply explore the internal differences
in the feature distribution of input data through cosine similarity and
euclidean distance. Extensive experiments on four public datasets show that
Sim-MPNet has better segmentation performance than other state-of-the-art
methods. Our code is available on https://github.com/vpsg-research/Sim-MPNet.

</details>


### [124] [Context-Aware Academic Emotion Dataset and Benchmark](https://arxiv.org/abs/2507.00586)
*Luming Zhao,Jingwen Xuan,Jiamin Lou,Yonghui Yu,Wenwu Yang*

Main category: cs.CV

Relevance: 30.0

TL;DR: 论文提出了一种基于CLIP的上下文感知学术情感识别方法（CLIP-CAER），并发布了首个涵盖多样化自然学习场景的学术情感数据集RAER。


<details>
  <summary>Details</summary>
Motivation: 学术情感分析对学生学习过程中的参与度和认知状态评估至关重要，但目前缺乏公开数据集和针对学术情感的有效识别方法。

Method: 利用CLIP模型，通过可学习的文本提示整合面部表情和上下文线索，提出CLIP-CAER方法。

Result: CLIP-CAER在学术情感识别任务中显著优于现有基于视频的面部表情识别方法。

Conclusion: 上下文信息对学术情感识别至关重要，CLIP-CAER为这一领域提供了有效解决方案。

Abstract: Academic emotion analysis plays a crucial role in evaluating students'
engagement and cognitive states during the learning process. This paper
addresses the challenge of automatically recognizing academic emotions through
facial expressions in real-world learning environments. While significant
progress has been made in facial expression recognition for basic emotions,
academic emotion recognition remains underexplored, largely due to the scarcity
of publicly available datasets. To bridge this gap, we introduce RAER, a novel
dataset comprising approximately 2,700 video clips collected from around 140
students in diverse, natural learning contexts such as classrooms, libraries,
laboratories, and dormitories, covering both classroom sessions and individual
study. Each clip was annotated independently by approximately ten annotators
using two distinct sets of academic emotion labels with varying granularity,
enhancing annotation consistency and reliability. To our knowledge, RAER is the
first dataset capturing diverse natural learning scenarios. Observing that
annotators naturally consider context cues-such as whether a student is looking
at a phone or reading a book-alongside facial expressions, we propose CLIP-CAER
(CLIP-based Context-aware Academic Emotion Recognition). Our method utilizes
learnable text prompts within the vision-language model CLIP to effectively
integrate facial expression and context cues from videos. Experimental results
demonstrate that CLIP-CAER substantially outperforms state-of-the-art
video-based facial expression recognition methods, which are primarily designed
for basic emotions, emphasizing the crucial role of context in accurately
recognizing academic emotions. Project page: https://zgsfer.github.io/CAER

</details>


### [125] [TopoStreamer: Temporal Lane Segment Topology Reasoning in Autonomous Driving](https://arxiv.org/abs/2507.00709)
*Yiming Yang,Yueru Luo,Bingkun He,Hongbin Lin,Suzhong Fu,Chao Yan,Kun Tang,Xinrui Yan,Chao Zheng,Shuguang Cui,Zhen Li*

Main category: cs.CV

Relevance: 30.0

TL;DR: TopoStreamer提出了一种端到端的时序感知模型，用于车道段拓扑推理，通过流式属性约束、动态车道边界位置编码和车道段去噪，显著提升了车道段感知和中心线感知任务的性能。


<details>
  <summary>Details</summary>
Motivation: 现有方法在一致位置嵌入和时序多属性学习方面的限制阻碍了准确的道路网络重建，影响了自动驾驶系统的性能。

Method: TopoStreamer引入了流式属性约束、动态车道边界位置编码和车道段去噪三项关键改进。

Result: 在OpenLane-V2数据集上，TopoStreamer在车道段感知和中心线感知任务中分别实现了+3.4% mAP和+2.1% OLS的性能提升。

Conclusion: TopoStreamer通过改进时序一致性和位置信息学习，显著提升了车道段拓扑推理的准确性。

Abstract: Lane segment topology reasoning constructs a comprehensive road network by
capturing the topological relationships between lane segments and their
semantic types. This enables end-to-end autonomous driving systems to perform
road-dependent maneuvers such as turning and lane changing. However, the
limitations in consistent positional embedding and temporal multiple attribute
learning in existing methods hinder accurate roadnet reconstruction. To address
these issues, we propose TopoStreamer, an end-to-end temporal perception model
for lane segment topology reasoning. Specifically, TopoStreamer introduces
three key improvements: streaming attribute constraints, dynamic lane boundary
positional encoding, and lane segment denoising. The streaming attribute
constraints enforce temporal consistency in both centerline and boundary
coordinates, along with their classifications. Meanwhile, dynamic lane boundary
positional encoding enhances the learning of up-to-date positional information
within queries, while lane segment denoising helps capture diverse lane segment
patterns, ultimately improving model performance. Additionally, we assess the
accuracy of existing models using a lane boundary classification metric, which
serves as a crucial measure for lane-changing scenarios in autonomous driving.
On the OpenLane-V2 dataset, TopoStreamer demonstrates significant improvements
over state-of-the-art methods, achieving substantial performance gains of +3.4%
mAP in lane segment perception and +2.1% OLS in centerline perception tasks.

</details>


### [126] [Biorthogonal Tunable Wavelet Unit with Lifting Scheme in Convolutional Neural Network](https://arxiv.org/abs/2507.00739)
*An Le,Hung Nguyen,Sungbal Seo,You-Suk Bae,Truong Nguyen*

Main category: cs.CV

Relevance: 30.0

TL;DR: 提出了一种新型双正交可调小波单元，通过提升方案放宽正交性和等长滤波器约束，增强CNN中的卷积、池化和下采样操作，提升图像分类和异常检测性能。


<details>
  <summary>Details</summary>
Motivation: 传统小波变换在正交性和滤波器长度上的限制限制了其在CNN中的灵活性和性能，因此需要一种更灵活的设计方法。

Method: 使用提升方案构造双正交可调小波单元，并将其集成到ResNet-18和ResNet-34中，验证其在图像分类和异常检测任务中的表现。

Result: 在CIFAR-10和DTD数据集上分类准确率分别提升2.12%和9.73%，在MVTec异常检测任务中表现优于现有方法。

Conclusion: 该方法通过灵活的小波设计显著提升了CNN的性能，尤其在细粒度特征捕捉和异常检测任务中表现突出。

Abstract: This work introduces a novel biorthogonal tunable wavelet unit constructed
using a lifting scheme that relaxes both the orthogonality and equal filter
length constraints, providing greater flexibility in filter design. The
proposed unit enhances convolution, pooling, and downsampling operations,
leading to improved image classification and anomaly detection in convolutional
neural networks (CNN). When integrated into an 18-layer residual neural network
(ResNet-18), the approach improved classification accuracy on CIFAR-10 by 2.12%
and on the Describable Textures Dataset (DTD) by 9.73%, demonstrating its
effectiveness in capturing fine-grained details. Similar improvements were
observed in ResNet-34. For anomaly detection in the hazelnut category of the
MVTec Anomaly Detection dataset, the proposed method achieved competitive and
wellbalanced performance in both segmentation and detection tasks,
outperforming existing approaches in terms of accuracy and robustness.

</details>


### [127] [Multi-Modal Graph Convolutional Network with Sinusoidal Encoding for Robust Human Action Segmentation](https://arxiv.org/abs/2507.00752)
*Hao Xing,Kai Zhe Boey,Yuankai Wu,Darius Burschka,Gordon Cheng*

Main category: cs.CV

Relevance: 30.0

TL;DR: 论文提出了一种多模态图卷积网络（MMGCN），通过整合低帧率视觉数据和高帧率运动数据来减少动作分割中的过分割错误，并引入了三种关键技术。


<details>
  <summary>Details</summary>
Motivation: 在协作机器人场景中，精确的时间动作分割对理解子活动标签及其时间结构至关重要，但现有方法因噪声问题导致过分割错误。

Method: 提出MMGCN框架，结合低帧率视觉数据和高帧率运动数据，采用正弦编码策略、时间图融合模块和数据增强技术SmoothLabelMix。

Result: 在Bimanual Actions Dataset上表现优于现有方法，F1@10达94.5%，F1@25达92.8%。

Conclusion: MMGCN通过多模态数据融合和新技术显著提升了动作分割的准确性和时间一致性。

Abstract: Accurate temporal segmentation of human actions is critical for intelligent
robots in collaborative settings, where a precise understanding of sub-activity
labels and their temporal structure is essential. However, the inherent noise
in both human pose estimation and object detection often leads to
over-segmentation errors, disrupting the coherence of action sequences. To
address this, we propose a Multi-Modal Graph Convolutional Network (MMGCN) that
integrates low-frame-rate (e.g., 1 fps) visual data with high-frame-rate (e.g.,
30 fps) motion data (skeleton and object detections) to mitigate fragmentation.
Our framework introduces three key contributions. First, a sinusoidal encoding
strategy that maps 3D skeleton coordinates into a continuous sin-cos space to
enhance spatial representation robustness. Second, a temporal graph fusion
module that aligns multi-modal inputs with differing resolutions via
hierarchical feature aggregation, Third, inspired by the smooth transitions
inherent to human actions, we design SmoothLabelMix, a data augmentation
technique that mixes input sequences and labels to generate synthetic training
examples with gradual action transitions, enhancing temporal consistency in
predictions and reducing over-segmentation artifacts.
  Extensive experiments on the Bimanual Actions Dataset, a public benchmark for
human-object interaction understanding, demonstrate that our approach
outperforms state-of-the-art methods, especially in action segmentation
accuracy, achieving F1@10: 94.5% and F1@25: 92.8%.

</details>


### [128] [Real-Time Inverse Kinematics for Generating Multi-Constrained Movements of Virtual Human Characters](https://arxiv.org/abs/2507.00792)
*Hendric Voss,Stefan Kopp*

Main category: cs.CV

Relevance: 30.0

TL;DR: 本文提出了一种基于TensorFlow的实时逆向运动学（IK）求解器，用于生成逼真的人体运动，解决了多约束问题中的误差累积和关节限制等挑战。


<details>
  <summary>Details</summary>
Motivation: 实时生成逼真的人体运动在计算机图形学、虚拟环境、机器人学和生物力学等领域具有重要意义。

Method: 利用TensorFlow的自动微分和即时编译技术，将正向和逆向运动学视为可微分操作，处理高自由度的人体骨架。

Result: 实验表明，该求解器在SMPLX人体骨架模型上表现优于现有方法，具有实时性能、快速收敛和低计算开销。

Conclusion: 该方法在多约束问题中表现优异，代码已开源。

Abstract: Generating accurate and realistic virtual human movements in real-time is of
high importance for a variety of applications in computer graphics, interactive
virtual environments, robotics, and biomechanics. This paper introduces a novel
real-time inverse kinematics (IK) solver specifically designed for realistic
human-like movement generation. Leveraging the automatic differentiation and
just-in-time compilation of TensorFlow, the proposed solver efficiently handles
complex articulated human skeletons with high degrees of freedom. By treating
forward and inverse kinematics as differentiable operations, our method
effectively addresses common challenges such as error accumulation and
complicated joint limits in multi-constrained problems, which are critical for
realistic human motion modeling. We demonstrate the solver's effectiveness on
the SMPLX human skeleton model, evaluating its performance against widely used
iterative-based IK algorithms, like Cyclic Coordinate Descent (CCD), FABRIK,
and the nonlinear optimization algorithm IPOPT. Our experiments cover both
simple end-effector tasks and sophisticated, multi-constrained problems with
realistic joint limits. Results indicate that our IK solver achieves real-time
performance, exhibiting rapid convergence, minimal computational overhead per
iteration, and improved success rates compared to existing methods. The project
code is available at https://github.com/hvoss-techfak/TF-JAX-IK

</details>


### [129] [Do Echo Top Heights Improve Deep Learning Nowcasts?](https://arxiv.org/abs/2507.00845)
*Peter Pavlík,Marc Schleiss,Anna Bou Ezzeddine,Viera Rozinajová*

Main category: cs.CV

Relevance: 30.0

TL;DR: 研究了利用Echo Top Height（ETH）作为辅助输入变量改进深度学习降水临近预报的方法，发现ETH在低雨量阈值下有效，但在高雨量下效果不一致。


<details>
  <summary>Details</summary>
Motivation: 降水临近预报对天气敏感行业至关重要，现有方法多忽略3D雷达数据的垂直信息，ETH可能提供额外价值。

Method: 采用单通道3D U-Net处理雷达反射率和ETH作为输入，验证ETH与降水强度的关系。

Result: ETH在低雨量阈值下提升预报能力，但在高雨量下效果不稳定且低估降水强度。

Conclusion: ETH作为辅助变量潜力有限，但为评估其他变量对预报性能的贡献提供了基础。

Abstract: Precipitation nowcasting -- the short-term prediction of rainfall using
recent radar observations -- is critical for weather-sensitive sectors such as
transportation, agriculture, and disaster mitigation. While recent deep
learning models have shown promise in improving nowcasting skill, most
approaches rely solely on 2D radar reflectivity fields, discarding valuable
vertical information available in the full 3D radar volume. In this work, we
explore the use of Echo Top Height (ETH), a 2D projection indicating the
maximum altitude of radar reflectivity above a given threshold, as an auxiliary
input variable for deep learning-based nowcasting. We examine the relationship
between ETH and radar reflectivity, confirming its relevance for predicting
rainfall intensity. We implement a single-pass 3D U-Net that processes both the
radar reflectivity and ETH as separate input channels. While our models are
able to leverage ETH to improve skill at low rain-rate thresholds, results are
inconsistent at higher intensities and the models with ETH systematically
underestimate precipitation intensity. Three case studies are used to
illustrate how ETH can help in some cases, but also confuse the models and
increase the error variance. Nonetheless, the study serves as a foundation for
critically assessing the potential contribution of additional variables to
nowcasting performance.

</details>


### [130] [SafeMap: Robust HD Map Construction from Incomplete Observations](https://arxiv.org/abs/2507.00861)
*Xiaoshuai Hao,Lingdong Kong,Rong Yin,Pengwei Wang,Jing Zhang,Yunfeng Diao,Shu Zhao*

Main category: cs.CV

Relevance: 30.0

TL;DR: SafeMap是一个用于自动驾驶的高清地图构建框架，通过G-PVR和D-BEVC模块处理不完整的多视角数据，提升鲁棒性和准确性。


<details>
  <summary>Details</summary>
Motivation: 现有方法在多视角相机数据不完整时表现不佳，SafeMap旨在解决这一问题，确保地图构建的准确性。

Method: 结合G-PVR（基于高斯分布的视角重建）和D-BEVC（基于蒸馏的鸟瞰图校正）模块，动态优化信息区域并校正不完整观测。

Result: 实验表明，SafeMap在完整和不完整数据场景下均显著优于现有方法。

Conclusion: SafeMap是一种即插即用的解决方案，能够无缝集成到现有系统中，提升高清地图构建的鲁棒性。

Abstract: Robust high-definition (HD) map construction is vital for autonomous driving,
yet existing methods often struggle with incomplete multi-view camera data.
This paper presents SafeMap, a novel framework specifically designed to secure
accuracy even when certain camera views are missing. SafeMap integrates two key
components: the Gaussian-based Perspective View Reconstruction (G-PVR) module
and the Distillation-based Bird's-Eye-View (BEV) Correction (D-BEVC) module.
G-PVR leverages prior knowledge of view importance to dynamically prioritize
the most informative regions based on the relationships among available camera
views. Furthermore, D-BEVC utilizes panoramic BEV features to correct the BEV
representations derived from incomplete observations. Together, these
components facilitate the end-to-end map reconstruction and robust HD map
generation. SafeMap is easy to implement and integrates seamlessly into
existing systems, offering a plug-and-play solution for enhanced robustness.
Experimental results demonstrate that SafeMap significantly outperforms
previous methods in both complete and incomplete scenarios, highlighting its
superior performance and reliability.

</details>


### [131] [Masks make discriminative models great again!](https://arxiv.org/abs/2507.00916)
*Tianshi Cao,Marie-Julie Rakotosaona,Ben Poole,Federico Tombari,Michael Niemeyer*

Main category: cs.CV

Relevance: 30.0

TL;DR: Image2GS提出了一种从单图像重建3D场景的新方法，专注于图像到3D的转换部分，通过解耦可见区域和未可见区域的训练，显著提升了重建质量。


<details>
  <summary>Details</summary>
Motivation: 解决从单图像重建3D场景的挑战，特别是图像到3D转换的确定性任务，避免生成未可见区域带来的不确定性。

Method: 使用优化的3D高斯飞溅生成的可见性掩码，在训练中排除未可见区域，专注于可见区域的重建。

Result: 在可见区域的重建质量显著优于基线方法，同时在完整场景评估中仍与最先进的判别模型竞争。

Conclusion: 判别模型在拟合未可见区域时存在根本性困难，专注于图像到3D转换的专门技术具有优势。

Abstract: We present Image2GS, a novel approach that addresses the challenging problem
of reconstructing photorealistic 3D scenes from a single image by focusing
specifically on the image-to-3D lifting component of the reconstruction
process. By decoupling the lifting problem (converting an image to a 3D model
representing what is visible) from the completion problem (hallucinating
content not present in the input), we create a more deterministic task suitable
for discriminative models. Our method employs visibility masks derived from
optimized 3D Gaussian splats to exclude areas not visible from the source view
during training. This masked training strategy significantly improves
reconstruction quality in visible regions compared to strong baselines.
Notably, despite being trained only on masked regions, Image2GS remains
competitive with state-of-the-art discriminative models trained on full target
images when evaluated on complete scenes. Our findings highlight the
fundamental struggle discriminative models face when fitting unseen regions and
demonstrate the advantages of addressing image-to-3D lifting as a distinct
problem with specialized techniques.

</details>


### [132] [MVP: Winning Solution to SMP Challenge 2025 Video Track](https://arxiv.org/abs/2507.00950)
*Liliang Ye,Yunyao Zhang,Yafeng Wu,Yi-Ping Phoebe Chen,Junqing Yu,Wei Yang,Zikai Song*

Main category: cs.CV

Relevance: 30.0

TL;DR: MVP是一个多模态视频预测框架，结合了预训练模型的视频特征、用户元数据和上下文信息，用于社交媒体视频的流行度预测，并在SMP Challenge 2025中获胜。


<details>
  <summary>Details</summary>
Motivation: 社交媒体视频的流行度预测对内容推荐、趋势检测和用户参与有重要应用价值。

Method: MVP通过整合预训练模型的视频特征、用户元数据和上下文信息，采用梯度提升回归模型进行多模态预测。

Result: MVP在SMP Challenge 2025的Video Track中排名第一，证明了其有效性和可靠性。

Conclusion: MVP为社交媒体平台的多模态视频流行度预测提供了一种高效且可靠的解决方案。

Abstract: Social media platforms serve as central hubs for content dissemination,
opinion expression, and public engagement across diverse modalities. Accurately
predicting the popularity of social media videos enables valuable applications
in content recommendation, trend detection, and audience engagement. In this
paper, we present Multimodal Video Predictor (MVP), our winning solution to the
Video Track of the SMP Challenge 2025. MVP constructs expressive post
representations by integrating deep video features extracted from pretrained
models with user metadata and contextual information. The framework applies
systematic preprocessing techniques, including log-transformations and outlier
removal, to improve model robustness. A gradient-boosted regression model is
trained to capture complex patterns across modalities. Our approach ranked
first in the official evaluation of the Video Track, demonstrating its
effectiveness and reliability for multimodal video popularity prediction on
social platforms. The source code is available at
https://anonymous.4open.science/r/SMPDVideo.

</details>


### [133] [Surgical Neural Radiance Fields from One Image](https://arxiv.org/abs/2507.00969)
*Alberto Neri,Maximilan Fehrentz,Veronica Penza,Leonardo S. Mattos,Nazim Haouchine*

Main category: cs.CV

Relevance: 30.0

TL;DR: 提出了一种利用单张术中图像和术前数据高效训练NeRF的方法，解决了手术场景中多视图数据不足的问题。


<details>
  <summary>Details</summary>
Motivation: 解决手术场景中因时间限制无法获取多视图数据的问题，利用术前数据和单张术中图像高效训练NeRF。

Method: 结合术前MRI数据和神经风格迁移（WTC2和STROTSS）构建训练集，实现单张图像的快速NeRF训练。

Result: 在四个神经外科案例中验证，结果显示高重建保真度和风格一致性，结构相似性接近真实数据。

Conclusion: 证明了单图像NeRF训练在手术场景中的可行性，克服了传统多视图方法的限制。

Abstract: Purpose: Neural Radiance Fields (NeRF) offer exceptional capabilities for 3D
reconstruction and view synthesis, yet their reliance on extensive multi-view
data limits their application in surgical intraoperative settings where only
limited data is available. In particular, collecting such extensive data
intraoperatively is impractical due to time constraints. This work addresses
this challenge by leveraging a single intraoperative image and preoperative
data to train NeRF efficiently for surgical scenarios.
  Methods: We leverage preoperative MRI data to define the set of camera
viewpoints and images needed for robust and unobstructed training.
Intraoperatively, the appearance of the surgical image is transferred to the
pre-constructed training set through neural style transfer, specifically
combining WTC2 and STROTSS to prevent over-stylization. This process enables
the creation of a dataset for instant and fast single-image NeRF training.
  Results: The method is evaluated with four clinical neurosurgical cases.
Quantitative comparisons to NeRF models trained on real surgical microscope
images demonstrate strong synthesis agreement, with similarity metrics
indicating high reconstruction fidelity and stylistic alignment. When compared
with ground truth, our method demonstrates high structural similarity,
confirming good reconstruction quality and texture preservation.
  Conclusion: Our approach demonstrates the feasibility of single-image NeRF
training in surgical settings, overcoming the limitations of traditional
multi-view methods.

</details>


### [134] [RTMap: Real-Time Recursive Mapping with Change Detection and Localization](https://arxiv.org/abs/2507.00980)
*Yuheng Du,Sheng Yang,Lingxuan Wang,Zhenghua Hou,Chengying Cai,Zhitao Tan,Mingxia Chen,Shi-Sheng Huang,Qiang Li*

Main category: cs.CV

Relevance: 30.0

TL;DR: RTMap提出了一种通过多轨迹众包构建高精地图的方法，解决了单次遍历方法的感知不准确、遮挡和无法融合多智能体观测的问题。


<details>
  <summary>Details</summary>
Motivation: 现有在线高精地图方法受限于感知不准确、交通密集时的遮挡以及无法融合多智能体观测，RTMap旨在通过众包多轨迹数据提升地图质量和实时性。

Method: RTMap采用端到端方法，包括不确定性感知的高精地图元素建模、基于概率的定位以及实时道路结构变化检测。

Result: 在多个自动驾驶数据集上，RTMap在地图质量和定位精度上表现优异，同时能异步提升众包地图的准确性和实时性。

Conclusion: RTMap通过众包多轨迹数据有效提升了高精地图的鲁棒性和实时性，支持下游预测和规划模块。

Abstract: While recent online HD mapping methods relieve burdened offline pipelines and
solve map freshness, they remain limited by perceptual inaccuracies, occlusion
in dense traffic, and an inability to fuse multi-agent observations. We propose
RTMap to enhance these single-traversal methods by persistently crowdsourcing a
multi-traversal HD map as a self-evolutional memory. On onboard agents, RTMap
simultaneously addresses three core challenges in an end-to-end fashion: (1)
Uncertainty-aware positional modeling for HD map elements, (2)
probabilistic-aware localization w.r.t. the crowdsourced prior-map, and (3)
real-time detection for possible road structural changes. Experiments on
several public autonomous driving datasets demonstrate our solid performance on
both the prior-aided map quality and the localization accuracy, demonstrating
our effectiveness of robustly serving downstream prediction and planning
modules while gradually improving the accuracy and freshness of the
crowdsourced prior-map asynchronously. Our source-code will be made publicly
available at https://github.com/CN-ADLab/RTMap (Camera ready version
incorporating reviewer suggestions will be updated soon).

</details>


### [135] [Evaluating Robustness of Monocular Depth Estimation with Procedural Scene Perturbations](https://arxiv.org/abs/2507.00981)
*Jack Nugent,Siyang Wu,Zeyu Ma,Beining Han,Meenal Parakh,Abhishek Joshi,Lingjie Mei,Alexander Raistrick,Xinyuan Li,Jia Deng*

Main category: cs.CV

Relevance: 30.0

TL;DR: 论文提出了PDE（Procedural Depth Evaluation）基准，用于系统评估单目深度估计模型的鲁棒性，填补了现有基准仅关注准确性的不足。


<details>
  <summary>Details</summary>
Motivation: 现有单目深度估计基准主要评估准确性，缺乏对鲁棒性的系统评估。PDE通过程序生成3D场景，测试模型对多种扰动的鲁棒性。

Method: PDE利用程序生成技术创建3D场景，模拟物体、相机、材质和光照变化等扰动，评估深度模型的鲁棒性。

Result: 研究发现当前先进的深度模型在某些扰动下表现不佳，为未来研究提供了方向。

Conclusion: PDE为深度估计模型的鲁棒性评估提供了新工具，揭示了现有模型的局限性。

Abstract: Recent years have witnessed substantial progress on monocular depth
estimation, particularly as measured by the success of large models on standard
benchmarks. However, performance on standard benchmarks does not offer a
complete assessment, because most evaluate accuracy but not robustness. In this
work, we introduce PDE (Procedural Depth Evaluation), a new benchmark which
enables systematic robustness evaluation. PDE uses procedural generation to
create 3D scenes that test robustness to various controlled perturbations,
including object, camera, material and lighting changes. Our analysis yields
interesting findings on what perturbations are challenging for state-of-the-art
depth models, which we hope will inform further research. Code and data are
available at https://github.com/princeton-vl/proc-depth-eval.

</details>


### [136] [ShapeEmbed: a self-supervised learning framework for 2D contour quantification](https://arxiv.org/abs/2507.01009)
*Anna Foix Romero,Craig Russell,Alexander Krull,Virginie Uhlmann*

Main category: cs.CV

Relevance: 30.0

TL;DR: ShapeEmbed是一个自监督学习框架，用于编码2D图像中物体的轮廓为形状描述符，具有平移、缩放、旋转、反射和点索引不变性。


<details>
  <summary>Details</summary>
Motivation: 解决形状量化中保持几何不变性的核心挑战，改进现有形状描述符的局限性。

Method: 提出ShapeEmbed框架，通过自监督学习将欧几里得距离矩阵编码为不变形状描述符。

Result: 在自然和生物图像形状分类任务中表现优于现有方法。

Conclusion: ShapeEmbed在生物成像应用中具有潜在价值。

Abstract: The shape of objects is an important source of visual information in a wide
range of applications. One of the core challenges of shape quantification is to
ensure that the extracted measurements remain invariant to transformations that
preserve an object's intrinsic geometry, such as changing its size,
orientation, and position in the image. In this work, we introduce ShapeEmbed,
a self-supervised representation learning framework designed to encode the
contour of objects in 2D images, represented as a Euclidean distance matrix,
into a shape descriptor that is invariant to translation, scaling, rotation,
reflection, and point indexing. Our approach overcomes the limitations of
traditional shape descriptors while improving upon existing state-of-the-art
autoencoder-based approaches. We demonstrate that the descriptors learned by
our framework outperform their competitors in shape classification tasks on
natural and biological images. We envision our approach to be of particular
relevance to biological imaging applications.

</details>


### [137] [DAM-VSR: Disentanglement of Appearance and Motion for Video Super-Resolution](https://arxiv.org/abs/2507.01012)
*Zhe Kong,Le Li,Yong Zhang,Feng Gao,Shaoshu Yang,Tao Wang,Kaihao Zhang,Zhuoliang Kang,Xiaoming Wei,Guanying Chen,Wenhan Luo*

Main category: cs.CV

Relevance: 30.0

TL;DR: DAM-VSR是一个用于视频超分辨率（VSR）的框架，通过解耦外观增强和运动控制问题，结合视频扩散模型和图像超分辨率模型，实现了高质量的细节生成和时间一致性。


<details>
  <summary>Details</summary>
Motivation: 现实世界的视频超分辨率面临复杂和不可预测的退化问题，现有方法在时间一致性上表现不佳。

Method: 提出DAM-VSR框架，解耦外观增强（通过参考图像超分辨率）和运动控制（通过视频ControlNet），并结合运动对齐的双向采样策略。

Result: 在真实世界和AIGC数据上达到最先进的性能，展示了强大的细节生成能力。

Conclusion: DAM-VSR通过解耦和结合生成先验，显著提升了视频超分辨率的质量和时间一致性。

Abstract: Real-world video super-resolution (VSR) presents significant challenges due
to complex and unpredictable degradations. Although some recent methods utilize
image diffusion models for VSR and have shown improved detail generation
capabilities, they still struggle to produce temporally consistent frames. We
attempt to use Stable Video Diffusion (SVD) combined with ControlNet to address
this issue. However, due to the intrinsic image-animation characteristics of
SVD, it is challenging to generate fine details using only low-quality videos.
To tackle this problem, we propose DAM-VSR, an appearance and motion
disentanglement framework for VSR. This framework disentangles VSR into
appearance enhancement and motion control problems. Specifically, appearance
enhancement is achieved through reference image super-resolution, while motion
control is achieved through video ControlNet. This disentanglement fully
leverages the generative prior of video diffusion models and the detail
generation capabilities of image super-resolution models. Furthermore, equipped
with the proposed motion-aligned bidirectional sampling strategy, DAM-VSR can
conduct VSR on longer input videos. DAM-VSR achieves state-of-the-art
performance on real-world data and AIGC data, demonstrating its powerful detail
generation capabilities.

</details>


### [138] [Real-Time Guidewire Tip Tracking Using a Siamese Network for Image-Guided Endovascular Procedures](https://arxiv.org/abs/2507.00051)
*Tianliang Yao,Zhiqiang Pei,Yong Li,Yixuan Yuan,Peng Qi*

Main category: eess.IV

Relevance: 30.0

TL;DR: 论文提出了一种基于Siamese网络和双重注意力机制的导丝尖端跟踪框架，用于心血管疾病的图像引导治疗，提高了跟踪的准确性和效率。


<details>
  <summary>Details</summary>
Motivation: AI在临床实践中的应用日益增多，但导丝尖端跟踪任务仍面临视觉模糊、组织变形和成像伪影等挑战。本文旨在通过改进的空间-时间特征学习方法解决这些问题。

Method: 采用Siamese网络结合自注意力和交叉注意力机制，增强对导丝尖端的跟踪能力，并在临床DSA序列上进行验证。

Result: 平均定位误差为0.421±0.138 mm，最大误差1.736 mm，平均IoU为0.782，处理速度为57.2帧/秒。

Conclusion: 该框架在临床和机器人平台上表现出色，满足了实时性和准确性的需求。

Abstract: An ever-growing incorporation of AI solutions into clinical practices
enhances the efficiency and effectiveness of healthcare services. This paper
focuses on guidewire tip tracking tasks during image-guided therapy for
cardiovascular diseases, aiding physicians in improving diagnostic and
therapeutic quality. A novel tracking framework based on a Siamese network with
dual attention mechanisms combines self- and cross-attention strategies for
robust guidewire tip tracking. This design handles visual ambiguities, tissue
deformations, and imaging artifacts through enhanced spatial-temporal feature
learning. Validation occurred on 3 randomly selected clinical digital
subtraction angiography (DSA) sequences from a dataset of 15 sequences,
covering multiple interventional scenarios. The results indicate a mean
localization error of 0.421 $\pm$ 0.138 mm, with a maximum error of 1.736 mm,
and a mean Intersection over Union (IoU) of 0.782. The framework maintains an
average processing speed of 57.2 frames per second, meeting the temporal
demands of endovascular imaging. Further validations with robotic platforms for
automating diagnostics and therapies in clinical routines yielded tracking
errors of 0.708 $\pm$ 0.695 mm and 0.148 $\pm$ 0.057 mm in two distinct
experimental scenarios.

</details>


### [139] [Rethink 3D Object Detection from Physical World](https://arxiv.org/abs/2507.00190)
*Satoshi Tanaka,Koji Minoda,Fumiya Watanabe,Takamasa Horibe*

Main category: cs.RO

Relevance: 30.0

TL;DR: 论文提出了两种新指标L-AP和P-AP，用于更全面地评估实时3D物体检测，考虑了硬件差异和运动规划的影响，并通过实验验证了其有效性。


<details>
  <summary>Details</summary>
Motivation: 现有3D物体检测研究忽略了速度与精度的权衡、硬件差异对实时应用的影响，以及检测性能对运动规划安全性的影响。

Method: 引入L-AP和P-AP指标，结合nuPlan数据集评估模型，并通过延迟感知超参数优化（L-HPO）开发高性能模型。

Result: 验证了新指标的有效性，优化了硬件和模型选择，并推翻了“点云越多性能越好”的假设。

Conclusion: 新指标为实时3D物体检测提供了更全面的评估方法，优化了系统性能。

Abstract: High-accuracy and low-latency 3D object detection is essential for autonomous
driving systems. While previous studies on 3D object detection often evaluate
performance based on mean average precision (mAP) and latency, they typically
fail to address the trade-off between speed and accuracy, such as 60.0 mAP at
100 ms vs 61.0 mAP at 500 ms. A quantitative assessment of the trade-offs
between different hardware devices and accelerators remains unexplored, despite
being critical for real-time applications. Furthermore, they overlook the
impact on collision avoidance in motion planning, for example, 60.0 mAP leading
to safer motion planning or 61.0 mAP leading to high-risk motion planning. In
this paper, we introduce latency-aware AP (L-AP) and planning-aware AP (P-AP)
as new metrics, which consider the physical world such as the concept of time
and physical constraints, offering a more comprehensive evaluation for
real-time 3D object detection. We demonstrate the effectiveness of our metrics
for the entire autonomous driving system using nuPlan dataset, and evaluate 3D
object detection models accounting for hardware differences and accelerators.
We also develop a state-of-the-art performance model for real-time 3D object
detection through latency-aware hyperparameter optimization (L-HPO) using our
metrics. Additionally, we quantitatively demonstrate that the assumption "the
more point clouds, the better the recognition performance" is incorrect for
real-time applications and optimize both hardware and model selection using our
metrics.

</details>


### [140] [FreNBRDF: A Frequency-Rectified Neural Material Representation](https://arxiv.org/abs/2507.00476)
*Chenliang Zhou,Zheyuan Hu,Cengiz Oztireli*

Main category: cs.GR

Relevance: 30.0

TL;DR: 论文提出FreNBRDF，一种基于频率校正的神经材质表示方法，通过球谐函数将频域考虑融入神经BRDF建模，提升材质重建和编辑的精度与鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 传统材质建模依赖表格化BRDF数据，而隐式神经表示虽灵活但频域行为理解不足，FreNBRDF旨在填补这一空白。

Method: 利用球谐函数设计频率校正损失，并将其融入可泛化、自适应的重建与编辑流程。

Result: 实验表明FreNBRDF在材质外观重建和编辑的精度与鲁棒性上优于现有方法。

Conclusion: FreNBRDF为下游任务提供了更结构化、可解释的框架。

Abstract: Accurate material modeling is crucial for achieving photorealistic rendering,
bridging the gap between computer-generated imagery and real-world photographs.
While traditional approaches rely on tabulated BRDF data, recent work has
shifted towards implicit neural representations, which offer compact and
flexible frameworks for a range of tasks. However, their behavior in the
frequency domain remains poorly understood. To address this, we introduce
FreNBRDF, a frequency-rectified neural material representation. By leveraging
spherical harmonics, we integrate frequency-domain considerations into neural
BRDF modeling. We propose a novel frequency-rectified loss, derived from a
frequency analysis of neural materials, and incorporate it into a generalizable
and adaptive reconstruction and editing pipeline. This framework enhances
fidelity, adaptability, and efficiency. Extensive experiments demonstrate that
\ours improves the accuracy and robustness of material appearance
reconstruction and editing compared to state-of-the-art baselines, enabling
more structured and interpretable downstream tasks and applications.

</details>


### [141] [MuteSwap: Silent Face-based Voice Conversion](https://arxiv.org/abs/2507.00498)
*Yifan Liu,Yu Fang,Zhouhan Lin*

Main category: cs.SD

Relevance: 30.0

TL;DR: 论文提出了一种名为MuteSwap的新框架，用于无声面部语音转换（SFVC），仅通过视觉输入完成语音转换，解决了在无干净音频情况下的挑战。


<details>
  <summary>Details</summary>
Motivation: 传统语音转换需要源和目标说话者的音频输入，但在无声视频或嘈杂环境中无法实现。SFVC任务旨在仅通过视觉输入完成语音转换，具有挑战性。

Method: MuteSwap框架采用对比学习对齐跨模态身份，并通过最小化互信息分离共享视觉特征。

Result: 实验表明，MuteSwap在语音合成和身份转换方面表现优异，尤其在嘈杂环境下优于依赖音频输入的方法。

Conclusion: MuteSwap证明了SFVC的可行性，并展示了其训练方法的有效性。

Abstract: Conventional voice conversion modifies voice characteristics from a source
speaker to a target speaker, relying on audio input from both sides. However,
this process becomes infeasible when clean audio is unavailable, such as in
silent videos or noisy environments. In this work, we focus on the task of
Silent Face-based Voice Conversion (SFVC), which does voice conversion entirely
from visual inputs. i.e., given images of a target speaker and a silent video
of a source speaker containing lip motion, SFVC generates speech aligning the
identity of the target speaker while preserving the speech content in the
source silent video. As this task requires generating intelligible speech and
converting identity using only visual cues, it is particularly challenging. To
address this, we introduce MuteSwap, a novel framework that employs contrastive
learning to align cross-modality identities and minimize mutual information to
separate shared visual features. Experimental results show that MuteSwap
achieves impressive performance in both speech synthesis and identity
conversion, especially under noisy conditions where methods dependent on audio
input fail to produce intelligible results, demonstrating both the
effectiveness of our training approach and the feasibility of SFVC.

</details>


### [142] [Mind the Detail: Uncovering Clinically Relevant Image Details in Accelerated MRI with Semantically Diverse Reconstructions](https://arxiv.org/abs/2507.00670)
*Jan Nikolas Morshuis,Christian Schlarmann,Thomas Küstner,Christian F. Baumgartner,Matthias Hein*

Main category: eess.IV

Relevance: 30.0

TL;DR: 论文提出了一种名为“语义多样性重建”（SDR）的方法，用于解决现有深度学习MRI重建技术可能遗漏小且罕见病理信息的问题，从而提高诊断准确性。


<details>
  <summary>Details</summary>
Motivation: 现有MRI重建技术在高加速因子下可能遗漏小且罕见病理信息，导致误诊（假阴性）。

Method: 提出SDR方法，生成具有增强语义多样性的重建图像，同时确保与测量数据完全一致。使用fastMRI+数据集训练目标检测器进行自动评估。

Result: SDR显著降低了假阴性诊断的概率（更高召回率），并提高了平均精度。

Conclusion: SDR方法在保留临床相关信息方面优于现有技术，有助于提高诊断准确性。

Abstract: In recent years, accelerated MRI reconstruction based on deep learning has
led to significant improvements in image quality with impressive results for
high acceleration factors. However, from a clinical perspective image quality
is only secondary; much more important is that all clinically relevant
information is preserved in the reconstruction from heavily undersampled data.
In this paper, we show that existing techniques, even when considering
resampling for diffusion-based reconstruction, can fail to reconstruct small
and rare pathologies, thus leading to potentially wrong diagnosis decisions
(false negatives). To uncover the potentially missing clinical information we
propose ``Semantically Diverse Reconstructions'' (\SDR), a method which, given
an original reconstruction, generates novel reconstructions with enhanced
semantic variability while all of them are fully consistent with the measured
data. To evaluate \SDR automatically we train an object detector on the
fastMRI+ dataset. We show that \SDR significantly reduces the chance of
false-negative diagnoses (higher recall) and improves mean average precision
compared to the original reconstructions. The code is available on
https://github.com/NikolasMorshuis/SDR

</details>


### [143] [Prompt2SegCXR:Prompt to Segment All Organs and Diseases in Chest X-rays](https://arxiv.org/abs/2507.00673)
*Abduz Zami,Shadman Sobhan,Rounaq Hossain,Md. Sawran Sorker,Mohiuddin Ahmed,Md. Redwan Hossain*

Main category: eess.IV

Relevance: 30.0

TL;DR: 论文提出了一种基于提示的交互式多器官和多疾病分割方法，特别针对胸部X光片，并介绍了轻量级模型Prompt2SegCXR。


<details>
  <summary>Details</summary>
Motivation: 传统分割模型局限于特定器官或疾病，缺乏灵活性。尽管近期出现了基于提示的分割方法，但尚未有针对胸部X光片的交互式多器官和多疾病分割研究。

Method: 生成医学专家的涂鸦提示数据集，并设计轻量级模型Prompt2SegCXR，采用多阶段特征融合技术。

Result: 模型在基于提示的胸部X光片分割任务中表现优异，优于现有预训练模型。

Conclusion: Prompt2SegCXR为胸部X光片的多器官和多疾病分割提供了可靠解决方案。

Abstract: Image segmentation plays a vital role in the medical field by isolating
organs or regions of interest from surrounding areas. Traditionally,
segmentation models are trained on a specific organ or a disease, limiting
their ability to handle other organs and diseases. At present, few advanced
models can perform multi-organ or multi-disease segmentation, offering greater
flexibility. Also, recently, prompt-based image segmentation has gained
attention as a more flexible approach. It allows models to segment areas based
on user-provided prompts. Despite these advances, there has been no dedicated
work on prompt-based interactive multi-organ and multi-disease segmentation,
especially for Chest X-rays. This work presents two main contributions: first,
generating doodle prompts by medical experts of a collection of datasets from
multiple sources with 23 classes, including 6 organs and 17 diseases,
specifically designed for prompt-based Chest X-ray segmentation. Second, we
introduce Prompt2SegCXR, a lightweight model for accurately segmenting multiple
organs and diseases from Chest X-rays. The model incorporates multi-stage
feature fusion, enabling it to combine features from various network layers for
better spatial and semantic understanding, enhancing segmentation accuracy.
Compared to existing pre-trained models for prompt-based image segmentation,
our model scores well, providing a reliable solution for segmenting Chest
X-rays based on user prompts.

</details>


### [144] [Research on Improving the High Precision and Lightweight Diabetic Retinopathy Detection of YOLOv8n](https://arxiv.org/abs/2507.00780)
*Fei Yuhuan,Sun Xufei,Zang Ran,Wang Gengchen,Su Meng,Liu Fenghao*

Main category: eess.IV

Relevance: 30.0

TL;DR: 论文提出了一种基于改进YOLOv8n的轻量级高精度检测模型YOLO-KFG，用于糖尿病视网膜病变的早期检测。通过动态卷积和特征扩散金字塔网络提升模型性能，实验结果显示其在准确性和效率上优于主流算法。


<details>
  <summary>Details</summary>
Motivation: 解决现有糖尿病视网膜病变检测方法在准确性和鲁棒性上的不足，尤其是对微病变的感知能力。

Method: 1. 设计了动态卷积KWConv和C2f-KW模块改进主干网络；2. 提出特征扩散金字塔网络FDPN整合多尺度信息；3. 设计轻量级共享检测头GSDHead减少参数量。

Result: 相比YOLOv8n，参数量减少20.7%，mAP@0.5提升4.1%，召回率提高7.9%；优于YOLOv5n和YOLOv10n等主流算法。

Conclusion: YOLO-KFG在糖尿病视网膜病变检测中表现出更高的准确性和效率，适合资源受限设备部署。

Abstract: Early detection and diagnosis of diabetic retinopathy is one of the current
research focuses in ophthalmology. However, due to the subtle features of
micro-lesions and their susceptibility to background interference, ex-isting
detection methods still face many challenges in terms of accuracy and
robustness. To address these issues, a lightweight and high-precision detection
model based on the improved YOLOv8n, named YOLO-KFG, is proposed. Firstly, a
new dynamic convolution KWConv and C2f-KW module are designed to improve the
backbone network, enhancing the model's ability to perceive micro-lesions.
Secondly, a fea-ture-focused diffusion pyramid network FDPN is designed to
fully integrate multi-scale context information, further improving the model's
ability to perceive micro-lesions. Finally, a lightweight shared detection head
GSDHead is designed to reduce the model's parameter count, making it more
deployable on re-source-constrained devices. Experimental results show that
compared with the base model YOLOv8n, the improved model reduces the parameter
count by 20.7%, increases mAP@0.5 by 4.1%, and improves the recall rate by
7.9%. Compared with single-stage mainstream algorithms such as YOLOv5n and
YOLOv10n, YOLO-KFG demonstrates significant advantages in both detection
accuracy and efficiency.

</details>


### [145] [DMCIE: Diffusion Model with Concatenation of Inputs and Errors to Improve the Accuracy of the Segmentation of Brain Tumors in MRI Images](https://arxiv.org/abs/2507.00983)
*Sara Yavari,Rahul Nitin Pandya,Jacob Furst*

Main category: eess.IV

Relevance: 30.0

TL;DR: 论文提出了一种基于扩散模型的新型脑肿瘤分割方法DMCIE，通过结合输入和误差图提升分割精度，在BraTS2020数据集上表现优异。


<details>
  <summary>Details</summary>
Motivation: 脑肿瘤MRI分割对临床诊断和治疗至关重要，扩散模型在图像生成和分割任务中表现出色，但如何进一步提升分割精度是关键。

Method: 提出DMCIE框架，利用3D U-Net生成初始分割掩码和误差图，结合原始MRI图像引导扩散模型优化分割。

Result: 在BraTS2020数据集上，DMCIE的Dice Score达93.46，HD95为5.94 mm，优于现有扩散模型方法。

Conclusion: 误差引导的扩散模型能显著提升脑肿瘤分割的精度和可靠性。

Abstract: Accurate segmentation of brain tumors in MRI scans is essential for reliable
clinical diagnosis and effective treatment planning. Recently, diffusion models
have demonstrated remarkable effectiveness in image generation and segmentation
tasks. This paper introduces a novel approach to corrective segmentation based
on diffusion models. We propose DMCIE (Diffusion Model with Concatenation of
Inputs and Errors), a novel framework for accurate brain tumor segmentation in
multi-modal MRI scans. We employ a 3D U-Net to generate an initial segmentation
mask, from which an error map is generated by identifying the differences
between the prediction and the ground truth. The error map, concatenated with
the original MRI images, are used to guide a diffusion model. Using multimodal
MRI inputs (T1, T1ce, T2, FLAIR), DMCIE effectively enhances segmentation
accuracy by focusing on misclassified regions, guided by the original inputs.
Evaluated on the BraTS2020 dataset, DMCIE outperforms several state-of-the-art
diffusion-based segmentation methods, achieving a Dice Score of 93.46 and an
HD95 of 5.94 mm. These results highlight the effectiveness of error-guided
diffusion in producing precise and reliable brain tumor segmentations.

</details>


### [146] [Box Pose and Shape Estimation and Domain Adaptation for Large-Scale Warehouse Automation](https://arxiv.org/abs/2507.00984)
*Xihang Yu,Rajat Talak,Jingnan Shi,Ulrich Viereck,Igor Gilitschenski,Luca Carlone*

Main category: cs.RO

Relevance: 30.0

TL;DR: 论文提出了一种自监督领域适应方法，利用未标注数据改进仓库自动化机器人的感知模型，专注于箱体姿态和形状估计。


<details>
  <summary>Details</summary>
Motivation: 仓库自动化系统产生大量未标注数据，传统方法依赖人工标注，成本高且效率低。

Method: 开发了一种自监督领域适应流程，结合真实未标注数据，提出correct-and-certify方法用于箱体姿态和形状估计。

Result: 在模拟和真实工业场景中验证，自监督模型显著优于仅基于模拟训练的模型和零样本3D边界框估计基线。

Conclusion: 自监督方法能有效利用未标注数据，提升感知模型性能，减少对人工标注的依赖。

Abstract: Modern warehouse automation systems rely on fleets of intelligent robots that
generate vast amounts of data -- most of which remains unannotated. This paper
develops a self-supervised domain adaptation pipeline that leverages
real-world, unlabeled data to improve perception models without requiring
manual annotations. Our work focuses specifically on estimating the pose and
shape of boxes and presents a correct-and-certify pipeline for self-supervised
box pose and shape estimation. We extensively evaluate our approach across a
range of simulated and real industrial settings, including adaptation to a
large-scale real-world dataset of 50,000 images. The self-supervised model
significantly outperforms models trained solely in simulation and shows
substantial improvements over a zero-shot 3D bounding box estimation baseline.

</details>


### [147] [Advancing Lung Disease Diagnosis in 3D CT Scans](https://arxiv.org/abs/2507.00993)
*Qingqiu Li,Runtian Yuan,Junlin Hou,Jilan Xu,Yuejie Zhang,Rui Feng,Hao Chen*

Main category: eess.IV

Relevance: 30.0

TL;DR: 提出了一种用于胸部CT扫描中肺部疾病诊断的模型，通过去除非肺部区域和加权交叉熵损失优化性能。


<details>
  <summary>Details</summary>
Motivation: 提高胸部CT扫描中肺部疾病诊断的准确性，尤其是对罕见类别的识别。

Method: 分析3D CT扫描特征，去除非肺部区域；采用ResNeSt50作为特征提取器，使用加权交叉熵损失解决类别不平衡问题。

Result: 在Fair Disease Diagnosis Challenge验证集上Macro F1 Score达到0.80。

Conclusion: 模型在区分不同肺部疾病方面表现优异。

Abstract: To enable more accurate diagnosis of lung disease in chest CT scans, we
propose a straightforward yet effective model. Firstly, we analyze the
characteristics of 3D CT scans and remove non-lung regions, which helps the
model focus on lesion-related areas and reduces computational cost. We adopt
ResNeSt50 as a strong feature extractor, and use a weighted cross-entropy loss
to mitigate class imbalance, especially for the underrepresented squamous cell
carcinoma category. Our model achieves a Macro F1 Score of 0.80 on the
validation set of the Fair Disease Diagnosis Challenge, demonstrating its
strong performance in distinguishing between different lung conditions.

</details>


### [148] [HistoART: Histopathology Artifact Detection and Reporting Tool](https://arxiv.org/abs/2507.00044)
*Seyed Kahaki,Alexander R. Webber,Ghada Zamzmi,Adarsh Subbaswamy,Rucha Deshpande,Aldo Badano*

Main category: cs.CV

Relevance: 20.0

TL;DR: 该论文提出了三种用于全切片成像（WSI）中伪影检测的方法，并比较了它们的性能。基于基础模型的方法（FMA）表现最佳。


<details>
  <summary>Details</summary>
Motivation: WSI在癌症诊断中广泛应用，但伪影会干扰图像分析。需要开发鲁棒的伪影检测方法以提高诊断准确性。

Method: 比较了三种方法：基于基础模型的FMA、基于ResNet50的DLA和基于手工特征的KBA。

Result: FMA的AUROC最高（0.995），优于DLA（0.977）和KBA（0.940）。

Conclusion: FMA在伪影检测中表现最优，可用于生成质量报告以指导诊断。

Abstract: In modern cancer diagnostics, Whole Slide Imaging (WSI) is widely used to
digitize tissue specimens for detailed, high-resolution examination; however,
other diagnostic approaches, such as liquid biopsy and molecular testing, are
also utilized based on the cancer type and clinical context. While WSI has
revolutionized digital histopathology by enabling automated, precise analysis,
it remains vulnerable to artifacts introduced during slide preparation and
scanning. These artifacts can compromise downstream image analysis. To address
this challenge, we propose and compare three robust artifact detection
approaches for WSIs: (1) a foundation model-based approach (FMA) using a
fine-tuned Unified Neural Image (UNI) architecture, (2) a deep learning
approach (DLA) built on a ResNet50 backbone, and (3) a knowledge-based approach
(KBA) leveraging handcrafted features from texture, color, and frequency-based
metrics. The methods target six common artifact types: tissue folds,
out-of-focus regions, air bubbles, tissue damage, marker traces, and blood
contamination. Evaluations were conducted on 50,000+ image patches from diverse
scanners (Hamamatsu, Philips, Leica Aperio AT2) across multiple sites. The FMA
achieved the highest patch-wise AUROC of 0.995 (95% CI [0.994, 0.995]),
outperforming the ResNet50-based method (AUROC: 0.977, 95% CI [0.977, 0.978])
and the KBA (AUROC: 0.940, 95% CI [0.933, 0.946]). To translate detection into
actionable insights, we developed a quality report scorecard that quantifies
high-quality patches and visualizes artifact distributions.

</details>


### [149] [An efficient plant disease detection using transfer learning approach](https://arxiv.org/abs/2507.00070)
*Bosubabu Sambana,Hillary Sunday Nnadi,Mohd Anas Wajid,Nwosu Ogochukwu Fidelia,Claudia Camacho-Zuñiga,Henry Dozie Ajuzie,Edeh Michael Onyema*

Main category: cs.CV

Relevance: 20.0

TL;DR: 该论文提出了一种基于YOLOv7和YOLOv8的迁移学习系统，用于植物病害的自动检测和监测，展示了YOLOv8在农业实践中的高效性。


<details>
  <summary>Details</summary>
Motivation: 植物病害对农业造成重大影响，早期检测至关重要。技术发展为自动化监测提供了机会。

Method: 使用YOLOv7和YOLOv8进行迁移学习，在植物叶片图像数据集上微调模型，检测细菌、真菌和病毒病害。

Result: 模型性能指标（mAP、F1-score、Precision、Recall）分别为91.05、89.40、91.22和87.66，YOLOv8表现最优。

Conclusion: 该系统为植物病害早期检测提供了可扩展的自动化解决方案，有助于提高作物产量和可持续农业。

Abstract: Plant diseases pose significant challenges to farmers and the agricultural
sector at large. However, early detection of plant diseases is crucial to
mitigating their effects and preventing widespread damage, as outbreaks can
severely impact the productivity and quality of crops. With advancements in
technology, there are increasing opportunities for automating the monitoring
and detection of disease outbreaks in plants. This study proposed a system
designed to identify and monitor plant diseases using a transfer learning
approach. Specifically, the study utilizes YOLOv7 and YOLOv8, two
state-ofthe-art models in the field of object detection. By fine-tuning these
models on a dataset of plant leaf images, the system is able to accurately
detect the presence of Bacteria, Fungi and Viral diseases such as Powdery
Mildew, Angular Leaf Spot, Early blight and Tomato mosaic virus. The model's
performance was evaluated using several metrics, including mean Average
Precision (mAP), F1-score, Precision, and Recall, yielding values of 91.05,
89.40, 91.22, and 87.66, respectively. The result demonstrates the superior
effectiveness and efficiency of YOLOv8 compared to other object detection
methods, highlighting its potential for use in modern agricultural practices.
The approach provides a scalable, automated solution for early any plant
disease detection, contributing to enhanced crop yield, reduced reliance on
manual monitoring, and supporting sustainable agricultural practices.

</details>


### [150] [Computer Vision for Objects used in Group Work: Challenges and Opportunities](https://arxiv.org/abs/2507.00224)
*Changsoo Jung,Sheikh Mannan,Jack Fitzgerald,Nathaniel Blanchard*

Main category: cs.CV

Relevance: 20.0

TL;DR: 论文提出FiboSB数据集，用于6D姿态估计，评估了现有方法并改进YOLO11-x以提升性能。


<details>
  <summary>Details</summary>
Motivation: 现有系统在协作任务中难以准确捕捉学生与物理对象的交互，6D姿态估计可解决此问题。

Method: 引入FiboSB数据集，评估四种6D姿态估计方法，并改进YOLO11-x。

Result: 改进后的YOLO11-x在FiboSB上达到mAP_50 0.898。

Conclusion: FiboSB数据集和YOLO11-x改进为协作场景中的6D姿态估计奠定了基础。

Abstract: Interactive and spatially aware technologies are transforming educational
frameworks, particularly in K-12 settings where hands-on exploration fosters
deeper conceptual understanding. However, during collaborative tasks, existing
systems often lack the ability to accurately capture real-world interactions
between students and physical objects. This issue could be addressed with
automatic 6D pose estimation, i.e., estimation of an object's position and
orientation in 3D space from RGB images or videos. For collaborative groups
that interact with physical objects, 6D pose estimates allow AI systems to
relate objects and entities. As part of this work, we introduce FiboSB, a novel
and challenging 6D pose video dataset featuring groups of three participants
solving an interactive task featuring small hand-held cubes and a weight scale.
This setup poses unique challenges for 6D pose because groups are holistically
recorded from a distance in order to capture all participants -- this, coupled
with the small size of the cubes, makes 6D pose estimation inherently
non-trivial. We evaluated four state-of-the-art 6D pose estimation methods on
FiboSB, exposing the limitations of current algorithms on collaborative group
work. An error analysis of these methods reveals that the 6D pose methods'
object detection modules fail. We address this by fine-tuning YOLO11-x for
FiboSB, achieving an overall mAP_50 of 0.898. The dataset, benchmark results,
and analysis of YOLO11-x errors presented here lay the groundwork for
leveraging the estimation of 6D poses in difficult collaborative contexts.

</details>


### [151] [VOCAL: Visual Odometry via ContrAstive Learning](https://arxiv.org/abs/2507.00243)
*Chi-Yao Huang,Zeel Bhatt,Yezhou Yang*

Main category: cs.CV

Relevance: 20.0

TL;DR: VOCAL是一种基于对比学习的视觉里程计框架，通过贝叶斯推理和表示学习提升特征的可解释性和多模态兼容性。


<details>
  <summary>Details</summary>
Motivation: 现有学习型视觉里程计方法依赖刚性几何假设，缺乏理论支持和可解释性。

Method: 将视觉里程计重新定义为标签排序问题，结合贝叶斯推理和表示学习，使视觉特征与相机状态对齐。

Result: 在KITTI数据集上验证了VOCAL的可解释性和灵活性。

Conclusion: VOCAL推动了视觉里程计向更通用和可解释的空间智能发展。

Abstract: Breakthroughs in visual odometry (VO) have fundamentally reshaped the
landscape of robotics, enabling ultra-precise camera state estimation that is
crucial for modern autonomous systems. Despite these advances, many
learning-based VO techniques rely on rigid geometric assumptions, which often
fall short in interpretability and lack a solid theoretical basis within fully
data-driven frameworks. To overcome these limitations, we introduce VOCAL
(Visual Odometry via ContrAstive Learning), a novel framework that reimagines
VO as a label ranking challenge. By integrating Bayesian inference with a
representation learning framework, VOCAL organizes visual features to mirror
camera states. The ranking mechanism compels similar camera states to converge
into consistent and spatially coherent representations within the latent space.
This strategic alignment not only bolsters the interpretability of the learned
features but also ensures compatibility with multimodal data sources. Extensive
evaluations on the KITTI dataset highlight VOCAL's enhanced interpretability
and flexibility, pushing VO toward more general and explainable spatial
intelligence.

</details>


### [152] [Room Scene Discovery and Grouping in Unstructured Vacation Rental Image Collections](https://arxiv.org/abs/2507.00263)
*Vignesh Ram Nithin Kappagantula,Shayan Hassantabar*

Main category: cs.CV

Relevance: 20.0

TL;DR: 论文提出了一种高效的机器学习流程，用于解决度假租赁平台中房间场景发现和分组问题，以及识别卧室中的床型。该方法结合了监督学习和聚类算法，并利用多模态大语言模型（MLLM）进行床型映射，性能显著优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 度假租赁平台上的大量未分类房产图像给旅行者理解空间布局带来挑战。论文旨在通过自动化的图像分组和床型识别，帮助用户更好地理解房产的布局和睡眠配置。

Method: 提出了一种计算高效的机器学习流程，包括监督房间类型检测模型、重叠检测模型和聚类算法，并结合多模态大语言模型（MLLM）进行床型映射。

Result: 该方法在性能上显著优于对比学习和预训练嵌入聚类等现有方法。

Conclusion: 论文提出的流程在实时和数据稀缺环境下表现优异，为度假租赁平台提供了有效的图像组织和理解解决方案。

Abstract: The rapid growth of vacation rental (VR) platforms has led to an increasing
volume of property images, often uploaded without structured categorization.
This lack of organization poses significant challenges for travelers attempting
to understand the spatial layout of a property, particularly when multiple
rooms of the same type are present. To address this issue, we introduce an
effective approach for solving the room scene discovery and grouping problem,
as well as identifying bed types within each bedroom group. This grouping is
valuable for travelers to comprehend the spatial organization, layout, and the
sleeping configuration of the property. We propose a computationally efficient
machine learning pipeline characterized by low latency and the ability to
perform effectively with sample-efficient learning, making it well-suited for
real-time and data-scarce environments. The pipeline integrates a supervised
room-type detection model, a supervised overlap detection model to identify the
overlap similarity between two images, and a clustering algorithm to group the
images of the same space together using the similarity scores. Additionally,
the pipeline maps each bedroom group to the corresponding bed types specified
in the property's metadata, based on the visual content present in the group's
images using a Multi-modal Large Language Model (MLLM) model. We evaluate the
aforementioned models individually and also assess the pipeline in its
entirety, observing strong performance that significantly outperforms
established approaches such as contrastive learning and clustering with
pretrained embeddings.

</details>


### [153] [MammoTracker: Mask-Guided Lesion Tracking in Temporal Mammograms](https://arxiv.org/abs/2507.00328)
*Xuan Liu,Yinhao Ren,Marc D. Ryser,Lars J. Grimm,Joseph Y. Lo*

Main category: cs.CV

Relevance: 20.0

TL;DR: MammoTracker是一个用于乳腺X光片中病变跟踪的框架，通过全局和局部搜索模块实现自动化病变定位，并在新数据集上表现优于基线模型。


<details>
  <summary>Details</summary>
Motivation: 乳腺X光片中病变的自动跟踪对乳腺癌监测和早期诊断至关重要，但现有CAD系统在此方面存在挑战。

Method: 采用粗到细策略，结合全局搜索、局部搜索和分数细化三个模块。

Result: 在730个病例的新数据集上，MammoTracker的平均重叠率为0.455，准确率为0.509，优于基线模型8%。

Conclusion: MammoTracker有望提升CAD系统的病变进展分析能力，相关数据集将公开。

Abstract: Accurate lesion tracking in temporal mammograms is essential for monitoring
breast cancer progression and facilitating early diagnosis. However, automated
lesion correspondence across exams remains a challenges in computer-aided
diagnosis (CAD) systems, limiting their effectiveness. We propose MammoTracker,
a mask-guided lesion tracking framework that automates lesion localization
across consecutively exams. Our approach follows a coarse-to-fine strategy
incorporating three key modules: global search, local search, and score
refinement. To support large-scale training and evaluation, we introduce a new
dataset with curated prior-exam annotations for 730 mass and calcification
cases from the public EMBED mammogram dataset, yielding over 20000 lesion
pairs, making it the largest known resource for temporal lesion tracking in
mammograms. Experimental results demonstrate that MammoTracker achieves 0.455
average overlap and 0.509 accuracy, surpassing baseline models by 8%,
highlighting its potential to enhance CAD-based lesion progression analysis.
Our dataset will be available at
https://gitlab.oit.duke.edu/railabs/LoGroup/mammotracker.

</details>


### [154] [Overtake Detection in Trucks Using CAN Bus Signals: A Comparative Study of Machine Learning Methods](https://arxiv.org/abs/2507.00593)
*Fernando Alonso-Fernandez,Talha Hanif Butt,Prayag Tiwari*

Main category: cs.CV

Relevance: 20.0

TL;DR: 论文研究了卡车安全超车动作的预测方法，使用CAN总线数据评估了三种分类器（ANN、RF、SVM），发现数据多样性和多车训练能提升性能，最终通过分数级融合策略达到较高准确率。


<details>
  <summary>Details</summary>
Motivation: 卡车安全超车动作的准确预测对ADAS系统至关重要，但现有数据多样性和条件限制影响了分类性能。

Method: 使用CAN总线数据，评估了ANN、RF和SVM三种分类器，分析了不同预处理配置的影响，并采用分数级融合策略提升性能。

Result: 通过多车训练和分数级融合，实现了TNR=93%和TPR=86.5%的准确率。

Conclusion: 数据多样性和多车训练对提升分类性能至关重要，分数级融合策略在大多数情况下表现最佳。

Abstract: Safe overtaking manoeuvres in trucks are vital for preventing accidents and
ensuring efficient traffic flow. Accurate prediction of such manoeuvres is
essential for Advanced Driver Assistance Systems (ADAS) to make timely and
informed decisions. In this study, we focus on overtake detection using
Controller Area Network (CAN) bus data collected from five in-service trucks
provided by the Volvo Group. We evaluate three common classifiers for vehicle
manoeuvre detection, Artificial Neural Networks (ANN), Random Forest (RF), and
Support Vector Machines (SVM), and analyse how different preprocessing
configurations affect performance. We find that variability in traffic
conditions strongly influences the signal patterns, particularly in the
no-overtake class, affecting classification performance if training data lacks
adequate diversity. Since the data were collected under unconstrained,
real-world conditions, class diversity cannot be guaranteed a priori. However,
training with data from multiple vehicles improves generalisation and reduces
condition-specific bias. Our pertruck analysis also reveals that classification
accuracy, especially for overtakes, depends on the amount of training data per
vehicle. To address this, we apply a score-level fusion strategy, which yields
the best per-truck performance across most cases. Overall, we achieve an
accuracy via fusion of TNR=93% (True Negative Rate) and TPR=86.5% (True
Positive Rate). This research has been part of the BIG FUN project, which
explores how Artificial Intelligence can be applied to logged vehicle data to
understand and predict driver behaviour, particularly in relation to Camera
Monitor Systems (CMS), being introduced as digital replacements for traditional
exterior mirrors.

</details>


### [155] [UMDATrack: Unified Multi-Domain Adaptive Tracking Under Adverse Weather Conditions](https://arxiv.org/abs/2507.00648)
*Siyuan Yao,Rui Zhu,Ziqi Wang,Wenqi Ren,Yanyang Yan,Xiaochun Cao*

Main category: cs.CV

Relevance: 20.0

TL;DR: UMDATrack是一种视觉目标跟踪方法，通过域适应框架在各种恶劣天气条件下保持高质量的目标状态预测。


<details>
  <summary>Details</summary>
Motivation: 现有方法主要在白天条件下学习目标表示，但在恶劣天气条件下性能显著下降。UMDATrack旨在解决这一问题。

Method: 使用可控场景生成器合成少量未标记视频，设计域定制适配器（DCA）和目标感知置信度对齐模块（TCA）。

Result: UMDATrack在实验中显著超越现有先进视觉跟踪器，达到新的最优性能。

Conclusion: UMDATrack通过域适应框架有效提升了视觉目标跟踪在恶劣天气条件下的性能。

Abstract: Visual object tracking has gained promising progress in past decades. Most of
the existing approaches focus on learning target representation in
well-conditioned daytime data, while for the unconstrained real-world scenarios
with adverse weather conditions, e.g. nighttime or foggy environment, the
tremendous domain shift leads to significant performance degradation. In this
paper, we propose UMDATrack, which is capable of maintaining high-quality
target state prediction under various adverse weather conditions within a
unified domain adaptation framework. Specifically, we first use a controllable
scenario generator to synthesize a small amount of unlabeled videos (less than
2% frames in source daytime datasets) in multiple weather conditions under the
guidance of different text prompts. Afterwards, we design a simple yet
effective domain-customized adapter (DCA), allowing the target objects'
representation to rapidly adapt to various weather conditions without redundant
model updating. Furthermore, to enhance the localization consistency between
source and target domains, we propose a target-aware confidence alignment
module (TCA) following optimal transport theorem. Extensive experiments
demonstrate that UMDATrack can surpass existing advanced visual trackers and
lead new state-of-the-art performance by a significant margin. Our code is
available at https://github.com/Z-Z188/UMDATrack.

</details>


### [156] [LoD-Loc v2: Aerial Visual Localization over Low Level-of-Detail City Models using Explicit Silhouette Alignment](https://arxiv.org/abs/2507.00659)
*Juelin Zhu,Shuaibang Peng,Long Wang,Hanlin Tan,Yu Liu,Maojun Zhang,Shen Yan*

Main category: cs.CV

Relevance: 20.0

TL;DR: 提出了一种基于低细节城市模型的空中视觉定位新方法LoD-Loc v2，通过粗到细的策略实现精准定位。


<details>
  <summary>Details</summary>
Motivation: 现有方法依赖高细节城市模型，但全球多数可用或规划的是低细节模型，限制了无人机定位的潜力。

Method: 采用建筑分割网络和姿态假设采样，结合粒子滤波优化，实现低细节模型的定位。

Result: LoD-Loc v2在高细节模型上提升了精度，首次实现低细节模型定位，并超越现有方法。

Conclusion: 该方法扩展了无人机定位的应用范围，尤其在低细节模型上表现优异。

Abstract: We propose a novel method for aerial visual localization over low
Level-of-Detail (LoD) city models. Previous wireframe-alignment-based method
LoD-Loc has shown promising localization results leveraging LoD models.
However, LoD-Loc mainly relies on high-LoD (LoD3 or LoD2) city models, but the
majority of available models and those many countries plan to construct
nationwide are low-LoD (LoD1). Consequently, enabling localization on low-LoD
city models could unlock drones' potential for global urban localization. To
address these issues, we introduce LoD-Loc v2, which employs a coarse-to-fine
strategy using explicit silhouette alignment to achieve accurate localization
over low-LoD city models in the air. Specifically, given a query image, LoD-Loc
v2 first applies a building segmentation network to shape building silhouettes.
Then, in the coarse pose selection stage, we construct a pose cost volume by
uniformly sampling pose hypotheses around a prior pose to represent the pose
probability distribution. Each cost of the volume measures the degree of
alignment between the projected and predicted silhouettes. We select the pose
with maximum value as the coarse pose. In the fine pose estimation stage, a
particle filtering method incorporating a multi-beam tracking approach is used
to efficiently explore the hypothesis space and obtain the final pose
estimation. To further facilitate research in this field, we release two
datasets with LoD1 city models covering 10.7 km , along with real RGB queries
and ground-truth pose annotations. Experimental results show that LoD-Loc v2
improves estimation accuracy with high-LoD models and enables localization with
low-LoD models for the first time. Moreover, it outperforms state-of-the-art
baselines by large margins, even surpassing texture-model-based methods, and
broadens the convergence basin to accommodate larger prior errors.

</details>


### [157] [Instant Particle Size Distribution Measurement Using CNNs Trained on Synthetic Data](https://arxiv.org/abs/2507.00822)
*Yasser El Jarida,Youssef Iraqi,Loubna Mekouar*

Main category: cs.CV

Relevance: 20.0

TL;DR: 该论文提出了一种基于CNN的方法，利用Blender生成的合成粒子图像进行训练，用于实时工业PSD测量。


<details>
  <summary>Details</summary>
Motivation: 传统PSD测量方法耗时且受限于粒子重叠，需要自动化解决方案。

Method: 使用Blender生成合成粒子图像，训练三种CNN架构（ResNet-50、InceptionV3、EfficientNet-B0）预测PSD参数。

Result: EfficientNet-B0在计算效率上表现最佳，适合工业实时部署。

Conclusion: 合成数据能有效训练CNN，为工业PSD监测提供自动化潜力。

Abstract: Accurate particle size distribution (PSD) measurement is important in
industries such as mining, pharmaceuticals, and fertilizer manufacturing,
significantly influencing product quality and operational efficiency.
Traditional PSD methods like sieve analysis and laser diffraction are manual,
time-consuming, and limited by particle overlap. Recent developments in
convolutional neural networks (CNNs) enable automated, real-time PSD estimation
directly from particle images. In this work, we present a CNN-based methodology
trained on realistic synthetic particle imagery generated using Blender's
advanced rendering capabilities. Synthetic data sets using this method can
replicate various industrial scenarios by systematically varying particle
shapes, textures, lighting, and spatial arrangements that closely resemble the
actual configurations. We evaluated three CNN-based architectures, ResNet-50,
InceptionV3, and EfficientNet-B0, for predicting critical PSD parameters (d10,
d50, d90). Results demonstrated comparable accuracy across models, with
EfficientNet-B0 achieving the best computational efficiency suitable for
real-time industrial deployment. This approach shows the effectiveness of
realistic synthetic data for robust CNN training, which offers significant
potential for automated industrial PSD monitoring. The code is released at :
https://github.com/YasserElj/Synthetic-Granular-Gen

</details>


### [158] [Accurate and Efficient Fetal Birth Weight Estimation from 3D Ultrasound](https://arxiv.org/abs/2507.00398)
*Jian Wang,Qiongying Ni,Hongkui Yu,Ruixuan Yao,Jinqiao Ying,Bin Zhang,Xingyi Yang,Jin Peng,Jiongquan Chen,Junxuan Yu,Wenlong Shi,Chaoyu Chen,Zhongnuo Yan,Mingyuan Luo,Gaocheng Cai,Dong Ni,Jing Lu,Xin Yang*

Main category: eess.IV

Relevance: 20.0

TL;DR: 提出了一种基于3D超声体积直接估计胎儿出生体重的方法，结合多尺度特征融合网络和合成样本学习框架，显著提高了预测准确性。


<details>
  <summary>Details</summary>
Motivation: 现有胎儿出生体重估计方法效率低且依赖操作者，深度学习方法因缺乏空间信息而受限。

Method: 采用多尺度特征融合网络（MFFN）和合成样本学习框架（SSLF），结合通道和空间注意力机制及半监督学习。

Result: 方法在绝对误差和百分比误差上优于现有方法，接近资深医生水平。

Conclusion: 该方法为胎儿出生体重估计提供了更准确和高效的解决方案。

Abstract: Accurate fetal birth weight (FBW) estimation is essential for optimizing
delivery decisions and reducing perinatal mortality. However, clinical methods
for FBW estimation are inefficient, operator-dependent, and challenging to
apply in cases of complex fetal anatomy. Existing deep learning methods are
based on 2D standard ultrasound (US) images or videos that lack spatial
information, limiting their prediction accuracy. In this study, we propose the
first method for directly estimating FBW from 3D fetal US volumes. Our approach
integrates a multi-scale feature fusion network (MFFN) and a synthetic
sample-based learning framework (SSLF). The MFFN effectively extracts and fuses
multi-scale features under sparse supervision by incorporating channel
attention, spatial attention, and a ranking-based loss function. SSLF generates
synthetic samples by simply combining fetal head and abdomen data from
different fetuses, utilizing semi-supervised learning to improve prediction
performance. Experimental results demonstrate that our method achieves superior
performance, with a mean absolute error of $166.4\pm155.9$ $g$ and a mean
absolute percentage error of $5.1\pm4.6$%, outperforming existing methods and
approaching the accuracy of a senior doctor. Code is available at:
https://github.com/Qioy-i/EFW.

</details>


### [159] [Tunable Wavelet Unit based Convolutional Neural Network in Optical Coherence Tomography Analysis Enhancement for Classifying Type of Epiretinal Membrane Surgery](https://arxiv.org/abs/2507.00743)
*An Le,Nehal Mehta,William Freeman,Ines Nagel,Melanie Tran,Anna Heinke,Akshay Agnihotri,Lingyun Cheng,Dirk-Uwe Bartsch,Hung Nguyen,Truong Nguyen,Cheolhong An*

Main category: eess.IV

Relevance: 20.0

TL;DR: 该研究开发了一种基于ResNet18的深度学习方法，用于分类视网膜前膜（ERM）手术类型，通过可调小波单元提升分类准确率至78%，优于人类评分者的50%准确率。


<details>
  <summary>Details</summary>
Motivation: 提高视网膜前膜手术分类的准确性和可靠性，以辅助临床决策。

Method: 使用ResNet18 CNN架构，结合可调小波单元（OrthLatt-UwU和PR-Relax-UwU）优化模型性能。

Result: 模型在预处理输入上达到78%准确率，显著优于人类评分者。

Conclusion: CNN模型结合可调小波单元在医疗分类任务中具有潜力。

Abstract: In this study, we developed deep learning-based method to classify the type
of surgery performed for epiretinal membrane (ERM) removal, either internal
limiting membrane (ILM) removal or ERM-alone removal. Our model, based on the
ResNet18 convolutional neural network (CNN) architecture, utilizes
postoperative optical coherence tomography (OCT) center scans as inputs. We
evaluated the model using both original scans and scans preprocessed with
energy crop and wavelet denoising, achieving 72% accuracy on preprocessed
inputs, outperforming the 66% accuracy achieved on original scans. To further
improve accuracy, we integrated tunable wavelet units with two key adaptations:
Orthogonal Lattice-based Wavelet Units (OrthLatt-UwU) and Perfect
Reconstruction Relaxation-based Wavelet Units (PR-Relax-UwU). These units
allowed the model to automatically adjust filter coefficients during training
and were incorporated into downsampling, stride-two convolution, and pooling
layers, enhancing its ability to distinguish between ERM-ILM removal and
ERM-alone removal, with OrthLattUwU boosting accuracy to 76% and PR-Relax-UwU
increasing performance to 78%. Performance comparisons showed that our AI model
outperformed a trained human grader, who achieved only 50% accuracy in
classifying the removal surgery types from postoperative OCT scans. These
findings highlight the potential of CNN based models to improve clinical
decision-making by providing more accurate and reliable classifications. To the
best of our knowledge, this is the first work to employ tunable wavelets for
classifying different types of ERM removal surgery.

</details>


### [160] [RaGNNarok: A Light-Weight Graph Neural Network for Enhancing Radar Point Clouds on Unmanned Ground Vehicles](https://arxiv.org/abs/2507.00937)
*David Hunt,Shaocheng Luo,Spencer Hallyburton,Shafii Nillongo,Yi Li,Tingjun Chen,Miroslav Pajic*

Main category: cs.RO

Relevance: 20.0

TL;DR: RaGNNarok是一个基于图神经网络（GNN）的轻量级框架，用于增强毫米波雷达点云数据，适用于低成本室内移动机器人。


<details>
  <summary>Details</summary>
Motivation: 解决现有雷达定位技术中点云稀疏、噪声和误检的问题，提供一种高效且通用的解决方案。

Method: 采用GNN框架实时处理雷达点云数据，优化点云质量。

Result: 在低成本设备（如Raspberry Pi 5）上实现7.3毫秒的推理时间，并在定位、SLAM和自主导航任务中表现出高可靠性和通用性。

Conclusion: RaGNNarok为低成本室内移动机器人提供了一种高效且鲁棒的解决方案。

Abstract: Low-cost indoor mobile robots have gained popularity with the increasing
adoption of automation in homes and commercial spaces. However, existing lidar
and camera-based solutions have limitations such as poor performance in
visually obscured environments, high computational overhead for data
processing, and high costs for lidars. In contrast, mmWave radar sensors offer
a cost-effective and lightweight alternative, providing accurate ranging
regardless of visibility. However, existing radar-based localization suffers
from sparse point cloud generation, noise, and false detections. Thus, in this
work, we introduce RaGNNarok, a real-time, lightweight, and generalizable graph
neural network (GNN)-based framework to enhance radar point clouds, even in
complex and dynamic environments. With an inference time of just 7.3 ms on the
low-cost Raspberry Pi 5, RaGNNarok runs efficiently even on such
resource-constrained devices, requiring no additional computational resources.
We evaluate its performance across key tasks, including localization, SLAM, and
autonomous navigation, in three different environments. Our results demonstrate
strong reliability and generalizability, making RaGNNarok a robust solution for
low-cost indoor mobile robots.

</details>


### [161] [Evolutionary computing-based image segmentation method to detect defects and features in Additive Friction Stir Deposition Process](https://arxiv.org/abs/2507.00046)
*Akshansh Mishra,Eyob Mesele Sefene,Shivraman Thapliyal*

Main category: cs.CV

Relevance: 10.0

TL;DR: 该论文提出了一种基于进化计算的图像分割方法，用于分析增材摩擦搅拌沉积（AFSD）过程中的完整性。通过粒子群优化（PSO）确定最佳分割阈值，并结合梯度幅度分析和距离变换，生成新颖的注意力加权可视化。


<details>
  <summary>Details</summary>
Motivation: 研究动机在于改进AFSD工艺中的缺陷检测和特征分析，以优化增材制造组件的质量评估。

Method: 方法包括使用PSO算法确定分割阈值，结合梯度幅度分析和距离变换，生成注意力加权可视化。

Result: 结果表明，该方法成功识别了AFSD接头中的不完全结合区域和非均匀性，并提供了定量指标。

Conclusion: 结论表明，基于注意力的分析为增材制造工艺优化和质量评估提供了有效工具。

Abstract: This work proposes an evolutionary computing-based image segmentation
approach for analyzing soundness in Additive Friction Stir Deposition (AFSD)
processes. Particle Swarm Optimization (PSO) was employed to determine optimal
segmentation thresholds for detecting defects and features in multilayer AFSD
builds. The methodology integrates gradient magnitude analysis with distance
transforms to create novel attention-weighted visualizations that highlight
critical interface regions. Five AFSD samples processed under different
conditions were analyzed using multiple visualization techniques i.e.
self-attention maps, and multi-channel visualization. These complementary
approaches reveal subtle material transition zones and potential defect regions
which were not readily observable through conventional imaging. The PSO
algorithm automatically identified optimal threshold values (ranging from
156-173) for each sample, enabling precise segmentation of material interfaces.
The multi-channel visualization technique effectively combines boundary
information (red channel), spatial relationships (green channel), and material
density data (blue channel) into cohesive representations that quantify
interface quality. The results demonstrate that attention-based analysis
successfully identifies regions of incomplete bonding and inhomogeneities in
AFSD joints, providing quantitative metrics for process optimization and
quality assessment of additively manufactured components.

</details>


### [162] [Graph-Based Deep Learning for Component Segmentation of Maize Plants](https://arxiv.org/abs/2507.00182)
*J. I. Ruíz,A. Méndez,E. Rodríguez*

Main category: cs.CV

Relevance: 10.0

TL;DR: 提出了一种基于图神经网络（GNN）和主成分分析（PCA）的新型深度学习架构，用于在LiDAR 3D点云数据中识别植物组件。


<details>
  <summary>Details</summary>
Motivation: 传统方法在处理3D数据和识别植物组件时存在局限性，需要更高效的解决方案。

Method: 结合GNN、PCA和KNN层构建图结构，利用Edge-Conv层增强特征，最后通过GAT分类植物组件。

Result: 在IoU平均指标上超过80%，优于现有基于点云的模型。

Conclusion: 图神经网络方法显著提升了植物组件的分割精度。

Abstract: In precision agriculture, one of the most important tasks when exploring crop
production is identifying individual plant components. There are several
attempts to accomplish this task by the use of traditional 2D imaging, 3D
reconstructions, and Convolutional Neural Networks (CNN). However, they have
several drawbacks when processing 3D data and identifying individual plant
components. Therefore, in this work, we propose a novel Deep Learning
architecture to detect components of individual plants on Light Detection and
Ranging (LiDAR) 3D Point Cloud (PC) data sets. This architecture is based on
the concept of Graph Neural Networks (GNN), and feature enhancing with
Principal Component Analysis (PCA). For this, each point is taken as a vertex
and by the use of a K-Nearest Neighbors (KNN) layer, the edges are established,
thus representing the 3D PC data set. Subsequently, Edge-Conv layers are used
to further increase the features of each point. Finally, Graph Attention
Networks (GAT) are applied to classify visible phenotypic components of the
plant, such as the leaf, stem, and soil. This study demonstrates that our
graph-based deep learning approach enhances segmentation accuracy for
identifying individual plant components, achieving percentages above 80% in the
IoU average, thus outperforming other existing models based on point clouds.

</details>


### [163] [VirtualFencer: Generating Fencing Bouts based on Strategies Extracted from In-the-Wild Videos](https://arxiv.org/abs/2507.00261)
*Zhiyin Lin,Purvi Goel,Joy Yun,C. Karen Liu,Joao Pedro Araujo*

Main category: cs.CV

Relevance: 10.0

TL;DR: VirtualFencer系统通过无监督学习从视频中提取3D击剑动作和策略，并生成逼真的击剑行为。


<details>
  <summary>Details</summary>
Motivation: 击剑运动的动作多样性与双人策略结合，适合数据驱动建模。

Method: 从无监督视频中提取3D动作和策略，生成击剑行为。

Result: 系统展示了自对抗、与真实击剑手对抗以及与专业击剑手交互的能力。

Conclusion: VirtualFencer展示了数据驱动建模在击剑运动中的潜力。

Abstract: Fencing is a sport where athletes engage in diverse yet strategically logical
motions. While most motions fall into a few high-level actions (e.g. step,
lunge, parry), the execution can vary widely-fast vs. slow, large vs. small,
offensive vs. defensive. Moreover, a fencer's actions are informed by a
strategy that often comes in response to the opponent's behavior. This
combination of motion diversity with underlying two-player strategy motivates
the application of data-driven modeling to fencing. We present VirtualFencer, a
system capable of extracting 3D fencing motion and strategy from in-the-wild
video without supervision, and then using that extracted knowledge to generate
realistic fencing behavior. We demonstrate the versatile capabilities of our
system by having it (i) fence against itself (self-play), (ii) fence against a
real fencer's motion from online video, and (iii) fence interactively against a
professional fencer.

</details>


### [164] [Robust Component Detection for Flexible Manufacturing: A Deep Learning Approach to Tray-Free Object Recognition under Variable Lighting](https://arxiv.org/abs/2507.00852)
*Fatemeh Sadat Daneshmand*

Main category: cs.CV

Relevance: 10.0

TL;DR: 论文提出了一种基于Mask R-CNN的计算机视觉系统，用于工业机器人在非结构化环境中检测和抓取笔组件，解决了无位置约束、光照变化和低成本摄像头的挑战。


<details>
  <summary>Details</summary>
Motivation: 工业4.0中的柔性制造系统需要机器人能够在非结构化环境中处理物体，而无需严格的定位约束。

Method: 采用Mask R-CNN方法，在完整的笔制造线上实现和评估，解决了无位置约束的物体检测、极端光照变化的鲁棒性以及低成本摄像头的可靠性问题。

Result: 系统在多样化光照条件下实现了95%的检测准确率，减少了30%的设置时间，显著提高了制造灵活性。

Conclusion: 该方法通过四种不同光照场景的广泛测试验证，展示了实际工业部署的适用性。

Abstract: Flexible manufacturing systems in Industry 4.0 require robots capable of
handling objects in unstructured environments without rigid positioning
constraints. This paper presents a computer vision system that enables
industrial robots to detect and grasp pen components in arbitrary orientations
without requiring structured trays, while maintaining robust performance under
varying lighting conditions. We implement and evaluate a Mask R-CNN-based
approach on a complete pen manufacturing line at ZHAW, addressing three
critical challenges: object detection without positional constraints,
robustness to extreme lighting variations, and reliable performance with
cost-effective cameras. Our system achieves 95% detection accuracy across
diverse lighting conditions while eliminating the need for structured component
placement, demonstrating a 30% reduction in setup time and significant
improvement in manufacturing flexibility. The approach is validated through
extensive testing under four distinct lighting scenarios, showing practical
applicability for real-world industrial deployment.

</details>


### [165] [Scope Meets Screen: Lessons Learned in Designing Composite Visualizations for Marksmanship Training Across Skill Levels](https://arxiv.org/abs/2507.00333)
*Emin Zerman,Jonas Carlsson,Mårten Sjöström*

Main category: cs.HC

Relevance: 10.0

TL;DR: 该研究开发了一个射击可视化系统，通过第一人称视频和图形化指标帮助射手和教练分析射击表现，结果显示复合视图（如仪表盘风格）最受欢迎且有效。


<details>
  <summary>Details</summary>
Motivation: 当前射击训练主要依赖重复练习和事后分析，缺乏实时和直观的反馈工具。

Method: 开发了五种复合可视化视图，结合第一人称视频和图形化指标，并通过混合方法研究（任务、偏好比较和访谈）评估其效果。

Result: 仪表盘风格的复合视图（结合原始视频、极坐标图和图表）在10名参与者中有9人偏好，且对不同技能水平的射手均有帮助。

Conclusion: 研究表明，第一人称视频与可视化分析的结合对射击训练有显著价值，并可推广到其他精准运动。

Abstract: Marksmanship practices are required in various professions, including police,
military personnel, hunters, as well as sports shooters, such as Olympic
shooting, biathlon, and modern pentathlon. The current form of training and
coaching is mostly based on repetition, where the coach does not see through
the eyes of the shooter, and analysis is limited to stance and accuracy
post-session. In this study, we present a shooting visualization system and
evaluate its perceived effectiveness for both novice and expert shooters. To
achieve this, five composite visualizations were developed using first-person
shooting video recordings enriched with overlaid metrics and graphical
summaries. These views were evaluated with 10 participants (5 expert marksmen,
5 novices) through a mixed-methods study including shot-count and aiming
interpretation tasks, pairwise preference comparisons, and semi-structured
interviews. The results show that a dashboard-style composite view, combining
raw video with a polar plot and selected graphs, was preferred in 9 of 10 cases
and supported understanding across skill levels. The insights gained from this
design study point to the broader value of integrating first-person video with
visual analytics for coaching, and we suggest directions for applying this
approach to other precision-based sports.

</details>


### [166] [Medical Image Segmentation Using Advanced Unet: VMSE-Unet and VM-Unet CBAM+](https://arxiv.org/abs/2507.00511)
*Sayandeep Kanrar,Raja Piyush,Qaiser Razi,Debanshi Chakraborty,Vikas Hassija,GSS Chalapathi*

Main category: eess.IV

Relevance: 10.0

TL;DR: 提出了两种改进的深度学习架构VMSE U-Net和VM-Unet CBAM+，用于提升医学图像分割性能。


<details>
  <summary>Details</summary>
Motivation: 通过结合Squeeze-and-Excitation (SE)和Convolutional Block Attention Module (CBAM)技术，优化传统VM U-Net框架，以提高分割精度和计算效率。

Method: 在VM U-Net中集成SE和CBAM模块，改进特征定位和计算效率。

Result: VMSE-Unet在多个数据集上表现最优，具有高精度、低损失和高效计算性能。

Conclusion: VMSE-Unet是医学图像分析的有力工具，未来研究可进一步优化其准确性和鲁棒性。

Abstract: In this paper, we present the VMSE U-Net and VM-Unet CBAM+ model, two
cutting-edge deep learning architectures designed to enhance medical image
segmentation. Our approach integrates Squeeze-and-Excitation (SE) and
Convolutional Block Attention Module (CBAM) techniques into the traditional VM
U-Net framework, significantly improving segmentation accuracy, feature
localization, and computational efficiency. Both models show superior
performance compared to the baseline VM-Unet across multiple datasets. Notably,
VMSEUnet achieves the highest accuracy, IoU, precision, and recall while
maintaining low loss values. It also exhibits exceptional computational
efficiency with faster inference times and lower memory usage on both GPU and
CPU. Overall, the study suggests that the enhanced architecture VMSE-Unet is a
valuable tool for medical image analysis. These findings highlight its
potential for real-world clinical applications, emphasizing the importance of
further research to optimize accuracy, robustness, and computational
efficiency.

</details>


### [167] [Stable Tracking of Eye Gaze Direction During Ophthalmic Surgery](https://arxiv.org/abs/2507.00635)
*Tinghe Hong,Shenlin Cai,Boyang Li,Kai Huang*

Main category: cs.RO

Relevance: 10.0

TL;DR: 论文提出了一种结合机器学习和传统算法的创新性眼动追踪方法，用于眼科手术机器人，解决了现有技术依赖额外传感器和面部检测的问题。


<details>
  <summary>Details</summary>
Motivation: 现有眼动追踪技术在手术中依赖额外传感器且易受遮挡影响，限制了术前导航的一致性和精确性。

Method: 结合机器学习和传统算法，无需依赖面部标志，能在不同光照和阴影条件下稳定检测虹膜和估计视线。

Result: 实验结果显示，眼动方向估计的平均误差为0.58度，机器人手臂运动的平均控制误差为2.08度。

Conclusion: 该方法显著提升了眼科手术机器人的眼动追踪精度和稳定性。

Abstract: Ophthalmic surgical robots offer superior stability and precision by reducing
the natural hand tremors of human surgeons, enabling delicate operations in
confined surgical spaces. Despite the advancements in developing vision- and
force-based control methods for surgical robots, preoperative navigation
remains heavily reliant on manual operation, limiting the consistency and
increasing the uncertainty. Existing eye gaze estimation techniques in the
surgery, whether traditional or deep learning-based, face challenges including
dependence on additional sensors, occlusion issues in surgical environments,
and the requirement for facial detection. To address these limitations, this
study proposes an innovative eye localization and tracking method that combines
machine learning with traditional algorithms, eliminating the requirements of
landmarks and maintaining stable iris detection and gaze estimation under
varying lighting and shadow conditions. Extensive real-world experiment results
show that our proposed method has an average estimation error of 0.58 degrees
for eye orientation estimation and 2.08-degree average control error for the
robotic arm's movement based on the calculated orientation.

</details>


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [168] [Thinking About Thinking: SAGE-nano's Inverse Reasoning for Self-Aware Language Models](https://arxiv.org/abs/2507.00092)
*Basab Jha,Firoj Paudel,Ujjwal Puri,Zhang Yuting,Choi Donghyuk,Wang Junhao*

Main category: cs.AI

Relevance: 90.0

TL;DR: 论文提出了一种名为“逆向推理”的新范式，使LLMs能够事后分解和解释其推理链。通过SAGE-nano模型，该方法在推理准确性和解释质量上表现优异。


<details>
  <summary>Details</summary>
Motivation: 解决LLMs推理过程的黑箱问题，提高其决策透明度和可解释性。

Method: 采用逆向推理和元认知结构，通过反向注意力流识别关键决策点并生成解释。

Result: SAGE-nano在AQUA-RAT等测试中表现出色（推理准确率74.6%，解释质量92.1%），性能接近Claude-3.5 Sonnet或GPT-4o。

Conclusion: 逆向推理框架显著提升了LLMs的透明性和推理性能，为AI安全、教育和科学发现开辟了新途径。

Abstract: Large Language Models (LLMs) have demonstrated remarkable capabilities at
solving complex reasoning tasks with Chain-of-Thought (CoT) prompting, but
their decision-making processes remain somewhat blackbox. We introduce
textbfinverse reasoning, a novel paradigm enabling LLMs to decompose and
explain their own reasoning chains post-hoc. Our approach, used in SAGE-nano, a
4-billion-parameter reasoning model, employs a metacognitive structure that
reflects back via attention processes to identify major decision points and
generate explanations of reasoning choices. While typical CoT approaches are
directed towards forward reasoning generation, inverse reasoning provides
insight into why specific reasoning chains were selected over others. Through
thorough testing of logical reasoning puzzles, math problems and ethical
dilemmas from AQUA-RAT, CommonsenseQA, and customized benchmarks, we
demonstrate that SAGE-nano is at the cutting edge both on reasoning accuracy
(74.6% on AQUA-RAT) and explanation quality (92.1% human preference score) for
its task, and offers performance almost on par with models like Claude-3.5
Sonnet or GPT-4o. Our contributions are: (i) the first rigorous framework for
LLM self-reflection via inverse reasoning, (ii) a novel metalearning framework
to reverse the attention flow, (iii) comprehensive evaluation frameworks for
reasoning transparency, and (iv) evidence that increasing reasoning using
inverse reasoning improves interpretability along with reasoning performance.
Our work creates new avenues for transparent AI systems and closes significant
gaps in AI safety, education, and scientific discovery.

</details>


### [169] [ASTRO: Teaching Language Models to Reason by Reflecting and Backtracking In-Context](https://arxiv.org/abs/2507.00417)
*Joongwon Kim,Anirudh Goyal,Liang Tan,Hannaneh Hajishirzi,Srinivasan Iyer,Tianlu Wang*

Main category: cs.AI

Relevance: 90.0

TL;DR: ASTRO框架通过自回归搜索训练语言模型，利用自我反思、回溯和探索提升推理能力，显著提高了Llama 3在数学问题上的表现。


<details>
  <summary>Details</summary>
Motivation: 提升非推理型语言模型（如Llama 3）的推理能力，通过结构化搜索行为训练模型。

Method: 使用蒙特卡洛树搜索（MCTS）生成合成数据集，将搜索轨迹转化为自然语言链式思维，并通过强化学习（RL）进一步优化。

Result: 在MATH-500、AMC 2023和AIME 2024上分别取得16.0%、26.9%和20.0%的性能提升。

Conclusion: 结构化搜索训练是提升开放LLM推理能力的有效方法。

Abstract: We introduce ASTRO, the "Autoregressive Search-Taught Reasoner", a framework
for training language models to reason like search algorithms, explicitly
leveraging self-reflection, backtracking, and exploration in their outputs.
Recently, training large language models (LLMs) via reinforcement learning (RL)
has led to the advent of reasoning models with greatly enhanced reasoning
capabilities. Open-source replications of reasoning models, while successful,
build upon models that already exhibit strong reasoning capabilities along with
search behavior observed even before RL. As a result, it is yet unclear how to
boost the reasoning capabilities of other non-reasoner models including Llama
3. ASTRO teaches such models to internalize structured search behavior through
a synthetic dataset derived from Monte Carlo Tree Search (MCTS) over
mathematical problem-solving trajectories. By converting search traces into
natural language chain-of-thoughts that capture both successes and recoveries
from failure, ASTRO bootstraps models with a rich prior for exploration during
RL. We finetune our models on these search-derived traces and further improve
performance via RL with verifiable rewards. We apply ASTRO to the Llama 3
family of models and achieve absolute performance gains of 16.0% on MATH-500,
26.9% on AMC 2023, and 20.0% on AIME 2024, especially improving upon
challenging problems that require iterative correction. Our results demonstrate
that search-inspired training offers a principled way to instill robust
reasoning capabilities into open LLMs.

</details>


### [170] [Enhancing Reasoning Capabilities in SLMs with Reward Guided Dataset Distillation](https://arxiv.org/abs/2507.00054)
*Shreyansh Padarha*

Main category: cs.AI

Relevance: 85.0

TL;DR: AdvDistill提出了一种奖励引导的数据集蒸馏框架，通过多代教师模型响应和基于规则的验证器分配奖励，显著提升了学生模型在数学和复杂推理任务上的性能。


<details>
  <summary>Details</summary>
Motivation: 当前的知识蒸馏技术通常局限于学生模型复制教师模型的分布内响应，限制了其泛化能力，尤其是在推理任务上。AdvDistill旨在通过奖励机制改进这一过程。

Method: 利用教师模型对每个提示生成多个响应，并通过基于规则的验证器分配奖励。这些奖励作为训练学生模型时的权重。

Result: AdvDistill显著提升了学生模型在数学和复杂推理任务上的性能。

Conclusion: 奖励机制在数据集蒸馏过程中的引入是有效的，能够显著提升学生模型的性能。

Abstract: The push to compress and impart the proficiency of Large Language Models
(LLMs) into more deployable and efficient Small Language Models (SLMs) has
benefited from improvements in knowledge distillation (KD) techniques. These
techniques allow a smaller student model to learn from a more capable and
larger teacher model's responses. However, distillation often revolves around
the student model merely copying the teacher's in-distribution responses,
limiting its generalisability. This limitation is amplified on reasoning tasks
and can be computationally expensive. In this study, we propose AdvDistill, a
reward-guided dataset distillation framework. We utilise multiple generations
(responses) from a teacher for each prompt and assign rewards based on
rule-based verifiers. These varying and normally distributed rewards serve as
weights when training student models. Our methods and their subsequent
behavioural analysis demonstrate a significant improvement in student model
performance for mathematical and complex reasoning tasks, showcasing the
efficacy and benefits of incorporating a rewarding mechanism in dataset
distillation processes.

</details>


### [171] [Does Math Reasoning Improve General LLM Capabilities? Understanding Transferability of LLM Reasoning](https://arxiv.org/abs/2507.00432)
*Maggie Huan,Yuetai Li,Tuney Zheng,Xiaoyu Xu,Seungone Kim,Minxin Du,Radha Poovendran,Graham Neubig,Xiang Yue*

Main category: cs.AI

Relevance: 85.0

TL;DR: 论文探讨了数学推理能力在大型语言模型（LLMs）中的表现是否具有广泛的问题解决能力，还是仅仅是对特定任务的过拟合。通过评估20多个模型，发现数学表现好的模型在其他领域表现不佳。实验表明，强化学习（RL）调优的模型具有更好的泛化能力，而监督微调（SFT）调优的模型容易遗忘通用能力。


<details>
  <summary>Details</summary>
Motivation: 研究数学推理能力的提升是否反映了LLMs的广泛问题解决能力，还是仅仅是对特定任务的过拟合。

Method: 评估20多个开源推理调优模型，涵盖数学、科学问答、代理规划、编码和指令遵循任务。通过Qwen3-14B模型进行对照实验，比较RL和SFT调优方法的效果。

Result: 数学表现好的模型在其他领域表现不佳；RL调优的模型泛化能力更强，而SFT调优的模型容易遗忘通用能力。

Conclusion: 需要重新思考后训练方法，尤其是依赖SFT蒸馏数据来提升推理模型的做法。

Abstract: Math reasoning has become the poster child of progress in large language
models (LLMs), with new models rapidly surpassing human-level performance on
benchmarks like MATH and AIME. But as math leaderboards improve week by week,
it is worth asking: do these gains reflect broader problem-solving ability or
just narrow overfitting? To answer this question, we evaluate over 20
open-weight reasoning-tuned models across a broad suite of tasks, including
math, scientific QA, agent planning, coding, and standard
instruction-following. We surprisingly find that most models that succeed in
math fail to transfer their gains to other domains. To rigorously study this
phenomenon, we conduct controlled experiments on Qwen3-14B models using
math-only data but different tuning methods. We find that reinforcement
learning (RL)-tuned models generalize well across domains, while supervised
fine-tuning (SFT)-tuned models often forget general capabilities. Latent-space
representation and token-space distribution shift analyses reveal that SFT
induces substantial representation and output drift, while RL preserves
general-domain structure. Our results suggest a need to rethink standard
post-training recipes, particularly the reliance on SFT-distilled data for
advancing reasoning models.

</details>


### [172] [Can Large Language Models Develop Strategic Reasoning? Post-training Insights from Learning Chess](https://arxiv.org/abs/2507.00726)
*Dongyoon Hwang,Hojoon Lee,Jaegul Choo,Dongmin Park,Jongho Park*

Main category: cs.AI

Relevance: 85.0

TL;DR: 论文探讨了通过强化学习（RL）提升大型语言模型（LLMs）在象棋中的战略推理能力，发现基于知识蒸馏的密集奖励优于稀疏奖励，但模型表现仍远低于专家水平。


<details>
  <summary>Details</summary>
Motivation: 研究LLMs是否可以通过RL在象棋中发展战略推理能力，填补这一领域的空白。

Method: 利用象棋预训练的动作价值网络为LLM的输出动作质量提供密集奖励（知识蒸馏），并与稀疏二进制奖励进行对比。

Result: 密集奖励表现优于稀疏奖励，但所有模型表现均远低于专家水平。

Conclusion: 模型在象棋理解上的预训练不足可能是限制因素，仅靠RL难以完全克服。

Abstract: While reinforcement learning (RL) for large language models (LLMs) has shown
promise in mathematical reasoning, strategic reasoning for LLMs using RL
remains largely unexplored. We investigate whether LLMs can develop strategic
reasoning capabilities through RL in chess. To this end, we leverage a
chess-pretrained action-value network to provide dense reward on the LLM's
output move quality, which can be seen as a form of knowledge distillation. Our
experiments show that our distillation-based dense rewards often outperform
sparse binary rewards. However, surprisingly, all models plateau far below
expert levels. We provide SFT and RL ablations on chess reasoning training and
find evidence that this limitation stems from a deficit in the pretrained
models' internal understanding of chess--a deficit which RL alone may not be
able to fully overcome.

</details>


### [173] [Thinking Beyond Tokens: From Brain-Inspired Intelligence to Cognitive Foundations for Artificial General Intelligence and its Societal Impact](https://arxiv.org/abs/2507.00951)
*Rizwan Qureshi,Ranjan Sapkota,Abbas Shah,Amgad Muneer,Anas Zafar,Ashmal Vayani,Maged Shoman,Abdelrahman B. M. Eldaly,Kai Zhang,Ferhat Sadak,Shaina Raza,Xinqi Fan,Ravid Shwartz-Ziv,Hong Yan,Vinjia Jain,Aman Chadha,Manoj Karkee,Jia Wu,Philip Torr,Seyedali Mirjalili*

Main category: cs.AI

Relevance: 85.0

TL;DR: 该论文探讨了实现人工通用智能（AGI）的跨学科方法，分析了当前模型的局限性，并提出了结合检索、规划和动态工具使用的Agentic RAG框架。


<details>
  <summary>Details</summary>
Motivation: 尽管现有大模型（如GPT-4.5、DeepSeek等）在多模态和部分推理能力上有所突破，但其仍受限于基于token的预测和缺乏实际代理能力。论文旨在通过跨学科研究推动AGI的发展。

Method: 论文综合了人工智能、认知神经科学、心理学等领域的研究，提出了Agentic RAG框架，并探讨了模块化推理、持久记忆和多智能体协调的作用。

Result: 论文强调了记忆与推理的整合对实现真正智能的重要性，并展示了神经符号系统、强化学习等技术在缩小统计学习与目标导向认知之间差距的潜力。

Conclusion: 实现AGI需要解决科学、技术和伦理挑战，未来的方向包括模块化、自适应和自改进的智能组件设计。

Abstract: Can machines truly think, reason and act in domains like humans? This
enduring question continues to shape the pursuit of Artificial General
Intelligence (AGI). Despite the growing capabilities of models such as GPT-4.5,
DeepSeek, Claude 3.5 Sonnet, Phi-4, and Grok 3, which exhibit multimodal
fluency and partial reasoning, these systems remain fundamentally limited by
their reliance on token-level prediction and lack of grounded agency. This
paper offers a cross-disciplinary synthesis of AGI development, spanning
artificial intelligence, cognitive neuroscience, psychology, generative models,
and agent-based systems. We analyze the architectural and cognitive foundations
of general intelligence, highlighting the role of modular reasoning, persistent
memory, and multi-agent coordination. In particular, we emphasize the rise of
Agentic RAG frameworks that combine retrieval, planning, and dynamic tool use
to enable more adaptive behavior. We discuss generalization strategies,
including information compression, test-time adaptation, and training-free
methods, as critical pathways toward flexible, domain-agnostic intelligence.
Vision-Language Models (VLMs) are reexamined not just as perception modules but
as evolving interfaces for embodied understanding and collaborative task
completion. We also argue that true intelligence arises not from scale alone
but from the integration of memory and reasoning: an orchestration of modular,
interactive, and self-improving components where compression enables adaptive
behavior. Drawing on advances in neurosymbolic systems, reinforcement learning,
and cognitive scaffolding, we explore how recent architectures begin to bridge
the gap between statistical learning and goal-directed cognition. Finally, we
identify key scientific, technical, and ethical challenges on the path to AGI.

</details>


### [174] [Enhancing LLM Agent Safety via Causal Influence Prompting](https://arxiv.org/abs/2507.00979)
*Dongyoon Hahm,Woogyeol Jin,June Suk Choi,Sungsoo Ahn,Kimin Lee*

Main category: cs.AI

Relevance: 85.0

TL;DR: CIP利用因果影响图（CIDs）提升LLM驱动的自主代理的安全性，通过结构化表示因果关系，优化决策过程。


<details>
  <summary>Details</summary>
Motivation: 确保LLM驱动的自主代理在辅助任务中的安全性和可靠性，防止意外后果。

Method: 1. 基于任务规范初始化CID；2. 使用CID指导代理与环境交互；3. 根据观察结果迭代优化CID。

Result: 实验表明，CIP在代码执行和移动设备控制任务中有效提升了安全性。

Conclusion: CIP通过CIDs显著增强了LLM代理的安全性，适用于需要高可靠性的任务。

Abstract: As autonomous agents powered by large language models (LLMs) continue to
demonstrate potential across various assistive tasks, ensuring their safe and
reliable behavior is crucial for preventing unintended consequences. In this
work, we introduce CIP, a novel technique that leverages causal influence
diagrams (CIDs) to identify and mitigate risks arising from agent
decision-making. CIDs provide a structured representation of cause-and-effect
relationships, enabling agents to anticipate harmful outcomes and make safer
decisions. Our approach consists of three key steps: (1) initializing a CID
based on task specifications to outline the decision-making process, (2)
guiding agent interactions with the environment using the CID, and (3)
iteratively refining the CID based on observed behaviors and outcomes.
Experimental results demonstrate that our method effectively enhances safety in
both code execution and mobile device control tasks.

</details>


### [175] [State and Memory is All You Need for Robust and Reliable AI Agents](https://arxiv.org/abs/2507.00081)
*Matthew Muhoberac,Atharva Parikh,Nirvi Vakharia,Saniya Virani,Aco Radujevic,Savannah Wood,Meghav Verma,Dimitri Metaxotos,Jeyaraman Soundararajan,Thierry Masquelin,Alexander G. Godfrey,Sean Gardner,Dobrila Rudnicki,Sam Michael,Gaurav Chopra*

Main category: cs.MA

Relevance: 85.0

TL;DR: SciBORG是一个模块化的代理框架，通过动态构建LLM代理并增强有限状态自动机（FSA）记忆，实现复杂科学工作流中的自主规划和可靠执行。


<details>
  <summary>Details</summary>
Motivation: 解决LLM在复杂科学工作流中因内存、规划和工具集成限制而应用受限的问题。

Method: 动态构建LLM代理，结合FSA记忆以实现状态跟踪和上下文感知决策，无需手动提示工程。

Result: SciBORG在物理和虚拟硬件中验证，展示了可靠执行、自适应规划和可解释状态转换。

Conclusion: 内存和状态感知是代理规划和可靠性的关键，为复杂环境中AI代理部署提供了通用基础。

Abstract: Large language models (LLMs) have enabled powerful advances in natural
language understanding and generation. Yet their application to complex,
real-world scientific workflows remain limited by challenges in memory,
planning, and tool integration. Here, we introduce SciBORG (Scientific Bespoke
Artificial Intelligence Agents Optimized for Research Goals), a modular agentic
framework that allows LLM-based agents to autonomously plan, reason, and
achieve robust and reliable domain-specific task execution. Agents are
constructed dynamically from source code documentation and augmented with
finite-state automata (FSA) memory, enabling persistent state tracking and
context-aware decision-making. This approach eliminates the need for manual
prompt engineering and allows for robust, scalable deployment across diverse
applications via maintaining context across extended workflows and to recover
from tool or execution failures. We validate SciBORG through integration with
both physical and virtual hardware, such as microwave synthesizers for
executing user-specified reactions, with context-aware decision making and
demonstrate its use in autonomous multi-step bioassay retrieval from the
PubChem database utilizing multi-step planning, reasoning, agent-to-agent
communication and coordination for execution of exploratory tasks. Systematic
benchmarking shows that SciBORG agents achieve reliable execution, adaptive
planning, and interpretable state transitions. Our results show that memory and
state awareness are critical enablers of agentic planning and reliability,
offering a generalizable foundation for deploying AI agents in complex
environments.

</details>


### [176] [Estimating Correctness Without Oracles in LLM-Based Code Generation](https://arxiv.org/abs/2507.00057)
*Thomas Valentin,Ardi Madadi,Gaetano Sapia,Marcel Böhme*

Main category: cs.PL

Relevance: 85.0

TL;DR: 提出了一种称为“不连贯性”的度量方法，用于在没有正确实现（oracle）的情况下量化LLM生成代码的错误概率，并证明其与oracle评估结果高度一致。


<details>
  <summary>Details</summary>
Motivation: LLM在代码生成中常产生语法正确但事实错误的输出，需一种无需oracle的错误量化方法。

Method: 提出“不连贯性”度量，作为错误概率的下界，并通过实验验证其有效性。

Result: 实验显示，该方法能自动识别约三分之二的错误程序，且与oracle评估排名高度一致。

Conclusion: 不连贯性评估可替代oracle评估，为LLM代码生成的可靠性提供新工具。

Abstract: Generating code from natural language specifications is one of the most
successful applications of Large Language Models (LLMs). Yet, they hallucinate:
LLMs produce outputs that may be grammatically correct but are factually
incorrect. Without an existing, correct implementation (i.e., an oracle), can
we quantify how likely the generated program is correct?
  In this paper, we propose a measure of incorrectness, called incoherence,
that can be estimated efficiently in the absence of an oracle and provides a
lower bound on the error, i.e., the probability that the LLM-generated program
for that specification is incorrect. Our experiments demonstrate an
extraordinary effectiveness. For the average code generation task, our
incoherence-based methodology can automatically identify about two-thirds of
incorrect programs without reports of false positives. In fact, an oracle-based
evaluation of LLMs can be reliably replaced by an incoherence-based evaluation.
In particular, we find a very strong agreement between the ranking of LLMs by
the number of programs deemed correct via an oracle (pass@1) and the ranking of
LLMs by the number of programs deemed correct via our incoherence.

</details>


### [177] [How large language models judge and influence human cooperation](https://arxiv.org/abs/2507.00088)
*Alexandre S. Pires,Laurens Samson,Sennay Ghebreab,Fernando P. Santos*

Main category: physics.soc-ph

Relevance: 85.0

TL;DR: 研究探讨了大型语言模型（LLMs）在社交决策中对人类合作行为的影响，发现LLMs在评价合作行为时存在差异，并可能影响长期合作动态。


<details>
  <summary>Details</summary>
Motivation: LLMs在社交决策中的广泛应用可能影响人类的道德和政治判断，但长期影响尚不明确，尤其是对人类合作行为的潜在影响。

Method: 通过21种不同的LLMs评估合作行为，并结合进化博弈论模型分析LLM驱动的判断对合作动态的长期影响。

Result: LLMs在评价合作行为时对良好对手表现一致，但对声誉不佳的个体存在差异；这些差异可能显著影响合作的普遍性。

Conclusion: 研究强调了在LLM应用中对其规范进行谨慎对齐的必要性，以保护人类合作行为。

Abstract: Humans increasingly rely on large language models (LLMs) to support decisions
in social settings. Previous work suggests that such tools shape people's moral
and political judgements. However, the long-term implications of LLM-based
social decision-making remain unknown. How will human cooperation be affected
when the assessment of social interactions relies on language models? This is a
pressing question, as human cooperation is often driven by indirect
reciprocity, reputations, and the capacity to judge interactions of others.
Here, we assess how state-of-the-art LLMs judge cooperative actions. We provide
21 different LLMs with an extensive set of examples where individuals cooperate
-- or refuse cooperating -- in a range of social contexts, and ask how these
interactions should be judged. Furthermore, through an evolutionary
game-theoretical model, we evaluate cooperation dynamics in populations where
the extracted LLM-driven judgements prevail, assessing the long-term impact of
LLMs on human prosociality. We observe a remarkable agreement in evaluating
cooperation against good opponents. On the other hand, we notice within- and
between-model variance when judging cooperation with ill-reputed individuals.
We show that the differences revealed between models can significantly impact
the prevalence of cooperation. Finally, we test prompts to steer LLM norms,
showing that such interventions can shape LLM judgements, particularly through
goal-oriented prompts. Our research connects LLM-based advices and long-term
social dynamics, and highlights the need to carefully align LLM norms in order
to preserve human cooperation.

</details>


### [178] [VoyagerVision: Investigating the Role of Multi-modal Information for Open-ended Learning Systems](https://arxiv.org/abs/2507.00079)
*Ethan Smyth,Alessandro Suglia*

Main category: cs.AI

Relevance: 75.0

TL;DR: 论文提出VoyagerVision，一种多模态模型，通过视觉反馈在Minecraft中构建结构，扩展了Voyager的能力。


<details>
  <summary>Details</summary>
Motivation: 探索如何通过视觉输入增强LLM的空间理解能力，从而扩展其任务执行范围。

Method: 结合视觉输入（截图）和多模态模型，设计VoyagerVision，并在Minecraft中测试其构建能力。

Result: VoyagerVision平均能在50次迭代中构建2.75个独特结构，且在平坦世界中成功率为50%。

Conclusion: 视觉输入显著提升了LLM的任务执行能力，尤其是在空间任务中。

Abstract: Open-endedness is an active field of research in the pursuit of capable
Artificial General Intelligence (AGI), allowing models to pursue tasks of their
own choosing. Simultaneously, recent advancements in Large Language Models
(LLMs) such as GPT-4o [9] have allowed such models to be capable of
interpreting image inputs. Implementations such as OMNI-EPIC [4] have made use
of such features, providing an LLM with pixel data of an agent's POV to parse
the environment and allow it to solve tasks. This paper proposes that providing
these visual inputs to a model gives it greater ability to interpret spatial
environments, and as such, can increase the number of tasks it can successfully
perform, extending its open-ended potential. To this aim, this paper proposes
VoyagerVision -- a multi-modal model capable of creating structures within
Minecraft using screenshots as a form of visual feedback, building on the
foundation of Voyager. VoyagerVision was capable of creating an average of 2.75
unique structures within fifty iterations of the system, as Voyager was
incapable of this, it is an extension in an entirely new direction.
Additionally, in a set of building unit tests VoyagerVision was successful in
half of all attempts in flat worlds, with most failures arising in more complex
structures. Project website is available at
https://esmyth-dev.github.io/VoyagerVision.github.io/

</details>


### [179] [SafeMobile: Chain-level Jailbreak Detection and Automated Evaluation for Multimodal Mobile Agents](https://arxiv.org/abs/2507.00841)
*Siyuan Liang,Tianmeng Fang,Zhe Liu,Aishan Liu,Yan Xiao,Jinyuan He,Ee-Chien Chang,Xiaochun Cao*

Main category: cs.AI

Relevance: 75.0

TL;DR: 该论文探讨了多模态基础模型在智能代理系统中的安全问题，提出了一种结合行为序列信息的风险识别机制，并设计了基于大型语言模型的自动化辅助评估方案。


<details>
  <summary>Details</summary>
Motivation: 随着多模态基础模型在智能代理系统中的广泛应用，系统面临越来越多的越狱风险，现有安全措施在复杂交互中仍有局限性，缺乏高效的自动化风险评估方法。

Method: 通过构建行为序列信息的风险识别机制，并设计基于大型语言模型的自动化辅助评估方案。

Result: 初步验证表明，该方法能一定程度上提高风险行为的识别率，降低代理被越狱的概率。

Conclusion: 该研究为多模态智能代理系统的安全风险建模和保护提供了有价值的参考。

Abstract: With the wide application of multimodal foundation models in intelligent
agent systems, scenarios such as mobile device control, intelligent assistant
interaction, and multimodal task execution are gradually relying on such large
model-driven agents. However, the related systems are also increasingly exposed
to potential jailbreak risks. Attackers may induce the agents to bypass the
original behavioral constraints through specific inputs, and then trigger
certain risky and sensitive operations, such as modifying settings, executing
unauthorized commands, or impersonating user identities, which brings new
challenges to system security. Existing security measures for intelligent
agents still have limitations when facing complex interactions, especially in
detecting potentially risky behaviors across multiple rounds of conversations
or sequences of tasks. In addition, an efficient and consistent automated
methodology to assist in assessing and determining the impact of such risks is
currently lacking. This work explores the security issues surrounding mobile
multimodal agents, attempts to construct a risk discrimination mechanism by
incorporating behavioral sequence information, and designs an automated
assisted assessment scheme based on a large language model. Through preliminary
validation in several representative high-risk tasks, the results show that the
method can improve the recognition of risky behaviors to some extent and assist
in reducing the probability of agents being jailbroken. We hope that this study
can provide some valuable references for the security risk modeling and
protection of multimodal intelligent agent systems.

</details>


### [180] [Feature Integration Spaces: Joint Training Reveals Dual Encoding in Neural Network Representations](https://arxiv.org/abs/2507.00269)
*Omar Claflin*

Main category: q-bio.NC

Relevance: 75.0

TL;DR: 论文提出神经网络的激活可以分解为特征身份和特征整合两个互补空间，开发了联合训练架构，显著提升了重构性能和减少了误差，并通过干预实验验证了整合特征的选择性敏感性。


<details>
  <summary>Details</summary>
Motivation: 现有稀疏自编码器（SAE）方法在消除多义性和行为错误方面表现不佳，作者假设神经网络信息编码分为特征身份和整合两个互补空间，并验证这一假设。

Method: 提出顺序和联合训练架构，同时捕捉身份和整合模式，通过非线性组件高效捕获计算关系，并进行干预实验验证。

Result: 联合训练提升重构性能41.3%，减少KL散度误差51.6%，非线性组件独立提升16.5%，干预实验显示整合特征对行为输出有系统性影响。

Conclusion: 研究证实了神经网络中的双重编码、非线性特征交互的重要性，并为下一代SAE设计提供了新范式。

Abstract: Current sparse autoencoder (SAE) approaches to neural network
interpretability assume that activations can be decomposed through linear
superposition into sparse, interpretable features. Despite high reconstruction
fidelity, SAEs consistently fail to eliminate polysemanticity and exhibit
pathological behavioral errors. We propose that neural networks encode
information in two complementary spaces compressed into the same substrate:
feature identity and feature integration. To test this dual encoding
hypothesis, we develop sequential and joint-training architectures to capture
identity and integration patterns simultaneously. Joint training achieves 41.3%
reconstruction improvement and 51.6% reduction in KL divergence errors. This
architecture spontaneously develops bimodal feature organization: low squared
norm features contributing to integration pathways and the rest contributing
directly to the residual. Small nonlinear components (3% of parameters) achieve
16.5% standalone improvements, demonstrating parameter-efficient capture of
computational relationships crucial for behavior. Additionally, intervention
experiments using 2x2 factorial stimulus designs demonstrated that integration
features exhibit selective sensitivity to experimental manipulations and
produce systematic behavioral effects on model outputs, including significant
interaction effects across semantic dimensions. This work provides systematic
evidence for (1) dual encoding in neural representations, (2) meaningful
nonlinearly encoded feature interactions, and (3) introduces an architectural
paradigm shift from post-hoc feature analysis to integrated computational
design, establishing foundations for next-generation SAEs.

</details>


### [181] [BadViM: Backdoor Attack against Vision Mamba](https://arxiv.org/abs/2507.00577)
*Yinghao Wu,Liyan Zhang*

Main category: cs.CR

Relevance: 75.0

TL;DR: 论文研究了Vision Mamba（ViM）对后门攻击的脆弱性，提出了专门针对ViM的后门攻击框架BadViM，并展示了其高攻击成功率和对抗防御措施的鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 探索Vision Mamba架构的安全隐患，尤其是其对后门攻击的脆弱性，填补了该领域的研究空白。

Method: 提出了BadViM框架，利用共振频率触发器（RFT）和隐藏状态对齐损失（Hidden State Alignment loss）来实施后门攻击。

Result: BadViM在攻击成功率和对抗防御措施（如PatchDrop、PatchShuffle和JPEG压缩）方面表现出色。

Conclusion: Vision Mamba对后门攻击具有显著脆弱性，BadViM为研究此类攻击提供了有效工具。

Abstract: Vision State Space Models (SSMs), particularly architectures like Vision
Mamba (ViM), have emerged as promising alternatives to Vision Transformers
(ViTs). However, the security implications of this novel architecture,
especially their vulnerability to backdoor attacks, remain critically
underexplored. Backdoor attacks aim to embed hidden triggers into victim
models, causing the model to misclassify inputs containing these triggers while
maintaining normal behavior on clean inputs. This paper investigates the
susceptibility of ViM to backdoor attacks by introducing BadViM, a novel
backdoor attack framework specifically designed for Vision Mamba. The proposed
BadViM leverages a Resonant Frequency Trigger (RFT) that exploits the frequency
sensitivity patterns of the victim model to create stealthy, distributed
triggers. To maximize attack efficacy, we propose a Hidden State Alignment loss
that strategically manipulates the internal representations of model by
aligning the hidden states of backdoor images with those of target classes.
Extensive experimental results demonstrate that BadViM achieves superior attack
success rates while maintaining clean data accuracy. Meanwhile, BadViM exhibits
remarkable resilience against common defensive measures, including PatchDrop,
PatchShuffle and JPEG compression, which typically neutralize normal backdoor
attacks.

</details>


### [182] [Large Language Model Powered Intelligent Urban Agents: Concepts, Capabilities, and Applications](https://arxiv.org/abs/2507.00914)
*Jindong Han,Yansong Ning,Zirui Yuan,Hang Ni,Fan Liu,Tengfei Lyu,Hao Liu*

Main category: cs.MA

Relevance: 75.0

TL;DR: 该论文探讨了如何利用大型语言模型（LLMs）作为智能代理，解决城市决策问题，并提出了Urban LLM Agents的概念、工作流程、应用领域及未来研究方向。


<details>
  <summary>Details</summary>
Motivation: 通过LLMs的语义理解和推理能力，实现智能城市的愿景，提升城市决策效率。

Method: 介绍了Urban LLM Agents的概念，并从感知、记忆、推理、执行和学习等方面综述了现有研究。

Result: 将Urban LLM Agents的应用分为城市规划、交通、环境、公共安全和城市社会五类，并讨论了可信度和评估问题。

Conclusion: 为Urban LLM Agents这一新兴领域奠定了基础，并提出了未来研究方向。

Abstract: The long-standing vision of intelligent cities is to create efficient,
livable, and sustainable urban environments using big data and artificial
intelligence technologies. Recently, the advent of Large Language Models (LLMs)
has opened new ways toward realizing this vision. With powerful semantic
understanding and reasoning capabilities, LLMs can be deployed as intelligent
agents capable of autonomously solving complex problems across domains. In this
article, we focus on Urban LLM Agents, which are LLM-powered agents that are
semi-embodied within the hybrid cyber-physical-social space of cities and used
for system-level urban decision-making. First, we introduce the concept of
urban LLM agents, discussing their unique capabilities and features. Second, we
survey the current research landscape from the perspective of agent workflows,
encompassing urban sensing, memory management, reasoning, execution, and
learning. Third, we categorize the application domains of urban LLM agents into
five groups: urban planning, transportation, environment, public safety, and
urban society, presenting representative works in each group. Finally, we
discuss trustworthiness and evaluation issues that are critical for real-world
deployment, and identify several open problems for future research. This survey
aims to establish a foundation for the emerging field of urban LLM agents and
to provide a roadmap for advancing the intersection of LLMs and urban
intelligence. A curated list of relevant papers and open-source resources is
maintained and continuously updated at
https://github.com/usail-hkust/Awesome-Urban-LLM-Agents.

</details>


### [183] [MambAttention: Mamba with Multi-Head Attention for Generalizable Single-Channel Speech Enhancement](https://arxiv.org/abs/2507.00966)
*Nikolai Lund Kühne,Jesper Jensen,Jan Østergaard,Zheng-Hua Tan*

Main category: cs.SD

Relevance: 75.0

TL;DR: 提出了一种结合Mamba和共享时频多头注意力模块的新架构MambAttention，用于单通道语音增强，并在挑战性数据集上显著优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 解决序列模型（如Mamba和LSTM）在语音增强任务中容易过拟合的问题，探索混合Mamba与时频注意力模型的潜力。

Method: 提出MambAttention架构，结合Mamba和共享时频多头注意力模块，并使用VB-DemandEx数据集进行训练。

Result: MambAttention在域外数据集（DNS 2020和EARS-WHAM_v2）上显著优于现有方法，同时在域内数据集上表现相当。

Conclusion: MambAttention在语音增强任务中表现出卓越的泛化性能，共享时频注意力模块是关键因素。

Abstract: With the advent of new sequence models like Mamba and xLSTM, several studies
have shown that these models match or outperform state-of-the-art models in
single-channel speech enhancement, automatic speech recognition, and
self-supervised audio representation learning. However, prior research has
demonstrated that sequence models like LSTM and Mamba tend to overfit to the
training set. To address this issue, previous works have shown that adding
self-attention to LSTMs substantially improves generalization performance for
single-channel speech enhancement. Nevertheless, neither the concept of hybrid
Mamba and time-frequency attention models nor their generalization performance
have been explored for speech enhancement. In this paper, we propose a novel
hybrid architecture, MambAttention, which combines Mamba and shared time- and
frequency-multi-head attention modules for generalizable single-channel speech
enhancement. To train our model, we introduce VoiceBank+Demand Extended
(VB-DemandEx), a dataset inspired by VoiceBank+Demand but with more challenging
noise types and lower signal-to-noise ratios. Trained on VB-DemandEx, our
proposed MambAttention model significantly outperforms existing
state-of-the-art LSTM-, xLSTM-, Mamba-, and Conformer-based systems of similar
complexity across all reported metrics on two out-of-domain datasets: DNS 2020
and EARS-WHAM_v2, while matching their performance on the in-domain dataset
VB-DemandEx. Ablation studies highlight the role of weight sharing between the
time- and frequency-multi-head attention modules for generalization
performance. Finally, we explore integrating the shared time- and
frequency-multi-head attention modules with LSTM and xLSTM, which yields a
notable performance improvement on the out-of-domain datasets. However, our
MambAttention model remains superior on both out-of-domain datasets across all
reported evaluation metrics.

</details>


### [184] [TalentMine: LLM-Based Extraction and Question-Answering from Multimodal Talent Tables](https://arxiv.org/abs/2507.00041)
*Varun Mannam,Fang Wang,Chaochun Liu,Xin Chen*

Main category: cs.AI

Relevance: 70.0

TL;DR: 论文提出TalentMine框架，通过LLM增强的语义丰富表示解决表格数据提取中的语义信息丢失问题，显著提升查询任务准确率。


<details>
  <summary>Details</summary>
Motivation: 传统表格提取方法在语义理解上表现不佳，导致检索增强型聊天应用中查询失败。论文旨在解决表格数据中语义关系丢失的问题。

Method: 引入TalentMine框架，结合多模态推理生成语义丰富的表格表示，替代传统的CSV或文本线性化方法。

Result: 实验显示TalentMine在查询任务中达到100%准确率，显著优于AWS Textract（0%）和AWS Textract Visual Q&A（40%）。

Conclusion: TalentMine通过语义增强的表格表示有效解决了当前表格提取中的语义信息丢失问题，适用于人才管理应用。

Abstract: In talent management systems, critical information often resides in complex
tabular formats, presenting significant retrieval challenges for conventional
language models. These challenges are pronounced when processing Talent
documentation that requires precise interpretation of tabular relationships for
accurate information retrieval and downstream decision-making. Current table
extraction methods struggle with semantic understanding, resulting in poor
performance when integrated into retrieval-augmented chat applications. This
paper identifies a key bottleneck - while structural table information can be
extracted, the semantic relationships between tabular elements are lost,
causing downstream query failures. To address this, we introduce TalentMine, a
novel LLM-enhanced framework that transforms extracted tables into semantically
enriched representations. Unlike conventional approaches relying on CSV or text
linearization, our method employs specialized multimodal reasoning to preserve
both structural and semantic dimensions of tabular data. Experimental
evaluation across employee benefits document collections demonstrates
TalentMine's superior performance, achieving 100% accuracy in query answering
tasks compared to 0% for standard AWS Textract extraction and 40% for AWS
Textract Visual Q&A capabilities. Our comparative analysis also reveals that
the Claude v3 Haiku model achieves optimal performance for talent management
applications. The key contributions of this work include (1) a systematic
analysis of semantic information loss in current table extraction pipelines,
(2) a novel LLM-based method for semantically enriched table representation,
(3) an efficient integration framework for retrieval-augmented systems as
end-to-end systems, and (4) comprehensive benchmarks on talent analytics tasks
showing substantial improvements across multiple categories.

</details>


### [185] [Control-Optimized Deep Reinforcement Learning for Artificially Intelligent Autonomous Systems](https://arxiv.org/abs/2507.00268)
*Oren Fivel,Matan Rudman,Kobi Cohen*

Main category: cs.RO

Relevance: 70.0

TL;DR: 提出了一种新型控制优化的深度强化学习框架，显式建模并补偿动作执行不匹配问题，提升实际应用中的鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 传统深度强化学习方法假设动作执行完美，忽略实际系统中的不确定性，导致性能下降。

Method: 采用两阶段流程：确定期望动作并选择控制信号，训练时考虑动作不匹配和控制器修正。

Result: 在五个开源机械仿真环境中验证，框架表现出对不确定性的鲁棒性。

Conclusion: 该方法为工程实践提供了实用高效的解决方案，填补了理想学习与实际实现的差距。

Abstract: Deep reinforcement learning (DRL) has become a powerful tool for complex
decision-making in machine learning and AI. However, traditional methods often
assume perfect action execution, overlooking the uncertainties and deviations
between an agent's selected actions and the actual system response. In
real-world applications, such as robotics, mechatronics, and communication
networks, execution mismatches arising from system dynamics, hardware
constraints, and latency can significantly degrade performance. This work
advances AI by developing a novel control-optimized DRL framework that
explicitly models and compensates for action execution mismatches, a challenge
largely overlooked in existing methods. Our approach establishes a structured
two-stage process: determining the desired action and selecting the appropriate
control signal to ensure proper execution. It trains the agent while accounting
for action mismatches and controller corrections. By incorporating these
factors into the training process, the AI agent optimizes the desired action
with respect to both the actual control signal and the intended outcome,
explicitly considering execution errors. This approach enhances robustness,
ensuring that decision-making remains effective under real-world uncertainties.
Our approach offers a substantial advancement for engineering practice by
bridging the gap between idealized learning and real-world implementation. It
equips intelligent agents operating in engineering environments with the
ability to anticipate and adjust for actuation errors and system disturbances
during training. We evaluate the framework in five widely used open-source
mechanical simulation environments we restructured and developed to reflect
real-world operating conditions, showcasing its robustness against
uncertainties and offering a highly practical and efficient solution for
control-oriented applications.

</details>


### [186] [Twill: Scheduling Compound AI Systems on Heterogeneous Mobile Edge Platforms](https://arxiv.org/abs/2507.00491)
*Zain Taufique,Aman Vyas,Antonio Miele,Pasi Liljeberg,Anil Kanduri*

Main category: cs.MA

Relevance: 70.0

TL;DR: Twill框架在移动边缘平台上调度复合AI（cAI）系统的并发推理任务，通过任务亲和性集群映射、优先级感知任务冻结/解冻和DVFS，显著降低延迟并满足功耗预算。


<details>
  <summary>Details</summary>
Motivation: cAI系统由多种AI模型（如DNNs、Transformers、LLMs）组成，其动态工作负载和异构性使得在移动边缘平台上的调度成为挑战。现有方法无法有效处理此类并发任务。

Method: 提出Twill框架，结合任务亲和性集群映射、优先级感知任务冻结/解冻和动态电压频率调整（DVFS），优化cAI系统的推理调度。

Result: 在Nvidia Jetson Orin NX平台上实现Twill，相比现有技术，平均降低54%的推理延迟，同时满足功耗预算。

Conclusion: Twill为移动边缘平台上的cAI系统提供了一种高效的调度解决方案，显著提升了性能和能效。

Abstract: Compound AI (cAI) systems chain multiple AI models to solve complex problems.
cAI systems are typically composed of deep neural networks (DNNs),
transformers, and large language models (LLMs), exhibiting a high degree of
computational diversity and dynamic workload variation. Deploying cAI services
on mobile edge platforms poses a significant challenge in scheduling concurrent
DNN-transformer inference tasks, which arrive dynamically in an unknown
sequence. Existing mobile edge AI inference strategies manage multi-DNN or
transformer-only workloads, relying on design-time profiling, and cannot handle
concurrent inference of DNNs and transformers required by cAI systems. In this
work, we address the challenge of scheduling cAI systems on heterogeneous
mobile edge platforms. We present Twill, a run-time framework to handle
concurrent inference requests of cAI workloads through task affinity-aware
cluster mapping and migration, priority-aware task freezing/unfreezing, and
DVFS, while minimizing inference latency within power budgets. We implement and
deploy our Twill framework on the Nvidia Jetson Orin NX platform. We evaluate
Twill against state-of-the-art edge AI inference techniques over contemporary
DNNs and LLMs, reducing inference latency by 54% on average, while honoring
power budgets.

</details>


### [187] [Serving LLMs in HPC Clusters: A Comparative Study of Qualcomm Cloud AI 100 Ultra and High-Performance GPUs](https://arxiv.org/abs/2507.00418)
*Mohammad Firas Sada,John J. Graham,Elham E Khoda,Mahidhar Tatineni,Dmitry Mishin,Rajesh K. Gupta,Rick Wagner,Larry Smarr,Thomas A. DeFanti,Frank Würthwein*

Main category: cs.DC

Relevance: 70.0

TL;DR: 对Qualcomm Cloud AI 100 Ultra（QAic）加速器在LLM推理中的能效和性能进行了基准测试，并与NVIDIA和AMD的GPU进行了比较。


<details>
  <summary>Details</summary>
Motivation: 评估QAic加速器在LLM推理中的能效和性能，为高性能计算（HPC）应用提供参考。

Method: 使用vLLM框架对15个开源LLM（参数规模从1.17亿到900亿）进行推理测试，比较QAic与NVIDIA（A100、H200）和AMD（MI300A）GPU的能效（吞吐量/瓦特）和性能。

Result: QAic在大多数情况下表现出较高的能效，性能表现良好。

Conclusion: QAic在高性能计算（HPC）应用中具有潜力，尤其是在能效方面。

Abstract: This study presents a benchmarking analysis of the Qualcomm Cloud AI 100
Ultra (QAic) accelerator for large language model (LLM) inference, evaluating
its energy efficiency (throughput per watt) and performance against leading
NVIDIA (A100, H200) and AMD (MI300A) GPUs within the National Research Platform
(NRP) ecosystem. A total of 15 open-source LLMs, ranging from 117 million to 90
billion parameters, are served using the vLLM framework. The QAic inference
cards appears to be energy efficient and performs well in the energy efficiency
metric in most cases. The findings offer insights into the potential of the
Qualcomm Cloud AI 100 Ultra for high-performance computing (HPC) applications
within the National Research Platform (NRP).

</details>


### [188] [Horus: A Protocol for Trustless Delegation Under Uncertainty](https://arxiv.org/abs/2507.00631)
*David Shi,Kevin Joo*

Main category: cs.GT

Relevance: 70.0

TL;DR: 提出一种通过抵押索赔和递归验证游戏确保AI代理正确性的协议。


<details>
  <summary>Details</summary>
Motivation: 在动态、低信任环境中，无法通过前期规范或集中监督确保AI代理的正确性。

Method: 任务以意图形式发布，解决者竞争完成。验证者事后检查正确性，挑战者可通过抵押触发验证过程。

Result: 当激励一致时，正确性成为纳什均衡。

Conclusion: 协议通过激励机制确保AI代理的正确性。

Abstract: Correctness is an emergent property of systems where exposing error is
cheaper than committing it. In dynamic, low-trust environments, autonomous AI
agents benefit from delegating work to sub-agents, yet correctness cannot be
assured through upfront specification or centralized oversight. We propose a
protocol that enforces correctness through collateralized claims in a recursive
verification game. Tasks are published as intents, and solvers compete to
fulfill them. Selected solvers carry out tasks under risk, with correctness
checked post hoc by verifiers. Any challenger can challenge a result by staking
against it to trigger the verification process. Incorrect agents are slashed
and correct opposition is rewarded, with an escalation path that penalizes
erroneous verifiers themselves. When incentives are aligned across solvers,
challengers, and verifiers, falsification conditions make correctness the Nash
equilibrium.

</details>


### [189] [Generative Exaggeration in LLM Social Agents: Consistency, Bias, and Toxicity](https://arxiv.org/abs/2507.00657)
*Jacopo Nudo,Mario Edoardo Pandolfo,Edoardo Loru,Mattia Samory,Matteo Cinelli,Walter Quattrociocchi*

Main category: cs.HC

Relevance: 70.0

TL;DR: 研究探讨了LLM在模拟社交媒体政治话语时的行为，发现其输出更多反映内部优化动态而非用户行为，导致结构偏差。


<details>
  <summary>Details</summary>
Motivation: 探究LLM在模拟政治话语时的表现及其对社会代理可靠性的影响。

Method: 基于2100万次X平台互动数据，构建了1186个真实用户的LLM代理，在零样本和少样本条件下评估其回复。

Result: LLM输出放大了极化、风格化信号和有害语言，表现出“生成夸张”现象。

Conclusion: LLM不适合作为社会代理用于内容审核、政策建模等场景。

Abstract: We investigate how Large Language Models (LLMs) behave when simulating
political discourse on social media. Leveraging 21 million interactions on X
during the 2024 U.S. presidential election, we construct LLM agents based on
1,186 real users, prompting them to reply to politically salient tweets under
controlled conditions. Agents are initialized either with minimal ideological
cues (Zero Shot) or recent tweet history (Few Shot), allowing one-to-one
comparisons with human replies. We evaluate three model families (Gemini,
Mistral, and DeepSeek) across linguistic style, ideological consistency, and
toxicity. We find that richer contextualization improves internal consistency
but also amplifies polarization, stylized signals, and harmful language. We
observe an emergent distortion that we call "generation exaggeration": a
systematic amplification of salient traits beyond empirical baselines. Our
analysis shows that LLMs do not emulate users, they reconstruct them. Their
outputs, indeed, reflect internal optimization dynamics more than observed
behavior, introducing structural biases that compromise their reliability as
social proxies. This challenges their use in content moderation, deliberative
simulations, and policy modeling.

</details>


### [190] [WebArXiv: Evaluating Multimodal Agents on Time-Invariant arXiv Tasks](https://arxiv.org/abs/2507.00938)
*Zihao Sun,Meng Fang,Ling Chen*

Main category: cs.IR

Relevance: 70.0

TL;DR: 论文介绍了WebArXiv，一个静态且时间不变的基准测试，用于评估自主网页代理的性能，并提出了一种动态反射机制以解决代理的常见失败模式。


<details>
  <summary>Details</summary>
Motivation: 评估自主网页代理的性能存在挑战，现有基准测试不稳定或不一致。

Method: 引入WebArXiv基准测试，包含275个基于arXiv平台的任务，并提出动态反射机制。

Result: 在WebArXiv上评估了10种先进网页代理，展示了性能差异并验证了反射机制的有效性。

Conclusion: WebArXiv提供了可靠且可复现的评估方法，动态反射机制改善了代理性能。

Abstract: Recent progress in large language models (LLMs) has enabled the development
of autonomous web agents capable of navigating and interacting with real
websites. However, evaluating such agents remains challenging due to the
instability and inconsistency of existing benchmarks, which often rely on
dynamic content or oversimplified simulations. In this work, we introduce
WebArXiv, a static and time-invariant benchmark comprising 275 web-based tasks
grounded in the arXiv platform. WebArXiv ensures reproducible and reliable
evaluation by anchoring tasks in fixed web snapshots with deterministic ground
truths and standardized action trajectories. Through behavioral analysis, we
identify a common failure mode, Rigid History Reflection, where agents
over-rely on fixed interaction histories. To address this, we propose a
lightweight dynamic reflection mechanism that allows agents to selectively
retrieve relevant past steps during decision-making. We evaluate ten
state-of-the-art web agents on WebArXiv. Results demonstrate clear performance
differences across agents and validate the effectiveness of our proposed
reflection strategy.

</details>


### [191] [From Sentences to Sequences: Rethinking Languages in Biological System](https://arxiv.org/abs/2507.00953)
*Ke Liu,Shuanke Shen,Hao Chen*

Main category: q-bio.BM

Relevance: 70.0

TL;DR: 论文探讨了将大语言模型（LLM）从自然语言处理（NLP）迁移到生物序列建模的可行性，强调了生物语言与自然语言在结构上的差异，并提出了基于3D结构的评估方法。


<details>
  <summary>Details</summary>
Motivation: 研究动机在于探索如何将NLP中的自回归生成和评估方法有效应用于生物序列建模，同时考虑生物语言与自然语言在结构上的本质差异。

Method: 通过将生物分子的3D结构视为句子的语义内容，并考虑残基或碱基之间的强相关性，提出了一种结构评估方法，并验证了自回归范式在生物语言建模中的适用性。

Result: 研究结果表明，自回归范式在生物语言建模中是可行的，但需要结合生物序列的结构特性进行专门评估。

Conclusion: 论文结论指出，将NLP的成功经验迁移到生物领域需要重新审视生物语言的定义，并结合结构评估方法。

Abstract: The paradigm of large language models in natural language processing (NLP)
has also shown promise in modeling biological languages, including proteins,
RNA, and DNA. Both the auto-regressive generation paradigm and evaluation
metrics have been transferred from NLP to biological sequence modeling.
However, the intrinsic structural correlations in natural and biological
languages differ fundamentally. Therefore, we revisit the notion of language in
biological systems to better understand how NLP successes can be effectively
translated to biological domains. By treating the 3D structure of biomolecules
as the semantic content of a sentence and accounting for the strong
correlations between residues or bases, we highlight the importance of
structural evaluation and demonstrate the applicability of the auto-regressive
paradigm in biological language modeling. Code can be found at
\href{https://github.com/zjuKeLiu/RiFold}{github.com/zjuKeLiu/RiFold}

</details>


### [192] [ChatGPT produces more "lazy" thinkers: Evidence of cognitive engagement decline](https://arxiv.org/abs/2507.00181)
*Georgios P. Georgiou*

Main category: cs.AI

Relevance: 60.0

TL;DR: 研究探讨ChatGPT对学生学术写作任务中认知参与的影响，发现AI辅助导致认知参与度显著降低。


<details>
  <summary>Details</summary>
Motivation: 随着LLMs在教育中的应用增加，担忧其对深度思考和主动学习的负面影响。

Method: 实验设计，随机分配学生至ChatGPT辅助组或对照组，完成写作任务并使用CES-AI量表评估认知参与。

Result: ChatGPT组的认知参与度显著低于对照组，表明AI可能导致认知卸载。

Conclusion: 需制定教学策略，促进学生对AI生成内容的主动反思，避免损害自我调节学习和深度认知参与。

Abstract: Despite the increasing use of large language models (LLMs) in education,
concerns have emerged about their potential to reduce deep thinking and active
learning. This study investigates the impact of generative artificial
intelligence (AI) tools, specifically ChatGPT, on the cognitive engagement of
students during academic writing tasks. The study employed an experimental
design with participants randomly assigned to either an AI-assisted (ChatGPT)
or a non-assisted (control) condition. Participants completed a structured
argumentative writing task followed by a cognitive engagement scale (CES), the
CES-AI, developed to assess mental effort, attention, deep processing, and
strategic thinking. The results revealed significantly lower cognitive
engagement scores in the ChatGPT group compared to the control group. These
findings suggest that AI assistance may lead to cognitive offloading. The study
contributes to the growing body of literature on the psychological implications
of AI in education and raises important questions about the integration of such
tools into academic practice. It calls for pedagogical strategies that promote
active, reflective engagement with AI-generated content to avoid compromising
self-regulated learning and deep cognitive involvement of students.

</details>


### [193] [Teaching Programming in the Age of Generative AI: Insights from Literature, Pedagogical Proposals, and Student Perspectives](https://arxiv.org/abs/2507.00108)
*Clemente Rubio-Manzano,Jazna Meza,Rodolfo Fernandez-Santibanez,Christian Vidal-Castro*

Main category: cs.CY

Relevance: 60.0

TL;DR: 论文探讨了基于大语言模型的自动代码生成工具对编程教育的变革，提出通过代码理解和执行的可视化方法改进教学。


<details>
  <summary>Details</summary>
Motivation: 研究动机是应对大语言模型在编程教育中的广泛应用，探讨如何调整教学内容和方法以适应这一变革。

Method: 方法包括文献综述和教学实践，强调代码理解和执行的可视化工具（如Java的可视化模拟）。

Result: 结果显示可视化方法能提升学生对编程的深层理解，学生反馈支持其有效性。

Conclusion: 结论认为可视化工具是编程教育的有益补充，尤其在代码理解和执行方面。

Abstract: Computer programming is undergoing a true transformation driven by powerful
new tools for automatic source code generation based on large language models.
This transformation is also manifesting in introductory programming courses at
universities around the world, generating an in-depth debate about how
programming content should be taught, learned, and assessed in the context of
generative artificial intelligence.
  This article aims, on the one hand, to review the most relevant studies on
this issue, highlighting the advantages and disadvantages identified in the
specialized literature. On the other hand, it proposes enriching teaching and
learning methodologies by focusing on code comprehension and execution rather
than on mere coding or program functionality. In particular, it advocates for
the use of visual representations of code and visual simulations of its
execution as effective tools for teaching, learning, and assessing programming,
thus fostering a deeper understanding among students.
  Finally, the opinions of students who took the object-oriented programming
course are presented to provide preliminary context supporting the
incorporation of visual simulations in Java (or other languages) as part of the
training process.

</details>


### [194] [iPanda: An Intelligent Protocol Testing and Debugging Agent for Conformance Testing](https://arxiv.org/abs/2507.00378)
*Xikai Sun,Fan Dang,Kebin Liu,Xin Miao,Zihao Yang,Haimo Lu,Yawen Zheng,Yunhao Liu*

Main category: cs.SE

Relevance: 60.0

TL;DR: iPanda是一个利用LLMs自动化协议一致性测试的端到端框架，显著提高了测试代码生成的效率和质量。


<details>
  <summary>Details</summary>
Motivation: 传统协议一致性测试方法依赖人工创建测试用例和脚本，效率低下且劳动密集，而LLMs的文本理解和代码生成能力为自动化提供了新机会。

Method: iPanda结合关键词生成测试用例、基于代码的检索增强生成方法以及迭代自校正机制，自动化生成并优化测试脚本。

Result: 实验表明，iPanda在测试代码生成成功率（Pass@1）上比纯LLM方法提升了4.675至10.751倍。

Conclusion: iPanda通过LLMs显著提升了协议一致性测试的自动化水平和效率。

Abstract: Conformance testing is essential for ensuring that protocol implementations
comply with their specifications. However, traditional testing approaches
involve manually creating numerous test cases and scripts, making the process
labor-intensive and inefficient. Recently, Large Language Models (LLMs) have
demonstrated impressive text comprehension and code generation abilities,
providing promising opportunities for automation. In this paper, we propose
iPanda, the first end-to-end framework that leverages LLMs to automate protocol
conformance testing. Given a protocol specification document and its
implementation, iPanda first employs a keyword-based method to automatically
generate comprehensive test cases. Then, it utilizes a code-based
retrieval-augmented generation approach to effectively interpret the
implementation and produce executable test code. To further enhance code
quality, iPanda incorporates an iterative self-correction mechanism to refine
generated test scripts interactively. Finally, by executing and analyzing the
generated tests, iPanda systematically verifies compliance between
implementations and protocol specifications. Comprehensive experiments on
various protocols show that iPanda significantly outperforms pure LLM-based
approaches, improving the success rate (Pass@1) of test-code generation by
factors ranging from 4.675 times to 10.751 times.

</details>


### [195] [DiMo-GUI: Advancing Test-time Scaling in GUI Grounding via Modality-Aware Visual Reasoning](https://arxiv.org/abs/2507.00008)
*Hang Wu,Hongkai Chen,Yujun Cai,Chang Liu,Qingwen Ye,Ming-Hsuan Yang,Yiwei Wang*

Main category: cs.AI

Relevance: 40.0

TL;DR: DiMo-GUI是一个无需训练的GUI自然语言查询框架，通过动态视觉定位和模态感知优化解决视觉元素多样性和语言歧义问题。


<details>
  <summary>Details</summary>
Motivation: 解决GUI中自然语言查询的挑战，如视觉元素多样性和语言歧义。

Method: 将GUI拆分为文本和图标元素，利用通用视觉语言模型独立处理，并通过动态聚焦区域逐步细化定位结果。

Result: 在标准GUI基准测试中表现优于基线方法，验证了模态分离与区域聚焦推理的有效性。

Conclusion: DiMo-GUI通过无需训练的方法有效解决了GUI中的自然语言查询问题。

Abstract: Grounding natural language queries in graphical user interfaces (GUIs) poses
unique challenges due to the diversity of visual elements, spatial clutter, and
the ambiguity of language. In this paper, we introduce DiMo-GUI, a
training-free framework for GUI grounding that leverages two core strategies:
dynamic visual grounding and modality-aware optimization. Instead of treating
the GUI as a monolithic image, our method splits the input into textual
elements and iconic elements, allowing the model to reason over each modality
independently using general-purpose vision-language models. When predictions
are ambiguous or incorrect, DiMo-GUI dynamically focuses attention by
generating candidate focal regions centered on the model's initial predictions
and incrementally zooms into subregions to refine the grounding result. This
hierarchical refinement process helps disambiguate visually crowded layouts
without the need for additional training or annotations. We evaluate our
approach on standard GUI grounding benchmarks and demonstrate consistent
improvements over baseline inference pipelines, highlighting the effectiveness
of combining modality separation with region-focused reasoning.

</details>


### [196] [BlackBoxToBlueprint: Extracting Interpretable Logic from Legacy Systems using Reinforcement Learning and Counterfactual Analysis](https://arxiv.org/abs/2507.00180)
*Vidhi Rathore*

Main category: cs.AI

Relevance: 40.0

TL;DR: 提出了一种基于强化学习的自动化方法，用于从遗留系统中提取可解释的决策逻辑。


<details>
  <summary>Details</summary>
Motivation: 传统方法（如行为克隆）仅复制输入输出行为，无法捕捉底层意图，而遗留系统通常缺乏文档，导致现代化改造困难。

Method: 使用强化学习代理探索输入空间，识别关键决策边界，并通过K-Means聚类和决策树提取可读规则。

Result: 在三种不同复杂度的遗留系统上验证，提取的规则准确反映了底层逻辑。

Conclusion: 该方法为遗留系统迁移提供了生成规范和测试用例的基础。

Abstract: Modernizing legacy software systems is a critical but challenging task, often
hampered by a lack of documentation and understanding of the original system's
intricate decision logic. Traditional approaches like behavioral cloning merely
replicate input-output behavior without capturing the underlying intent. This
paper proposes a novel pipeline to automatically extract interpretable decision
logic from legacy systems treated as black boxes. The approach uses a
Reinforcement Learning (RL) agent to explore the input space and identify
critical decision boundaries by rewarding actions that cause meaningful changes
in the system's output. These counterfactual state transitions, where the
output changes, are collected and clustered using K-Means. Decision trees are
then trained on these clusters to extract human-readable rules that approximate
the system's decision logic near the identified boundaries. I demonstrated the
pipeline's effectiveness on three dummy legacy systems with varying complexity,
including threshold-based, combined-conditional, and non-linear range logic.
Results show that the RL agent successfully focuses exploration on relevant
boundary regions, and the extracted rules accurately reflect the core logic of
the underlying dummy systems, providing a promising foundation for generating
specifications and test cases during legacy migration.

</details>


### [197] [Holistic Artificial Intelligence in Medicine; improved performance and explainability](https://arxiv.org/abs/2507.00205)
*Periklis Petridis,Georgios Margaritis,Vasiliki Stoumpou,Dimitris Bertsimas*

Main category: cs.AI

Relevance: 40.0

TL;DR: xHAIM（可解释的HAIM）是一个利用生成式AI增强预测和可解释性的框架，通过结构化步骤提升临床任务的表现，并将AI从黑盒转变为可解释的决策支持系统。


<details>
  <summary>Details</summary>
Motivation: 解决HAIM框架在任务无关数据使用和缺乏可解释性方面的局限性。

Method: 通过四个步骤实现：识别任务相关数据、生成患者摘要、改进预测建模、提供临床解释。

Result: 在HAIM-MIMIC-MM数据集上，xHAIM将平均AUC从79.9%提升至90.3%。

Conclusion: xHAIM成功将AI转化为可解释的临床决策支持工具，增强了临床实用性。

Abstract: With the increasing interest in deploying Artificial Intelligence in
medicine, we previously introduced HAIM (Holistic AI in Medicine), a framework
that fuses multimodal data to solve downstream clinical tasks. However, HAIM
uses data in a task-agnostic manner and lacks explainability. To address these
limitations, we introduce xHAIM (Explainable HAIM), a novel framework
leveraging Generative AI to enhance both prediction and explainability through
four structured steps: (1) automatically identifying task-relevant patient data
across modalities, (2) generating comprehensive patient summaries, (3) using
these summaries for improved predictive modeling, and (4) providing clinical
explanations by linking predictions to patient-specific medical knowledge.
Evaluated on the HAIM-MIMIC-MM dataset, xHAIM improves average AUC from 79.9%
to 90.3% across chest pathology and operative tasks. Importantly, xHAIM
transforms AI from a black-box predictor into an explainable decision support
system, enabling clinicians to interactively trace predictions back to relevant
patient data, bridging AI advancements with clinical utility.

</details>


### [198] [Integrating Universal Generative AI Platforms in Educational Labs to Foster Critical Thinking and Digital Literacy](https://arxiv.org/abs/2507.00007)
*Vasiliy Znamenskiy,Rafael Niyazov,Joel Hernandez*

Main category: cs.CY

Relevance: 40.0

TL;DR: 论文提出了一种教育框架，将生成式AI平台（如ChatGPT、Claude、Gemini）融入实验室活动，以培养本科生的批判性思维和数字素养。


<details>
  <summary>Details</summary>
Motivation: 认识到对大型语言模型（LLMs）不加批判的依赖存在局限性和风险，该教学模型将生成式AI重新定义为研究主题和认知工具。

Method: 学生制定学科特定的提示，并评估生成式AI在文本、图像和视频模态中的响应。在一门面向非理科生的普通天文学课程中进行了试点实施。

Result: 试点显示学生参与度高且进行了批判性反思，许多学生在课后继续活动并在研究研讨会上展示成果。

Conclusion: 研究强调了结构化AI互动在教育中的重要性，并提出生成式AI结合反思性评估方法可改善学习成果。

Abstract: This paper presents a new educational framework for integrating generative
artificial intelligence (GenAI) platforms such as ChatGPT, Claude, and Gemini
into laboratory activities aimed at developing critical thinking and digital
literacy among undergraduate students. Recognizing the limitations and risks of
uncritical reliance on large language models (LLMs), the proposed pedagogical
model reframes GenAI as a research subject and cognitive tool. Students
formulate discipline-specific prompts and evaluate GenAI-generated responses in
text, image, and video modalities. A pilot implementation in a general
astronomy course for non-science majors demonstrated high levels of engagement
and critical reflection, with many students continuing the activity after class
and presenting results at a research symposium. The results highlight the
importance of structured AI interactions in education and suggest that GenAI
can improve learning outcomes when combined with reflective assessment methods.
The study proposes a replicable model for interdisciplinary AI-integrated lab
work, adaptable to scientific disciplines. See the guide to learning activities
based on Generative-Ai platforms: https://doi.org/10.5281/zenodo.15555802

</details>


### [199] [Ken Utilization Layer: Hebbian Replay Within a Student's Ken for Adaptive Knowledge Tracing](https://arxiv.org/abs/2507.00032)
*Grey Kuling,Marinka Zitnik*

Main category: cs.CY

Relevance: 40.0

TL;DR: KUL-KT是一种受生物学启发的知识追踪架构，结合了Hebbian记忆编码和基于梯度的巩固，支持少样本个性化和自然遗忘，在多个基准测试中表现优异。


<details>
  <summary>Details</summary>
Motivation: 受神经系统中记忆巩固机制的启发，旨在设计一个可扩展、输入无关的框架，用于学生建模，支持个性化学习和自然遗忘。

Method: 结合时间衰减的Hebbian记忆更新和Loss-aligned Internal Target (LIT)方法，通过快速Hebbian记忆和慢速线性网络实现知识巩固。

Result: 在十个公共知识追踪基准测试中表现优于基线模型，训练速度更快且内存占用更少，实际部署中显著提升学习者体验。

Conclusion: KUL-KT是一个高效、灵活且生物启发的个性化学习框架，适用于大规模应用。

Abstract: We introduce KUL-KT, a biologically inspired architecture for knowledge
tracing (KT), combining Hebbian memory encoding with gradient-based
consolidation in a scalable, input-agnostic framework. KUL-KT adapts the
principle of memory consolidation in neural systems, to student modeling by
introducing two key innovations: (i) a time-decaying Hebbian memory update that
enables graceful forgetting, and (ii) a novel Loss-aligned Internal Target
(LIT) method to compute an ideal internal state, allowing continual learning
without backpropagation through time. The architecture consists of a fast
Hebbian memory that captures each learner interaction via a single associative
update, and a slower linear network that consolidates recalled samples through
gradient descent. This design enables few-shot personalization and natural
forgetting without storing raw data or relying on large cohort training.
Operating entirely in embedding space, KUL-KT supports both structured
(tabular) and unstructured (short-answer) inputs. Empirically, KUL-KT
outperforms strong baselines on ten public KT benchmarks in rank-sensitive
metrics such as nDCG and Recall@10. In a classroom deployment, KUL-KT
personalized quizzes from short-answer data, leading to improved
learner-perceived helpfulness and reduced difficulty (p < 0.05). Ablation
studies confirm that Hebbian decay and LIT are critical for continual
adaptation. Compared to a strong graph-based KT model, KUL-KT trains 1.75x
faster and uses 99.01\% less memory. These results position KUL-KT as a
biologically grounded, memory-efficient, and input-flexible framework for
personalized learning at scale.

</details>


### [200] [Designing an Adaptive Storytelling Platform to Promote Civic Education in Politically Polarized Learning Environments](https://arxiv.org/abs/2507.00161)
*Christopher M. Wegemer,Edward Halim,Jeff Burke*

Main category: cs.HC

Relevance: 40.0

TL;DR: 论文探讨了如何利用AI技术（如情感计算和GPT-4）设计自适应叙事平台，以减少政治极化并促进开放思维。


<details>
  <summary>Details</summary>
Motivation: 政治极化阻碍了民主公民教育，AI技术提供了新的干预机会。

Method: 采用设计研究方法（DBR），开发了AI-DCS平台，结合情感识别和注意力追踪，利用GPT-4实现语言自适应。

Result: 开发的原型平台能实时调整叙事内容，维持学生对不同政治观点的情感投入。

Conclusion: AI支持的情感敏感策略为减少极化提供了新途径，同时保护学习者自主性。

Abstract: Political polarization undermines democratic civic education by exacerbating
identity-based resistance to opposing viewpoints. Emerging AI technologies
offer new opportunities to advance interventions that reduce polarization and
promote political open-mindedness. We examined novel design strategies that
leverage adaptive and emotionally-responsive civic narratives that may sustain
students' emotional engagement in stories, and in turn, promote
perspective-taking toward members of political out-groups. Drawing on theories
from political psychology and narratology, we investigate how affective
computing techniques can support three storytelling mechanisms: transportation
into a story world, identification with characters, and interaction with the
storyteller. Using a design-based research (DBR) approach, we iteratively
developed and refined an AI-mediated Digital Civic Storytelling (AI-DCS)
platform. Our prototype integrates facial emotion recognition and attention
tracking to assess users' affective and attentional states in real time.
Narrative content is organized around pre-structured story outlines, with
beat-by-beat language adaptation implemented via GPT-4, personalizing
linguistic tone to sustain students' emotional engagement in stories that
center political perspectives different from their own. Our work offers a
foundation for AI-supported, emotionally-sensitive strategies that address
affective polarization while preserving learner autonomy. We conclude with
implications for civic education interventions, algorithmic literacy, and HCI
challenges associated with AI dialogue management and affect-adaptive learning
environments.

</details>


### [201] [Multimodal, Multi-Disease Medical Imaging Foundation Model (MerMED-FM)](https://arxiv.org/abs/2507.00185)
*Yang Zhou,Chrystie Wan Ning Quek,Jun Zhou,Yan Wang,Yang Bai,Yuhe Ke,Jie Yao,Laura Gutierrez,Zhen Ling Teo,Darren Shu Jeng Ting,Brian T. Soetikno,Christopher S. Nielsen,Tobias Elze,Zengxiang Li,Linh Le Dinh,Lionel Tim-Ee Cheng,Tran Nguyen Tuan Anh,Chee Leong Cheng,Tien Yin Wong,Nan Liu,Iain Beehuat Tan,Tony Kiat Hon Lim,Rick Siow Mong Goh,Yong Liu,Daniel Shu Wei Ting*

Main category: eess.IV

Relevance: 40.0

TL;DR: MerMED-FM是一个多模态、多专业的医学影像基础模型，通过自监督学习和内存模块训练，在多种疾病和模态上表现优异。


<details>
  <summary>Details</summary>
Motivation: 当前医学影像AI模型多为单模态、单疾病，且临床准确性不一致。MerMED-FM旨在解决这些问题，提供更通用的解决方案。

Method: 使用自监督学习和内存模块训练，基于330万张医学影像数据，涵盖10个专业和7种模态。

Result: 在多种疾病和模态上表现优异，AUROC值在0.858至0.988之间。

Conclusion: MerMED-FM是一个高度适应性强、多功能的跨专业基础模型，适用于多样化的医学影像解读。

Abstract: Current artificial intelligence models for medical imaging are predominantly
single modality and single disease. Attempts to create multimodal and
multi-disease models have resulted in inconsistent clinical accuracy.
Furthermore, training these models typically requires large, labour-intensive,
well-labelled datasets. We developed MerMED-FM, a state-of-the-art multimodal,
multi-specialty foundation model trained using self-supervised learning and a
memory module. MerMED-FM was trained on 3.3 million medical images from over
ten specialties and seven modalities, including computed tomography (CT), chest
X-rays (CXR), ultrasound (US), pathology patches, color fundus photography
(CFP), optical coherence tomography (OCT) and dermatology images. MerMED-FM was
evaluated across multiple diseases and compared against existing foundational
models. Strong performance was achieved across all modalities, with AUROCs of
0.988 (OCT); 0.982 (pathology); 0.951 (US); 0.943 (CT); 0.931 (skin); 0.894
(CFP); 0.858 (CXR). MerMED-FM has the potential to be a highly adaptable,
versatile, cross-specialty foundation model that enables robust medical imaging
interpretation across diverse medical disciplines.

</details>


### [202] [Investigating Stochastic Methods for Prosody Modeling in Speech Synthesis](https://arxiv.org/abs/2507.00227)
*Paul Mayer,Florian Lux,Alejandro Pérez-González-de-Martos,Angelina Elizarova,Lindsey Vanderlyn,Dirk Väth,Ngoc Thang Vu*

Main category: eess.AS

Relevance: 40.0

TL;DR: 该论文研究了在文本到语音合成中，使用随机方法（如归一化流、条件流匹配和整流流）生成表达性韵律的效果，并与传统确定性方法和人类发音进行了比较。结果表明，随机方法能生成与人类发音相媲美的自然韵律，并提供了额外的可控性。


<details>
  <summary>Details</summary>
Motivation: 生成表达性韵律在文本到语音合成中仍具挑战性，尤其是在需要显式建模韵律参数（如音高、能量和持续时间）以保持可解释性和可控性的系统中。

Method: 比较了随机方法（归一化流、条件流匹配和整流流）与传统确定性方法及人类发音的效果，通过主客观评估进行分析。

Result: 随机方法生成的韵律自然度与人类发音相当，并能捕捉人类语音的变异性，同时通过调节采样温度提供额外的可控性。

Conclusion: 随机方法在生成自然韵律和提供可控性方面优于传统方法，为文本到语音合成提供了新的可能性。

Abstract: While generative methods have progressed rapidly in recent years, generating
expressive prosody for an utterance remains a challenging task in
text-to-speech synthesis. This is particularly true for systems that model
prosody explicitly through parameters such as pitch, energy, and duration,
which is commonly done for the sake of interpretability and controllability. In
this work, we investigate the effectiveness of stochastic methods for this
task, including Normalizing Flows, Conditional Flow Matching, and Rectified
Flows. We compare these methods to a traditional deterministic baseline, as
well as to real human realizations. Our extensive subjective and objective
evaluations demonstrate that stochastic methods produce natural prosody on par
with human speakers by capturing the variability inherent in human speech.
Further, they open up additional controllability options by allowing the
sampling temperature to be tuned.

</details>


### [203] [A High-Fidelity Speech Super Resolution Network using a Complex Global Attention Module with Spectro-Temporal Loss](https://arxiv.org/abs/2507.00229)
*Tarikul Islam Tamiti,Biraj Joshi,Rida Hasan,Rashedul Hasan,Taieba Athay,Nursad Mamun,Anomadarshi Barua*

Main category: cs.SD

Relevance: 40.0

TL;DR: CTFT-Net是一种用于语音超分辨率的复杂时频变换网络，通过同时重建幅度和相位，结合全局注意力和复杂Conformer模块，显著提升了语音质量。


<details>
  <summary>Details</summary>
Motivation: 现有语音超分辨率方法主要关注幅度重建，而相位重建对感知质量同样重要。因此，提出CTFT-Net以同时优化两者。

Method: CTFT-Net结合复杂全局注意力块和复杂Conformer模块，建模音素间和频率间依赖关系，并使用时域和多分辨率频域损失函数。

Result: 在VCTK数据集上，CTFT-Net在极端上采样任务（2 kHz至48 kHz）中优于现有模型（如NU-Wave、WSRGlow等），有效重建高频且无噪声伪影。

Conclusion: CTFT-Net通过复杂域重建和高效架构设计，显著提升了语音超分辨率的性能。

Abstract: Speech super-resolution (SSR) enhances low-resolution speech by increasing
the sampling rate. While most SSR methods focus on magnitude reconstruction,
recent research highlights the importance of phase reconstruction for improved
perceptual quality. Therefore, we introduce CTFT-Net, a Complex Time-Frequency
Transformation Network that reconstructs both magnitude and phase in complex
domains for improved SSR tasks. It incorporates a complex global attention
block to model inter-phoneme and inter-frequency dependencies and a complex
conformer to capture long-range and local features, improving frequency
reconstruction and noise robustness. CTFT-Net employs time-domain and
multi-resolution frequency-domain loss functions for better generalization.
Experiments show CTFT-Net outperforms state-of-the-art models (NU-Wave,
WSRGlow, NVSR, AERO) on the VCTK dataset, particularly for extreme upsampling
(2 kHz to 48 kHz), reconstructing high frequencies effectively without noisy
artifacts.

</details>


### [204] [Visual Privacy Management with Generative AI for Blind and Low-Vision People](https://arxiv.org/abs/2507.00286)
*Tanusree Sharma,Yu-Yun Tseng,Lotus Zhang,Ayae Ide,Kelly Avery Mack,Leah Findlater,Danna Gurari,Yang Wang*

Main category: cs.HC

Relevance: 40.0

TL;DR: BLV个体使用GenAI工具处理视觉内容，研究通过访谈揭示其隐私实践与设计偏好，提出支持视觉隐私的设计建议。


<details>
  <summary>Details</summary>
Motivation: 探讨GenAI工具如何增强BLV个体的视觉内容可访问性，同时解决其带来的视觉隐私挑战。

Method: 对21名BLV参与者进行访谈研究，分析其GenAI使用实践和设计偏好。

Result: 发现GenAI使用中隐私、效率和情感代理的平衡实践，提出六种隐私场景和五项设计偏好。

Conclusion: 提出支持用户中心视觉隐私的设计建议，扩展隐私概念及数据责任处理。

Abstract: Blind and low vision (BLV) individuals use Generative AI (GenAI) tools to
interpret and manage visual content in their daily lives. While such tools can
enhance the accessibility of visual content and so enable greater user
independence, they also introduce complex challenges around visual privacy. In
this paper, we investigate the current practices and future design preferences
of blind and low vision individuals through an interview study with 21
participants. Our findings reveal a range of current practices with GenAI that
balance privacy, efficiency, and emotional agency, with users accounting for
privacy risks across six key scenarios, such as self-presentation,
indoor/outdoor spatial privacy, social sharing, and handling professional
content. Our findings reveal design preferences, including on-device
processing, zero-retention guarantees, sensitive content redaction,
privacy-aware appearance indicators, and multimodal tactile mirrored
interaction methods. We conclude with actionable design recommendations to
support user-centered visual privacy through GenAI, expanding the notion of
privacy and responsible handling of others data.

</details>


### [205] [VTS-Guided AI Interaction Workflow for Business Insights](https://arxiv.org/abs/2507.00347)
*Sun Ding,Ude Enebeli,Atilhan,Manay,Ryan Pua,Kamal Kotak*

Main category: cs.SE

Relevance: 40.0

TL;DR: VTS-AI是一个结合视觉思维策略的AI系统，用于从非结构化报告中提取业务洞察，速度快且结果丰富。


<details>
  <summary>Details</summary>
Motivation: 解决企业在处理非结构化报告时效率低、缺乏敏捷性的问题。

Method: 集成视觉思维策略，通过三层架构（微观、中观、宏观）提取、标记和链接信息，生成可搜索的YAML文件。

Result: 在18页商业报告测试中，VTS-AI速度与ChatGPT相当，但提供更丰富的结果（如页面位置、因果链接等）。

Conclusion: VTS-AI有望成为生产就绪的业务分析工具，未来将增强财务模型和数据安全性。

Abstract: Modern firms face a flood of dense, unstructured reports. Turning these
documents into usable insights takes heavy effort and is far from agile when
quick answers are needed. VTS-AI tackles this gap. It integrates Visual
Thinking Strategies, which emphasize evidence-based observation, linking, and
thinking, into AI agents, so the agents can extract business insights from
unstructured text, tables, and images at scale. The system works in three tiers
(micro, meso, macro). It tags issues, links them to source pages, and rolls
them into clear action levers stored in a searchable YAML file. In tests on an
18-page business report, VTS-AI matched the speed of a one-shot ChatGPT prompt
yet produced richer findings: page locations, verbatim excerpts, severity
scores, and causal links. Analysts can accept or adjust these outputs in the
same IDE, keeping human judgment in the loop. Early results show VTS-AI spots
the direction of key metrics and flags where deeper number-crunching is needed.
Next steps include mapping narrative tags to financial ratios, adding
finance-tuned language models through a Model-Context Protocol, and building a
Risk & Safety Layer to stress-test models and secure data. These upgrades aim
to make VTS-AI a production-ready, audit-friendly tool for rapid business
analysis.

</details>


### [206] [RoboEval: Where Robotic Manipulation Meets Structured and Scalable Evaluation](https://arxiv.org/abs/2507.00435)
*Yi Ru Wang,Carter Ung,Grant Tannert,Jiafei Duan,Josephine Li,Amy Le,Rishabh Oswal,Markus Grotz,Wilbert Pumacay,Yuquan Deng,Ranjay Krishna,Dieter Fox,Siddhartha Srinivasa*

Main category: cs.RO

Relevance: 40.0

TL;DR: RoboEval是一个用于评估双手机器人操作策略的仿真基准和结构化框架，揭示了现有策略的潜在弱点。


<details>
  <summary>Details</summary>
Motivation: 当前的双手机器人操作策略评估仅报告二元任务成功率，掩盖了策略行为中的关键缺陷，如协调不良或抓取滑动。RoboEval旨在通过细粒度指标和任务分解，提供更深入的评估。

Method: RoboEval设计了一套分层的语义任务，分解为技能特定阶段，并引入系统性的空间、物理和协调挑战。任务配有细粒度诊断指标和3000+人类演示以支持模仿学习。

Result: 实验表明，成功率相近的策略在执行方式上存在显著差异，行为指标在超过一半的任务-指标对中与成功率相关。

Conclusion: RoboEval通过揭示策略失败的具体原因，提供了更可操作的评估工具，强调了超越二元成功率的必要性。

Abstract: We present RoboEval, a simulation benchmark and structured evaluation
framework designed to reveal the limitations of current bimanual manipulation
policies. While prior benchmarks report only binary task success, we show that
such metrics often conceal critical weaknesses in policy behavior -- such as
poor coordination, slipping during grasping, or asymmetric arm usage. RoboEval
introduces a suite of tiered, semantically grounded tasks decomposed into
skill-specific stages, with variations that systematically challenge spatial,
physical, and coordination capabilities. Tasks are paired with fine-grained
diagnostic metrics and 3000+ human demonstrations to support imitation
learning. Our experiments reveal that policies with similar success rates
diverge in how tasks are executed -- some struggle with alignment, others with
temporally consistent bimanual control. We find that behavioral metrics
correlate with success in over half of task-metric pairs, and remain
informative even when binary success saturates. By pinpointing when and how
policies fail, RoboEval enables a deeper, more actionable understanding of
robotic manipulation -- and highlights the need for evaluation tools that go
beyond success alone.

</details>


### [207] [Rethinking Group Recommender Systems in the Era of Generative AI: From One-Shot Recommendations to Agentic Group Decision Support](https://arxiv.org/abs/2507.00535)
*Dietmar Jannach,Amra Delić,Francesco Ricci,Markus Zanker*

Main category: cs.IR

Relevance: 40.0

TL;DR: 论文探讨了群体推荐系统的研究现状，指出尽管学术研究丰富，但实际应用很少，并提出了利用现代生成式AI（如ChatGPT）重新设计系统的方向。


<details>
  <summary>Details</summary>
Motivation: 群体推荐系统研究已有25年历史，但实际应用寥寥无几，作者质疑学术研究的假设是否与用户需求匹配，呼吁重新定位研究方向。

Method: 提出利用生成式AI助手（如ChatGPT）设计新型群体推荐系统，通过AI代理辅助群体决策过程。

Result: 建议未来群体推荐系统应更自然地融入群体决策环境，以提高实际应用率。

Conclusion: 生成式AI为群体推荐系统提供了新的可能性，有望推动其在实际中的广泛应用。

Abstract: More than twenty-five years ago, first ideas were developed on how to design
a system that can provide recommendations to groups of users instead of
individual users. Since then, a rich variety of algorithmic proposals were
published, e.g., on how to acquire individual preferences, how to aggregate
them, and how to generate recommendations for groups of users. However, despite
the rich literature on the topic, barely any examples of real-world group
recommender systems can be found. This lets us question common assumptions in
academic research, in particular regarding communication processes in a group
and how recommendation-supported decisions are made. In this essay, we argue
that these common assumptions and corresponding system designs often may not
match the needs or expectations of users. We thus call for a reorientation in
this research area, leveraging the capabilities of modern Generative AI
assistants like ChatGPT. Specifically, as one promising future direction, we
envision group recommender systems to be systems where human group members
interact in a chat and an AI-based group recommendation agent assists the
decision-making process in an agentic way. Ultimately, this shall lead to a
more natural group decision-making environment and finally to wider adoption of
group recommendation systems in practice.

</details>


### [208] [Robotic Manipulation by Imitating Generated Videos Without Physical Demonstrations](https://arxiv.org/abs/2507.00990)
*Shivansh Patel,Shraddhaa Mohan,Hanlin Mai,Unnat Jain,Svetlana Lazebnik,Yunzhu Li*

Main category: cs.RO

Relevance: 40.0

TL;DR: RIGVid系统通过模仿AI生成的视频，使机器人无需物理演示或特定训练即可完成复杂操作任务。


<details>
  <summary>Details</summary>
Motivation: 探索如何利用AI生成的视频作为机器人操作的监督来源，减少对物理演示的依赖。

Method: 结合视频扩散模型生成演示视频，视觉语言模型过滤不符合命令的视频，6D姿态跟踪器提取物体轨迹并适配到机器人。

Result: 生成的视频与真实演示效果相当，性能随生成质量提升；优于关键点预测等方法。

Conclusion: AI生成的视频可作为机器人操作的有效监督来源。

Abstract: This work introduces Robots Imitating Generated Videos (RIGVid), a system
that enables robots to perform complex manipulation tasks--such as pouring,
wiping, and mixing--purely by imitating AI-generated videos, without requiring
any physical demonstrations or robot-specific training. Given a language
command and an initial scene image, a video diffusion model generates potential
demonstration videos, and a vision-language model (VLM) automatically filters
out results that do not follow the command. A 6D pose tracker then extracts
object trajectories from the video, and the trajectories are retargeted to the
robot in an embodiment-agnostic fashion. Through extensive real-world
evaluations, we show that filtered generated videos are as effective as real
demonstrations, and that performance improves with generation quality. We also
show that relying on generated videos outperforms more compact alternatives
such as keypoint prediction using VLMs, and that strong 6D pose tracking
outperforms other ways to extract trajectories, such as dense feature point
tracking. These findings suggest that videos produced by a state-of-the-art
off-the-shelf model can offer an effective source of supervision for robotic
manipulation.

</details>


### [209] [Echoes of AI: Investigating the Downstream Effects of AI Assistants on Software Maintainability](https://arxiv.org/abs/2507.00788)
*Markus Borg,Dave Hewett,Nadim Hagatulah,Noric Couderc,Emma Söderberg,Donald Graham,Uttam Kini,Dave Farley*

Main category: cs.SE

Relevance: 40.0

TL;DR: 研究探讨AI助手对软件可维护性的影响，发现AI辅助开发能提高开发速度，但对代码可维护性影响不大。


<details>
  <summary>Details</summary>
Motivation: 调查AI助手在软件开发中对代码可维护性的影响，填补现有研究的空白。

Method: 两阶段控制实验，151名参与者（95%为专业开发者），分别在有/无AI辅助下完成任务并评估代码可维护性。

Result: AI辅助开发显著提高开发速度（中位数减少30.7%时间），但对代码可维护性影响较小，仅对习惯性AI用户有显著提升。

Conclusion: AI助手能有效加速开发，未发现代码可维护性下降的迹象，建议未来研究关注代码膨胀和认知债务风险。

Abstract: [Context] AI assistants, like GitHub Copilot and Cursor, are transforming
software engineering. While several studies highlight productivity
improvements, their impact on maintainability requires further investigation.
[Objective] This study investigates whether co-development with AI assistants
affects software maintainability, specifically how easily other developers can
evolve the resulting source code. [Method] We conducted a two-phase controlled
experiment involving 151 participants, 95% of whom were professional
developers. In Phase 1, participants added a new feature to a Java web
application, with or without AI assistance. In Phase 2, a randomized controlled
trial, new participants evolved these solutions without AI assistance.
[Results] AI-assisted development in Phase 1 led to a modest speedup in
subsequent evolution and slightly higher average CodeHealth. Although neither
difference was significant overall, the increase in CodeHealth was
statistically significant when habitual AI users completed Phase 1. For Phase
1, we also observed a significant effect that corroborates previous
productivity findings: using an AI assistant yielded a 30.7% median decrease in
task completion time. Moreover, for habitual AI users, the mean speedup was
55.9%. [Conclusions] Our study adds to the growing evidence that AI assistants
can effectively accelerate development. Moreover, we did not observe warning
signs of degraded code-level maintainability. We recommend that future research
focus on risks such as code bloat from excessive code generation and the
build-up of cognitive debt as developers invest less mental effort during
implementation.

</details>


### [210] [HumanoidGen: Data Generation for Bimanual Dexterous Manipulation via LLM Reasoning](https://arxiv.org/abs/2507.00833)
*Zhi Jing,Siyuan Yang,Jicong Ao,Ting Xiao,Yugang Jiang,Chenjia Bai*

Main category: cs.RO

Relevance: 40.0

TL;DR: HumanoidGen是一个自动化任务创建和演示收集框架，利用原子操作和LLM推理生成关系约束，用于人形机器人的双手灵巧操作。


<details>
  <summary>Details</summary>
Motivation: 现有机器人数据集和仿真基准主要针对单臂机器人，缺乏针对人形机器人（配备双臂和灵巧手）的高质量仿真任务和演示。

Method: 结合原子灵巧操作和LLM推理生成空间约束，使用蒙特卡洛树搜索增强LLM的长时任务规划能力，并创建新基准评估数据质量。

Result: 实验表明，生成的2D和3D扩散策略性能随数据集规模提升。

Conclusion: HumanoidGen为双手灵巧操作提供了高效的数据生成和规划方法。

Abstract: For robotic manipulation, existing robotics datasets and simulation
benchmarks predominantly cater to robot-arm platforms. However, for humanoid
robots equipped with dual arms and dexterous hands, simulation tasks and
high-quality demonstrations are notably lacking. Bimanual dexterous
manipulation is inherently more complex, as it requires coordinated arm
movements and hand operations, making autonomous data collection challenging.
This paper presents HumanoidGen, an automated task creation and demonstration
collection framework that leverages atomic dexterous operations and LLM
reasoning to generate relational constraints. Specifically, we provide spatial
annotations for both assets and dexterous hands based on the atomic operations,
and perform an LLM planner to generate a chain of actionable spatial
constraints for arm movements based on object affordances and scenes. To
further improve planning ability, we employ a variant of Monte Carlo tree
search to enhance LLM reasoning for long-horizon tasks and insufficient
annotation. In experiments, we create a novel benchmark with augmented
scenarios to evaluate the quality of the collected data. The results show that
the performance of the 2D and 3D diffusion policies can scale with the
generated dataset. Project page is https://openhumanoidgen.github.io.

</details>


### [211] [The Age of Sensorial Zero Trust: Why We Can No Longer Trust Our Senses](https://arxiv.org/abs/2507.00907)
*Fabio Correa Xavier*

Main category: cs.CR

Relevance: 40.0

TL;DR: 论文提出了一种名为“感官零信任”的安全框架，旨在通过系统验证感官信息来应对生成式AI带来的欺诈风险。


<details>
  <summary>Details</summary>
Motivation: 随着深度伪造和克隆声音等技术的成熟，传统的信任机制已不足以应对新兴的安全威胁，因此需要一种新的安全思维模式。

Method: 结合了带外验证、视觉语言模型（VLMs）、加密来源和人类训练等概念，扩展了零信任原则到感官信息。

Result: 提出了一个基于实证研究的框架，强调在AI生成内容的时代，感官信息需要经过严格验证。

Conclusion: 呼吁领导者培养方法论怀疑的文化，以保护组织在这种新威胁环境中的完整性。

Abstract: In a world where deepfakes and cloned voices are emerging as sophisticated
attack vectors, organizations require a new security mindset: Sensorial Zero
Trust [9]. This article presents a scientific analysis of the need to
systematically doubt information perceived through the senses, establishing
rigorous verification protocols to mitigate the risks of fraud based on
generative artificial intelligence. Key concepts, such as Out-of-Band
verification, Vision-Language Models (VLMs) as forensic collaborators,
cryptographic provenance, and human training, are integrated into a framework
that extends Zero Trust principles to human sensory information. The approach
is grounded in empirical findings and academic research, emphasizing that in an
era of AI-generated realities, even our eyes and ears can no longer be
implicitly trusted without verification. Leaders are called to foster a culture
of methodological skepticism to protect organizational integrity in this new
threat landscape.

</details>


### [212] [Turning AI Data Centers into Grid-Interactive Assets: Results from a Field Demonstration in Phoenix, Arizona](https://arxiv.org/abs/2507.00909)
*Philip Colangelo,Ayse K. Coskun,Jack Megrue,Ciaran Roberts,Shayan Sengupta,Varun Sivaram,Ethan Tiao,Aroon Vijaykar,Chris Williams,Daniel C. Wilson,Zack MacFarland,Daniel Dreiling,Nathan Morey,Anuja Ratnayake,Baskar Vairamohan*

Main category: cs.DC

Relevance: 40.0

TL;DR: Emerald Conductor软件方法通过实时电网信号协调AI工作负载，无需硬件修改或储能，将AI数据中心转变为灵活电网资源，试验中在峰值电网事件期间减少25%的集群功耗，同时保持AI服务质量。


<details>
  <summary>Details</summary>
Motivation: AI的快速发展导致电力需求激增，威胁电网可靠性并增加基础设施成本。本文旨在通过软件方法优化AI数据中心的电力使用，缓解电网压力。

Method: 在256-GPU集群上运行代表性AI工作负载，通过Emerald Conductor平台实时协调工作负载，基于电网信号调整功耗。

Result: 在峰值电网事件期间，集群功耗减少25%，同时保持AI服务质量。

Conclusion: Emerald Conductor展示了数据中心作为电网互动资产的潜力，可提升电网可靠性、降低成本和加速AI发展。

Abstract: Artificial intelligence (AI) is fueling exponential electricity demand
growth, threatening grid reliability, raising prices for communities paying for
new energy infrastructure, and stunting AI innovation as data centers wait for
interconnection to constrained grids. This paper presents the first field
demonstration, in collaboration with major corporate partners, of a
software-only approach--Emerald Conductor--that transforms AI data centers into
flexible grid resources that can efficiently and immediately harness existing
power systems without massive infrastructure buildout. Conducted at a 256-GPU
cluster running representative AI workloads within a commercial, hyperscale
cloud data center in Phoenix, Arizona, the trial achieved a 25% reduction in
cluster power usage for three hours during peak grid events while maintaining
AI quality of service (QoS) guarantees. By orchestrating AI workloads based on
real-time grid signals without hardware modifications or energy storage, this
platform reimagines data centers as grid-interactive assets that enhance grid
reliability, advance affordability, and accelerate AI's development.

</details>


### [213] [A collaborative digital twin built on FAIR data and compute infrastructure](https://arxiv.org/abs/2507.00048)
*Thomas M. Deucher,Juan C. Verduzco,Michael Titus,Alejandro Strachan*

Main category: cs.AI

Relevance: 30.0

TL;DR: 论文提出了一种基于FAIR数据基础设施的分布式自驱动实验室（SDL）框架，用于加速科学和工程中的发现与优化任务。


<details>
  <summary>Details</summary>
Motivation: 通过整合机器学习和自动化实验，促进SDL之间的协作，提高数据共享和优化效率。

Method: 利用nanoHUB服务构建分布式SDL，支持在线模拟和FAIR数据管理，通过主动学习进行序列优化。

Result: 实现了基于FAIR数据的协作优化框架，适用于食品染料配方的优化问题，并可扩展到其他领域。

Conclusion: 该框架为科学和工程中的优化问题提供了通用且可扩展的解决方案。

Abstract: The integration of machine learning with automated experimentation in
self-driving laboratories (SDL) offers a powerful approach to accelerate
discovery and optimization tasks in science and engineering applications. When
supported by findable, accessible, interoperable, and reusable (FAIR) data
infrastructure, SDLs with overlapping interests can collaborate more
effectively. This work presents a distributed SDL implementation built on
nanoHUB services for online simulation and FAIR data management. In this
framework, geographically dispersed collaborators conducting independent
optimization tasks contribute raw experimental data to a shared central
database. These researchers can then benefit from analysis tools and machine
learning models that automatically update as additional data become available.
New data points are submitted through a simple web interface and automatically
processed using a nanoHUB Sim2L, which extracts derived quantities and indexes
all inputs and outputs in a FAIR data repository called ResultsDB. A separate
nanoHUB workflow enables sequential optimization using active learning, where
researchers define the optimization objective, and machine learning models are
trained on-the-fly with all existing data, guiding the selection of future
experiments. Inspired by the concept of ``frugal twin", the optimization task
seeks to find the optimal recipe to combine food dyes to achieve the desired
target color. With easily accessible and inexpensive materials, researchers and
students can set up their own experiments, share data with collaborators, and
explore the combination of FAIR data, predictive ML models, and sequential
optimization. The tools introduced are generally applicable and can easily be
extended to other optimization problems.

</details>


### [214] [SEZ-HARN: Self-Explainable Zero-shot Human Activity Recognition Network](https://arxiv.org/abs/2507.00050)
*Devin Y. De Silva,Sandareka Wickramanayake,Dulani Meedeniya,Sanka Rasnayaka*

Main category: cs.AI

Relevance: 30.0

TL;DR: 论文提出了一种自解释的零样本人类活动识别模型（SEZ-HARN），能够识别未见过的活动并提供解释性骨架视频，性能接近黑盒模型。


<details>
  <summary>Details</summary>
Motivation: 解决IMU传感器数据在人类活动识别（HAR）中缺乏透明性和数据不足的问题。

Method: 提出SEZ-HARN模型，结合零样本学习和自解释能力，生成骨架视频解释决策过程。

Result: 在四个基准数据集上，SEZ-HARN的零样本识别准确率接近最佳黑盒模型，同时提供可理解的解释。

Conclusion: SEZ-HARN在保持竞争力的同时提高了模型透明性，适用于实际应用。

Abstract: Human Activity Recognition (HAR), which uses data from Inertial Measurement
Unit (IMU) sensors, has many practical applications in healthcare and assisted
living environments. However, its use in real-world scenarios has been limited
by the lack of comprehensive IMU-based HAR datasets that cover a wide range of
activities and the lack of transparency in existing HAR models. Zero-shot HAR
(ZS-HAR) overcomes the data limitations, but current models struggle to explain
their decisions, making them less transparent. This paper introduces a novel
IMU-based ZS-HAR model called the Self-Explainable Zero-shot Human Activity
Recognition Network (SEZ-HARN). It can recognize activities not encountered
during training and provide skeleton videos to explain its decision-making
process. We evaluate the effectiveness of the proposed SEZ-HARN on four
benchmark datasets PAMAP2, DaLiAc, HTD-MHAD and MHealth and compare its
performance against three state-of-the-art black-box ZS-HAR models. The
experiment results demonstrate that SEZ-HARN produces realistic and
understandable explanations while achieving competitive Zero-shot recognition
accuracy. SEZ-HARN achieves a Zero-shot prediction accuracy within 3\% of the
best-performing black-box model on PAMAP2 while maintaining comparable
performance on the other three datasets.

</details>


### [215] [Learning for routing: A guided review of recent developments and future directions](https://arxiv.org/abs/2507.00218)
*Fangting Zhou,Attila Lischka,Balazs Kulcsar,Jiaming Wu,Morteza Haghir Chehreghani,Gilbert Laporte*

Main category: cs.AI

Relevance: 30.0

TL;DR: 该论文综述了机器学习在解决NP难组合优化问题（如TSP和VRP）中的应用，提出了一种分类方法，旨在整合传统运筹学方法与现代ML技术。


<details>
  <summary>Details</summary>
Motivation: 由于NP难问题的复杂性，传统方法难以高效求解，而ML技术的成功为这些问题提供了新的解决思路。

Method: 提出了一种分类法，将基于ML的路由方法分为构造型和改进型，并分析了其适用性。

Result: 综述了当前ML在路由问题中的应用进展，并提出了一个结构化框架。

Conclusion: 该研究为未来研究提供了指导，并有助于解决新兴的VRP变体。

Abstract: This paper reviews the current progress in applying machine learning (ML)
tools to solve NP-hard combinatorial optimization problems, with a focus on
routing problems such as the traveling salesman problem (TSP) and the vehicle
routing problem (VRP). Due to the inherent complexity of these problems, exact
algorithms often require excessive computational time to find optimal
solutions, while heuristics can only provide approximate solutions without
guaranteeing optimality. With the recent success of machine learning models,
there is a growing trend in proposing and implementing diverse ML techniques to
enhance the resolution of these challenging routing problems. We propose a
taxonomy categorizing ML-based routing methods into construction-based and
improvement-based approaches, highlighting their applicability to various
problem characteristics. This review aims to integrate traditional OR methods
with state-of-the-art ML techniques, providing a structured framework to guide
future research and address emerging VRP variants.

</details>


### [216] [Efficient Conformance Checking of Rich Data-Aware Declare Specifications (Extended)](https://arxiv.org/abs/2507.00094)
*Jacobo Casas-Ramos,Sarah Winkler,Alessandro Gianola,Marco Montali,Manuel Mucientes,Manuel Lama*

Main category: cs.DB

Relevance: 30.0

TL;DR: 论文提出了一种计算数据感知最优对齐的新方法，结合A*搜索和SMT求解，支持更丰富的数据依赖关系。


<details>
  <summary>Details</summary>
Motivation: 现有对齐方法主要关注控制流或简单数据依赖，无法处理复杂数据条件。本文旨在解决这一限制。

Method: 结合A*搜索和SMT求解，引入修复动作逐步解决约束冲突。

Result: 实验表明，该方法在性能和表达性上优于现有技术。

Conclusion: 该方法高效且支持复杂数据依赖，具有实际应用潜力。

Abstract: Despite growing interest in process analysis and mining for data-aware
specifications, alignment-based conformance checking for declarative process
models has focused on pure control-flow specifications, or mild data-aware
extensions limited to numerical data and variable-to-constant comparisons. This
is not surprising: finding alignments is computationally hard, even more so in
the presence of data dependencies. In this paper, we challenge this problem in
the case where the reference model is captured using data-aware Declare with
general data types and data conditions. We show that, unexpectedly, it is
possible to compute data-aware optimal alignments in this rich setting,
enjoying at once efficiency and expressiveness. This is achieved by carefully
combining the two best-known approaches to deal with control flow and data
dependencies when computing alignments, namely A* search and SMT solving.
Specifically, we introduce a novel algorithmic technique that efficiently
explores the search space, generating descendant states through the application
of repair actions aiming at incrementally resolving constraint violations. We
prove the correctness of our algorithm and experimentally show its efficiency.
The evaluation witnesses that our approach matches or surpasses the performance
of the state of the art while also supporting significantly more expressive
data dependencies, showcasing its potential to support real-world applications.

</details>


### [217] [AI-Governed Agent Architecture for Web-Trustworthy Tokenization of Alternative Assets](https://arxiv.org/abs/2507.00096)
*Ailiya Borjigin,Wei Zhou,Cong He*

Main category: cs.CR

Relevance: 30.0

TL;DR: 论文提出了一种结合AI治理和多智能体系统的区块链架构，用于可信的替代资产代币化，并通过案例研究验证了其有效性。


<details>
  <summary>Details</summary>
Motivation: 解决基于网络的代币化生态系统中信任问题，如数据真实性和欺诈。

Method: 提出AI治理的智能体架构，整合区块链和AI驱动的治理层，实现资产验证、合规检查等。

Result: 增强了代币化生态的透明度、安全性和合规性，有效减少欺诈和洗钱风险。

Conclusion: AI治理与多智能体系统及区块链结合，显著提升代币化资产生态的信任度。

Abstract: Alternative Assets tokenization is transforming non-traditional financial
instruments are represented and traded on the web. However, ensuring
trustworthiness in web-based tokenized ecosystems poses significant challenges,
from verifying off-chain asset data to enforcing regulatory compliance. This
paper proposes an AI-governed agent architecture that integrates intelligent
agents with blockchain to achieve web-trustworthy tokenization of alternative
assets. In the proposed architecture, autonomous agents orchestrate the
tokenization process (asset verification, valuation, compliance checking, and
lifecycle management), while an AI-driven governance layer monitors agent
behavior and enforces trust through adaptive policies and cryptoeconomic
incentives. We demonstrate that this approach enhances transparency, security,
and compliance in asset tokenization, addressing key concerns around data
authenticity and fraud. A case study on tokenizing real estate assets
illustrates how the architecture mitigates risks (e.g., fraudulent listings and
money laundering) through real-time AI anomaly detection and on-chain
enforcement. Our evaluation and analysis suggest that combining AI governance
with multi-agent systems and blockchain can significantly bolster trust in
tokenized asset ecosystems. This work offers a novel framework for trustworthy
asset tokenization on the web and provides insights for practitioners aiming to
deploy secure, compliant tokenization platforms.

</details>


### [218] [AI-Hybrid TRNG: Kernel-Based Deep Learning for Near-Uniform Entropy Harvesting from Physical Noise](https://arxiv.org/abs/2507.00145)
*Hasan Yiğit*

Main category: cs.CR

Relevance: 30.0

TL;DR: AI-Hybrid TRNG是一个深度学习框架，直接从物理噪声中提取高熵随机数，无需量子设备或昂贵硬件。它通过低成本RF前端和CPU时序抖动训练，生成32位高熵流，并通过严格测试。


<details>
  <summary>Details</summary>
Motivation: 传统随机数生成器依赖昂贵硬件，AI-Hybrid TRNG旨在通过低成本方案实现高质量随机数生成，适用于资源受限平台。

Method: 结合动态内外网络，利用自适应自然源和重新播种技术，生成不可预测的随机序列。

Result: 生成的随机数通过NIST SP 800-22测试和19项定制统计测试，满足密码学标准，无预测偏差。

Conclusion: AI-Hybrid TRNG为安全系统、加密协议和边缘设备提供了高效、低成本的随机数生成方案。

Abstract: AI-Hybrid TRNG is a deep-learning framework that extracts near-uniform
entropy directly from physical noise, eliminating the need for bulky quantum
devices or expensive laboratory-grade RF receivers. Instead, it relies on a
low-cost, thumb-sized RF front end, plus CPU-timing jitter, for training, and
then emits 32-bit high-entropy streams without any quantization step.
  Unlike deterministic or trained artificial intelligence random number
generators (RNGs), our dynamic inner-outer network couples adaptive natural
sources and reseeding, yielding truly unpredictable and autonomous sequences.
Generated numbers pass the NIST SP 800-22 battery better than a CPU-based
method. It also passes nineteen bespoke statistical tests for both bit- and
integer-level analysis. All results satisfy cryptographic standards, while
forward and backward prediction experiments reveal no exploitable biases. The
model's footprint is below 0.5 MB, making it deployable on MCUs and FPGA soft
cores, as well as suitable for other resource-constrained platforms.
  By detaching randomness quality from dedicated hardware, AI-Hybrid TRNG
broadens the reach of high-integrity random number generators across secure
systems, cryptographic protocols, embedded and edge devices, stochastic
simulations, and server applications that need randomness.

</details>


### [219] [Reconfiguring Digital Accountability: AI-Powered Innovations and Transnational Governance in a Postnational Accounting Context](https://arxiv.org/abs/2507.00288)
*Claire Li,David Freeborn*

Main category: econ.TH

Relevance: 30.0

TL;DR: 研究探讨AI驱动的数字创新如何重塑跨国治理中的组织问责制，结合TAM、ANT和制度理论分析AI技术的采纳。


<details>
  <summary>Details</summary>
Motivation: AI系统在审计和财务报告等领域的决策中日益重要，传统问责机制受到挑战，需研究如何在全球范围内实现负责任的AI采纳。

Method: 整合技术接受模型（TAM）、行动者网络理论（ANT）和制度理论，分析组织在跨国压力下采纳AI的策略。

Result: 提出问责制是全球社会技术网络中共同构建的关系性属性，并设计两种组织策略以促进负责任的AI采纳。

Conclusion: 问责制需通过内部治理重构和外部行动者网络参与实现，以应对跨国治理中的AI挑战。

Abstract: This study explores how AI-powered digital innovations are reshaping
organisational accountability in a transnational governance context. As AI
systems increasingly mediate decision-making in domains such as auditing and
financial reporting, traditional mechanisms of accountability, based on
control, transparency, and auditability, are being destabilised. We integrate
the Technology Acceptance Model (TAM), Actor-Network Theory (ANT), and
institutional theory to examine how organisations adopt AI technologies in
response to regulatory, ethical, and cultural pressures that transcend national
boundaries. We argue that accountability is co-constructed within global
socio-technical networks, shaped not only by user perceptions but also by
governance logics and normative expectations. Extending TAM, we incorporate
compliance and legitimacy as key factors in perceived usefulness and usability.
Drawing on ANT, we reconceptualise accountability as a relational and emergent
property of networked assemblages. We propose two organisational strategies
including internal governance reconfiguration and external actor-network
engagement to foster responsible, legitimate, and globally accepted AI adoption
in the accounting domain.

</details>


### [220] [An AST-guided LLM Approach for SVRF Code Synthesis](https://arxiv.org/abs/2507.00352)
*Abanoub E. Abdelmalak,Mohamed A. Elsayed,David Abercrombie,Ilhami Torunoglu*

Main category: cs.SE

Relevance: 30.0

TL;DR: 论文提出了一种结合AST嵌入和RAG的新方法，用于优化半导体设计中的SVRF代码生成，提高了准确性和效率。


<details>
  <summary>Details</summary>
Motivation: 随着半导体设计规则的复杂化，传统SVRF开发方法效率低下且存在专业知识缺口，需要更高效的代码生成方法。

Method: 采用AST嵌入和RAG技术，结合领域知识进行代码生成，并通过T5模型和自定义评分框架进行评估。

Result: 在740个DRC规则测试中，代码生成准确率提升了40%。

Conclusion: 该方法显著提升了SVRF开发的效率和准确性，减少了人工错误。

Abstract: Standard Verification Rule Format (SVRF) is essential for semiconductor
applications like Design Rule Check (DRC), Layout Versus Schematic (LVS), and
Optical Proximity Correction (OPC) and it faces challenges as advancing nodes
create complex design rules that renders traditional SVRF development
ineffective and highlight an expertise gap. This paper introduces a novel
methodology integrating Abstract Syntax Tree (AST) embedding and
Retrieval-Augmented Generation (RAG) for enhanced SVRF code synthesis, ensuring
semantic accuracy and error minimization through structural validation with
domain-specific insights for precise code generation.
  We evaluate different T5-based models and propose an innovative SVRF-specific
scoring framework that complements standard metrics like BLEU and ROUGE-L. In
our approach, AST provides rigorous structural validation, while RAG infuses
relevant domain knowledge, effectively enhancing the code generation workflow.
  Testing on a comprehensive benchmark of 740 DRC rule implementations, our
methodology demonstrates up to a 40\% improvement in code generation accuracy
compared to basic text-based fine-tuning process. This fusion of industry
expertise with advanced coding strategies not only optimizes SVRF development
under limited dataset constraints but also creates a more intuitive and
efficient coding environment. Consequently, users can rapidly iterate through
design cycles, reduce manual error correction, and significantly improve
overall productivity.

</details>


### [221] [Augmenting Molecular Graphs with Geometries via Machine Learning Interatomic Potentials](https://arxiv.org/abs/2507.00407)
*Cong Fu,Yuchao Lin,Zachary Krueger,Haiyang Yu,Maho Nakata,Jianwen Xie,Emine Kucukbenli,Xiaofeng Qian,Shuiwang Ji*

Main category: physics.chem-ph

Relevance: 30.0

TL;DR: 论文提出了一种基于机器学习原子间势（MLIP）基础模型的方法，用于预测分子几何结构，并通过几何优化和微调提升分子属性预测的准确性。


<details>
  <summary>Details</summary>
Motivation: 传统方法（如密度泛函理论）获取分子几何结构成本高昂，因此探索仅依赖MLIP模型的方法以降低成本并提高效率。

Method: 首先构建了一个包含350万分子和3亿快照的大规模分子弛豫数据集，然后通过监督学习训练MLIP基础模型预测能量和力。模型可用于显式（几何优化）或隐式（直接微调）获取几何结构。

Result: 训练后的MLIP基础模型能够提供有价值的分子几何结构，显著提升下游分子属性预测的准确性。

Conclusion: MLIP基础模型在分子几何结构预测中具有潜力，能够替代传统高成本方法，并为分子属性预测提供支持。

Abstract: Accurate molecular property predictions require 3D geometries, which are
typically obtained using expensive methods such as density functional theory
(DFT). Here, we attempt to obtain molecular geometries by relying solely on
machine learning interatomic potential (MLIP) models. To this end, we first
curate a large-scale molecular relaxation dataset comprising 3.5 million
molecules and 300 million snapshots. Then MLIP foundation models are trained
with supervised learning to predict energy and forces given 3D molecular
structures. Once trained, we show that the foundation models can be used in
different ways to obtain geometries either explicitly or implicitly. First, it
can be used to obtain low-energy 3D geometries via geometry optimization,
providing relaxed 3D geometries for downstream molecular property predictions.
To mitigate potential biases and enhance downstream predictions, we introduce
geometry fine-tuning based on the relaxed 3D geometries. Second, the foundation
models can be directly fine-tuned for property prediction when ground truth 3D
geometries are available. Our results demonstrate that MLIP foundation models
trained on relaxation data can provide valuable molecular geometries that
benefit property predictions.

</details>


### [222] [Geological Everything Model 3D: A Promptable Foundation Model for Unified and Zero-Shot Subsurface Understanding](https://arxiv.org/abs/2507.00419)
*Yimin Dou,Xinming Wu,Nathan L Bangs,Harpreet Singh Sethi,Jintao Li,Hang Gao,Zhixiang Guo*

Main category: physics.geo-ph

Relevance: 30.0

TL;DR: GEM是一个统一的地质生成模型，通过提示条件推理实现多任务零样本泛化，无需针对新任务或数据源重新训练。


<details>
  <summary>Details</summary>
Motivation: 解决地下分析任务碎片化问题，提出统一模型以替代多个任务特定模型。

Method: 采用两阶段训练：自监督表示学习与对抗性微调，结合结构框架和提示条件推理。

Result: GEM在多种地质任务中表现出广泛适用性，包括火星雷达地层分析和地震解释等。

Conclusion: GEM为地质AI提供了可扩展、人类参与的统一推理系统。

Abstract: Understanding Earth's subsurface is critical for energy transition, natural
hazard mitigation, and planetary science. Yet subsurface analysis remains
fragmented, with separate models required for structural interpretation,
stratigraphic analysis, geobody segmentation, and property modeling-each
tightly coupled to specific data distributions and task formulations. We
introduce the Geological Everything Model 3D (GEM), a unified generative
architecture that reformulates all these tasks as prompt-conditioned inference
along latent structural frameworks derived from subsurface imaging. This
formulation moves beyond task-specific models by enabling a shared inference
mechanism, where GEM propagates human-provided prompts-such as well logs,
masks, or structural sketches-along inferred structural frameworks to produce
geologically coherent outputs. Through this mechanism, GEM achieves zero-shot
generalization across tasks with heterogeneous prompt types, without retraining
for new tasks or data sources. This capability emerges from a two-stage
training process that combines self-supervised representation learning on
large-scale field seismic data with adversarial fine-tuning using mixed prompts
and labels across diverse subsurface tasks. GEM demonstrates broad
applicability across surveys and tasks, including Martian radar stratigraphy
analysis, structural interpretation in subduction zones, full seismic
stratigraphic interpretation, geobody delineation, and property modeling. By
bridging expert knowledge with generative reasoning in a structurally aware
manner, GEM lays the foundation for scalable, human-in-the-loop geophysical
AI-transitioning from fragmented pipelines to a vertically integrated,
promptable reasoning system. Project page: https://douyimin.github.io/GEM

</details>


### [223] [Process-aware and high-fidelity microstructure generation using stable diffusion](https://arxiv.org/abs/2507.00459)
*Hoang Cuong Phan,Minh Tien Tran,Chihun Lee,Hoheok Kim,Sehyok Oh,Dong-Kyu Kim,Ho Won Lee*

Main category: cond-mat.mtrl-sci

Relevance: 30.0

TL;DR: 论文提出了一种基于SD3.5-Large的生成模型，用于合成受工艺参数调控的微观结构图像，解决了数据稀缺和连续变量编码的挑战。


<details>
  <summary>Details</summary>
Motivation: 理解材料设计中工艺-结构关系需要合成真实的微观结构图像，但现有方法受限于数据稀缺和连续变量的处理。

Method: 采用SD3.5-Large模型，引入数值感知嵌入编码连续变量，并通过DreamBooth和LoRA进行高效微调。

Result: 合成图像在语义分割和物理描述符分析中表现优异，准确率达97.1%，均交并比为85.7%。

Conclusion: 该方法首次将SD3.5-Large应用于工艺感知的微观结构生成，为数据驱动的材料设计提供了可扩展方案。

Abstract: Synthesizing realistic microstructure images conditioned on processing
parameters is crucial for understanding process-structure relationships in
materials design. However, this task remains challenging due to limited
training micrographs and the continuous nature of processing variables. To
overcome these challenges, we present a novel process-aware generative modeling
approach based on Stable Diffusion 3.5 Large (SD3.5-Large), a state-of-the-art
text-to-image diffusion model adapted for microstructure generation. Our method
introduces numeric-aware embeddings that encode continuous variables (annealing
temperature, time, and magnification) directly into the model's conditioning,
enabling controlled image generation under specified process conditions and
capturing process-driven microstructural variations. To address data scarcity
and computational constraints, we fine-tune only a small fraction of the
model's weights via DreamBooth and Low-Rank Adaptation (LoRA), efficiently
transferring the pre-trained model to the materials domain. We validate realism
using a semantic segmentation model based on a fine-tuned U-Net with a VGG16
encoder on 24 labeled micrographs. It achieves 97.1% accuracy and 85.7% mean
IoU, outperforming previous methods. Quantitative analyses using physical
descriptors and spatial statistics show strong agreement between synthetic and
real microstructures. Specifically, two-point correlation and lineal-path
errors remain below 2.1% and 0.6%, respectively. Our method represents the
first adaptation of SD3.5-Large for process-aware microstructure generation,
offering a scalable approach for data-driven materials design.

</details>


### [224] [Physics-Aware Style Transfer for Adaptive Holographic Reconstruction](https://arxiv.org/abs/2507.00482)
*Chanseok Lee,Fakhriyya Mammadova,Jiseong Barg,Mooseok Jang*

Main category: physics.optics

Relevance: 30.0

TL;DR: 论文提出了一种基于物理感知的风格迁移方法，用于解决全息成像中的逆问题，无需高质量真实数据即可重建物体复振幅。


<details>
  <summary>Details</summary>
Motivation: 解决全息成像中逆问题的传统方法需要高质量的真实数据集，而该研究旨在开发一种仅需强度测量数据即可学习逆映射的方法。

Method: 利用物体到传感器的距离作为衍射图案中的隐式风格，通过风格域构建循环图像翻译，自适应学习逆映射操作。

Result: 该方法在动态流动的红细胞形态重建中展示了生物医学应用潜力，支持实时、无标记成像。

Conclusion: 该方法为难以获取真实数据的成像应用提供了一种实用的学习策略。

Abstract: Inline holographic imaging presents an ill-posed inverse problem of
reconstructing objects' complex amplitude from recorded diffraction patterns.
Although recent deep learning approaches have shown promise over classical
phase retrieval algorithms, they often require high-quality ground truth
datasets of complex amplitude maps to achieve a statistical inverse mapping
operation between the two domains. Here, we present a physics-aware style
transfer approach that interprets the object-to-sensor distance as an implicit
style within diffraction patterns. Using the style domain as the intermediate
domain to construct cyclic image translation, we show that the inverse mapping
operation can be learned in an adaptive manner only with datasets composed of
intensity measurements. We further demonstrate its biomedical applicability by
reconstructing the morphology of dynamically flowing red blood cells,
highlighting its potential for real-time, label-free imaging. As a framework
that leverages physical cues inherently embedded in measurements, the presented
method offers a practical learning strategy for imaging applications where
ground truth is difficult or impossible to obtain.

</details>


### [225] [Automated anatomy-based post-processing reduces false positives and improved interpretability of deep learning intracranial aneurysm detection](https://arxiv.org/abs/2507.00832)
*Jisoo Kim,Chu-Hsuan Lin,Alberto Ceballos-Arroyo,Ping Liu,Huaizu Jiang,Shrikanth Yadav,Qi Wan,Lei Qin,Geoffrey S Young*

Main category: eess.IV

Relevance: 30.0

TL;DR: 论文提出了一种基于解剖学的启发式学习混合动脉-静脉分割后处理方法，用于减少颅内动脉瘤检测中的假阳性率。


<details>
  <summary>Details</summary>
Motivation: 尽管深度学习模型在颅内动脉瘤检测上有所改进，但高假阳性率仍是临床应用的障碍。本文旨在通过解剖学知识和启发式学习相结合的方法进一步降低假阳性率。

Method: 使用两种深度学习模型（CPM-Net和3D-CNN-TR）训练并评估CTA数据，结合脑部、动脉、静脉等分割掩码进行后处理以减少假阳性。

Result: 最佳后处理方法（方法5）显著降低了假阳性率（CPM-Net减少70.6%，3D-CNN-TR减少51.6%），同时未减少真阳性。

Conclusion: 基于解剖学的后处理方法可以提升深度学习模型的性能，为临床接受度提供支持。

Abstract: Introduction: Deep learning (DL) models can help detect intracranial
aneurysms on CTA, but high false positive (FP) rates remain a barrier to
clinical translation, despite improvement in model architectures and strategies
like detection threshold tuning. We employed an automated, anatomy-based,
heuristic-learning hybrid artery-vein segmentation post-processing method to
further reduce FPs. Methods: Two DL models, CPM-Net and a deformable 3D
convolutional neural network-transformer hybrid (3D-CNN-TR), were trained with
1,186 open-source CTAs (1,373 annotated aneurysms), and evaluated with 143
held-out private CTAs (218 annotated aneurysms). Brain, artery, vein, and
cavernous venous sinus (CVS) segmentation masks were applied to remove possible
FPs in the DL outputs that overlapped with: (1) brain mask; (2) vein mask; (3)
vein more than artery masks; (4) brain plus vein mask; (5) brain plus vein more
than artery masks. Results: CPM-Net yielded 139 true-positives (TP); 79
false-negative (FN); 126 FP. 3D-CNN-TR yielded 179 TP; 39 FN; 182 FP. FPs were
commonly extracranial (CPM-Net 27.3%; 3D-CNN-TR 42.3%), venous (CPM-Net 56.3%;
3D-CNN-TR 29.1%), arterial (CPM-Net 11.9%; 3D-CNN-TR 53.3%), and non-vascular
(CPM-Net 25.4%; 3D-CNN-TR 9.3%) structures. Method 5 performed best, reducing
CPM-Net FP by 70.6% (89/126) and 3D-CNN-TR FP by 51.6% (94/182), without
reducing TP, lowering the FP/case rate from 0.88 to 0.26 for CPM-NET, and from
1.27 to 0.62 for the 3D-CNN-TR. Conclusion: Anatomy-based, interpretable
post-processing can improve DL-based aneurysm detection model performance. More
broadly, automated, domain-informed, hybrid heuristic-learning processing holds
promise for improving the performance and clinical acceptance of aneurysm
detection models.

</details>


### [226] [Inverse Design in Nanophotonics via Representation Learning](https://arxiv.org/abs/2507.00546)
*Reza Marzban,Ali Adibi,Raphael Pestourie*

Main category: physics.app-ph

Relevance: 30.0

TL;DR: 该论文综述了机器学习在纳米光子学逆向设计中的应用，分为输出侧和输入侧方法，并讨论了混合框架的优势与挑战。


<details>
  <summary>Details</summary>
Motivation: 传统直觉驱动或迭代优化方法在高维非凸设计空间和计算需求大的电磁模拟中表现不佳，机器学习为解决这些问题提供了新思路。

Method: 通过表示学习的视角，将方法分为输出侧（学习解空间表示以加速优化）和输入侧（学习设备几何的潜在空间表示以实现全局探索）。

Result: 混合框架结合了物理优化和数据驱动表示，能够避免局部最优、提高可扩展性并促进知识迁移。

Conclusion: 论文总结了当前挑战，如复杂性管理、几何无关表示、制造约束集成和多物理协同设计的进展。

Abstract: Inverse design in nanophotonics, the computational discovery of structures
achieving targeted electromagnetic (EM) responses, has become a key tool for
recent optical advances. Traditional intuition-driven or iterative optimization
methods struggle with the inherently high-dimensional, non-convex design spaces
and the substantial computational demands of EM simulations. Recently, machine
learning (ML) has emerged to address these bottlenecks effectively. This review
frames ML-enhanced inverse design methodologies through the lens of
representation learning, classifying them into two categories: output-side and
input-side approaches. Output-side methods use ML to learn a representation in
the solution space to create a differentiable solver that accelerates
optimization. Conversely, input-side techniques employ ML to learn compact,
latent-space representations of feasible device geometries, enabling efficient
global exploration through generative models. Each strategy presents unique
trade-offs in data requirements, generalization capacity, and novel design
discovery potentials. Hybrid frameworks that combine physics-based optimization
with data-driven representations help escape poor local optima, improve
scalability, and facilitate knowledge transfer. We conclude by highlighting
open challenges and opportunities, emphasizing complexity management,
geometry-independent representations, integration of fabrication constraints,
and advancements in multiphysics co-designs.

</details>


### [227] [Deep learning-based segmentation of T1 and T2 cardiac MRI maps for automated disease detection](https://arxiv.org/abs/2507.00903)
*Andreea Bianca Popescu,Andreas Seitz,Heiko Mahrholdt,Jens Wetzl,Athira Jacob,Lucian Mihai Itu,Constantin Suciu,Teodora Chitiboi*

Main category: eess.IV

Relevance: 30.0

TL;DR: 该研究探讨了深度学习在心脏组织分割中的准确性，并评估了多特征机器学习在疾病检测中的效果。


<details>
  <summary>Details</summary>
Motivation: 减少心脏组织特征量化中的人工标注变异性，并探索深度学习与传统方法的比较。

Method: 使用深度学习模型进行心脏组织分割，并结合统计特征和机器学习方法进行疾病检测。

Result: 深度学习分割模型的DICE得分为85.4%，优于人工标注一致性；随机森林结合多特征显著提高了疾病检测的F1分数（92.7%）。

Conclusion: 深度学习可用于心脏组织分割，多特征机器学习能有效提升疾病检测性能。

Abstract: Objectives Parametric tissue mapping enables quantitative cardiac tissue
characterization but is limited by inter-observer variability during manual
delineation. Traditional approaches relying on average relaxation values and
single cutoffs may oversimplify myocardial complexity. This study evaluates
whether deep learning (DL) can achieve segmentation accuracy comparable to
inter-observer variability, explores the utility of statistical features beyond
mean T1/T2 values, and assesses whether machine learning (ML) combining
multiple features enhances disease detection. Materials & Methods T1 and T2
maps were manually segmented. The test subset was independently annotated by
two observers, and inter-observer variability was assessed. A DL model was
trained to segment left ventricle blood pool and myocardium. Average (A), lower
quartile (LQ), median (M), and upper quartile (UQ) were computed for the
myocardial pixels and employed in classification by applying cutoffs or in ML.
Dice similarity coefficient (DICE) and mean absolute percentage error evaluated
segmentation performance. Bland-Altman plots assessed inter-user and
model-observer agreement. Receiver operating characteristic analysis determined
optimal cutoffs. Pearson correlation compared features from model and manual
segmentations. F1-score, precision, and recall evaluated classification
performance. Wilcoxon test assessed differences between classification methods,
with p < 0.05 considered statistically significant. Results 144 subjects were
split into training (100), validation (15) and evaluation (29) subsets.
Segmentation model achieved a DICE of 85.4%, surpassing inter-observer
agreement. Random forest applied to all features increased F1-score (92.7%, p <
0.001). Conclusion DL facilitates segmentation of T1/ T2 maps. Combining
multiple features with ML improves disease detection.

</details>


### [228] [Customer Service Representative's Perception of the AI Assistant in an Organization's Call Center](https://arxiv.org/abs/2507.00513)
*Kai Qin,Kexin Du,Yimeng Chen,Yueyan Liu,Jie Cai,Zhiqiang Nie,Nan Gao,Guohui Wei,Shengzhu Wang,Chun Yu*

Main category: cs.HC

Relevance: 30.0

TL;DR: 研究探讨了电力网格客服中心员工对AI辅助的看法，发现AI减轻了传统负担但引入了新负担。


<details>
  <summary>Details</summary>
Motivation: 研究AI在组织环境中的整合及其对员工的影响。

Method: 通过实地考察和半结构化访谈13名客服代表。

Result: AI减轻了传统负担（如打字和记忆），但引入了新负担（如学习、合规和心理负担）。

Conclusion: 研究揭示了AI整合的复杂性及员工适应新系统的努力与负担。

Abstract: The integration of various AI tools creates a complex socio-technical
environment where employee-customer interactions form the core of work
practices. This study investigates how customer service representatives (CSRs)
at the power grid service customer service call center perceive AI assistance
in their interactions with customers. Through a field visit and semi-structured
interviews with 13 CSRs, we found that AI can alleviate some traditional
burdens during the call (e.g., typing and memorizing) but also introduces new
burdens (e.g., earning, compliance, psychological burdens). This research
contributes to a more nuanced understanding of AI integration in organizational
settings and highlights the efforts and burdens undertaken by CSRs to adapt to
the updated system.

</details>


### [229] [High-resolution spatial memory requires grid-cell-like neural codes](https://arxiv.org/abs/2507.00598)
*Madison Cotteret,Christopher J. Kymn,Hugh Greatorex,Martin Ziegler,Elisabetta Chicca,Friedrich T. Sommer*

Main category: cs.NE

Relevance: 30.0

TL;DR: 论文研究了连续吸引子网络（CANs）如何通过稀疏二进制分布式编码实现高稳定性和高分辨率，解决了传统模型中的稳定性与分辨率矛盾。


<details>
  <summary>Details</summary>
Motivation: 生物系统中的噪声和异质性使得连续吸引子网络对记忆机制非常敏感，传统模型在稳定性和分辨率之间存在矛盾。

Method: 采用基于随机特征嵌入的稀疏二进制分布式编码，模拟神经元具有空间周期性感受野的网格细胞样编码。

Result: 理论和模拟表明，这种编码方式使CANs同时实现高稳定性和高分辨率，并支持非线性流形的嵌入和灵活计算。

Conclusion: 该研究为大脑如何高分辨率且稳健地表示连续变量提供了理论支持。

Abstract: Continuous attractor networks (CANs) are widely used to model how the brain
temporarily retains continuous behavioural variables via persistent recurrent
activity, such as an animal's position in an environment. However, this memory
mechanism is very sensitive to even small imperfections, such as noise or
heterogeneity, which are both common in biological systems. Previous work has
shown that discretising the continuum into a finite set of discrete attractor
states provides robustness to these imperfections, but necessarily reduces the
resolution of the represented variable, creating a dilemma between stability
and resolution. We show that this stability-resolution dilemma is most severe
for CANs using unimodal bump-like codes, as in traditional models. To overcome
this, we investigate sparse binary distributed codes based on random feature
embeddings, in which neurons have spatially-periodic receptive fields. We
demonstrate theoretically and with simulations that such grid-cell-like codes
enable CANs to achieve both high stability and high resolution simultaneously.
The model extends to embedding arbitrary nonlinear manifolds into a CAN, such
as spheres or tori, and generalises linear path integration to integration
along freely-programmable on-manifold vector fields. Together, this work
provides a theory of how the brain could robustly represent continuous
variables with high resolution and perform flexible computations over
task-relevant manifolds.

</details>


### [230] [LearnAFE: Circuit-Algorithm Co-design Framework for Learnable Audio Analog Front-End](https://arxiv.org/abs/2507.00755)
*Jinhai Hu,Zhongyi Zhang,Cong Sheng Leow,Wang Ling Goh,Yuan Gao*

Main category: eess.AS

Relevance: 30.0

TL;DR: 论文提出了一种电路-算法协同设计框架，用于音频信号分类中的可学习模拟前端（AFE），通过联合优化后端分类器和AFE的传递函数实现系统级优化。


<details>
  <summary>Details</summary>
Motivation: 传统方法将AFE和后端分类器分开设计，效果不佳。论文旨在通过协同优化提升系统性能。

Method: 提出了一种SNR感知的训练循环，优化模拟带通滤波器（BPF）组的传递函数参数，并使用协同设计损失函数LBPF。

Result: 在SKY130 130nm CMOS工艺中实现，优化设计在10关键词分类任务中达到90.5%-94.2%的准确率，功耗和电容面积分别减少8.7%和12.9%。

Conclusion: 协同设计方法显著提升了系统性能和效率。

Abstract: This paper presents a circuit-algorithm co-design framework for learnable
analog front-end (AFE) in audio signal classification. Designing AFE and
backend classifiers separately is a common practice but non-ideal, as shown in
this paper. Instead, this paper proposes a joint optimization of the backend
classifier with the AFE's transfer function to achieve system-level optimum.
More specifically, the transfer function parameters of an analog bandpass
filter (BPF) bank are tuned in a signal-to-noise ratio (SNR)-aware training
loop for the classifier. Using a co-design loss function LBPF, this work shows
superior optimization of both the filter bank and the classifier. Implemented
in open-source SKY130 130nm CMOS process, the optimized design achieved
90.5%-94.2% accuracy for 10-keyword classification task across a wide range of
input signal SNR from 5 dB to 20 dB, with only 22k classifier parameters.
Compared to conventional approach, the proposed audio AFE achieves 8.7% and
12.9% reduction in power and capacitor area respectively.

</details>


### [231] [PI-WAN: A Physics-Informed Wind-Adaptive Network for Quadrotor Dynamics Prediction in Unknown Environments](https://arxiv.org/abs/2507.00816)
*Mengyun Wang,Bo Wang,Yifeng Niu,Chang Wang*

Main category: cs.RO

Relevance: 30.0

TL;DR: PI-WAN结合知识驱动和数据驱动方法，通过物理约束嵌入训练过程，提升四旋翼飞行器的动态建模和轨迹跟踪性能。


<details>
  <summary>Details</summary>
Motivation: 传统物理建模在未知环境中受限，数据驱动方法在分布外数据上泛化能力差，需要一种结合两者的方法。

Method: 使用Temporal Convolutional Network (TCN)架构捕获时间依赖性，并通过物理约束损失函数提升泛化能力。

Result: 在仿真和实际飞行实验中，PI-WAN在预测精度、跟踪性能和鲁棒性上优于基线方法。

Conclusion: PI-WAN通过结合物理约束和数据驱动方法，显著提升了四旋翼飞行器在未知环境中的性能。

Abstract: Accurate dynamics modeling is essential for quadrotors to achieve precise
trajectory tracking in various applications. Traditional physical
knowledge-driven modeling methods face substantial limitations in unknown
environments characterized by variable payloads, wind disturbances, and
external perturbations. On the other hand, data-driven modeling methods suffer
from poor generalization when handling out-of-distribution (OoD) data,
restricting their effectiveness in unknown scenarios. To address these
challenges, we introduce the Physics-Informed Wind-Adaptive Network (PI-WAN),
which combines knowledge-driven and data-driven modeling methods by embedding
physical constraints directly into the training process for robust quadrotor
dynamics learning. Specifically, PI-WAN employs a Temporal Convolutional
Network (TCN) architecture that efficiently captures temporal dependencies from
historical flight data, while a physics-informed loss function applies physical
principles to improve model generalization and robustness across previously
unseen conditions. By incorporating real-time prediction results into a model
predictive control (MPC) framework, we achieve improvements in closed-loop
tracking performance. Comprehensive simulations and real-world flight
experiments demonstrate that our approach outperforms baseline methods in terms
of prediction accuracy, tracking precision, and robustness to unknown
environments.

</details>


### [232] [Advancing Local Search in SMT-NRA with MCSAT Integration](https://arxiv.org/abs/2507.00557)
*Tianyi Ding,Haokun Li,Xinpeng Ni,Bican Xia,Tianqi Zhao*

Main category: cs.AI

Relevance: 20.0

TL;DR: 论文提出了一种名为$2d$-cell-jump的二维单元跳跃操作，扩展了SMT-NRA的局部搜索方法，并结合MCSAT框架和sample-cell投影算子，显著提高了搜索效率。


<details>
  <summary>Details</summary>
Motivation: 提升SMT-NRA问题的局部搜索效率，通过扩展搜索操作和结合多种框架优化性能。

Method: 引入$2d$-cell-jump操作，扩展$2d$-LS框架，结合MCSAT和sample-cell投影算子，设计混合框架。

Result: 实验结果表明，所提方法显著提高了局部搜索性能。

Conclusion: 通过结合多种技术和框架，论文有效提升了SMT-NRA问题的搜索效率。

Abstract: In this paper, we advance local search for Satisfiability Modulo the Theory
of Nonlinear Real Arithmetic (SMT-NRA for short). First, we introduce a
two-dimensional cell-jump move, called \emph{$2d$-cell-jump}, generalizing the
key operation, cell-jump, of the local search method for SMT-NRA. Then, we
propose an extended local search framework, named \emph{$2d$-LS} (following the
local search framework, LS, for SMT-NRA), integrating the model constructing
satisfiability calculus (MCSAT) framework to improve search efficiency. To
further improve the efficiency of MCSAT, we implement a recently proposed
technique called \emph{sample-cell projection operator} for MCSAT, which is
well suited for CDCL-style search in the real domain and helps guide the search
away from conflicting states. Finally, we design a hybrid framework for SMT-NRA
combining MCSAT, $2d$-LS and OpenCAD, to improve search efficiency through
information exchange. The experimental results demonstrate improvements in
local search performance, highlighting the effectiveness of the proposed
methods.

</details>


### [233] [A Robust Algorithm for Non-IID Machine Learning Problems with Convergence Analysis](https://arxiv.org/abs/2507.00810)
*Qing Xu,Xiaohua Xuan*

Main category: cs.AI

Relevance: 20.0

TL;DR: 提出了一种基于非光滑优化、二次规划和迭代过程的改进数值算法，用于解决极小极大问题，并提供了收敛性证明。


<details>
  <summary>Details</summary>
Motivation: 极小极大问题在鲁棒优化和不平衡学习等领域有广泛应用，但现有算法在收敛性和效率上存在不足。

Method: 结合非光滑优化、二次规划和迭代过程设计新算法，并在梯度连续性和有界性等温和假设下证明其收敛性。

Result: 算法在理论上有收敛保证，适用于多种应用场景。

Conclusion: 该算法为解决极小极大问题提供了一种有效工具，具有广泛的应用潜力。

Abstract: In this paper, we propose an improved numerical algorithm for solving minimax
problems based on nonsmooth optimization, quadratic programming and iterative
process. We also provide a rigorous proof of convergence for our algorithm
under some mild assumptions, such as gradient continuity and boundedness. Such
an algorithm can be widely applied in various fields such as robust
optimization, imbalanced learning, etc.

</details>


### [234] [InSight-R: A Framework for Risk-informed Human Failure Event Identification and Interface-Induced Risk Assessment Driven by AutoGraph](https://arxiv.org/abs/2507.00066)
*Xingyu Xiao,Jiejuan Tong,Peng Chen,Jun Sun,Zhe Sui,Jingang Liang,Hongru Zhao,Jun Zhao,Haitao Wang*

Main category: cs.HC

Relevance: 20.0

TL;DR: 论文提出了一种基于AutoGraph的框架（InSight-R），用于自动化识别人为故障事件（HFE）和评估界面设计对操作员表现的影响，解决了传统HRA方法的主观性和数据整合不足问题。


<details>
  <summary>Details</summary>
Motivation: 传统HRA方法依赖专家判断，存在主观性和数据整合不足的问题，特别是在评估人机界面设计对操作员表现的影响方面。

Method: 通过AutoGraph构建界面嵌入知识图（IE-KG），结合实证行为数据，自动化识别HFE并评估界面设计风险。

Result: InSight-R提高了HFE识别的客观性和可解释性，并为动态实时HRA提供了可扩展的途径。

Conclusion: 该框架为界面设计优化提供了实用见解，并推动了机制驱动的HRA方法的发展。

Abstract: Human reliability remains a critical concern in safety-critical domains such
as nuclear power, where operational failures are often linked to human error.
While conventional human reliability analysis (HRA) methods have been widely
adopted, they rely heavily on expert judgment for identifying human failure
events (HFEs) and assigning performance influencing factors (PIFs). This
reliance introduces challenges related to reproducibility, subjectivity, and
limited integration of interface-level data. In particular, current approaches
lack the capacity to rigorously assess how human-machine interface design
contributes to operator performance variability and error susceptibility. To
address these limitations, this study proposes a framework for risk-informed
human failure event identification and interface-induced risk assessment driven
by AutoGraph (InSight-R). By linking empirical behavioral data to the
interface-embedded knowledge graph (IE-KG) constructed by the automated
graph-based execution framework (AutoGraph), the InSight-R framework enables
automated HFE identification based on both error-prone and time-deviated
operational paths. Furthermore, we discuss the relationship between
designer-user conflicts and human error. The results demonstrate that InSight-R
not only enhances the objectivity and interpretability of HFE identification
but also provides a scalable pathway toward dynamic, real-time human
reliability assessment in digitalized control environments. This framework
offers actionable insights for interface design optimization and contributes to
the advancement of mechanism-driven HRA methodologies.

</details>


### [235] [$σ$-Maximal Ancestral Graphs](https://arxiv.org/abs/2507.00093)
*Binghua Yao,Joris M. Mooij*

Main category: cs.DM

Relevance: 20.0

TL;DR: 该论文提出了σ-MAGs，扩展了MAGs以表示可能包含循环因果关系的图结构，解决了MAGs无法处理循环关系的限制。


<details>
  <summary>Details</summary>
Motivation: MAGs无法表示循环因果关系，限制了其在因果发现中的应用。该研究旨在填补这一空白。

Method: 引入并研究了σ-MAGs，分析了其性质，并提供了马尔可夫等价类的表征。

Result: σ-MAGs能够表示包含循环因果关系的图结构，扩展了MAGs的应用范围。

Conclusion: σ-MAGs为处理循环因果关系提供了一种新的抽象表示方法。

Abstract: Maximal Ancestral Graphs (MAGs) provide an abstract representation of
Directed Acyclic Graphs (DAGs) with latent (selection) variables. These
graphical objects encode information about ancestral relations and
d-separations of the DAGs they represent. This abstract representation has been
used amongst others to prove the soundness and completeness of the FCI
algorithm for causal discovery, and to derive a do-calculus for its output. One
significant inherent limitation of MAGs is that they rule out the possibility
of cyclic causal relationships. In this work, we address that limitation. We
introduce and study a class of graphical objects that we coin
''$\sigma$-Maximal Ancestral Graphs'' (''$\sigma$-MAGs''). We show how these
graphs provide an abstract representation of (possibly cyclic) Directed Graphs
(DGs) with latent (selection) variables, analogously to how MAGs represent
DAGs. We study the properties of these objects and provide a characterization
of their Markov equivalence classes.

</details>


### [236] [Discovering the underlying analytic structure within Standard Model constants using artificial intelligence](https://arxiv.org/abs/2507.00225)
*S. V. Chekanov,H. Kjellerstrand*

Main category: hep-ph

Relevance: 20.0

TL;DR: 论文使用符号回归和遗传编程寻找标准模型（SM）基本参数间的解析结构，发现了一些简单关系，精度优于1%。


<details>
  <summary>Details</summary>
Motivation: 探索标准模型基本参数间的隐藏模式，为模型构建和AI方法提供输入。

Method: 采用符号回归和遗传编程分析参数关系。

Result: 发现约一千个精度优于1%的简单解析关系。

Conclusion: 这些结果可能揭示更深层次的基本规律。

Abstract: This paper presents a search for underlying analytic structures among the
fundamental parameters of the Standard Model (SM) using symbolic regression and
genetic programming. We identify the simplest analytic relationships connecting
pairs of these constants and report several notable observations based on about
a thousand expressions with relative precision better than 1%. These results
may serve as valuable inputs for model builders and artificial intelligence
methods aimed at uncovering hidden patterns among the SM constants, or
potentially used as building blocks for a deeper underlying law that connects
all parameters of the SM through a small set of fundamental constants.

</details>


### [237] [Novel Pigeon-inspired 3D Obstacle Detection and Avoidance Maneuver for Multi-UAV Systems](https://arxiv.org/abs/2507.00443)
*Reza Ahmadvand,Sarah Safura Sharif,Yaser Mike Banad*

Main category: cs.RO

Relevance: 20.0

TL;DR: 该论文提出了一种受自然启发的多无人机系统碰撞避免编队控制方法，结合集中式与分布式控制策略，并在2D和3D环境中验证了其有效性。


<details>
  <summary>Details</summary>
Motivation: 城市环境中多无人机系统的需求增加，但面临静态和动态障碍物的挑战，因此需要一种有效的碰撞避免编队控制方法。

Method: 采用半分布式控制方法，结合概率性Lloyd算法进行集中式引导，分布式控制用于避障和碰撞避免，并扩展到3D空间。

Result: 在动态环境中（包括静态和移动障碍物）验证了方法的有效性。

Conclusion: 提出的框架在多无人机系统中实现了有效的碰撞避免编队控制，适用于复杂动态环境。

Abstract: Recent advances in multi-agent systems manipulation have demonstrated a
rising demand for the implementation of multi-UAV systems in urban areas, which
are always subjected to the presence of static and dynamic obstacles. Inspired
by the collective behavior of tilapia fish and pigeons, the focus of the
presented research is on the introduction of a nature-inspired collision-free
formation control for a multi-UAV system, considering the obstacle avoidance
maneuvers. The developed framework in this study utilizes a semi-distributed
control approach, in which, based on a probabilistic Lloyd's algorithm, a
centralized guidance algorithm works for optimal positioning of the UAVs, while
a distributed control approach has been used for the intervehicle collision and
obstacle avoidance. Further, the presented framework has been extended to the
3D space with a novel definition of 3D maneuvers. Finally, the presented
framework has been applied to multi-UAV systems in 2D and 3D scenarios, and the
obtained results demonstrated the validity of the presented method in dynamic
environments with stationary and moving obstacles.

</details>


### [238] [Novel Complex-Valued Hopfield Neural Networks with Phase and Magnitude Quantization](https://arxiv.org/abs/2507.00461)
*Garimella Ramamurthy,Marcos Eduardo Valle,Tata Jagannadha Swamy*

Main category: cs.NE

Relevance: 20.0

TL;DR: 论文提出了两种新型复值Hopfield神经网络（CvHNNs），通过相位和幅值量化增加状态数量，扩展了应用范围。


<details>
  <summary>Details</summary>
Motivation: 现有复值Hopfield神经网络的状态数量有限，限制了其应用潜力。

Method: 设计了两种CvHNNs：一种基于直角坐标表示，另一种基于极坐标表示，均采用天花板型激活函数进行相位和幅值量化。

Result: 提出的CvHNNs显著增加了状态数量，扩展了应用范围。

Conclusion: 通过相位和幅值量化，CvHNNs在复杂任务中具有更广泛的应用潜力。

Abstract: This research paper introduces two novel complex-valued Hopfield neural
networks (CvHNNs) that incorporate phase and magnitude quantization. The first
CvHNN employs a ceiling-type activation function that operates on the
rectangular coordinate representation of the complex net contribution. The
second CvHNN similarly incorporates phase and magnitude quantization but
utilizes a ceiling-type activation function based on the polar coordinate
representation of the complex net contribution. The proposed CvHNNs, with their
phase and magnitude quantization, significantly increase the number of states
compared to existing models in the literature, thereby expanding the range of
potential applications for CvHNNs.

</details>


### [239] [Physics-Informed Neural ODEs for Temporal Dynamics Modeling in Cardiac T1 Mapping](https://arxiv.org/abs/2507.00613)
*Nuno Capitão,Yi Zhang,Yidong Zhao,Qian Tao*

Main category: eess.IV

Relevance: 20.0

TL;DR: 提出了一种基于物理信息神经ODE的加速T1映射框架，用于心脏成像，减少扫描时间并提高准确性。


<details>
  <summary>Details</summary>
Motivation: 传统MOLLI方法扫描时间长，易受患者呼吸运动影响，且现有深度学习方法忽视物理约束，影响可解释性和泛化性。

Method: 利用物理信息神经ODE建模时间动态，开发连续时间LSTM-ODE模型，支持选择性Look-Locker数据采集。

Result: 实验显示在原生和对比后序列中T1估计性能优越，物理建模优于直接数据驱动方法。

Conclusion: 该方法通过结合物理约束，显著提升了T1映射的效率和准确性。

Abstract: Spin-lattice relaxation time ($T_1$) is an important biomarker in cardiac
parametric mapping for characterizing myocardial tissue and diagnosing
cardiomyopathies. Conventional Modified Look-Locker Inversion Recovery (MOLLI)
acquires 11 breath-hold baseline images with interleaved rest periods to ensure
mapping accuracy. However, prolonged scanning can be challenging for patients
with poor breathholds, often leading to motion artifacts that degrade image
quality. In addition, $T_1$ mapping requires voxel-wise nonlinear fitting to a
signal recovery model involving an iterative estimation process. Recent studies
have proposed deep-learning approaches for rapid $T_1$ mapping using shortened
sequences to reduce acquisition time for patient comfort. Nevertheless,
existing methods overlook important physics constraints, limiting
interpretability and generalization. In this work, we present an accelerated,
end-to-end $T_1$ mapping framework leveraging Physics-Informed Neural Ordinary
Differential Equations (ODEs) to model temporal dynamics and address these
challenges. Our method achieves high-accuracy $T_1$ estimation from a sparse
subset of baseline images and ensures efficient null index estimation at test
time. Specifically, we develop a continuous-time LSTM-ODE model to enable
selective Look-Locker (LL) data acquisition with arbitrary time lags.
Experimental results show superior performance in $T_1$ estimation for both
native and post-contrast sequences and demonstrate the strong benefit of our
physics-based formulation over direct data-driven $T_1$ priors.

</details>


### [240] [Constellation as a Service: Tailored Connectivity Management in Direct-Satellite-to-Device Networks](https://arxiv.org/abs/2507.00902)
*Feng Wang,Shengyu Zhang,Een-Kee Hong,Tony Q. S. Quek*

Main category: eess.SY

Relevance: 20.0

TL;DR: 论文提出了一种名为CaaS的框架，用于管理多星座卫星通信中的DS2D连接，通过动态子星座优化和生成式AI预测技术提升服务性能。


<details>
  <summary>Details</summary>
Motivation: 多星座卫星通信中存在高干扰和频繁切换问题，现有方法局限于单星座，无法充分利用多星座潜力。

Method: 提出CaaS框架，动态形成子星座，结合生成式AI预测和预配置切换路径。

Result: 仿真显示CaaS显著提升服务率并减少切换开销。

Conclusion: CaaS是管理多星座DS2D连接的高效可持续解决方案。

Abstract: Direct-satellite-to-device (DS2D) communication is emerging as a promising
solution for global mobile service extension, leveraging the deployment of
satellite constellations. However, the challenge of managing DS2D connectivity
for multi-constellations becomes outstanding, including high interference and
frequent handovers caused by multi-coverage overlap and rapid satellite
movement. Moreover, existing approaches primarily operate within
single-constellation shell, which inherently limits the ability to exploit the
vast potential of multi-constellation connectivity provision, resulting in
suboptimal DS2D service performances. To address these challenges, this article
proposes a Constellation as a Service (CaaS) framework, which treats the entire
multi-constellation infrastructure as a shared resource pool and dynamically
forms optimal sub-constellations (SCs) for each DS2D service region. The
formation of each SC integrates satellites from various orbits to provide
tailored connectivity based on user demands, guided by two innovative
strategies: predictive satellite beamforming using generative artificial
intelligence (GenAI) and pre-configured handover path for efficient satellite
access and mobility management. Simulation results demonstrate that CaaS
significantly improves satellite service rates while reducing handover
overhead, making it an efficient and continuable solution for managing DS2D
connectivity in multi-constellation environments.

</details>


### [241] [SurgiSR4K: A High-Resolution Endoscopic Video Dataset for Robotic-Assisted Minimally Invasive Procedures](https://arxiv.org/abs/2507.00209)
*Fengyi Jiang,Xiaorui Zhang,Lingbo Jin,Ruixing Liang,Yuxin Chen,Adi Chola Venkatesh,Jason Culman,Tiantian Wu,Lirong Shao,Wenqing Sun,Cong Gao,Hallie McNamara,Jingpei Lu,Omid Mohareri*

Main category: eess.IV

Relevance: 10.0

TL;DR: SurgiSR4K是首个公开的4K分辨率手术图像和视频数据集，专为机器人辅助微创手术设计，涵盖多种视觉挑战场景，支持多项计算机视觉任务。


<details>
  <summary>Details</summary>
Motivation: 尽管4K内窥镜系统逐渐普及，但缺乏公开的4K手术数据集，限制了高分辨率手术成像研究的进展。

Method: 提出SurgiSR4K数据集，包含机器人辅助手术中的真实场景（如镜面反射、工具遮挡等），用于高分辨率计算机视觉任务。

Result: 数据集支持超分辨率、烟雾去除、手术器械检测等任务，为高分辨率手术成像研究提供基础。

Conclusion: SurgiSR4K推动了高分辨率手术成像研究，有助于提升图像引导机器人手术的性能和安全性。

Abstract: High-resolution imaging is crucial for enhancing visual clarity and enabling
precise computer-assisted guidance in minimally invasive surgery (MIS). Despite
the increasing adoption of 4K endoscopic systems, there remains a significant
gap in publicly available native 4K datasets tailored specifically for
robotic-assisted MIS. We introduce SurgiSR4K, the first publicly accessible
surgical imaging and video dataset captured at a native 4K resolution,
representing realistic conditions of robotic-assisted procedures. SurgiSR4K
comprises diverse visual scenarios including specular reflections, tool
occlusions, bleeding, and soft tissue deformations, meticulously designed to
reflect common challenges faced during laparoscopic and robotic surgeries. This
dataset opens up possibilities for a broad range of computer vision tasks that
might benefit from high resolution data, such as super resolution (SR), smoke
removal, surgical instrument detection, 3D tissue reconstruction, monocular
depth estimation, instance segmentation, novel view synthesis, and
vision-language model (VLM) development. SurgiSR4K provides a robust foundation
for advancing research in high-resolution surgical imaging and fosters the
development of intelligent imaging technologies aimed at enhancing performance,
safety, and usability in image-guided robotic surgeries.

</details>


### [242] [MTCNet: Motion and Topology Consistency Guided Learning for Mitral Valve Segmentationin 4D Ultrasound](https://arxiv.org/abs/2507.00660)
*Rusi Chen,Yuanting Yang,Jiezhi Yao,Hongning Song,Ji Zhang,Yongsong Zhou,Yuhao Huang,Ronghao Yang,Dan Jia,Yuhan Zhang,Xing Tao,Haoran Dou,Qing Zhou,Xin Yang,Dong Ni*

Main category: eess.IV

Relevance: 10.0

TL;DR: 提出了一种用于半监督学习的运动-拓扑引导一致性网络（MTCNet），用于4D二尖瓣超声分割，解决了现有方法中缺乏相位间依赖性的问题。


<details>
  <summary>Details</summary>
Motivation: 4D二尖瓣分析因缺乏相位间依赖性和其他挑战（如运动伪影和成像质量差）而受限，需要一种新方法来提升分割准确性。

Method: 设计了跨相位运动引导一致性学习策略和拓扑引导相关性正则化，利用双向注意力记忆库传播时空特征。

Result: 在包含160名患者1408个相位的数据集上，MTCNet表现出色（Dice: 87.30%, HD: 1.75mm），优于其他先进方法。

Conclusion: MTCNet通过结合运动引导和拓扑引导策略，显著提升了4D二尖瓣分割的准确性和一致性。

Abstract: Mitral regurgitation is one of the most prevalent cardiac disorders.
Four-dimensional (4D) ultrasound has emerged as the primary imaging modality
for assessing dynamic valvular morphology. However, 4D mitral valve (MV)
analysis remains challenging due to limited phase annotations, severe motion
artifacts, and poor imaging quality. Yet, the absence of inter-phase dependency
in existing methods hinders 4D MV analysis. To bridge this gap, we propose a
Motion-Topology guided consistency network (MTCNet) for accurate 4D MV
ultrasound segmentation in semi-supervised learning (SSL). MTCNet requires only
sparse end-diastolic and end-systolic annotations. First, we design a
cross-phase motion-guided consistency learning strategy, utilizing a
bi-directional attention memory bank to propagate spatio-temporal features.
This enables MTCNet to achieve excellent performance both per- and inter-phase.
Second, we devise a novel topology-guided correlation regularization that
explores physical prior knowledge to maintain anatomically plausible.
Therefore, MTCNet can effectively leverage structural correspondence between
labeled and unlabeled phases. Extensive evaluations on the first largest 4D MV
dataset, with 1408 phases from 160 patients, show that MTCNet performs superior
cross-phase consistency compared to other advanced methods (Dice: 87.30%, HD:
1.75mm). Both the code and the dataset are available at
https://github.com/crs524/MTCNet.

</details>


<div id='cs.LG'></div>

# cs.LG [[Back]](#toc)

### [243] [LoRA-Mixer: Coordinate Modular LoRA Experts Through Serial Attention Routing](https://arxiv.org/abs/2507.00029)
*Wenbing Li,Zikai Song,Hang Zhou,Yunyao Zhang,Junqing Yu,Wei Yang*

Main category: cs.LG

Relevance: 90.0

TL;DR: LoRA-Mixer是一个轻量级的MoE框架，通过动态路由任务特定的LoRA专家，提升LLMs在多任务中的参数效率和任务保真度。


<details>
  <summary>Details</summary>
Motivation: 现有方法在结合LoRA与MoE时存在参数效率低和任务保真度不足的问题，需要更高效的解决方案。

Method: 提出LoRA-Mixer框架，替换注意力模块的线性层投影矩阵为动态路由的LoRA专家，支持联合优化或直接部署预训练模块。

Result: 在多个基准数据集上显著提升性能（如GSM8K提升7.61%），且参数效率更高（仅需48%参数）。

Conclusion: LoRA-Mixer在效率和性能上优于现有方法，适用于多种基础模型。

Abstract: Recent efforts to combine low-rank adaptation (LoRA) with mixture-of-experts
(MoE) for adapting large language models (LLMs) to multiple tasks still exhibit
prevailing limitations: they either swap entire attention/feed-forward layers
for switch experts or bolt on parallel expert branches, diluting parameter
efficiency and task fidelity. We propose the LoRA-Mixer, a modular and
lightweight MoE framework that integrates LoRA experts. Our core innovation
lies in replacing the projection matrices of the attention module's
input/output linear layers with dynamically routed, task-specific LoRA experts.
This design ensures seamless compatibility with diverse foundation models,
including transformers and state space models (SSMs), by leveraging their
inherent linear projection structures. The framework supports two operational
paradigms: (1) joint optimization of LoRA experts and routing mechanisms via a
novel hard-soft routing strategy, or (2) direct deployment of pre-trained,
frozen LoRA modules sourced from external repositories. To enable robust router
training with limited data while ensuring stable routing decisions and
maximizing expert reuse, we introduce an adaptive Specialization Balance Loss
(SBL) that jointly optimizes expert balance and task-specific alignment.
Extensive experiments on seven benchmark datasets, including MedQA, CoLA,
SST-2, GSM8K, ARC-E, ARC-C, and HumanEval, demonstrate the effectiveness of
LoRA-Mixer. On datasets such as GSM8K, HumanEval, and MedQA, LoRA-Mixer
achieves significant improvements of 7.61%, 4.88%, and 3.08% over the base
models, respectively. Compared with state-of-the-art methods, LoRA-Mixer
achieves additional improvements of 1.09%, 1.45%, and 1.68%, respectively,
using only 48% of the parameters, demonstrating its efficiency and strong
performance.

</details>


### [244] [ZeCO: Zero Communication Overhead Sequence Parallelism for Linear Attention](https://arxiv.org/abs/2507.01004)
*Yuhong Chou,Zehao Liu,Ruijie Zhu,Xinyi Wan,Tianjian Li,Congying Chu,Qian Liu,Jibin Wu,Zejun Ma*

Main category: cs.LG

Relevance: 90.0

TL;DR: ZeCO是一种新的序列并行方法，用于线性注意力模型，通过All-Scan通信原语消除通信开销，实现高效长序列训练。


<details>
  <summary>Details</summary>
Motivation: 现有序列并行方法在长序列训练中因通信开销成为瓶颈，限制了线性注意力模型的潜力。

Method: 提出ZeCO方法，利用All-Scan通信原语，为每个设备提供所需初始状态，最小化通信开销。

Result: 在256 GPU上处理8M序列时，ZeCO比当前最优方法快60%。

Conclusion: ZeCO为高效训练超长序列的下一代LLM提供了可行路径。

Abstract: Linear attention mechanisms deliver significant advantages for Large Language
Models (LLMs) by providing linear computational complexity, enabling efficient
processing of ultra-long sequences (e.g., 1M context). However, existing
Sequence Parallelism (SP) methods, essential for distributing these workloads
across devices, become the primary bottleneck due to substantial communication
overhead. In this paper, we introduce ZeCO (Zero Communication Overhead)
sequence parallelism for linear attention models, a new SP method designed to
overcome these limitations and achieve end-to-end near-linear scalability for
long sequence training. For example, training a model with a 1M sequence length
across 64 devices using ZeCO takes roughly the same time as training with an
16k sequence on a single device. At the heart of ZeCO lies All-Scan, a new
collective communication primitive. All-Scan provides each SP rank with
precisely the initial operator state it requires while maintaining a minimal
communication footprint, effectively eliminating communication overhead.
Theoretically, we prove the optimaity of ZeCO, showing that it introduces only
negligible time and space overhead. Empirically, we compare the communication
costs of different sequence parallelism strategies and demonstrate that
All-Scan achieves the fastest communication in SP scenarios. Specifically, on
256 GPUs with an 8M sequence length, ZeCO achieves a 60\% speedup compared to
the current state-of-the-art (SOTA) SP method. We believe ZeCO establishes a
clear path toward efficiently training next-generation LLMs on previously
intractable sequence lengths.

</details>


### [245] [Hypertokens: Holographic Associative Memory in Tokenized LLMs](https://arxiv.org/abs/2507.00002)
*Christopher James Augeri*

Main category: cs.LG

Relevance: 85.0

TL;DR: 论文提出HDRAM框架，通过信息论方法解决LLM中的信息扩散问题，结合经典纠错码、全息计算和量子启发搜索，显著提升关联检索能力。


<details>
  <summary>Details</summary>
Motivation: LLMs存在信息扩散问题，导致精度损失。研究将其重新定义为信息论通信问题，并提出解决方案。

Method: 引入HDRAM框架，利用超令牌（hypertokens）和相位相干内存地址，结合ECC、全息计算和量子启发搜索，实现高效键值操作和搜索。

Result: HDRAM显著提升了关联检索能力，且无需改变模型架构。

Conclusion: CHQ（经典-全息-量子启发）原则可增强Transformer架构。

Abstract: Large Language Models (LLMs) exhibit remarkable capabilities but suffer from
apparent precision loss, reframed here as information spreading. This reframing
shifts the problem from computational precision to an information-theoretic
communication issue. We address the K:V and V:K memory problem in LLMs by
introducing HDRAM (Holographically Defined Random Access Memory), a symbolic
memory framework treating transformer latent space as a spread-spectrum
channel. Built upon hypertokens, structured symbolic codes integrating
classical error-correcting codes (ECC), holographic computing, and
quantum-inspired search, HDRAM recovers distributed information through
principled despreading. These phase-coherent memory addresses enable efficient
key-value operations and Grover-style search in latent space. By combining ECC
grammar with compressed sensing and Krylov subspace alignment, HDRAM
significantly improves associative retrieval without architectural changes,
demonstrating how Classical-Holographic-Quantum-inspired (CHQ) principles can
fortify transformer architectures.

</details>


### [246] [A Theory of Inference Compute Scaling: Reasoning through Directed Stochastic Skill Search](https://arxiv.org/abs/2507.00004)
*Austin R. Ellis-Mohr,Anuj K. Nayak,Lav R. Varshney*

Main category: cs.LG

Relevance: 85.0

TL;DR: 论文提出了一种名为DS3的框架，用于优化大型语言模型（LLMs）的推理成本，通过随机技能图遍历实现高效推理，并分析了不同推理策略的效率和任务难度关系。


<details>
  <summary>Details</summary>
Motivation: LLMs在训练和推理阶段消耗大量资源，现有方法未能全面优化推理成本。DS3框架旨在填补这一空白，提供更高效的推理策略。

Method: 提出DS3框架，通过随机技能图遍历建模推理过程，并推导任务成功率和计算成本的闭式表达式，分析不同推理策略（如CoT和ToT）的效率。

Result: 理论分析揭示了线性精度与对数计算的关系、任务难度与模型能力对推理策略的影响，以及推理行为在参数扩展时的涌现现象。

Conclusion: DS3框架通过明确训练与推理的相互依赖关系，为算法设计和资源分配提供了理论基础。

Abstract: Large language models (LLMs) demand considerable computational, energy, and
financial resources during both training and deployment. While scaling laws for
training have guided much of the field's recent progress, inference costs now
represent a significant and growing component of the overall resource burden,
particularly for reasoning-focused models. Existing characterizations of
compute-optimality that consider model size, dataset size, and inference tokens
in isolation or in fixed combinations risk overlooking more efficient operating
points. We introduce directed stochastic skill search (DS3), a general
framework that represents inference as stochastic traversal over a learned
skill graph. From a simplified yet expressive instantiation, we derive
closed-form expressions for task success and compute cost across a wide range
of inference strategies -- including chain-of-thought (CoT) and tree-of-thought
(ToT) -- enabling comparative analysis as a function of task difficulty and
model capability. To that end, we extend a prior first-principles tripartite
graph framework of LLM training to incorporate inference, and separately bridge
DS3 with empirical methods that characterize LLM scaling behavior. We
theoretically recover empirically observed patterns, including: linear accuracy
scaling with logarithmic compute; variation in preferred inference strategies
as a function of task difficulty and model capability; emergent behavior
elicited by reasoning even when performance plateaus under parameter scaling;
and both best-of-N (BoN) and majority voting behavior captured within a unified
analytical framework. By explicitly characterizing training-inference
interdependencies, our framework deepens theoretical understanding and supports
principled algorithmic design and resource allocation.

</details>


### [247] [SWE-Bench-CL: Continual Learning for Coding Agents](https://arxiv.org/abs/2507.00014)
*Thomas Joshi,Shayan Chowdhury,Fatih Uysal*

Main category: cs.LG

Relevance: 85.0

TL;DR: SWE-Bench-CL是一个基于SWE-Bench Verified数据集的持续学习基准，用于评估LLM在动态软件工程环境中的适应性和知识迁移能力。


<details>
  <summary>Details</summary>
Motivation: 现有LLM在静态代码生成任务中表现优异，但缺乏对动态软件工程场景（如持续演化的GitHub问题）的适应能力。

Method: 构建了SWE-Bench-CL数据集，包含按时间顺序排列的GitHub问题；开发了基于LangGraph的交互式评估框架和FAISS语义记忆模块；提出了一套持续学习指标。

Result: 提供了可复现的实验平台和工具，支持评估LLM在动态环境中的表现。

Conclusion: SWE-Bench-CL为开发更适应动态软件工程的AI代理提供了基准和工具。

Abstract: Large Language Models (LLMs) have achieved impressive results on static
code-generation benchmarks, but real-world software development unfolds as a
continuous stream of evolving issues, fixes, and feature requests. We introduce
SWE-Bench-CL, a novel continual learning benchmark built on the human-verified
SWE-Bench Verified dataset introduced by OpenAI and Princeton-NLP in 2024. By
organizing GitHub issues into chronologically ordered sequences that reflect
natural repository evolution, SWE-Bench-CL enables direct evaluation of an
agent's ability to accumulate experience, transfer knowledge across tasks, and
resist catastrophic forgetting. We complement the dataset with (i) a
preliminary analysis of inter-task structural similarity and contextual
sensitivity, (ii) an interactive LangGraph-based evaluation framework augmented
with a FAISS-backed semantic memory module, and (iii) a suite of specialized
continual learning metrics -- including average accuracy, forgetting,
forward/backward transfer, tool-use efficiency, and a generalized Composite
Continual Learning Score and CL-F-beta score -- to capture the
stability-plasticity trade-off. We outline a rigorous experimental protocol
comparing memory-enabled and memory-disabled agents across diverse Python
repositories. All code and data are publicly available at
https://github.com/thomasjoshi/agents-never-forget, providing the community
with a reproducible platform for developing more adaptive and robust AI agents
in software engineering.

</details>


### [248] [Gradient-based Fine-Tuning through Pre-trained Model Regularization](https://arxiv.org/abs/2507.00016)
*Xuanbo Liu,Liu Liu,Fuxiang Wu,Fusheng Hao,Xianglong Liu*

Main category: cs.LG

Relevance: 85.0

TL;DR: 提出了一种高效的基于梯度和正则化的微调方法（GRFT），通过更新权重矩阵的行或列来减少存储开销并提高参数选择效率，性能优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 预训练模型微调需要大量计算资源和存储，现有方法如GPS虽减少训练参数但增加资源需求。

Method: GRFT方法选择梯度平方和最高的行或列进行更新，并加入正则化以增强知识迁移。

Result: 在FGVC和VTAB数据集上仅需更新1.22%和0.30%的参数，性能优于GPS、Adapter Tuning和LoRA。

Conclusion: GRFT在高效性和性能上均表现优异，适用于大规模预训练模型的微调。

Abstract: Large pre-trained models have demonstrated extensive applications across
various fields. However, fine-tuning these models for specific downstream tasks
demands significant computational resources and storage. One fine-tuning
method, gradient-based parameter selection (GPS), focuses on fine-tuning only
the parameters with high gradients in each neuron, thereby reducing the number
of training parameters. Nevertheless, this approach increases computational
resource requirements and storage demands. In this paper, we propose an
efficient gradient-based and regularized fine-tuning method (GRFT) that updates
the rows or columns of the weight matrix. We theoretically demonstrate that the
rows or columns with the highest sum of squared gradients are optimal for
updating. This strategy effectively reduces storage overhead and improves the
efficiency of parameter selection. Additionally, we incorporate regularization
to enhance knowledge transfer from the pre-trained model. GRFT achieves
state-of-the-art performance, surpassing existing methods such as GPS, Adapter
Tuning, and LoRA. Notably, GRFT requires updating only 1.22% and 0.30% of the
total parameters on FGVC and VTAB datasets, respectively, demonstrating its
high efficiency and effectiveness. The source code will be released soon.

</details>


### [249] [Implicit Reward as the Bridge: A Unified View of SFT and DPO Connections](https://arxiv.org/abs/2507.00018)
*Bo Wang,Qinyuan Cheng,Runyu Peng,Rong Bao,Peiji Li,Qipeng Guo,Linyang Li,Zhiyuan Zeng,Yunhua Zhou,Xipeng Qiu*

Main category: cs.LG

Relevance: 85.0

TL;DR: 论文提出了一个统一的理论框架，将监督微调（SFT）和偏好学习（如DPO）联系起来，揭示了SFT的局限性并提出改进方法。


<details>
  <summary>Details</summary>
Motivation: 研究动机是解决SFT在LLM后训练中的局限性，尤其是KL散度项在优化过程中失效的问题。

Method: 通过数学推导，证明SFT和偏好学习在同一最优策略-奖励子空间中运行，并提出学习率降低方法和基于f-散度的替代目标。

Result: 实验显示改进方法在指令跟随任务中带来25%的相对增益和6%的绝对胜率提升。

Conclusion: 论文扩展了LLM logits与Q函数的关系，为SFT和偏好学习提供了理论支持。

Abstract: Post-training processes are essential phases in grounding pre-trained
language models to real-world tasks, with learning from demonstrations or
preference signals playing a crucial role in this adaptation. We present a
unified theoretical framework bridging Supervised Fine-Tuning (SFT) and
preference learning in Large Language Model (LLM) post-training. Through
rigorous mathematical derivation, we demonstrate that both SFT and preference
learning methods like Direct Preference Optimization (DPO) operate within the
same optimal policy-reward subspace, with SFT representing a special case of
implicit reward learning. Our analysis reveals a critical limitation in
conventional SFT: the KL divergence term in distribution matching becomes
constant with respect to the policy during optimization, failing to constrain
model updates. To address this, we propose a simple yet effective learning rate
reduction approach that yields significant performance improvements (up to
\textbf{25\%} relative gain and \textbf{6\%} absolute win rate increase in
instruction following tasks. Additionally, we derive alternative SFT objectives
from various f-divergence functions that preserve the KL term during
optimization, further enhancing post-DPO model performance. Finally, we extend
the theoretical relationship between LLM logits and Q-functions from preference
learning to the SFT context, providing mathematical derivations and
experimental validation.

</details>


### [250] [ROSE: Toward Reality-Oriented Safety Evaluation of Large Language Models](https://arxiv.org/abs/2507.00026)
*Jiale Ding,Xiang Zheng,Cong Wang,Wei-Bin Lee,Xingjun Ma,Yu-Gang Jiang*

Main category: cs.LG

Relevance: 85.0

TL;DR: 论文提出ROSE框架，通过多目标强化学习生成多样化和情境丰富的对抗性提示，以更全面地评估LLM的安全性。


<details>
  <summary>Details</summary>
Motivation: 随着LLM在现实应用中的广泛部署，评估其安全性（尤其是对抗性提示下的表现）变得至关重要。现有手动基准的静态性和更新成本高，难以跟上LLM的快速发展。

Method: 提出ROSE框架，利用多目标强化学习微调对抗性LLM，生成多样化和情境丰富的对抗性提示。

Result: 实验表明，ROSE在发现LLM安全漏洞方面优于现有方法，综合评估指标显著提升。

Conclusion: ROSE为更实用和现实导向的LLM安全评估迈出了一步。

Abstract: As Large Language Models (LLMs) are increasingly deployed as black-box
components in real-world applications, evaluating their safety-especially under
adversarial prompting-has become critical. Arguably, effective safety
evaluations should be adaptive, evolving with LLM capabilities, and also cover
a broad spectrum of harmful topics and real-world scenarios to fully expose
potential vulnerabilities. Existing manual safety benchmarks, built on
handcrafted adversarial prompts, are limited by their static nature and the
intensive labor required to update them, making it difficult to keep pace with
rapidly advancing LLMs. In contrast, automated adversarial prompt generation
offers a promising path toward adaptive evaluation. However, current methods
often suffer from insufficient adversarial topic coverage (topic-level
diversity) and weak alignment with real-world contexts. These shortcomings stem
from the exploration-exploitation dilemma in black-box optimization and a lack
of real-world contextualization, resulting in adversarial prompts that are both
topically narrow and scenario-repetitive. To address these issues, we propose
Reality-Oriented Safety Evaluation (ROSE), a novel framework that uses
multi-objective reinforcement learning to fine-tune an adversarial LLM for
generating topically diverse and contextually rich adversarial prompts.
Experiments show that ROSE outperforms existing methods in uncovering safety
vulnerabilities in state-of-the-art LLMs, with notable improvements in
integrated evaluation metrics. We hope ROSE represents a step toward more
practical and reality-oriented safety evaluation of LLMs. WARNING: This paper
contains examples of potentially harmful text.

</details>


### [251] [Theoretical Modeling of LLM Self-Improvement Training Dynamics Through Solver-Verifier Gap](https://arxiv.org/abs/2507.00075)
*Yifan Sun,Yushan Liang,Zhen Zhang,Jiaye Teng*

Main category: cs.LG

Relevance: 85.0

TL;DR: 本文通过理论建模和实证验证，探讨了LLM自改进过程中的性能演化，提出了求解器-验证器间隙的概念，并预测了自改进的最终效果。


<details>
  <summary>Details</summary>
Motivation: 研究LLM自改进过程中性能演化的理论机制，填补这一领域的空白。

Method: 通过理论建模求解器-验证器间隙，并基于早期训练信息预测自改进效果。

Result: 理论模型在多种LLM和数据集上得到验证，并发现有限外部数据对最终性能影响较小。

Conclusion: 自改进性能演化可通过求解器-验证器间隙理论解释，且外部数据在有限条件下不影响最终效果。

Abstract: Self-improvement is among the most prominent techniques within the realm of
large language models (LLM), aiming to enhance the LLM performance without
relying on external data. Despite its significance, generally how LLM
performances evolve during the self-improvement process remains underexplored.
In this paper, we theoretically model the training dynamics of self-improvement
via the concept of solver-verifier gap. This is inspired by the conjecture that
the performance enhancement of self-improvement stems from the gap between
LLM's solver capability and verifier capability. Based on the theoretical
framework, we further introduce how to predict the ultimate power of
self-improvement using only information from the first few training epochs. We
empirically validate the effectiveness of the theoretical model on various LLMs
and datasets. Beyond self-improvement, we extend our analysis to investigate
how external data influences these dynamics within the framework. Notably, we
find that under limited external data regimes, such external data can be
utilized at any stage without significantly affecting final performances, which
accords with the empirical observations.

</details>


### [252] [Federated Learning-Enabled Hybrid Language Models for Communication-Efficient Token Transmission](https://arxiv.org/abs/2507.00082)
*Faranaksadat Solat,Joohyung Lee,Mohamed Seif,Dusit Niyato,H. Vincent Poor*

Main category: cs.LG

Relevance: 85.0

TL;DR: FedHLM是一种通信高效的混合语言模型框架，结合了不确定性感知推理和联邦学习，显著减少了LLM的通信开销。


<details>
  <summary>Details</summary>
Motivation: 解决传统混合语言模型在带宽受限环境下频繁调用LLM导致的通信开销问题。

Method: 通过联邦学习动态优化不确定性阈值，并利用P2P语义相似性减少LLM调用。

Result: 在大规模新闻分类任务中，FedHLM减少了95%以上的LLM传输，且精度损失可忽略。

Conclusion: FedHLM适用于高效、可扩展的边缘AI应用。

Abstract: Hybrid Language Models (HLMs) combine the low-latency efficiency of Small
Language Models (SLMs) on edge devices with the high accuracy of Large Language
Models (LLMs) on centralized servers. Unlike traditional end-to-end LLM
inference, HLMs reduce latency and communication by invoking LLMs only when
local SLM predictions are uncertain, i.e., when token-level confidence is low
or entropy is high. However, ambiguous or low-confidence predictions still
require frequent offloading to the LLM, leading to significant communication
overhead in bandwidth-constrained settings. To address this, we propose FedHLM,
a communication-efficient HLM framework that integrates uncertainty-aware
inference with Federated Learning (FL). FedHLM's key innovation lies in
collaboratively learning token-level uncertainty thresholds that govern when
LLM assistance is needed. Rather than using static or manually tuned
thresholds, FedHLM employs FL to optimize these thresholds in a
privacy-preserving, distributed manner. Additionally, it leverages
embedding-based token representations for Peer-to-Peer (P2P) resolution,
enabling clients to reuse tokens inferred by semantically similar peers without
engaging the LLM. We further introduce hierarchical model aggregation: edge
servers refine local routing policies through client updates, while
cross-cluster coordination aligns global decision boundaries. This layered
design captures recurring uncertainty patterns, reducing redundant LLM queries.
Experiments on large-scale news classification tasks show that FedHLM reduces
LLM transmissions by over 95 percent with negligible accuracy loss, making it
well-suited for scalable and efficient edge-AI applications.

</details>


### [253] [Double Q-learning for Value-based Deep Reinforcement Learning, Revisited](https://arxiv.org/abs/2507.00275)
*Prabhat Nagarajan,Martha White,Marlos C. Machado*

Main category: cs.LG

Relevance: 85.0

TL;DR: 论文研究了深度双Q学习（DDQL）在减少强化学习中的高估问题上的表现，并证明其优于双DQN。


<details>
  <summary>Details</summary>
Motivation: 解决深度强化学习中Q-learning普遍存在的高估问题，尤其是双DQN未能完全实现双Q学习的核心思想。

Method: 提出深度双Q学习（DDQL），通过训练两个Q函数并相互引导来减少高估，并在57款Atari 2600游戏上进行实验。

Result: DDQL显著减少了高估现象，并在性能上优于双DQN，且无需额外超参数调整。

Conclusion: DDQL是一种有效的深度强化学习算法，能够减少高估并提升性能。

Abstract: Overestimation is pervasive in reinforcement learning (RL), including in
Q-learning, which forms the algorithmic basis for many value-based deep RL
algorithms. Double Q-learning is an algorithm introduced to address
Q-learning's overestimation by training two Q-functions and using both to
de-correlate action-selection and action-evaluation in bootstrap targets.
Shortly after Q-learning was adapted to deep RL in the form of deep Q-networks
(DQN), Double Q-learning was adapted to deep RL in the form of Double DQN.
However, Double DQN only loosely adapts Double Q-learning, forgoing the
training of two different Q-functions that bootstrap off one another. In this
paper, we study algorithms that adapt this core idea of Double Q-learning for
value-based deep RL. We term such algorithms Deep Double Q-learning (DDQL). Our
aim is to understand whether DDQL exhibits less overestimation than Double DQN
and whether performant instantiations of DDQL exist. We answer both questions
affirmatively, demonstrating that DDQL reduces overestimation and outperforms
Double DQN in aggregate across 57 Atari 2600 games, without requiring
additional hyperparameters. We also study several aspects of DDQL, including
its network architecture, replay ratio, and minibatch sampling strategy.

</details>


### [254] [Open-ended Scientific Discovery via Bayesian Surprise](https://arxiv.org/abs/2507.00310)
*Dhruv Agarwal,Bodhisattwa Prasad Majumder,Reece Adamson,Megha Chakravorty,Satvika Reddy Gavireddy,Aditya Parashar,Harshit Surana,Bhavana Dalvi Mishra,Andrew McCallum,Ashish Sabharwal,Peter Clark*

Main category: cs.LG

Relevance: 85.0

TL;DR: AutoDS利用贝叶斯惊喜驱动开放自主科学发现，通过蒙特卡洛树搜索策略在固定预算下显著提升发现数量。


<details>
  <summary>Details</summary>
Motivation: 加速科学发现需要AI自主探索，而非依赖人类指定问题。现有方法在假设空间导航和兴趣度定义上存在不足。

Method: 结合贝叶斯惊喜量化LLM信念变化，采用蒙特卡洛树搜索策略（MCTS）探索嵌套假设空间。

Result: 在21个真实数据集上，AutoDS比竞争对手多产生5-29%的惊喜发现，且三分之二被专家认可。

Conclusion: AutoDS是开放自主科学发现的重要进展。

Abstract: The promise of autonomous scientific discovery (ASD) hinges not only on
answering questions, but also on knowing which questions to ask. Most recent
works in ASD explore the use of large language models (LLMs) in goal-driven
settings, relying on human-specified research questions to guide hypothesis
generation. However, scientific discovery may be accelerated further by
allowing the AI system to drive exploration by its own criteria. The few
existing approaches in open-ended ASD select hypotheses based on diversity
heuristics or subjective proxies for human interestingness, but the former
struggles to meaningfully navigate the typically vast hypothesis space, and the
latter suffers from imprecise definitions. This paper presents AutoDS -- a
method for open-ended ASD that instead drives scientific exploration using
Bayesian surprise. Here, we quantify the epistemic shift from the LLM's prior
beliefs about a hypothesis to its posterior beliefs after gathering
experimental results. To efficiently explore the space of nested hypotheses,
our method employs a Monte Carlo tree search (MCTS) strategy with progressive
widening using surprisal as the reward function. We evaluate AutoDS in the
setting of data-driven discovery across 21 real-world datasets spanning domains
such as biology, economics, finance, and behavioral science. Our results
demonstrate that under a fixed budget, AutoDS substantially outperforms
competitors by producing 5--29\% more discoveries deemed surprising by the LLM.
Our human evaluation further finds that two-thirds of AutoDS discoveries are
surprising to the domain experts, suggesting this is an important step forward
towards building open-ended ASD systems.

</details>


### [255] [MoNE: Replacing Redundant Experts with Lightweight Novices for Structured Pruning of MoE](https://arxiv.org/abs/2507.00390)
*Geng Zhang,Yuxuan Han,Yuxuan Lou,Wangbo Zhao,Yiqi Zhang,Yang You*

Main category: cs.LG

Relevance: 85.0

TL;DR: 提出了一种名为MoNE的新方法，通过用轻量级novices替换冗余experts来压缩MoE模型，减少内存开销，同时保持性能。


<details>
  <summary>Details</summary>
Motivation: MoE模型在内存开销上存在显著问题，现有剪枝方法在性能和不稳定性上表现不佳。

Method: MoNE通过评估experts的使用频率和输出方差，识别并替换冗余experts为轻量级novices。

Result: 在多种剪枝比例下，MoNE优于基线方法，性能下降最小，下游任务准确率提升显著。

Conclusion: MoNE是一种有效且鲁棒的MoE模型压缩方法。

Abstract: Mixture-of-Experts (MoE) enables efficient scaling of large language models
by activating only a subset of experts per input token. However, deploying
MoE-based models incurs significant memory overhead due to the need to retain
all experts in memory. While structured pruning is promising to reduce memory
costs, existing methods often show suboptimal performance and unstable
degradation in three dimensions: model architectures, calibration data sources,
and calibration sample sizes. This paper proposes
Mixture-of-Novices-and-Experts (MoNE), a novel expert pruning method that
replaces redundant experts with lightweight novices to achieve effective and
robust model compression. MoNE evaluates expert redundancy based on two
metrics: access frequency and output variance. Experts exhibiting low usage and
stable outputs are pruned and replaced with lightweight novices-unbiased
estimations of their original outputs-minimizing performance degradation.
Extensive experiments demonstrate that MoNE consistently outperforms baseline
methods with minimal accuracy degradation across the three dimensions,
confirming its effectiveness and robustness. Notably, it improves the average
zero shot accuracy across nine downstream tasks by up to 2.71 under 25\%
pruning ratio and 3.61 under 50\% pruning. The code is available at
https://github.com/zxgx/mode-pd.

</details>


### [256] [Flexible Language Modeling in Continuous Space with Transformer-based Autoregressive Flows](https://arxiv.org/abs/2507.00425)
*Ruixiang Zhang,Shuangfei Zhai,Jiatao Gu,Yizhe Zhang,Huangjie Zheng,Tianrong Chen,Miguel Angel Bautista,Josh Susskind,Navdeep Jaitly*

Main category: cs.LG

Relevance: 85.0

TL;DR: TarFlowLM proposes a continuous latent space language modeling framework using transformer-based autoregressive normalizing flows, enabling bi-directional context and flexible generation.


<details>
  <summary>Details</summary>
Motivation: To explore beyond the limitations of discrete token-based autoregressive models by leveraging continuous latent spaces for greater flexibility.

Method: Uses transformer-based autoregressive normalizing flows to model continuous representations, with new mixture-based coupling transformations for complex dependencies.

Result: Demonstrates strong likelihood performance on benchmarks and flexible modeling capabilities.

Conclusion: TarFlowLM offers a promising alternative to discrete autoregressive models with enhanced flexibility and performance.

Abstract: Autoregressive models have driven remarkable progress in language modeling.
Their foundational reliance on discrete tokens, unidirectional context, and
single-pass decoding, while central to their success, also inspires the
exploration of a design space that could offer new axes of modeling
flexibility. In this work, we explore an alternative paradigm, shifting
language modeling from a discrete token space to a continuous latent space. We
propose a novel framework TarFlowLM, that employs transformer-based
autoregressive normalizing flows to model these continuous representations.
This approach unlocks substantial flexibility, enabling the construction of
models that can capture global bi-directional context through stacked,
alternating-direction autoregressive transformations, support block-wise
generation with flexible token patch sizes, and facilitate a hierarchical
multi-pass generation process. We further propose new mixture-based coupling
transformations designed to capture complex dependencies within the latent
space shaped by discrete data, and demonstrate theoretical connections to
conventional discrete autoregressive models. Extensive experiments on language
modeling benchmarks demonstrate strong likelihood performance and highlight the
flexible modeling capabilities inherent in our framework.

</details>


### [257] [HelixPipe: Efficient Distributed Training of Long Sequence Transformers with Attention Parallel Pipeline Parallelism](https://arxiv.org/abs/2507.00394)
*Geng Zhang,Shenggan Cheng,Xuanlei Zhao,Ziming Liu,Yang You*

Main category: cs.LG

Relevance: 85.0

TL;DR: HelixPipe是一种新型的流水线并行方法，用于长序列Transformer训练，通过注意力并行分区和优化的微批次调度，显著提升了性能和可扩展性。


<details>
  <summary>Details</summary>
Motivation: 随着Transformer序列长度的增加，现有流水线并行方法因二次注意力计算和内存开销导致性能不佳。HelixPipe旨在解决这些问题。

Method: HelixPipe采用注意力并行分区和两阶段先进后出的微批次调度，结合无注意力重计算和分块MLP，以减少流水线气泡和内存碎片。

Result: 实验表明，HelixPipe在长序列训练中表现优异，吞吐量和可扩展性优于现有方法，例如在64个H100 GPU上训练7B模型时速度提升26%。

Conclusion: HelixPipe为长序列Transformer训练提供了一种高效的流水线并行解决方案，显著提升了性能和资源利用率。

Abstract: As transformer sequence lengths grow, existing pipeline parallelisms incur
suboptimal performance due to the quadratic attention computation and the
substantial memory overhead. To relieve these challenges, we propose HelixPipe,
a novel pipeline parallelism for long sequence transformer training. First,
HelixPipe introduces attention parallel partition, which schedules attention
computations of different micro batches across different pipeline stages in
parallel, reducing pipeline bubbles. Second, it employs a two-fold
first-in-last-out micro batch schedule to balance memory usage and overlap
communication with computation. Additionally, HelixPipe utilizes recomputation
without attention and chunked MLP to mitigate fragmentation and enable longer
sequences. Experiments demonstrate that HelixPipe gains increasing advantages
with longer sequence lengths, and outperforms existing methods in throughput
and scalability across varying pipeline sizes, model sizes, and cluster
configurations. Notably, it achieves a 26\% speedup over baseline methods when
training a 7B model with 128k sequence length on 64 H20 GPUs. Code is available
at https://github.com/code-tunnel/Megatron-LM/tree/dev.

</details>


### [258] [Overcoming Long-Context Limitations of State-Space Models via Context-Dependent Sparse Attention](https://arxiv.org/abs/2507.00449)
*Zhihao Zhan,Jianan Zhao,Zhaocheng Zhu,Jian Tang*

Main category: cs.LG

Relevance: 85.0

TL;DR: 论文提出了一种改进状态空间模型（SSMs）长上下文建模能力的方法，通过引入联合召回任务和上下文依赖稀疏注意力（CDSA），解决了SSMs在多查询联合召回任务中的表达力不足问题。


<details>
  <summary>Details</summary>
Motivation: 长上下文建模是NLP的关键挑战，现有SSMs在捕捉长距离依赖时表现不佳，且合成任务（如关联召回）无法充分反映现实复杂性。

Method: 提出联合召回任务，证明SSMs在多查询联合召回任务中的局限性，并提出基于CDSA的解决方案，进一步设计了HAX方法以适配自然语言领域。

Result: 实验表明，HAX在合成和真实长上下文基准测试中均优于SSM基线和CISA方法。

Conclusion: 通过理论分析和实践验证，HAX有效提升了SSMs的长上下文建模能力。

Abstract: Efficient long-context modeling remains a critical challenge for natural
language processing (NLP), as the time complexity of the predominant
Transformer architecture scales quadratically with the sequence length. While
state-space models (SSMs) offer alternative sub-quadratic solutions, they
struggle to capture long-range dependencies effectively. In this work, we focus
on analyzing and improving the long-context modeling capabilities of SSMs. We
show that the widely used synthetic task, associative recall, which requires a
model to recall a value associated with a single key without context,
insufficiently represents the complexities of real-world long-context modeling.
To address this limitation, we extend the associative recall to a novel
synthetic task, \emph{joint recall}, which requires a model to recall the value
associated with a key given in a specified context. Theoretically, we prove
that SSMs do not have the expressiveness to solve multi-query joint recall in
sub-quadratic time complexity. To resolve this issue, we propose a solution
based on integrating SSMs with Context-Dependent Sparse Attention (CDSA), which
has the expressiveness to solve multi-query joint recall with sub-quadratic
computation. To bridge the gap between theoretical analysis and real-world
applications, we propose locality-sensitive Hashing Attention with sparse Key
Selection (HAX), which instantiates the theoretical solution and is further
tailored to natural language domains. Extensive experiments on both synthetic
and real-world long-context benchmarks show that HAX consistently outperforms
SSM baselines and SSMs integrated with context-independent sparse attention
(CISA).

</details>


### [259] [Recurrent Memory-Augmented Transformers with Chunked Attention for Long-Context Language Modeling](https://arxiv.org/abs/2507.00453)
*Ankit Kashyap*

Main category: cs.LG

Relevance: 85.0

TL;DR: 提出一种结合全局注意力与局部注意力及门控FIFO内存的Transformer架构，高效处理长上下文语言建模。


<details>
  <summary>Details</summary>
Motivation: 解决传统Transformer在长上下文建模中注意力成本二次增长的问题，同时兼顾短程和长程依赖。

Method: 结合全局注意力、分块局部注意力和门控FIFO内存机制，使用旋转位置编码。

Result: 模型在对话建模、代码补全和文档理解等任务中表现轻量且可扩展。

Conclusion: 该架构为长上下文语言建模提供了一种高效且灵活的解决方案。

Abstract: We present a Transformer architecture for long-context language modeling that
combines global attention with two biologically inspired components: chunked
local attention and a gated FIFO memory mechanism. This unified attention block
allows the model to efficiently handle both short-range and long-range
dependencies without increasing attention cost quadratically. The memory module
persistently stores past token representations using a gated update mechanism
inspired by recurrent networks. Rotary positional encoding is applied per
attention head to enable directionally disentangled, scale-invariant positional
signals. The architecture is implemented entirely from scratch in PyTorch, with
no reliance on high-level libraries, enabling transparent and modular
experimentation. Our model offers a lightweight and extensible design for tasks
such as dialogue modeling, code completion, and document understanding.

</details>


### [260] [PNAct: Crafting Backdoor Attacks in Safe Reinforcement Learning](https://arxiv.org/abs/2507.00485)
*Weiran Guo,Guanjun Liu,Ziyuan Zhou,Ling Wang*

Main category: cs.LG

Relevance: 85.0

TL;DR: 本文提出了一种针对安全强化学习（Safe RL）的后门攻击框架PNAct，通过正负动作样本植入后门，揭示了Safe RL的潜在风险。


<details>
  <summary>Details</summary>
Motivation: Safe RL在确保安全约束的同时可能受到后门攻击的威胁，本文旨在填补这一研究空白。

Method: 提出PNAct框架，结合正负动作样本植入后门，并设计攻击算法。

Result: 实验验证了PNAct攻击框架的有效性，揭示了Safe RL的脆弱性。

Conclusion: Safe RL存在后门攻击风险，PNAct框架为相关研究提供了新视角。

Abstract: Reinforcement Learning (RL) is widely used in tasks where agents interact
with an environment to maximize rewards. Building on this foundation, Safe
Reinforcement Learning (Safe RL) incorporates a cost metric alongside the
reward metric, ensuring that agents adhere to safety constraints during
decision-making. In this paper, we identify that Safe RL is vulnerable to
backdoor attacks, which can manipulate agents into performing unsafe actions.
First, we introduce the relevant concepts and evaluation metrics for backdoor
attacks in Safe RL. It is the first attack framework in the Safe RL field that
involves both Positive and Negative Action sample (PNAct) is to implant
backdoors, where positive action samples provide reference actions and negative
action samples indicate actions to be avoided. We theoretically point out the
properties of PNAct and design an attack algorithm. Finally, we conduct
experiments to evaluate the effectiveness of our proposed backdoor attack
framework, evaluating it with the established metrics. This paper highlights
the potential risks associated with Safe RL and underscores the feasibility of
such attacks. Our code and supplementary material are available at
https://github.com/azure-123/PNAct.

</details>


### [261] [Residual Reward Models for Preference-based Reinforcement Learning](https://arxiv.org/abs/2507.00611)
*Chenyang Cao,Miguel Rogel-García,Mohamed Nabail,Xueqian Wang,Nicholas Rhinehart*

Main category: cs.LG

Relevance: 85.0

TL;DR: 提出了一种基于残差奖励模型（RRM）的偏好强化学习方法，通过结合先验奖励和学习奖励，显著提升了性能和学习速度。


<details>
  <summary>Details</summary>
Motivation: 解决偏好强化学习（PbRL）中因奖励模型训练导致的收敛速度慢问题，同时避免预训练和微调中不同损失函数带来的优化挑战。

Method: 设计残差奖励模型（RRM），将环境奖励分为先验奖励和学习奖励两部分，先验奖励来自用户猜测或逆强化学习，学习奖励通过偏好训练。

Result: 在Meta-World环境中的多个任务上验证了RRM的有效性，显著提升了基线方法的性能，并在真实机器人（Franka Panda）上实现了更快的学习速度。

Conclusion: RRM方法通过有效利用先验知识，显著提升了偏好强化学习的性能和效率。

Abstract: Preference-based Reinforcement Learning (PbRL) provides a way to learn
high-performance policies in environments where the reward signal is hard to
specify, avoiding heuristic and time-consuming reward design. However, PbRL can
suffer from slow convergence speed since it requires training in a reward
model. Prior work has proposed learning a reward model from demonstrations and
fine-tuning it using preferences. However, when the model is a neural network,
using different loss functions for pre-training and fine-tuning can pose
challenges to reliable optimization. In this paper, we propose a method to
effectively leverage prior knowledge with a Residual Reward Model (RRM). An RRM
assumes that the true reward of the environment can be split into a sum of two
parts: a prior reward and a learned reward. The prior reward is a term
available before training, for example, a user's ``best guess'' reward
function, or a reward function learned from inverse reinforcement learning
(IRL), and the learned reward is trained with preferences. We introduce
state-based and image-based versions of RRM and evaluate them on several tasks
in the Meta-World environment suite. Experimental results show that our method
substantially improves the performance of a common PbRL method. Our method
achieves performance improvements for a variety of different types of prior
rewards, including proxy rewards, a reward obtained from IRL, and even a
negated version of the proxy reward. We also conduct experiments with a Franka
Panda to show that our method leads to superior performance on a real robot. It
significantly accelerates policy learning for different tasks, achieving
success in fewer steps than the baseline. The videos are presented at
https://sunlighted.github.io/RRM-web/.

</details>


### [262] [Cognitive Load-Aware Inference: A Neuro-Symbolic Framework for Optimizing the Token Economy of Large Language Models](https://arxiv.org/abs/2507.00653)
*Yilun Zhang*

Main category: cs.LG

Relevance: 85.0

TL;DR: 论文提出了一种基于认知负载理论（CLT）的LLM推理框架CLAI，通过量化认知负载指标（ICL、ECL、GCL）优化推理过程，显著减少计算开销。


<details>
  <summary>Details</summary>
Motivation: 当前LLM推理的高计算成本限制了其广泛应用，现有方法缺乏理论指导。本文旨在通过认知理论优化推理效率。

Method: 提出CLAI框架，包括CLAI-Prompt（零样本方法）和CLAI-Tune（微调模型），量化认知负载并优化推理过程。

Result: 在复杂推理、长上下文问答和代码生成任务中，CLAI方法减少高达45%的token消耗且不损失准确性。

Conclusion: 通过模拟人脑资源管理策略，CLAI框架显著提升了LLM的推理效率和能力。

Abstract: The escalating computational costs of Large Language Model (LLM) inference
have become a critical barrier to their widespread and sustainable deployment.
While existing optimization strategies are effective, they are predominantly
based on statistical heuristics or architectural modifications, lacking a
guiding cognitive theory to manage the inference process itself. This paper
aims to bridge this gap by introducing a novel paradigm: the Cognitive
Load-Aware Inference (CLAI) framework, which operationalizes principles from
Cognitive Load Theory (CLT) and neuroscience for LLM inference. We formalize
the concepts of Intrinsic Cognitive Load, Extraneous Cognitive Load, and
Germane Cognitive Load into quantifiable LLM metrics ($ICL_{LLM}$, $ECL_{LLM}$,
and $GCL_{LLM}$), thereby reframing the inference process as a cognitive
economics optimization problem: based on the intrinsic complexity of a problem
($ICL_{LLM}$), minimize wasteful computation ($ECL_{LLM}$), and strategically
allocate the token budget to productive reasoning ($GCL_{LLM}$). We propose two
implementation paths: CLAI-Prompt, a zero-shot method that guides a base LLM
through cognitive control steps via a structured meta-prompt, and CLAI-Tune, a
fine-tuned model that internalizes these principles for spontaneous cognitive
economy. Across a range of benchmarks in complex reasoning, long-context
question answering, and code generation, our methods achieve significant
reductions in token consumption (up to 45\%) without sacrificing accuracy.
Furthermore, CLAI-Tune exhibits an emergent ability to autonomously decompose
difficult problems, a key characteristic of human expert cognition. This work
demonstrates that by emulating the brain's resource management strategies, we
can build more efficient, robust, and capable artificial intelligence systems.

</details>


### [263] [Large Reasoning Models are not thinking straight: on the unreliability of thinking trajectories](https://arxiv.org/abs/2507.00711)
*Jhouben Cuesta-Ramirez,Samuel Beaussant,Mehdi Mounsif*

Main category: cs.LG

Relevance: 85.0

TL;DR: LLMs trained via RL generate longer but ineffective reasoning steps, often ignoring correct solutions and leading to incorrect conclusions.


<details>
  <summary>Details</summary>
Motivation: To investigate whether benchmark improvements in LLMs reflect actual reasoning enhancements or superficial behaviors like overthinking.

Method: Experiments on three state-of-the-art models using the AIME2024 math benchmark to analyze their reasoning behaviors.

Result: Models often disregard correct solutions, generating unnecessary reasoning steps that result in errors.

Conclusion: Highlights limitations in LLMs' ability to integrate corrective information, posing challenges for robust and interpretable reasoning.

Abstract: Large Language Models (LLMs) trained via Reinforcement Learning (RL) have
recently achieved impressive results on reasoning benchmarks. Yet, growing
evidence shows that these models often generate longer but ineffective chains
of thought (CoTs), calling into question whether benchmark gains reflect real
reasoning improvements. We present new evidence of overthinking, where models
disregard correct solutions even when explicitly provided, instead continuing
to generate unnecessary reasoning steps that often lead to incorrect
conclusions. Experiments on three state-of-the-art models using the AIME2024
math benchmark reveal critical limitations in these models ability to integrate
corrective information, posing new challenges for achieving robust and
interpretable reasoning.

</details>


### [264] [Reasoning as an Adaptive Defense for Safety](https://arxiv.org/abs/2507.00971)
*Taeyoun Kim,Fahim Tajwar,Aditi Raghunathan,Aviral Kumar*

Main category: cs.LG

Relevance: 85.0

TL;DR: 论文提出了一种名为TARS的方法，通过强化学习训练LLM在推理时自适应分配计算资源，以提高安全性。


<details>
  <summary>Details</summary>
Motivation: 研究如何利用自适应推理方法提升LLM在安全性方面的鲁棒性，并验证其有效性。

Method: 采用强化学习（RL）框架，结合链式思维（chain-of-thought）和奖励信号，设计三个关键步骤：轻量级SFT预热、多样化的提示混合、防止推理能力退化的奖励函数。

Result: TARS训练的模型在模糊查询上分配更多计算资源，改善了安全性与任务完成的权衡，并提升了对抗攻击（如GCG和PAIR）的鲁棒性。

Conclusion: TARS为训练LLM抵御越狱和有害请求提供了一种有效的开放式方法。

Abstract: Reasoning methods that adaptively allocate test-time compute have advanced
LLM performance on easy to verify domains such as math and code. In this work,
we study how to utilize this approach to train models that exhibit a degree of
robustness to safety vulnerabilities, and show that doing so can provide
benefits. We build a recipe called $\textit{TARS}$ (Training Adaptive Reasoners
for Safety), a reinforcement learning (RL) approach that trains models to
reason about safety using chain-of-thought traces and a reward signal that
balances safety with task completion. To build TARS, we identify three critical
design choices: (1) a "lightweight" warmstart SFT stage, (2) a mix of harmful,
harmless, and ambiguous prompts to prevent shortcut behaviors such as too many
refusals, and (3) a reward function to prevent degeneration of reasoning
capabilities during training. Models trained with TARS exhibit adaptive
behaviors by spending more compute on ambiguous queries, leading to better
safety-refusal trade-offs. They also internally learn to better distinguish
between safe and unsafe prompts and attain greater robustness to both white-box
(e.g., GCG) and black-box attacks (e.g., PAIR). Overall, our work provides an
effective, open recipe for training LLMs against jailbreaks and harmful
requests by reasoning per prompt.

</details>


### [265] [Testing the spin-bath view of self-attention: A Hamiltonian analysis of GPT-2 Transformer](https://arxiv.org/abs/2507.00683)
*Satadeep Bhattacharjee,Seung-Cheol Lee*

Main category: cond-mat.mtrl-sci

Relevance: 85.0

TL;DR: 论文通过物理框架将LLM的注意力机制建模为双自旋系统，验证了自旋浴类比在生产级GPT-2模型中的有效性。


<details>
  <summary>Details</summary>
Motivation: 探索LLM注意力机制的物理解释，以提供可解释性和新生成模型的基础。

Method: 提取GPT-2的Query-Key权重矩阵，推导有效哈密顿量，并通过理论logit间隙与实证token排名的相关性验证假设。

Result: 发现理论logit间隙与模型token排名呈强负相关（r≈-0.70），并通过针对性消融实验确认因果联系。

Conclusion: 研究首次为生产级模型中的自旋浴类比提供了实证支持，为可解释性和新模型设计奠定了基础。

Abstract: The recently proposed physics-based framework by Huo and
Johnson~\cite{huo2024capturing} models the attention mechanism of Large
Language Models (LLMs) as an interacting two-body spin system, offering a
first-principles explanation for phenomena like repetition and bias. Building
on this hypothesis, we extract the complete Query-Key weight matrices from a
production-grade GPT-2 model and derive the corresponding effective Hamiltonian
for every attention head. From these Hamiltonians we obtain analytic
\textit{phase boundaries} logit gap criteria that predict which token should
dominate the next-token distribution for a given context. A systematic
evaluation on 144 heads across 20 factual-recall prompts reveals a strong
negative correlation between the theoretical logit gaps and the model's
empirical token rankings ($r\approx-0.70$, $p<10^{-3}$).Targeted ablations
further show that suppressing the heads most aligned with the spin-bath
predictions induces the anticipated shifts in output probabilities, confirming
a causal link rather than a coincidental association. Taken together, our
findings provide the first strong empirical evidence for the spin-bath analogy
in a production-grade model. This validation not only furnishes a tractable,
physics-inspired lens for interpretability but also provides the groundwork for
novel generative models, bridging the gap between theoretical condensed matter
physics and AI.

</details>


### [266] [The language of time: a language model perspective on time-series foundation models](https://arxiv.org/abs/2507.00078)
*Yi Xie,Yun Xiong,Zejian Shi,Hao Niu,Zhengfu Liu*

Main category: cs.LG

Relevance: 80.0

TL;DR: 论文探讨了基于补丁的时间序列基础模型的表示学习机制和泛化能力，解释了其与语言模型的相似性及其在跨域任务中的成功。


<details>
  <summary>Details</summary>
Motivation: 解决时间序列基础模型在跨域任务中表现优异的理论矛盾，揭示其与语言模型的联系。

Method: 从理论和实验角度分析补丁时间序列模型的表示学习和泛化能力，提出其扩展了语言模型的表示范式。

Result: 证明时间序列补丁可量化为离散词汇，统计特性与自然语言一致，继承了语言模型的表示和迁移能力。

Conclusion: 为理解、评估和改进大规模时间序列基础模型的安全性和可靠性提供了理论基础。

Abstract: With the rise of large language models, the paradigm of training foundation
models with massive parameter counts on vast datasets has been adopted in
multiple domains to achieve remarkable success. Time series foundation models
represent a significant extension of this paradigm, demonstrating exceptional
expressive power, generalization, and cross-domain transferability. However,
this gives rise to a fundamental paradox: time series data reflect distinct
dynamical systems, making cross-domain transfer intuitively implausible, yet
this is contradicted by the models' empirical success. To resolve this paradox,
this paper investigates, from both theoretical and experimental perspectives,
the representation learning mechanisms and generalization capabilities of
patch-based time series foundation models. We argue that such models are not
merely applying a new architecture but are fundamentally generalizing the
representation paradigm of language models by extending deterministic
vector-based representations to latent probabilistic distributional forms. Our
theoretical analysis supports this framework by demonstrating that continuous
time-series patches can be faithfully quantized into a discrete vocabulary
whose key statistical properties are highly consistent with those of natural
language. This generalization allows time series models to inherit the robust
representation and transfer abilities of large language models, thereby
explaining their superior performance in temporal tasks. Ultimately, our work
provides a rigorous theoretical cornerstone for understanding, evaluating, and
improving the safety and reliability of large-scale time series foundation
models.

</details>


### [267] [GLU Attention Improve Transformer](https://arxiv.org/abs/2507.00022)
*Zehao Wang*

Main category: cs.LG

Relevance: 75.0

TL;DR: 论文提出了一种名为GLU Attention的新型注意力机制，通过引入非线性增强注意力值，提升模型性能和收敛速度，且无需额外参数和计算成本。


<details>
  <summary>Details</summary>
Motivation: 探索如何通过改进注意力机制来提升神经网络的性能，尤其是在多模态任务中。

Method: 提出GLU Attention机制，将非线性引入注意力值，并与现有技术（如Flash Attention、RoPE等）无缝集成。

Result: 实验表明，GLU Attention在文本和视觉任务中均能显著提升模型性能和收敛速度。

Conclusion: GLU Attention是一种轻量级且高效的注意力机制改进方法，具有广泛的应用潜力。

Abstract: Gated Linear Units (GLU) have shown great potential in enhancing neural
network performance. In this paper, I introduce a novel attention mechanism
called GLU Attention, which introduces nonlinearity into the values of
Attention. My experiments demonstrate that GLU Attention improves both model
performance and convergence speed across text and vision modalities with zero
additional parameters and negligible computational costs. GLU Attention is
lightweight and can seamlessly integrate with other technologies, such as Flash
Attention, Rotary Position Embedding (RoPE), and various Multi-Head Attention
(MHA) variants such as Grouped-Query Attention (GQA). This project is
open-sourced at github.

</details>


### [268] [Model Fusion via Neuron Interpolation](https://arxiv.org/abs/2507.00037)
*Phoomraphee Luenam,Andreas Spanopoulos,Amit Sant,Thomas Hofmann,Sotiris Anagnostidis,Sidak Pal Singh*

Main category: cs.LG

Relevance: 75.0

TL;DR: 提出了一种基于神经元中心的新型模型融合算法，通过神经元分组和属性评分，有效整合多个训练好的神经网络，适用于任意层类型。


<details>
  <summary>Details</summary>
Motivation: 解决模型融合中因内部表示差异（如排列不变性、随机初始化或不同数据分布）导致的挑战。

Method: 利用神经元分组和属性评分，创建目标表示，并通过子网络近似这些表示。

Result: 在多个基准数据集上，新算法在零样本和非独立同分布场景中优于现有融合技术。

Conclusion: 新算法能够有效整合不同模型的知识，提升融合效果。

Abstract: Model fusion aims to combine the knowledge of multiple models by creating one
representative model that captures the strengths of all of its parents.
However, this process is non-trivial due to differences in internal
representations, which can stem from permutation invariance, random
initialization, or differently distributed training data. We present a novel,
neuron-centric family of model fusion algorithms designed to integrate multiple
trained neural networks into a single network effectively regardless of
training data distribution. Our algorithms group intermediate neurons of parent
models to create target representations that the fused model approximates with
its corresponding sub-network. Unlike prior approaches, our approach
incorporates neuron attribution scores into the fusion process. Furthermore,
our algorithms can generalize to arbitrary layer types. Experimental results on
various benchmark datasets demonstrate that our algorithms consistently
outperform previous fusion techniques, particularly in zero-shot and non-IID
fusion scenarios. The code is available at
https://github.com/AndrewSpano/neuron-interpolation-model-fusion.

</details>


### [269] [Fractional Policy Gradients: Reinforcement Learning with Long-Term Memory](https://arxiv.org/abs/2507.00073)
*Urvi Pawar,Kunal Telangi*

Main category: cs.LG

Relevance: 75.0

TL;DR: 提出了Fractional Policy Gradients (FPG)，一种结合分数阶微积分进行长期时间建模的强化学习框架，显著提升样本效率和方差减少。


<details>
  <summary>Details</summary>
Motivation: 标准策略梯度方法受限于马尔可夫假设，导致高方差和低效采样。FPG通过分数阶导数建模长期依赖关系。

Method: 使用Caputo分数阶导数重新定义梯度，提出高效的递归计算技术处理分数阶时间差分误差。

Result: 理论分析显示FPG实现渐进方差减少O(t^(-alpha))，实验验证样本效率提升35-68%，方差减少24-52%。

Conclusion: FPG为利用长期依赖关系提供了数学基础方法，且无额外计算开销。

Abstract: We propose Fractional Policy Gradients (FPG), a reinforcement learning
framework incorporating fractional calculus for long-term temporal modeling in
policy optimization. Standard policy gradient approaches face limitations from
Markovian assumptions, exhibiting high variance and inefficient sampling. By
reformulating gradients using Caputo fractional derivatives, FPG establishes
power-law temporal correlations between state transitions. We develop an
efficient recursive computation technique for fractional temporal-difference
errors with constant time and memory requirements. Theoretical analysis shows
FPG achieves asymptotic variance reduction of order O(t^(-alpha)) versus
standard policy gradients while preserving convergence. Empirical validation
demonstrates 35-68% sample efficiency gains and 24-52% variance reduction
versus state-of-the-art baselines. This framework provides a mathematically
grounded approach for leveraging long-range dependencies without computational
overhead.

</details>


### [270] [Interpretable AI for Time-Series: Multi-Model Heatmap Fusion with Global Attention and NLP-Generated Explanations](https://arxiv.org/abs/2507.00234)
*Jiztom Kavalakkatt Francis,Matthew J Darr*

Main category: cs.LG

Relevance: 75.0

TL;DR: 提出了一种结合ResNet和Transformer热图的新框架，解决了现有可解释性方法中的时空错位问题，显著提升了医疗和工业领域的模型性能。


<details>
  <summary>Details</summary>
Motivation: 解决卷积网络无法捕捉全局上下文和Transformer缺乏局部精确性的问题，以提升安全关键领域的可解释性。

Method: 融合ResNet的梯度加权激活图和Transformer注意力展开，实现时空对齐，并加入NLP模块生成领域特定解释。

Result: 在PhysioNet和UCI数据集上分别达到94.1%准确率和RMSE=0.28 kWh，优于基线模型3.8-12.4%。

Conclusion: 通过因果保真度和时空对齐，该方法为透明决策提供了可扩展的解决方案。

Abstract: In this paper, we present a novel framework for enhancing model
interpretability by integrating heatmaps produced separately by ResNet and a
restructured 2D Transformer with globally weighted input saliency. We address
the critical problem of spatial-temporal misalignment in existing
interpretability methods, where convolutional networks fail to capture global
context and Transformers lack localized precision - a limitation that impedes
actionable insights in safety-critical domains like healthcare and industrial
monitoring. Our method merges gradient-weighted activation maps (ResNet) and
Transformer attention rollout into a unified visualization, achieving full
spatial-temporal alignment while preserving real-time performance. Empirical
evaluations on clinical (ECG arrhythmia detection) and industrial (energy
consumption prediction) datasets demonstrate significant improvements: the
hybrid framework achieves 94.1% accuracy (F1 0.93) on the PhysioNet dataset and
reduces regression error to RMSE = 0.28 kWh (R2 = 0.95) on the UCI Energy
Appliance dataset-outperforming standalone ResNet, Transformer, and
InceptionTime baselines by 3.8-12.4%. An NLP module translates fused heatmaps
into domain-specific narratives (e.g., "Elevated ST-segment between 2-4 seconds
suggests myocardial ischemia"), validated via BLEU-4 (0.586) and ROUGE-L
(0.650) scores. By formalizing interpretability as causal fidelity and
spatial-temporal alignment, our approach bridges the gap between technical
outputs and stakeholder understanding, offering a scalable solution for
transparent, time-aware decision-making.

</details>


### [271] [Data-Driven Exploration for a Class of Continuous-Time Linear--Quadratic Reinforcement Learning Problems](https://arxiv.org/abs/2507.00358)
*Yilie Huang,Xun Yu Zhou*

Main category: cs.LG

Relevance: 75.0

TL;DR: 论文提出了一种用于连续时间随机线性二次控制问题的自适应探索机制，通过动态调整熵正则化和策略方差，提升了学习效率，并匹配了已知的最佳模型无关遗憾界。


<details>
  <summary>Details</summary>
Motivation: 研究动机在于解决现有探索机制需要大量调参且忽略学习进度的问题，提出自适应探索方法以提升学习效率和性能。

Method: 采用模型无关的数据驱动方法，通过动态调整熵正则化（critic）和策略方差（actor）实现自适应探索。

Result: 数值实验表明，自适应探索方法在收敛速度和遗憾性能上优于非自适应模型无关和基于模型的方法。

Conclusion: 自适应探索机制在提升学习效率的同时，保持了与最佳模型无关方法相同的遗憾界。

Abstract: We study reinforcement learning (RL) for the same class of continuous-time
stochastic linear--quadratic (LQ) control problems as in
\cite{huang2024sublinear}, where volatilities depend on both states and
controls while states are scalar-valued and running control rewards are absent.
We propose a model-free, data-driven exploration mechanism that adaptively
adjusts entropy regularization by the critic and policy variance by the actor.
Unlike the constant or deterministic exploration schedules employed in
\cite{huang2024sublinear}, which require extensive tuning for implementations
and ignore learning progresses during iterations, our adaptive exploratory
approach boosts learning efficiency with minimal tuning. Despite its
flexibility, our method achieves a sublinear regret bound that matches the
best-known model-free results for this class of LQ problems, which were
previously derived only with fixed exploration schedules. Numerical experiments
demonstrate that adaptive explorations accelerate convergence and improve
regret performance compared to the non-adaptive model-free and model-based
counterparts.

</details>


### [272] [Leveraging Genetic Algorithms for Efficient Demonstration Generation in Real-World Reinforcement Learning Environments](https://arxiv.org/abs/2507.00762)
*Tom Maus,Asma Atamna,Tobias Glasmachers*

Main category: cs.LG

Relevance: 75.0

TL;DR: 论文提出了一种利用遗传算法（GA）生成专家演示来增强强化学习（RL）性能的方法，通过结合DQN和PPO，显著提升了训练效果。


<details>
  <summary>Details</summary>
Motivation: 解决RL在工业应用中的样本效率低和学习动态不稳定的问题，探索启发式搜索与数据驱动RL的结合潜力。

Method: 使用GA生成专家演示，将其融入DQN的回放缓冲区，并作为PPO的初始轨迹，加速训练收敛。

Result: 实验表明，GA生成的演示显著提升了RL性能，PPO结合GA数据获得了更高的累积奖励。

Conclusion: 混合学习范式（启发式搜索+RL）在工业应用中具有潜力，公开框架支持进一步研究。

Abstract: Reinforcement Learning (RL) has demonstrated significant potential in certain
real-world industrial applications, yet its broader deployment remains limited
by inherent challenges such as sample inefficiency and unstable learning
dynamics. This study investigates the utilization of Genetic Algorithms (GAs)
as a mechanism for improving RL performance in an industrially inspired sorting
environment. We propose a novel approach in which GA-generated expert
demonstrations are used to enhance policy learning. These demonstrations are
incorporated into a Deep Q-Network (DQN) replay buffer for experience-based
learning and utilized as warm-start trajectories for Proximal Policy
Optimization (PPO) agents to accelerate training convergence. Our experiments
compare standard RL training with rule-based heuristics, brute-force
optimization, and demonstration data, revealing that GA-derived demonstrations
significantly improve RL performance. Notably, PPO agents initialized with
GA-generated data achieved superior cumulative rewards, highlighting the
potential of hybrid learning paradigms, where heuristic search methods
complement data-driven RL. The utilized framework is publicly available and
enables further research into adaptive RL strategies for real-world
applications.

</details>


### [273] [Gym4ReaL: A Suite for Benchmarking Real-World Reinforcement Learning](https://arxiv.org/abs/2507.00257)
*Davide Salaorni,Vincenzo De Paola,Samuele Delpero,Giovanni Dispoto,Paolo Bonetti,Alessio Russo,Giuseppe Calcagno,Francesco Trovò,Matteo Papini,Alberto Maria Metelli,Marco Mussi,Marcello Restelli*

Main category: cs.LG

Relevance: 70.0

TL;DR: 论文介绍了Gym4ReaL，一套用于开发和评估在现实场景中运行的RL算法的环境套件，旨在解决现实世界中的挑战。


<details>
  <summary>Details</summary>
Motivation: 当前RL研究多关注理想化环境，而忽视了现实世界的复杂性（如大状态-动作空间、非平稳性和部分可观测性）。

Method: 开发了Gym4ReaL，包含多样化的任务以模拟现实挑战。

Result: 标准RL算法在这些环境中表现良好，但仍需新方法以充分发挥RL潜力。

Conclusion: Gym4ReaL为RL在现实应用中的研究提供了重要工具。

Abstract: In recent years, \emph{Reinforcement Learning} (RL) has made remarkable
progress, achieving superhuman performance in a wide range of simulated
environments. As research moves toward deploying RL in real-world applications,
the field faces a new set of challenges inherent to real-world settings, such
as large state-action spaces, non-stationarity, and partial observability.
Despite their importance, these challenges are often underexplored in current
benchmarks, which tend to focus on idealized, fully observable, and stationary
environments, often neglecting to incorporate real-world complexities
explicitly. In this paper, we introduce \texttt{Gym4ReaL}, a comprehensive
suite of realistic environments designed to support the development and
evaluation of RL algorithms that can operate in real-world scenarios. The suite
includes a diverse set of tasks that expose algorithms to a variety of
practical challenges. Our experimental results show that, in these settings,
standard RL algorithms confirm their competitiveness against rule-based
benchmarks, motivating the development of new methods to fully exploit the
potential of RL to tackle the complexities of real-world tasks.

</details>


### [274] [Exploring Large Action Sets with Hyperspherical Embeddings using von Mises-Fisher Sampling](https://arxiv.org/abs/2507.00518)
*Walid Bendada,Guillaume Salha-Galvan,Romain Hennequin,Théo Bontempelli,Thomas Bouabça,Tristan Cazenave*

Main category: cs.LG

Relevance: 70.0

TL;DR: vMF-exp是一种可扩展的强化学习方法，用于探索大动作集，通过超球面嵌入向量表示动作，解决了Boltzmann Exploration的扩展性问题。


<details>
  <summary>Details</summary>
Motivation: 解决Boltzmann Exploration在大动作集探索中的扩展性问题，提出一种更高效的替代方法。

Method: 使用von Mises-Fisher分布采样状态嵌入表示，并探索其最近邻，适用于超球面嵌入的大动作集。

Result: 理论证明vMF-exp与Boltzmann Exploration具有相同的探索概率，实验验证了其在大规模推荐系统中的有效性。

Conclusion: vMF-exp是Boltzmann Exploration的可扩展替代方案，适用于超球面嵌入的大动作集探索。

Abstract: This paper introduces von Mises-Fisher exploration (vMF-exp), a scalable
method for exploring large action sets in reinforcement learning problems where
hyperspherical embedding vectors represent these actions. vMF-exp involves
initially sampling a state embedding representation using a von Mises-Fisher
distribution, then exploring this representation's nearest neighbors, which
scales to virtually unlimited numbers of candidate actions. We show that, under
theoretical assumptions, vMF-exp asymptotically maintains the same probability
of exploring each action as Boltzmann Exploration (B-exp), a popular
alternative that, nonetheless, suffers from scalability issues as it requires
computing softmax values for each action. Consequently, vMF-exp serves as a
scalable alternative to B-exp for exploring large action sets with
hyperspherical embeddings. Experiments on simulated data, real-world public
data, and the successful large-scale deployment of vMF-exp on the recommender
system of a global music streaming service empirically validate the key
properties of the proposed method.

</details>


### [275] [Evaluating LLMs and Prompting Strategies for Automated Hardware Diagnosis from Textual User-Reports](https://arxiv.org/abs/2507.00742)
*Carlos Caminha,Maria de Lourdes M. Silva,Iago C. Chaves,Felipe T. Brito,Victor A. E. Farias,Javam C. Machado*

Main category: cs.LG

Relevance: 70.0

TL;DR: 该研究评估了27个开源和2个专有LLM在设备故障报告分类任务中的表现，使用四种提示策略，发现三个模型在性能和效率上表现最佳。


<details>
  <summary>Details</summary>
Motivation: 设备故障报告通常模糊且缺乏细节，LLM有望解决这一问题，但需要评估不同模型和提示策略的效果。

Method: 研究评估了27个开源模型和2个专有模型，采用四种提示策略（Zero-Shot、Few-Shot、Chain-of-Thought、CoT+Few-Shot），进行了大量推理实验。

Result: 最高F1分数达0.76，三个模型（mistral-small-24b-instruct、llama-3.2-1b-instruct、gemma-2-2b-it）在性能和效率上表现最佳。

Conclusion: 研究展示了LLM在设备故障分类中的潜力，并推荐了适合终端设备的高效模型。

Abstract: Computer manufacturers offer platforms for users to describe device faults
using textual reports such as "My screen is flickering". Identifying the faulty
component from the report is essential for automating tests and improving user
experience. However, such reports are often ambiguous and lack detail, making
this task challenging. Large Language Models (LLMs) have shown promise in
addressing such issues. This study evaluates 27 open-source models (1B-72B
parameters) and 2 proprietary LLMs using four prompting strategies: Zero-Shot,
Few-Shot, Chain-of-Thought (CoT), and CoT+Few-Shot (CoT+FS). We conducted
98,948 inferences, processing over 51 million input tokens and generating 13
million output tokens. We achieve f1-score up to 0.76. Results show that three
models offer the best balance between size and performance:
mistral-small-24b-instruct and two smaller models, llama-3.2-1b-instruct and
gemma-2-2b-it, that offer competitive performance with lower VRAM usage,
enabling efficient inference on end-user devices as modern laptops or
smartphones with NPUs.

</details>


### [276] [Description of the Training Process of Neural Networks via Ergodic Theorem : Ghost nodes](https://arxiv.org/abs/2507.01003)
*Eun-Ji Park,Sangwon Yun*

Main category: cs.LG

Relevance: 70.0

TL;DR: 提出了一种通过随机梯度下降理解和加速深度神经网络训练的统一框架，引入Lyapunov指数诊断，并提出幽灵类别扩展以优化训练过程。


<details>
  <summary>Details</summary>
Motivation: 从遍历角度理解训练过程，解决深度神经网络训练中的收敛问题和鞍点稳定性问题。

Method: 分析目标函数的几何景观，引入Lyapunov指数诊断，提出幽灵类别扩展以增加下降方向。

Result: 幽灵扩展严格减少近似误差，且在收敛后与原始模型一致，同时加速早期训练。

Conclusion: 提供了一种架构层面的干预方法，加速早期训练并保持渐近行为。

Abstract: Recent studies have proposed interpreting the training process from an
ergodic perspective. Building on this foundation we present a unified framework
for understanding and accelerating the training of deep neural networks via
stochastic gradient descent. By analyzing the geometric landscape of the
objective function we introduce a practical diagnostic, the running estimate of
the largest Lyapunov exponent, which provably distinguishes genuine convergence
toward stable minimizers from mere statistical stabilization near saddle
points. We then propose a ghost category extension for standard classifiers
that adds auxiliary ghost output nodes so the model gains extra descent
directions that open a lateral corridor around narrow loss barriers and enable
the optimizer to bypass poor basins during the early training phase. We show
that this extension strictly reduces approximation error and that after
sufficient convergence the ghost dimensions collapse and the extended model's
invariant law coincides with that of the original and there exists a path in
the enlarged parameter space along which the total loss does not increase while
the original loss decreases by an arbitrary margin. Taken together these
results provide a principled architecture level intervention that accelerates
early stage trainability while preserving asymptotic behavior.

</details>


### [277] [Towards Undistillable Models by Minimizing Conditional Mutual Information](https://arxiv.org/abs/2507.00012)
*Linfeng Ye,Shayan Mohajer Hamidi,En-hui Yang*

Main category: cs.LG

Relevance: 60.0

TL;DR: 论文提出了一种称为CMIM的训练方法，通过最小化交叉熵损失和条件互信息（CMI）值，构建不可蒸馏的深度神经网络（DNN），以保护知识产权。


<details>
  <summary>Details</summary>
Motivation: 为了保护DNN的知识产权，研究如何构建不可蒸馏的DNN，即学生模型无法通过知识蒸馏超越独立训练的模型。

Method: 提出CMIM方法，联合最小化交叉熵损失和温度缩放后各簇的CMI值，训练DNN。

Result: 实验表明，CMIM模型对所有测试的知识蒸馏方法均不可蒸馏，且其预测精度优于仅使用交叉熵损失的模型。

Conclusion: CMIM方法有效构建了不可蒸馏的DNN，同时提升了模型自身的性能。

Abstract: A deep neural network (DNN) is said to be undistillable if, when used as a
black-box input-output teacher, it cannot be distilled through knowledge
distillation (KD). In this case, the distilled student (referred to as the
knockoff student) does not outperform a student trained independently with
label smoothing (LS student) in terms of prediction accuracy. To protect
intellectual property of DNNs, it is desirable to build undistillable DNNs. To
this end, it is first observed that an undistillable DNN may have the trait
that each cluster of its output probability distributions in response to all
sample instances with the same label should be highly concentrated to the
extent that each cluster corresponding to each label should ideally collapse
into one probability distribution. Based on this observation and by measuring
the concentration of each cluster in terms of conditional mutual information
(CMI), a new training method called CMI minimized (CMIM) method is proposed,
which trains a DNN by jointly minimizing the conventional cross entropy (CE)
loss and the CMI values of all temperature scaled clusters across the entire
temperature spectrum. The resulting CMIM model is shown, by extensive
experiments, to be undistillable by all tested KD methods existing in the
literature. That is, the knockoff students distilled by these KD methods from
the CMIM model underperform the respective LS students. In addition, the CMIM
model is also shown to performs better than the model trained with the CE loss
alone in terms of their own prediction accuracy.

</details>


### [278] [Adaptive Action Duration with Contextual Bandits for Deep Reinforcement Learning in Dynamic Environments](https://arxiv.org/abs/2507.00030)
*Abhishek Verma,Nallarasan V,Balaraman Ravindran*

Main category: cs.LG

Relevance: 60.0

TL;DR: 论文提出了一种结合上下文多臂老虎机和深度强化学习（DRL）的新范式，用于自适应选择动作持续时间，提升策略灵活性和计算效率。


<details>
  <summary>Details</summary>
Motivation: 当前DRL在复杂序列决策任务中表现优异，但动作执行的时间尺度问题尚未充分探索。

Method: 通过增强深度Q网络（DQN）的上下文多臂老虎机模块，学习基于状态上下文选择最优动作重复频率。

Result: 在Atari 2600游戏中，该方法显著优于静态持续时间的基线方法。

Conclusion: 自适应时间抽象为DRL提供了可扩展的解决方案，适用于需要动态动作持续时间的实时应用。

Abstract: Deep Reinforcement Learning (DRL) has achieved remarkable success in complex
sequential decision-making tasks, such as playing Atari 2600 games and
mastering board games. A critical yet underexplored aspect of DRL is the
temporal scale of action execution. We propose a novel paradigm that integrates
contextual bandits with DRL to adaptively select action durations, enhancing
policy flexibility and computational efficiency. Our approach augments a Deep
Q-Network (DQN) with a contextual bandit module that learns to choose optimal
action repetition rates based on state contexts. Experiments on Atari 2600
games demonstrate significant performance improvements over static duration
baselines, highlighting the efficacy of adaptive temporal abstractions in DRL.
This paradigm offers a scalable solution for real-time applications like gaming
and robotics, where dynamic action durations are critical.

</details>


### [279] [Quality over Quantity: An Effective Large-Scale Data Reduction Strategy Based on Pointwise V-Information](https://arxiv.org/abs/2507.00038)
*Fei Chen,Wenchi Zhou*

Main category: cs.LG

Relevance: 60.0

TL;DR: 提出了一种基于点状V信息（PVI）的数据缩减策略，通过筛选高难度实例提升训练效率和模型性能。


<details>
  <summary>Details</summary>
Motivation: 解决在大规模数据集中选择最优实例以提升数据质量和训练效率的核心挑战。

Method: 1. 使用PVI量化实例难度并静态过滤低难度实例；2. 采用渐进式学习方法按PVI升序训练分类器。

Result: 移除10%-30%数据后分类器性能损失仅为0.0001%至0.76%；渐进式学习加速收敛并提升0.8%准确率。

Conclusion: 有效的数据缩减策略可提升模型性能和训练效率，且PVI框架成功应用于跨语言任务。

Abstract: Data reduction plays a vital role in data-centric AI by identifying the most
informative instance within large-scale datasets to enhance model training
efficiency. The core challenge lies in how to select the optimal
instances-rather than the entire datasets-to improve data quality and training
efficiency. In this paper, we propose an effective data reduction strategy
based on Pointwise V-information(PVI). First, we quantify instance difficulty
using PVI and filter out low-difficulty instances enabling a static approach.
Experiments demonstrate that removing 10%-30% of the data preserves the
classifier performance with only a 0.0001% to 0.76% loss in accuracy.Second, we
use a progressive learning approach to training the classifiers on instances
sorted by ascending PVI, accelerating convergence and achieving a 0.8% accuracy
gain over conventional training. Our results suggest that with the effective
data reduction strategy, training a classifier on the selected optimal subset
could enhance the model performance and boost training efficiency. Moreover, we
have transferred the PVI framework, which previously applied only to English
datasets, to diverse Chinese NLP tasks and base models, leading to valuable
insights for cross-lingual data reduction and faster training. The codes are
released at https://github.com/zhouwenchi/DatasetReductionStrategy.

</details>


### [280] [Diffusion Classifier Guidance for Non-robust Classifiers](https://arxiv.org/abs/2507.00687)
*Philipp Vaeth,Dibyanshu Kumar,Benjamin Paassen,Magda Gregorová*

Main category: cs.LG

Relevance: 60.0

TL;DR: 论文提出了一种扩展分类器引导的方法，使其适用于非鲁棒分类器，解决了噪声条件下分类器性能下降的问题，并通过实验验证了方法的有效性。


<details>
  <summary>Details</summary>
Motivation: 分类器引导通常依赖于鲁棒分类器，限制了其应用范围。本文旨在扩展分类器引导的适用性，使其能够利用非鲁棒分类器。

Method: 通过分析非鲁棒分类器在噪声条件下的性能，提出了一种基于一步去噪图像预测和稳定化技术的方法。

Result: 实验表明，该方法提高了分类器引导的稳定性，同时保持了样本多样性和视觉质量。

Conclusion: 该研究扩展了生成模型中条件采样技术的应用范围，使更多分类器可用作引导分类器。

Abstract: Classifier guidance is intended to steer a diffusion process such that a
given classifier reliably recognizes the generated data point as a certain
class. However, most classifier guidance approaches are restricted to robust
classifiers, which were specifically trained on the noise of the diffusion
forward process. We extend classifier guidance to work with general,
non-robust, classifiers that were trained without noise. We analyze the
sensitivity of both non-robust and robust classifiers to noise of the diffusion
process on the standard CelebA data set, the specialized SportBalls data set
and the high-dimensional real-world CelebA-HQ data set. Our findings reveal
that non-robust classifiers exhibit significant accuracy degradation under
noisy conditions, leading to unstable guidance gradients. To mitigate these
issues, we propose a method that utilizes one-step denoised image predictions
and implements stabilization techniques inspired by stochastic optimization
methods, such as exponential moving averages. Experimental results demonstrate
that our approach improves the stability of classifier guidance while
maintaining sample diversity and visual quality. This work contributes to
advancing conditional sampling techniques in generative models, enabling a
broader range of classifiers to be used as guidance classifiers.

</details>


### [281] [Aligning Learning and Endogenous Decision-Making](https://arxiv.org/abs/2507.00851)
*Rares Cristian,Pavithra Harsha,Georgia Perakis,Brian Quanz*

Main category: cs.LG

Relevance: 60.0

TL;DR: 论文提出了一种端到端方法，用于在决策中考虑机器学习模型的下游影响，并引入鲁棒优化变体以处理模型不确定性。通过实验验证，该方法在定价和库存推荐问题上优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 解决决策中缺乏反事实信息和模型不确定性的问题，提升机器学习模型在决策阶段的实用性。

Method: 提出端到端学习方法，结合鲁棒优化和两阶段随机优化，处理模型不确定性和信息收集问题。

Result: 实验表明，该方法在定价和库存推荐问题上性能优于现有方法。

Conclusion: 该方法能有效处理决策中的不确定性和信息收集问题，提升决策质量。

Abstract: Many of the observations we make are biased by our decisions. For instance,
the demand of items is impacted by the prices set, and online checkout choices
are influenced by the assortments presented. The challenge in decision-making
under this setting is the lack of counterfactual information, and the need to
learn it instead. We introduce an end-to-end method under endogenous
uncertainty to train ML models to be aware of their downstream, enabling their
effective use in the decision-making stage. We further introduce a robust
optimization variant that accounts for uncertainty in ML models -- specifically
by constructing uncertainty sets over the space of ML models and optimizing
actions to protect against worst-case predictions. We prove guarantees that
this robust approach can capture near-optimal decisions with high probability
as a function of data. Besides this, we also introduce a new class of two-stage
stochastic optimization problems to the end-to-end learning framework that can
now be addressed through our framework. Here, the first stage is an
information-gathering problem to decide which random variable to poll and gain
information about before making a second-stage decision based off of it. We
present several computational experiments for pricing and inventory
assortment/recommendation problems. We compare against existing methods in
online learning/bandits/offline reinforcement learning and show our approach
has consistent improved performance over these. Just as in the endogenous
setting, the model's prediction also depends on the first-stage decision made.
While this decision does not affect the random variable in this setting, it
does affect the correct point forecast that should be made.

</details>


### [282] [NN-Former: Rethinking Graph Structure in Neural Architecture Representation](https://arxiv.org/abs/2507.00880)
*Ruihan Xu,Haokui Zhang,Yaowei Wang,Wei Zeng,Shiliang Zhang*

Main category: cs.LG

Relevance: 60.0

TL;DR: 论文提出了一种结合GNN和Transformer优势的新型神经网络预测器，通过引入兄弟节点和双向图同构前馈网络，提升了DAG拓扑学习的性能。


<details>
  <summary>Details</summary>
Motivation: 解决GNN和Transformer在神经网络架构表示中的局限性，GNN难以表示复杂特征，Transformer在架构深度增加时泛化能力差。

Method: 提出了一种新型预测器，结合GNN和Transformer，引入兄弟节点的新颖token mixer和双向图同构前馈网络。

Result: 在准确性和延迟预测方面表现优异，为DAG拓扑学习提供了新见解。

Conclusion: 结合GNN和Transformer的方法有效解决了各自缺点，提升了神经网络架构预测的性能。

Abstract: The growing use of deep learning necessitates efficient network design and
deployment, making neural predictors vital for estimating attributes such as
accuracy and latency. Recently, Graph Neural Networks (GNNs) and transformers
have shown promising performance in representing neural architectures. However,
each of both methods has its disadvantages. GNNs lack the capabilities to
represent complicated features, while transformers face poor generalization
when the depth of architecture grows. To mitigate the above issues, we rethink
neural architecture topology and show that sibling nodes are pivotal while
overlooked in previous research. We thus propose a novel predictor leveraging
the strengths of GNNs and transformers to learn the enhanced topology. We
introduce a novel token mixer that considers siblings, and a new channel mixer
named bidirectional graph isomorphism feed-forward network. Our approach
consistently achieves promising performance in both accuracy and latency
prediction, providing valuable insights for learning Directed Acyclic Graph
(DAG) topology. The code is available at https://github.com/XuRuihan/NNFormer.

</details>


### [283] [Benchmarking the Discovery Engine](https://arxiv.org/abs/2507.00964)
*Jack Foxabbott,Arush Tagade,Andrew Cusick,Robbie McCorkell,Leo McKee-Reid,Jugal Patel,Jamie Rumbelow,Jessica Rumbelow,Zohreh Shams*

Main category: cs.LG

Relevance: 60.0

TL;DR: Discovery Engine是一个结合机器学习和可解释性的自动化科学发现系统，在多个领域的基准测试中表现优异。


<details>
  <summary>Details</summary>
Motivation: 开发一个能够快速、稳健地从多样化数据中提取科学洞察的自动化系统。

Method: 结合机器学习和最先进的可解释性技术，对五个领域的科学出版物进行基准测试。

Result: 在预测性能和可解释性方面均优于或匹配现有方法。

Conclusion: Discovery Engine有望成为自动化、可解释科学建模的新标准。

Abstract: The Discovery Engine is a general purpose automated system for scientific
discovery, which combines machine learning with state-of-the-art ML
interpretability to enable rapid and robust scientific insight across diverse
datasets. In this paper, we benchmark the Discovery Engine against five recent
peer-reviewed scientific publications applying machine learning across
medicine, materials science, social science, and environmental science. In each
case, the Discovery Engine matches or exceeds prior predictive performance
while also generating deeper, more actionable insights through rich
interpretability artefacts. These results demonstrate its potential as a new
standard for automated, interpretable scientific modelling that enables complex
knowledge discovery from data.

</details>


### [284] [Scalable Feature Learning on Huge Knowledge Graphs for Downstream Machine Learning](https://arxiv.org/abs/2507.00965)
*Félix Lefebvre,Gaël Varoquaux*

Main category: cs.LG

Relevance: 60.0

TL;DR: SEPAL是一种可扩展的嵌入传播算法，用于大规模知识图谱，通过优化核心实体嵌入并通过消息传递传播，显著提升下游任务性能。


<details>
  <summary>Details</summary>
Motivation: 当前知识图谱嵌入方法主要针对链接预测优化，且难以扩展到大规模图谱。SEPAL旨在解决这些问题，提供高质量、可扩展的嵌入。

Method: SEPAL通过优化核心实体嵌入并通过消息传递传播到整个图谱，实现全局嵌入对齐。

Result: 在7个大规模知识图谱和46个下游任务中，SEPAL显著优于现有方法，并能扩展到大型图谱。

Conclusion: SEPAL为大规模知识图谱提供了一种高效、可扩展的嵌入方法，适用于多种下游任务。

Abstract: Many machine learning tasks can benefit from external knowledge. Large
knowledge graphs store such knowledge, and embedding methods can be used to
distill it into ready-to-use vector representations for downstream
applications. For this purpose, current models have however two limitations:
they are primarily optimized for link prediction, via local contrastive
learning, and they struggle to scale to the largest graphs due to GPU memory
limits. To address these, we introduce SEPAL: a Scalable Embedding Propagation
ALgorithm for large knowledge graphs designed to produce high-quality
embeddings for downstream tasks at scale. The key idea of SEPAL is to enforce
global embedding alignment by optimizing embeddings only on a small core of
entities, and then propagating them to the rest of the graph via message
passing. We evaluate SEPAL on 7 large-scale knowledge graphs and 46 downstream
machine learning tasks. Our results show that SEPAL significantly outperforms
previous methods on downstream tasks. In addition, SEPAL scales up its base
embedding model, enabling fitting huge knowledge graphs on commodity hardware.

</details>


### [285] [Generalization performance of narrow one-hidden layer networks in the teacher-student setting](https://arxiv.org/abs/2507.00629)
*Jean Barbier,Federica Gerace,Alessandro Ingrosso,Clarissa Lauditi,Enrico M. Malatesta,Gibbs Nwemadji,Rodrigo Pérez Ortiz*

Main category: cond-mat.dis-nn

Relevance: 60.0

TL;DR: 论文提出了一种关于窄神经网络（隐藏单元数远小于输入维度）在教师-学生设置下的泛化能力的理论，使用统计物理学方法推导了泛化误差的闭式表达式。


<details>
  <summary>Details</summary>
Motivation: 理解神经网络在简单输入-输出分布下的泛化能力，为实际数据集上的学习性能提供理论支持。

Method: 使用统计物理学方法，推导了窄神经网络在有限温度（贝叶斯）和经验风险最小化估计下的泛化误差闭式表达式。

Result: 理论预测了神经网络在回归或分类任务中的泛化误差，并揭示了隐藏神经元在样本数足够大时的专业化现象。

Conclusion: 该理论为窄神经网络的泛化能力提供了完整的理论框架，适用于多种训练方法。

Abstract: Understanding the generalization abilities of neural networks for simple
input-output distributions is crucial to account for their learning performance
on real datasets. The classical teacher-student setting, where a network is
trained from data obtained thanks to a label-generating teacher model, serves
as a perfect theoretical test bed. In this context, a complete theoretical
account of the performance of fully connected one-hidden layer networks in the
presence of generic activation functions is lacking. In this work, we develop
such a general theory for narrow networks, i.e. networks with a large number of
hidden units, yet much smaller than the input dimension. Using methods from
statistical physics, we provide closed-form expressions for the typical
performance of both finite temperature (Bayesian) and empirical risk
minimization estimators, in terms of a small number of weight statistics. In
doing so, we highlight the presence of a transition where hidden neurons
specialize when the number of samples is sufficiently large and proportional to
the number of parameters of the network. Our theory accurately predicts the
generalization error of neural networks trained on regression or classification
tasks with either noisy full-batch gradient descent (Langevin dynamics) or
full-batch gradient descent.

</details>


### [286] [Harnessing the Power of Reinforcement Learning for Adaptive MCMC](https://arxiv.org/abs/2507.00671)
*Congye Wang,Matthew A. Fisher,Heishiro Kanagawa,Wilson Chen,Chris. J. Oates*

Main category: stat.CO

Relevance: 60.0

TL;DR: 论文提出了一种基于强化学习的Metropolis-Hastings采样器（RLMH），通过对比散度设计新型奖励函数，解决了传统奖励信号不足的问题，并在实验中验证了其有效性。


<details>
  <summary>Details</summary>
Motivation: 现有采样算法调优负担重，强化学习为自适应调优提供了新思路，但Wang et al (2025)的理论工作未解决实际应用问题。

Method: 将Metropolis-Hastings建模为马尔可夫决策过程，提出基于对比散度的奖励函数，设计自适应梯度采样器。

Result: 在posteriordb基准测试中，RLMH表现出优于传统奖励函数的性能。

Conclusion: RLMH通过新型奖励函数和自适应设计，为采样算法调优提供了实用解决方案。

Abstract: Sampling algorithms drive probabilistic machine learning, and recent years
have seen an explosion in the diversity of tools for this task. However, the
increasing sophistication of sampling algorithms is correlated with an increase
in the tuning burden. There is now a greater need than ever to treat the tuning
of samplers as a learning task in its own right. In a conceptual breakthrough,
Wang et al (2025) formulated Metropolis-Hastings as a Markov decision process,
opening up the possibility for adaptive tuning using Reinforcement Learning
(RL). Their emphasis was on theoretical foundations; realising the practical
benefit of Reinforcement Learning Metropolis-Hastings (RLMH) was left for
subsequent work. The purpose of this paper is twofold: First, we observe the
surprising result that natural choices of reward, such as the acceptance rate,
or the expected squared jump distance, provide insufficient signal for training
RLMH. Instead, we propose a novel reward based on the contrastive divergence,
whose superior performance in the context of RLMH is demonstrated. Second, we
explore the potential of RLMH and present adaptive gradient-based samplers that
balance flexibility of the Markov transition kernel with learnability of the
associated RL task. A comprehensive simulation study using the posteriordb
benchmark supports the practical effectiveness of RLMH.

</details>


### [287] [Time Series Foundation Models are Flow Predictors](https://arxiv.org/abs/2507.00945)
*Massimiliano Luca,Ciro Beneduce,Bruno Lepri*

Main category: cs.LG

Relevance: 50.0

TL;DR: 论文研究了时间序列基础模型（TSFMs）在人群流量预测中的有效性，重点关注Moirai和TimesFM模型。在零样本设置下，这些模型在三个真实世界数据集上表现优于统计和深度学习基线。


<details>
  <summary>Details</summary>
Motivation: 探索时间序列基础模型在缺乏空间信息的情况下对人群流量预测的准确性和可扩展性。

Method: 使用Moirai和TimesFM模型，在零样本设置下评估三个真实世界数据集（Bike NYC、Taxi Beijing和Spanish national OD flows）。

Result: 模型在RMSE、MAE和CPC指标上显著优于现有方法，最高提升33%、39%和49%。

Conclusion: TSFMs在有限标注数据或缺失空间信息的情况下仍能提供准确、可扩展的流量预测。

Abstract: We investigate the effectiveness of time series foundation models (TSFMs) for
crowd flow prediction, focusing on Moirai and TimesFM. Evaluated on three
real-world mobility datasets-Bike NYC, Taxi Beijing, and Spanish national OD
flows-these models are deployed in a strict zero-shot setting, using only the
temporal evolution of each OD flow and no explicit spatial information. Moirai
and TimesFM outperform both statistical and deep learning baselines, achieving
up to 33% lower RMSE, 39% lower MAE and up to 49% higher CPC compared to
state-of-the-art competitors. Our results highlight the practical value of
TSFMs for accurate, scalable flow prediction, even in scenarios with limited
annotated data or missing spatial context.

</details>


### [288] [Enhancing Interpretability in Generative Modeling: Statistically Disentangled Latent Spaces Guided by Generative Factors in Scientific Datasets](https://arxiv.org/abs/2507.00298)
*Arkaprabha Ganguli,Nesar Ramachandra,Julie Bessac,Emil Constantinescu*

Main category: stat.ML

Relevance: 50.0

TL;DR: 提出Aux-VAE，一种基于变分自编码器的新架构，通过辅助变量实现潜在变量的解耦。


<details>
  <summary>Details</summary>
Motivation: 解决从高维数据中无监督或半监督地提取生成因素的挑战。

Method: 在经典变分自编码器框架中引入辅助变量，通过调整潜在空间形状实现解耦。

Result: 在多个数据集（包括天文模拟）上验证了Aux-VAE的有效性。

Conclusion: Aux-VAE通过简单修改标准VAE损失函数，实现了潜在变量的解耦。

Abstract: This study addresses the challenge of statistically extracting generative
factors from complex, high-dimensional datasets in unsupervised or
semi-supervised settings. We investigate encoder-decoder-based generative
models for nonlinear dimensionality reduction, focusing on disentangling
low-dimensional latent variables corresponding to independent physical factors.
Introducing Aux-VAE, a novel architecture within the classical Variational
Autoencoder framework, we achieve disentanglement with minimal modifications to
the standard VAE loss function by leveraging prior statistical knowledge
through auxiliary variables. These variables guide the shaping of the latent
space by aligning latent factors with learned auxiliary variables. We validate
the efficacy of Aux-VAE through comparative assessments on multiple datasets,
including astronomical simulations.

</details>


### [289] [Deciding When Not to Decide: Indeterminacy-Aware Intrusion Detection with NeutroSENSE](https://arxiv.org/abs/2507.00003)
*Eyhab Al-Masri*

Main category: cs.LG

Relevance: 40.0

TL;DR: NeutroSENSE是一个基于中性逻辑的集成框架，用于物联网环境中的可解释入侵检测，通过分解预测置信度为真、假和不确定性成分，实现不确定性量化和主动审查。


<details>
  <summary>Details</summary>
Motivation: 提升物联网入侵检测的准确性和可解释性，特别是在边缘部署中，通过中性逻辑量化不确定性，支持更可信的AI决策。

Method: 集成随机森林、XGBoost和逻辑回归，结合中性逻辑分解预测置信度（T、F、I），并使用全局和类别特定阈值标记高不确定性预测。

Result: 在IoT-CAD数据集上达到97%准确率，误分类样本的不确定性显著高于正确分类样本（I=0.62 vs. I=0.24）。

Conclusion: 中性逻辑提升了准确性和可解释性，为边缘和雾计算物联网安全系统提供了信任感知AI的实践基础。

Abstract: This paper presents NeutroSENSE, a neutrosophic-enhanced ensemble framework
for interpretable intrusion detection in IoT environments. By integrating
Random Forest, XGBoost, and Logistic Regression with neutrosophic logic, the
system decomposes prediction confidence into truth (T), falsity (F), and
indeterminacy (I) components, enabling uncertainty quantification and
abstention. Predictions with high indeterminacy are flagged for review using
both global and adaptive, class-specific thresholds. Evaluated on the IoT-CAD
dataset, NeutroSENSE achieved 97% accuracy, while demonstrating that
misclassified samples exhibit significantly higher indeterminacy (I = 0.62)
than correct ones (I = 0.24). The use of indeterminacy as a proxy for
uncertainty enables informed abstention and targeted review-particularly
valuable in edge deployments. Figures and tables validate the correlation
between I-scores and error likelihood, supporting more trustworthy,
human-in-the-loop AI decisions. This work shows that neutrosophic logic
enhances both accuracy and explainability, providing a practical foundation for
trust-aware AI in edge and fog-based IoT security systems.

</details>


### [290] [ST-MTM: Masked Time Series Modeling with Seasonal-Trend Decomposition for Time Series Forecasting](https://arxiv.org/abs/2507.00013)
*Hyunwoo Seo,Chiehyeon Lim*

Main category: cs.LG

Relevance: 40.0

TL;DR: 提出了一种基于季节性-趋势分解的掩码时间序列建模框架ST-MTM，通过分解方法捕捉时间序列中的复杂语义信息，并结合对比学习任务提升预测性能。


<details>
  <summary>Details</summary>
Motivation: 现有掩码时间序列建模方法忽略了时间序列中的固有语义结构，导致学习到虚假的时序模式，因此需要一种分解方法来捕捉不同的时序语义。

Method: ST-MTM框架结合季节性-趋势分解，提出针对季节性和趋势成分的新型掩码策略，并引入对比学习任务增强上下文一致性。

Result: 实验表明ST-MTM在预测性能上优于现有掩码建模、对比学习和监督预测方法。

Conclusion: ST-MTM通过分解和对比学习有效捕捉了复杂时序语义，提升了预测性能。

Abstract: Forecasting complex time series is an important yet challenging problem that
involves various industrial applications. Recently, masked time-series modeling
has been proposed to effectively model temporal dependencies for forecasting by
reconstructing masked segments from unmasked ones. However, since the semantic
information in time series is involved in intricate temporal variations
generated by multiple time series components, simply masking a raw time series
ignores the inherent semantic structure, which may cause MTM to learn spurious
temporal patterns present in the raw data. To capture distinct temporal
semantics, we show that masked modeling techniques should address entangled
patterns through a decomposition approach. Specifically, we propose ST-MTM, a
masked time-series modeling framework with seasonal-trend decomposition, which
includes a novel masking method for the seasonal-trend components that
incorporates different temporal variations from each component. ST-MTM uses a
period masking strategy for seasonal components to produce multiple masked
seasonal series based on inherent multi-periodicity and a sub-series masking
strategy for trend components to mask temporal regions that share similar
variations. The proposed masking method presents an effective pre-training task
for learning intricate temporal variations and dependencies. Additionally,
ST-MTM introduces a contrastive learning task to support masked modeling by
enhancing contextual consistency among multiple masked seasonal
representations. Experimental results show that our proposed ST-MTM achieves
consistently superior forecasting performance compared to existing masked
modeling, contrastive learning, and supervised forecasting methods.

</details>


### [291] [Vision Transformer with Adversarial Indicator Token against Adversarial Attacks in Radio Signal Classifications](https://arxiv.org/abs/2507.00015)
*Lu Zhang,Sangarapillai Lambotharan,Gan Zheng,Guisheng Liao,Xuekang Liu,Fabio Roli,Carsten Maple*

Main category: cs.LG

Relevance: 40.0

TL;DR: 提出了一种新的视觉Transformer架构，通过引入对抗指示器（AdvI）令牌来检测对抗攻击，结合对抗训练和检测机制，提升了模型在对抗攻击下的鲁棒性。


<details>
  <summary>Details</summary>
Motivation: Transformer在自动调制分类中的应用易受对抗攻击，需要一种防御策略来提升其安全性。

Method: 提出AdvI令牌的ViT架构，结合对抗训练和检测机制，通过注意力机制分析其工作原理。

Result: 实验表明，该方法在处理白盒攻击场景中优于多种竞争方法。

Conclusion: AdvI令牌在ViT中有效检测对抗攻击，减少了系统复杂性并提升了防御效果。

Abstract: The remarkable success of transformers across various fields such as natural
language processing and computer vision has paved the way for their
applications in automatic modulation classification, a critical component in
the communication systems of Internet of Things (IoT) devices. However, it has
been observed that transformer-based classification of radio signals is
susceptible to subtle yet sophisticated adversarial attacks. To address this
issue, we have developed a defensive strategy for transformer-based modulation
classification systems to counter such adversarial attacks. In this paper, we
propose a novel vision transformer (ViT) architecture by introducing a new
concept known as adversarial indicator (AdvI) token to detect adversarial
attacks. To the best of our knowledge, this is the first work to propose an
AdvI token in ViT to defend against adversarial attacks. Integrating an
adversarial training method with a detection mechanism using AdvI token, we
combine a training time defense and running time defense in a unified neural
network model, which reduces architectural complexity of the system compared to
detecting adversarial perturbations using separate models. We investigate into
the operational principles of our method by examining the attention mechanism.
We show the proposed AdvI token acts as a crucial element within the ViT,
influencing attention weights and thereby highlighting regions or features in
the input data that are potentially suspicious or anomalous. Through
experimental results, we demonstrate that our approach surpasses several
competitive methods in handling white-box attack scenarios, including those
utilizing the fast gradient method, projected gradient descent attacks and
basic iterative method.

</details>


### [292] [AIMatDesign: Knowledge-Augmented Reinforcement Learning for Inverse Materials Design under Data Scarcity](https://arxiv.org/abs/2507.00024)
*Yeyong Yu,Xilei Bian,Jie Xiong,Xing Wu,Quan Qian*

Main category: cs.LG

Relevance: 40.0

TL;DR: AIMatDesign是一个强化学习框架，通过结合实验数据和领域专家知识，优化材料逆向设计。


<details>
  <summary>Details</summary>
Motivation: 解决高维材料组成空间中机器学习模型的不可靠性和缺乏专家知识的问题。

Method: 使用强化学习框架，结合基于差异的算法和大型语言模型（LLMs）进行自动修正。

Result: 在发现效率、收敛速度和成功率上显著优于传统方法，实验验证了其可靠性。

Conclusion: AIMatDesign展示了在闭环材料发现中的潜力。

Abstract: With the growing demand for novel materials, machine learning-driven inverse
design methods face significant challenges in reconciling the high-dimensional
materials composition space with limited experimental data. Existing approaches
suffer from two major limitations: (I) machine learning models often lack
reliability in high-dimensional spaces, leading to prediction biases during the
design process; (II) these models fail to effectively incorporate domain expert
knowledge, limiting their capacity to support knowledge-guided inverse design.
To address these challenges, we introduce AIMatDesign, a reinforcement learning
framework that addresses these limitations by augmenting experimental data
using difference-based algorithms to build a trusted experience pool,
accelerating model convergence. To enhance model reliability, an automated
refinement strategy guided by large language models (LLMs) dynamically corrects
prediction inconsistencies, reinforcing alignment between reward signals and
state value functions. Additionally, a knowledge-based reward function
leverages expert domain rules to improve stability and efficiency during
training. Our experiments demonstrate that AIMatDesign significantly surpasses
traditional machine learning and reinforcement learning methods in discovery
efficiency, convergence speed, and success rates. Among the numerous candidates
proposed by AIMatDesign, experimental synthesis of representative Zr-based
alloys yielded a top-performing BMG with 1.7GPa yield strength and 10.2\%
elongation, closely matching predictions. Moreover, the framework accurately
captured the trend of yield strength variation with composition, demonstrating
its reliability and potential for closed-loop materials discovery.

</details>


### [293] [Generalizing to New Dynamical Systems via Frequency Domain Adaptation](https://arxiv.org/abs/2507.00025)
*Tiexin Qin,Hong Yan,Haoliang Li*

Main category: cs.LG

Relevance: 40.0

TL;DR: 提出了一种参数高效的方法FNSDA，通过傅里叶空间的自适应泛化到新动态系统。


<details>
  <summary>Details</summary>
Motivation: 当前方法在特定领域内预测可靠性和泛化到未见系统方面存在局限。

Method: 基于傅里叶模式自动分区，学习调整特定环境的模式，利用低维潜在系统参数进行高效泛化。

Result: 在四个动态系统家族上表现优于或与现有方法相当，且参数成本显著降低。

Conclusion: FNSDA在动态系统建模中实现了高效泛化。

Abstract: Learning the underlying dynamics from data with deep neural networks has
shown remarkable potential in modeling various complex physical dynamics.
However, current approaches are constrained in their ability to make reliable
predictions in a specific domain and struggle with generalizing to unseen
systems that are governed by the same general dynamics but differ in
environmental characteristics. In this work, we formulate a parameter-efficient
method, Fourier Neural Simulator for Dynamical Adaptation (FNSDA), that can
readily generalize to new dynamics via adaptation in the Fourier space.
Specifically, FNSDA identifies the shareable dynamics based on the known
environments using an automatic partition in Fourier modes and learns to adjust
the modes specific for each new environment by conditioning on low-dimensional
latent systematic parameters for efficient generalization. We evaluate our
approach on four representative families of dynamic systems, and the results
show that FNSDA can achieve superior or competitive generalization performance
compared to existing methods with a significantly reduced parameter cost. Our
code is available at https://github.com/WonderSeven/FNSDA.

</details>


### [294] [Pattern-Based Graph Classification: Comparison of Quality Measures and Importance of Preprocessing](https://arxiv.org/abs/2507.00039)
*Lucas Potin,Rosa Figueiredo,Vincent Labatut,Christine Largeron*

Main category: cs.LG

Relevance: 40.0

TL;DR: 该论文对图分类中的38种质量度量进行了理论和实证比较，提出了基于聚类的预处理方法以提高分类性能，并发现某些广泛使用的度量效果不佳。


<details>
  <summary>Details</summary>
Motivation: 图分类中质量度量的选择缺乏系统评估，导致广泛使用的度量可能并非最优。

Method: 理论分析38种质量度量的数学性质，构建基准数据集和黄金标准排名，提出基于聚类的预处理方法。

Result: 实验表明预处理方法有效减少模式数量且性能相当，某些流行度量效果不佳。

Conclusion: 需更谨慎选择质量度量，聚类预处理方法有潜力。

Abstract: Graph classification aims to categorize graphs based on their structural and
attribute features, with applications in diverse fields such as social network
analysis and bioinformatics. Among the methods proposed to solve this task,
those relying on patterns (i.e. subgraphs) provide good explainability, as the
patterns used for classification can be directly interpreted. To identify
meaningful patterns, a standard approach is to use a quality measure, i.e. a
function that evaluates the discriminative power of each pattern. However, the
literature provides tens of such measures, making it difficult to select the
most appropriate for a given application. Only a handful of surveys try to
provide some insight by comparing these measures, and none of them specifically
focuses on graphs. This typically results in the systematic use of the most
widespread measures, without thorough evaluation. To address this issue, we
present a comparative analysis of 38 quality measures from the literature. We
characterize them theoretically, based on four mathematical properties. We
leverage publicly available datasets to constitute a benchmark, and propose a
method to elaborate a gold standard ranking of the patterns. We exploit these
resources to perform an empirical comparison of the measures, both in terms of
pattern ranking and classification performance. Moreover, we propose a
clustering-based preprocessing step, which groups patterns appearing in the
same graphs to enhance classification performance. Our experimental results
demonstrate the effectiveness of this step, reducing the number of patterns to
be processed while achieving comparable performance. Additionally, we show that
some popular measures widely used in the literature are not associated with the
best results.

</details>


### [295] [Leveraging Unlabeled Audio-Visual Data in Speech Emotion Recognition using Knowledge Distillation](https://arxiv.org/abs/2507.00055)
*Varsha Pendyala,Pedro Morgado,William Sethares*

Main category: cs.LG

Relevance: 40.0

TL;DR: 该论文提出了一种名为LiSER的知识蒸馏框架，利用未标记的音频-视觉数据，通过大型教师模型训练轻量级学生模型，以减少对标记数据的依赖。


<details>
  <summary>Details</summary>
Motivation: 语音情感识别（SER）在多模态人机交互中具有重要作用，但获取大量标记数据成本高昂。因此，研究旨在通过知识蒸馏减少对标记数据的依赖。

Method: 提出LiSER框架，利用基于先进语音和面部表示模型的大型教师模型，将知识转移到轻量级学生模型。

Result: 在RAVDESS和CREMA-D数据集上的实验表明，LiSER能够减少对标记数据的依赖。

Conclusion: LiSER为多模态情感识别提供了一种高效的解决方案，降低了数据标注成本。

Abstract: Voice interfaces integral to the human-computer interaction systems can
benefit from speech emotion recognition (SER) to customize responses based on
user emotions. Since humans convey emotions through multi-modal audio-visual
cues, developing SER systems using both the modalities is beneficial. However,
collecting a vast amount of labeled data for their development is expensive.
This paper proposes a knowledge distillation framework called LightweightSER
(LiSER) that leverages unlabeled audio-visual data for SER, using large teacher
models built on advanced speech and face representation models. LiSER transfers
knowledge regarding speech emotions and facial expressions from the teacher
models to lightweight student models. Experiments conducted on two benchmark
datasets, RAVDESS and CREMA-D, demonstrate that LiSER can reduce the dependence
on extensive labeled datasets for SER tasks.

</details>


### [296] [pUniFind: a unified large pre-trained deep learning model pushing the limit of mass spectra interpretation](https://arxiv.org/abs/2507.00087)
*Jiale Zhao,Pengzhi Mao,Kaifei Wang,Yiming Li,Yaping Peng,Ranfei Chen,Shuqi Lu,Xiaohong Ji,Jiaxiang Ding,Xin Zhang,Yucheng Liao,Weinan E,Weijie Zhang,Han Wen,Hao Chi*

Main category: cs.LG

Relevance: 40.0

TL;DR: pUniFind是一个大规模多模态预训练模型，用于蛋白质组学分析，整合了端到端的肽段-谱图评分和零样本从头测序，显著提高了肽段识别数量和修饰覆盖范围。


<details>
  <summary>Details</summary>
Motivation: 当前深度学习模型在质谱数据分析中多为特征提取器，缺乏统一的评分框架。pUniFind旨在填补这一空白，提供更灵敏、更全面的蛋白质组学分析工具。

Method: pUniFind通过跨模态预测对齐谱图和肽段模态，训练基于超过1亿个开放搜索衍生的谱图。模型支持超过1,300种修饰，并包含基于深度学习的质量控制模块。

Result: pUniFind在免疫肽组学中肽段识别数量提高了42.6%，比现有从头测序方法多识别60%的PSM（搜索空间大300倍）。质量控制模块额外恢复了38.5%的肽段。

Conclusion: pUniFind为蛋白质组学分析提供了一个统一、可扩展的深度学习框架，显著提升了灵敏度、修饰覆盖范围和可解释性。

Abstract: Deep learning has advanced mass spectrometry data interpretation, yet most
models remain feature extractors rather than unified scoring frameworks. We
present pUniFind, the first large-scale multimodal pre-trained model in
proteomics that integrates end-to-end peptide-spectrum scoring with open,
zero-shot de novo sequencing. Trained on over 100 million open search-derived
spectra, pUniFind aligns spectral and peptide modalities via cross modality
prediction and outperforms traditional engines across diverse datasets,
particularly achieving a 42.6 percent increase in the number of identified
peptides in immunopeptidomics. Supporting over 1,300 modifications, pUniFind
identifies 60 percent more PSMs than existing de novo methods despite a
300-fold larger search space. A deep learning based quality control module
further recovers 38.5 percent additional peptides including 1,891 mapped to the
genome but absent from reference proteomes while preserving full fragment ion
coverage. These results establish a unified, scalable deep learning framework
for proteomic analysis, offering improved sensitivity, modification coverage,
and interpretability.

</details>


### [297] [Text-to-Level Diffusion Models With Various Text Encoders for Super Mario Bros](https://arxiv.org/abs/2507.00184)
*Jacob Schrum,Olivia Kilday,Emilio Salas,Bess Hagan,Reid Williams*

Main category: cs.LG

Relevance: 40.0

TL;DR: 论文探讨了扩散模型在文本到游戏关卡生成中的应用，提出了自动标注和训练策略，并比较了不同模型的性能。


<details>
  <summary>Details</summary>
Motivation: 研究扩散模型在文本到关卡生成中的潜力，解决现有方法在标注、训练和生成完整关卡方面的不足。

Method: 使用预训练文本编码器和简单Transformer模型训练扩散模型，自动标注关卡数据集，并评估生成关卡的多样性和可玩性。

Result: 最佳扩散模型采用简单Transformer文本嵌入，训练时间短于复杂编码器模型，且不依赖大型语言模型。

Conclusion: 扩散模型在文本到关卡生成中表现优异，简单模型即可实现高效训练和生成。

Abstract: Recent research shows how diffusion models can unconditionally generate
tile-based game levels, but use of diffusion models for text-to-level
generation is underexplored. There are practical considerations for creating a
usable model: caption/level pairs are needed, as is a text embedding model, and
a way of generating entire playable levels, rather than individual scenes. We
present strategies to automatically assign descriptive captions to an existing
level dataset, and train diffusion models using both pretrained text encoders
and simple transformer models trained from scratch. Captions are automatically
assigned to generated levels so that the degree of overlap between input and
output captions can be compared. We also assess the diversity and playability
of the resulting levels. Results are compared with an unconditional diffusion
model and a generative adversarial network, as well as the text-to-level
approaches Five-Dollar Model and MarioGPT. Notably, the best diffusion model
uses a simple transformer model for text embedding, and takes less time to
train than diffusion models employing more complex text encoders, indicating
that reliance on larger language models is not necessary. We also present a GUI
allowing designers to construct long levels from model-generated scenes.

</details>


### [298] [Beyond Sensor Data: Foundation Models of Behavioral Data from Wearables Improve Health Predictions](https://arxiv.org/abs/2507.00191)
*Eray Erturk,Fahad Kamran,Salar Abbaspourazad,Sean Jewell,Harsh Sharma,Yujie Li,Sinead Williamson,Nicholas J Foti,Joseph Futoma*

Main category: cs.LG

Relevance: 40.0

TL;DR: 该论文开发了一种针对可穿戴设备行为信号的基础模型，优化了架构和标记化策略，并在57项健康相关任务中表现出色。


<details>
  <summary>Details</summary>
Motivation: 尽管行为数据在生理相关时间尺度和数量上更具信息性，但基础模型主要应用于低级传感器数据。因此，研究团队旨在开发专门针对可穿戴设备行为信号的基础模型。

Method: 使用来自162K个体的2.5B小时可穿戴数据，系统优化了架构和标记化策略。

Result: 模型在57项健康任务中表现优异，尤其在行为驱动任务（如睡眠预测）中表现突出，结合原始传感器数据后性能进一步提升。

Conclusion: 研究表明，为基础模型设计专门针对可穿戴设备的策略至关重要，并展示了其在健康应用中的潜力。

Abstract: Wearable devices record physiological and behavioral signals that can improve
health predictions. While foundation models are increasingly used for such
predictions, they have been primarily applied to low-level sensor data, despite
behavioral data often being more informative due to their alignment with
physiologically relevant timescales and quantities. We develop foundation
models of such behavioral signals using over 2.5B hours of wearable data from
162K individuals, systematically optimizing architectures and tokenization
strategies for this unique dataset. Evaluated on 57 health-related tasks, our
model shows strong performance across diverse real-world applications including
individual-level classification and time-varying health state prediction. The
model excels in behavior-driven tasks like sleep prediction, and improves
further when combined with representations of raw sensor data. These results
underscore the importance of tailoring foundation model design to wearables and
demonstrate the potential to enable new health applications.

</details>


### [299] [What Makes Local Updates Effective: The Role of Data Heterogeneity and Smoothness](https://arxiv.org/abs/2507.00195)
*Kumar Kshitij Patel*

Main category: cs.LG

Relevance: 40.0

TL;DR: 该论文深入研究了分布式和联邦优化中的本地更新算法（如Local SGD），在数据异构性假设下，证明了其优于集中式或小批量方法的条件，并提出了新的分析框架。


<details>
  <summary>Details</summary>
Motivation: 研究动机在于理解在数据异构性环境下，本地更新算法（如Local SGD）何时以及为何能提供可证明的优势。

Method: 通过有界的二阶异构性假设，结合精细的共识误差分析框架，推导了凸和非凸设置下的紧致上下界。

Result: 论文确立了多种本地更新算法的紧致上下界，并扩展到了在线联邦学习，提供了基本遗憾界。

Conclusion: 研究明确了本地更新算法在异构环境中的优势条件，为分析Local SGD提供了自包含的指南。

Abstract: This thesis contributes to the theoretical understanding of local update
algorithms, especially Local SGD, in distributed and federated optimization
under realistic models of data heterogeneity. A central focus is on the bounded
second-order heterogeneity assumption, which is shown to be both necessary and
sufficient for local updates to outperform centralized or mini-batch methods in
convex and non-convex settings. The thesis establishes tight upper and lower
bounds in several regimes for various local update algorithms and characterizes
the min-max complexity of multiple problem classes. At its core is a
fine-grained consensus-error-based analysis framework that yields sharper
finite-time convergence bounds under third-order smoothness and relaxed
heterogeneity assumptions. The thesis also extends to online federated
learning, providing fundamental regret bounds under both first-order and bandit
feedback. Together, these results clarify when and why local updates offer
provable advantages, and the thesis serves as a self-contained guide for
analyzing Local SGD in heterogeneous environments.

</details>


### [300] [Who Should I Listen To? Adaptive Collaboration in Personalized Federated Learning](https://arxiv.org/abs/2507.00259)
*Amr Abourayya,Jens Kleesiek,Bharat Rao,Michael Kamp*

Main category: cs.LG

Relevance: 40.0

TL;DR: FEDMOSAIC是一种基于自适应协作的个性化联邦学习方法，通过客户端在共享未标记数据集上交换预测，实现细粒度的信任决策，优于现有PFL方法。


<details>
  <summary>Details</summary>
Motivation: 解决联邦学习中数据异构性的挑战，现有PFL方法在协作结构与数据不匹配时表现不佳。

Method: 提出FEDMOSAIC，客户端自适应决定依赖他人程度及信任对象，通过共享未标记数据集交换预测，调整损失权重和贡献全局伪标签。

Result: 在多种非独立同分布设置中优于现有PFL方法，并提供收敛性保证。

Conclusion: 数据感知协作具有实现鲁棒和有效个性化的潜力。

Abstract: Data heterogeneity is a central challenge in federated learning, and
personalized federated learning (PFL) aims to address it by tailoring models to
each client's distribution. Yet many PFL methods fail to outperform local or
centralized baselines, suggesting a mismatch between the collaboration they
enforce and the structure of the data. We propose an approach based on adaptive
collaboration, where clients decide adaptively not only how much to rely on
others, but also whom to trust at the level of individual examples. We
instantiate this principle in FEDMOSAIC, a federated co-training method in
which clients exchange predictions over a shared unlabeled dataset. This
enables fine-grained trust decisions that are difficult to achieve with
parameter sharing alone. Each client adjusts its loss weighting based on the
agreement between private and public data, and contributes to global
pseudo-labels in proportion to its estimated per-example confidence.
Empirically, FEDMOSAIC improves upon state-of-the-art PFL methods across
diverse non-IID settings, and we provide convergence guarantees under standard
assumptions. Our results demonstrate the potential of data-aware collaboration
for robust and effective personalization.

</details>


### [301] [Examining Reject Relations in Stimulus Equivalence Simulations](https://arxiv.org/abs/2507.00265)
*Alexis Carrillo,Asieh Abolpour Mofrad,Anis Yazidi,Moises Betancort*

Main category: cs.LG

Relevance: 40.0

TL;DR: 该研究通过计算模型探讨了拒绝关系在刺激等价（SE）形成中的作用，发现人工神经网络（包括Transformer模型）可能依赖关联学习而非SE。


<details>
  <summary>Details</summary>
Motivation: 研究动机是探索拒绝关系是否会影响刺激等价的形成，以及人工神经网络是否能真正表现出等价类形成。

Method: 使用前馈神经网络（FFN）、BERT和GPT模型，在18种条件下进行匹配到样本（MTS）模拟，比较不同训练结构和关系类型的效果。

Result: 结果显示，拒绝关系影响模型表现，但高准确率可能与关联学习策略相关，而非真正的等价类形成。

Conclusion: 结论指出，计算模型需要更严格的标准来区分关联学习和等价类形成。

Abstract: Simulations offer a valuable tool for exploring stimulus equivalence (SE),
yet the potential of reject relations to disrupt the assessment of equivalence
class formation is contentious. This study investigates the role of reject
relations in the acquisition of stimulus equivalence using computational
models. We examined feedforward neural networks (FFNs), bidirectional encoder
representations from transformers (BERT), and generative pre-trained
transformers (GPT) across 18 conditions in matching-to-sample (MTS)
simulations. Conditions varied in training structure (linear series,
one-to-many, and many-to-one), relation type (select-only, reject-only, and
select-reject), and negative comparison selection (standard and biased). A
probabilistic agent served as a benchmark, embodying purely associative
learning. The primary goal was to determine whether artificial neural networks
could demonstrate equivalence class formation or whether their performance
reflected associative learning. Results showed that reject relations influenced
agent performance. While some agents achieved high accuracy on equivalence
tests, particularly with reject relations and biased negative comparisons, this
performance was comparable to the probabilistic agent. These findings suggest
that artificial neural networks, including transformer models, may rely on
associative strategies rather than SE. This underscores the need for careful
consideration of reject relations and more stringent criteria in computational
models of equivalence.

</details>


### [302] [$μ^2$Tokenizer: Differentiable Multi-Scale Multi-Modal Tokenizer for Radiology Report Generation](https://arxiv.org/abs/2507.00316)
*Siyou Li,Pengyao Qin,Huanan Wu,Dong Nie,Arun J. Thirunavukarasu,Juntao Yu,Le Zhang*

Main category: cs.LG

Relevance: 40.0

TL;DR: 论文提出了一种多尺度多模态大语言模型（μ²LLM）用于放射学报告生成（RRG），通过μ²Tokenizer整合多模态特征，并利用直接偏好优化（DPO）提升报告质量，实验表明其在有限数据下优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 放射学报告生成（RRG）面临从复杂影像数据中提取信息及客观评估模型生成报告与专家报告差异的挑战，需要高效解决方案。

Method: 提出μ²LLM，结合多尺度视觉分词器和文本分词器，通过μ²Tokenizer整合多模态特征，并采用DPO优化报告生成。

Result: 在四个大型CT数据集上的实验表明，μ²LLM在有限数据下优于现有方法。

Conclusion: μ²LLM展示了在有限数据下高效生成高质量放射学报告的潜力。

Abstract: Automated radiology report generation (RRG) aims to produce detailed textual
reports from clinical imaging, such as computed tomography (CT) scans, to
improve the accuracy and efficiency of diagnosis and provision of management
advice. RRG is complicated by two key challenges: (1) inherent complexity in
extracting relevant information from imaging data under resource constraints,
and (2) difficulty in objectively evaluating discrepancies between
model-generated and expert-written reports. To address these challenges, we
propose $\mu^2$LLM, a $\underline{\textbf{mu}}$ltiscale
$\underline{\textbf{mu}}$ltimodal large language models for RRG tasks. The
novel ${\mu}^2$Tokenizer, as an intermediate layer, integrates multi-modal
features from the multiscale visual tokenizer and the text tokenizer, then
enhances report generation quality through direct preference optimization
(DPO), guided by GREEN-RedLlama. Experimental results on four large CT
image-report medical datasetdemonstrate that our method outperforms existing
approaches, highlighting the potential of our fine-tuned $\mu^2$LLMs on limited
data for RRG tasks.

</details>


### [303] [Diffusion Disambiguation Models for Partial Label Learning](https://arxiv.org/abs/2507.00411)
*Jinfu Fan,Xiaohui Zhong,Kangrui Ren,Jiangnan Li,Linqing Huang*

Main category: cs.LG

Relevance: 40.0

TL;DR: 该论文提出了一种基于扩散模型的标签去歧方法（DDMP），用于部分标签学习（PLL），通过生成模型视角解决标签模糊问题。


<details>
  <summary>Details</summary>
Motivation: 部分标签学习（PLL）在实际应用中存在标签模糊问题，传统方法难以有效解决。扩散模型在生成任务中表现优异，因此探索其在标签去噪中的潜力。

Method: 通过生成模型视角重新定义标签去歧问题，利用扩散模型逐步优化初始随机猜测的标签。提出DDMP方法，利用实例和标签的互补信息构建伪干净标签，并引入动态更新的转移感知矩阵估计真实标签。

Result: 实验表明，DDMP在部分标签学习中具有优势，能够有效提高分类器的性能。

Conclusion: DDMP通过扩散模型和动态标签估计，成功解决了PLL中的标签模糊问题，为生成模型在标签去噪中的应用提供了新思路。

Abstract: Learning from ambiguous labels is a long-standing problem in practical
machine learning applications. The purpose of \emph{partial label learning}
(PLL) is to identify the ground-truth label from a set of candidate labels
associated with a given instance. Inspired by the remarkable performance of
diffusion models in various generation tasks, this paper explores their
potential to denoise ambiguous labels through the reverse denoising process.
Therefore, this paper reformulates the label disambiguation problem from the
perspective of generative models, where labels are generated by iteratively
refining initial random guesses. This perspective enables the diffusion model
to learn how label information is generated stochastically. By modeling the
generation uncertainty, we can use the maximum likelihood estimate of the label
for classification inference. However, such ambiguous labels lead to a mismatch
between instance and label, which reduces the quality of generated data. To
address this issue, this paper proposes a \emph{diffusion disambiguation model
for PLL} (DDMP), which first uses the potential complementary information
between instances and labels to construct pseudo-clean labels for initial
diffusion training. Furthermore, a transition-aware matrix is introduced to
estimate the potential ground-truth labels, which are dynamically updated
during the diffusion generation. During training, the ground-truth label is
progressively refined, improving the classifier. Experiments show the advantage
of the DDMP and its suitability for PLL.

</details>


### [304] [A Recipe for Causal Graph Regression: Confounding Effects Revisited](https://arxiv.org/abs/2507.00440)
*Yujia Yin,Tianyi Qu,Zihao Wang,Yifan Chen*

Main category: cs.LG

Relevance: 40.0

TL;DR: 该论文提出了一种因果图回归（CGR）方法，通过改进现有因果图学习（CGL）技术，解决了图学习中更具挑战性的回归任务，并利用对比学习框架提升了模型的泛化能力。


<details>
  <summary>Details</summary>
Motivation: 现有的因果图学习技术主要关注分类任务，而回归任务在图学习中更具挑战性且被忽视。本文旨在填补这一空白，改进因果图学习在回归任务中的应用。

Method: 通过重新处理混杂效应，将分类任务中的因果干预技术推广到回归任务中，并结合对比学习框架。

Result: 在图的OOD基准测试中，实验验证了所提CGR方法的有效性。

Conclusion: 该研究为图学习中的回归任务提供了一种有效的因果图学习方法，并展示了其在实际应用中的潜力。

Abstract: Through recognizing causal subgraphs, causal graph learning (CGL) has risen
to be a promising approach for improving the generalizability of graph neural
networks under out-of-distribution (OOD) scenarios. However, the empirical
successes of CGL techniques are mostly exemplified in classification settings,
while regression tasks, a more challenging setting in graph learning, are
overlooked. We thus devote this work to tackling causal graph regression (CGR);
to this end we reshape the processing of confounding effects in existing CGL
studies, which mainly deal with classification. Specifically, we reflect on the
predictive power of confounders in graph-level regression, and generalize
classification-specific causal intervention techniques to regression through a
lens of contrastive learning. Extensive experiments on graph OOD benchmarks
validate the efficacy of our proposals for CGR. The model implementation and
the code are provided on https://github.com/causal-graph/CGR.

</details>


### [305] [Iterative Distillation for Reward-Guided Fine-Tuning of Diffusion Models in Biomolecular Design](https://arxiv.org/abs/2507.00445)
*Xingyu Su,Xiner Li,Masatoshi Uehara,Sunwoo Kim,Yulai Zhao,Gabriele Scalia,Ehsan Hajiramezanali,Tommaso Biancalani,Degui Zhi,Shuiwang Ji*

Main category: cs.LG

Relevance: 40.0

TL;DR: 提出了一种基于迭代蒸馏的微调框架，用于扩散模型的奖励引导生成，解决了现有RL方法的不稳定性和样本效率低的问题。


<details>
  <summary>Details</summary>
Motivation: 现实应用中需要扩散模型不仅能生成高保真数据，还需优化不可微奖励函数（如物理模拟或科学知识奖励）。现有RL方法存在不稳定性和样本效率低的问题。

Method: 采用迭代蒸馏框架，通过收集离线数据、模拟奖励驱动的软最优策略，并最小化KL散度来更新模型。

Result: 在蛋白质、小分子和调控DNA设计中，该方法表现出更高的奖励优化效果和稳定性。

Conclusion: 该方法显著提升了扩散模型在奖励引导生成任务中的性能，优于现有RL方法。

Abstract: We address the problem of fine-tuning diffusion models for reward-guided
generation in biomolecular design. While diffusion models have proven highly
effective in modeling complex, high-dimensional data distributions, real-world
applications often demand more than high-fidelity generation, requiring
optimization with respect to potentially non-differentiable reward functions
such as physics-based simulation or rewards based on scientific knowledge.
Although RL methods have been explored to fine-tune diffusion models for such
objectives, they often suffer from instability, low sample efficiency, and mode
collapse due to their on-policy nature. In this work, we propose an iterative
distillation-based fine-tuning framework that enables diffusion models to
optimize for arbitrary reward functions. Our method casts the problem as policy
distillation: it collects off-policy data during the roll-in phase, simulates
reward-based soft-optimal policies during roll-out, and updates the model by
minimizing the KL divergence between the simulated soft-optimal policy and the
current model policy. Our off-policy formulation, combined with KL divergence
minimization, enhances training stability and sample efficiency compared to
existing RL-based methods. Empirical results demonstrate the effectiveness and
superior reward optimization of our approach across diverse tasks in protein,
small molecule, and regulatory DNA design.

</details>


### [306] [Best Agent Identification for General Game Playing](https://arxiv.org/abs/2507.00451)
*Matthew Stephenson,Alex Newcombe,Eric Piette,Dennis Soemers*

Main category: cs.LG

Relevance: 40.0

TL;DR: 提出了一种基于Wilson得分区间的乐观选择方法（Optimistic-WS），用于在多任务领域中高效识别每个子任务的最佳算法。


<details>
  <summary>Details</summary>
Motivation: 解决在多任务领域中快速准确识别最佳算法的问题，特别是在算法运行时间较长的情况下。

Method: 将问题建模为多臂老虎机中的最佳臂识别问题，提出Optimistic-WS方法，基于Wilson得分区间对算法进行乐观排序。

Result: 在GVGAI和Ludii游戏框架中，Optimistic-WS显著降低了平均简单遗憾，优于现有方法。

Conclusion: Optimistic-WS能显著提升多任务领域中算法评估的质量和准确性。

Abstract: We present an efficient and generalised procedure to accurately identify the
best performing algorithm for each sub-task in a multi-problem domain. Our
approach treats this as a set of best arm identification problems for
multi-armed bandits, where each bandit corresponds to a specific task and each
arm corresponds to a specific algorithm or agent. We propose an optimistic
selection process based on the Wilson score interval (Optimistic-WS) that ranks
each arm across all bandits in terms of their potential regret reduction. We
evaluate the performance of Optimistic-WS on two of the most popular general
game domains, the General Video Game AI (GVGAI) framework and the Ludii general
game playing system, with the goal of identifying the highest performing agent
for each game within a limited number of trials. Compared to previous best arm
identification algorithms for multi-armed bandits, our results demonstrate a
substantial performance improvement in terms of average simple regret. This
novel approach can be used to significantly improve the quality and accuracy of
agent evaluation procedures for general game frameworks, as well as other
multi-task domains with high algorithm runtimes.

</details>


### [307] [Posterior Inference in Latent Space for Scalable Constrained Black-box Optimization](https://arxiv.org/abs/2507.00480)
*Kiyoung Om,Kyuil Sim,Taeyoung Yun,Hyeongyu Kang,Jinkyoo Park*

Main category: cs.LG

Relevance: 40.0

TL;DR: 提出了一种基于生成模型和流模型的新框架，用于解决高维黑盒约束优化问题，克服了传统方法的维度灾难和模式崩溃问题。


<details>
  <summary>Details</summary>
Motivation: 高维黑盒约束优化问题在科学和工程中普遍存在，但传统方法（如贝叶斯优化）难以应对维度灾难和模式崩溃。

Method: 通过两阶段迭代：1) 训练流模型捕捉数据分布和代理模型预测函数值与约束违反；2) 将候选选择问题转化为后验推断问题，利用流模型的潜在空间平滑性解决多模态问题。

Result: 在合成和真实世界的约束黑盒优化任务中表现优异。

Conclusion: 该方法为高维约束优化提供了一种高效且可扩展的解决方案。

Abstract: Optimizing high-dimensional black-box functions under black-box constraints
is a pervasive task in a wide range of scientific and engineering problems.
These problems are typically harder than unconstrained problems due to
hard-to-find feasible regions. While Bayesian optimization (BO) methods have
been developed to solve such problems, they often struggle with the curse of
dimensionality. Recently, generative model-based approaches have emerged as a
promising alternative for constrained optimization. However, they suffer from
poor scalability and are vulnerable to mode collapse, particularly when the
target distribution is highly multi-modal. In this paper, we propose a new
framework to overcome these challenges. Our method iterates through two stages.
First, we train flow-based models to capture the data distribution and
surrogate models that predict both function values and constraint violations
with uncertainty quantification. Second, we cast the candidate selection
problem as a posterior inference problem to effectively search for promising
candidates that have high objective values while not violating the constraints.
During posterior inference, we find that the posterior distribution is highly
multi-modal and has a large plateau due to constraints, especially when
constraint feedback is given as binary indicators of feasibility. To mitigate
this issue, we amortize the sampling from the posterior distribution in the
latent space of flow-based models, which is much smoother than that in the data
space. We empirically demonstrate that our method achieves superior performance
on various synthetic and real-world constrained black-box optimization tasks.
Our code is publicly available \href{https://github.com/umkiyoung/CiBO}{here}.

</details>


### [308] [Foundation Models for Clinical Records at Health System Scale](https://arxiv.org/abs/2507.00574)
*Haresh Rengaraj Rajamohan,Xiang Gao,Weicheng Zhu,Shih-Lun Huang,Long Chen,Kyunghyun Cho,Cem M. Deniz,Narges Razavian*

Main category: cs.LG

Relevance: 40.0

TL;DR: 论文提出了一种针对结构化电子健康记录（EHR）的生成式预训练策略，通过预测下次访问事件来学习生成临床事件。模型在零样本预测任务中表现优异，无需任务特定微调。


<details>
  <summary>Details</summary>
Motivation: 探索大规模预训练在医疗健康领域的潜力，特别是针对结构化EHR数据的建模。

Method: 采用自回归生成模型预测下次访问事件，并引入对重复事件的预测正则化。

Result: 模型在预测痴呆和膝骨关节炎发病率的任务中表现与完全微调的Transformer基线相当。

Conclusion: 该方法能够捕捉复杂的临床依赖关系，无需昂贵的任务特定微调。

Abstract: Large-scale pretraining has transformed modeling of language and other data
types, but its potential remains underexplored in healthcare with structured
electronic health records (EHRs). We present a novel generative pretraining
strategy for sequential EHR data using next-visit event prediction. Our model
learns to autoregressively generate various tokenized clinical events for the
next visit based on patient history and inherently handles the joint prediction
of heterogeneous data types. Additionally, we introduce regularization on
predicting repeated events and highlight a key pitfall in EHR-based foundation
model evaluations: repeated event tokens can inflate performance metrics when
new onsets are not distinguished from subsequent occurrences. Our model is
evaluated via zero-shot prediction for forecasting dementia and knee
osteoarthritis incidence within 2 and 5 years, and the model performance rivals
a fully fine-tuned masked pretrained Transformer baseline, demonstrating that
our approach captures complex clinical dependencies without requiring costly
task-specific fine-tuning.

</details>


### [309] [Quantum Circuit Structure Optimization for Quantum Reinforcement Learning](https://arxiv.org/abs/2507.00589)
*Seok Bin Son,Joongheon Kim*

Main category: cs.LG

Relevance: 40.0

TL;DR: 本文提出了一种结合量子神经架构搜索（QNAS）的量子强化学习算法（QRL-NAS），以优化量子强化学习中的参数化量子电路（PQC）结构，实验证明其优于固定电路的方法。


<details>
  <summary>Details</summary>
Motivation: 传统强化学习在高维空间中效率低下，量子强化学习（QRL）利用量子计算的叠加和纠缠特性提高效率，但现有研究使用的固定PQC结构未经优化。

Method: 提出QRL-NAS算法，结合QNAS优化PQC结构，通过实验验证其性能。

Result: QRL-NAS比固定电路的QRL获得更高的奖励，证明了其有效性。

Conclusion: QRL-NAS为量子强化学习中的PQC结构优化提供了实用解决方案。

Abstract: Reinforcement learning (RL) enables agents to learn optimal policies through
environmental interaction. However, RL suffers from reduced learning efficiency
due to the curse of dimensionality in high-dimensional spaces. Quantum
reinforcement learning (QRL) addresses this issue by leveraging superposition
and entanglement in quantum computing, allowing efficient handling of
high-dimensional problems with fewer resources. QRL combines quantum neural
networks (QNNs) with RL, where the parameterized quantum circuit (PQC) acts as
the core computational module. The PQC performs linear and nonlinear
transformations through gate operations, similar to hidden layers in classical
neural networks. Previous QRL studies, however, have used fixed PQC structures
based on empirical intuition without verifying their optimality. This paper
proposes a QRL-NAS algorithm that integrates quantum neural architecture search
(QNAS) to optimize PQC structures within QRL. Experiments demonstrate that
QRL-NAS achieves higher rewards than QRL with fixed circuits, validating its
effectiveness and practical utility.

</details>


### [310] [Cooperative Sheaf Neural Networks](https://arxiv.org/abs/2507.00647)
*André Ribeiro,Ana Luiza Tenório,Juan Belieni,Amauri H. Souza,Diego Mesquita*

Main category: cs.LG

Relevance: 40.0

TL;DR: 论文探讨了Sheaf扩散在图形表示学习中的局限性，提出了基于有向图的细胞Sheaf概念，并设计了Cooperative Sheaf Neural Networks (CSNNs)以解决现有方法的不足。


<details>
  <summary>Details</summary>
Motivation: 研究Sheaf扩散是否能够实现协作行为，发现现有方法因缺乏消息方向性而失败，从而提出改进方案。

Method: 引入有向图的细胞Sheaf概念，定义其入度和出度Laplacian，并设计CSNNs。

Result: 理论分析表明CSNNs能选择性关注远距离节点，实验显示其性能优于现有Sheaf扩散和协作图神经网络方法。

Conclusion: CSNNs通过引入方向性解决了Sheaf扩散的协作问题，提升了性能。

Abstract: Sheaf diffusion has recently emerged as a promising design pattern for graph
representation learning due to its inherent ability to handle heterophilic data
and avoid oversmoothing. Meanwhile, cooperative message passing has also been
proposed as a way to enhance the flexibility of information diffusion by
allowing nodes to independently choose whether to propagate/gather information
from/to neighbors. A natural question ensues: is sheaf diffusion capable of
exhibiting this cooperative behavior? Here, we provide a negative answer to
this question. In particular, we show that existing sheaf diffusion methods
fail to achieve cooperative behavior due to the lack of message directionality.
To circumvent this limitation, we introduce the notion of cellular sheaves over
directed graphs and characterize their in- and out-degree Laplacians. We
leverage our construction to propose Cooperative Sheaf Neural Networks (CSNNs).
Theoretically, we characterize the receptive field of CSNN and show it allows
nodes to selectively attend (listen) to arbitrarily far nodes while ignoring
all others in their path, potentially mitigating oversquashing. Our experiments
show that CSNN presents overall better performance compared to prior art on
sheaf diffusion as well as cooperative graph neural networks.

</details>


### [311] [GANs Secretly Perform Approximate Bayesian Model Selection](https://arxiv.org/abs/2507.00651)
*Maurizio Filippone,Marius P. Linhard*

Main category: cs.LG

Relevance: 40.0

TL;DR: 该论文将GANs解释为概率生成模型，并将其视为部分随机性的贝叶斯神经网络，从而优化其正则化和训练策略。


<details>
  <summary>Details</summary>
Motivation: 解释GANs的成功与局限性，并提出新的正则化和优化策略以改善其性能。

Method: 将GANs视为贝叶斯神经网络，通过优化边际似然代理来定义正则化和优化策略。

Result: 实验结果表明，这些策略能提升性能并帮助理解GANs的正则化机制。

Conclusion: 该研究为GANs的正则化提供了新视角，并展示了性能改进的潜力。

Abstract: Generative Adversarial Networks (GANs) are popular and successful generative
models. Despite their success, optimization is notoriously challenging and they
require regularization against overfitting. In this work, we explain the
success and limitations of GANs by interpreting them as probabilistic
generative models. This interpretation enables us to view GANs as Bayesian
neural networks with partial stochasticity, allowing us to establish conditions
of universal approximation. We can then cast the adversarial-style optimization
of several variants of GANs as the optimization of a proxy for the marginal
likelihood. Taking advantage of the connection between marginal likelihood
optimization and Occam's razor, we can define regularization and optimization
strategies to smooth the loss landscape and search for solutions with minimum
description length, which are associated with flat minima and good
generalization. The results on a wide range of experiments indicate that these
strategies lead to performance improvements and pave the way to a deeper
understanding of regularization strategies for GANs.

</details>


### [312] [A Test-Function Approach to Incremental Stability](https://arxiv.org/abs/2507.00695)
*Daniel Pfrommer,Max Simchowitz,Ali Jadbabaie*

Main category: cs.LG

Relevance: 40.0

TL;DR: 论文提出了一种基于奖励作为“测试函数”的新框架，用于分析增量输入到状态稳定性（δISS），与传统控制理论中的Lyapunov函数不同，RL值函数通过指数衰减Lipschitz奖励函数构建，可能非光滑且无界。


<details>
  <summary>Details</summary>
Motivation: 传统控制理论中的Lyapunov函数无法直接解释RL中的值函数，因此需要一种新的方法来理解RL值函数与增量稳定性之间的关系。

Method: 通过建立闭环系统在给定策略下的增量输入到状态稳定性与Hölder连续奖励函数下RL值函数规律性之间的等价关系。

Result: 揭示了RL值函数的规律性及其与增量稳定性的联系，提供了一种不同于传统Lyapunov方法的稳定性证明途径。

Conclusion: 该研究为理解RL值函数与增量稳定性之间的关系提供了新视角，区别于传统控制理论方法。

Abstract: This paper presents a novel framework for analyzing
Incremental-Input-to-State Stability ($\delta$ISS) based on the idea of using
rewards as "test functions." Whereas control theory traditionally deals with
Lyapunov functions that satisfy a time-decrease condition, reinforcement
learning (RL) value functions are constructed by exponentially decaying a
Lipschitz reward function that may be non-smooth and unbounded on both sides.
Thus, these RL-style value functions cannot be directly understood as Lyapunov
certificates. We develop a new equivalence between a variant of incremental
input-to-state stability of a closed-loop system under given a policy, and the
regularity of RL-style value functions under adversarial selection of a
H\"older-continuous reward function. This result highlights that the regularity
of value functions, and their connection to incremental stability, can be
understood in a way that is distinct from the traditional Lyapunov-based
approach to certifying stability in control theory.

</details>


### [313] [Aleatoric and Epistemic Uncertainty Measures for Ordinal Classification through Binary Reduction](https://arxiv.org/abs/2507.00733)
*Stefan Haas,Eyke Hüllermeier*

Main category: cs.LG

Relevance: 40.0

TL;DR: 论文提出了一种新的序数分类不确定性量化方法，分解为偶然性和认知性不确定性，显著优于传统方法。


<details>
  <summary>Details</summary>
Motivation: 序数分类问题在高风险领域（如医学和金融）中普遍存在，但现有研究主要关注名义分类和回归，缺乏针对序数分类的不确定性量化方法。

Method: 通过将序数分类问题转化为二元分类，基于熵和方差的方法量化不确定性，并采用梯度提升树和多层感知机进行贝叶斯推断。

Result: 在多个序数基准数据集上，新方法在错误检测和分布外检测中表现优于传统方法。

Conclusion: 序数分类问题的不确定性量化应考虑其序数特性，新方法为此提供了有效工具。

Abstract: Ordinal classification problems, where labels exhibit a natural order, are
prevalent in high-stakes fields such as medicine and finance. Accurate
uncertainty quantification, including the decomposition into aleatoric
(inherent variability) and epistemic (lack of knowledge) components, is crucial
for reliable decision-making. However, existing research has primarily focused
on nominal classification and regression. In this paper, we introduce a novel
class of measures of aleatoric and epistemic uncertainty in ordinal
classification, which is based on a suitable reduction to (entropy- and
variance-based) measures for the binary case. These measures effectively
capture the trade-off in ordinal classification between exact hit-rate and
minimial error distances. We demonstrate the effectiveness of our approach on
various tabular ordinal benchmark datasets using ensembles of gradient-boosted
trees and multi-layer perceptrons for approximate Bayesian inference. Our
method significantly outperforms standard and label-wise entropy and
variance-based measures in error detection, as indicated by misclassification
rates and mean absolute error. Additionally, the ordinal measures show
competitive performance in out-of-distribution (OOD) detection. Our findings
highlight the importance of considering the ordinal nature of classification
problems when assessing uncertainty.

</details>


### [314] [Ordinality in Discrete-level Question Difficulty Estimation: Introducing Balanced DRPS and OrderedLogitNN](https://arxiv.org/abs/2507.00736)
*Arthur Thuy,Ekaterina Loginova,Dries F. Benoit*

Main category: cs.LG

Relevance: 40.0

TL;DR: 论文提出了一种新的有序回归方法OrderedLogitNN，并引入平衡的DRPS指标，用于解决QDE任务中的有序性和类别不平衡问题。


<details>
  <summary>Details</summary>
Motivation: 现有QDE研究忽视了任务的有序性，且评估指标未能有效处理类别不平衡，导致性能评估偏差。

Method: 提出了OrderedLogitNN，扩展了有序logit模型到神经网络，并使用平衡的DRPS指标评估三种模型输出（离散回归、分类、有序回归）。

Result: OrderedLogitNN在复杂任务上表现显著优于其他方法，平衡DRPS提供了鲁棒且公平的评估。

Conclusion: 研究为QDE任务提供了新的有序回归方法和评估指标，为未来研究奠定了基础。

Abstract: Recent years have seen growing interest in Question Difficulty Estimation
(QDE) using natural language processing techniques. Question difficulty is
often represented using discrete levels, framing the task as ordinal regression
due to the inherent ordering from easiest to hardest. However, the literature
has neglected the ordinal nature of the task, relying on classification or
discretized regression models, with specialized ordinal regression methods
remaining unexplored. Furthermore, evaluation metrics are tightly coupled to
the modeling paradigm, hindering cross-study comparability. While some metrics
fail to account for the ordinal structure of difficulty levels, none adequately
address class imbalance, resulting in biased performance assessments. This
study addresses these limitations by benchmarking three types of model outputs
-- discretized regression, classification, and ordinal regression -- using the
balanced Discrete Ranked Probability Score (DRPS), a novel metric that jointly
captures ordinality and class imbalance. In addition to using popular ordinal
regression methods, we propose OrderedLogitNN, extending the ordered logit
model from econometrics to neural networks. We fine-tune BERT on the RACE++ and
ARC datasets and find that OrderedLogitNN performs considerably better on
complex tasks. The balanced DRPS offers a robust and fair evaluation metric for
discrete-level QDE, providing a principled foundation for future research.

</details>


### [315] [A Probabilistic Approach to Wildfire Spread Prediction Using a Denoising Diffusion Surrogate Model](https://arxiv.org/abs/2507.00761)
*Wenbo Yu,Anirbit Ghosh,Tobias Sebastian Finn,Rossella Arcucci,Marc Bocquet,Sibo Cheng*

Main category: cs.LG

Relevance: 40.0

TL;DR: 提出了一种基于去噪扩散模型的野火蔓延预测方法，能够生成多种可能的场景，以应对野火动态的不确定性。


<details>
  <summary>Details</summary>
Motivation: 传统野火预测模型通常只能生成单一确定性结果，无法捕捉野火动态的不确定性。本研究旨在利用生成式AI模拟野火蔓延的多种可能场景。

Method: 采用去噪扩散模型，学习模拟野火蔓延的多种可能场景，生成反映物理意义的预测分布。

Result: 模型能够生成野火蔓延的多种可能场景，优于传统确定性方法。

Conclusion: 该技术有望为野火风险评估和响应规划提供更智能、快速和可靠的工具。

Abstract: Thanks to recent advances in generative AI, computers can now simulate
realistic and complex natural processes. We apply this capability to predict
how wildfires spread, a task made difficult by the unpredictable nature of fire
and the variety of environmental conditions it depends on. In this study, We
present the first denoising diffusion model for predicting wildfire spread, a
new kind of AI framework that learns to simulate fires not just as one fixed
outcome, but as a range of possible scenarios. By doing so, it accounts for the
inherent uncertainty of wildfire dynamics, a feature that traditional models
typically fail to represent. Unlike deterministic approaches that generate a
single prediction, our model produces ensembles of forecasts that reflect
physically meaningful distributions of where fire might go next. This
technology could help us develop smarter, faster, and more reliable tools for
anticipating wildfire behavior, aiding decision-makers in fire risk assessment
and response planning.

</details>


### [316] [TABASCO: A Fast, Simplified Model for Molecular Generation with Improved Physical Quality](https://arxiv.org/abs/2507.00899)
*Carlos Vonessen,Charles Harris,Miruna Cretu,Pietro Liò*

Main category: cs.LG

Relevance: 40.0

TL;DR: TABASCO是一种基于非等变Transformer架构的3D分子生成模型，简化了架构并提高了数据吞吐量，在GEOM-Drugs基准测试中表现优异。


<details>
  <summary>Details</summary>
Motivation: 现有3D分子生成模型依赖强归纳偏置（如SE(3)等变性），但仍难以保证物理合理性。TABASCO旨在通过简化架构提升效率和性能。

Method: 采用标准非等变Transformer架构，将分子原子视为序列生成，并确定性重建键。

Result: 在GEOM-Drugs基准测试中，TABASCO在PoseBusters有效性上达到SOTA，推理速度比基线快10倍，且表现出未硬编码的旋转等变性。

Conclusion: TABASCO为特定任务（如药物设计）提供了简约、高效的生成模型蓝图。

Abstract: State-of-the-art models for 3D molecular generation are based on significant
inductive biases, SE(3), permutation equivariance to respect symmetry and graph
message-passing networks to capture local chemistry, yet the generated
molecules still struggle with physical plausibility. We introduce TABASCO which
relaxes these assumptions: The model has a standard non-equivariant transformer
architecture, treats atoms in a molecule as sequences and reconstructs bonds
deterministically after generation. The absence of equivariant layers and
message passing allows us to significantly simplify the model architecture and
scale data throughput. On the GEOM-Drugs benchmark TABASCO achieves
state-of-the-art PoseBusters validity and delivers inference roughly 10x faster
than the strongest baseline, while exhibiting emergent rotational equivariance
despite symmetry not being hard-coded. Our work offers a blueprint for training
minimalist, high-throughput generative models suited to specialised tasks such
as structure- and pharmacophore-based drug design. We provide a link to our
implementation at github.com/carlosinator/tabasco.

</details>


### [317] [Privacy-Preserving Quantized Federated Learning with Diverse Precision](https://arxiv.org/abs/2507.00920)
*Dang Qua Nguyen,Morteza Hashemi,Erik Perrins,Sergiy A. Vorobyov,David J. Love,Taejoon Kim*

Main category: cs.LG

Relevance: 40.0

TL;DR: 提出了一种新型随机量化器（SQ），用于在联邦学习中同时实现差分隐私和最小量化误差，并通过聚类优化和线性融合提高模型聚合精度。


<details>
  <summary>Details</summary>
Motivation: 解决联邦学习中隐私风险和量化异质性对学习效用的负面影响。

Method: 引入随机量化器（SQ）保证差分隐私和最小量化误差，结合聚类优化和线性融合技术。

Result: 数值模拟显示，相比传统LaplaceSQ-FL算法，新方法在隐私保护和学习效用上表现更优。

Conclusion: 新方法有效解决了联邦学习中的隐私和量化异质性问题，提升了学习效用。

Abstract: Federated learning (FL) has emerged as a promising paradigm for distributed
machine learning, enabling collaborative training of a global model across
multiple local devices without requiring them to share raw data. Despite its
advancements, FL is limited by factors such as: (i) privacy risks arising from
the unprotected transmission of local model updates to the fusion center (FC)
and (ii) decreased learning utility caused by heterogeneity in model
quantization resolution across participating devices. Prior work typically
addresses only one of these challenges because maintaining learning utility
under both privacy risks and quantization heterogeneity is a non-trivial task.
In this paper, our aim is therefore to improve the learning utility of a
privacy-preserving FL that allows clusters of devices with different
quantization resolutions to participate in each FL round. Specifically, we
introduce a novel stochastic quantizer (SQ) that is designed to simultaneously
achieve differential privacy (DP) and minimum quantization error. Notably, the
proposed SQ guarantees bounded distortion, unlike other DP approaches. To
address quantization heterogeneity, we introduce a cluster size optimization
technique combined with a linear fusion approach to enhance model aggregation
accuracy. Numerical simulations validate the benefits of our approach in terms
of privacy protection and learning utility compared to the conventional
LaplaceSQ-FL algorithm.

</details>


### [318] [MVGBench: Comprehensive Benchmark for Multi-view Generation Models](https://arxiv.org/abs/2507.00006)
*Xianghui Xie,Chuhang Zou,Meher Gitika Karumuri,Jan Eric Lenssen,Gerard Pons-Moll*

Main category: cs.GR

Relevance: 40.0

TL;DR: MVGBench是一个多视图图像生成模型（MVG）的基准测试，评估3D几何和纹理一致性、图像质量及语义。通过引入3D自一致性指标，避免依赖真实目标视图，系统比较了12种MVG，并提出了ViFiGen方法，在3D一致性上表现最佳。


<details>
  <summary>Details</summary>
Motivation: 现有MVG评估方法依赖真实目标视图，不适用于多解生成任务，且缺乏对鲁棒性和泛化性的全面评估。MVGBench旨在填补这一空白。

Method: MVGBench评估三方面：最佳配置性能、泛化到真实数据及鲁棒性。引入3D自一致性指标，比较不相交生成多视图的3D重建。

Result: 系统比较12种MVG，发现现有方法在鲁棒性和泛化性上的局限，并提出ViFiGen方法，在3D一致性上超越所有评估模型。

Conclusion: MVGBench为MVG提供了严谨评估框架，揭示了关键设计选择，并推动了ViFiGen的提出。

Abstract: We propose MVGBench, a comprehensive benchmark for multi-view image
generation models (MVGs) that evaluates 3D consistency in geometry and texture,
image quality, and semantics (using vision language models). Recently, MVGs
have been the main driving force in 3D object creation. However, existing
metrics compare generated images against ground truth target views, which is
not suitable for generative tasks where multiple solutions exist while
differing from ground truth. Furthermore, different MVGs are trained on
different view angles, synthetic data and specific lightings -- robustness to
these factors and generalization to real data are rarely evaluated thoroughly.
Without a rigorous evaluation protocol, it is also unclear what design choices
contribute to the progress of MVGs. MVGBench evaluates three different aspects:
best setup performance, generalization to real data and robustness. Instead of
comparing against ground truth, we introduce a novel 3D self-consistency metric
which compares 3D reconstructions from disjoint generated multi-views. We
systematically compare 12 existing MVGs on 4 different curated real and
synthetic datasets. With our analysis, we identify important limitations of
existing methods specially in terms of robustness and generalization, and we
find the most critical design choices. Using the discovered best practices, we
propose ViFiGen, a method that outperforms all evaluated MVGs on 3D
consistency. Our code, model, and benchmark suite will be publicly released.

</details>


### [319] [Disentangled Feature Importance](https://arxiv.org/abs/2507.00260)
*Jin-Hong Du,Kathryn Roeder,Larry Wasserman*

Main category: stat.ML

Relevance: 40.0

TL;DR: 论文提出了一种名为DFI的方法，通过最优传输消除特征相关性对重要性评估的偏差，提供了非参数化的特征重要性分解。


<details>
  <summary>Details</summary>
Motivation: 现有方法在特征相关时会低估特征重要性，DFI旨在解决这一问题。

Method: DFI通过最优传输将相关特征转换为独立潜在变量，并在解耦空间中计算重要性。

Result: DFI实现了根号n一致性和渐近正态性，计算效率高。

Conclusion: DFI为特征重要性评估提供了理论支持和方法改进。

Abstract: Feature importance quantification faces a fundamental challenge: when
predictors are correlated, standard methods systematically underestimate their
contributions. We prove that major existing approaches target identical
population functionals under squared-error loss, revealing why they share this
correlation-induced bias.
  To address this limitation, we introduce \emph{Disentangled Feature
Importance (DFI)}, a nonparametric generalization of the classical $R^2$
decomposition via optimal transport. DFI transforms correlated features into
independent latent variables using a transport map, eliminating correlation
distortion. Importance is computed in this disentangled space and attributed
back through the transport map's sensitivity. DFI provides a principled
decomposition of importance scores that sum to the total predictive variability
for latent additive models and to interaction-weighted functional ANOVA
variances more generally, under arbitrary feature dependencies.
  We develop a comprehensive semiparametric theory for DFI. For general
transport maps, we establish root-$n$ consistency and asymptotic normality of
importance estimators in the latent space, which extends to the original
feature space for the Bures-Wasserstein map. Notably, our estimators achieve
second-order estimation error, which vanishes if both regression function and
transport map estimation errors are $o_{\mathbb{P}}(n^{-1/4})$. By design, DFI
avoids the computational burden of repeated submodel refitting and the
challenges of conditional covariate distribution estimation, thereby achieving
computational efficiency.

</details>


### [320] [Find a Scapegoat: Poisoning Membership Inference Attack and Defense to Federated Learning](https://arxiv.org/abs/2507.00423)
*Wenjin Mo,Zhiyuan Li,Minghong Fang,Mingwei Fang*

Main category: cs.CR

Relevance: 40.0

TL;DR: 论文提出了FedPoisonMIA，一种针对联邦学习的投毒成员推理攻击，并提出了防御机制。


<details>
  <summary>Details</summary>
Motivation: 联邦学习因其隐私保护特性被广泛采用，但其分布式特性易受投毒攻击，现有攻击多关注模型完整性而非隐私。

Method: 提出FedPoisonMIA攻击，恶意客户端通过本地模型更新推断成员信息；同时设计防御机制。

Result: 实验证明攻击有效，防御机制能减轻其影响。

Conclusion: FedPoisonMIA揭示了联邦学习中隐私攻击的新威胁，防御机制提供了缓解方案。

Abstract: Federated learning (FL) allows multiple clients to collaboratively train a
global machine learning model with coordination from a central server, without
needing to share their raw data. This approach is particularly appealing in the
era of privacy regulations like the GDPR, leading many prominent companies to
adopt it. However, FL's distributed nature makes it susceptible to poisoning
attacks, where malicious clients, controlled by an attacker, send harmful data
to compromise the model. Most existing poisoning attacks in FL aim to degrade
the model's integrity, such as reducing its accuracy, with limited attention to
privacy concerns from these attacks. In this study, we introduce FedPoisonMIA,
a novel poisoning membership inference attack targeting FL. FedPoisonMIA
involves malicious clients crafting local model updates to infer membership
information. Additionally, we propose a robust defense mechanism to mitigate
the impact of FedPoisonMIA attacks. Extensive experiments across various
datasets demonstrate the attack's effectiveness, while our defense approach
reduces its impact to a degree.

</details>


### [321] [Guided Unconditional and Conditional Generative Models for Super-Resolution and Inference of Quasi-Geostrophic Turbulence](https://arxiv.org/abs/2507.00719)
*Anantha Narayanan Suresh Babu,Akhil Sadam,Pierre F. J. Lermusiaux*

Main category: physics.flu-dyn

Relevance: 40.0

TL;DR: 论文研究了四种生成扩散模型在二维准地转湍流超分辨和推断中的应用，比较了不同方法的性能。


<details>
  <summary>Details</summary>
Motivation: 解决海洋、天气和气候数值模拟中因观测数据稀疏和不完整导致的问题。

Method: 应用四种生成扩散模型（SDEdit、DPS、条件模型及其变体）处理不同观测条件下的湍流数据。

Result: SDEdit生成不物理的场，DPS效果较好但细节平滑；条件模型能重建细节但需重新训练。

Conclusion: 不同方法在实现难度、保真度和一致性之间存在权衡，为地球物理反问题提供了实用指导。

Abstract: Typically, numerical simulations of the ocean, weather, and climate are
coarse, and observations are sparse and gappy. In this work, we apply four
generative diffusion modeling approaches to super-resolution and inference of
forced two-dimensional quasi-geostrophic turbulence on the beta-plane from
coarse, sparse, and gappy observations. Two guided approaches minimally adapt a
pre-trained unconditional model: SDEdit modifies the initial condition, and
Diffusion Posterior Sampling (DPS) modifies the reverse diffusion process
score. The other two conditional approaches, a vanilla variant and
classifier-free guidance, require training with paired high-resolution and
observation data. We consider eight test cases spanning: two regimes, eddy and
anisotropic-jet turbulence; two Reynolds numbers, 10^3 and 10^4; and two
observation types, 4x coarse-resolution fields and coarse, sparse and gappy
observations. Our comprehensive skill metrics include norms of the
reconstructed vorticity fields, turbulence statistical quantities, and
quantification of the super-resolved probabilistic ensembles and their errors.
We also study the sensitivity to tuning parameters such as guidance strength.
Results show that SDEdit generates unphysical fields, while DPS generates
reasonable reconstructions at low computational cost but with smoothed
fine-scale features. Both conditional approaches require re-training, but they
reconstruct missing fine-scale features, are cycle-consistent with
observations, and possess the correct statistics such as energy spectra.
Further, their mean model errors are highly correlated with and predictable
from their ensemble standard deviations. Results highlight the trade-offs
between ease of implementation, fidelity (sharpness), and cycle-consistency of
the diffusion models, and offer practical guidance for deployment in
geophysical inverse problems.

</details>


### [322] [Novel RL approach for efficient Elevator Group Control Systems](https://arxiv.org/abs/2507.00011)
*Nathan Vaartjes,Vincent Francois-Lavet*

Main category: cs.LG

Relevance: 30.0

TL;DR: 论文提出了一种基于强化学习的电梯群控系统（EGCS），通过马尔可夫决策过程建模，优化电梯调度以减少乘客等待时间和能耗。


<details>
  <summary>Details</summary>
Motivation: 传统基于启发式或模式检测的电梯控制器难以应对调度中的随机性和组合复杂性，因此需要更智能的解决方案。

Method: 使用马尔可夫决策过程建模电梯系统，提出新的动作空间编码和奖励信号设计，探索基于Dueling Double Deep Q-learning的强化学习架构。

Result: 提出的RL-based EGCS能够适应波动的交通模式，在高度随机环境中学习，性能优于传统规则算法。

Conclusion: 强化学习方法在电梯调度问题中具有潜力，能够有效提升系统性能。

Abstract: Efficient elevator traffic management in large buildings is critical for
minimizing passenger travel times and energy consumption. Because heuristic- or
pattern-detection-based controllers struggle with the stochastic and
combinatorial nature of dispatching, we model the six-elevator, fifteen-floor
system at Vrije Universiteit Amsterdam as a Markov Decision Process and train
an end-to-end Reinforcement Learning (RL) Elevator Group Control System (EGCS).
Key innovations include a novel action space encoding to handle the
combinatorial complexity of elevator dispatching, the introduction of
infra-steps to model continuous passenger arrivals, and a tailored reward
signal to improve learning efficiency. In addition, we explore various ways to
adapt the discounting factor to the infra-step formulation. We investigate RL
architectures based on Dueling Double Deep Q-learning, showing that the
proposed RL-based EGCS adapts to fluctuating traffic patterns, learns from a
highly stochastic environment, and thereby outperforms a traditional rule-based
algorithm.

</details>


### [323] [Quantum Inspired Encoding Strategies for Machine Learning Models: Proposing and Evaluating Instance Level, Global Discrete, and Class Conditional Representations](https://arxiv.org/abs/2507.00019)
*Minati Rath,Hema Date*

Main category: cs.LG

Relevance: 30.0

TL;DR: 该研究提出并比较了三种量子启发的数据编码策略（ILS、GDS、CCVS），用于将经典数据转换为量子数据，以优化纯经典机器学习模型的性能。


<details>
  <summary>Details</summary>
Motivation: 目标是减少高编码时间，同时确保编码值的正确性，并分析其对分类性能的影响。

Method: 提出了三种编码策略：实例级策略（ILS）、全局离散策略（GDS）和类条件值策略（CCVS），并在分类任务中评估其效率、正确性和计算成本。

Result: 研究分析了编码时间、精度和预测性能之间的权衡，为优化量子启发的数据转换提供了见解。

Conclusion: 该研究为经典机器学习工作流中的量子启发数据转换提供了优化方向。

Abstract: In this study, we propose, evaluate and compare three quantum inspired data
encoding strategies, Instance Level Strategy (ILS), Global Discrete Strategy
(GDS) and Class Conditional Value Strategy (CCVS), for transforming classical
data into quantum data for use in pure classical machine learning models. The
primary objective is to reduce high encoding time while ensuring correct
encoding values and analyzing their impact on classification performance. The
Instance Level Strategy treats each row of dataset independently; mimics local
quantum states. Global Discrete Value Based encoding strategy maps all unique
feature values across the full dataset to quantum states uniformly. In
contrast, the Class conditional Value based encoding strategy encodes unique
values separately for each class, preserving class dependent information.
  We apply these encoding strategies to a classification task and assess their
impact on en-coding efficiency, correctness, model accuracy, and computational
cost. By analyzing the trade offs between encoding time, precision, and
predictive performance, this study provides insights into optimizing quantum
inspired data transformations for classical machine learning workflows.

</details>


### [324] [Variational Autoencoder for Generating Broader-Spectrum prior Proposals in Markov chain Monte Carlo Methods](https://arxiv.org/abs/2507.00020)
*Marcio Borges,Felipe Pereira,Michel Tosin*

Main category: cs.LG

Relevance: 30.0

TL;DR: 使用变分自编码器（VAE）改进马尔可夫链蒙特卡洛（McMC）方法，通过生成更广泛的先验提案提高效率和适用性。


<details>
  <summary>Details</summary>
Motivation: 传统方法（如KLE）需要已知协方差函数，但在实际应用中往往不可得。VAE提供了一种数据驱动的方法，灵活捕捉贝叶斯逆问题中的相关性结构。

Method: 采用VAE框架，应用于地下水流动反演问题，通过压力数据估计渗透率场。

Result: VAE方法在已知相关长度时与KLE精度相当，在相关长度假设偏离真实值时优于KLE，并显著降低随机维度，提高计算效率。

Conclusion: 深度生成模型在McMC方法中的应用可以提高高维贝叶斯推断的适应性和效率。

Abstract: This study uses a Variational Autoencoder method to enhance the efficiency
and applicability of Markov Chain Monte Carlo (McMC) methods by generating
broader-spectrum prior proposals. Traditional approaches, such as the
Karhunen-Lo\`eve Expansion (KLE), require previous knowledge of the covariance
function, often unavailable in practical applications. The VAE framework
enables a data-driven approach to flexibly capture a broader range of
correlation structures in Bayesian inverse problems, particularly subsurface
flow modeling. The methodology is tested on a synthetic groundwater flow
inversion problem, where pressure data is used to estimate permeability fields.
Numerical experiments demonstrate that the VAE-based parameterization achieves
comparable accuracy to KLE when the correlation length is known and outperforms
KLE when the assumed correlation length deviates from the true value. Moreover,
the VAE approach significantly reduces stochastic dimensionality, improving
computational efficiency. The results suggest that leveraging deep generative
models in McMC methods can lead to more adaptable and efficient Bayesian
inference in high-dimensional problems.

</details>


### [325] [HiT-JEPA: A Hierarchical Self-supervised Trajectory Embedding Framework for Similarity Computation](https://arxiv.org/abs/2507.00028)
*Lihuan Li,Hao Xue,Shuang Ao,Yang Song,Flora Salim*

Main category: cs.LG

Relevance: 30.0

TL;DR: 提出了一种名为HiT-JEPA的层次化轨迹表示框架，用于捕捉多尺度城市轨迹信息。


<details>
  <summary>Details</summary>
Motivation: 现有方法难以同时捕捉轨迹的细粒度细节和高层次摘要，限制了模型对长期依赖和局部细节的关注。

Method: HiT-JEPA采用三层层次结构，逐步捕捉点级细节、中间模式和高层轨迹抽象。

Result: 在多个真实数据集上的实验表明，HiT-JEPA能生成更丰富的多尺度表示。

Conclusion: HiT-JEPA通过层次化设计，成功整合了局部动态和全局语义。

Abstract: The representation of urban trajectory data plays a critical role in
effectively analyzing spatial movement patterns. Despite considerable progress,
the challenge of designing trajectory representations that can capture diverse
and complementary information remains an open research problem. Existing
methods struggle in incorporating trajectory fine-grained details and
high-level summary in a single model, limiting their ability to attend to both
long-term dependencies while preserving local nuances. To address this, we
propose HiT-JEPA (Hierarchical Interactions of Trajectory Semantics via a Joint
Embedding Predictive Architecture), a unified framework for learning
multi-scale urban trajectory representations across semantic abstraction
levels. HiT-JEPA adopts a three-layer hierarchy that progressively captures
point-level fine-grained details, intermediate patterns, and high-level
trajectory abstractions, enabling the model to integrate both local dynamics
and global semantics in one coherent structure. Extensive experiments on
multiple real-world datasets for trajectory similarity computation show that
HiT-JEPA's hierarchical design yields richer, multi-scale representations. Code
is available at: https://anonymous.4open.science/r/HiT-JEPA.

</details>


### [326] [Enhancing Spatio-Temporal Forecasting with Spatial Neighbourhood Fusion:A Case Study on COVID-19 Mobility in Peru](https://arxiv.org/abs/2507.00031)
*Chuan Li,Jiang You,Hassine Moungla,Vincent Gauthier,Miguel Nunez-del-Prado,Hugo Alatrista-Salas*

Main category: cs.LG

Relevance: 30.0

TL;DR: 论文提出了一种轻量级且模型无关的空间邻域融合（SPN）技术，用于改善稀疏时空数据的预测性能，特别是在公共卫生危机期间的流动性预测。


<details>
  <summary>Details</summary>
Motivation: 准确建模人类流动性对于理解疫情传播和及时部署干预措施至关重要。然而，空间稀疏性限制了传统时间序列模型的预测能力。

Method: 提出了SPN技术，通过聚合邻近H3网格单元的信号来增强每个单元的特征，并在NLinear、PatchTST和K-U-Net三种预测模型上进行了评估。

Result: SPN技术显著提升了预测性能，测试MSE降低了9.85%。

Conclusion: 空间平滑稀疏流动性信号是一种简单而有效的时空预测方法。

Abstract: Accurate modeling of human mobility is critical for understanding epidemic
spread and deploying timely interventions. In this work, we leverage a
large-scale spatio-temporal dataset collected from Peru's national Digital
Contact Tracing (DCT) application during the COVID-19 pandemic to forecast
mobility flows across urban regions. A key challenge lies in the spatial
sparsity of hourly mobility counts across hexagonal grid cells, which limits
the predictive power of conventional time series models. To address this, we
propose a lightweight and model-agnostic Spatial Neighbourhood Fusion (SPN)
technique that augments each cell's features with aggregated signals from its
immediate H3 neighbors. We evaluate this strategy on three forecasting
backbones: NLinear, PatchTST, and K-U-Net, under various historical input
lengths. Experimental results show that SPN consistently improves forecasting
performance, achieving up to 9.85 percent reduction in test MSE. Our findings
demonstrate that spatial smoothing of sparse mobility signals provides a simple
yet effective path toward robust spatio-temporal forecasting during public
health crises.

</details>


### [327] [IDRIFTNET: Physics-Driven Spatiotemporal Deep Learning for Iceberg Drift Forecasting](https://arxiv.org/abs/2507.00036)
*Rohan Putatunda,Sanjay Purushotham,Ratnaksha Lele,Vandana P. Janeja*

Main category: cs.LG

Relevance: 30.0

TL;DR: 提出了一种混合IDRIFTNET模型，结合物理驱动和深度学习，用于预测冰山漂移轨迹，优于现有模型。


<details>
  <summary>Details</summary>
Motivation: 冰山漂移轨迹预测对气候系统和极地导航至关重要，但现有方法因数据稀缺和非线性动态难以准确预测。

Method: IDRIFTNET结合解析冰山漂移物理模型和增强残差学习模型，通过旋转增强谱神经网络捕捉全局和局部模式。

Result: 在A23A和B22A冰山测试中，IDRIFTNET的FDE和ADE均优于现有模型。

Conclusion: IDRIFTNET能有效预测冰山漂移轨迹，适用于数据有限和动态环境条件。

Abstract: Drifting icebergs in the polar oceans play a key role in the Earth's climate
system, impacting freshwater fluxes into the ocean and regional ecosystems
while also posing a challenge to polar navigation. However, accurately
forecasting iceberg trajectories remains a formidable challenge, primarily due
to the scarcity of spatiotemporal data and the complex, nonlinear nature of
iceberg motion, which is also impacted by environmental variables. The iceberg
motion is influenced by multiple dynamic environmental factors, creating a
highly variable system that makes trajectory identification complex. These
limitations hinder the ability of deep learning models to effectively capture
the underlying dynamics and provide reliable predictive outcomes. To address
these challenges, we propose a hybrid IDRIFTNET model, a physics-driven deep
learning model that combines an analytical formulation of iceberg drift
physics, with an augmented residual learning model. The model learns the
pattern of mismatch between the analytical solution and ground-truth
observations, which is combined with a rotate-augmented spectral neural network
that captures both global and local patterns from the data to forecast future
iceberg drift positions. We compare IDRIFTNET model performance with
state-of-the-art models on two Antarctic icebergs: A23A and B22A. Our findings
demonstrate that IDRIFTNET outperforms other models by achieving a lower Final
Displacement Error (FDE) and Average Displacement Error (ADE) across a variety
of time points. These results highlight IDRIFTNET's effectiveness in capturing
the complex, nonlinear drift of icebergs for forecasting iceberg trajectories
under limited data and dynamic environmental conditions.

</details>


### [328] [Smooth-Distill: A Self-distillation Framework for Multitask Learning with Wearable Sensor Data](https://arxiv.org/abs/2507.00061)
*Hoang-Dieu Vu,Duc-Nghia Tran,Quang-Tu Pham,Hieu H. Pham,Nicolas Vuillerme,Duc-Tan Tran*

Main category: cs.LG

Relevance: 30.0

TL;DR: Smooth-Distill是一种新颖的自蒸馏框架，用于同时执行人类活动识别（HAR）和传感器放置检测，通过统一的CNN架构MTL-net处理加速度计数据，并利用模型的历史平滑版本作为教师模型，减少计算开销。


<details>
  <summary>Details</summary>
Motivation: 解决传统蒸馏方法需要独立教师和学生模型的问题，同时提升多任务学习的效率和性能。

Method: 采用MTL-net架构处理加速度计数据，利用历史平滑模型作为教师模型进行自蒸馏。

Result: 在多个评估场景中优于其他方法，提升了HAR和传感器放置检测的准确性，同时减少了过拟合和训练计算开销。

Conclusion: Smooth-Distill为多任务学习提供了一种高效且准确的解决方案，尤其适用于资源受限的平台。

Abstract: This paper introduces Smooth-Distill, a novel self-distillation framework
designed to simultaneously perform human activity recognition (HAR) and sensor
placement detection using wearable sensor data. The proposed approach utilizes
a unified CNN-based architecture, MTL-net, which processes accelerometer data
and branches into two outputs for each respective task. Unlike conventional
distillation methods that require separate teacher and student models, the
proposed framework utilizes a smoothed, historical version of the model itself
as the teacher, significantly reducing training computational overhead while
maintaining performance benefits. To support this research, we developed a
comprehensive accelerometer-based dataset capturing 12 distinct sleep postures
across three different wearing positions, complementing two existing public
datasets (MHealth and WISDM). Experimental results show that Smooth-Distill
consistently outperforms alternative approaches across different evaluation
scenarios, achieving notable improvements in both human activity recognition
and device placement detection tasks. This method demonstrates enhanced
stability in convergence patterns during training and exhibits reduced
overfitting compared to traditional multitask learning baselines. This
framework contributes to the practical implementation of knowledge distillation
in human activity recognition systems, offering an effective solution for
multitask learning with accelerometer data that balances accuracy and training
efficiency. More broadly, it reduces the computational cost of model training,
which is critical for scenarios requiring frequent model updates or training on
resource-constrained platforms. The code and model are available at
https://github.com/Kuan2vn/smooth\_distill.

</details>


### [329] [Strategic Counterfactual Modeling of Deep-Target Airstrike Systems via Intervention-Aware Spatio-Causal Graph Networks](https://arxiv.org/abs/2507.00083)
*Wei Meng*

Main category: cs.LG

Relevance: 30.0

TL;DR: 提出了一种干预感知的时空图神经网络（IA-STGNN），用于解决战略级模拟中的因果建模问题，显著优于基线模型。


<details>
  <summary>Details</summary>
Motivation: 当前战略级模拟缺乏对战术打击行为与战略延迟之间结构化因果建模的能力，尤其是中间变量的捕捉瓶颈。

Method: 结合图注意力机制、反事实模拟单元和空间干预节点重建，构建IA-STGNN框架，利用多物理模拟平台生成训练数据。

Result: IA-STGNN在MAE上降低12.8%，Top-5准确率提升18.4%，同时提高了因果路径一致性和干预稳定性。

Conclusion: IA-STGNN为高层政策建模提供了结构化且透明的AI决策支持机制，适用于核威慑模拟等应用。

Abstract: This study addresses the lack of structured causal modeling between tactical
strike behavior and strategic delay in current strategic-level simulations,
particularly the structural bottlenecks in capturing intermediate variables
within the "resilience - nodal suppression - negotiation window" chain. We
propose the Intervention-Aware Spatio-Temporal Graph Neural Network (IA-STGNN),
a novel framework that closes the causal loop from tactical input to strategic
delay output. The model integrates graph attention mechanisms, counterfactual
simulation units, and spatial intervention node reconstruction to enable
dynamic simulations of strike configurations and synchronization strategies.
Training data are generated from a multi-physics simulation platform (GEANT4 +
COMSOL) under NIST SP 800-160 standards, ensuring structural traceability and
policy-level validation. Experimental results demonstrate that IA-STGNN
significantly outperforms baseline models (ST-GNN, GCN-LSTM, XGBoost),
achieving a 12.8 percent reduction in MAE and 18.4 percent increase in Top-5
percent accuracy, while improving causal path consistency and intervention
stability. IA-STGNN enables interpretable prediction of strategic delay and
supports applications such as nuclear deterrence simulation, diplomatic window
assessment, and multi-strategy optimization, providing a structured and
transparent AI decision-support mechanism for high-level policy modeling.

</details>


### [330] [A Joint Topology-Data Fusion Graph Network for Robust Traffic Speed Prediction with Data Anomalism](https://arxiv.org/abs/2507.00085)
*Ruiyuan Jiang,Dongyao Jia,Eng Gee Lim,Pengfei Fan,Yuli Zhang,Shangbo Wang*

Main category: cs.LG

Relevance: 30.0

TL;DR: 提出了一种名为GFEN的图融合增强网络，用于交通速度预测，通过拓扑时空图融合技术和混合方法显著提升了预测精度和收敛速度。


<details>
  <summary>Details</summary>
Motivation: 现有交通预测方法难以处理复杂的时空动态和非平稳数据，GFEN旨在解决这些问题。

Method: GFEN结合了拓扑时空图融合技术和基于注意力的深度学习结构，动态平滑数据并建模多尺度时空特征。

Result: GFEN在预测精度上优于现有方法6.3%，收敛速度是其他混合模型的两倍。

Conclusion: GFEN在交通预测中表现出色，具有实际应用潜力。

Abstract: Accurate traffic prediction is essential for Intelligent Transportation
Systems (ITS), yet current methods struggle with the inherent complexity and
non-linearity of traffic dynamics, making it difficult to integrate spatial and
temporal characteristics. Furthermore, existing approaches use static
techniques to address non-stationary and anomalous historical data, which
limits adaptability and undermines data smoothing. To overcome these
challenges, we propose the Graph Fusion Enhanced Network (GFEN), an innovative
framework for network-level traffic speed prediction. GFEN introduces a novel
topological spatiotemporal graph fusion technique that meticulously extracts
and merges spatial and temporal correlations from both data distribution and
network topology using trainable methods, enabling the modeling of multi-scale
spatiotemporal features. Additionally, GFEN employs a hybrid methodology
combining a k-th order difference-based mathematical framework with an
attention-based deep learning structure to adaptively smooth historical
observations and dynamically mitigate data anomalies and non-stationarity.
Extensive experiments demonstrate that GFEN surpasses state-of-the-art methods
by approximately 6.3% in prediction accuracy and exhibits convergence rates
nearly twice as fast as recent hybrid models, confirming its superior
performance and potential to significantly enhance traffic prediction system
efficiency.

</details>


### [331] [Generating Heterogeneous Multi-dimensional Data : A Comparative Study](https://arxiv.org/abs/2507.00090)
*Corbeau Michael,Claeys Emmanuelle,Serrurier Mathieu,Zaraté Pascale*

Main category: cs.LG

Relevance: 30.0

TL;DR: 该论文比较了多种数据生成方法在消防员干预场景中的效果，并提出了领域特定的评估指标。


<details>
  <summary>Details</summary>
Motivation: 消防员干预的资源分配高度依赖模拟数据，传统评估指标无法满足需求，因此需要更有效的数据生成和评估方法。

Method: 比较了随机采样、变分自编码器、生成对抗网络、条件生成对抗网络和扩散概率模型等方法，并使用领域特定指标和标准指标进行评估。

Result: 研究发现传统指标不足以评估合成数据质量，领域特定指标能更好地捕捉数据的复杂性和实际需求。

Conclusion: 领域特定指标对于评估消防员干预场景的合成数据至关重要，未来研究应进一步优化数据生成方法。

Abstract: Allocation of personnel and material resources is highly sensible in the case
of firefighter interventions. This allocation relies on simulations to
experiment with various scenarios. The main objective of this allocation is the
global optimization of the firefighters response. Data generation is then
mandatory to study various scenarios In this study, we propose to compare
different data generation methods. Methods such as Random Sampling, Tabular
Variational Autoencoders, standard Generative Adversarial Networks, Conditional
Tabular Generative Adversarial Networks and Diffusion Probabilistic Models are
examined to ascertain their efficacy in capturing the intricacies of
firefighter interventions. Traditional evaluation metrics often fall short in
capturing the nuanced requirements of synthetic datasets for real-world
scenarios. To address this gap, an evaluation of synthetic data quality is
conducted using a combination of domain-specific metrics tailored to the
firefighting domain and standard measures such as the Wasserstein distance.
Domain-specific metrics include response time distribution, spatial-temporal
distribution of interventions, and accidents representation. These metrics are
designed to assess data variability, the preservation of fine and complex
correlations and anomalies such as event with a very low occurrence, the
conformity with the initial statistical distribution and the operational
relevance of the synthetic data. The distribution has the particularity of
being highly unbalanced, none of the variables following a Gaussian
distribution, adding complexity to the data generation process.

</details>


### [332] [DFReg: A Physics-Inspired Framework for Global Weight Distribution Regularization in Neural Networks](https://arxiv.org/abs/2507.00101)
*Giovanni Ruggieri*

Main category: cs.LG

Relevance: 30.0

TL;DR: DFReg是一种基于物理启发的正则化方法，通过全局权重分布优化深度神经网络。


<details>
  <summary>Details</summary>
Motivation: 传统正则化方法（如Dropout或L2衰减）仅局部或随机扰动权重，缺乏全局结构约束。DFReg旨在通过密度泛函理论（DFT）实现全局平滑和多样化的权重配置。

Method: DFReg应用功能惩罚，直接作用于权重的全局分布，无需架构修改或随机扰动。

Result: 该方法能够在不改变网络结构的情况下，实现更平滑、多样且分布良好的权重配置。

Conclusion: DFReg为深度神经网络的正则化提供了一种新的全局视角，可能提升模型的泛化能力和鲁棒性。

Abstract: We introduce DFReg, a physics-inspired regularization method for deep neural
networks that operates on the global distribution of weights. Drawing from
Density Functional Theory (DFT), DFReg applies a functional penalty to
encourage smooth, diverse, and well-distributed weight configurations. Unlike
traditional techniques such as Dropout or L2 decay, DFReg imposes global
structural regularity without architectural changes or stochastic
perturbations.

</details>


### [333] [Towards transparent and data-driven fault detection in manufacturing: A case study on univariate, discrete time series](https://arxiv.org/abs/2507.00102)
*Bernd Hofmann,Patrick Bruendl,Huong Giang Nguyen,Joerg Franke*

Main category: cs.LG

Relevance: 30.0

TL;DR: 论文提出了一种结合数据驱动和透明性的工业故障检测方法，包括多类故障分类、SHAP解释和领域特定可视化技术，验证了其高准确性和可解释性。


<details>
  <summary>Details</summary>
Motivation: 传统质量控制方法缺乏适应性且依赖专家知识，而数据驱动方法虽高效但缺乏可解释性。本文旨在解决这一矛盾。

Method: 整合监督学习模型、SHAP解释和领域可视化技术，并通过定量扰动分析和专家评估验证解释效果。

Result: 故障检测准确率达95.9%，定量和定性评估均证实了解释的相关性和可解释性。

Conclusion: 该方法提升了数据驱动故障检测的可信度和可解释性，适用于工业质量控制。

Abstract: Ensuring consistent product quality in modern manufacturing is crucial,
particularly in safety-critical applications. Conventional quality control
approaches, reliant on manually defined thresholds and features, lack
adaptability to the complexity and variability inherent in production data and
necessitate extensive domain expertise. Conversely, data-driven methods, such
as machine learning, demonstrate high detection performance but typically
function as black-box models, thereby limiting their acceptance in industrial
environments where interpretability is paramount. This paper introduces a
methodology for industrial fault detection, which is both data-driven and
transparent. The approach integrates a supervised machine learning model for
multi-class fault classification, Shapley Additive Explanations for post-hoc
interpretability, and a do-main-specific visualisation technique that maps
model explanations to operator-interpretable features. Furthermore, the study
proposes an evaluation methodology that assesses model explanations through
quantitative perturbation analysis and evaluates visualisations by qualitative
expert assessment. The approach was applied to the crimping process, a
safety-critical joining technique, using a dataset of univariate, discrete time
series. The system achieves a fault detection accuracy of 95.9 %, and both
quantitative selectivity analysis and qualitative expert evaluations confirmed
the relevance and inter-pretability of the generated explanations. This
human-centric approach is designed to enhance trust and interpretability in
data-driven fault detection, thereby contributing to applied system design in
industrial quality control.

</details>


### [334] [PPFL-RDSN: Privacy-Preserving Federated Learning-based Residual Dense Spatial Networks for Encrypted Lossy Image Reconstruction](https://arxiv.org/abs/2507.00230)
*Peilin He,James Joshi*

Main category: cs.LG

Relevance: 30.0

TL;DR: 提出了一种基于联邦学习的隐私保护RDSN框架（PPFL-RDSN），用于有损图像重建，结合本地差分隐私和模型水印技术，确保数据安全和隐私。


<details>
  <summary>Details</summary>
Motivation: 解决集中式训练中的隐私风险（如数据泄露和推理攻击）和高计算成本问题。

Method: 结合联邦学习（FL）、本地差分隐私和鲁棒模型水印技术。

Result: PPFL-RDSN性能与集中式方法相当，同时降低计算负担并有效缓解安全和隐私漏洞。

Conclusion: PPFL-RDSN是安全且隐私保护的协作计算机视觉应用的实用解决方案。

Abstract: Reconstructing high-quality images from low-resolution inputs using Residual
Dense Spatial Networks (RDSNs) is crucial yet challenging, particularly in
collaborative scenarios where centralized training poses significant privacy
risks, including data leakage and inference attacks, as well as high
computational costs. We propose a novel Privacy-Preserving Federated
Learning-based RDSN (PPFL-RDSN) framework specifically tailored for lossy image
reconstruction. PPFL-RDSN integrates Federated Learning (FL), local
differential privacy, and robust model watermarking techniques, ensuring data
remains secure on local devices, safeguarding sensitive information, and
maintaining model authenticity without revealing underlying data. Empirical
evaluations show that PPFL-RDSN achieves comparable performance to the
state-of-the-art centralized methods while reducing computational burdens, and
effectively mitigates security and privacy vulnerabilities, making it a
practical solution for secure and privacy-preserving collaborative computer
vision applications.

</details>


### [335] [Structure-preserving Lift & Learn: Scientific machine learning for nonlinear conservative partial differential equations](https://arxiv.org/abs/2507.00301)
*Harsh Sharma,Juan Diego Draxl Giannoni,Boris Kramer*

Main category: cs.LG

Relevance: 30.0

TL;DR: 提出了一种结构保持的Lift & Learn方法，用于学习非线性偏微分方程（PDEs）的结构保持降阶模型，结合能量二次化策略和混合学习方法。


<details>
  <summary>Details</summary>
Motivation: 解决非线性PDEs的降阶模型问题，同时保持其物理结构，提高计算效率和准确性。

Method: 通过能量二次化策略将非线性PDE转化为二次系统，然后利用混合学习方法学习线性降阶算子，保持结构。

Result: 在多个数值实验中，该方法在准确性和计算效率上与现有最优方法竞争。

Conclusion: 提出的方法能够有效学习结构保持的降阶模型，适用于多种非线性PDEs。

Abstract: This work presents structure-preserving Lift & Learn, a scientific machine
learning method that employs lifting variable transformations to learn
structure-preserving reduced-order models for nonlinear partial differential
equations (PDEs) with conservation laws. We propose a hybrid learning approach
based on a recently developed energy-quadratization strategy that uses
knowledge of the nonlinearity at the PDE level to derive an equivalent
quadratic lifted system with quadratic system energy. The lifted dynamics
obtained via energy quadratization are linear in the old variables, making
model learning very effective in the lifted setting. Based on the lifted
quadratic PDE model form, the proposed method derives quadratic reduced terms
analytically and then uses those derived terms to formulate a constrained
optimization problem to learn the remaining linear reduced operators in a
structure-preserving way. The proposed hybrid learning approach yields
computationally efficient quadratic reduced-order models that respect the
underlying physics of the high-dimensional problem. We demonstrate the
generalizability of quadratic models learned via the proposed
structure-preserving Lift & Learn method through three numerical examples: the
one-dimensional wave equation with exponential nonlinearity, the
two-dimensional sine-Gordon equation, and the two-dimensional
Klein-Gordon-Zakharov equations. The numerical results show that the proposed
learning approach is competitive with the state-of-the-art structure-preserving
data-driven model reduction method in terms of both accuracy and computational
efficiency.

</details>


### [336] [MamNet: A Novel Hybrid Model for Time-Series Forecasting and Frequency Pattern Analysis in Network Traffic](https://arxiv.org/abs/2507.00304)
*Yujun Zhang,Runlong Li,Xiaoxiang Liang,Xinhao Yang,Tian Su,Bo Liu,Yan Zhou*

Main category: cs.LG

Relevance: 30.0

TL;DR: 提出了一种结合时间域和频率域特征的网络流量预测与异常检测模型MamNet，在性能上优于现有主流模型。


<details>
  <summary>Details</summary>
Motivation: 网络流量的异常波动可能预示安全威胁或系统故障，因此高效的预测与检测方法对网络安全至关重要。

Method: 结合Mamba模块（时间域建模）和傅里叶变换（频率域特征提取），通过多尺度信息融合提升异常检测能力。

Result: 在UNSW-NB15和CAIDA数据集上，MamNet在准确率、召回率和F1-Score上优于其他模型，性能提升2%-4%。

Conclusion: MamNet能有效捕捉不同时间尺度的网络流量异常，适用于网络安全和流量管理任务。未来可结合外部事件信息进一步优化。

Abstract: The abnormal fluctuations in network traffic may indicate potential security
threats or system failures. Therefore, efficient network traffic prediction and
anomaly detection methods are crucial for network security and traffic
management. This paper proposes a novel network traffic prediction and anomaly
detection model, MamNet, which integrates time-domain modeling and
frequency-domain feature extraction. The model first captures the long-term
dependencies of network traffic through the Mamba module (time-domain
modeling), and then identifies periodic fluctuations in the traffic using
Fourier Transform (frequency-domain feature extraction). In the feature fusion
layer, multi-scale information is integrated to enhance the model's ability to
detect network traffic anomalies. Experiments conducted on the UNSW-NB15 and
CAIDA datasets demonstrate that MamNet outperforms several recent mainstream
models in terms of accuracy, recall, and F1-Score. Specifically, it achieves an
improvement of approximately 2% to 4% in detection performance for complex
traffic patterns and long-term trend detection. The results indicate that
MamNet effectively captures anomalies in network traffic across different time
scales and is suitable for anomaly detection tasks in network security and
traffic management. Future work could further optimize the model structure by
incorporating external network event information, thereby improving the model's
adaptability and stability in complex network environments.

</details>


### [337] [Diversity Conscious Refined Random Forest](https://arxiv.org/abs/2507.00467)
*Sijan Bhattarai,Saurav Bhandari,Girija Bhusal,Saroj Shakya,Tapendra Pandey*

Main category: cs.LG

Relevance: 30.0

TL;DR: 提出了一种改进的随机森林分类器，通过动态选择信息特征和最大化树多样性来提高分类精度。


<details>
  <summary>Details</summary>
Motivation: 随机森林通常依赖大量树和所有输入特征，导致推理成本高和模型冗余。本文旨在通过动态选择特征和最大化树多样性来优化模型。

Method: 迭代移除不重要的特征，动态生成新树，并通过相关性聚类去除冗余树。

Result: 在多个基准数据集上，改进模型比标准随机森林具有更高的分类精度。

Conclusion: 该方法有效减少了模型冗余并提高了分类性能。

Abstract: Random Forest (RF) is a widely used ensemble learning technique known for its
robust classification performance across diverse domains. However, it often
relies on hundreds of trees and all input features, leading to high inference
cost and model redundancy. In this work, our goal is to grow trees dynamically
only on informative features and then enforce maximal diversity by clustering
and retaining uncorrelated trees. Therefore, we propose a Refined Random Forest
Classifier that iteratively refines itself by first removing the least
informative features and then analytically determines how many new trees should
be grown, followed by correlation-based clustering to remove redundant trees.
The classification accuracy of our model was compared against the standard RF
on the same number of trees. Experiments on 8 multiple benchmark datasets,
including binary and multiclass datasets, demonstrate that the proposed model
achieves improved accuracy compared to standard RF.

</details>


### [338] [Audio-3DVG: Unified Audio - Point Cloud Fusion for 3D Visual Grounding](https://arxiv.org/abs/2507.00669)
*Duc Cao-Dinh,Khai Le-Duc,Anh Dao,Bach Phan Tat,Chris Ngo,Duy M. H. Nguyen,Nguyen X. Khanh,Thanh Nguyen-Tang*

Main category: cs.LG

Relevance: 30.0

TL;DR: 论文提出Audio-3DVG框架，通过分解任务为对象提及检测和音频引导注意力模块，结合音频与空间信息，提升3D视觉定位性能。


<details>
  <summary>Details</summary>
Motivation: 现有3D视觉定位主要依赖文本描述，而基于语音的定位尚未充分探索。利用自动语音识别和语音表示学习的进展，论文提出整合音频与空间信息的方法。

Method: 1. 对象提及检测：多标签分类任务，识别音频中提及的对象；2. 音频引导注意力模块：捕捉候选对象与语音线索的交互。

Result: Audio-3DVG在音频定位中达到新SOTA，性能接近文本方法。

Conclusion: 语音与3D视觉任务的结合具有潜力。

Abstract: 3D Visual Grounding (3DVG) involves localizing target objects in 3D point
clouds based on natural language. While prior work has made strides using
textual descriptions, leveraging spoken language-known as Audio-based 3D Visual
Grounding-remains underexplored and challenging. Motivated by advances in
automatic speech recognition (ASR) and speech representation learning, we
propose Audio-3DVG, a simple yet effective framework that integrates audio and
spatial information for enhanced grounding. Rather than treating speech as a
monolithic input, we decompose the task into two complementary components.
First, we introduce Object Mention Detection, a multi-label classification task
that explicitly identifies which objects are referred to in the audio, enabling
more structured audio-scene reasoning. Second, we propose an Audio-Guided
Attention module that captures interactions between candidate objects and
relational speech cues, improving target discrimination in cluttered scenes. To
support benchmarking, we synthesize audio descriptions for standard 3DVG
datasets, including ScanRefer, Sr3D, and Nr3D. Experimental results demonstrate
that Audio-3DVG not only achieves new state-of-the-art performance in
audio-based grounding, but also competes with text-based methods-highlighting
the promise of integrating spoken language into 3D vision tasks.

</details>


### [339] [SCAWaveNet: A Spatial-Channel Attention-based Network for Global Significant Wave Height Retrieval](https://arxiv.org/abs/2507.00701)
*Chong Zhang,Xichao Liu,Yibing Zhan,Dapeng Tao,Jun Ni*

Main category: cs.LG

Relevance: 30.0

TL;DR: 论文提出了一种基于空间通道注意力的网络SCAWaveNet，用于从GNSS数据中提取有效波高（SWH），通过多通道信息交互提升性能。


<details>
  <summary>Details</summary>
Motivation: 现有深度学习模型在处理GNSS数据时未充分利用多通道信息交互，限制了性能提升。

Method: 设计了一种空间通道注意力机制，将多通道特征建模为独立注意力头，并融合空间和通道信息。

Result: SCAWaveNet在ERA5和NDBC数据集上的RMSE分别为0.438m和0.432m，优于现有模型。

Conclusion: SCAWaveNet通过多通道信息交互显著提升了SWH提取的精度。

Abstract: Recent advancements in spaceborne GNSS missions have produced extensive
global datasets, providing a robust basis for deep learning-based significant
wave height (SWH) retrieval. While existing deep learning models predominantly
utilize CYGNSS data with four-channel information, they often adopt
single-channel inputs or simple channel concatenation without leveraging the
benefits of cross-channel information interaction during training. To address
this limitation, a novel spatial-channel attention-based network, namely
SCAWaveNet, is proposed for SWH retrieval. Specifically, features from each
channel of the DDMs are modeled as independent attention heads, enabling the
fusion of spatial and channel-wise information. For auxiliary parameters, a
lightweight attention mechanism is designed to assign weights along the spatial
and channel dimensions. The final feature integrates both spatial and
channel-level characteristics. Model performance is evaluated using
four-channel CYGNSS data. When ERA5 is used as a reference, SCAWaveNet achieves
an average RMSE of 0.438 m. When using buoy data from NDBC, the average RMSE
reaches 0.432 m. Compared to state-of-the-art models, SCAWaveNet reduces the
average RMSE by at least 3.52% on the ERA5 dataset and by 5.47% on the NDBC
buoy observations. The code is available at
https://github.com/Clifx9908/SCAWaveNet.

</details>


### [340] [BoltzNCE: Learning Likelihoods for Boltzmann Generation with Stochastic Interpolants and Noise Contrastive Estimation](https://arxiv.org/abs/2507.00846)
*Rishal Aggrwal,Jacky Chen,Nicholas M. Boffi,David Ryan Koes*

Main category: cs.LG

Relevance: 30.0

TL;DR: 提出了一种通过能量模型和噪声对比估计学习生成分布似然的方法，避免了计算昂贵的雅可比矩阵，显著提高了大分子系统采样的效率。


<details>
  <summary>Details</summary>
Motivation: 解决Boltzmann生成器中因计算雅可比矩阵导致的大分子系统采样效率低下的问题。

Method: 利用能量模型和噪声对比估计学习生成分布的似然，结合随机插值在生成分布和先验分布之间进行退火。

Result: 在丙氨酸二肽系统中，方法生成的自由能分布和能量分布与精确似然方法相当，且能快速准确估计亚稳态间的自由能差。

Conclusion: 该方法显著提高了大分子系统采样的效率，适用于物理系统建模。

Abstract: Efficient sampling from the Boltzmann distribution defined by an energy
function is a key challenge in modeling physical systems such as molecules.
Boltzmann Generators tackle this by leveraging Continuous Normalizing Flows
that transform a simple prior into a distribution that can be reweighted to
match the Boltzmann distribution using sample likelihoods. However, obtaining
likelihoods requires computing costly Jacobians during integration, making it
impractical for large molecular systems. To overcome this, we propose learning
the likelihood of the generated distribution via an energy-based model trained
with noise contrastive estimation and score matching. By using stochastic
interpolants to anneal between the prior and generated distributions, we
combine both the objective functions to efficiently learn the density function.
On the alanine dipeptide system, we demonstrate that our method yields free
energy profiles and energy distributions comparable to those obtained with
exact likelihoods. Additionally, we show that free energy differences between
metastable states can be estimated accurately with orders-of-magnitude speedup.

</details>


### [341] [Understanding Generalization in Node and Link Prediction](https://arxiv.org/abs/2507.00927)
*Antonis Vasileiou,Timo Stoll,Christopher Morris*

Main category: cs.LG

Relevance: 30.0

TL;DR: 该论文提出了一个统一框架，用于分析消息传递图神经网络（MPNNs）在归纳和传导节点与链接预测任务中的泛化能力，并量化了图结构的影响。


<details>
  <summary>Details</summary>
Motivation: MPNNs在节点和链接预测任务中广泛应用，但其泛化能力尚未被充分理解，尤其是在非独立同分布（non-i.i.d.）假设下。

Method: 引入了一个统一框架，结合多种架构参数和损失函数，分析MPNNs的泛化性能，并量化图结构的影响。

Result: 实证研究支持了理论分析，深化了对MPNNs在这些任务中泛化能力的理解。

Conclusion: 该框架不仅适用于图任务，还可扩展到其他分类任务。

Abstract: Using message-passing graph neural networks (MPNNs) for node and link
prediction is crucial in various scientific and industrial domains, which has
led to the development of diverse MPNN architectures. Besides working well in
practical settings, their ability to generalize beyond the training set remains
poorly understood. While some studies have explored MPNNs' generalization in
graph-level prediction tasks, much less attention has been given to node- and
link-level predictions. Existing works often rely on unrealistic i.i.d.\@
assumptions, overlooking possible correlations between nodes or links, and
assuming fixed aggregation and impractical loss functions while neglecting the
influence of graph structure. In this work, we introduce a unified framework to
analyze the generalization properties of MPNNs in inductive and transductive
node and link prediction settings, incorporating diverse architectural
parameters and loss functions and quantifying the influence of graph structure.
Additionally, our proposed generalization framework can be applied beyond
graphs to any classification task under the inductive or transductive setting.
Our empirical study supports our theoretical insights, deepening our
understanding of MPNNs' generalization capabilities in these tasks.

</details>


### [342] [SwarmFusion: Revolutionizing Disaster Response with Swarm Intelligence and Deep Learning](https://arxiv.org/abs/2507.00005)
*Vasavi Lankipalle*

Main category: cs.NE

Relevance: 30.0

TL;DR: SwarmFusion结合粒子群优化和卷积神经网络，优化灾害响应中的实时资源分配和路径规划，显著提升效率和覆盖率。


<details>
  <summary>Details</summary>
Motivation: 灾害响应需要在混乱环境中快速决策，现有方法效率不足。

Method: 结合粒子群优化和卷积神经网络，处理实时卫星、无人机和传感器数据。

Result: 在DisasterSim2025数据集上，响应时间快40%，幸存者覆盖率达90%。

Conclusion: SwarmFusion为时间紧迫的灾害管理提供了可扩展的数据驱动解决方案。

Abstract: Disaster response requires rapid, adaptive decision-making in chaotic
environments. SwarmFusion, a novel hybrid framework, integrates particle swarm
optimization with convolutional neural networks to optimize real-time resource
allocation and path planning. By processing live satellite, drone, and sensor
data, SwarmFusion enhances situational awareness and operational efficiency in
flood and wildfire scenarios. Simulations using the DisasterSim2025 dataset
demonstrate up to 40 percentage faster response times and 90 percentage
survivor coverage compared to baseline methods. This scalable, data-driven
approach offers a transformative solution for time-critical disaster
management, with potential applications across diverse crisis scenarios.

</details>


### [343] [GRAND: Graph Release with Assured Node Differential Privacy](https://arxiv.org/abs/2507.00402)
*Suqing Liu,Xuan Bi,Tianxi Li*

Main category: stat.ML

Relevance: 30.0

TL;DR: GRAND是首个确保节点级差分隐私并保留网络结构特性的网络发布机制。


<details>
  <summary>Details</summary>
Motivation: 现有方法在节点级隐私保护上存在局限性，无法同时满足隐私保护和结构保留的需求。

Method: 提出GRAND机制，基于潜在空间模型，确保发布的网络在节点级差分隐私下保持原始网络的分布特性。

Result: 实验证明GRAND在合成和真实数据集上均有效。

Conclusion: GRAND为节点级隐私保护的网络发布提供了可行方案。

Abstract: Differential privacy is a well-established framework for safeguarding
sensitive information in data. While extensively applied across various
domains, its application to network data -- particularly at the node level --
remains underexplored. Existing methods for node-level privacy either focus
exclusively on query-based approaches, which restrict output to pre-specified
network statistics, or fail to preserve key structural properties of the
network. In this work, we propose GRAND (Graph Release with Assured Node
Differential privacy), which is, to the best of our knowledge, the first
network release mechanism that releases entire networks while ensuring
node-level differential privacy and preserving structural properties. Under a
broad class of latent space models, we show that the released network
asymptotically follows the same distribution as the original network. The
effectiveness of the approach is evaluated through extensive experiments on
both synthetic and real-world datasets.

</details>


### [344] [Simulation-Efficient Cosmological Inference with Multi-Fidelity SBI](https://arxiv.org/abs/2507.00514)
*Leander Thiele,Adrian E. Bayer,Naoya Takeishi*

Main category: astro-ph.CO

Relevance: 30.0

TL;DR: 通过结合不同精度的模拟数据集，提出了一种基于特征匹配和知识蒸馏的多精度推理方法，降低了宇宙学模拟推理的成本，并在小预算和复杂问题上提升了后验质量。


<details>
  <summary>Details</summary>
Motivation: 宇宙学模拟推理成本高，多精度模拟集结合可降低成本，但需要有效方法。

Method: 基于特征匹配和知识蒸馏的多精度推理方法。

Result: 在小预算和复杂推理问题上，后验质量得到提升。

Conclusion: 该方法有效降低了成本并提升了推理质量，适用于资源受限的场景。

Abstract: The simulation cost for cosmological simulation-based inference can be
decreased by combining simulation sets of varying fidelity. We propose an
approach to such multi-fidelity inference based on feature matching and
knowledge distillation. Our method results in improved posterior quality,
particularly for small simulation budgets and difficult inference problems.

</details>


### [345] [A Practical Guide to Interpretable Role-Based Clustering in Multi-Layer Financial Networks](https://arxiv.org/abs/2507.00600)
*Christian Franssen,Iman van Lelyveld,Bernd Heidergott*

Main category: cs.SI

Relevance: 30.0

TL;DR: 提出了一种可解释的基于角色的聚类方法，用于分析多层金融网络中金融机构的功能角色。


<details>
  <summary>Details</summary>
Motivation: 理解金融机构在互联市场中的功能角色对监管、系统性风险评估和解决方案规划至关重要。

Method: 基于邻近度度量、聚类评估标准和算法选择的一般聚类框架，构建可解释的节点嵌入，捕捉直接和间接交易关系。

Result: 使用ECB的MMSR数据，识别出市场中介、跨段连接器和外围借贷者等异质角色。

Conclusion: 基于角色的聚类方法在分析金融网络和理解复杂市场结构中的机构行为方面具有灵活性和实用价值。

Abstract: Understanding the functional roles of financial institutions within
interconnected markets is critical for effective supervision, systemic risk
assessment, and resolution planning. We propose an interpretable role-based
clustering approach for multi-layer financial networks, designed to identify
the functional positions of institutions across different market segments. Our
method follows a general clustering framework defined by proximity measures,
cluster evaluation criteria, and algorithm selection. We construct explainable
node embeddings based on egonet features that capture both direct and indirect
trading relationships within and across market layers. Using transaction-level
data from the ECB's Money Market Statistical Reporting (MMSR), we demonstrate
how the approach uncovers heterogeneous institutional roles such as market
intermediaries, cross-segment connectors, and peripheral lenders or borrowers.
The results highlight the flexibility and practical value of role-based
clustering in analyzing financial networks and understanding institutional
behavior in complex market structures.

</details>


### [346] [Geometric Gaussian Approximations of Probability Distributions](https://arxiv.org/abs/2507.00616)
*Nathaël Da Costa,Bálint Mucsányi,Philipp Hennig*

Main category: math.DG

Relevance: 30.0

TL;DR: 该论文研究了几何高斯近似的表达能力，探讨了通过微分同胚或黎曼指数映射的高斯推近方法，证明了其通用性，并讨论了统一微分同胚的可能性。


<details>
  <summary>Details</summary>
Motivation: 近似复杂概率分布（如贝叶斯后验分布）在许多应用中具有重要意义，本文旨在探索几何高斯近似的表达能力及其通用性。

Method: 论文首先回顾了两种几何高斯近似方法（微分同胚和黎曼指数映射），分析了它们之间的关系，并提供了构造性证明其通用性。

Result: 证明了几何高斯近似可以捕获任何概率分布，具有通用性。

Conclusion: 几何高斯近似在理论上具有通用性，但统一微分同胚的可行性仍需进一步研究。

Abstract: Approximating complex probability distributions, such as Bayesian posterior
distributions, is of central interest in many applications. We study the
expressivity of geometric Gaussian approximations. These consist of
approximations by Gaussian pushforwards through diffeomorphisms or Riemannian
exponential maps. We first review these two different kinds of geometric
Gaussian approximations. Then we explore their relationship to one another. We
further provide a constructive proof that such geometric Gaussian
approximations are universal, in that they can capture any probability
distribution. Finally, we discuss whether, given a family of probability
distributions, a common diffeomorphism can be found to obtain uniformly
high-quality geometric Gaussian approximations for that family.

</details>


### [347] [Forward Reverse Kernel Regression for the Schrödinger bridge problem](https://arxiv.org/abs/2507.00640)
*Denis Belomestny,John. Schoenmakers*

Main category: stat.ML

Relevance: 30.0

TL;DR: 本文研究了Schrödinger桥问题（SBP），提出了一种非参数化的蒙特卡洛迭代方法，用于近似Schrödinger势能，并通过核回归和固定点迭代实现收敛。


<details>
  <summary>Details</summary>
Motivation: SBP在熵最优传输中具有核心地位，研究其非参数化近似方法有助于理论发展和实际应用。

Method: 采用前向-反向迭代蒙特卡洛方法，结合核回归和固定点迭代，确保收敛性和最优性。

Result: 证明了算法的收敛性，并给出了势能估计的收敛速率及其最优性。

Conclusion: 该方法为Schrödinger桥问题的求解提供了有效的非参数化工具，并展示了在条件扩散模拟中的应用潜力。

Abstract: In this paper, we study the Schr\"odinger Bridge Problem (SBP), which is
central to entropic optimal transport. For general reference processes and
begin--endpoint distributions, we propose a forward-reverse iterative Monte
Carlo procedure to approximate the Schr\"odinger potentials in a nonparametric
way. In particular, we use kernel based Monte Carlo regression in the context
of Picard iteration of a corresponding fixed point problem. By preserving in
the iteration positivity and contractivity in a Hilbert metric sense, we
develop a provably convergent algorithm. Furthermore, we provide convergence
rates for the potential estimates and prove their optimality. Finally, as an
application, we propose a non-nested Monte Carlo procedure for the final
dimensional distributions of the Schr\"odinger Bridge process, based on the
constructed potentials and the forward-reverse simulation method for
conditional diffusions.

</details>


### [348] [Hebbian Physics Networks: A Self-Organizing Computational Architecture Based on Local Physical Laws](https://arxiv.org/abs/2507.00641)
*Gunjan Auti,Hirofumi Daiguji,Gouhei Tanaka*

Main category: nlin.AO

Relevance: 30.0

TL;DR: 论文提出了一种自组织的Hebbian物理网络（HPN），通过局部Hebbian更新学习物理约束，无需全局优化。


<details>
  <summary>Details</summary>
Motivation: 传统物理机器学习方法依赖全局优化，限制了可解释性且需外部强制物理约束。HPN旨在通过局部动力学直接编码物理定律，提高可解释性和物理一致性。

Method: HPN利用守恒定律的违反作为热力学信号，驱动局部Hebbian可塑性更新权重。该方法基于非平衡热力学和耗散结构理论。

Result: 在不可压缩流体流动和连续扩散问题中，HPN从随机初始条件中无监督地生成物理一致的结构。

Conclusion: HPN将计算重构为残差驱动的热力学过程，为复杂动力系统建模提供了可解释、可扩展且物理基础的方法。

Abstract: Traditional machine learning approaches in physics rely on global
optimization, limiting interpretability and enforcing physical constraints
externally. We introduce the Hebbian Physics Network (HPN), a self-organizing
computational framework in which learning emerges from local Hebbian updates
driven by violations of conservation laws. Grounded in non-equilibrium
thermodynamics and inspired by Prigogine/'s theory of dissipative structures,
HPNs eliminate the need for global loss functions by encoding physical laws
directly into the system/'s local dynamics. Residuals - quantified imbalances
in continuity, momentum, or energy - serve as thermodynamic signals that drive
weight adaptation through generalized Hebbian plasticity. We demonstrate this
approach on incompressible fluid flow and continuum diffusion, where physically
consistent structures emerge from random initial conditions without
supervision. HPNs reframe computation as a residual-driven thermodynamic
process, offering an interpretable, scalable, and physically grounded
alternative for modeling complex dynamical systems.

</details>


### [349] [SINDy on slow manifolds](https://arxiv.org/abs/2507.00747)
*Diemen Delgado-Cano,Erick Kracht,Urban Fasel,Benjamin Herrmann*

Main category: math.DS

Relevance: 30.0

TL;DR: 提出了一种改进的SINDy方法，用于高效识别高维慢-快动力学系统的慢流形和慢变量动态。


<details>
  <summary>Details</summary>
Motivation: 高维慢-快动力学系统的回归问题计算复杂且病态，传统SINDy方法难以处理。

Method: 分两步：(i)识别慢流形，(ii)在流形上学习慢变量动态，并构建定制函数库以减少计算复杂度。

Result: 显著降低了SINDy库的条件数和规模，提高了慢流形动态识别的准确性。

Conclusion: 该方法为高维慢-快动力学系统提供了一种高效且鲁棒的建模工具。

Abstract: The sparse identification of nonlinear dynamics (SINDy) has been established
as an effective method to learn interpretable models of dynamical systems from
data. However, for high-dimensional slow-fast dynamical systems, the regression
problem becomes simultaneously computationally intractable and ill-conditioned.
Although, in principle, modeling only the dynamics evolving on the underlying
slow manifold addresses both of these challenges, the truncated fast variables
have to be compensated by including higher-order nonlinearities as candidate
terms for the model, leading to an explosive growth in the size of the SINDy
library. In this work, we develop a SINDy variant that is able to robustly and
efficiently identify slow-fast dynamics in two steps: (i) identify the slow
manifold, that is, an algebraic equation for the fast variables as functions of
the slow ones, and (ii) learn a model for the dynamics of the slow variables
restricted to the manifold. Critically, the equation learned in (i) is
leveraged to build a manifold-informed function library for (ii) that contains
only essential higher-order nonlinearites as candidate terms. Rather than
containing all monomials of up to a certain degree, the resulting custom
library is a sparse subset of the latter that is tailored to the specific
problem at hand. The approach is demonstrated on numerical examples of a
snap-through buckling beam and the flow over a NACA 0012 airfoil. We find that
our method significantly reduces both the condition number and the size of the
SINDy library, thus enabling accurate identification of the dynamics on slow
manifolds.

</details>


### [350] [Template-Fitting Meets Deep Learning: Redshift Estimation Using Physics-Guided Neural Networks](https://arxiv.org/abs/2507.00866)
*Jonas Chris Ferrao,Dickson Dias,Pranav Naik,Glory D'Cruz,Anish Naik,Siya Khandeparkar,Manisha Gokuldas Fal Dessai*

Main category: astro-ph.IM

Relevance: 30.0

TL;DR: 提出了一种结合模板拟合和深度学习的混合方法，用于光度红移估计，通过物理引导的神经网络嵌入光谱能量分布模板，并在PREML数据集上验证了其性能。


<details>
  <summary>Details</summary>
Motivation: 光度红移估计在观测宇宙学中至关重要，尤其是在大规模调查中，传统方法各有局限性。

Method: 结合模板拟合与深度学习，使用物理引导的神经网络，嵌入光谱能量分布模板，采用多模态设计和交叉注意力机制，融合光度与图像数据，并引入贝叶斯层进行不确定性估计。

Result: 在PREML数据集上，RMS误差为0.0507，3-sigma异常值率为0.13%，偏差为0.0028，满足LSST红移估计要求中的两项。

Conclusion: 物理引导模板与数据驱动模型的结合为未来宇宙学调查中的稳健红移估计提供了潜力。

Abstract: Accurate photometric redshift estimation is critical for observational
cosmology, especially in large-scale surveys where spectroscopic measurements
are impractical. Traditional approaches include template fitting and machine
learning, each with distinct strengths and limitations. We present a hybrid
method that integrates template fitting with deep learning using physics-guided
neural networks. By embedding spectral energy distribution templates into the
network architecture, our model encodes physical priors into the training
process. The system employs a multimodal design, incorporating cross-attention
mechanisms to fuse photometric and image data, along with Bayesian layers for
uncertainty estimation. We evaluate our model on the publicly available PREML
dataset, which includes approximately 400,000 galaxies from the Hyper
Suprime-Cam PDR3 release, with 5-band photometry, multi-band imaging, and
spectroscopic redshifts. Our approach achieves an RMS error of 0.0507, a
3-sigma catastrophic outlier rate of 0.13%, and a bias of 0.0028. The model
satisfies two of the three LSST photometric redshift requirements for redshifts
below 3. These results highlight the potential of combining physically
motivated templates with data-driven models for robust redshift estimation in
upcoming cosmological surveys.

</details>


### [351] [An in depth look at the Procrustes-Wasserstein distance: properties and barycenters](https://arxiv.org/abs/2507.00894)
*Davide Adamo,Marco Corneli,Manon Vuillien,Emmanuelle Vila*

Main category: stat.ML

Relevance: 30.0

TL;DR: 论文介绍了Procrustes-Wasserstein (PW)距离，用于点云对齐和比较，并提出了PW重心概念及其算法，展示了在考古学中的实用性。


<details>
  <summary>Details</summary>
Motivation: PW距离对刚体变换（如旋转和反射）具有不变性，更适合点云对齐和比较任务。

Method: 构建离散概率测度空间，验证PW为距离；提出多种初始化策略，引入PW重心概念并设计算法。

Result: 在需要精确对齐和形状保持的场景中，PW方法优于现有OT方法。

Conclusion: PW在机器学习和计算几何的点云分析中具有潜力。

Abstract: Due to its invariance to rigid transformations such as rotations and
reflections, Procrustes-Wasserstein (PW) was introduced in the literature as an
optimal transport (OT) distance, alternative to Wasserstein and more suited to
tasks such as the alignment and comparison of point clouds. Having that
application in mind, we carefully build a space of discrete probability
measures and show that over that space PW actually is a distance. Algorithms to
solve the PW problems already exist, however we extend the PW framework by
discussing and testing several initialization strategies. We then introduce the
notion of PW barycenter and detail an algorithm to estimate it from the data.
The result is a new method to compute representative shapes from a collection
of point clouds. We benchmark our method against existing OT approaches,
demonstrating superior performance in scenarios requiring precise alignment and
shape preservation. We finally show the usefulness of the PW barycenters in an
archaeological context. Our results highlight the potential of PW in boosting
2D and 3D point cloud analysis for machine learning and computational geometry
applications.

</details>


### [352] [Data Collection with Non-Uniform Axial Power for Phase II of the OECD/NEA AI/ML Critical Heat Flux Benchmark](https://arxiv.org/abs/2507.00034)
*Reece Bourisaw,Reid McCants,Jean-Marie Le Corre,Anna Iskhakova,Arsen S. Iskhakov*

Main category: cs.LG

Relevance: 20.0

TL;DR: 该论文整理了临界热通量（CHF）数据集，用于支持OECD/NEA AI/ML基准测试的第二阶段，重点关注非均匀加热条件下的模型性能。


<details>
  <summary>Details</summary>
Motivation: 支持OECD/NEA AI/ML基准测试的第二阶段，解决非均匀加热条件下CHF预测的挑战。

Method: 数据集通过提取技术报告中的加热曲线，插值到一致的轴向网格，并通过能量平衡验证。

Result: 传统CHF相关性在非均匀条件下表现不佳，神经网络在均匀数据上表现良好但无法泛化到非均匀场景。

Conclusion: 研究为高级迁移学习策略和不确定性量化奠定了基础。

Abstract: Critical heat flux (CHF) marks the onset of boiling crisis in light-water
reactors, defining safe thermal-hydraulic operating limits. To support Phase II
of the OECD/NEA AI/ML CHF benchmark, which introduces spatially varying power
profiles, this work compiles and digitizes a broad CHF dataset covering both
uniform and non-uniform axial heating conditions. Heating profiles were
extracted from technical reports, interpolated onto a consistent axial mesh,
validated via energy-balance checks, and encoded in machine-readable formats
for benchmark compatibility.
  Classical CHF correlations exhibit substantial errors under uniform heating
and degrade markedly when applied to non-uniform profiles, while modern tabular
methods offer improved but still imperfect predictions. A neural network
trained solely on uniform data performs well in that regime but fails to
generalize to spatially varying scenarios, underscoring the need for models
that explicitly incorporate axial power distributions. By providing these
curated datasets and baseline modeling results, this study lays the groundwork
for advanced transfer-learning strategies, rigorous uncertainty quantification,
and design-optimization efforts in the next phase of the CHF benchmark.

</details>


### [353] [A new machine learning framework for occupational accidents forecasting with safety inspections integration](https://arxiv.org/abs/2507.00089)
*Aho Yapi,Pierre Latouche,Arnaud Guillin,Yan Bailly*

Main category: cs.LG

Relevance: 20.0

TL;DR: 提出了一种基于安全检查和二元时间序列的短期职业事故预测框架，LSTM表现最佳，平衡准确率达0.86。


<details>
  <summary>Details</summary>
Motivation: 将安全检查数据转化为可操作的每周风险评估，帮助决策者提前干预高风险时期。

Method: 结合滑动窗口交叉验证和多种机器学习算法（如逻辑回归、树模型和LSTM）进行预测。

Result: LSTM表现最优，平衡准确率为0.86，能有效识别高风险时期。

Conclusion: 该方法能将安全检查数据转化为实用的风险评估工具，提升安全投资回报。

Abstract: We propose a generic framework for short-term occupational accident
forecasting that leverages safety inspections and models accident occurrences
as binary time series. The approach generates daily predictions, which are then
aggregated into weekly safety assessments to better inform decision making. To
ensure the reliability and operational applicability of the forecasts, we apply
a sliding-window cross-validation procedure specifically designed for time
series data, combined with an evaluation based on aggregated period-level
metrics. Several machine learning algorithms, including logistic regression,
tree-based models, and neural networks, are trained and systematically compared
within this framework. Unlike the other approaches, the long short-term memory
(LSTM) network outperforms the other approaches and detects the upcoming
high-risk periods with a balanced accuracy of 0.86, confirming the robustness
of our methodology and demonstrating that a binary time series model can
anticipate these critical periods based on safety inspections. The proposed
methodology converts routine safety inspection data into clear weekly risk
scores, detecting the periods when accidents are most likely. Decision-makers
can integrate these scores into their planning tools to classify inspection
priorities, schedule targeted interventions, and funnel resources to the sites
or shifts classified as highest risk, stepping in before incidents occur and
getting the greatest return on safety investments.

</details>


### [354] [Graph Neural Networks in Wind Power Forecasting](https://arxiv.org/abs/2507.00105)
*Javier Castellano,Ignacio Villanueva*

Main category: cs.LG

Relevance: 20.0

TL;DR: 该论文研究了图神经网络（GNN）在风能预测中的应用，发现某些架构的性能与最佳CNN基准相当。


<details>
  <summary>Details</summary>
Motivation: 探索GNN在风能预测中的适用性，以替代或补充传统的CNN方法。

Method: 在三个风电场使用五年历史数据和数值天气预报（NWP）变量作为预测因子，评估了24至36小时预测范围内的模型性能。

Result: 某些GNN架构的表现与最佳CNN基准相当。

Conclusion: GNN在风能预测中具有潜力，可作为CNN的替代方案。

Abstract: We study the applicability of GNNs to the problem of wind energy forecasting.
We find that certain architectures achieve performance comparable to our best
CNN-based benchmark. The study is conducted on three wind power facilities
using five years of historical data. Numerical Weather Prediction (NWP)
variables were used as predictors, and models were evaluated on a 24 to 36 hour
ahead test horizon.

</details>


### [355] [Exploring Theory-Laden Observations in the Brain Basis of Emotional Experience](https://arxiv.org/abs/2507.00320)
*Christiana Westlin,Ashutosh Singh,Deniz Erdogmus,Georgios Stratis,Lisa Feldman Barrett*

Main category: cs.LG

Relevance: 20.0

TL;DR: 论文重新分析了情绪科学中的数据，发现情绪类别内部的脑模式存在显著个体差异，挑战了传统情绪分类的假设。


<details>
  <summary>Details</summary>
Motivation: 传统情绪科学假设情绪类别是生物和心理类型学，但本研究提出情绪类别是可变、情境化的实例集合，旨在验证这一观点。

Method: 重新分析了一项基于类型学假设的研究数据，采用最小假设的方差结构分析方法。

Result: 未观察到原始研究中的情绪类别与脑模式映射，而是发现情绪类别内部存在显著个体差异。

Conclusion: 研究强调了初始假设对科学结论的影响，建议采用多种分析方法验证假设。

Abstract: In the science of emotion, it is widely assumed that folk emotion categories
form a biological and psychological typology, and studies are routinely
designed and analyzed to identify emotion-specific patterns. This approach
shapes the observations that studies report, ultimately reinforcing the
assumption that guided the investigation. Here, we reanalyzed data from one
such typologically-guided study that reported mappings between individual brain
patterns and group-averaged ratings of 34 emotion categories. Our reanalysis
was guided by an alternative view of emotion categories as populations of
variable, situated instances, and which predicts a priori that there will be
significant variation in brain patterns within a category across instances.
Correspondingly, our analysis made minimal assumptions about the structure of
the variance present in the data. As predicted, we did not observe the original
mappings and instead observed significant variation across individuals. These
findings demonstrate how starting assumptions can ultimately impact scientific
conclusions and suggest that a hypothesis must be supported using multiple
analytic methods before it is taken seriously.

</details>


### [356] [Quantum Approximate Optimization Algorithm for Spatiotemporal Forecasting of HIV Clusters](https://arxiv.org/abs/2507.00848)
*Don Roosan,Saif Nirzhor,Rubayat Khan,Fahmida Hai,Mohammad Rifat Haidar*

Main category: cs.LG

Relevance: 20.0

TL;DR: 论文使用量子加速机器学习方法分析HIV流行数据，在聚类检测和预测方面优于经典方法，并揭示了社会因素与HIV传播的因果关系。


<details>
  <summary>Details</summary>
Motivation: HIV流行病数据日益复杂，需要先进的计算方法进行准确的聚类检测和预测，以指导干预措施和资源分配。

Method: 结合量子近似优化算法（QAOA）进行聚类检测，开发混合量子-经典神经网络进行HIV流行预测，并使用量子贝叶斯网络分析社会因素与HIV发病率的因果关系。

Result: QAOA方法在1.6秒内达到92%的聚类检测准确率，混合神经网络预测准确率为94%，量子贝叶斯分析揭示了住房不稳定是HIV传播的关键驱动因素。

Conclusion: 量子增强方法在HIV监测中提供了更高的精度和效率，并为针对性干预提供了科学依据。

Abstract: HIV epidemiological data is increasingly complex, requiring advanced
computation for accurate cluster detection and forecasting. We employed
quantum-accelerated machine learning to analyze HIV prevalence at the ZIP-code
level using AIDSVu and synthetic SDoH data for 2022. Our approach compared
classical clustering (DBSCAN, HDBSCAN) with a quantum approximate optimization
algorithm (QAOA), developed a hybrid quantum-classical neural network for HIV
prevalence forecasting, and used quantum Bayesian networks to explore causal
links between SDoH factors and HIV incidence. The QAOA-based method achieved
92% accuracy in cluster detection within 1.6 seconds, outperforming classical
algorithms. Meanwhile, the hybrid quantum-classical neural network predicted
HIV prevalence with 94% accuracy, surpassing a purely classical counterpart.
Quantum Bayesian analysis identified housing instability as a key driver of HIV
cluster emergence and expansion, with stigma exerting a geographically variable
influence. These quantum-enhanced methods deliver greater precision and
efficiency in HIV surveillance while illuminating critical causal pathways.
This work can guide targeted interventions, optimize resource allocation for
PrEP, and address structural inequities fueling HIV transmission.

</details>


### [357] [HyperFusion: Hierarchical Multimodal Ensemble Learning for Social Media Popularity Prediction](https://arxiv.org/abs/2507.00926)
*Liliang Ye,Yunyao Zhang,Yafeng Wu,Yi-Ping Phoebe Chen,Junqing Yu,Wei Yang,Zikai Song*

Main category: cs.MM

Relevance: 20.0

TL;DR: HyperFusion是一个用于社交媒体流行度预测的多模态集成学习框架，采用分层融合架构和两阶段训练方法，在SMP挑战数据集上表现优异。


<details>
  <summary>Details</summary>
Motivation: 社交媒体流行度预测对内容优化和用户参与至关重要，但由于多模态数据的复杂性，预测具有挑战性。

Method: 使用分层融合架构整合视觉、文本和时间-空间元数据特征，结合CatBoost、TabNet和多层感知机，采用两阶段训练和伪标签技术。

Result: 在SMP Challenge 2025（图像赛道）中获得第三名，表现优异。

Conclusion: HyperFusion通过多模态特征融合和分层集成策略，有效提升了社交媒体流行度预测的准确性。

Abstract: Social media popularity prediction plays a crucial role in content
optimization, marketing strategies, and user engagement enhancement across
digital platforms. However, predicting post popularity remains challenging due
to the complex interplay between visual, textual, temporal, and user behavioral
factors. This paper presents HyperFusion, a hierarchical multimodal ensemble
learning framework for social media popularity prediction. Our approach employs
a three-tier fusion architecture that progressively integrates features across
abstraction levels: visual representations from CLIP encoders, textual
embeddings from transformer models, and temporal-spatial metadata with user
characteristics. The framework implements a hierarchical ensemble strategy
combining CatBoost, TabNet, and custom multi-layer perceptrons. To address
limited labeled data, we propose a two-stage training methodology with
pseudo-labeling and iterative refinement. We introduce novel cross-modal
similarity measures and hierarchical clustering features that capture
inter-modal dependencies. Experimental results demonstrate that HyperFusion
achieves competitive performance on the SMP challenge dataset. Our team
achieved third place in the SMP Challenge 2025 (Image Track). The source code
is available at https://anonymous.4open.science/r/SMPDImage.

</details>


### [358] [Online Meal Detection Based on CGM Data Dynamics](https://arxiv.org/abs/2507.00080)
*Ali Tavasoli,Heman Shakeri*

Main category: cs.LG

Relevance: 10.0

TL;DR: 利用动态模式从连续血糖监测数据中检测餐食事件，提高准确性和可解释性。


<details>
  <summary>Details</summary>
Motivation: 通过捕捉血糖动态的关键特征，改进餐食事件的检测方法。

Method: 利用动态模式作为特征，提取血糖数据的模式和异常。

Result: 提高了检测准确性，增强了动态特征的可解释性。

Conclusion: 该方法优于传统方法，适用于多样化数据集和实际应用。

Abstract: We utilize dynamical modes as features derived from Continuous Glucose
Monitoring (CGM) data to detect meal events. By leveraging the inherent
properties of underlying dynamics, these modes capture key aspects of glucose
variability, enabling the identification of patterns and anomalies associated
with meal consumption. This approach not only improves the accuracy of meal
detection but also enhances the interpretability of the underlying glucose
dynamics. By focusing on dynamical features, our method provides a robust
framework for feature extraction, facilitating generalization across diverse
datasets and ensuring reliable performance in real-world applications. The
proposed technique offers significant advantages over traditional approaches,
improving detection accuracy,

</details>


### [359] [Neural Augmented Kalman Filters for Road Network assisted GNSS positioning](https://arxiv.org/abs/2507.00654)
*Hans van Gorp,Davide Belli,Amir Jalalirad,Bence Major*

Main category: cs.LG

Relevance: 10.0

TL;DR: 论文提出了一种基于时间图神经网络（TGNN）的方法，结合道路网络数据改进GNSS定位精度，相比传统方法减少了29%的误差。


<details>
  <summary>Details</summary>
Motivation: 解决GNSS在密集城市环境中的多路径和非视距误差问题，提升定位精度。

Method: 使用TGNN预测正确的道路段及其不确定性，并将其整合到卡尔曼滤波器中。

Result: 在真实GNSS数据和开源道路网络上验证，定位误差减少了29%。

Conclusion: 首次提出结合道路网络数据和GNSS测量的深度学习定位方法。

Abstract: The Global Navigation Satellite System (GNSS) provides critical positioning
information globally, but its accuracy in dense urban environments is often
compromised by multipath and non-line-of-sight errors. Road network data can be
used to reduce the impact of these errors and enhance the accuracy of a
positioning system. Previous works employing road network data are either
limited to offline applications, or rely on Kalman Filter (KF) heuristics with
little flexibility and robustness. We instead propose training a Temporal Graph
Neural Network (TGNN) to integrate road network information into a KF. The TGNN
is designed to predict the correct road segment and its associated uncertainty
to be used in the measurement update step of the KF. We validate our approach
with real-world GNSS data and open-source road networks, observing a 29%
decrease in positioning error for challenging scenarios compared to a GNSS-only
KF. To the best of our knowledge, ours is the first deep learning-based
approach jointly employing road network data and GNSS measurements to determine
the user position on Earth.

</details>


### [360] [Machine Learning-based Early Detection of Potato Sprouting Using Electrophysiological Signals](https://arxiv.org/abs/2507.00862)
*Davide Andreoletti,Aris Marcolongo,Natasa Sarafijanovic Djukic,Julien Roulet,Stefano Billeter,Andrzej Kurenda,Margot Visse-Mansiaux,Brice Dupuis,Carrol Annette Plummer,Beatrice Paoli,Omran Ayoub*

Main category: cs.LG

Relevance: 10.0

TL;DR: 论文提出了一种基于机器学习的早期预测马铃薯发芽的方法，利用电生理信号和不确定性量化技术，实验结果显示了一定的预测准确性，但仍需进一步优化。


<details>
  <summary>Details</summary>
Motivation: 由于传统视觉识别方法只能在发芽形态变化后检测，无法实现早期干预，且现有化学抑制剂成本高，因此需要一种可靠的早期预测方法。

Method: 通过专用传感器记录电生理信号，预处理后提取小波域特征，训练监督学习模型，并结合不确定性量化技术。

Result: 实验结果表明，该方法能准确预测部分马铃薯的发芽日期，整体平均误差可接受，但最大偏差仍需优化。

Conclusion: 该方法为马铃薯早期发芽预测提供了可行方案，但需进一步减少预测误差以提高实用性。

Abstract: Accurately predicting potato sprouting before the emergence of any visual
signs is critical for effective storage management, as sprouting degrades both
the commercial and nutritional value of tubers. Effective forecasting allows
for the precise application of anti-sprouting chemicals (ASCs), minimizing
waste and reducing costs. This need has become even more pressing following the
ban on Isopropyl N-(3-chlorophenyl) carbamate (CIPC) or Chlorpropham due to
health and environmental concerns, which has led to the adoption of
significantly more expensive alternative ASCs. Existing approaches primarily
rely on visual identification, which only detects sprouting after morphological
changes have occurred, limiting their effectiveness for proactive management. A
reliable early prediction method is therefore essential to enable timely
intervention and improve the efficiency of post-harvest storage strategies,
where early refers to detecting sprouting before any visible signs appear. In
this work, we address the problem of early prediction of potato sprouting. To
this end, we propose a novel machine learning (ML)-based approach that enables
early prediction of potato sprouting using electrophysiological signals
recorded from tubers using proprietary sensors. Our approach preprocesses the
recorded signals, extracts relevant features from the wavelet domain, and
trains supervised ML models for early sprouting detection. Additionally, we
incorporate uncertainty quantification techniques to enhance predictions.
Experimental results demonstrate promising performance in the early detection
of potato sprouting by accurately predicting the exact day of sprouting for a
subset of potatoes and while showing acceptable average error across all
potatoes. Despite promising results, further refinements are necessary to
minimize prediction errors, particularly in reducing the maximum observed
deviations.

</details>


### [361] [Augmented Physics-Based Li-ion Battery Model via Adaptive Ensemble Sparse Learning and Conformal Prediction](https://arxiv.org/abs/2507.00353)
*Samuel Filgueira da Silva,Mehmet Fatih Ozkan,Faissal El Idrissi,Marcello Canova*

Main category: eess.SY

Relevance: 10.0

TL;DR: 论文提出了一种自适应集成稀疏识别（AESI）框架，用于提高锂离子电池简化模型的准确性，通过补偿不可预测的动态行为，并结合保形预测方法提供不确定性量化。


<details>
  <summary>Details</summary>
Motivation: 锂离子电池的简化模型在捕捉复杂非线性行为（如高倍率条件下的电压响应动态）时存在局限性，需要提高其准确性和可靠性。

Method: 结合扩展单粒子模型（ESPM）与进化集成稀疏学习策略，构建混合模型，并引入保形预测方法进行不确定性量化。

Result: 混合模型（ESPM + AESI）在未见数据上电压预测精度提高，均方误差降低达46%，保形预测提供了统计有效的预测区间。

Conclusion: AESI框架显著提升了锂离子电池简化模型的准确性和可靠性，适用于实际应用。

Abstract: Accurate electrochemical models are essential for the safe and efficient
operation of lithium-ion batteries in real-world applications such as
electrified vehicles and grid storage. Reduced-order models (ROM) offer a
balance between fidelity and computational efficiency but often struggle to
capture complex and nonlinear behaviors, such as the dynamics in the cell
voltage response under high C-rate conditions. To address these limitations,
this study proposes an Adaptive Ensemble Sparse Identification (AESI) framework
that enhances the accuracy of reduced-order li-ion battery models by
compensating for unpredictable dynamics. The approach integrates an Extended
Single Particle Model (ESPM) with an evolutionary ensemble sparse learning
strategy to construct a robust hybrid model. In addition, the AESI framework
incorporates a conformal prediction method to provide theoretically guaranteed
uncertainty quantification for voltage error dynamics, thereby improving the
reliability of the model's predictions. Evaluation across diverse operating
conditions shows that the hybrid model (ESPM + AESI) improves the voltage
prediction accuracy, achieving mean squared error reductions of up to 46% on
unseen data. Prediction reliability is further supported by conformal
prediction, yielding statistically valid prediction intervals with coverage
ratios of 96.85% and 97.41% for the ensemble models based on bagging and
stability selection, respectively.

</details>


### [362] [Atmospheric model-trained machine learning selection and classification of ultracool TY dwarfs](https://arxiv.org/abs/2507.00957)
*Ankit Biswas*

Main category: astro-ph.SR

Relevance: 10.0

TL;DR: 论文提出了一种新的机器学习框架，用于检测和分类晚型T和Y矮星，完全基于大气模型合成的光度数据训练。


<details>
  <summary>Details</summary>
Motivation: 现有检测框架通常只能识别M、L和早期T矮星，晚型T和Y矮星的普查仍不完整。

Method: 利用ATMO 2020和Sonora Bobcat模型生成合成光度数据，训练分类器。

Result: 模型在合成和实证数据集上表现优异，分类准确率>99%，光谱类型精度平均为0.35 +/- 0.37子类型。

Conclusion: 该方法能够从光度目录中发现暗淡的晚型矮星，验证了其有效性。

Abstract: The T and Y spectral classes represent the coolest and lowest-mass population
of brown dwarfs, yet their census remains incomplete due to limited statistics.
Existing detection frameworks are often constrained to identifying M, L, and
early T dwarfs, owing to the sparse observational sample of ultracool dwarfs
(UCDs) at later types. This paper presents a novel machine learning framework
capable of detecting and classifying late-T and Y dwarfs, trained entirely on
synthetic photometry from atmospheric models. Utilizing grids from the ATMO
2020 and Sonora Bobcat models, I produce a training dataset over two orders of
magnitude larger than any empirical set of >T6 UCDs. Polynomial color relations
fitted to the model photometry are used to assign spectral types to these
synthetic models, which in turn train an ensemble of classifiers to identify
and classify the spectral type of late UCDs. The model is highly performant
when validating on both synthetic and empirical datasets, verifying catalogs of
known UCDs with object classification metrics >99% and an average spectral type
precision within 0.35 +/- 0.37 subtypes. Application of the model to a 1.5
degree region around Pisces and the UKIDSS UDS field results in the discovery
of one previously uncatalogued T8.2 candidate, demonstrating the ability of
this model-trained approach in discovering faint, late-type UCDs from
photometric catalogs.

</details>
