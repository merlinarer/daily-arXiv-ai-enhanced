<div id=toc></div>

# Table of Contents

- [cs.CL](#cs.CL) [Total: 115]
- [cs.CV](#cs.CV) [Total: 159]
- [cs.AI](#cs.AI) [Total: 104]
- [cs.LG](#cs.LG) [Total: 169]


<div id='cs.CL'></div>

# cs.CL [[Back]](#toc)

### [1] [Evaluating the Unseen Capabilities: How Many Theorems Do LLMs Know?](https://arxiv.org/abs/2506.02058)
*Xiang Li,Jiayi Xin,Qi Long,Weijie J. Su*

Main category: cs.CL

Relevance: 90.0

TL;DR: 论文提出KnowSum框架，通过量化未观察到的知识来更全面地评估LLMs，解决了当前评估方法忽略未观察知识的问题。


<details>
  <summary>Details</summary>
Motivation: 当前LLM评估方法未能全面反映模型的实际能力，尤其是忽略了未观察到的知识，导致评估危机。

Method: 引入KnowSum统计框架，通过外推观察到的知识实例频率来估计未观察部分。

Result: 实验表明，KnowSum能显著改进LLM的知识评估，揭示传统方法遗漏的大量知识，并改变常见LLM的排名。

Conclusion: KnowSum为LLM评估提供了更全面的视角，强调了未观察知识的重要性。

Abstract: Accurate evaluation of large language models (LLMs) is crucial for
understanding their capabilities and guiding their development. However,
current evaluations often inconsistently reflect the actual capacities of these
models. In this paper, we demonstrate that one of many contributing factors to
this \textit{evaluation crisis} is the oversight of unseen knowledge --
information encoded by LLMs but not directly observed or not yet observed
during evaluations. We introduce KnowSum, a statistical framework designed to
provide a more comprehensive assessment by quantifying the unseen knowledge for
a class of evaluation tasks. KnowSum estimates the unobserved portion by
extrapolating from the appearance frequencies of observed knowledge instances.
We demonstrate the effectiveness and utility of KnowSum across three critical
applications: estimating total knowledge, evaluating information retrieval
effectiveness, and measuring output diversity. Our experiments reveal that a
substantial volume of knowledge is omitted when relying solely on observed LLM
performance. Importantly, KnowSum yields significantly different comparative
rankings for several common LLMs based on their internal knowledge.

</details>


### [2] [Consultant Decoding: Yet Another Synergistic Mechanism](https://arxiv.org/abs/2506.02391)
*Chuanghao Ding,Jiaping Wang,Ziqing Yang,Xiaoliang Wang,Dahua Lin,Cam-Tu Nguyen,Fei Tan*

Main category: cs.CL

Relevance: 90.0

TL;DR: 论文提出了一种名为Consultant Decoding (CD)的新机制，通过改进验证方法显著提升了大型语言模型(LLMs)的推理速度，同时保持了生成质量。


<details>
  <summary>Details</summary>
Motivation: 现有的Speculative Decoding (SD)方法因高拒绝率导致效率低下，需要多次调用LLMs验证草稿标记。

Method: CD采用基于LLM计算的标记级似然性验证候选草稿，而非SD的重要性采样指标。

Result: CD实现了推理速度最高提升2.5倍，生成质量接近目标模型的100%，且大幅降低对大模型的调用频率。

Conclusion: CD不仅超越了SD的性能上限，还在高要求任务中表现出色。

Abstract: The synergistic mechanism based on Speculative Decoding (SD) has garnered
considerable attention as a simple yet effective approach for accelerating the
inference of large language models (LLMs). Nonetheless, the high rejection
rates require repeated LLMs calls to validate draft tokens, undermining the
overall efficiency gain of SD. In this work, we revisit existing verification
mechanisms and propose a novel synergetic mechanism Consultant Decoding (CD).
Unlike SD, which relies on a metric derived from importance sampling for
verification, CD verifies candidate drafts using token-level likelihoods
computed solely by the LLM. CD achieves up to a 2.5-fold increase in inference
speed compared to the target model, while maintaining comparable generation
quality (around 100% of the target model's performance). Interestingly, this is
achieved by combining models whose parameter sizes differ by two orders of
magnitude. In addition, CD reduces the call frequency of the large target model
to below 10%, particularly in more demanding tasks. CD's performance was even
found to surpass that of the large target model, which theoretically represents
the upper bound for speculative decoding.

</details>


### [3] [EvaLearn: Quantifying the Learning Capability and Efficiency of LLMs via Sequential Problem Solving](https://arxiv.org/abs/2506.02672)
*Shihan Dou,Ming Zhang,Chenhao Huang,Jiayi Chen,Feng Chen,Shichun Liu,Yan Liu,Chenxiao Liu,Cheng Zhong,Zongzhang Zhang,Tao Gui,Chao Xin,Wei Chengzhi,Lin Yan,Qi Zhang,Xuanjing Huang*

Main category: cs.CL

Relevance: 90.0

TL;DR: EvaLearn是一个新的基准测试，用于评估大型语言模型（LLMs）在挑战性任务中的学习能力和效率，填补了现有评估的空白。


<details>
  <summary>Details</summary>
Motivation: 现有基准测试主要评估模型的静态能力，而忽视了学习能力的动态评估，EvaLearn旨在填补这一空白。

Method: EvaLearn包含648个挑战性问题，分为182个序列，要求模型按顺序解决问题以评估其学习能力。提供了五种自动化指标。

Result: 测试了九种前沿模型，发现学习能力差异显著，部分模型表现出色，而部分模型甚至出现负迁移。

Conclusion: EvaLearn为评估LLM潜力提供了新视角，揭示了静态能力与学习能力之间的差异。

Abstract: We introduce EvaLearn, a pioneering benchmark designed to evaluate large
language models (LLMs) on their learning capability and efficiency in
challenging tasks, a critical, yet underexplored aspect of model potential.
EvaLearn contains 648 challenging problems across six task types, grouped into
182 sequences, each sequence dedicated to one task type. Diverging from most
existing benchmarks that evaluate models in parallel, EvaLearn requires models
to solve problems sequentially, allowing them to leverage the experience gained
from previous solutions. EvaLearn provides five comprehensive automated metrics
to evaluate models and quantify their learning capability and efficiency. We
extensively benchmark nine frontier models and observe varied performance
profiles: some models, such as Claude-3.7-sonnet, start with moderate initial
performance but exhibit strong learning ability, while some models struggle to
benefit from experience and may even show negative transfer. Moreover, we
investigate model performance under two learning settings and find that
instance-level rubrics and teacher-model feedback further facilitate model
learning. Importantly, we observe that current LLMs with stronger static
abilities do not show a clear advantage in learning capability across all
tasks, highlighting that EvaLearn evaluates a new dimension of model
performance. We hope EvaLearn provides a novel evaluation perspective for
assessing LLM potential and understanding the gap between models and human
capabilities, promoting the development of deeper and more dynamic evaluation
approaches. All datasets, the automatic evaluation framework, and the results
studied in this paper are available at the GitHub repository.

</details>


### [4] [RACE-Align: Retrieval-Augmented and Chain-of-Thought Enhanced Preference Alignment for Large Language Models](https://arxiv.org/abs/2506.02726)
*Qihang Yan,Xinyu Zhang,Luming Guo,Qi Zhang,Feifan Liu*

Main category: cs.CL

Relevance: 90.0

TL;DR: RACE-Align框架通过结合检索增强和思维链优化，改进了LLM在垂直领域的准确性、推理能力和可解释性。


<details>
  <summary>Details</summary>
Motivation: 解决LLM在垂直领域中的准确性、推理和可解释性问题，传统偏好对齐方法（如RLHF和DPO）忽略了知识来源和推理逻辑。

Method: RACE-Align结合外部知识检索和显式思维链推理构建偏好数据集，使用DPO算法对齐LLM。

Result: 在传统中医领域实验中，RACE-Align显著提升了答案准确性、信息丰富度、推理逻辑和可解释性。

Conclusion: RACE-Align为提升LLM在复杂垂直领域中的知识应用、推理可靠性和过程透明度提供了有效途径。

Abstract: Large Language Models (LLMs) struggle with accuracy, domain-specific
reasoning, and interpretability in vertical domains. Traditional preference
alignment methods like Reinforcement Learning from Human Feedback (RLHF) and
Direct Preference Optimization (DPO) often overlook the underlying knowledge
sources and reasoning logic. This paper introduces RACE-Align
(Retrieval-Augmented and Chain-of-Thought Enhanced Alignment), a novel
framework designed to address these limitations. RACE-Align systematically
constructs a binary preference dataset incorporating external knowledge support
and explicit Chain-of-Thought (CoT) reasoning, then aligns LLMs using the DPO
algorithm. The core innovation lies in its preference data construction
strategy: it integrates AI-driven retrieval for factual grounding, enhancing
knowledgeability and accuracy, and emphasizes the optimization of
domain-specific CoT, treating the reasoning process itself as a key preference
dimension. A multi-stage, AI-driven refinement pipeline cost-effectively
generates these preference pairs. Experimental validation in Traditional
Chinese Medicine (TCM) using Qwen3-1.7B as the base model demonstrates that
RACE-Align significantly outperforms the original base model and a model
fine-tuned only with Supervised Fine-Tuning (SFT). Improvements were observed
across multiple dimensions, including answer accuracy, information richness,
application of TCM thinking patterns, logicality and depth of reasoning, and
interpretability. These findings suggest RACE-Align offers an effective pathway
to enhance LLMs' knowledge application, reasoning reliability, and process
transparency in complex vertical domains.

</details>


### [5] [No Free Lunch in Active Learning: LLM Embedding Quality Dictates Query Strategy Success](https://arxiv.org/abs/2506.01992)
*Lukas Rauch,Moritz Wirth,Denis Huseljic,Marek Herde,Bernhard Sick,Matthias Aßenmacher*

Main category: cs.CL

Relevance: 85.0

TL;DR: 论文研究了利用冻结的LLM嵌入降低深度主动学习（AL）计算成本的方法，通过基准测试分析了嵌入质量对AL查询策略的影响。


<details>
  <summary>Details</summary>
Motivation: 探索如何利用通用LLM嵌入优化深度主动学习的效率，减少迭代微调的计算开销。

Method: 使用MTEB排行榜上的五种高性能模型和两种基线模型，在十种文本分类任务中评估嵌入质量对AL策略的影响。

Result: 发现多样性采样与高质量嵌入协同提升早期AL性能，且查询策略选择对嵌入质量敏感。Badge策略在跨任务中表现稳健。

Conclusion: 强调AL策略需结合嵌入质量和目标任务进行上下文特定评估。

Abstract: The advent of large language models (LLMs) capable of producing
general-purpose representations lets us revisit the practicality of deep active
learning (AL): By leveraging frozen LLM embeddings, we can mitigate the
computational costs of iteratively fine-tuning large backbones. This study
establishes a benchmark and systematically investigates the influence of LLM
embedding quality on query strategies in deep AL. We employ five top-performing
models from the massive text embedding benchmark (MTEB) leaderboard and two
baselines for ten diverse text classification tasks. Our findings reveal key
insights: First, initializing the labeled pool using diversity-based sampling
synergizes with high-quality embeddings, boosting performance in early AL
iterations. Second, the choice of the optimal query strategy is sensitive to
embedding quality. While the computationally inexpensive Margin sampling can
achieve performance spikes on specific datasets, we find that strategies like
Badge exhibit greater robustness across tasks. Importantly, their effectiveness
is often enhanced when paired with higher-quality embeddings. Our results
emphasize the need for context-specific evaluation of AL strategies, as
performance heavily depends on embedding quality and the target task.

</details>


### [6] [NovelHopQA: Diagnosing Multi-Hop Reasoning Failures in Long Narrative Contexts](https://arxiv.org/abs/2506.02000)
*Abhay Gupta,Michael Lu,Kevin Zhu,Sean O'Brien,Vasu Sharma*

Main category: cs.CL

Relevance: 85.0

TL;DR: 论文提出了NovelHopQA基准，用于评估大语言模型在长上下文和多跳推理任务中的表现，发现即使前沿模型在增加推理深度和上下文长度时准确率也会下降。


<details>
  <summary>Details</summary>
Motivation: 当前大语言模型在涉及多跳推理的长上下文任务中表现不佳，缺乏同时评估上下文长度和推理深度的基准。

Method: 通过关键词引导的流程构建基于长篇小说的多跳问答链，评估六种前沿模型，并应用上下文过滤确保问题可回答。

Result: 模型在多跳推理和长上下文任务中准确率下降，揭示了规模并不保证推理能力。

Conclusion: NovelHopQA为大规模多跳推理提供了诊断工具，揭示了模型的常见失败模式。

Abstract: Current large language models (LLMs) struggle to answer questions that span
tens of thousands of tokens, especially when multi-hop reasoning is involved.
While prior benchmarks explore long-context comprehension or multi-hop
reasoning in isolation, none jointly vary context length and reasoning depth in
natural narrative settings. We introduce NovelHopQA, the first benchmark to
evaluate k1-4 hop QA over 64k-128k-token excerpts from 83 full-length
public-domain novels. A keyword-guided pipeline builds hop-separated chains
grounded in coherent storylines. We evaluate six state-of-the-art (SOTA) models
and apply oracle-context filtering to ensure all questions are genuinely
answerable. Human annotators validate both alignment and hop depth. We noticed
consistent accuracy drops with increased hops and context length, even in
frontier models-revealing that sheer scale does not guarantee robust reasoning.
Our failure mode analysis highlights common breakdowns, such as missed
final-hop integration and long-range drift. NovelHopQA offers a controlled
diagnostic setting to stress-test multi-hop reasoning at scale.

</details>


### [7] [Enhancing Multimodal Continual Instruction Tuning with BranchLoRA](https://arxiv.org/abs/2506.02041)
*Duzhen Zhang,Yong Ren,Zhong-Zhi Li,Yahan Yu,Jiahua Dong,Chenxing Li,Zhilong Ji,Jinfeng Bai*

Main category: cs.CL

Relevance: 85.0

TL;DR: 论文提出BranchLoRA框架，用于多模态持续指令调优（MCIT），通过非对称设计和任务专用分支缓解灾难性遗忘，提升效率和性能。


<details>
  <summary>Details</summary>
Motivation: 现有基于MoE LoRA的方法在多模态持续指令调优中因参数低效和灾难性遗忘问题表现不佳。

Method: 提出BranchLoRA框架，引入调优-冻结机制和任务专用路由器，优化分支分配和任务选择。

Result: 在MCIT基准测试中，BranchLoRA显著优于MoELoRA，且适用于不同规模的MLLM。

Conclusion: BranchLoRA通过高效参数利用和任务协作设计，解决了MCIT中的关键挑战。

Abstract: Multimodal Continual Instruction Tuning (MCIT) aims to finetune Multimodal
Large Language Models (MLLMs) to continually align with human intent across
sequential tasks. Existing approaches often rely on the Mixture-of-Experts
(MoE) LoRA framework to preserve previous instruction alignments. However,
these methods are prone to Catastrophic Forgetting (CF), as they aggregate all
LoRA blocks via simple summation, which compromises performance over time. In
this paper, we identify a critical parameter inefficiency in the MoELoRA
framework within the MCIT context. Based on this insight, we propose
BranchLoRA, an asymmetric framework to enhance both efficiency and performance.
To mitigate CF, we introduce a flexible tuning-freezing mechanism within
BranchLoRA, enabling branches to specialize in intra-task knowledge while
fostering inter-task collaboration. Moreover, we incrementally incorporate
task-specific routers to ensure an optimal branch distribution over time,
rather than favoring the most recent task. To streamline inference, we
introduce a task selector that automatically routes test inputs to the
appropriate router without requiring task identity. Extensive experiments on
the latest MCIT benchmark demonstrate that BranchLoRA significantly outperforms
MoELoRA and maintains its superiority across various MLLM sizes.

</details>


### [8] [Knowledge or Reasoning? A Close Look at How LLMs Think Across Domains](https://arxiv.org/abs/2506.02126)
*Juncheng Wu,Sheng Liu,Haoqin Tu,Hang Yu,Xiaoke Huang,James Zou,Cihang Xie,Yuyin Zhou*

Main category: cs.CL

Relevance: 85.0

TL;DR: 该论文研究了推理增强型大语言模型（如OpenAI-o1/3和DeepSeek-R1）的内部推理过程，提出了一个细粒度评估框架，重点关注知识和推理质量，并在医学和数学领域进行了实验。


<details>
  <summary>Details</summary>
Motivation: 尽管推理增强型大语言模型在复杂任务上表现优异，但其内部推理过程的质量和透明度尚未充分研究。

Method: 通过将推理轨迹分解为知识和推理两部分，引入知识指数（KI）和信息增益（InfoGain）作为评估指标，研究了基于监督微调（SFT）和强化学习（RL）训练的模型。

Result: 发现SFT提高最终答案准确率但降低推理质量，而RL在医学领域通过修剪不准确知识提升推理和知识正确性。

Conclusion: SFT和RL在不同领域的效果各异，需根据任务需求选择合适方法。

Abstract: Recent advances in reasoning-enhanced Large Language Models such as
OpenAI-o1/3 and DeepSeek-R1 have significantly improved performance on complex
tasks. However, the quality and transparency of their internal reasoning
processes remain underexplored. This work moves beyond the final-answer
accuracy and investigates step-by-step reasoning in the medical and
mathematical domains by explicitly decomposing the thinking trajectories into
two parts: knowledge and reasoning. Specifically, we introduce a fine-grained
evaluation framework that judges: (1) the correctness of knowledge used
(measured by Knowledge Index (KI)) and (2) the quality of reasoning (measured
by Information Gain (InfoGain)). Using this framework, we study R1-distilled
and base Qwen models trained with supervised fine-tuning (SFT) and/or
reinforcement learning (RL) in the medical and math domains. Three intriguing
findings emerge: (1) The general reasoning abilities in R1-distilled models do
not transfer effectively to the medical domain through either SFT or RL. (2)
SFT raises final-answer accuracy in both domains, but often at the cost of
reasoning quality: InfoGain drops by 38.9% on average compared with untrained
models; In the medical domain, however, SFT remains crucial because domain
knowledge is indispensable. (3) RL enhances medical reasoning by pruning
inaccurate or irrelevant knowledge from reasoning paths, thereby improving both
reasoning accuracy and knowledge correctness.

</details>


### [9] [Model Internal Sleuthing: Finding Lexical Identity and Inflectional Morphology in Modern Language Models](https://arxiv.org/abs/2506.02132)
*Michael Li,Nishant Subramani*

Main category: cs.CL

Relevance: 85.0

TL;DR: 研究探讨了BERT、GPT-2等经典模型及现代大语言模型（如Pythia、Llama-3.1等）如何表示词汇和形态变化信息，发现词汇信息在早期层线性集中，后期层非线性增强，而形态信息在各层均匀线性可分。


<details>
  <summary>Details</summary>
Motivation: 理解现代大语言模型如何编码语言信息，填补早期模型（如BERT、GPT-2）与现代模型之间的研究空白。

Method: 在模型各层激活上训练线性和非线性分类器，预测词汇词根和形态特征。

Result: 模型在早期层线性编码词汇信息，后期层非线性增强；形态信息在各层均匀线性可分。词汇编码依赖记忆，形态编码依赖抽象。16种模型均表现一致。

Conclusion: 尽管LLM技术大幅进步，Transformer模型组织语言信息的方式相似，这些特性可能是预训练早期学习到的。

Abstract: Large transformer-based language models dominate modern NLP, yet our
understanding of how they encode linguistic information is rooted in studies of
early models like BERT and GPT-2. To better understand today's language models,
we investigate how both classical architectures (BERT, DeBERTa, GPT-2)and
contemporary large language models (Pythia, OLMo-2, Gemma-2, Qwen2.5,
Llama-3.1) represent lexical identity and inflectional morphology. We train
linear and nonlinear classifiers on layer-wise activations to predict word
lemmas and inflectional features. We discover that models concentrate lexical
information linearly in early layers and increasingly nonlinearly in later
layers, while keeping inflectional information uniformly accessible and
linearly separable throughout the layers. Further analysis reveals that these
models encode inflectional morphology through generalizable abstractions, but
rely predominantly on memorization to encode lexical identity. Remarkably,
these patterns emerge across all 16 models we test, despite differences in
architecture, size, and training regime (including pretrained and
instruction-tuned variants). This consistency suggests that, despite
substantial advances in LLM technologies, transformer models organize
linguistic information in similar ways, indicating that these properties could
be fundamental for next token prediction and are learned early during
pretraining. Our code is available at
https://github.com/ml5885/model_internal_sleuthing.

</details>


### [10] [AI Debate Aids Assessment of Controversial Claims](https://arxiv.org/abs/2506.02175)
*Salman Rahman,Sheriff Issaka,Ashima Suvarna,Genglin Liu,James Shiffer,Jaeyoung Lee,Md Rizwan Parvez,Hamid Palangi,Shi Feng,Nanyun Peng,Yejin Choi,Julian Michael,Liwei Jiang,Saadia Gabriel*

Main category: cs.CL

Relevance: 85.0

TL;DR: AI debate improves judgment accuracy and confidence calibration, especially for biased human judges, and AI judges with human-like personas outperform humans in truthfulness evaluation.


<details>
  <summary>Details</summary>
Motivation: To address the risk of AI amplifying misinformation and social divides, especially in high-stakes areas like public health, by exploring scalable oversight methods like AI debate.

Method: Conducted two studies: one with human judges evaluating COVID-19 claims via AI debate or consultancy, and another with personalized AI judges mimicking human belief systems.

Result: Debate improved human judgment accuracy by 10%, with mainstream belief judges benefiting most (+15.2%). AI judges with personas achieved 78.5% accuracy, surpassing humans (70.1%).

Conclusion: AI debate is a promising scalable oversight method, leveraging diverse human and AI judgments to enhance truthfulness in contested domains.

Abstract: As AI grows more powerful, it will increasingly shape how we understand the
world. But with this influence comes the risk of amplifying misinformation and
deepening social divides-especially on consequential topics like public health
where factual accuracy directly impacts well-being. Scalable Oversight aims to
ensure AI truthfulness by enabling humans to supervise systems that may exceed
human capabilities--yet humans themselves hold different beliefs and biases
that impair their judgment. We study whether AI debate can guide biased judges
toward the truth by having two AI systems debate opposing sides of
controversial COVID-19 factuality claims where people hold strong prior
beliefs. We conduct two studies: one with human judges holding either
mainstream or skeptical beliefs evaluating factuality claims through
AI-assisted debate or consultancy protocols, and a second examining the same
problem with personalized AI judges designed to mimic these different human
belief systems. In our human study, we find that debate-where two AI advisor
systems present opposing evidence-based arguments-consistently improves
judgment accuracy and confidence calibration, outperforming consultancy with a
single-advisor system by 10% overall. The improvement is most significant for
judges with mainstream beliefs (+15.2% accuracy), though debate also helps
skeptical judges who initially misjudge claims move toward accurate views
(+4.7% accuracy). In our AI judge study, we find that AI judges with human-like
personas achieve even higher accuracy (78.5%) than human judges (70.1%) and
default AI judges without personas (69.8%), suggesting their potential for
supervising frontier AI models. These findings highlight AI debate as a
promising path toward scalable, bias-resilient oversight--leveraging both
diverse human and AI judgments to move closer to truth in contested domains.

</details>


### [11] [BehaviorBox: Automated Discovery of Fine-Grained Performance Differences Between Language Models](https://arxiv.org/abs/2506.02204)
*Lindia Tjuatja,Graham Neubig*

Main category: cs.CL

Relevance: 85.0

TL;DR: 论文提出了一种名为BehaviorBox的自动比较语言模型的方法，通过性能感知的上下文嵌入发现模型间的细粒度差异。


<details>
  <summary>Details</summary>
Motivation: 语言模型评估复杂且模糊，需要一种自动化的方法来发现模型间的具体性能差异。

Method: 使用性能感知的上下文嵌入提取细粒度特征，比较不同模型在特定上下文中的表现。

Result: BehaviorBox能够识别模型在特定上下文（如条件句或感叹句）中的性能差异，这些差异无法通过困惑度等传统指标发现。

Conclusion: BehaviorBox为语言模型评估提供了新的视角，揭示了传统方法无法捕捉的性能差异。

Abstract: Language model evaluation is a daunting task: prompts are brittle,
corpus-level perplexities are vague, and the choice of benchmarks are endless.
Finding examples that show meaningful, generalizable differences between two
LMs is crucial to understanding where one model succeeds and another fails. Can
this process be done automatically? In this work, we propose methodology for
automated comparison of language models that uses performance-aware contextual
embeddings to find fine-grained features of text where one LM outperforms
another. Our method, which we name BehaviorBox, extracts coherent features that
demonstrate differences with respect to the ease of generation between two LMs.
Specifically, BehaviorBox finds features that describe groups of words in
fine-grained contexts, such as "conditional 'were' in the phrase 'if you were'"
and "exclamation marks after emotional statements", where one model outperforms
another within a particular datatset. We apply BehaviorBox to compare models
that vary in size, model family, and post-training, and enumerate insights into
specific contexts that illustrate meaningful differences in performance which
cannot be found by measures such as corpus-level perplexity alone.

</details>


### [12] [ImpRAG: Retrieval-Augmented Generation with Implicit Queries](https://arxiv.org/abs/2506.02279)
*Wenzheng Zhang,Xi Victoria Lin,Karl Stratos,Wen-tau Yih,Mingda Chen*

Main category: cs.CL

Relevance: 85.0

TL;DR: ImpRAG提出了一种无需显式查询的检索增强生成系统，通过统一检索和生成任务，显著提升了模型在未见任务上的表现。


<details>
  <summary>Details</summary>
Motivation: 传统RAG系统将检索和生成分离，限制了模型的泛化能力。ImpRAG旨在通过统一模型解决这一问题。

Method: 将预训练的解码器语言模型分为专门层组，采用两阶段推理过程，共享参数优化检索和生成任务。

Result: 在8个知识密集型任务上，ImpRAG在未见任务上的精确匹配分数提升了3.6-11.5。

Conclusion: ImpRAG通过平衡检索和生成参数，利用生成困惑度作为检索训练目标，显著提升了性能。

Abstract: Retrieval-Augmented Generation (RAG) systems traditionally treat retrieval
and generation as separate processes, requiring explicit textual queries to
connect them. This separation can limit the ability of models to generalize
across diverse tasks. In this work, we propose a query-free RAG system, named
ImpRAG, which integrates retrieval and generation into a unified model. ImpRAG
allows models to implicitly express their information needs, eliminating the
need for human-specified queries. By dividing pretrained decoder-only language
models into specialized layer groups, ImpRAG optimizes retrieval and generation
tasks simultaneously. Our approach employs a two-stage inference process, using
the same model parameters and forward pass for both retrieval and generation,
thereby minimizing the disparity between retrievers and language models.
Experiments on 8 knowledge-intensive tasks demonstrate that ImpRAG achieves
3.6-11.5 improvements in exact match scores on unseen tasks with diverse
formats, highlighting its effectiveness in enabling models to articulate their
own information needs and generalize across tasks. Our analysis underscores the
importance of balancing retrieval and generation parameters and leveraging
generation perplexities as retrieval training objectives for enhanced
performance.

</details>


### [13] [LAM SIMULATOR: Advancing Data Generation for Large Action Model Training via Online Exploration and Trajectory Feedback](https://arxiv.org/abs/2506.02298)
*Thai Hoang,Kung-Hsiang Huang,Shirley Kokane,Jianguo Zhang,Zuxin Liu,Ming Zhu,Jake Grigsby,Tian Lan,Michael S Ryoo,Chien-Sheng Wu,Shelby Heinecke,Huan Wang,Silvio Savarese,Caiming Xiong,Juan Carlos Niebles*

Main category: cs.CL

Relevance: 85.0

TL;DR: LAM SIMULATOR框架通过动态任务查询生成器和实时反馈环境，为大型动作模型（LAMs）生成高质量训练数据，显著提升AI代理性能。


<details>
  <summary>Details</summary>
Motivation: 解决多步骤任务中高质量训练数据不足的问题，以支持AI代理的自主探索和学习。

Method: 提出LAM SIMULATOR框架，包含动态任务生成器、工具库和交互环境，用于生成LAMs的训练数据。

Result: 在ToolBench和CRMArena基准测试中，性能提升高达49.3%。

Conclusion: LAM SIMULATOR高效且有效，能加速AI代理的开发。

Abstract: Large Action Models (LAMs) for AI Agents offer incredible potential but face
challenges due to the need for high-quality training data, especially for
multi-steps tasks that involve planning, executing tool calls, and responding
to feedback. To address these issues, we present LAM SIMULATOR, a comprehensive
framework designed for online exploration of agentic tasks with high-quality
feedback. Our framework features a dynamic task query generator, an extensive
collection of tools, and an interactive environment where Large Language Model
(LLM) Agents can call tools and receive real-time feedback. This setup enables
LLM Agents to explore and solve tasks autonomously, facilitating the discovery
of multiple approaches to tackle any given task. The resulting action
trajectory data are then used to create high-quality training datasets for
LAMs. Our experiments on popular agentic benchmarks, ToolBench and CRMArena,
highlight the effectiveness of LAM SIMULATOR: models trained with
self-generated datasets using our framework achieve significant performance
gains, up to a 49.3\% improvement over their original baselines. LAM SIMULATOR
requires minimal human input during dataset creation, highlighting LAM
SIMULATOR's efficiency and effectiveness in speeding up development of AI
agents.

</details>


### [14] [Explain-then-Process: Using Grammar Prompting to Enhance Grammatical Acceptability Judgments](https://arxiv.org/abs/2506.02302)
*Russell Scheinberg,Ameeta Agrawal,Amber Shore,So Young Lee*

Main category: cs.CL

Relevance: 85.0

TL;DR: 论文提出了一种名为“语法提示”的方法，通过先让大语言模型生成语法规则解释，再将其作为上下文输入目标模型，显著提升了模型在判断句子语法性时的表现。


<details>
  <summary>Details</summary>
Motivation: 尽管大语言模型能够解释语法规则，但在实际应用这些规则判断句子可接受性时表现不佳。本文旨在通过“语法提示”方法缩小规则知识与实际应用之间的差距。

Method: 采用“解释-处理”范式：先由大语言模型生成语法现象的解释，再将解释作为额外上下文输入目标模型（大语言模型或小语言模型）进行句子语法性判断。

Result: 在多个语言基准测试中，该方法显著提升了模型表现，缩小了大语言模型与小语言模型之间的性能差距，尤其在结合链式思考时效果更佳。

Conclusion: “语法提示”是一种轻量级、语言无关的方法，能够帮助低成本小语言模型在多语言环境中接近前沿大语言模型的性能。

Abstract: Large language models (LLMs) can explain grammatical rules, yet they often
fail to apply those rules when judging sentence acceptability. We present
"grammar prompting", an explain-then-process paradigm: a large LLM first
produces a concise explanation of the relevant syntactic phenomenon, then that
explanation is fed back as additional context to the target model -- either an
LLM or a smaller language model (SLM) -- before deciding which sentence of a
minimal pair is grammatical. On the English BLiMP, Chinese SLING, and Russian
RuBLiMP benchmarks, this simple prompt design yields substantial improvements
over strong baselines across many syntactic phenomena. Feeding an LLM's
metalinguistic explanation back to the target model bridges the gap between
knowing a rule and using it. On SLMs, grammar prompting alone trims the average
LLM-SLM accuracy gap by about 20%, and when paired with chain-of-thought, by
56% (13.0 pp -> 5.8 pp), all at negligible cost. The lightweight,
language-agnostic cue lets low-cost SLMs approach frontier-LLM performance in
multilingual settings.

</details>


### [15] [One Missing Piece for Open-Source Reasoning Models: A Dataset to Mitigate Cold-Starting Short CoT LLMs in RL](https://arxiv.org/abs/2506.02338)
*Hyungjoo Chae,Dongjin Kang,Jihyuk Kim,Beong-woo Kwak,Sunghyun Park,Haeju Park,Jinyoung Yeo,Moontae Lee,Kyungjae Lee*

Main category: cs.CL

Relevance: 85.0

TL;DR: 论文探讨了如何通过构建长链推理（CoT）数据集独立开发大型推理模型（LRM），而非依赖现有模型（如R1）。提出了Long CoT Collection数据集，并通过实验验证其质量和训练效果。


<details>
  <summary>Details</summary>
Motivation: 减少对现有大型推理模型（如R1）的依赖，推动独立LRM开发，同时解决推理时的过度思考问题。

Method: 构建Long CoT Collection数据集（100K CoT标注），开发诱导短CoT LLMs生成长推理策略的流程，并引入可控性管理推理预算。

Result: 数据集质量接近R1，训练后模型在通用推理和强化学习（RLVR）中表现显著提升（2-3倍增益）。

Conclusion: 独立构建长CoT数据集可行且有效，为LRM开发提供了新方向。

Abstract: With the release of R1, a publicly available large reasoning model (LRM),
researchers commonly train new LRMs by training language models on R1's long
chain-of-thought (CoT) inferences. While prior works show that LRMs'
capabilities can be reproduced through direct distillation, the continued
reliance on the existing models (e.g., R1) remains a critical limitation in
advancing the field. As a first step toward independent LRM development, this
paper explores the possibility of constructing a long CoT dataset with LLMs
that are not trained for inference-time scaling. To this end, we present the
Long CoT Collection, a dataset of 100K CoT rationales annotated using existing
short CoT LLMs. We develop a pipeline that induces o1's novel reasoning
strategies into short CoT LLMs, enabling them to think longer and introducing
controllability over the thought budget to better manage the overthinking
problem. Our extensive analyses validate that our dataset achieves quality
comparable to--or slightly below--R1. Furthermore, our experiments demonstrate
that training on our dataset not only strengthens general reasoning skills, but
also provides a strong foundation for reinforcement learning--models
initialized on our data achieve 2-3x larger gains with RLVR.

</details>


### [16] [Exploring Explanations Improves the Robustness of In-Context Learning](https://arxiv.org/abs/2506.02378)
*Ukyo Honda,Tatsushi Oka*

Main category: cs.CL

Relevance: 85.0

TL;DR: 论文提出了一种扩展的解释性上下文学习框架（X²-ICL），通过系统探索所有可能标签的解释，提升了大型语言模型（LLMs）在分布外数据上的鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 现有上下文学习（ICL）在分布外数据上泛化能力不足，解释性上下文学习（X-ICL）通过提供正确标签的推理提升了可靠性，但仍需进一步优化。

Method: 扩展X-ICL为X²-ICL，系统探索所有可能标签的解释，以支持更全面和鲁棒的决策。

Result: 在多个自然语言理解数据集上，X²-ICL显著提升了分布外数据的鲁棒性，优于现有ICL方法。

Conclusion: X²-ICL通过系统化解释探索，显著增强了LLMs的鲁棒性和泛化能力。

Abstract: In-context learning (ICL) has emerged as a successful paradigm for leveraging
large language models (LLMs). However, it often struggles to generalize beyond
the distribution of the provided demonstrations. A recent advancement in
enhancing robustness is ICL with explanations (X-ICL), which improves
prediction reliability by guiding LLMs to understand and articulate the
reasoning behind correct labels. Building on this approach, we introduce an
advanced framework that extends X-ICL by systematically exploring explanations
for all possible labels (X$^2$-ICL), thereby enabling more comprehensive and
robust decision-making. Experimental results on multiple natural language
understanding datasets validate the effectiveness of X$^2$-ICL, demonstrating
significantly improved robustness to out-of-distribution data compared to the
existing ICL approaches.

</details>


### [17] [GraphRAG-Bench: Challenging Domain-Specific Reasoning for Evaluating Graph Retrieval-Augmented Generation](https://arxiv.org/abs/2506.02404)
*Yilin Xiao,Junnan Dong,Chuang Zhou,Su Dong,Qianwen Zhang,Di Yin,Xing Sun,Xiao Huang*

Main category: cs.CL

Relevance: 85.0

TL;DR: GraphRAG-Bench是一个针对GraphRAG模型的大规模、领域特定基准测试，旨在通过挑战性问题设计、多样化任务覆盖和全面评估框架，更全面地评估GraphRAG模型的推理能力提升。


<details>
  <summary>Details</summary>
Motivation: 当前GraphRAG模型的评估主要依赖传统问答数据集，其问题和评估指标范围有限，无法全面评估GraphRAG模型带来的推理能力提升。

Method: 提出了GraphRAG-Bench基准测试，包含挑战性问题设计（如多跳推理）、多样化任务覆盖（16个学科的多种题型）和全面评估框架（涵盖GraphRAG管道的各个环节）。

Result: 通过应用九种当代GraphRAG方法，验证了GraphRAG-Bench在量化基于图的结构化如何提升模型推理能力方面的实用性，并揭示了关于图架构、检索效果和推理能力的关键见解。

Conclusion: GraphRAG-Bench为研究社区提供了量化GraphRAG模型推理能力提升的工具，并为未来的研究提供了可操作的指导。

Abstract: Graph Retrieval Augmented Generation (GraphRAG) has garnered increasing
recognition for its potential to enhance large language models (LLMs) by
structurally organizing domain-specific corpora and facilitating complex
reasoning. However, current evaluations of GraphRAG models predominantly rely
on traditional question-answering datasets. Their limited scope in questions
and evaluation metrics fails to comprehensively assess the reasoning capacity
improvements enabled by GraphRAG models. To address this gap, we introduce
GraphRAG-Bench, a large-scale, domain-specific benchmark designed to rigorously
evaluate GraphRAG models. Our benchmark offers three key superiorities: \((i)\)
Challenging question design. Featuring college-level, domain-specific questions
that demand multi-hop reasoning, the benchmark ensures that simple content
retrieval is insufficient for problem-solving. For example, some questions
require mathematical reasoning or programming. \((ii)\) Diverse task coverage.
The dataset includes a broad spectrum of reasoning tasks, multiple-choice,
true/false, multi-select, open-ended, and fill-in-the-blank. It spans 16
disciplines in twenty core textbooks. \((iii)\) Holistic evaluation framework.
GraphRAG-Bench provides comprehensive assessment across the entire GraphRAG
pipeline, including graph construction, knowledge retrieval, and answer
generation. Beyond final-answer correctness, it evaluates the logical coherence
of the reasoning process. By applying nine contemporary GraphRAG methods to
GraphRAG-Bench, we demonstrate its utility in quantifying how graph-based
structuring improves model reasoning capabilities. Our analysis reveals
critical insights about graph architectures, retrieval efficacy, and reasoning
capabilities, offering actionable guidance for the research community.

</details>


### [18] [Should LLM Safety Be More Than Refusing Harmful Instructions?](https://arxiv.org/abs/2506.02442)
*Utsav Maskey,Mark Dras,Usman Naseem*

Main category: cs.CL

Relevance: 85.0

TL;DR: 本文系统评估了大型语言模型（LLMs）在长尾分布（加密）文本上的行为及其安全影响，提出了一个二维安全评估框架，并揭示了模型在解密能力与安全机制之间的不匹配问题。


<details>
  <summary>Details</summary>
Motivation: 研究LLMs在长尾加密文本场景下的安全性，揭示其安全机制的潜在漏洞，为开发更鲁棒的安全机制提供方向。

Method: 引入二维安全评估框架（指令拒绝和生成安全性），通过实验验证模型在解密能力与安全机制之间的不匹配问题。

Result: 实验表明，具备解密能力的模型可能在安全机制上存在漏洞，导致不安全响应或过度拒绝。

Conclusion: 本研究为理解LLMs在长尾文本场景中的安全性提供了新视角，并为开发更鲁棒的安全机制指明了方向。

Abstract: This paper presents a systematic evaluation of Large Language Models' (LLMs)
behavior on long-tail distributed (encrypted) texts and their safety
implications. We introduce a two-dimensional framework for assessing LLM
safety: (1) instruction refusal-the ability to reject harmful obfuscated
instructions, and (2) generation safety-the suppression of generating harmful
responses. Through comprehensive experiments, we demonstrate that models that
possess capabilities to decrypt ciphers may be susceptible to
mismatched-generalization attacks: their safety mechanisms fail on at least one
safety dimension, leading to unsafe responses or over-refusal. Based on these
findings, we evaluate a number of pre-LLM and post-LLM safeguards and discuss
their strengths and limitations. This work contributes to understanding the
safety of LLM in long-tail text scenarios and provides directions for
developing robust safety mechanisms.

</details>


### [19] [MidPO: Dual Preference Optimization for Safety and Helpfulness in Large Language Models via a Mixture of Experts Framework](https://arxiv.org/abs/2506.02460)
*Yupeng Qi,Ziyu Lyu,Min Yang,Yanlin Wang,Lu Bai,Lixin Cui*

Main category: cs.CL

Relevance: 85.0

TL;DR: 论文提出MidPO，一种基于混合专家（MoE）的安全与有用性双重偏好优化框架，通过独立优化安全性和有用性专家，并动态路由平衡两者，显著优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 随着大语言模型（LLM）广泛应用，如何在保持有用性的同时增强安全性成为关键挑战。现有方法在平衡安全性和有用性上存在不足。

Method: MidPO通过单偏好增强的直接偏好优化将基础模型转化为独立的安全性和有用性专家，并在MoE框架中动态路由平衡两者。

Result: 在三个流行数据集上的实验表明，MidPO在安全性和有用性上均显著优于现有方法。

Conclusion: MidPO通过动态平衡安全性和有用性，为LLM的安全应用提供了有效解决方案。

Abstract: As large language models (LLMs) are increasingly applied across various
domains, enhancing safety while maintaining the helpfulness of LLMs has become
a critical challenge. Recent studies solve this problem through
safety-constrained online preference optimization or safety-constrained offline
preference optimization. However, the safety-constrained online methods often
suffer from excessive safety, which might reduce helpfulness, while the
safety-constrained offline methods perform poorly in adaptively balancing
safety and helpfulness. To address these limitations, we propose MidPO, a
\textbf{\underline{Mi}}xture of Experts (MoE) framework for safety-helpfulness
\textbf{\underline{d}}ual \textbf{\underline{P}}reference
\textbf{\underline{O}}ptimization. Firstly, MidPO devises single-preference
enhanced direct preference optimization approach to transform the base model
into two independent experts, termed safety and helpfulness experts, and
fine-tunes the two independent experts for optimal safety or helpfulness
performance. Secondly, to achieve an effective balance between safety and
helpfulness, MidPO incorporates the two experts into the MoE framework and
designs a dynamic routing mechanism to allocate contributions from each expert
adaptively. We conduct quantitative and qualitative experiments on three
popular datasets to demonstrate the proposed MidPO significantly outperforms
state-of-the-art approaches in both safety and helpfulness. The code and models
will be released.

</details>


### [20] [XToM: Exploring the Multilingual Theory of Mind for Large Language Models](https://arxiv.org/abs/2506.02461)
*Chunkit Chan,Yauwai Yim,Hongchuan Zeng,Zhiying Zou,Xinyuan Cheng,Zhifan Sun,Zheye Deng,Kawai Chung,Yuzhuo Ao,Yixiang Fan,Cheng Jiayang,Ercong Nie,Ginny Y. Wong,Helmut Schmid,Hinrich Schütze,Simon See,Yangqiu Song*

Main category: cs.CL

Relevance: 85.0

TL;DR: XToM是一个多语言基准测试，用于评估LLMs在五种语言中的心智理论（ToM）能力，发现模型在多语言理解上表现优异，但ToM能力因语言而异。


<details>
  <summary>Details</summary>
Motivation: 现有对LLMs心智理论的评估主要限于英语，忽略了语言多样性对人类认知的影响，因此需要研究LLMs是否具备多语言心智理论能力。

Method: 开发了XToM基准测试，涵盖五种语言和多样化的任务场景，用于系统评估LLMs（如DeepSeek R1）的ToM表现。

Result: LLMs在多语言理解上表现优异，但ToM能力在不同语言间存在显著差异，未能完全复现人类跨语言的心智推理能力。

Conclusion: 研究揭示了LLMs在多语言心智理论能力上的局限性，强调了语言多样性对模型评估的重要性。

Abstract: Theory of Mind (ToM), the ability to infer mental states in others, is
pivotal for human social cognition. Existing evaluations of ToM in LLMs are
largely limited to English, neglecting the linguistic diversity that shapes
human cognition. This limitation raises a critical question: can LLMs exhibit
Multilingual Theory of Mind, which is the capacity to reason about mental
states across diverse linguistic contexts? To address this gap, we present
XToM, a rigorously validated multilingual benchmark that evaluates ToM across
five languages and incorporates diverse, contextually rich task scenarios.
Using XToM, we systematically evaluate LLMs (e.g., DeepSeek R1), revealing a
pronounced dissonance: while models excel in multilingual language
understanding, their ToM performance varies across languages. Our findings
expose limitations in LLMs' ability to replicate human-like mentalizing across
linguistic contexts.

</details>


### [21] [FroM: Frobenius Norm-Based Data-Free Adaptive Model Merging](https://arxiv.org/abs/2506.02478)
*Zijian Li,Xiaocheng Feng,Huixin Liu,Yichong Huang,Ting Liu,Bing Qin*

Main category: cs.CL

Relevance: 85.0

TL;DR: 本文提出了一种名为FroM的自适应模型合并方法，通过直接测量模型参数的Frobenius范数来缓解任务干扰问题，无需训练数据。


<details>
  <summary>Details</summary>
Motivation: 随着大语言模型的发展，微调成为提升特定场景性能的有效方法，但传统模型合并方法在任务干扰问题上表现不佳。

Method: 改进RegMean方法，提出FroM方法，直接测量模型参数的Frobenius范数，并引入超参数控制。

Result: FroM在多种微调场景中优于基线方法，有效缓解了任务干扰问题。

Conclusion: FroM是一种无需训练数据的自适应模型合并方法，显著提升了模型合并的效果。

Abstract: With the development of large language models, fine-tuning has emerged as an
effective method to enhance performance in specific scenarios by injecting
domain-specific knowledge. In this context, model merging techniques provide a
solution for fusing knowledge from multiple fine-tuning models by combining
their parameters. However, traditional methods often encounter task
interference when merging full fine-tuning models, and this problem becomes
even more evident in parameter-efficient fine-tuning scenarios. In this paper,
we introduce an improvement to the RegMean method, which indirectly leverages
the training data to approximate the outputs of the linear layers before and
after merging. We propose an adaptive merging method called FroM, which
directly measures the model parameters using the Frobenius norm, without any
training data. By introducing an additional hyperparameter for control, FroM
outperforms baseline methods across various fine-tuning scenarios, alleviating
the task interference problem.

</details>


### [22] [ORPP: Self-Optimizing Role-playing Prompts to Enhance Language Model Capabilities](https://arxiv.org/abs/2506.02480)
*Yifan Duan,Yihong Tang,Kehai Chen,Liqiang Nie,Min Zhang*

Main category: cs.CL

Relevance: 85.0

TL;DR: ORPP（优化角色扮演提示）框架通过优化和生成角色扮演提示来提升LLM性能，减少计算开销并提高适用性。


<details>
  <summary>Details</summary>
Motivation: 现有提示优化方法计算开销高或依赖模型优化能力，限制了广泛适用性。ORPP旨在通过角色扮演提示激活模型内在能力。

Method: ORPP在少量训练样本上迭代优化生成高质量角色扮演提示，利用少样本学习能力扩展到其他样本。

Result: ORPP在性能上匹配或超越主流提示优化方法，并具备出色的即插即用能力。

Conclusion: ORPP通过角色扮演提示优化显著提升LLM性能，且兼容其他提示方法。

Abstract: High-quality prompts are crucial for eliciting outstanding performance from
large language models (LLMs) on complex tasks. Existing research has explored
model-driven strategies for prompt optimization. However, these methods often
suffer from high computational overhead or require strong optimization
capabilities from the model itself, which limits their broad applicability.To
address these challenges, we propose ORPP (Optimized Role-Playing Prompt),a
framework that enhances model performance by optimizing and generating
role-playing prompts. The core idea of ORPP is to confine the prompt search
space to role-playing scenarios, thereby fully activating the model's intrinsic
capabilities through carefully crafted, high-quality role-playing prompts.
Specifically, ORPP first performs iterative optimization on a small subset of
training samples to generate high-quality role-playing prompts. Then,
leveraging the model's few-shot learning capability, it transfers the
optimization experience to efficiently generate suitable prompts for the
remaining samples.Our experimental results show that ORPP not only matches but
in most cases surpasses existing mainstream prompt optimization methods in
terms of performance. Notably, ORPP demonstrates superior "plug-and-play"
capability. In most cases, it can be integrated with various other prompt
methods and further enhance their effectiveness.

</details>


### [23] [Do Language Models Think Consistently? A Study of Value Preferences Across Varying Response Lengths](https://arxiv.org/abs/2506.02481)
*Inderjeet Nair,Lu Wang*

Main category: cs.CL

Relevance: 85.0

TL;DR: 研究发现，LLMs在短形式和长形式响应中的价值偏好相关性较弱，且对齐仅带来有限的改进。


<details>
  <summary>Details</summary>
Motivation: 探索LLMs在短形式和长形式响应中价值偏好的一致性，以评估实际应用中的伦理风险。

Method: 比较五种LLMs在短形式和长形式响应中的价值偏好，分析参数数量和生成设置的影响。

Result: 短形式与长形式响应中的价值偏好相关性较弱，对齐改进有限。

Conclusion: 需要更稳健的方法确保LLMs在不同应用中的价值表达一致性。

Abstract: Evaluations of LLMs' ethical risks and value inclinations often rely on
short-form surveys and psychometric tests, yet real-world use involves
long-form, open-ended responses -- leaving value-related risks and preferences
in practical settings largely underexplored. In this work, we ask: Do value
preferences inferred from short-form tests align with those expressed in
long-form outputs? To address this question, we compare value preferences
elicited from short-form reactions and long-form responses, varying the number
of arguments in the latter to capture users' differing verbosity preferences.
Analyzing five LLMs (llama3-8b, gemma2-9b, mistral-7b, qwen2-7b, and olmo-7b),
we find (1) a weak correlation between value preferences inferred from
short-form and long-form responses across varying argument counts, and (2)
similarly weak correlation between preferences derived from any two distinct
long-form generation settings. (3) Alignment yields only modest gains in the
consistency of value expression. Further, we examine how long-form generation
attributes relate to value preferences, finding that argument specificity
negatively correlates with preference strength, while representation across
scenarios shows a positive correlation. Our findings underscore the need for
more robust methods to ensure consistent value expression across diverse
applications.

</details>


### [24] [Enhancing Large Language Models with Neurosymbolic Reasoning for Multilingual Tasks](https://arxiv.org/abs/2506.02483)
*Sina Bagheri Nezhad,Ameeta Agrawal*

Main category: cs.CL

Relevance: 85.0

TL;DR: NSAR结合神经与符号推理，提升多目标长上下文推理能力，显著优于RAG和高级提示策略。


<details>
  <summary>Details</summary>
Motivation: 解决LLMs在多目标长上下文推理中的信息分散问题。

Method: 结合神经与符号推理，提取符号事实并生成可执行Python代码。

Result: 在七种语言和不同上下文长度下，NSAR显著优于基线方法。

Conclusion: 神经与符号推理结合在多语言环境中实现鲁棒、可解释且可扩展的推理。

Abstract: Large language models (LLMs) often struggle to perform multi-target reasoning
in long-context scenarios where relevant information is scattered across
extensive documents. To address this challenge, we introduce NeuroSymbolic
Augmented Reasoning (NSAR), which combines the benefits of neural and symbolic
reasoning during inference. NSAR explicitly extracts symbolic facts from text
and generates executable Python code to handle complex reasoning steps. Through
extensive experiments across seven languages and diverse context lengths, we
demonstrate that NSAR significantly outperforms both a vanilla RAG baseline and
advanced prompting strategies in accurately identifying and synthesizing
multiple pieces of information. Our results highlight the effectiveness of
combining explicit symbolic operations with neural inference for robust,
interpretable, and scalable reasoning in multilingual settings.

</details>


### [25] [KARE-RAG: Knowledge-Aware Refinement and Enhancement for RAG](https://arxiv.org/abs/2506.02503)
*Yongjian Li,HaoCheng Chu,Yukun Yan,Zhenghao Liu,Shi Yu,Zheni Zeng,Ruobing Wang,Sen Song,Zhiyuan Liu,Maosong Sun*

Main category: cs.CL

Relevance: 85.0

TL;DR: KARE-RAG通过结构化知识表示、改进的训练目标和对比数据生成，提升了RAG在噪声文档中的知识利用能力，显著提高了任务性能。


<details>
  <summary>Details</summary>
Motivation: 尽管RAG让LLMs能访问更广泛的知识源，但检索文档中的噪声仍导致事实不一致。本文旨在通过改进生成模型处理噪声内容的能力，提升RAG的鲁棒性。

Method: 1. 结构化知识表示以检测训练中的错误；2. DDPO训练目标优先纠正关键错误；3. 对比数据生成保持语义一致性并修正事实错误。

Result: 实验表明，KARE-RAG显著提升了标准RAG的性能，适用于不同规模模型和任务，且数据效率高。

Conclusion: 通过改进模型处理检索内容的学习方式，可以提升RAG在多样化推理范式中的性能。

Abstract: Retrieval-Augmented Generation (RAG) enables large language models (LLMs) to
access broader knowledge sources, yet factual inconsistencies persist due to
noise in retrieved documents-even with advanced retrieval methods. We
demonstrate that enhancing generative models' capacity to process noisy content
is equally critical for robust performance. In this paper, we present KARE-RAG
(Knowledge-Aware Refinement and Enhancement for RAG), which improves knowledge
utilization through three key innovations: (1) structured knowledge
representations that facilitate error detection during training, (2) Dense
Direct Preference Optimization (DDPO)-a refined training objective that
prioritizes correction of critical errors, and (3) a contrastive data
generation pipeline that maintains semantic consistency while rectifying
factual inaccuracies. Experiments show our method significantly enhances
standard RAG pipelines across model scales, improving both in-domain and
out-of-domain task performance without compromising general capabilities.
Notably, these gains are achieved with modest training data, suggesting
data-efficient optimization is possible through targeted learning strategies.
Our findings establish a new direction for RAG improvement: by improving how
models learn to process retrieved content, we can enhance performance across
diverse inference paradigms. All data and code will be publicly available on
Github.

</details>


### [26] [Learning Together to Perform Better: Teaching Small-Scale LLMs to Collaborate via Preferential Rationale Tuning](https://arxiv.org/abs/2506.02519)
*Sohan Patnaik,Milan Aggarwal,Sumit Bhatia,Balaji Krishnamurthy*

Main category: cs.CL

Relevance: 85.0

TL;DR: COLLATE是一个可训练框架，旨在提升小型LLM的推理能力，通过生成多样化理性输出并优化选择，无需依赖大型LLM的知识蒸馏。


<details>
  <summary>Details</summary>
Motivation: 解决小型LLM因依赖大型LLM（如GPT-4）而面临的法律和版权问题，同时提升其独立推理能力。

Method: COLLATE通过多样化理性生成和偏好优化，选择最佳输出以提升任务表现。

Result: 在数学问题解决、自然语言推理和常识推理等5个数据集上优于基线方法。

Conclusion: COLLATE有效提升了小型LLM的推理能力，适用于不同参数规模的模型。

Abstract: LLMssuch as GPT-4 have shown a remarkable ability to solve complex questions
by generating step-by-step rationales. Prior works have utilized this
capability to improve smaller and cheaper LMs (say, with 7B parameters).
However, various practical constraints, such as copyright and legal issues,
owing to lack of transparency in the pre-training data of large (often closed)
models, prevent their use in commercial settings. Little focus has been given
to improving the innate reasoning ability of smaller models without distilling
information from larger LLMs. To address this, we propose COLLATE, a trainable
framework that tunes a (small) LLM to generate those outputs from a pool of
diverse rationales that selectively improves the downstream task. COLLATE
enforces multiple instances of the same LLM to exhibit distinct behavior and
employs them to generate rationales to obtain diverse outputs. The LLM is then
tuned via preference optimization to choose the candidate rationale which
maximizes the likelihood of ground-truth answer. COLLATE outperforms several
trainable and prompting baselines on 5 datasets across 3 domains: maths problem
solving, natural language inference, and commonsense reasoning. We show the eff
icacy of COLLATE on LLMs from different model families across varying parameter
scales (1B to 8B) and demonstrate the benefit of multiple rationale providers
guided by the end task through ablations. Code is released here
(https://github.com/Sohanpatnaik106/collate).

</details>


### [27] [Answer Convergence as a Signal for Early Stopping in Reasoning](https://arxiv.org/abs/2506.02536)
*Xin Liu,Lu Wang*

Main category: cs.CL

Relevance: 85.0

TL;DR: 论文研究了Chain-of-thought (CoT)提示在大型语言模型中的冗余问题，提出了三种推理时策略以提高效率，显著减少了token使用且几乎不影响准确性。


<details>
  <summary>Details</summary>
Motivation: CoT提示虽然增强了推理能力，但导致冗长输出和推理成本增加。作者假设许多推理步骤是不必要的，并探究了最小推理需求。

Method: 通过系统性研究确定最小推理需求，提出三种策略：(1)基于答案一致性的早期停止，(2)增强结束信号生成概率，(3)基于内部激活的监督学习停止方法。

Result: 在五个基准测试和五个开源LLM上，方法显著减少了token使用（如NaturalQuestions上减少40%），且准确性几乎无下降。

Conclusion: 研究强调了推理时高效方法的重要性，为实际应用提供了实用价值。

Abstract: Chain-of-thought (CoT) prompting enhances reasoning in large language models
(LLMs) but often leads to verbose and redundant outputs, thus increasing
inference cost. We hypothesize that many reasoning steps are unnecessary for
producing correct answers. To investigate this, we start with a systematic
study to examine what is the minimum reasoning required for a model to reach a
stable decision. We find that on math reasoning tasks like math, models
typically converge to their final answers after 60\% of the reasoning steps,
suggesting substantial redundancy in the remaining content. Based on these
insights, we propose three inference-time strategies to improve efficiency: (1)
early stopping via answer consistency, (2) boosting the probability of
generating end-of-reasoning signals, and (3) a supervised method that learns
when to stop based on internal activations. Experiments across five benchmarks
and five open-weights LLMs show that our methods significantly reduce token
usage with little or no accuracy drop. In particular, on NaturalQuestions,
Answer Consistency reduces tokens by over 40\% while further improving
accuracy. Our work underscores the importance of cost-effective reasoning
methods that operate at inference time, offering practical benefits for
real-world applications.

</details>


### [28] [Pruning General Large Language Models into Customized Expert Models](https://arxiv.org/abs/2506.02561)
*Yirao Zhao,Guizhen Chen,Kenji Kawaguchi,Lidong Bing,Wenxuan Zhang*

Main category: cs.CL

Relevance: 85.0

TL;DR: 论文提出了一种名为Cus-Prun的自定义剪枝方法，用于将大型通用语言模型剪枝为小型轻量级专家模型，无需后训练即可保持性能。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型（LLMs）通常需要大量计算资源，而现有剪枝方法往往无法兼顾通用能力和专家能力。

Method: Cus-Prun通过沿“语言”、“领域”和“任务”三个维度识别并剪枝无关神经元，生成专家模型。

Result: 实验表明，Cus-Prun在不同模型家族和规模中均优于其他方法，专家和通用能力损失最小。

Conclusion: Cus-Prun是一种高效且无需后训练的剪枝方法，适用于生成轻量级专家模型。

Abstract: Large language models (LLMs) have revolutionized natural language processing,
yet their substantial model sizes often require substantial computational
resources. To preserve computing resources and accelerate inference speed, it
is crucial to prune redundant parameters, especially for experienced users who
often need compact expert models tailored to specific downstream scenarios.
However, most existing pruning methods focus on preserving the model's general
capabilities, often requiring extensive post-training or suffering from
degraded performance due to coarse-grained pruning. In this work, we design a
$\underline{Cus}$tom $\underline{Prun}$ing method ($\texttt{Cus-Prun}$) to
prune a large general model into a smaller lightweight expert model, which is
positioned along the "language", "domain" and "task" dimensions. By identifying
and pruning irrelevant neurons of each dimension, $\texttt{Cus-Prun}$ creates
expert models without any post-training. Our experiments demonstrate that
$\texttt{Cus-Prun}$ consistently outperforms other methods, achieving minimal
loss in both expert and general capabilities across various models from
different model families and sizes.

</details>


### [29] [Beyond the Surface: Measuring Self-Preference in LLM Judgments](https://arxiv.org/abs/2506.02592)
*Zhi-Yuan Chen,Hao Wang,Xinyu Zhang,Enrui Hu,Yankai Lin*

Main category: cs.CL

Relevance: 85.0

TL;DR: 论文提出了一种新方法（DBG分数）来测量LLM作为评委时的自我偏好偏差，通过引入黄金判断来消除响应质量对偏差测量的干扰。


<details>
  <summary>Details</summary>
Motivation: 现有方法在测量LLM自我偏好偏差时混淆了偏差与响应质量，无法准确区分。

Method: 引入黄金判断作为响应质量的代理，提出DBG分数来衡量自我偏好偏差，并实验验证其有效性。

Result: 实验表明DBG分数能更准确地测量偏差，并分析了影响偏差的因素（如响应文本风格和训练数据）。

Conclusion: DBG分数为LLM自我偏好偏差提供了更准确的测量工具，并揭示了潜在机制。

Abstract: Recent studies show that large language models (LLMs) exhibit self-preference
bias when serving as judges, meaning they tend to favor their own responses
over those generated by other models. Existing methods typically measure this
bias by calculating the difference between the scores a judge model assigns to
its own responses and those it assigns to responses from other models. However,
this approach conflates self-preference bias with response quality, as
higher-quality responses from the judge model may also lead to positive score
differences, even in the absence of bias. To address this issue, we introduce
gold judgments as proxies for the actual quality of responses and propose the
DBG score, which measures self-preference bias as the difference between the
scores assigned by the judge model to its own responses and the corresponding
gold judgments. Since gold judgments reflect true response quality, the DBG
score mitigates the confounding effect of response quality on bias measurement.
Using the DBG score, we conduct comprehensive experiments to assess
self-preference bias across LLMs of varying versions, sizes, and reasoning
abilities. Additionally, we investigate two factors that influence and help
alleviate self-preference bias: response text style and the post-training data
of judge models. Finally, we explore potential underlying mechanisms of
self-preference bias from an attention-based perspective. Our code and data are
available at https://github.com/zhiyuanc2001/self-preference.

</details>


### [30] [Are Economists Always More Introverted? Analyzing Consistency in Persona-Assigned LLMs](https://arxiv.org/abs/2506.02659)
*Manon Reusens,Bart Baesens,David Jurgens*

Main category: cs.CL

Relevance: 85.0

TL;DR: 提出了一个标准化框架，用于分析个性化LLMs在不同任务和运行中的一致性表现，发现一致性受多种因素影响。


<details>
  <summary>Details</summary>
Motivation: 现有研究缺乏对不同人物设定和任务类型的全面分析，需评估LLMs在人物设定下的响应一致性。

Method: 引入新框架，定义一致性为模型在不同任务和运行中保持连贯响应的能力，评估四类人物设定和多种任务维度。

Result: 一致性受人物设定、刻板印象和模型设计影响，结构化任务和额外上下文可提高一致性。

Conclusion: 框架为个性化LLMs的一致性评估提供了标准化方法，揭示了影响因素和任务差异。

Abstract: Personalized Large Language Models (LLMs) are increasingly used in diverse
applications, where they are assigned a specific persona - such as a happy high
school teacher - to guide their responses. While prior research has examined
how well LLMs adhere to predefined personas in writing style, a comprehensive
analysis of consistency across different personas and task types is lacking. In
this paper, we introduce a new standardized framework to analyze consistency in
persona-assigned LLMs. We define consistency as the extent to which a model
maintains coherent responses when assigned the same persona across different
tasks and runs. Our framework evaluates personas across four different
categories (happiness, occupation, personality, and political stance) spanning
multiple task dimensions (survey writing, essay generation, social media post
generation, single turn, and multi-turn conversations). Our findings reveal
that consistency is influenced by multiple factors, including the assigned
persona, stereotypes, and model design choices. Consistency also varies across
tasks, increasing with more structured tasks and additional context. All code
is available on GitHub.

</details>


### [31] [TL;DR: Too Long, Do Re-weighting for Effcient LLM Reasoning Compression](https://arxiv.org/abs/2506.02678)
*Zhong-Zhi Li,Xiao Liang,Zihao Tang,Lei Ji,Peijie Wang,Haotian Xu,Xing W,Haizhen Huang,Weiwei Deng,Ying Nian Wu,Yeyun Gong,Zhijiang Guo,Xiao Liu,Fei Yin,Cheng-Lin Liu*

Main category: cs.CL

Relevance: 85.0

TL;DR: 提出了一种动态比例训练方法，显著减少推理输出长度40%，同时保持准确性。


<details>
  <summary>Details</summary>
Motivation: 解决LLMs在长输出推理中的效率问题，避免复杂数据标注或多模型插值。

Method: 动态平衡System-1和System-2数据权重，消除冗余推理过程。

Result: 在DeepSeek-R1-Distill-7B和14B上验证，输出token减少40%，准确性不变。

Conclusion: 方法高效且通用，适用于不同难度任务。

Abstract: Large Language Models (LLMs) have recently achieved remarkable progress by
leveraging Reinforcement Learning and extended Chain-of-Thought (CoT)
techniques. However, the challenge of performing efficient language
reasoning--especially during inference with extremely long outputs--has drawn
increasing attention from the research community. In this work, we propose a
dynamic ratio-based training pipeline that does not rely on sophisticated data
annotations or interpolation between multiple models. We continuously balance
the weights between the model's System-1 and System-2 data to eliminate
redundant reasoning processes while preserving the model's reasoning
capability. We validate our approach across models on DeepSeek-R1-Distill-7B
and DeepSeek-R1-Distill-14B and on a diverse set of benchmarks with varying
difficulty levels. Our method significantly reduces the number of output tokens
by nearly 40% while maintaining the accuracy of the reasoning. Our code and
data will be available soon.

</details>


### [32] [Decompose, Plan in Parallel, and Merge: A Novel Paradigm for Large Language Models based Planning with Multiple Constraints](https://arxiv.org/abs/2506.02683)
*Zhengdong Lu,Weikai Lu,Yiling Tao,Yun Dai,ZiXuan Chen,Huiping Zhuang,Cen Chen,Hao Peng,Ziqian Zeng*

Main category: cs.CL

Relevance: 85.0

TL;DR: 提出了一种名为DPPM的并行规划范式，用于解决LLM在规划任务中的约束和级联错误问题。


<details>
  <summary>Details</summary>
Motivation: 现有规划方法存在约束过重和级联错误的局限性，影响LLM在规划任务中的表现。

Method: DPPM通过分解任务、并行生成子计划并合并为全局计划，结合验证和细化模块纠正错误。

Result: 实验表明DPPM在旅行规划任务中显著优于现有方法。

Conclusion: DPPM为LLM在复杂规划任务中提供了更高效的解决方案。

Abstract: Despite significant advances in Large Language Models (LLMs), planning tasks
still present challenges for LLM-based agents. Existing planning methods face
two key limitations: heavy constraints and cascading errors. To address these
limitations, we propose a novel parallel planning paradigm, which Decomposes,
Plans for subtasks in Parallel, and Merges subplans into a final plan (DPPM).
Specifically, DPPM decomposes the complex task based on constraints into
subtasks, generates the subplan for each subtask in parallel, and merges them
into a global plan. In addition, our approach incorporates a verification and
refinement module, enabling error correction and conflict resolution.
Experimental results demonstrate that DPPM significantly outperforms existing
methods in travel planning tasks.

</details>


### [33] [MASTER: Enhancing Large Language Model via Multi-Agent Simulated Teaching](https://arxiv.org/abs/2506.02689)
*Liang Yue,Yihong Tang,Kehai Chen,Jie Liu,Min Zhang*

Main category: cs.CL

Relevance: 85.0

TL;DR: MASTER是一种数据增强方法，通过多智能体交互生成高质量微调数据，显著提升模型性能。


<details>
  <summary>Details</summary>
Motivation: 解决高质量微调数据获取难和成本高的问题。

Method: 提出MASTER方法，模拟教学场景，通过多智能体对话生成数据，构建BOOST-QA数据集。

Result: 微调后的模型在多任务基准测试中表现优异，推理能力显著提升。

Conclusion: MASTER为高质量数据生成提供了新思路，对复杂任务推理能力有显著帮助。

Abstract: Instruction fine-tuning is crucial in NLP tasks, enhancing pretrained models'
instruction-following capabilities and task-specific performance. However,
obtaining high-quality fine-tuning data for large models is challenging due to
data collection difficulties and high production costs. To address this, we
propose MASTER, a novel data augmentation method that enriches original data
through interactions among multiple agents with varying cognitive levels. We
simulate three pedagogically grounded teaching scenarios, leveraging
multi-agent conversations to generate high-quality teacher-student interaction
data. Utilizing MASTER, we construct BOOST-QA, a fine-tuning dataset augmented
from existing datasets like Orca-Math-200k, ProcQA, and OpenHermes2.5.
Experiments show that models fine-tuned with BOOST-QA perform excellently
across multiple benchmarks, demonstrating strong multitask generalization.
Notably, MASTER significantly improves models' reasoning abilities in complex
tasks, providing valuable insights for future research.

</details>


### [34] [On Entity Identification in Language Models](https://arxiv.org/abs/2506.02701)
*Masaki Sakata,Sho Yokoi,Benjamin Heinzerling,Takumi Ito,Kentaro Inui*

Main category: cs.CL

Relevance: 85.0

TL;DR: 论文通过聚类分析量化了语言模型内部表示对命名实体的识别和区分能力，发现模型能有效处理实体提及的歧义和变异性，且实体信息在早期层中以低维线性子空间紧凑表示。


<details>
  <summary>Details</summary>
Motivation: 研究语言模型内部表示如何识别和区分命名实体提及，解决实体与提及之间的多对多对应问题。

Method: 提出基于聚类分析的框架，量化模型内部表示中相同实体提及的聚类和不同实体提及的分离程度，实验涵盖五个基于Transformer的自回归模型。

Result: 模型能有效识别和区分实体，指标（类似精确率和召回率）在0.66到0.9之间；实体信息在早期层中以低维线性子空间紧凑表示。

Conclusion: 语言模型内部表示与实体中心知识结构存在同构关系，揭示了模型如何组织和利用实体信息。

Abstract: We analyze the extent to which internal representations of language models
(LMs) identify and distinguish mentions of named entities, focusing on the
many-to-many correspondence between entities and their mentions. We first
formulate two problems of entity mentions -- ambiguity and variability -- and
propose a framework analogous to clustering quality metrics. Specifically, we
quantify through cluster analysis of LM internal representations the extent to
which mentions of the same entity cluster together and mentions of different
entities remain separated. Our experiments examine five Transformer-based
autoregressive models, showing that they effectively identify and distinguish
entities with metrics analogous to precision and recall ranging from 0.66 to
0.9. Further analysis reveals that entity-related information is compactly
represented in a low-dimensional linear subspace at early LM layers.
Additionally, we clarify how the characteristics of entity representations
influence word prediction performance. These findings are interpreted through
the lens of isomorphism between LM representations and entity-centric knowledge
structures in the real world, providing insights into how LMs internally
organize and use entity information.

</details>


### [35] [ProcrustesGPT: Compressing LLMs with Structured Matrices and Orthogonal Transformations](https://arxiv.org/abs/2506.02818)
*Ekaterina Grishina,Mikhail Gorbunov,Maxim Rakhuba*

Main category: cs.CL

Relevance: 85.0

TL;DR: 论文提出了一种利用正交变换优化LLM权重矩阵压缩的方法，以减少计算和内存需求。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型（LLMs）在自然语言处理任务中表现出色，但需要大量计算和内存资源。结构化矩阵表示是一种减少参数数量的方法，但直接应用于预训练模型可能不准确。

Method: 利用LLM输出在特定正交变换下的不变性，识别能显著提升权重矩阵在结构化类别中可压缩性的变换。

Result: 该方法适用于多种支持高效投影操作的结构化矩阵类型，并提供了开源代码。

Conclusion: 通过正交变换优化权重矩阵压缩，为LLMs的高效部署提供了可行方案。

Abstract: Large language models (LLMs) demonstrate impressive results in natural
language processing tasks but require a significant amount of computational and
memory resources. Structured matrix representations are a promising way for
reducing the number of parameters of these models. However, it seems
unrealistic to expect that weight matrices of pretrained models can be
accurately represented by structured matrices without any fine-tuning. To
overcome this issue, we utilize the fact that LLM output is invariant under
certain orthogonal transformations of weight matrices. This insight can be
leveraged to identify transformations that significantly improve the
compressibility of weights within structured classes. The proposed approach is
applicable to various types of structured matrices that support efficient
projection operations. Code is available at
https://github.com/GrishKate/ProcrustesGPT

</details>


### [36] [TO-GATE: Clarifying Questions and Summarizing Responses with Trajectory Optimization for Eliciting Human Preference](https://arxiv.org/abs/2506.02827)
*Yulin Dou,Jiangming Liu*

Main category: cs.CL

Relevance: 85.0

TL;DR: TO-GATE通过轨迹优化提升LLM在对话中生成问题与总结的能力，显著优于基线方法。


<details>
  <summary>Details</summary>
Motivation: 现有基于自学习推理的方法难以识别最优对话轨迹并避免无关问题，需改进。

Method: 提出TO-GATE框架，包含澄清解析器（优化提问轨迹）和总结器（确保任务对齐）。

Result: 实验显示TO-GATE在偏好引导任务上比基线方法提升9.32%。

Conclusion: TO-GATE通过轨迹优化有效提升LLM在任务导向对话中的表现。

Abstract: Large language models (LLMs) can effectively elicit human preferences through
multi-turn dialogue. Complex tasks can be accomplished through iterative
clarifying questions and final responses generated by an LLM acting as a
questioner (STaR-GATE; Andukuri et al., 2024}). However, existing approaches
based on self-taught reasoning struggle to identify optimal dialogue
trajectories and avoid irrelevant questions to the tasks. To address this
limitation, we propose TO-GATE, a novel framework that enhances question
generation through trajectory optimization, which consists of two key
components: a clarification resolver that generates optimal questioning
trajectories, and a summarizer that ensures task-aligned final responses. The
trajectory optimization enables the model to produce effective elicitation
questions and summary responses tailored to specific tasks. Experimental
results demonstrate that TO-GATE significantly outperforms baseline methods,
achieving a 9.32% improvement on standard preference elicitation tasks.

</details>


### [37] [CoT is Not True Reasoning, It Is Just a Tight Constraint to Imitate: A Theory Perspective](https://arxiv.org/abs/2506.02878)
*Jintian Shao,Yiming Cheng*

Main category: cs.CL

Relevance: 85.0

TL;DR: 论文提出Chain-of-Thought (CoT) 提示并非引发真正的抽象推理，而是通过结构性约束引导大语言模型模仿推理形式。


<details>
  <summary>Details</summary>
Motivation: 探讨CoT提示是否真正引发大语言模型的推理能力，还是仅通过序列预测和模式匹配模仿推理过程。

Method: 理论分析CoT提示的作用机制，强调其作为结构性约束的功能。

Result: CoT提示通过生成中间步骤，利用模型的序列预测能力，而非真正实现抽象推理。

Conclusion: CoT提示是一种有效的模仿工具，但并未真正赋予模型推理能力。

Abstract: Chain-of-Thought (CoT) prompting has demonstrably enhanced the performance of
Large Language Models on tasks requiring multi-step inference. This success has
led to widespread claims of emergent reasoning capabilities in these models. In
this paper, we present a theoretical counter-perspective: Chain-of-Thought
(CoT) does not elicit genuine, abstract reasoning. Instead, we argue that
Chain-of-Thought functions as a powerful structural constraint that guides
Large Language Models to imitate the form of reasoning. By forcing the
generation of intermediate steps, Chain-of-Thought leverages the model immense
capacity for sequence prediction and pattern matching, effectively constraining
its output to sequences that resemble coherent thought processes.
Chain-of-Thought (CoT) prompting has demonstrably enhanced the performance of
Large Language Models on tasks requiring multi-step inference. This success has
led to widespread claims of emergent reasoning capabilities in these models. In
this paper, we present a theoretical counter-perspective: Chain-of-Thought
(CoT) does not elicit genuine, abstract reasoning. Instead, we argue that
Chain-of-Thought functions as a powerful structural constraint that guides
Large Language Models to imitate the form of reasoning. By forcing the
generation of intermediate steps, Chain-of-Thought leverages the model immense
capacity for sequence prediction and pattern matching, effectively constraining
its output to sequences that resemble coherent thought processes.

</details>


### [38] [A Controllable Examination for Long-Context Language Models](https://arxiv.org/abs/2506.02921)
*Yijun Yang,Zeyu Huang,Wenhao Zhu,Zihan Qiu,Fei Yuan,Jeff Z. Pan,Ivan Titov*

Main category: cs.CL

Relevance: 85.0

TL;DR: 论文提出了LongBioBench，一种新的长上下文语言模型评估基准，通过人工生成的传记作为受控环境，评估模型的理解、推理和可信赖性。


<details>
  <summary>Details</summary>
Motivation: 现有评估长上下文语言模型（LCLM）的框架存在局限性：真实任务复杂且易受数据污染，合成任务缺乏上下文连贯性。

Method: 设计LongBioBench基准，利用人工生成的传记，评估18种LCLM在理解、推理和可信赖性方面的表现。

Result: 大多数模型在语义理解和基础推理上存在不足，且随着上下文长度增加，可信赖性下降。

Conclusion: LongBioBench在模拟真实任务和保持可控性之间取得了更好的平衡，且具有高度可解释性和可配置性。

Abstract: Existing frameworks for evaluating long-context language models (LCLM) can be
broadly categorized into real-world and synthetic tasks. Despite their utility,
both approaches are accompanied by certain intrinsic limitations. Real-world
tasks are too complex to interpret or characterize and are susceptible to data
contamination. In contrast, synthetic tasks often adopt the
needle-in-the-haystack (NIAH) format, wherein a lack of coherence between the
"needle" and the "haystack" compromises their validity as proxies for realistic
applications. In response to these challenges, we posit that an ideal
long-context evaluation framework should be characterized by three essential
features: $\textit{seamless context}$, $\textit{controllable setting}$, and
$\textit{sound evaluation}$. This study introduces $\textbf{LongBioBench}$, a
novel benchmark that utilizes artificially generated biographies as a
controlled environment for assessing LCLMs across dimensions of
$\textit{understanding}$, $\textit{reasoning}$, and $\textit{trustworthiness}$.
Our experimental evaluation, which includes $\textbf{18}$ LCLMs in total,
demonstrates that most models still exhibit deficiencies in semantic
understanding and elementary reasoning over retrieved results and are less
trustworthy as context length increases. Our further analysis indicates some
design choices employed by existing synthetic benchmarks, such as contextual
non-coherence, numerical needles, and the absence of distractors, rendering
them vulnerable to test the model long-context capabilities. Moreover, we also
reveal that long-context continual pretraining primarily adjusts RoPE embedding
to accommodate extended context lengths. To sum up, compared to previous
synthetic benchmarks, LongBioBench achieves a better trade-off between
mirroring authentic language tasks and maintaining controllability, and is
highly interpretable and configurable.

</details>


### [39] [Quantitative LLM Judges](https://arxiv.org/abs/2506.02945)
*Aishwarya Sahoo,Jeevana Kruthi Karnuthala,Tushar Parmanand Budhwani,Pranchal Agarwal,Sankaran Vaidyanathan,Alexa Siu,Franck Dernoncourt,Jennifer Healey,Nedim Lipka,Ryan Rossi,Uttaran Bhattacharya,Branislav Kveton*

Main category: cs.CL

Relevance: 85.0

TL;DR: 提出了一种名为LLM-as-a-judge的框架，通过回归模型将LLM的评分与人类评分对齐，提升评估效果。


<details>
  <summary>Details</summary>
Motivation: 解决现有LLM评估方法在计算效率和统计效率上的不足，尤其是在人类反馈有限的情况下。

Method: 使用回归模型对LLM的文本评估和评分进行训练，提出四种定量评估方法。

Result: 在四个数据集上验证了定量评估方法能有效提升现有评估的预测能力。

Conclusion: 该框架在计算效率和统计效率上优于监督微调，适用于人类反馈有限的应用场景。

Abstract: LLM-as-a-judge is a framework in which a large language model (LLM)
automatically evaluates the output of another LLM. We propose quantitative LLM
judges, which align evaluation scores of existing LLM judges to human scores in
a given domain using regression models. The models are trained to improve the
score of the original judge by using the judge's textual evaluation and score.
We present four quantitative judges for different types of absolute and
relative feedback, which showcases the generality and versatility of our
framework. Our framework is more computationally efficient than supervised
fine-tuning and can be more statistically efficient when human feedback is
limited, which is expected in most applications of our work. We validate these
claims empirically on four datasets using two base judges. Our experiments show
that quantitative judges can effectively improve the predictive power of
existing judges through post-hoc modeling.

</details>


### [40] [Adaptive Graph Pruning for Multi-Agent Communication](https://arxiv.org/abs/2506.02951)
*Boyi Li,Zhonghan Zhao,Der-Horng Lee,Gaoang Wang*

Main category: cs.CL

Relevance: 85.0

TL;DR: 提出了一种名为自适应图剪枝（AGP）的多智能体协作框架，通过动态优化智能体数量和通信拓扑结构，显著提升了任务适应性和性能。


<details>
  <summary>Details</summary>
Motivation: 当前多智能体系统依赖固定数量和静态通信结构，限制了任务适应性。

Method: 采用两阶段训练策略：先独立训练软剪枝网络确定最优智能体数量和通信拓扑，再联合优化硬剪枝和软剪枝。

Result: 在六个基准测试中表现优异，性能提升2.58%∼9.84%，且训练高效、节省token。

Conclusion: AGP框架在任务适应性、性能和效率上均超越现有基线。

Abstract: Large Language Model (LLM) based multi-agent systems have shown remarkable
performance in various tasks, especially when enhanced through collaborative
communication. However, current methods often rely on a fixed number of agents
and static communication structures, limiting their ability to adapt to varying
task complexities. In this paper, we propose Adaptive Graph Pruning (AGP), a
novel task-adaptive multi-agent collaboration framework that jointly optimizes
agent quantity (hard-pruning) and communication topology (soft-pruning).
Specifically, our method employs a two-stage training strategy: firstly,
independently training soft-pruning networks for different agent quantities to
determine optimal agent-quantity-specific complete graphs and positional masks
across specific tasks; and then jointly optimizing hard-pruning and
soft-pruning within a maximum complete graph to dynamically configure the
number of agents and their communication topologies per task. Extensive
experiments demonstrate that our approach is: (1) High-performing, achieving
state-of-the-art results across six benchmarks and consistently generalizes
across multiple mainstream LLM architectures, with a increase in performance of
$2.58\%\sim 9.84\%$; (2) Task-adaptive, dynamically constructing optimized
communication topologies tailored to specific tasks, with an extremely high
performance in all three task categories (general reasoning, mathematical
reasoning, and code generation); (3) Token-economical, having fewer training
steps and token consumption at the same time, with a decrease in token
consumption of $90\%+$; and (4) Training-efficient, achieving high performance
with very few training steps compared with other methods. The performance will
surpass the existing baselines after about ten steps of training under six
benchmarks.

</details>


### [41] [HACo-Det: A Study Towards Fine-Grained Machine-Generated Text Detection under Human-AI Coauthoring](https://arxiv.org/abs/2506.02959)
*Zhixiong Su,Yichen Wang,Herun Wan,Zhaohan Zhang,Minnan Luo*

Main category: cs.CL

Relevance: 85.0

TL;DR: 论文探讨了细粒度机器生成文本（MGT）检测在人类与AI合著场景下的可行性，提出了数据集HACo-Det并改造了现有检测器，结果显示微调模型表现更优，但问题仍未完全解决。


<details>
  <summary>Details</summary>
Motivation: LLM的滥用风险促使研究机器生成文本检测，现有方法多为文档级检测，忽视了人类与AI合著的文本。

Method: 提出数据集HACo-Det，改造七种文档级检测器用于词级检测，评估其在词级和句子级任务中的表现。

Result: 基于指标的方法表现较差（平均F1 0.462），微调模型表现更优且泛化能力更强，但问题仍未完全解决。

Conclusion: 细粒度合著文本检测仍有挑战，需进一步优化上下文窗口等因素。

Abstract: The misuse of large language models (LLMs) poses potential risks, motivating
the development of machine-generated text (MGT) detection. Existing literature
primarily concentrates on binary, document-level detection, thereby neglecting
texts that are composed jointly by human and LLM contributions. Hence, this
paper explores the possibility of fine-grained MGT detection under human-AI
coauthoring. We suggest fine-grained detectors can pave pathways toward
coauthored text detection with a numeric AI ratio. Specifically, we propose a
dataset, HACo-Det, which produces human-AI coauthored texts via an automatic
pipeline with word-level attribution labels. We retrofit seven prevailing
document-level detectors to generalize them to word-level detection. Then we
evaluate these detectors on HACo-Det on both word- and sentence-level detection
tasks. Empirical results show that metric-based methods struggle to conduct
fine-grained detection with a 0.462 average F1 score, while finetuned models
show superior performance and better generalization across domains. However, we
argue that fine-grained co-authored text detection is far from solved. We
further analyze factors influencing performance, e.g., context window, and
highlight the limitations of current methods, pointing to potential avenues for
improvement.

</details>


### [42] [FlowerTune: A Cross-Domain Benchmark for Federated Fine-Tuning of Large Language Models](https://arxiv.org/abs/2506.02961)
*Yan Gao,Massimo Roberto Scamarcia,Javier Fernandez-Marques,Mohammad Naseri,Chong Shen Ng,Dimitris Stripelis,Zexi Li,Tao Shen,Jiamu Bai,Daoyuan Chen,Zikai Zhang,Rui Hu,InSeo Song,Lee KangYoon,Hong Jia,Ting Dang,Junyan Wang,Zheyuan Liu,Daniel Janes Beutel,Lingjuan Lyu,Nicholas D. Lane*

Main category: cs.CL

Relevance: 85.0

TL;DR: 该论文提出了FlowerTune LLM Leaderboard，用于评估联邦学习环境下预训练大语言模型（LLMs）的微调性能，涵盖多个领域，并提供了模型性能、资源限制和领域适应的实用见解。


<details>
  <summary>Details</summary>
Motivation: 解决LLMs在联邦学习环境下的兼容性和性能问题，以应对数据稀缺和敏感信息访问限制的挑战。

Method: 开发了一个名为FlowerTune的基准测试套件，评估26种预训练LLMs在联邦学习环境下的微调性能，涵盖通用NLP、金融、医疗和编程领域。

Result: 提供了首个全面的联邦学习环境下LLMs性能比较，揭示了模型在不同领域的表现、资源需求和适应能力。

Conclusion: 该研究为开发隐私保护的领域专用LLMs奠定了基础。

Abstract: Large Language Models (LLMs) have achieved state-of-the-art results across
diverse domains, yet their development remains reliant on vast amounts of
publicly available data, raising concerns about data scarcity and the lack of
access to domain-specific, sensitive information. Federated Learning (FL)
presents a compelling framework to address these challenges by enabling
decentralized fine-tuning on pre-trained LLMs without sharing raw data.
However, the compatibility and performance of pre-trained LLMs in FL settings
remain largely under explored. We introduce the FlowerTune LLM Leaderboard, a
first-of-its-kind benchmarking suite designed to evaluate federated fine-tuning
of LLMs across four diverse domains: general NLP, finance, medical, and coding.
Each domain includes federated instruction-tuning datasets and domain-specific
evaluation metrics. Our results, obtained through a collaborative, open-source
and community-driven approach, provide the first comprehensive comparison
across 26 pre-trained LLMs with different aggregation and fine-tuning
strategies under federated settings, offering actionable insights into model
performance, resource constraints, and domain adaptation. This work lays the
foundation for developing privacy-preserving, domain-specialized LLMs for
real-world applications.

</details>


### [43] [Expanding before Inferring: Enhancing Factuality in Large Language Models through Premature Layers Interpolation](https://arxiv.org/abs/2506.02973)
*Dingwei Chen,Ziqiang Liu,Feiteng Fang,Chak Tou Leong,Shiwen Ni,Ahmadreza Argha,Hamid Alinejad-Rokny,Min Yang,Chengming Li*

Main category: cs.CL

Relevance: 85.0

TL;DR: 论文提出了一种名为PLI（Premature Layers Interpolation）的新方法，通过数学插值在LLM的早期层插入干预，无需训练即可减少幻觉现象，提升事实一致性。


<details>
  <summary>Details</summary>
Motivation: LLM在生成文本时存在事实不一致（幻觉）问题，现有方法主要在输入或输出层面解决，忽略了内部信息处理过程。PLI旨在通过干预早期层提升事实性。

Method: PLI通过在相邻层之间进行数学插值，插入新的早期层，扩展信息处理深度，无需额外训练。

Result: 在四个公开数据集上，PLI有效减少了幻觉现象，并在多数情况下优于现有基线方法。

Conclusion: PLI的成功与LLM内部机制密切相关，是一种高效、无需训练的干预方法。

Abstract: Large Language Models (LLMs) demonstrate remarkable capabilities in text
understanding and generation. However, their tendency to produce factually
inconsistent outputs, commonly referred to as ''hallucinations'', remains a
critical challenge. Existing approaches, such as retrieval-based and
inference-time correction methods, primarily address this issue at the input or
output level, often overlooking the intrinsic information refinement process
and the role of premature layers. Meanwhile, alignment- and fine-tuning-based
methods are resource-intensive. In this paper, we propose PLI (Premature Layers
Interpolation), a novel, training-free, and plug-and-play intervention designed
to enhance factuality. PLI mitigates hallucinations by inserting premature
layers formed through mathematical interpolation with adjacent layers. Inspired
by stable diffusion and sampling steps, PLI extends the depth of information
processing and transmission in LLMs, improving factual coherence. Experiments
on four publicly available datasets demonstrate that PLI effectively reduces
hallucinations while outperforming existing baselines in most cases. Further
analysis suggests that the success of layer interpolation is closely linked to
LLMs' internal mechanisms. To promote reproducibility, we will release our code
and data upon acceptance.

</details>


### [44] [Towards Analyzing and Understanding the Limitations of VAPO: A Theoretical Perspective](https://arxiv.org/abs/2506.03038)
*Jintian Shao,Yiming Cheng*

Main category: cs.CL

Relevance: 85.0

TL;DR: 论文探讨了强化学习（RL）在大型语言模型（LLMs）长链推理中的局限性，特别是VAPO框架在长期价值建模方面的不足。


<details>
  <summary>Details</summary>
Motivation: 研究旨在揭示RL在长链推理中的根本限制，尤其是信用分配、价值函数表示能力以及稀疏奖励下的策略改进问题。

Method: 通过理论分析，研究了VAPO框架在长期价值建模中的局限性，包括信用分配和价值信号传递的挑战。

Result: 研究发现VAPO框架在长期价值建模中存在根本性限制，尤其是在稀疏奖励环境下。

Conclusion: 论文建议未来研究应关注更鲁棒的LLM代理，以克服当前RL在长链推理中的局限性。

Abstract: Reinforcement learning (RL) enhances large language models (LLMs) in complex,
long-chain-of-thought (long-CoT) reasoning. The advanced VAPO framework,
despite sophisticated mechanisms like Decoupled GAE, theoretically faces
fundamental limitations in comprehensively modeling and leveraging deep,
long-term value for fine-grained, step-by-step policy guidance in extended
reasoning chains. We argue these limitations stem from inherent difficulties in
credit assignment, value function representational capacity with temporally
abstracted goals, and translating global value signals into local policy
improvements, especially with sparse rewards. Our theoretical analysis examines
these aspects to illuminate VAPO's boundaries in long-term value modeling,
aiming to deepen understanding of current RL for advanced reasoning and suggest
future research for more robust LLM agents.

</details>


### [45] [Beyond Text Compression: Evaluating Tokenizers Across Scales](https://arxiv.org/abs/2506.03101)
*Jonas F. Lotz,António V. Lopes,Stephan Peitz,Hendra Setiawan,Leonardo Emili*

Main category: cs.CL

Relevance: 85.0

TL;DR: 提出了一种通过小模型预测分词器对大型模型性能影响的方法，并开发了新的分词器评估指标。


<details>
  <summary>Details</summary>
Motivation: 解决分词器对语言模型性能影响评估的挑战，提供高效的分词器选择方法。

Method: 利用小模型预测分词器对大型模型的影响，提出基于Zipf定律的新指标，并结合多指标评估分词器。

Result: 英语任务中分词器影响较小，多语言任务中影响显著；新指标比文本压缩更能预测下游性能。

Conclusion: 提供了一种高效的分词器评估框架，为未来语言模型开发提供支持。

Abstract: The choice of tokenizer can profoundly impact language model performance, yet
accessible and reliable evaluations of tokenizer quality remain an open
challenge. Inspired by scaling consistency, we show that smaller models can
accurately predict significant differences in tokenizer impact on larger models
at a fraction of the compute cost. By systematically evaluating both
English-centric and multilingual tokenizers, we find that tokenizer choice has
negligible effects on tasks in English but results in consistent performance
differences in multilingual settings. We propose new intrinsic tokenizer
metrics inspired by Zipf's law that correlate more strongly with downstream
performance than text compression when modeling unseen languages. By combining
several metrics to capture multiple aspects of tokenizer behavior, we develop a
reliable framework for intrinsic tokenizer evaluations. Our work offers a more
efficient path to informed tokenizer selection in future language model
development.

</details>


### [46] [Critique-GRPO: Advancing LLM Reasoning with Natural Language and Numerical Feedback](https://arxiv.org/abs/2506.03106)
*Xiaoying Zhang,Hao Sun,Yipeng Zhang,Kaituo Feng,Chao Yang,Helen Meng*

Main category: cs.CL

Relevance: 85.0

TL;DR: 论文提出Critique-GRPO框架，结合自然语言和数值反馈优化RL，提升LLM在复杂任务中的表现。


<details>
  <summary>Details</summary>
Motivation: 解决仅依赖数值反馈的RL在LLM训练中遇到的性能瓶颈、自反思效果有限和持续失败问题。

Method: 提出Critique-GRPO框架，整合自然语言和数值反馈进行在线RL优化，支持初始响应和基于反馈的改进同时学习。

Result: 在多个数学、STEM和通用推理任务中，Critique-GRPO显著优于监督学习和RL微调方法，平均pass@1分数提升约4.5%-5%。

Conclusion: 自然语言反馈能有效突破RL性能瓶颈，Critique-GRPO为LLM优化提供了新思路。

Abstract: Recent advances in reinforcement learning (RL) with numerical feedback, such
as scalar rewards, have significantly enhanced the complex reasoning
capabilities of large language models (LLMs). Despite this success, we identify
three key challenges encountered by RL with solely numerical feedback:
performance plateaus, limited effectiveness of self-reflection, and persistent
failures. We then demonstrate that RL-finetuned models, even after exhibiting
performance plateaus, can generate correct refinements on persistently failed
problems by leveraging natural language feedback in the form of critiques.
Building on this insight, we propose Critique-GRPO, an online RL framework that
integrates both natural language and numerical feedback for effective policy
optimization. Critique-GRPO enables LLMs to learn from initial responses and
critique-guided refinements simultaneously while maintaining exploration.
Extensive experiments using Qwen2.5-7B-Base and Qwen3-8B-Base show that
Critique-GRPO consistently outperforms supervised learning-based and RL-based
fine-tuning approaches across eight challenging mathematical, STEM, and general
reasoning tasks, improving average pass@1 scores by approximately 4.5% and 5%,
respectively. Notably, Critique-GRPO surpasses a strong baseline that
incorporates expert demonstrations within online RL. Further analysis reveals
two critical insights about policy exploration: (1) higher entropy does not
always guarantee efficient learning from exploration, and (2) longer responses
do not necessarily lead to more effective exploration.

</details>


### [47] [Causal Estimation of Tokenisation Bias](https://arxiv.org/abs/2506.03149)
*Pietro Lesci,Clara Meister,Thomas Hofmann,Andreas Vlachos,Tiago Pimentel*

Main category: cs.CL

Relevance: 85.0

TL;DR: 论文研究了分词器对语言模型输出概率的影响，提出了一种量化分词偏差的方法，并发现分词选择对模型输出有显著影响。


<details>
  <summary>Details</summary>
Motivation: 研究分词器（tokeniser）对语言模型输出概率的影响，量化分词偏差（tokenisation bias），以解决分词选择对模型性能的影响问题。

Method: 采用回归不连续设计（regression discontinuity design）作为因果效应估计方法，通过比较分词器词汇表中边界附近的子词来量化分词偏差。

Result: 实验表明，分词选择对模型输出有显著影响，子词的存在可能使字符概率增加高达17倍。

Conclusion: 分词器是语言模型设计中的关键因素，其选择对模型性能有重要影响。

Abstract: Modern language models are typically trained over subword sequences, but
ultimately define probabilities over character-strings. Ideally, the choice of
the tokeniser -- which maps character-strings to subwords -- should not affect
the probability assigned to the underlying character-string; in practice, it
does. We define this mismatch as tokenisation bias. In this work, we quantify
one particular type of tokenisation bias: the effect of including or not a
subword (e.g., $\langle hello \rangle$) in a tokeniser's vocabulary on the
probability a trained model assigns to the corresponding characters (i.e.,
\textit{``hello''}). Estimating this effect is challenging because each model
is trained with only one tokeniser. We address this by framing tokenisation
bias as a causal effect and estimating it using the regression discontinuity
design. Specifically, we exploit the fact that tokenisation algorithms rank
subwords and add the first $K$ to a tokeniser's vocabulary, where $K$ is an
arbitrary cutoff point. As such, we can estimate a causal effect by comparing
similar subwords around this cutoff. Experimentally, we find that tokenisation
consistently affects models' outputs across scales, vocabularies, and
tokenisers. Notably, a subword's presence in a small model's vocabulary may
increase its characters' probability by up to 17 times, highlighting
tokenisation as a key design choice in language modelling.

</details>


### [48] [BitBypass: A New Direction in Jailbreaking Aligned Large Language Models with Bitstream Camouflage](https://arxiv.org/abs/2506.02479)
*Kalyan Nakka,Nitesh Saxena*

Main category: cs.CR

Relevance: 85.0

TL;DR: 论文提出了一种名为BitBypass的新型黑盒越狱攻击方法，通过利用连字符分隔的比特流伪装来绕过对齐LLMs的安全防护。


<details>
  <summary>Details</summary>
Motivation: LLMs的安全对齐面临对抗性攻击的挑战，现有方法（如监督微调、RLHF等）存在漏洞，需要探索新的攻击方向以提高对齐模型的鲁棒性。

Method: 开发了BitBypass攻击方法，利用比特流伪装而非传统的提示工程或对抗性操作，对五种先进LLMs（如GPT-4o、Gemini 1.5等）进行测试。

Result: BitBypass成功绕过LLMs的安全对齐，生成有害内容，且在隐蔽性和攻击成功率上优于现有越狱攻击方法。

Conclusion: BitBypass展示了其在越狱先进LLMs中的高效性和有效性，揭示了安全对齐的新漏洞。

Abstract: The inherent risk of generating harmful and unsafe content by Large Language
Models (LLMs), has highlighted the need for their safety alignment. Various
techniques like supervised fine-tuning, reinforcement learning from human
feedback, and red-teaming were developed for ensuring the safety alignment of
LLMs. However, the robustness of these aligned LLMs is always challenged by
adversarial attacks that exploit unexplored and underlying vulnerabilities of
the safety alignment. In this paper, we develop a novel black-box jailbreak
attack, called BitBypass, that leverages hyphen-separated bitstream camouflage
for jailbreaking aligned LLMs. This represents a new direction in jailbreaking
by exploiting fundamental information representation of data as continuous
bits, rather than leveraging prompt engineering or adversarial manipulations.
Our evaluation of five state-of-the-art LLMs, namely GPT-4o, Gemini 1.5, Claude
3.5, Llama 3.1, and Mixtral, in adversarial perspective, revealed the
capabilities of BitBypass in bypassing their safety alignment and tricking them
into generating harmful and unsafe content. Further, we observed that BitBypass
outperforms several state-of-the-art jailbreak attacks in terms of stealthiness
and attack success. Overall, these results highlights the effectiveness and
efficiency of BitBypass in jailbreaking these state-of-the-art LLMs.

</details>


### [49] [FinS-Pilot: A Benchmark for Online Financial System](https://arxiv.org/abs/2506.02037)
*Feng Wang,Yiding Sun,Jiaxin Mao,Wei Xue,Danqing Xu*

Main category: cs.CL

Relevance: 75.0

TL;DR: FinS-Pilot是一个针对在线金融应用的RAG系统评估基准，结合实时API数据和结构化文本，填补了金融领域专用评估工具的空白。


<details>
  <summary>Details</summary>
Motivation: 金融RAG基准的开发受限于数据保密性和动态数据整合的缺乏，FinS-Pilot旨在解决这一问题。

Method: 基于真实金融助手交互数据，结合实时API和结构化文本，通过意图分类框架组织数据。

Result: 实验验证了FinS-Pilot在评估金融助手处理静态知识和动态市场信息能力上的有效性。

Conclusion: FinS-Pilot为金融NLP系统研究提供了实用的评估框架和数据集。

Abstract: Large language models (LLMs) have demonstrated remarkable capabilities across
various professional domains, with their performance typically evaluated
through standardized benchmarks. However, the development of financial RAG
benchmarks has been constrained by data confidentiality issues and the lack of
dynamic data integration. To address this issue, we introduces FinS-Pilot, a
novel benchmark for evaluating RAG systems in online financial applications.
Constructed from real-world financial assistant interactions, our benchmark
incorporates both real-time API data and structured text sources, organized
through an intent classification framework covering critical financial domains
such as equity analysis and macroeconomic forecasting. The benchmark enables
comprehensive evaluation of financial assistants' capabilities in handling both
static knowledge and time-sensitive market information. Through systematic
experiments with multiple Chinese leading LLMs, we demonstrate FinS-Pilot's
effectiveness in identifying models suitable for financial applications while
addressing the current gap in specialized evaluation tools for the financial
domain. Our work contributes both a practical evaluation framework and a
curated dataset to advance research in financial NLP systems. The code and
dataset are accessible on
GitHub\footnote{https://github.com/PhealenWang/financial\_rag\_benchmark}.

</details>


### [50] [Something Just Like TRuST : Toxicity Recognition of Span and Target](https://arxiv.org/abs/2506.02326)
*Berk Atil,Namrata Sureddy,Rebecca J. Passonneau*

Main category: cs.CL

Relevance: 75.0

TL;DR: TRuST是一个用于改进毒性检测的综合数据集，包含毒性、目标社会群体和毒性范围的标签。研究发现，微调模型在毒性检测任务上优于零样本和少样本提示，但LLMs的社会推理能力较弱。


<details>
  <summary>Details</summary>
Motivation: 在线内容（包括语言模型生成的内容）中的毒性问题对心理和社会有负面影响，需要更好的检测方法。

Method: 引入TRuST数据集，合并现有数据并添加标签，评估LLMs在毒性检测、目标群体识别和毒性范围提取上的表现。

Result: 微调模型表现优于零样本和少样本提示，但对某些社会群体性能仍低；LLMs的社会推理能力有限。

Conclusion: 需要进一步改进LLMs的社会推理能力，尤其是在毒性检测任务中。

Abstract: Toxicity in online content, including content generated by language models,
has become a critical concern due to its potential for negative psychological
and social impact. This paper introduces TRuST, a comprehensive dataset
designed to improve toxicity detection that merges existing datasets, and has
labels for toxicity, target social group, and toxic spans. It includes a
diverse range of target groups such as ethnicity, gender, religion, disability,
and politics, with both human/machine-annotated and human machine-generated
data. We benchmark state-of-the-art large language models (LLMs) on toxicity
detection, target group identification, and toxic span extraction. We find that
fine-tuned models consistently outperform zero-shot and few-shot prompting,
though performance remains low for certain social groups. Further, reasoning
capabilities do not significantly improve performance, indicating that LLMs
have weak social reasoning skills.

</details>


### [51] [Truth over Tricks: Measuring and Mitigating Shortcut Learning in Misinformation Detection](https://arxiv.org/abs/2506.02350)
*Herun Wan,Jiaying Wu,Minnan Luo,Zhi Zeng,Zhixiong Su*

Main category: cs.CL

Relevance: 75.0

TL;DR: 论文提出TruthOverTricks评估范式，检测虚假信息检测模型中的捷径学习问题，并提出SMF框架增强模型鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 虚假信息检测模型依赖训练数据中的表面线索（捷径），难以泛化到真实场景，且大语言模型（LLMs）加剧了这一问题。

Method: TruthOverTricks将捷径行为分类为内在诱导和外在注入，评估7种检测器在14个基准和2个新数据集上的表现。SMF框架通过数据增强（如改写、事实摘要）减少捷径依赖。

Result: 现有检测器在自然和对抗性捷径下性能显著下降，SMF在16个基准上提升鲁棒性。

Conclusion: SMF通过深层语义理解减少捷径依赖，促进虚假信息检测发展。

Abstract: Misinformation detection models often rely on superficial cues (i.e.,
\emph{shortcuts}) that correlate with misinformation in training data but fail
to generalize to the diverse and evolving nature of real-world misinformation.
This issue is exacerbated by large language models (LLMs), which can easily
generate convincing misinformation through simple prompts. We introduce
TruthOverTricks, a unified evaluation paradigm for measuring shortcut learning
in misinformation detection. TruthOverTricks categorizes shortcut behaviors
into intrinsic shortcut induction and extrinsic shortcut injection, and
evaluates seven representative detectors across 14 popular benchmarks, along
with two new factual misinformation datasets, NQ-Misinfo and Streaming-Misinfo.
Empirical results reveal that existing detectors suffer severe performance
degradation when exposed to both naturally occurring and adversarially crafted
shortcuts. To address this, we propose SMF, an LLM-augmented data augmentation
framework that mitigates shortcut reliance through paraphrasing, factual
summarization, and sentiment normalization. SMF consistently enhances
robustness across 16 benchmarks, encouraging models to rely on deeper semantic
understanding rather than shortcut cues. To promote the development of
misinformation detectors, we have published the resources publicly at
https://github.com/whr000001/TruthOverTricks.

</details>


### [52] [Multimodal DeepResearcher: Generating Text-Chart Interleaved Reports From Scratch with Agentic Framework](https://arxiv.org/abs/2506.02454)
*Zhaorui Yang,Bo Pan,Han Wang,Yiyao Wang,Xingyu Liu,Minfeng Zhu,Bo Zhang,Wei Chen*

Main category: cs.CL

Relevance: 75.0

TL;DR: 论文提出了一种多模态深度研究框架，结合文本和可视化生成，解决了现有框架仅生成文本的问题。


<details>
  <summary>Details</summary>
Motivation: 现有深度研究框架主要生成文本内容，缺乏文本与可视化的结合，限制了信息传达效果。

Method: 提出FDV（可视化形式描述）作为结构化文本表示，并开发Multimodal DeepResearcher框架，分四阶段生成多模态报告。

Result: 在Claude 3.7 Sonnet模型上，Multimodal DeepResearcher的总体胜率为82%。

Conclusion: FDV和多模态框架有效提升了文本与可视化的结合能力，为多模态研究提供了新方向。

Abstract: Visualizations play a crucial part in effective communication of concepts and
information. Recent advances in reasoning and retrieval augmented generation
have enabled Large Language Models (LLMs) to perform deep research and generate
comprehensive reports. Despite its progress, existing deep research frameworks
primarily focus on generating text-only content, leaving the automated
generation of interleaved texts and visualizations underexplored. This novel
task poses key challenges in designing informative visualizations and
effectively integrating them with text reports. To address these challenges, we
propose Formal Description of Visualization (FDV), a structured textual
representation of charts that enables LLMs to learn from and generate diverse,
high-quality visualizations. Building on this representation, we introduce
Multimodal DeepResearcher, an agentic framework that decomposes the task into
four stages: (1) researching, (2) exemplar report textualization, (3) planning,
and (4) multimodal report generation. For the evaluation of generated
multimodal reports, we develop MultimodalReportBench, which contains 100
diverse topics served as inputs along with 5 dedicated metrics. Extensive
experiments across models and evaluation methods demonstrate the effectiveness
of Multimodal DeepResearcher. Notably, utilizing the same Claude 3.7 Sonnet
model, Multimodal DeepResearcher achieves an 82\% overall win rate over the
baseline method.

</details>


### [53] [M$^3$FinMeeting: A Multilingual, Multi-Sector, and Multi-Task Financial Meeting Understanding Evaluation Dataset](https://arxiv.org/abs/2506.02510)
*Jie Zhu,Junhui Li,Yalong Wen,Xiandong Li,Lifan Guo,Feng Chen*

Main category: cs.CL

Relevance: 75.0

TL;DR: 论文提出了一个多语言、多行业、多任务的金融会议理解基准M³FinMeeting，用于评估LLMs在金融领域的表现。


<details>
  <summary>Details</summary>
Motivation: 当前金融领域的基准测试主要依赖新闻或报告，难以捕捉真实金融会议的动态，因此需要更全面的评估工具。

Method: 构建了支持英语、中文和日语的M³FinMeeting数据集，涵盖多个行业和任务（摘要、QA对提取和问答）。

Result: 实验表明，即使是先进的长上下文LLMs在金融会议理解上仍有显著改进空间。

Conclusion: M³FinMeeting是一个有效的基准，可全面评估LLMs的金融会议理解能力。

Abstract: Recent breakthroughs in large language models (LLMs) have led to the
development of new benchmarks for evaluating their performance in the financial
domain. However, current financial benchmarks often rely on news articles,
earnings reports, or announcements, making it challenging to capture the
real-world dynamics of financial meetings. To address this gap, we propose a
novel benchmark called $\texttt{M$^3$FinMeeting}$, which is a multilingual,
multi-sector, and multi-task dataset designed for financial meeting
understanding. First, $\texttt{M$^3$FinMeeting}$ supports English, Chinese, and
Japanese, enhancing comprehension of financial discussions in diverse
linguistic contexts. Second, it encompasses various industry sectors defined by
the Global Industry Classification Standard (GICS), ensuring that the benchmark
spans a broad range of financial activities. Finally,
$\texttt{M$^3$FinMeeting}$ includes three tasks: summarization, question-answer
(QA) pair extraction, and question answering, facilitating a more realistic and
comprehensive evaluation of understanding. Experimental results with seven
popular LLMs reveal that even the most advanced long-context models have
significant room for improvement, demonstrating the effectiveness of
$\texttt{M$^3$FinMeeting}$ as a benchmark for assessing LLMs' financial meeting
comprehension skills.

</details>


### [54] [IndoSafety: Culturally Grounded Safety for LLMs in Indonesian Languages](https://arxiv.org/abs/2506.02573)
*Muhammad Falensi Azmi,Muhammad Dehan Al Kautsar,Alfan Farizki Wicaksono,Fajri Koto*

Main category: cs.CL

Relevance: 75.0

TL;DR: 论文提出了IndoSafety，一个针对印尼文化背景的高质量安全评估数据集，用于评估区域特定LLM的安全性。


<details>
  <summary>Details</summary>
Motivation: 研究印尼特定LLM的安全性，尤其是在多语言和文化多样性的背景下，确保模型符合当地社会文化规范。

Method: 扩展现有安全框架，构建涵盖五种印尼语言变体的数据集IndoSafety，并通过微调LLM验证其效果。

Result: 现有印尼中心LLM在非正式和本地语言环境下常生成不安全输出，而基于IndoSafety微调显著提升安全性且不影响任务性能。

Conclusion: 强调文化背景对LLM安全评估的重要性，为多语言环境下负责任部署LLM提供实践步骤。

Abstract: Although region-specific large language models (LLMs) are increasingly
developed, their safety remains underexplored, particularly in culturally
diverse settings like Indonesia, where sensitivity to local norms is essential
and highly valued by the community. In this work, we present IndoSafety, the
first high-quality, human-verified safety evaluation dataset tailored for the
Indonesian context, covering five language varieties: formal and colloquial
Indonesian, along with three major local languages: Javanese, Sundanese, and
Minangkabau. IndoSafety is constructed by extending prior safety frameworks to
develop a taxonomy that captures Indonesia's sociocultural context. We find
that existing Indonesian-centric LLMs often generate unsafe outputs,
particularly in colloquial and local language settings, while fine-tuning on
IndoSafety significantly improves safety while preserving task performance. Our
work highlights the critical need for culturally grounded safety evaluation and
provides a concrete step toward responsible LLM deployment in multilingual
settings. Warning: This paper contains example data that may be offensive,
harmful, or biased.

</details>


### [55] [On Generalization across Measurement Systems: LLMs Entail More Test-Time Compute for Underrepresented Cultures](https://arxiv.org/abs/2506.02591)
*Minh Duc Bui,Kyung Eun Park,Goran Glavaš,Fabian David Schmidt,Katharina von der Wense*

Main category: cs.CL

Relevance: 75.0

TL;DR: 论文研究了大型语言模型（LLMs）在不同测量系统（如货币）中的表现，发现其默认使用数据中占主导地位的测量系统，且性能在不同系统间不稳定。通过推理方法（如CoT）可部分缓解问题，但会增加计算成本。


<details>
  <summary>Details</summary>
Motivation: 确保LLMs能为不同文化背景的用户提供准确的测量信息，无论其使用的测量系统如何。

Method: 使用新编译的数据集测试七个开源LLMs，研究其默认测量系统、性能差异及推理方法的效果。

Result: LLMs默认使用数据中占主导的测量系统，性能在不同系统间不稳定；CoT可部分缓解问题但增加计算成本。

Conclusion: LLMs在测量系统多样性方面存在不足，需进一步优化以减少偏见和计算成本。

Abstract: Measurement systems (e.g., currencies) differ across cultures, but the
conversions between them are well defined so that humans can state facts using
any measurement system of their choice. Being available to users from diverse
cultural backgrounds, large language models (LLMs) should also be able to
provide accurate information irrespective of the measurement system at hand.
Using newly compiled datasets we test if this is the case for seven open-source
LLMs, addressing three key research questions: (RQ1) What is the default system
used by LLMs for each type of measurement? (RQ2) Do LLMs' answers and their
accuracy vary across different measurement systems? (RQ3) Can LLMs mitigate
potential challenges w.r.t. underrepresented systems via reasoning? Our
findings show that LLMs default to the measurement system predominantly used in
the data. Additionally, we observe considerable instability and variance in
performance across different measurement systems. While this instability can in
part be mitigated by employing reasoning methods such as chain-of-thought
(CoT), this implies longer responses and thereby significantly increases
test-time compute (and inference costs), marginalizing users from cultural
backgrounds that use underrepresented measurement systems.

</details>


### [56] [Facts Do Care About Your Language: Assessing Answer Quality of Multilingual LLMs](https://arxiv.org/abs/2506.03051)
*Yuval Kansal,Shmuel Berman,Lydia Liu*

Main category: cs.CL

Relevance: 75.0

TL;DR: 评估Llama3.1系列模型在多语言教育场景中的事实准确性，发现其存在信息冗余、真实性不足及对稀有语言的偏见问题。


<details>
  <summary>Details</summary>
Motivation: 随着LLM在教育领域的广泛应用，确保其多语言环境下的准确性至关重要，尤其是针对稀有语言的事实性问题。

Method: 通过设计适合初高中学生的事实性问题，对Llama3.1系列模型在多语言环境中的表现进行评估。

Result: 模型不仅提供冗余且真实性较低的信息，还加剧了对稀有语言的偏见。

Conclusion: LLM在多语言教育场景中的事实准确性仍需改进，尤其是对稀有语言的支持。

Abstract: Factuality is a necessary precursor to useful educational tools. As adoption
of Large Language Models (LLMs) in education continues of grow, ensuring
correctness in all settings is paramount. Despite their strong English
capabilities, LLM performance in other languages is largely untested. In this
work, we evaluate the correctness of the Llama3.1 family of models in answering
factual questions appropriate for middle and high school students. We
demonstrate that LLMs not only provide extraneous and less truthful
information, but also exacerbate existing biases against rare languages.

</details>


### [57] [Co-Evolving LLM Coder and Unit Tester via Reinforcement Learning](https://arxiv.org/abs/2506.03136)
*Yinjie Wang,Ling Yang,Ye Tian,Ke Shen,Mengdi Wang*

Main category: cs.CL

Relevance: 75.0

TL;DR: CURE是一个新颖的强化学习框架，通过协同进化代码生成和单元测试能力，无需真实代码监督，显著提升了代码生成和测试的准确性。


<details>
  <summary>Details</summary>
Motivation: 解决代码生成和单元测试的协同优化问题，避免对真实代码的依赖，提高灵活性和可扩展性。

Method: 采用强化学习框架，设计专用奖励机制，协同优化代码生成和单元测试能力。

Result: 在Qwen2.5-Instruct模型上优化后，代码生成准确率提升5.3%，Best-of-N准确率提升9.0%，并在下游任务中表现优异。

Conclusion: CURE框架有效提升了代码生成和测试能力，并可作为强化学习的奖励模型。

Abstract: We propose CURE, a novel reinforcement learning framework with a dedicated
reward design that co-evolves coding and unit test generation capabilities
based on their interaction outcomes, without any ground-truth code as
supervision. This approach enables flexible and scalable training and allows
the unit tester to learn directly from the coder's mistakes. Our derived
ReasonFlux-Coder-7B and 14B models improve code generation accuracy by 5.3% and
Best-of-N accuracy by 9.0% after optimization on Qwen2.5-Instruct models,
outperforming similarly sized Qwen-Coder, DeepSeek-Coder, and Seed-Coder. They
naturally extend to downstream tasks such as test-time scaling and agentic
coding-achieving a 8.1% improvement over the base model. For the long-CoT
model, our ReasonFlux-Coder-4B consistently outperforms Qwen3-4B while
achieving 64.8% inference efficiency in unit test generation. Notably, we also
find that our model can serve as an effective reward model for reinforcement
learning on base models. Project: https://github.com/Gen-Verse/CURE

</details>


### [58] [CoDial: Interpretable Task-Oriented Dialogue Systems Through Dialogue Flow Alignment](https://arxiv.org/abs/2506.02264)
*Radin Shayanfar,Chu Fei Luo,Rohan Bhambhoria,Samuel Dahan,Xiaodan Zhu*

Main category: cs.CL

Relevance: 70.0

TL;DR: CoDial框架通过将专家知识转化为可执行的对话逻辑，支持非技术专家轻松定义和优化任务导向对话系统，并在STAR和MultiWOZ数据集上表现优异。


<details>
  <summary>Details</summary>
Motivation: 解决领域特定任务对话系统的高成本和技术难度问题，促进跨学科合作。

Method: 引入CoDial框架，将专家知识表示为结构化异构图并转化为对话逻辑，支持零样本任务定义和迭代优化。

Result: 在STAR数据集上达到最先进性能，在MultiWOZ数据集上与基线模型竞争。

Conclusion: CoDial是专家引导LLM对齐的实用工具，适用于高风险领域。

Abstract: It is often challenging to teach specialized, unseen tasks to dialogue
systems due to the high cost of expert knowledge, training data, and high
technical difficulty. To support domain-specific applications - such as law,
medicine, or finance - it is essential to build frameworks that enable
non-technical experts to define, test, and refine system behaviour with minimal
effort. Achieving this requires cross-disciplinary collaboration between
developers and domain specialists. In this work, we introduce a novel
framework, CoDial (Code for Dialogue), that converts expert knowledge,
represented as a novel structured heterogeneous graph, into executable
conversation logic. CoDial can be easily implemented in existing guardrailing
languages, such as Colang, to enable interpretable, modifiable, and true
zero-shot specification of task-oriented dialogue systems. Empirically, CoDial
achieves state-of-the-art performance on the STAR dataset for inference-based
models and is competitive with similar baselines on the well-known MultiWOZ
dataset. We also demonstrate CoDial's iterative improvement via manual and
LLM-aided feedback, making it a practical tool for expert-guided alignment of
LLMs in high-stakes domains.

</details>


### [59] [AnswerCarefully: A Dataset for Improving the Safety of Japanese LLM Output](https://arxiv.org/abs/2506.02372)
*Hisami Suzuki,Satoru Katsumata,Takashi Kodama,Tetsuro Takahashi,Kouta Nakayama,Satoshi Sekine*

Main category: cs.CL

Relevance: 70.0

TL;DR: AnswerCarefully是一个用于提升日语LLM输出安全性和适当性的数据集，包含1,800对问题和参考答案，覆盖多种风险类别，并针对日本社会文化背景设计。


<details>
  <summary>Details</summary>
Motivation: 解决日语LLM输出中的安全和适当性问题，填补现有英语数据集的不足，并适应日本社会文化背景。

Method: 通过手动创建反映日本社会文化背景的问题和参考答案，构建数据集，并用于微调日语LLM。

Result: 微调后的日语LLM在保持通用回答实用性的同时提升了安全性，同时评估了12个日语LLM的安全性。

Conclusion: AnswerCarefully数据集有效提升了日语LLM的安全性和适当性，并提供了英语翻译和标注以支持多语言和地区的数据集开发。

Abstract: In this paper we present AnswerCarefully, a dataset for promoting the safety
and appropriateness of Japanese LLM outputs. The dataset consists of 1,800
pairs of questions and reference answers, where the questions require special
attention in answering. It covers a wide range of risk categories established
in prior English-language datasets, but the data samples are original in that
they are manually created to reflect the socio-cultural context of LLM usage in
Japan. We show that using this dataset for instruction to fine-tune a Japanese
LLM led to improved output safety without compromising the utility of general
responses. We also report the results of a safety evaluation of 12 Japanese
LLMs using this dataset as a benchmark. Finally, we describe the latest update
on the dataset which provides English translations and annotations of the
questions, aimed at facilitating the derivation of similar datasets in
different languages and regions.

</details>


### [60] [Comparative Analysis of AI Agent Architectures for Entity Relationship Classification](https://arxiv.org/abs/2506.02426)
*Maryam Berijanian,Kuldeep Singh,Amin Sehati*

Main category: cs.CL

Relevance: 70.0

TL;DR: 论文比较了三种基于LLM的AI代理架构在关系分类任务中的表现，发现多智能体协作优于标准少样本提示，接近微调模型性能。


<details>
  <summary>Details</summary>
Motivation: 解决信息提取中关系分类任务在有限标注数据和复杂关系结构下的挑战。

Method: 比较三种代理架构：反射自评估、分层任务分解和多智能体动态示例生成，利用不同的推理和提示适应模式。

Result: 多智能体协作表现最佳，接近微调模型性能。

Conclusion: 为模块化、通用性LLM系统设计提供了实用指导。

Abstract: Entity relationship classification remains a challenging task in information
extraction, especially in scenarios with limited labeled data and complex
relational structures. In this study, we conduct a comparative analysis of
three distinct AI agent architectures designed to perform relation
classification using large language models (LLMs). The agentic architectures
explored include (1) reflective self-evaluation, (2) hierarchical task
decomposition, and (3) a novel multi-agent dynamic example generation
mechanism, each leveraging different modes of reasoning and prompt adaptation.
In particular, our dynamic example generation approach introduces real-time
cooperative and adversarial prompting. We systematically compare their
performance across multiple domains and model backends. Our experiments
demonstrate that multi-agent coordination consistently outperforms standard
few-shot prompting and approaches the performance of fine-tuned models. These
findings offer practical guidance for the design of modular, generalizable
LLM-based systems for structured relation extraction. The source codes and
dataset are available at
\href{https://github.com/maryambrj/ALIEN.git}{https://github.com/maryambrj/ALIEN.git}.

</details>


### [61] [From Anger to Joy: How Nationality Personas Shape Emotion Attribution in Large Language Models](https://arxiv.org/abs/2506.02431)
*Mahammed Kamruzzaman,Abdullah Al Monsur,Gene Louis Kim,Anshuman Chhabra*

Main category: cs.CL

Relevance: 70.0

TL;DR: 研究探讨了大型语言模型（LLMs）在扮演特定国家角色时是否表现出情感刻板印象，发现模型输出的情感分配存在显著的国家差异，并与人类情感反应不一致。


<details>
  <summary>Details</summary>
Motivation: 情感是人类体验的核心，而LLMs作为角色扮演代理的广泛应用引发了对其是否反映情感刻板印象的疑问。

Method: 通过分析预训练LLMs对不同国家角色的情感分配，并与文化规范对比。

Result: 发现LLMs输出的情感存在国家差异，尤其是负面情感与人类反应不一致，揭示了潜在的偏见。

Conclusion: LLMs在情感表达上存在刻板印象，需进一步优化以减少偏见。

Abstract: Emotions are a fundamental facet of human experience, varying across
individuals, cultural contexts, and nationalities. Given the recent success of
Large Language Models (LLMs) as role-playing agents, we examine whether LLMs
exhibit emotional stereotypes when assigned nationality-specific personas.
Specifically, we investigate how different countries are represented in
pre-trained LLMs through emotion attributions and whether these attributions
align with cultural norms. Our analysis reveals significant nationality-based
differences, with emotions such as shame, fear, and joy being
disproportionately assigned across regions. Furthermore, we observe notable
misalignment between LLM-generated and human emotional responses, particularly
for negative emotions, highlighting the presence of reductive and potentially
biased stereotypes in LLM outputs.

</details>


### [62] [IP-Dialog: Evaluating Implicit Personalization in Dialogue Systems with Synthetic Data](https://arxiv.org/abs/2506.02449)
*Bo Peng,Zhiheng Wang,Heyang Gong,Chaochao Lu*

Main category: cs.CL

Relevance: 70.0

TL;DR: 论文提出了一种自动生成合成数据的方法，并引入了IP-Dialog基准和训练数据集，用于评估和改进对话系统中用户背景的隐式推理能力。


<details>
  <summary>Details</summary>
Motivation: 现代对话系统需要从对话中隐式推断用户背景以实现个性化辅助，但高质量数据的稀缺性限制了这一能力的评估和改进。

Method: 提出自动合成数据生成方法，构建IP-Dialog基准和训练数据集，开发系统评估框架和因果图分析模型推理路径。

Result: 实验验证了数据集的可靠性，并提供了关于模型推理能力的深入观察。

Conclusion: 该方法为解决数据稀缺问题提供了有效途径，并为对话系统的个性化能力评估提供了新工具。

Abstract: In modern dialogue systems, the ability to implicitly infer user backgrounds
from conversations and leverage this information for personalized assistance is
crucial. However, the scarcity of high-quality data remains a fundamental
challenge to evaluating and improving this capability. Traditional dataset
construction methods are labor-intensive, resource-demanding, and raise privacy
concerns. To address these issues, we propose a novel approach for automatic
synthetic data generation and introduce the Implicit Personalized Dialogue
(IP-Dialog) benchmark along with a training dataset, covering 10 tasks and 12
user attribute types. Additionally, we develop a systematic evaluation
framework with four metrics to assess both attribute awareness and reasoning
capabilities. We further propose five causal graphs to elucidate models'
reasoning pathways during implicit personalization. Extensive experiments yield
insightful observations and prove the reliability of our dataset.

</details>


### [63] [Minos: A Multimodal Evaluation Model for Bidirectional Generation Between Image and Text](https://arxiv.org/abs/2506.02494)
*Junzhe Zhang,Huixuan Zhang,Xinyu Hu,Li Lin,Mingqi Gao,Shi Qiu,Xiaojun Wan*

Main category: cs.CL

Relevance: 70.0

TL;DR: 论文提出了Minos-Corpus，一个结合人类和GPT评估数据的大规模多模态评估数据集，并基于此开发了Minos模型，在T2I生成任务评估中表现优异。


<details>
  <summary>Details</summary>
Motivation: 现有工作忽视了文本到图像（T2I）生成任务的评估能力发展和大规模人类评估数据的结合，因此需要填补这一空白。

Method: 提出了Minos-Corpus数据集，结合人类和GPT的评估数据；开发了Data Selection and Balance、Mix-SFT训练方法，并应用DPO训练Minos模型。

Result: Minos在T2I生成任务评估中优于所有开源和闭源模型，并在所有任务的平均评估性能上达到同类规模模型的最优水平。

Conclusion: 高质量人类评估数据和联合训练I2T与T2I任务评估数据的重要性得到验证。

Abstract: Evaluation is important for multimodal generation tasks. With the rapid
progress of MLLMs, there is growing interest in applying MLLMs to build general
evaluation systems. However, existing work overlooks two aspects: (1) the
development of evaluation capabilities for text-to-image (T2I) generation task,
and (2) the incorporation of large-scale human evaluation data. In this paper,
we introduce Minos-Corpus, a large-scale multimodal evaluation dataset that
combines evaluation data from both human and GPT. The corpus contains
evaluation data across both image-to-text(I2T) and T2I generation tasks. Based
on this corpus, we propose Data Selection and Balance, Mix-SFT training
methods, and apply DPO to develop Minos, a multimodal evaluation model built
upon a 7B backbone. Minos achieves state-of-the-art (SoTA) performance among
all open-source evaluation models of similar scale on the average of evaluation
performance on all tasks, and outperforms all open-source and closed-source
models on evaluation of T2I generation task. Extensive experiments demonstrate
the importance of leveraging high-quality human evaluation data and jointly
training on evaluation data from both I2T and T2I generation tasks.

</details>


### [64] [FinChain: A Symbolic Benchmark for Verifiable Chain-of-Thought Financial Reasoning](https://arxiv.org/abs/2506.02515)
*Zhuohan Xie,Dhruv Sahnan,Debopriyo Banerjee,Georgi Georgiev,Rushil Thareja,Hachem Madmoun,Jinyan Su,Aaryamonvikram Singh,Yuxia Wang,Rui Xing,Fajri Koto,Haonan Li,Ivan Koychev,Tanmoy Chakraborty,Salem Lahlou,Veselin Stoyanov,Preslav Nakov*

Main category: cs.CL

Relevance: 70.0

TL;DR: 论文介绍了FinChain，首个用于验证链式思维（CoT）金融推理的符号基准，填补了现有数据集仅关注最终答案的不足。


<details>
  <summary>Details</summary>
Motivation: 现有金融任务数据集（如FinQA和ConvFinQA）仅监督最终数值答案，缺乏对中间推理步骤的评估，限制了多步符号推理能力的系统性评测。

Method: FinChain涵盖12个金融领域的54个主题，每个主题提供5个参数化模板，支持自动生成训练数据。同时提出ChainEval指标，自动评估最终答案和中间推理。

Result: 在30个LLMs上的测试表明，即使是先进模型在多步金融推理上仍有显著改进空间。

Conclusion: FinChain为金融推理提供了系统性评测工具，推动了LLMs在多步推理任务中的进步。

Abstract: Multi-step symbolic reasoning is critical for advancing downstream
performance on financial tasks. Yet, benchmarks for systematically evaluating
this capability are lacking. Existing datasets like FinQA and ConvFinQA
supervise only final numerical answers, without assessing intermediate
reasoning steps. To address this, we introduce FinChain, the first symbolic
benchmark designed for verifiable Chain-of- Thought (CoT) financial reasoning.
Spanning 54 topics across 12 financial domains, Fin- Chain offers five
parameterized templates per topic, each varying in reasoning complexity and
domain expertise required. Each dataset instance includes an executable Python
trace, enabling automatic generation of extensive training data and easy
adaptation to other domains. We also introduce ChainEval, a new metric for
automatic evaluation of both final answers and intermediate reasoning.
Benchmarking 30 LLMs on our dataset, we find that even state-of-the-art models
have considerable room for improvement in multi-step financial reasoning. All
templates and evaluation metrics for FinChain are available at https:
//github.com/mbzuai-nlp/finchain.

</details>


### [65] [ReasoningFlow: Semantic Structure of Complex Reasoning Traces](https://arxiv.org/abs/2506.02532)
*Jinu Lee,Sagnik Mukherjee,Dilek Hakkani-Tur,Julia Hockenmaier*

Main category: cs.CL

Relevance: 70.0

TL;DR: ReasoningFlow 是一种统一的分析框架，用于解析大型推理模型（LRMs）生成的复杂推理轨迹，将其转化为有向无环图，以识别不同的推理模式。


<details>
  <summary>Details</summary>
Motivation: 研究动机是理解和改进大型推理模型的推理过程，通过分析其生成的复杂推理轨迹。

Method: 方法是将推理轨迹解析为有向无环图（DAG），并通过子图结构表征不同的推理模式。

Result: 结果是一种人类可解释的表示方法，可用于理解、评估和增强 LRMs 的推理过程。

Conclusion: 结论是 ReasoningFlow 为分析和改进大型推理模型的推理能力提供了有效工具。

Abstract: Large reasoning models (LRMs) generate complex reasoning traces with
planning, reflection, verification, and backtracking. In this work, we
introduce ReasoningFlow, a unified schema for analyzing the semantic structures
of these complex traces. ReasoningFlow parses traces into directed acyclic
graphs, enabling the characterization of distinct reasoning patterns as
subgraph structures. This human-interpretable representation offers promising
applications in understanding, evaluating, and enhancing the reasoning
processes of LRMs.

</details>


### [66] [CoRe-MMRAG: Cross-Source Knowledge Reconciliation for Multimodal RAG](https://arxiv.org/abs/2506.02544)
*Yang Tian,Fan Liu,Jingyuan Zhang,Victoria W.,Yupeng Hu,Liqiang Nie*

Main category: cs.CL

Relevance: 70.0

TL;DR: CoRe-MMRAG是一个端到端框架，通过协调多模态检索增强生成中的知识不一致性，显著提升了多模态大语言模型的性能。


<details>
  <summary>Details</summary>
Motivation: 解决多模态检索增强生成中的参数化与检索知识不一致（PRKI）以及视觉与文本知识不一致（VTKI）问题。

Method: 采用四阶段流程：生成内部响应、选择多模态证据、生成外部响应、整合生成可靠答案，并通过专门训练范式增强知识源判别和多模态整合。

Result: 在KB-VQA基准测试中，性能分别提升5.6%和9.3%。

Conclusion: CoRe-MMRAG有效解决了多模态知识不一致问题，提升了模型性能。

Abstract: Multimodal Retrieval-Augmented Generation (MMRAG) has been introduced to
enhance Multimodal Large Language Models by incorporating externally retrieved
multimodal knowledge, but it introduces two challenges: Parametric-Retrieved
Knowledge Inconsistency (PRKI), where discrepancies between parametric and
retrieved knowledge create uncertainty in determining reliability, and
Visual-Textual Knowledge Inconsistency (VTKI), where misalignment between
visual and textual sources disrupts entity representation. To address these
challenges, we propose \textbf{C}r\textbf{o}ss-source knowledge
\textbf{Re}conciliation for \textbf{M}ulti\textbf{M}odal \textbf{RAG}
(CoRe-MMRAG), a novel end-to-end framework that effectively reconciles
inconsistencies across knowledge sources. CoRe-MMRAG follows a four-stage
pipeline: it first generates an internal response from parametric knowledge,
then selects the most relevant multimodal evidence via joint similarity
assessment, generates an external response, and finally integrates both to
produce a reliable answer. Additionally, a specialized training paradigm
enhances knowledge source discrimination, multimodal integration, and unified
answer generation. Experiments on KB-VQA benchmarks show that CoRe-MMRAG
achieves substantial improvements over baseline methods, achieving 5.6\% and
9.3\% performance gains on InfoSeek and Encyclopedic-VQA, respectively. We
release code and data at
\href{https://github.com/TyangJN/CoRe-MMRAG}{https://github.com/TyangJN/CoRe-MMRAG}.

</details>


### [67] [SemVink: Advancing VLMs' Semantic Understanding of Optical Illusions via Visual Global Thinking](https://arxiv.org/abs/2506.02803)
*Sifan Li,Yujun Cai,Yiwei Wang*

Main category: cs.CL

Relevance: 70.0

TL;DR: 论文提出HC-Bench基准测试，揭示视觉语言模型（VLMs）在检测隐藏内容（如光学错觉或AI生成图像）时表现极差（0-5.36%准确率），并提出SemVink方法，通过降低图像分辨率显著提升准确率至>99%。


<details>
  <summary>Details</summary>
Motivation: 现有VLMs在语义任务上表现优异，但在人类本能感知的隐藏内容检测上表现不佳，这限制了其在真实场景中的应用。

Method: 引入HC-Bench基准测试，评估VLMs在隐藏内容检测上的表现；提出SemVink方法，通过降低图像分辨率消除视觉噪声。

Result: VLMs在HC-Bench上表现极差（0-5.36%准确率），而SemVink方法显著提升准确率至>99%。

Conclusion: VLMs过度依赖高层语义而忽视低层视觉操作，需设计混合模型以整合多尺度处理，提升真实场景鲁棒性。

Abstract: Vision-language models (VLMs) excel in semantic tasks but falter at a core
human capability: detecting hidden content in optical illusions or AI-generated
images through perceptual adjustments like zooming. We introduce HC-Bench, a
benchmark of 112 images with hidden text, objects, and illusions, revealing
that leading VLMs achieve near-zero accuracy (0-5.36%)-even with explicit
prompting. Humans resolve such ambiguities instinctively, yet VLMs fail due to
an overreliance on high-level semantics. Strikingly, we propose SemVink
(Semantic Visual Thinking) by simply scaling images to low resolutions (32-128
pixels), which unlocks >99% accuracy by eliminating redundant visual noise.
This exposes a critical architectural flaw: VLMs prioritize abstract reasoning
over low-level visual operations crucial for real-world robustness. Our work
urges a shift toward hybrid models integrating multi-scale processing, bridging
the gap between computational vision and human cognition for applications in
medical imaging, security, and beyond.

</details>


### [68] [Performance of leading large language models in May 2025 in Membership of the Royal College of General Practitioners-style examination questions: a cross-sectional analysis](https://arxiv.org/abs/2506.02987)
*Richard Armitage*

Main category: cs.CL

Relevance: 70.0

TL;DR: 论文测试了2025年5月的领先LLM（o3、Claude Opus 4、Grok3和Gemini 2.5 Pro）在回答MRCGP考试问题中的表现，发现它们显著优于普通GP医生。


<details>
  <summary>Details</summary>
Motivation: 评估领先LLM在初级医疗教育中的能力，尤其是推理模型的表现。

Method: 让四个LLM回答100个MRCGP考试问题，评分并与GP医生平均分对比。

Result: o3得分99%，其他模型95%，均显著高于GP平均分73%。

Conclusion: LLM（尤其是推理模型）在初级医疗支持中具有潜力。

Abstract: Background: Large language models (LLMs) have demonstrated substantial
potential to support clinical practice. Other than Chat GPT4 and its
predecessors, few LLMs, especially those of the leading and more powerful
reasoning model class, have been subjected to medical specialty examination
questions, including in the domain of primary care. This paper aimed to test
the capabilities of leading LLMs as of May 2025 (o3, Claude Opus 4, Grok3, and
Gemini 2.5 Pro) in primary care education, specifically in answering Member of
the Royal College of General Practitioners (MRCGP) style examination questions.
  Methods: o3, Claude Opus 4, Grok3, and Gemini 2.5 Pro were tasked to answer
100 randomly chosen multiple choice questions from the Royal College of General
Practitioners GP SelfTest on 25 May 2025. Questions included textual
information, laboratory results, and clinical images. Each model was prompted
to answer as a GP in the UK and was provided with full question information.
Each question was attempted once by each model. Responses were scored against
correct answers provided by GP SelfTest.
  Results: The total score of o3, Claude Opus 4, Grok3, and Gemini 2.5 Pro was
99.0%, 95.0%, 95.0%, and 95.0%, respectively. The average peer score for the
same questions was 73.0%.
  Discussion: All models performed remarkably well, and all substantially
exceeded the average performance of GPs and GP registrars who had answered the
same questions. o3 demonstrated the best performance, while the performances of
the other leading models were comparable with each other and were not
substantially lower than that of o3. These findings strengthen the case for
LLMs, particularly reasoning models, to support the delivery of primary care,
especially those that have been specifically trained on primary care clinical
data.

</details>


### [69] [Coding Agents with Multimodal Browsing are Generalist Problem Solvers](https://arxiv.org/abs/2506.03011)
*Aditya Bharat Soni,Boxuan Li,Xingyao Wang,Valerie Chen,Graham Neubig*

Main category: cs.CL

Relevance: 70.0

TL;DR: OpenHands-Versa是一个通用AI代理，使用少量通用工具（如代码编辑、网页搜索等）在多样化任务中表现优异，超越了专用代理。


<details>
  <summary>Details</summary>
Motivation: 研究目标是找到最小通用工具集，以构建能在多样化任务中表现优异的通用代理，解决专用代理泛化能力不足的问题。

Method: 开发OpenHands-Versa代理，使用代码编辑、网页搜索、多模态网页浏览和文件访问等通用工具。

Result: 在SWE-Bench Multimodal、GAIA和The Agent Company三个基准测试中，OpenHands-Versa表现优于专用代理，成功率分别提升9.1、1.3和9.1个百分点。

Conclusion: 证明了构建通用代理的可行性，OpenHands-Versa为未来研究提供了强基线。

Abstract: Modern human labor is characterized by specialization; we train for years and
develop particular tools that allow us to perform well across a variety of
tasks. In addition, AI agents have been specialized for domains such as
software engineering, web navigation, and workflow automation. However, this
results in agents that are good for one thing but fail to generalize beyond
their intended scope. One reason for this is that agent developers provide a
highly specialized set of tools or make architectural decisions optimized for a
specific use case or benchmark. In this work, we ask the question: what is the
minimal set of general tools that can be used to achieve high performance
across a diverse set of tasks? Our answer is OpenHands-Versa, a generalist
agent built with a modest number of general tools: code editing and execution,
web search, as well as multimodal web browsing and file access. Importantly,
OpenHands-Versa demonstrates superior or competitive performance over leading
specialized agents across three diverse and challenging benchmarks: SWE-Bench
Multimodal, GAIA, and The Agent Company, outperforming the best-performing
previously published results with absolute improvements in success rate of 9.1,
1.3, and 9.1 points respectively. Further, we show how existing
state-of-the-art multi-agent systems fail to generalize beyond their target
domains. These results demonstrate the feasibility of developing a generalist
agent to solve diverse tasks and establish OpenHands-Versa as a strong baseline
for future research.

</details>


### [70] [Literary Evidence Retrieval via Long-Context Language Models](https://arxiv.org/abs/2506.03090)
*Katherine Thai,Mohit Iyyer*

Main category: cs.CL

Relevance: 70.0

TL;DR: 论文探讨了现代长上下文语言模型对文学小说的理解能力，通过文学证据检索任务评估模型表现。实验显示，闭源模型如Gemini Pro 2.5表现优于人类专家，而开源模型表现较差，揭示了模型在文学分析中的局限性。


<details>
  <summary>Details</summary>
Motivation: 研究现代长上下文语言模型在文学分析任务中的表现，尤其是对全局叙事推理和细节文本理解的能力。

Method: 利用RELiC数据集构建基准任务，要求模型从完整文本中生成缺失的文学引用。通过筛选和人工验证，构建了292个高质量示例。

Result: 闭源模型Gemini Pro 2.5表现优于人类专家（62.5% vs. 50%），而最佳开源模型仅达到29.1%。模型在文学信号理解和生成控制上仍有困难。

Conclusion: 尽管闭源模型表现优异，但文学分析任务仍存在挑战，尤其是对开源模型。研究鼓励未来进一步探索。

Abstract: How well do modern long-context language models understand literary fiction?
We explore this question via the task of literary evidence retrieval,
repurposing the RELiC dataset of That et al. (2022) to construct a benchmark
where the entire text of a primary source (e.g., The Great Gatsby) is provided
to an LLM alongside literary criticism with a missing quotation from that work.
This setting, in which the model must generate the missing quotation, mirrors
the human process of literary analysis by requiring models to perform both
global narrative reasoning and close textual examination. We curate a
high-quality subset of 292 examples through extensive filtering and human
verification. Our experiments show that recent reasoning models, such as Gemini
Pro 2.5 can exceed human expert performance (62.5% vs. 50% accuracy). In
contrast, the best open-weight model achieves only 29.1% accuracy, highlighting
a wide gap in interpretive reasoning between open and closed-weight models.
Despite their speed and apparent accuracy, even the strongest models struggle
with nuanced literary signals and overgeneration, signaling open challenges for
applying LLMs to literary analysis. We release our dataset and evaluation code
to encourage future work in this direction.

</details>


### [71] [An Exploratory Framework for Future SETI Applications: Detecting Generative Reactivity via Language Models](https://arxiv.org/abs/2506.02730)
*Po-Chieh Yu*

Main category: astro-ph.IM

Relevance: 70.0

TL;DR: 该论文探讨了噪声输入是否能引发语言模型的结构化响应，提出了一个评估框架，并通过实验发现某些非语义数据（如鲸鱼和鸟鸣）能比白噪声更有效地触发模型的结构化输出。


<details>
  <summary>Details</summary>
Motivation: 研究动机是探索语言模型是否能从噪声输入中检测潜在结构，而无需假设输入具有符号编码。这为传统SETI方法提供了一种补充思路。

Method: 方法包括使用四种声学输入（人类语音、鲸鱼叫声、鸟鸣和白噪声）测试GPT-2 small模型，并定义了一个综合评分SIP来评估反应性。

Result: 结果显示，鲸鱼和鸟鸣的SIP评分高于白噪声，人类语音反应中等，表明语言模型能检测非语义数据中的潜在结构。

Conclusion: 结论是这种方法可补充传统SETI方法，通过生成模型的反应性识别值得关注的数据。

Abstract: We present an exploratory framework to test whether noise-like input can
induce structured responses in language models. Instead of assuming that
extraterrestrial signals must be decoded, we evaluate whether inputs can
trigger linguistic behavior in generative systems. This shifts the focus from
decoding to viewing structured output as a sign of underlying regularity in the
input. We tested GPT-2 small, a 117M-parameter model trained on English text,
using four types of acoustic input: human speech, humpback whale vocalizations,
Phylloscopus trochilus birdsong, and algorithmically generated white noise. All
inputs were treated as noise-like, without any assumed symbolic encoding. To
assess reactivity, we defined a composite score called Semantic Induction
Potential (SIP), combining entropy, syntax coherence, compression gain, and
repetition penalty. Results showed that whale and bird vocalizations had higher
SIP scores than white noise, while human speech triggered only moderate
responses. This suggests that language models may detect latent structure even
in data without conventional semantics. We propose that this approach could
complement traditional SETI methods, especially in cases where communicative
intent is unknown. Generative reactivity may offer a different way to identify
data worth closer attention.

</details>


### [72] [Enhancing Paraphrase Type Generation: The Impact of DPO and RLHF Evaluated with Human-Ranked Data](https://arxiv.org/abs/2506.02018)
*Christopher Lee Lübbers*

Main category: cs.CL

Relevance: 65.0

TL;DR: 该论文提出了一种基于人类偏好数据的方法（DPO）来改进复述类型生成，提高了语义准确性和人类偏好评分，并创建了新数据集支持未来评估。


<details>
  <summary>Details</summary>
Motivation: 现有复述类型生成方法因依赖自动化指标和有限人工标注数据，导致与人类偏好不一致，影响语义保真度和语言转换质量。

Method: 利用人类排名的复述类型数据集，结合直接偏好优化（DPO）方法，直接对齐模型输出与人类判断。

Result: DPO训练使复述类型生成准确率提升3%，人类偏好评分提高7%。检测模型在特定复述类型上F1分数达0.91。

Conclusion: 偏好数据和DPO训练能生成更可靠、语义准确的复述，支持下游应用如摘要和问答，并为未来基于人类标准的评估奠定基础。

Abstract: Paraphrasing re-expresses meaning to enhance applications like text
simplification, machine translation, and question-answering. Specific
paraphrase types facilitate accurate semantic analysis and robust language
models. However, existing paraphrase-type generation methods often misalign
with human preferences due to reliance on automated metrics and limited
human-annotated training data, obscuring crucial aspects of semantic fidelity
and linguistic transformations.
  This study addresses this gap by leveraging a human-ranked paraphrase-type
dataset and integrating Direct Preference Optimization (DPO) to align model
outputs directly with human judgments. DPO-based training increases
paraphrase-type generation accuracy by 3 percentage points over a supervised
baseline and raises human preference ratings by 7 percentage points. A newly
created human-annotated dataset supports more rigorous future evaluations.
Additionally, a paraphrase-type detection model achieves F1 scores of 0.91 for
addition/deletion, 0.78 for same polarity substitution, and 0.70 for
punctuation changes.
  These findings demonstrate that preference data and DPO training produce more
reliable, semantically accurate paraphrases, enabling downstream applications
such as improved summarization and more robust question-answering. The PTD
model surpasses automated metrics and provides a more reliable framework for
evaluating paraphrase quality, advancing paraphrase-type research toward
richer, user-aligned language generation and establishing a stronger foundation
for future evaluations grounded in human-centric criteria.

</details>


### [73] [BabyLM's First Constructions: Causal interventions provide a signal of learning](https://arxiv.org/abs/2506.02147)
*Joshua Rozner,Leonie Weissweiler,Cory Shain*

Main category: cs.CL

Relevance: 60.0

TL;DR: 该论文研究了在训练数据量符合儿童语言学习发展的情况下，预训练语言模型（PLMs）是否能学习到构造（形式-意义配对）。结果表明，即使在发展合理的数据量下，模型也能表示多样化的构造，且构造表现与模型性能相关。


<details>
  <summary>Details</summary>
Motivation: 验证预训练语言模型在符合儿童语言学习数据量的情况下是否能学习构造，并探讨构造学习与模型性能的关系。

Method: 使用Rozner等人的方法，评估2024 BabyLM挑战赛中的模型在构造学习上的表现。

Result: 模型在发展合理的数据量下能表示多样化的构造，且构造表现与BabyLM基准测试性能相关。

Conclusion: 构造学习在预训练语言模型中具有功能性意义，且与模型性能相关。

Abstract: Construction grammar posits that children acquire constructions (form-meaning
pairings) from the statistics of their environment. Recent work supports this
hypothesis by showing sensitivity to constructions in pretrained language
models (PLMs), including one recent study (Rozner et al., 2025) demonstrating
that constructions shape the PLM's output distribution. However, models under
study have generally been trained on developmentally implausible amounts of
data, casting doubt on their relevance to human language learning. Here we use
Rozner et al.'s methods to evaluate constructional learning in models from the
2024 BabyLM challenge. Our results show that even when trained on
developmentally plausible quantities of data, models represent diverse
constructions, even hard cases that are superficially indistinguishable. We
further find correlational evidence that constructional performance may be
functionally relevant: models that better represent constructions perform
better on the BabyLM benchmarks.

</details>


### [74] [DIAMOND: An LLM-Driven Agent for Context-Aware Baseball Highlight Summarization](https://arxiv.org/abs/2506.02351)
*Jeonghun Kang,Soonmok Kwon,Joonseok Lee,Byung-Hak Kim*

Main category: cs.CL

Relevance: 60.0

TL;DR: DIAMOND是一个基于LLM的智能体，用于棒球比赛高光时刻的上下文感知摘要生成，结合了结构化体育分析和自然语言推理，显著提升了性能。


<details>
  <summary>Details</summary>
Motivation: 传统方法（如WPA排名或计算机视觉事件检测）缺乏战略深度和叙事价值，而人工标注成本高且不可扩展。DIAMOND旨在解决这些问题。

Method: DIAMOND结合了棒球统计特征（如Win Expectancy、WPA和Leverage Index）和LLM模块，通过量化比赛重要性和上下文叙事价值生成摘要。

Result: 在韩国棒球联盟的五场比赛中，DIAMOND的F1分数从42.9%（仅WPA）提升到84.8%，优于商业和统计基线。

Conclusion: DIAMOND展示了模块化、可解释的智能体框架在体育及其他领域事件级摘要中的潜力。

Abstract: Traditional approaches -- such as Win Probability Added (WPA)-based ranking
or computer vision-driven event detection -- can identify scoring plays but
often miss strategic depth, momentum shifts, and storyline progression. Manual
curation remains the gold standard but is resource-intensive and not scalable.
We introduce DIAMOND, an LLM-driven agent for context-aware baseball highlight
summarization that integrates structured sports analytics with natural language
reasoning. DIAMOND leverages sabermetric features -- Win Expectancy, WPA, and
Leverage Index -- to quantify play importance, while an LLM module enhances
selection based on contextual narrative value. This hybrid approach ensures
both quantitative rigor and qualitative richness, surpassing the limitations of
purely statistical or vision-based systems. Evaluated on five diverse Korean
Baseball Organization League games, DIAMOND improves F1-score from 42.9%
(WPA-only) to 84.8%, outperforming both commercial and statistical baselines.
Though limited in scale, our results highlight the potential of modular,
interpretable agent-based frameworks for event-level summarization in sports
and beyond.

</details>


### [75] [EssayBench: Evaluating Large Language Models in Multi-Genre Chinese Essay Writing](https://arxiv.org/abs/2506.02596)
*Fan Gao,Dongyuan Li,Ding Xia,Fei Mi,Yasheng Wang,Lifeng Shang,Baojun Wang*

Main category: cs.CL

Relevance: 60.0

TL;DR: 论文提出了一个针对中文写作的多类型基准测试（\benchName），旨在填补LLM在中文写作评估领域的空白，并分析了15个大型LLM的表现。


<details>
  <summary>Details</summary>
Motivation: 现有基准测试在中文写作评估中过于粗粒度，忽略了结构和修辞的复杂性，因此需要更精细的评估方法。

Method: 设计了包含728个真实世界提示的多类型基准测试（Argumentative, Narrative, Descriptive, Expository），并开发了细粒度的评分框架。

Result: 通过人类一致性研究验证了评估协议，并分析了15个LLM在不同类型和指令下的表现。

Conclusion: \benchName为中文写作评估提供了新工具，并推动了教育场景中LLM生成能力的改进研究。

Abstract: Chinese essay writing and its evaluation are critical in educational
contexts, yet the capabilities of Large Language Models (LLMs) in this domain
remain largely underexplored. Existing benchmarks often rely on coarse-grained
text quality metrics, largely overlooking the structural and rhetorical
complexities of Chinese essays, particularly across diverse genres. To address
this gap, we propose \benchName, a multi-genre benchmark specifically designed
for Chinese essay writing across four major genres: Argumentative, Narrative,
Descriptive, and Expository. We curate and refine a total of 728 real-world
prompts to ensure authenticity and meticulously categorize them into the
\textit{Open-Ended} and \textit{Constrained} sets to capture diverse writing
scenarios. To reliably evaluate generated essays, we develop a fine-grained,
genre-specific scoring framework that hierarchically aggregates scores. We
further validate our evaluation protocol through a comprehensive human
agreement study. Finally, we benchmark 15 large-sized LLMs, analyzing their
strengths and limitations across genres and instruction types. With \benchName,
we aim to advance LLM-based Chinese essay evaluation and inspire future
research on improving essay generation in educational settings.

</details>


### [76] [It's Not a Walk in the Park! Challenges of Idiom Translation in Speech-to-text Systems](https://arxiv.org/abs/2506.02995)
*Iuliia Zaitova,Badr M. Abdullah,Wei Xue,Dietrich Klakow,Bernd Möbius,Tania Avgustinova*

Main category: cs.CL

Relevance: 60.0

TL;DR: 论文系统评估了习语翻译在文本到文本和语音到文本翻译系统中的表现，发现语音到文本系统在习语翻译上表现较差，而大型语言模型表现较好。


<details>
  <summary>Details</summary>
Motivation: 习语翻译是机器翻译中的一大挑战，尤其在语音到文本系统中研究较少。本文旨在填补这一空白。

Method: 比较了多种翻译系统（包括语音到文本、文本到文本和大型语言模型）在习语翻译上的表现。

Result: 语音到文本系统在习语翻译上表现显著下降，而大型语言模型表现较好。

Conclusion: 需要针对习语设计特定策略并改进语音到文本系统的内部表示。

Abstract: Idioms are defined as a group of words with a figurative meaning not
deducible from their individual components. Although modern machine translation
systems have made remarkable progress, translating idioms remains a major
challenge, especially for speech-to-text systems, where research on this topic
is notably sparse. In this paper, we systematically evaluate idiom translation
as compared to conventional news translation in both text-to-text machine
translation (MT) and speech-to-text translation (SLT) systems across two
language pairs (German to English, Russian to English). We compare
state-of-the-art end-to-end SLT systems (SeamlessM4T SLT-to-text, Whisper Large
v3) with MT systems (SeamlessM4T SLT-to-text, No Language Left Behind), Large
Language Models (DeepSeek, LLaMA) and cascaded alternatives. Our results reveal
that SLT systems experience a pronounced performance drop on idiomatic data,
often reverting to literal translations even in higher layers, whereas MT
systems and Large Language Models demonstrate better handling of idioms. These
findings underscore the need for idiom-specific strategies and improved
internal representations in SLT architectures.

</details>


### [77] [Conditioning Large Language Models on Legal Systems? Detecting Punishable Hate Speech](https://arxiv.org/abs/2506.03009)
*Florian Ludwig,Torsten Zesch,Frederike Zufall*

Main category: cs.CL

Relevance: 60.0

TL;DR: 该论文研究了如何通过不同抽象层次的法律知识对大型语言模型（LLMs）进行条件化，以检测德国刑法规定的仇恨言论。结果显示，模型在法律专家评估仇恨言论方面仍存在显著差距。


<details>
  <summary>Details</summary>
Motivation: 探索LLMs在不同法律抽象层次上的表现，以评估其在法律问题中的应用潜力，特别是在仇恨言论检测任务中。

Method: 提出并比较了多种条件化LLMs的方法，分别基于抽象和具体的法律知识，用于分类社交媒体帖子是否构成德国刑法中的仇恨言论。

Result: 模型在仇恨言论分类任务中表现不佳，尤其是基于抽象法律知识的模型缺乏深度理解，而基于具体知识的模型在识别目标群体时表现较好，但在分类行为时仍有困难。

Conclusion: LLMs在法律问题中的应用仍需改进，特别是在任务理解和一致性方面。

Abstract: The assessment of legal problems requires the consideration of a specific
legal system and its levels of abstraction, from constitutional law to
statutory law to case law. The extent to which Large Language Models (LLMs)
internalize such legal systems is unknown. In this paper, we propose and
investigate different approaches to condition LLMs at different levels of
abstraction in legal systems. This paper examines different approaches to
conditioning LLMs at multiple levels of abstraction in legal systems to detect
potentially punishable hate speech. We focus on the task of classifying whether
a specific social media posts falls under the criminal offense of incitement to
hatred as prescribed by the German Criminal Code. The results show that there
is still a significant performance gap between models and legal experts in the
legal assessment of hate speech, regardless of the level of abstraction with
which the models were conditioned. Our analysis revealed, that models
conditioned on abstract legal knowledge lacked deep task understanding, often
contradicting themselves and hallucinating answers, while models using concrete
legal knowledge performed reasonably well in identifying relevant target
groups, but struggled with classifying target conducts.

</details>


### [78] [Leveraging Information Retrieval to Enhance Spoken Language Understanding Prompts in Few-Shot Learning](https://arxiv.org/abs/2506.03035)
*Pierre Lepagnol,Sahar Ghannay,Thomas Gerald,Christophe Servan,Sophie Rosset*

Main category: cs.CL

Relevance: 60.0

TL;DR: 论文提出了一种利用信息检索（IR）方法选择示例以增强提示的方法，用于提升少样本设置下的口语理解（SLU）任务性能。


<details>
  <summary>Details</summary>
Motivation: 当前SLU技术依赖大量训练数据，但特定任务或语言的标注数据有限。指令调优的大型语言模型（LLMs）在少样本设置下表现优异，但如何构建高效提示是关键。

Method: 通过信息检索（IR）方法选择示例，构建增强提示，应用于SLU任务。

Result: 实验表明，基于词汇的IR方法显著提升性能，且不增加提示长度。

Conclusion: IR方法可有效提升LLMs在少样本SLU任务中的表现。

Abstract: Understanding user queries is fundamental in many applications, such as home
assistants, booking systems, or recommendations. Accordingly, it is crucial to
develop accurate Spoken Language Understanding (SLU) approaches to ensure the
reliability of the considered system. Current State-of-the-Art SLU techniques
rely on large amounts of training data; however, only limited annotated
examples are available for specific tasks or languages.
  In the meantime, instruction-tuned large language models (LLMs) have shown
exceptional performance on unseen tasks in a few-shot setting when provided
with adequate prompts. In this work, we propose to explore example selection by
leveraging Information retrieval (IR) approaches to build an enhanced prompt
that is applied to an SLU task. We evaluate the effectiveness of the proposed
method on several SLU benchmarks. Experimental results show that lexical IR
methods significantly enhance performance without increasing prompt length.

</details>


### [79] [Research on Medical Named Entity Identification Based On Prompt-Biomrc Model and Its Application in Intelligent Consultation System](https://arxiv.org/abs/2506.01961)
*Jinzhu Yang*

Main category: cs.CL

Relevance: 40.0

TL;DR: 该研究提出Prompt-bioMRC模型，结合硬模板和软提示设计，提升医学领域命名实体识别（NER）的精度和效率。


<details>
  <summary>Details</summary>
Motivation: 探索提示学习方法在医学领域NER中的应用，以改进传统模型的性能。

Method: 集成硬模板和软提示设计，提出Prompt-bioMRC模型，并在多个医学数据集上进行实验。

Result: 实验表明，该方法在医学NER任务中优于传统模型。

Conclusion: 该方法为智能诊断系统等应用提供了可靠技术支持，推动了医学数据处理的自动化。

Abstract: This study is dedicated to exploring the application of prompt learning
methods to advance Named Entity Recognition (NER) within the medical domain. In
recent years, the emergence of large-scale models has driven significant
progress in NER tasks, particularly with the introduction of the BioBERT
language model, which has greatly enhanced NER capabilities in medical texts.
Our research introduces the Prompt-bioMRC model, which integrates both hard
template and soft prompt designs aimed at refining the precision and efficiency
of medical entity recognition. Through extensive experimentation across diverse
medical datasets, our findings consistently demonstrate that our approach
surpasses traditional models. This enhancement not only validates the efficacy
of our methodology but also highlights its potential to provide reliable
technological support for applications like intelligent diagnosis systems. By
leveraging advanced NER techniques, this study contributes to advancing
automated medical data processing, facilitating more accurate medical
information extraction, and supporting efficient healthcare decision-making
processes.

</details>


### [80] [Pruning for Performance: Efficient Idiom and Metaphor Classification in Low-Resource Konkani Using mBERT](https://arxiv.org/abs/2506.02005)
*Timothy Do,Pranav Saran,Harshita Poojary,Pranav Prabhu,Sean O'Brien,Vasu Sharma,Kevin Zhu*

Main category: cs.CL

Relevance: 40.0

TL;DR: 论文提出了一种结合mBERT、双向LSTM和线性分类器的混合模型，用于低资源语言（如Konkani）中的比喻语言分类，并通过注意力头剪枝提高效率。


<details>
  <summary>Details</summary>
Motivation: 解决低资源语言中比喻语言对NLP系统的挑战。

Method: 结合mBERT、双向LSTM和线性分类器，并使用梯度注意力头剪枝优化。

Result: 比喻分类准确率78%，习语分类准确率83%。

Conclusion: 注意力头剪枝对低资源语言NLP工具高效构建有效。

Abstract: In this paper, we address the persistent challenges that figurative language
expressions pose for natural language processing (NLP) systems, particularly in
low-resource languages such as Konkani. We present a hybrid model that
integrates a pre-trained Multilingual BERT (mBERT) with a bidirectional LSTM
and a linear classifier. This architecture is fine-tuned on a newly introduced
annotated dataset for metaphor classification, developed as part of this work.
To improve the model's efficiency, we implement a gradient-based attention head
pruning strategy. For metaphor classification, the pruned model achieves an
accuracy of 78%. We also applied our pruning approach to expand on an existing
idiom classification task, achieving 83% accuracy. These results demonstrate
the effectiveness of attention head pruning for building efficient NLP tools in
underrepresented languages.

</details>


### [81] [HENT-SRT: Hierarchical Efficient Neural Transducer with Self-Distillation for Joint Speech Recognition and Translation](https://arxiv.org/abs/2506.02157)
*Amir Hussein,Cihan Xiao,Matthew Wiesner,Dan Povey,Leibny Paola Garcia,Sanjeev Khudanpur*

Main category: cs.CL

Relevance: 40.0

TL;DR: 论文提出HENT-SRT框架，通过分层任务分解和自蒸馏技术改进神经转录器在语音翻译中的性能，同时提升计算效率。


<details>
  <summary>Details</summary>
Motivation: 解决神经转录器在语音翻译中存在的词序问题和性能下降，以及高计算成本。

Method: 采用分层任务分解、自蒸馏与CTC一致性正则化、高效编码器和解码器优化。

Result: 在阿拉伯语、西班牙语和普通话数据集上达到NT模型的最新性能，缩小与AED系统的差距。

Conclusion: HENT-SRT框架显著提升语音翻译性能并降低计算成本。

Abstract: Neural transducers (NT) provide an effective framework for speech streaming,
demonstrating strong performance in automatic speech recognition (ASR).
However, the application of NT to speech translation (ST) remains challenging,
as existing approaches struggle with word reordering and performance
degradation when jointly modeling ASR and ST, resulting in a gap with
attention-based encoder-decoder (AED) models. Existing NT-based ST approaches
also suffer from high computational training costs. To address these issues, we
propose HENT-SRT (Hierarchical Efficient Neural Transducer for Speech
Recognition and Translation), a novel framework that factorizes ASR and
translation tasks to better handle reordering. To ensure robust ST while
preserving ASR performance, we use self-distillation with CTC consistency
regularization. Moreover, we improve computational efficiency by incorporating
best practices from ASR transducers, including a down-sampled hierarchical
encoder, a stateless predictor, and a pruned transducer loss to reduce training
complexity. Finally, we introduce a blank penalty during decoding, reducing
deletions and improving translation quality. Our approach is evaluated on three
conversational datasets Arabic, Spanish, and Mandarin achieving new
state-of-the-art performance among NT models and substantially narrowing the
gap with AED-based systems.

</details>


### [82] [Different Speech Translation Models Encode and Translate Speaker Gender Differently](https://arxiv.org/abs/2506.02172)
*Dennis Fucci,Marco Gaido,Matteo Negri,Luisa Bentivogli,Andre Martins,Giuseppe Attanasio*

Main category: cs.CL

Relevance: 40.0

TL;DR: 研究探讨语音翻译模型中隐藏状态是否捕捉说话者性别特征，发现传统模型能捕捉性别信息，而新架构（通过适配器整合语音编码器和翻译系统）则不能，且性别编码能力低会导致翻译偏向男性。


<details>
  <summary>Details</summary>
Motivation: 探索语音翻译模型是否捕捉说话者性别特征及其对翻译性别分配的影响。

Method: 使用探测方法评估不同语音翻译模型中的性别编码能力，涵盖三种语言方向（英语-法语/意大利语/西班牙语）。

Result: 传统编码器-解码器模型能捕捉性别信息，新架构则不能；性别编码能力低会导致翻译偏向男性。

Conclusion: 新架构在性别编码方面表现较差，可能导致翻译中的性别偏见。

Abstract: Recent studies on interpreting the hidden states of speech models have shown
their ability to capture speaker-specific features, including gender. Does this
finding also hold for speech translation (ST) models? If so, what are the
implications for the speaker's gender assignment in translation? We address
these questions from an interpretability perspective, using probing methods to
assess gender encoding across diverse ST models. Results on three language
directions (English-French/Italian/Spanish) indicate that while traditional
encoder-decoder models capture gender information, newer architectures --
integrating a speech encoder with a machine translation system via adapters --
do not. We also demonstrate that low gender encoding capabilities result in
systems' tendency toward a masculine default, a translation bias that is more
pronounced in newer architectures.

</details>


### [83] [Echoes of Phonetics: Unveiling Relevant Acoustic Cues for ASR via Feature Attribution](https://arxiv.org/abs/2506.02181)
*Dennis Fucci,Marco Gaido,Matteo Negri,Mauro Cettolo,Luisa Bentivogli*

Main category: cs.CL

Relevance: 40.0

TL;DR: 该研究通过特征归因技术分析了现代Conformer-based ASR系统依赖的声学线索，发现模型对元音、摩擦音和爆破音的声学特性有不同关注点。


<details>
  <summary>Details</summary>
Motivation: 尽管ASR技术取得显著进展，但模型依赖的具体声学线索仍不明确，此前研究仅针对有限音素和过时模型。

Method: 应用特征归因技术，分析元音、摩擦音和爆破音的声学特性在时间和频率域中的表现。

Result: ASR模型更关注元音的全时间跨度（尤其是前两个共振峰）、摩擦音的频谱特性（尤其是咝音），以及爆破音的释放阶段（尤其是爆破特性）。

Conclusion: 研究提升了ASR模型的可解释性，并指出了未来研究的方向以揭示模型鲁棒性潜在不足。

Abstract: Despite significant advances in ASR, the specific acoustic cues models rely
on remain unclear. Prior studies have examined such cues on a limited set of
phonemes and outdated models. In this work, we apply a feature attribution
technique to identify the relevant acoustic cues for a modern Conformer-based
ASR system. By analyzing plosives, fricatives, and vowels, we assess how
feature attributions align with their acoustic properties in the time and
frequency domains, also essential for human speech perception. Our findings
show that the ASR model relies on vowels' full time spans, particularly their
first two formants, with greater saliency in male speech. It also better
captures the spectral characteristics of sibilant fricatives than non-sibilants
and prioritizes the release phase in plosives, especially burst
characteristics. These insights enhance the interpretability of ASR models and
highlight areas for future research to uncover potential gaps in model
robustness.

</details>


### [84] [Leveraging Natural Language Processing to Unravel the Mystery of Life: A Review of NLP Approaches in Genomics, Transcriptomics, and Proteomics](https://arxiv.org/abs/2506.02212)
*Ella Rannon,David Burstein*

Main category: cs.CL

Relevance: 40.0

TL;DR: 该论文综述了NLP方法在生物序列数据（如DNA、RNA、蛋白质）中的应用，探讨了从经典方法到先进模型（如Transformer）的适应性，并评估了其在不同生物任务中的效果。


<details>
  <summary>Details</summary>
Motivation: 探索NLP技术在生物信息学中的应用潜力，以推动对生物过程的理解。

Method: 综述了NLP方法（如word2vec、Transformer、hyena算子）在生物序列分析中的适应性，并讨论了模型架构和分词策略。

Result: 展示了NLP方法在结构预测、基因表达和进化分析等生物任务中的潜力。

Conclusion: NLP与生物信息学的结合有望从大规模基因组数据中提取有意义的见解。

Abstract: Natural Language Processing (NLP) has transformed various fields beyond
linguistics by applying techniques originally developed for human language to
the analysis of biological sequences. This review explores the application of
NLP methods to biological sequence data, focusing on genomics, transcriptomics,
and proteomics. We examine how various NLP methods, from classic approaches
like word2vec to advanced models employing transformers and hyena operators,
are being adapted to analyze DNA, RNA, protein sequences, and entire genomes.
The review also examines tokenization strategies and model architectures,
evaluating their strengths, limitations, and suitability for different
biological tasks. We further cover recent advances in NLP applications for
biological data, such as structure prediction, gene expression, and
evolutionary analysis, highlighting the potential of these methods for
extracting meaningful insights from large-scale genomic data. As language
models continue to advance, their integration into bioinformatics holds immense
promise for advancing our understanding of biological processes in all domains
of life.

</details>


### [85] [Quantifying Misattribution Unfairness in Authorship Attribution](https://arxiv.org/abs/2506.02321)
*Pegah Alipoormolabashi,Ajay Patel,Niranjan Balasubramanian*

Main category: cs.CL

Relevance: 40.0

TL;DR: 论文提出了一种衡量作者归属误判不公平性的指标MAUIk，并发现现有模型在误判风险上存在不公平性，尤其是对某些作者风险更高。


<details>
  <summary>Details</summary>
Motivation: 研究作者归属误判的公平性问题，因为现有评估标准未考虑误判风险的不公平性。

Method: 引入Misattribution Unfairness Index (MAUIk)指标，评估五种模型在两个数据集上的误判不公平性。

Result: 所有模型均表现出高不公平性，某些作者误判风险更高，且与模型在潜在搜索空间中的嵌入方式相关。

Conclusion: 需在模型构建和使用中明确误判风险，以减少潜在危害。

Abstract: Authorship misattribution can have profound consequences in real life. In
forensic settings simply being considered as one of the potential authors of an
evidential piece of text or communication can result in undesirable scrutiny.
This raises a fairness question: Is every author in the candidate pool at equal
risk of misattribution? Standard evaluation measures for authorship attribution
systems do not explicitly account for this notion of fairness. We introduce a
simple measure, Misattribution Unfairness Index (MAUIk), which is based on how
often authors are ranked in the top k for texts they did not write. Using this
measure we quantify the unfairness of five models on two different datasets.
All models exhibit high levels of unfairness with increased risks for some
authors. Furthermore, we find that this unfairness relates to how the models
embed the authors as vectors in the latent search space. In particular, we
observe that the risk of misattribution is higher for authors closer to the
centroid (or center) of the embedded authors in the haystack. These results
indicate the potential for harm and the need for communicating with and
calibrating end users on misattribution risk when building and providing such
models for downstream use.

</details>


### [86] [STORYTELLER: An Enhanced Plot-Planning Framework for Coherent and Cohesive Story Generation](https://arxiv.org/abs/2506.02347)
*Jiaming Li,Yukun Chen,Ziqiang Liu,Minghuan Tan,Lei Zhang,Yunshui Li,Run Luo,Longze Chen,Jing Luo,Ahmadreza Argha,Hamid Alinejad-Rokny,Wei Zhou,Min Yang*

Main category: cs.CL

Relevance: 40.0

TL;DR: Storyteller是一种基于SV0三元组的自动故事生成方法，通过动态模块提升叙事连贯性和逻辑一致性，显著优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有自动故事生成方法在叙事连贯性和逻辑一致性上表现不佳，影响了故事体验，需要改进。

Method: 引入基于SV0三元组的剧情节点结构，结合动态模块STORYLINE和NEKG，持续优化故事生成过程。

Result: 在人类偏好评估中平均胜率为84.33%，在创造力、连贯性、参与度和相关性方面表现优异。

Conclusion: Storyteller通过系统化方法显著提升了自动故事生成的质量。

Abstract: Stories are central to human culture, serving to share ideas, preserve
traditions, and foster connections. Automatic story generation, a key
advancement in artificial intelligence (AI), offers new possibilities for
creating personalized content, exploring creative ideas, and enhancing
interactive experiences. However, existing methods struggle to maintain
narrative coherence and logical consistency. This disconnect compromises the
overall storytelling experience, underscoring the need for substantial
improvements. Inspired by human cognitive processes, we introduce Storyteller,
a novel approach that systemically improves the coherence and consistency of
automatically generated stories. Storyteller introduces a plot node structure
based on linguistically grounded subject verb object (SVO) triplets, which
capture essential story events and ensure a consistent logical flow. Unlike
previous methods, Storyteller integrates two dynamic modules, the STORYLINE and
narrative entity knowledge graph (NEKG),that continuously interact with the
story generation process. This integration produces structurally sound,
cohesive and immersive narratives. Extensive experiments demonstrate that
Storyteller significantly outperforms existing approaches, achieving an 84.33%
average win rate through human preference evaluation. At the same time, it is
also far ahead in other aspects including creativity, coherence, engagement,
and relevance.

</details>


### [87] [SingaKids: A Multilingual Multimodal Dialogic Tutor for Language Learning](https://arxiv.org/abs/2506.02412)
*Zhengyuan Liu,Geyu Lin,Hui Li Tan,Huayun Zhang,Yanfeng Lu,Xiaoxue Gao,Stella Xin Yin,He Sun,Hock Huan Goh,Lung Hsiang Wong,Nancy F. Chen*

Main category: cs.CL

Relevance: 40.0

TL;DR: SingaKids是一个多语言对话式语言学习系统，通过图片描述任务促进语言学习，结合密集图像字幕、多语言对话交互和语音技术。


<details>
  <summary>Details</summary>
Motivation: 解决生成式AI在教育应用中跨语言和文化背景的鲁棒性问题，同时满足儿童友好的设计需求。

Method: 集成密集图像字幕、多语言对话交互、语音理解和生成，并通过多语言预训练、任务特定调优和支架优化改进系统。

Result: 实证研究表明，SingaKids对不同水平的学习者均有效。

Conclusion: SingaKids为多语言儿童语言学习提供了有效的对话式教学工具。

Abstract: The integration of generative artificial intelligence into educational
applications has enhanced personalized and interactive learning experiences,
and it shows strong potential to promote young learners language acquisition.
However, it is still challenging to ensure consistent and robust performance
across different languages and cultural contexts, and kids-friendly design
requires simplified instructions, engaging interactions, and age-appropriate
scaffolding to maintain motivation and optimize learning outcomes. In this
work, we introduce SingaKids, a dialogic tutor designed to facilitate language
learning through picture description tasks. Our system integrates dense image
captioning, multilingual dialogic interaction, speech understanding, and
engaging speech generation to create an immersive learning environment in four
languages: English, Mandarin, Malay, and Tamil. We further improve the system
through multilingual pre-training, task-specific tuning, and scaffolding
optimization. Empirical studies with elementary school students demonstrate
that SingaKids provides effective dialogic teaching, benefiting learners at
different performance levels.

</details>


### [88] [Multilingual Information Retrieval with a Monolingual Knowledge Base](https://arxiv.org/abs/2506.02527)
*Yingying Zhuang,Aman Gupta,Anurag Beniwal*

Main category: cs.CL

Relevance: 40.0

TL;DR: 论文提出了一种通过加权采样微调多语言嵌入模型的新策略，用于跨语言信息检索，性能提升显著。


<details>
  <summary>Details</summary>
Motivation: 高质知识库资源稀缺且语言有限，需有效嵌入模型实现跨语言知识共享。

Method: 采用加权采样的对比学习方法微调多语言嵌入模型。

Result: 加权采样策略在MRR和Recall@3上分别提升31.03%和33.98%。

Conclusion: 方法语言无关，适用于多语言和代码混合场景。

Abstract: Multilingual information retrieval has emerged as powerful tools for
expanding knowledge sharing across languages. On the other hand, resources on
high quality knowledge base are often scarce and in limited languages,
therefore an effective embedding model to transform sentences from different
languages into a feature vector space same as the knowledge base language
becomes the key ingredient for cross language knowledge sharing, especially to
transfer knowledge available in high-resource languages to low-resource ones.
In this paper we propose a novel strategy to fine-tune multilingual embedding
models with weighted sampling for contrastive learning, enabling multilingual
information retrieval with a monolingual knowledge base. We demonstrate that
the weighted sampling strategy produces performance gains compared to standard
ones by up to 31.03\% in MRR and up to 33.98\% in Recall@3. Additionally, our
proposed methodology is language agnostic and applicable for both multilingual
and code switching use cases.

</details>


### [89] [Prosodic Structure Beyond Lexical Content: A Study of Self-Supervised Learning](https://arxiv.org/abs/2506.02584)
*Sarenne Wallbridge,Christoph Minixhofer,Catherine Lai,Peter Bell*

Main category: cs.CL

Relevance: 40.0

TL;DR: 论文研究了韵律（如语调、节奏和响度）在独立于词汇内容时对文本结构预测的作用，利用自监督学习（SSL）分析其时间粒度。


<details>
  <summary>Details</summary>
Motivation: 探索韵律在文本理解中的独立作用，填补词汇内容之外的预测性结构研究空白。

Method: 提出Masked Prosody Model，通过自监督学习分析韵律的时间粒度，并进行探测实验比较不同感知标签的表现。

Result: 模型在涉及长期结构（如情感识别）的标签上表现最佳，显著优于传统特征（如音高、能量）。

Conclusion: SSL训练目标的时间尺度至关重要，复杂SSL编码结构比传统结构更具价值。

Abstract: People exploit the predictability of lexical structures during text
comprehension. Though predictable structure is also present in speech, the
degree to which prosody, e.g. intonation, tempo, and loudness, contributes to
such structure independently of the lexical content is unclear. This study
leverages self-supervised learning (SSL) to examine the temporal granularity of
structures in the acoustic correlates of prosody. Representations from our
proposed Masked Prosody Model can predict perceptual labels dependent on local
information, such as word boundaries, but provide the most value for labels
involving longer-term structures, like emotion recognition. Probing experiments
across various perceptual labels show strong relative gains over untransformed
pitch, energy, and voice activity features. Our results reveal the importance
of SSL training objective timescale and highlight the value of complex
SSL-encoded structures compared to more constrained classical structures.

</details>


### [90] [Evaluating Named Entity Recognition Models for Russian Cultural News Texts: From BERT to LLM](https://arxiv.org/abs/2506.02589)
*Maria Levchenko*

Main category: cs.CL

Relevance: 40.0

TL;DR: 该论文研究了在俄罗斯文化事件新闻文本中的人名命名实体识别（NER），比较了多种模型（包括Transformer和LLMs），发现GPT-4o在特定提示下表现最佳（F1=0.93），GPT-4精度最高（0.99）。后续评估显示GPT-4.1进一步提升了性能（F1=0.94）。


<details>
  <summary>Details</summary>
Motivation: 研究在俄语文化事件新闻文本中的人名NER，填补了形态丰富语言和特定领域的研究空白。

Method: 使用SPbLitGuide数据集，比较了多种NER模型（如DeepPavlov、RoBERTa、SpaCy、GPT-3.5、GPT-4、GPT-4o），并评估其性能。

Result: GPT-4o在JSON输出提示下表现最佳（F1=0.93），GPT-4精度最高（0.99）。GPT-4.1进一步提升了性能（F1=0.94）。

Conclusion: 研究揭示了当前NER模型在俄语文化领域的优势和局限，为研究人员和实践者提供了参考。

Abstract: This paper addresses the challenge of Named Entity Recognition (NER) for
person names within the specialized domain of Russian news texts concerning
cultural events. The study utilizes the unique SPbLitGuide dataset, a
collection of event announcements from Saint Petersburg spanning 1999 to 2019.
A comparative evaluation of diverse NER models is presented, encompassing
established transformer-based architectures such as DeepPavlov, RoBERTa, and
SpaCy, alongside recent Large Language Models (LLMs) including GPT-3.5, GPT-4,
and GPT-4o. Key findings highlight the superior performance of GPT-4o when
provided with specific prompting for JSON output, achieving an F1 score of
0.93. Furthermore, GPT-4 demonstrated the highest precision at 0.99. The
research contributes to a deeper understanding of current NER model
capabilities and limitations when applied to morphologically rich languages
like Russian within the cultural heritage domain, offering insights for
researchers and practitioners. Follow-up evaluation with GPT-4.1 (April 2025)
achieves F1=0.94 for both simple and structured prompts, demonstrating rapid
progress across model families and simplified deployment requirements.

</details>


### [91] [Overcoming Data Scarcity in Multi-Dialectal Arabic ASR via Whisper Fine-Tuning](https://arxiv.org/abs/2506.02627)
*Ömer Tarik Özyilmaz,Matt Coler,Matias Valdenegro-Toro*

Main category: cs.CL

Relevance: 40.0

TL;DR: 研究了Whisper模型在五种阿拉伯方言上的微调效果，发现少量MSA数据微调即可显著提升小模型性能，但MSA预训练效果有限。方言池化模型表现接近方言专用模型。


<details>
  <summary>Details</summary>
Motivation: 解决商用阿拉伯语ASR系统对方言语音识别效果不佳的问题。

Method: 使用Mozilla Common Voice和MASC数据集，对Whisper模型进行微调，评估MSA数据量和预训练的影响，比较方言专用与池化模型。

Result: 少量MSA数据微调显著提升小模型性能；MSA预训练效果有限；方言池化模型表现接近方言专用模型。

Conclusion: 方言池化模型可缓解低资源ASR数据稀缺问题，且性能损失小。

Abstract: Although commercial Arabic automatic speech recognition (ASR) systems support
Modern Standard Arabic (MSA), they struggle with dialectal speech. We
investigate the effect of fine-tuning OpenAI's Whisper on five major Arabic
dialects (Gulf, Levantine, Iraqi, Egyptian, Maghrebi) using Mozilla Common
Voice for MSA and the MASC dataset for dialectal speech. We evaluate MSA
training size effects, benefits of pre-training on MSA data, and
dialect-specific versus dialect-pooled models. We find that small amounts of
MSA fine-tuning data yield substantial improvements for smaller models,
matching larger non-fine-tuned models. While MSA pre-training shows minimal
benefit, suggesting limited shared features between MSA and dialects, our
dialect-pooled models perform comparably to dialect-specific ones. This
indicates that pooling dialectal data, when properly balanced, can help address
data scarcity in low-resource ASR without significant performance loss.

</details>


### [92] [Stereotypical gender actions can be extracted from Web text](https://arxiv.org/abs/2506.02740)
*Amaç Herdağdelen,Marco Baroni*

Main category: cs.CL

Relevance: 40.0

TL;DR: 研究通过文本和Twitter数据提取性别特定行为，并与刻板印象对比，利用OMCS和性别启发式方法计算性别偏见，验证了方法的可行性。


<details>
  <summary>Details</summary>
Motivation: 探索如何利用自然文本数据（尤其是Twitter）增强常识知识库中的性别刻板印象信息。

Method: 使用OMCS和Twitter用户性别信息，结合启发式方法计算性别偏见，并与人类标注标准对比。

Result: Spearman相关性为0.47，ROC曲线下面积为0.76，验证了方法的有效性。

Conclusion: 自然文本可用于增强常识知识库中的性别刻板印象信息，并提供了两个数据集。

Abstract: We extracted gender-specific actions from text corpora and Twitter, and
compared them to stereotypical expectations of people. We used Open Mind Common
Sense (OMCS), a commonsense knowledge repository, to focus on actions that are
pertinent to common sense and daily life of humans. We use the gender
information of Twitter users and Web-corpus-based pronoun/name gender
heuristics to compute the gender bias of the actions. With high recall, we
obtained a Spearman correlation of 0.47 between corpus-based predictions and a
human gold standard, and an area under the ROC curve of 0.76 when predicting
the polarity of the gold standard. We conclude that it is feasible to use
natural text (and a Twitter-derived corpus in particular) in order to augment
commonsense repositories with the stereotypical gender expectations of actions.
We also present a dataset of 441 commonsense actions with human judges' ratings
on whether the action is typically/slightly masculine/feminine (or neutral),
and another larger dataset of 21,442 actions automatically rated by the methods
we investigate in this study.

</details>


### [93] [Multi-task Learning with Active Learning for Arabic Offensive Speech Detection](https://arxiv.org/abs/2506.02753)
*Aisha Alansari,Hamzah Luqman*

Main category: cs.CL

Relevance: 40.0

TL;DR: 论文提出了一种结合多任务学习和主动学习的新框架，用于提升阿拉伯社交媒体文本中的攻击性言论检测，通过动态调整任务权重和主动学习策略，显著提高了检测准确率。


<details>
  <summary>Details</summary>
Motivation: 社交媒体中攻击性言论的快速传播带来了社会和网络安全问题，而阿拉伯语文本的复杂性、方言多样性和标注数据稀缺性增加了检测难度。

Method: 整合多任务学习和主动学习，动态调整任务权重，采用不确定性采样选择信息量大的样本，并引入加权表情处理。

Result: 在OSACT2022数据集上实现了85.42%的宏F1分数，优于现有方法且使用更少的微调样本。

Conclusion: 多任务学习与主动学习的结合在资源受限环境下能高效准确地检测攻击性语言。

Abstract: The rapid growth of social media has amplified the spread of offensive,
violent, and vulgar speech, which poses serious societal and cybersecurity
concerns. Detecting such content in Arabic text is particularly complex due to
limited labeled data, dialectal variations, and the language's inherent
complexity. This paper proposes a novel framework that integrates multi-task
learning (MTL) with active learning to enhance offensive speech detection in
Arabic social media text. By jointly training on two auxiliary tasks, violent
and vulgar speech, the model leverages shared representations to improve the
detection accuracy of the offensive speech. Our approach dynamically adjusts
task weights during training to balance the contribution of each task and
optimize performance. To address the scarcity of labeled data, we employ an
active learning strategy through several uncertainty sampling techniques to
iteratively select the most informative samples for model training. We also
introduce weighted emoji handling to better capture semantic cues. Experimental
results on the OSACT2022 dataset show that the proposed framework achieves a
state-of-the-art macro F1-score of 85.42%, outperforming existing methods while
using significantly fewer fine-tuning samples. The findings of this study
highlight the potential of integrating MTL with active learning for efficient
and accurate offensive language detection in resource-constrained settings.

</details>


### [94] [Exploiting the English Vocabulary Profile for L2 word-level vocabulary assessment with LLMs](https://arxiv.org/abs/2506.02758)
*Stefano Bannò,Kate Knill,Mark Gales*

Main category: cs.CL

Relevance: 40.0

TL;DR: 论文提出了一种利用大语言模型（LLMs）和英语词汇档案（EVP）进行细粒度词汇评估的新方法，用于评估第二语言学习者的词汇使用水平。


<details>
  <summary>Details</summary>
Motivation: 传统词汇评估方法通常关注上下文无关的词性使用，无法捕捉词汇在句子中的精确用法。本文旨在通过结合LLMs和EVP，实现更精细的词汇评估。

Method: 结合LLMs和EVP，评估LLMs在L2学习者写作中为单词分配熟练度水平的能力，解决多义性、上下文变化和多词表达等挑战。

Result: LLMs能够利用额外的语义信息，表现优于基于词性的基线方法，并探索了单词级熟练度与作文级熟练度的相关性。

Conclusion: LLMs非常适合词汇评估任务，且EVP的熟练度水平具有一致性。

Abstract: Vocabulary use is a fundamental aspect of second language (L2) proficiency.
To date, its assessment by automated systems has typically examined the
context-independent, or part-of-speech (PoS) related use of words. This paper
introduces a novel approach to enable fine-grained vocabulary evaluation
exploiting the precise use of words within a sentence. The scheme combines
large language models (LLMs) with the English Vocabulary Profile (EVP). The EVP
is a standard lexical resource that enables in-context vocabulary use to be
linked with proficiency level. We evaluate the ability of LLMs to assign
proficiency levels to individual words as they appear in L2 learner writing,
addressing key challenges such as polysemy, contextual variation, and
multi-word expressions. We compare LLMs to a PoS-based baseline. LLMs appear to
exploit additional semantic information that yields improved performance. We
also explore correlations between word-level proficiency and essay-level
proficiency. Finally, the approach is applied to examine the consistency of the
EVP proficiency levels. Results show that LLMs are well-suited for the task of
vocabulary assessment.

</details>


### [95] [Token and Span Classification for Entity Recognition in French Historical Encyclopedias](https://arxiv.org/abs/2506.02872)
*Ludovic Moncla,Hédi Zeghidi*

Main category: cs.CL

Relevance: 40.0

TL;DR: 论文研究了历史文本中的命名实体识别（NER），比较了多种方法，包括传统模型和基于Transformer的架构，并探讨了生成模型在低资源场景下的潜力。


<details>
  <summary>Details</summary>
Motivation: 历史文本中的NER面临语言非标准化、拼写古老和实体嵌套等独特挑战，需要探索更有效的解决方案。

Method: 在GeoEDdA数据集上测试了多种NER方法，包括CRF、spaCy、CamemBERT和Flair，并提出了基于标记和跨度的分类框架。还评估了生成模型的少样本提示能力。

Result: Transformer模型在嵌套实体上表现最佳，而生成模型在低资源情况下显示出潜力。

Conclusion: 历史NER仍具挑战性，未来可结合符号和神经方法以更好地处理早期现代法语文本的复杂性。

Abstract: Named Entity Recognition (NER) in historical texts presents unique challenges
due to non-standardized language, archaic orthography, and nested or
overlapping entities. This study benchmarks a diverse set of NER approaches,
ranging from classical Conditional Random Fields (CRFs) and spaCy-based models
to transformer-based architectures such as CamemBERT and sequence-labeling
models like Flair. Experiments are conducted on the GeoEDdA dataset, a richly
annotated corpus derived from 18th-century French encyclopedias. We propose
framing NER as both token-level and span-level classification to accommodate
complex nested entity structures typical of historical documents. Additionally,
we evaluate the emerging potential of few-shot prompting with generative
language models for low-resource scenarios. Our results demonstrate that while
transformer-based models achieve state-of-the-art performance, especially on
nested entities, generative models offer promising alternatives when labeled
data are scarce. The study highlights ongoing challenges in historical NER and
suggests avenues for hybrid approaches combining symbolic and neural methods to
better capture the intricacies of early modern French text.

</details>


### [96] [IMPARA-GED: Grammatical Error Detection is Boosting Reference-free Grammatical Error Quality Estimator](https://arxiv.org/abs/2506.02899)
*Yusuke Sakai,Takumi Goto,Taro Watanabe*

Main category: cs.CL

Relevance: 40.0

TL;DR: IMPARA-GED是一种无需参考的自动语法错误修正（GEC）评估方法，具备语法错误检测（GED）能力。


<details>
  <summary>Details</summary>
Motivation: 改进现有的自动GEC评估方法IMPARA，通过增强GED能力提升评估质量。

Method: 利用预训练语言模型构建IMPARA-GED的质量估计器，增强其GED能力。

Result: 在SEEDA数据集上，IMPARA-GED与人工句子级评估的相关性最高。

Conclusion: IMPARA-GED在自动GEC评估中表现出色，尤其在GED能力上。

Abstract: We propose IMPARA-GED, a novel reference-free automatic grammatical error
correction (GEC) evaluation method with grammatical error detection (GED)
capabilities. We focus on the quality estimator of IMPARA, an existing
automatic GEC evaluation method, and construct that of IMPARA-GED using a
pre-trained language model with enhanced GED capabilities. Experimental results
on SEEDA, a meta-evaluation dataset for automatic GEC evaluation methods,
demonstrate that IMPARA-GED achieves the highest correlation with human
sentence-level evaluations.

</details>


### [97] [Cell-o1: Training LLMs to Solve Single-Cell Reasoning Puzzles with Reinforcement Learning](https://arxiv.org/abs/2506.02911)
*Yin Fang,Qiao Jin,Guangzhi Xiong,Bowen Jin,Xianrui Zhong,Siru Ouyang,Aidong Zhang,Jiawei Han,Zhiyong Lu*

Main category: cs.CL

Relevance: 40.0

TL;DR: 论文提出CellPuzzles任务，通过模仿专家标注细胞类型的方式，利用LLM（Cell-o1）实现批量细胞标注，显著优于现有基线。


<details>
  <summary>Details</summary>
Motivation: 现有基础模型在单细胞RNA测序数据标注中缺乏批量上下文考虑和解释性推理，而人类专家则能基于领域知识进行标注。

Method: 提出CellPuzzles任务，训练7B参数的LLM（Cell-o1），结合监督微调和批量级奖励的强化学习。

Result: Cell-o1在批量标注任务中表现优异，比基线模型（OpenAI o1）提升73%，并展现出专家级推理能力。

Conclusion: Cell-o1为批量细胞标注提供了高效解决方案，并揭示了训练动态和推理行为的洞察。

Abstract: Cell type annotation is a key task in analyzing the heterogeneity of
single-cell RNA sequencing data. Although recent foundation models automate
this process, they typically annotate cells independently, without considering
batch-level cellular context or providing explanatory reasoning. In contrast,
human experts often annotate distinct cell types for different cell clusters
based on their domain knowledge. To mimic this workflow, we introduce the
CellPuzzles task, where the objective is to assign unique cell types to a batch
of cells. This benchmark spans diverse tissues, diseases, and donor conditions,
and requires reasoning across the batch-level cellular context to ensure label
uniqueness. We find that off-the-shelf large language models (LLMs) struggle on
CellPuzzles, with the best baseline (OpenAI's o1) achieving only 19.0%
batch-level accuracy. To fill this gap, we propose Cell-o1, a 7B LLM trained
via supervised fine-tuning on distilled reasoning traces, followed by
reinforcement learning with batch-level rewards. Cell-o1 achieves
state-of-the-art performance, outperforming o1 by over 73% and generalizing
well across contexts. Further analysis of training dynamics and reasoning
behaviors provides insights into batch-level annotation performance and
emergent expert-like reasoning. Code and data are available at
https://github.com/ncbi-nlp/cell-o1.

</details>


### [98] [INESC-ID @ eRisk 2025: Exploring Fine-Tuned, Similarity-Based, and Prompt-Based Approaches to Depression Symptom Identification](https://arxiv.org/abs/2506.02924)
*Diogo A. P. Nunes,Eugénio Ribeiro*

Main category: cs.CL

Relevance: 40.0

TL;DR: 论文描述了团队在eRisk 2025任务1中的方法，通过微调基础模型和合成数据缓解类别不平衡，最终在信息检索评估中表现最佳。


<details>
  <summary>Details</summary>
Motivation: 解决BDI问卷中抑郁症状相关句子的分类和排序问题，优化信息检索性能。

Method: 采用基础模型微调、句子相似度、LLM提示和集成技术，并通过合成数据缓解类别不平衡。

Result: 微调基础模型表现最佳，团队提交的五个独立测试运行在官方评估中击败了其他16个团队。

Conclusion: 基础模型微调结合合成数据是解决此类任务的有效方法，且不同症状需采用不同策略。

Abstract: In this work, we describe our team's approach to eRisk's 2025 Task 1: Search
for Symptoms of Depression. Given a set of sentences and the Beck's Depression
Inventory - II (BDI) questionnaire, participants were tasked with submitting up
to 1,000 sentences per depression symptom in the BDI, sorted by relevance.
Participant submissions were evaluated according to standard Information
Retrieval (IR) metrics, including Average Precision (AP) and R-Precision
(R-PREC). The provided training data, however, consisted of sentences labeled
as to whether a given sentence was relevant or not w.r.t. one of BDI's
symptoms. Due to this labeling limitation, we framed our development as a
binary classification task for each BDI symptom, and evaluated accordingly. To
that end, we split the available labeled data into training and validation
sets, and explored foundation model fine-tuning, sentence similarity, Large
Language Model (LLM) prompting, and ensemble techniques. The validation results
revealed that fine-tuning foundation models yielded the best performance,
particularly when enhanced with synthetic data to mitigate class imbalance. We
also observed that the optimal approach varied by symptom. Based on these
insights, we devised five independent test runs, two of which used ensemble
methods. These runs achieved the highest scores in the official IR evaluation,
outperforming submissions from 16 other teams.

</details>


### [99] [Towards a Japanese Full-duplex Spoken Dialogue System](https://arxiv.org/abs/2506.02979)
*Atsumoto Ohashi,Shinya Iizuka,Jingjing Jiang,Ryuichiro Higashinaka*

Main category: cs.CL

Relevance: 40.0

TL;DR: 本文提出了首个公开可用的日语全双工口语对话模型，基于英文模型Moshi，通过两阶段训练（预训练和微调）及合成数据增强，在自然性和意义性上优于基线模型。


<details>
  <summary>Details</summary>
Motivation: 针对日语全双工口语对话系统研究不足的问题，填补日语领域的空白。

Method: 两阶段训练：预训练大规模日语口语数据，微调高质量立体声对话数据，并引入多流文本转语音系统生成的合成数据。

Result: 模型在自然性和意义性上优于日语基线模型。

Conclusion: 提出的模型为日语全双工口语对话系统提供了有效解决方案。

Abstract: Full-duplex spoken dialogue systems, which can model simultaneous
bidirectional features of human conversations such as speech overlaps and
backchannels, have attracted significant attention recently. However, the study
of full-duplex spoken dialogue systems for the Japanese language has been
limited, and the research on their development in Japanese remains scarce. In
this paper, we present the first publicly available full-duplex spoken dialogue
model in Japanese, which is built upon Moshi, a full-duplex dialogue model in
English. Our model is trained through a two-stage process: pre-training on a
large-scale spoken dialogue data in Japanese, followed by fine-tuning on
high-quality stereo spoken dialogue data. We further enhance the model's
performance by incorporating synthetic dialogue data generated by a
multi-stream text-to-speech system. Evaluation experiments demonstrate that the
trained model outperforms Japanese baseline models in both naturalness and
meaningfulness.

</details>


### [100] [A Multi-Agent Framework for Mitigating Dialect Biases in Privacy Policy Question-Answering Systems](https://arxiv.org/abs/2506.02998)
*Đorđe Klisura,Astrid R Bernaga Torres,Anna Karen Gárate-Escamilla,Rajesh Roshan Biswal,Ke Yang,Hilal Pataci,Anthony Rios*

Main category: cs.CL

Relevance: 40.0

TL;DR: 提出了一种多智能体框架，通过结合方言代理和隐私政策代理，减少隐私政策问答系统中的方言偏见，无需重新训练或方言特定微调。


<details>
  <summary>Details</summary>
Motivation: 隐私政策的复杂性限制了不同人群的可访问性，现有系统在英语方言间存在性能差异，影响了非标准方言使用者的体验。

Method: 采用多智能体框架，包括方言代理（将查询转换为标准英语）和隐私政策代理（利用领域知识优化预测）。

Result: 在PrivacyQA和PolicyQA数据集上，框架将GPT-4o-mini的零样本准确率分别从0.394提升至0.601和从0.352提升至0.464。

Conclusion: 结构化智能体协作有效减少方言偏见，设计考虑语言多样性的NLP系统对隐私信息公平访问至关重要。

Abstract: Privacy policies inform users about data collection and usage, yet their
complexity limits accessibility for diverse populations. Existing Privacy
Policy Question Answering (QA) systems exhibit performance disparities across
English dialects, disadvantaging speakers of non-standard varieties. We propose
a novel multi-agent framework inspired by human-centered design principles to
mitigate dialectal biases. Our approach integrates a Dialect Agent, which
translates queries into Standard American English (SAE) while preserving
dialectal intent, and a Privacy Policy Agent, which refines predictions using
domain expertise. Unlike prior approaches, our method does not require
retraining or dialect-specific fine-tuning, making it broadly applicable across
models and domains. Evaluated on PrivacyQA and PolicyQA, our framework improves
GPT-4o-mini's zero-shot accuracy from 0.394 to 0.601 on PrivacyQA and from
0.352 to 0.464 on PolicyQA, surpassing or matching few-shot baselines without
additional training data. These results highlight the effectiveness of
structured agent collaboration in mitigating dialect biases and underscore the
importance of designing NLP systems that account for linguistic diversity to
ensure equitable access to privacy information.

</details>


### [101] [AUTOCIRCUIT-RL: Reinforcement Learning-Driven LLM for Automated Circuit Topology Generation](https://arxiv.org/abs/2506.03122)
*Prashanth Vijayaraghavan,Luyao Shi,Ehsan Degan,Vandana Mukherjee,Xin Zhang*

Main category: cs.CL

Relevance: 40.0

TL;DR: AUTOCIRCUIT-RL是一个基于强化学习的框架，利用LLM自动生成模拟电路拓扑，通过指令调优和RL优化，显著提高了电路设计的有效性和效率。


<details>
  <summary>Details</summary>
Motivation: 模拟电路拓扑合成的设计空间大且约束严格，传统方法效率低，因此利用LLM和RL技术实现自动化设计。

Method: 分为两阶段：指令调优（LLM学习生成电路拓扑）和RL优化（通过奖励模型改进生成质量）。

Result: 生成的有效电路增加12%，效率提升14%，重复生成率降低38%，训练数据有限时成功率超60%。

Conclusion: AUTOCIRCUIT-RL在复杂电路设计中表现出高效性和泛化能力，是AI驱动电路设计的重要进展。

Abstract: Analog circuit topology synthesis is integral to Electronic Design Automation
(EDA), enabling the automated creation of circuit structures tailored to
specific design requirements. However, the vast design search space and strict
constraint adherence make efficient synthesis challenging. Leveraging the
versatility of Large Language Models (LLMs), we propose AUTOCIRCUIT-RL,a novel
reinforcement learning (RL)-based framework for automated analog circuit
synthesis. The framework operates in two phases: instruction tuning, where an
LLM learns to generate circuit topologies from structured prompts encoding
design constraints, and RL refinement, which further improves the
instruction-tuned model using reward models that evaluate validity, efficiency,
and output voltage. The refined model is then used directly to generate
topologies that satisfy the design constraints. Empirical results show that
AUTOCIRCUIT-RL generates ~12% more valid circuits and improves efficiency by
~14% compared to the best baselines, while reducing duplicate generation rates
by ~38%. It achieves over 60% success in synthesizing valid circuits with
limited training data, demonstrating strong generalization. These findings
highlight the framework's effectiveness in scaling to complex circuits while
maintaining efficiency and constraint adherence, marking a significant
advancement in AI-driven circuit design.

</details>


### [102] [GUI-Actor: Coordinate-Free Visual Grounding for GUI Agents](https://arxiv.org/abs/2506.03143)
*Qianhui Wu,Kanzhi Cheng,Rui Yang,Chaoyun Zhang,Jianwei Yang,Huiqiang Jiang,Jian Mu,Baolin Peng,Bo Qiao,Reuben Tan,Si Qin,Lars Liden,Qingwei Lin,Huan Zhang,Tong Zhang,Jianbing Zhang,Dongmei Zhang,Jianfeng Gao*

Main category: cs.CL

Relevance: 40.0

TL;DR: GUI-Actor提出了一种基于视觉语言模型（VLM）的无坐标GUI定位方法，通过注意力机制和验证器提升视觉定位性能。


<details>
  <summary>Details</summary>
Motivation: 解决现有GUI代理在视觉定位中的空间语义对齐弱、模糊监督目标处理能力差以及视觉特征与坐标不匹配的问题。

Method: 引入基于注意力的动作头和对齐<ACTOR>标记的机制，设计验证器筛选最佳动作区域。

Result: 在多个基准测试中超越现有方法，GUI-Actor-7B表现优于UI-TARS-72B，且仅微调动作头即可达到SOTA性能。

Conclusion: GUI-Actor能在不损害VLM通用能力的前提下，赋予其高效的视觉定位能力。

Abstract: One of the principal challenges in building VLM-powered GUI agents is visual
grounding, i.e., localizing the appropriate screen region for action execution
based on both the visual content and the textual plans. Most existing work
formulates this as a text-based coordinate generation task. However, these
approaches suffer from several limitations: weak spatial-semantic alignment,
inability to handle ambiguous supervision targets, and a mismatch between the
dense nature of screen coordinates and the coarse, patch-level granularity of
visual features extracted by models like Vision Transformers. In this paper, we
propose GUI-Actor, a VLM-based method for coordinate-free GUI grounding. At its
core, GUI-Actor introduces an attention-based action head that learns to align
a dedicated <ACTOR> token with all relevant visual patch tokens, enabling the
model to propose one or more action regions in a single forward pass. In line
with this, we further design a grounding verifier to evaluate and select the
most plausible action region from the candidates proposed for action execution.
Extensive experiments show that GUI-Actor outperforms prior state-of-the-art
methods on multiple GUI action grounding benchmarks, with improved
generalization to unseen screen resolutions and layouts. Notably, GUI-Actor-7B
even surpasses UI-TARS-72B (38.1) on ScreenSpot-Pro, achieving scores of 40.7
with Qwen2-VL and 44.6 with Qwen2.5-VL as backbones. Furthermore, by
incorporating the verifier, we find that fine-tuning only the newly introduced
action head (~100M parameters for 7B model) while keeping the VLM backbone
frozen is sufficient to achieve performance comparable to previous
state-of-the-art models, highlighting that GUI-Actor can endow the underlying
VLM with effective grounding capabilities without compromising its
general-purpose strengths.

</details>


### [103] [Entity-Augmented Neuroscience Knowledge Retrieval Using Ontology and Semantic Understanding Capability of LLM](https://arxiv.org/abs/2506.03145)
*Pralaypati Ta,Sriram Venkatesaperumal,Keerthi Ram,Mohanasankar Sivaprakasam*

Main category: cs.CL

Relevance: 40.0

TL;DR: 论文提出了一种利用LLM、神经科学本体和文本嵌入从未标记的大规模神经科学研究语料库中构建知识图谱的新方法，显著提升了知识发现效果。


<details>
  <summary>Details</summary>
Motivation: 解决神经科学领域知识分散且现有方法依赖标注数据和领域专家的问题。

Method: 结合LLM、神经科学本体和文本嵌入构建知识图谱，并引入实体增强的信息检索算法。

Result: 实体提取F1得分为0.84，知识图谱中的知识提升了54%以上问题的回答质量。

Conclusion: 该方法有效解决了神经科学领域知识发现的挑战，为无标注数据下的知识图谱构建提供了新思路。

Abstract: Neuroscience research publications encompass a vast wealth of knowledge.
Accurately retrieving existing information and discovering new insights from
this extensive literature is essential for advancing the field. However, when
knowledge is dispersed across multiple sources, current state-of-the-art
retrieval methods often struggle to extract the necessary information. A
knowledge graph (KG) can integrate and link knowledge from multiple sources,
but existing methods for constructing KGs in neuroscience often rely on labeled
data and require domain expertise. Acquiring large-scale, labeled data for a
specialized area like neuroscience presents significant challenges. This work
proposes novel methods for constructing KG from unlabeled large-scale
neuroscience research corpus utilizing large language models (LLM),
neuroscience ontology, and text embeddings. We analyze the semantic relevance
of neuroscience text segments identified by LLM for building the knowledge
graph. We also introduce an entity-augmented information retrieval algorithm to
extract knowledge from the KG. Several experiments were conducted to evaluate
the proposed approaches, and the results demonstrate that our methods
significantly enhance knowledge discovery from the unlabeled neuroscience
research corpus. It achieves an F1 score of 0.84 for entity extraction, and the
knowledge obtained from the KG improves answers to over 54% of the questions.

</details>


### [104] [A Dynamic Framework for Semantic Grouping of Common Data Elements (CDE) Using Embeddings and Clustering](https://arxiv.org/abs/2506.02160)
*Madan Krishnamurthy,Daniel Korn,Melissa A Haendel,Christopher J Mungall,Anne E Thessen*

Main category: cs.IR

Relevance: 40.0

TL;DR: 论文提出了一种动态可扩展的框架，利用LLMs和HDBSCAN聚类实现生物医学数据元素的语义统一，提升数据互操作性。


<details>
  <summary>Details</summary>
Motivation: 解决生物医学数据元素（CDEs）的语义异质性和结构多样性问题，以促进数据集成和科学发现。

Method: 1) LLM生成上下文感知的文本嵌入；2) HDBSCAN聚类；3) LLM自动标注；4) 监督学习分类器。

Result: 在24,000+ CDEs上识别118个聚类，分类器准确率90.46%，外部验证表现良好。

Conclusion: 该框架为CDE统一提供了高效、可扩展的解决方案。

Abstract: This research aims to develop a dynamic and scalable framework to facilitate
harmonization of Common Data Elements (CDEs) across heterogeneous biomedical
datasets by addressing challenges such as semantic heterogeneity, structural
variability, and context dependence to streamline integration, enhance
interoperability, and accelerate scientific discovery. Our methodology
leverages Large Language Models (LLMs) for context-aware text embeddings that
convert CDEs into dense vectors capturing semantic relationships and patterns.
These embeddings are clustered using Hierarchical Density-Based Spatial
Clustering of Applications with Noise (HDBSCAN) to group semantically similar
CDEs. The framework incorporates four key steps: (1) LLM-based text embedding
to mathematically represent semantic context, (2) unsupervised clustering of
embeddings via HDBSCAN, (3) automated labeling using LLM summarization, and (4)
supervised learning to train a classifier assigning new or unclustered CDEs to
labeled clusters. Evaluated on the NIH NLM CDE Repository with over 24,000
CDEs, the system identified 118 meaningful clusters at an optimized minimum
cluster size of 20. The classifier achieved 90.46 percent overall accuracy,
performing best in larger categories. External validation against Gravity
Projects Social Determinants of Health domains showed strong agreement
(Adjusted Rand Index 0.52, Normalized Mutual Information 0.78), indicating that
embeddings effectively capture cluster characteristics. This adaptable and
scalable approach offers a practical solution to CDE harmonization, improving
selection efficiency and supporting ongoing data interoperability.

</details>


### [105] [Cocktail-Party Audio-Visual Speech Recognition](https://arxiv.org/abs/2506.02178)
*Thai-Binh Nguyen,Ngoc-Quan Pham,Alexander Waibel*

Main category: cs.SD

Relevance: 40.0

TL;DR: 论文提出了一种新的音频-视觉鸡尾酒会数据集，用于评估AVSR系统在真实噪声环境中的表现，并展示了其性能提升。


<details>
  <summary>Details</summary>
Motivation: 当前AVSR模型在理想化场景中表现良好，但在真实噪声环境（如鸡尾酒会）中效果不佳，尤其是面对说话和沉默面部片段混合的情况。

Method: 研究引入了一个包含1526小时音频-视觉数据的鸡尾酒会数据集，并设计了一种无需显式分割线索的方法。

Result: 在极端噪声环境下，该方法将WER从119%降低到39.2%，相对提升了67%。

Conclusion: 该研究为AVSR在真实噪声环境中的应用提供了有效的数据集和方法，显著提升了性能。

Abstract: Audio-Visual Speech Recognition (AVSR) offers a robust solution for speech
recognition in challenging environments, such as cocktail-party scenarios,
where relying solely on audio proves insufficient. However, current AVSR models
are often optimized for idealized scenarios with consistently active speakers,
overlooking the complexities of real-world settings that include both speaking
and silent facial segments. This study addresses this gap by introducing a
novel audio-visual cocktail-party dataset designed to benchmark current AVSR
systems and highlight the limitations of prior approaches in realistic noisy
conditions. Additionally, we contribute a 1526-hour AVSR dataset comprising
both talking-face and silent-face segments, enabling significant performance
gains in cocktail-party environments. Our approach reduces WER by 67% relative
to the state-of-the-art, reducing WER from 119% to 39.2% in extreme noise,
without relying on explicit segmentation cues.

</details>


### [106] [StarVC: A Unified Auto-Regressive Framework for Joint Text and Speech Generation in Voice Conversion](https://arxiv.org/abs/2506.02414)
*Fengjin Li,Jie Wang,Yadong Niu,Yongqing Wang,Meng Meng,Jian Luan,Zhiyong Wu*

Main category: cs.MM

Relevance: 40.0

TL;DR: StarVC是一种自回归语音转换框架，通过预测文本标记再合成声学特征，优于传统方法。


<details>
  <summary>Details</summary>
Motivation: 传统语音转换方法忽略显式利用语义内容，而StarVC通过结合文本建模提升性能。

Method: 提出StarVC框架，先预测文本标记再合成声学特征。

Result: 实验表明StarVC在保留语言内容和说话人特征上优于传统方法。

Conclusion: StarVC通过显式文本建模提升了语音转换性能。

Abstract: Voice Conversion (VC) modifies speech to match a target speaker while
preserving linguistic content. Traditional methods usually extract speaker
information directly from speech while neglecting the explicit utilization of
linguistic content. Since VC fundamentally involves disentangling speaker
identity from linguistic content, leveraging structured semantic features could
enhance conversion performance. However, previous attempts to incorporate
semantic features into VC have shown limited effectiveness, motivating the
integration of explicit text modeling. We propose StarVC, a unified
autoregressive VC framework that first predicts text tokens before synthesizing
acoustic features. The experiments demonstrate that StarVC outperforms
conventional VC methods in preserving both linguistic content (i.e., WER and
CER) and speaker characteristics (i.e., SECS and MOS). Audio demo can be found
at: https://thuhcsi.github.io/StarVC/.

</details>


### [107] [Synthetic Speech Source Tracing using Metric Learning](https://arxiv.org/abs/2506.02590)
*Dimitrios Koutsianos,Stavros Zacharopoulos,Yannis Panagakis,Themos Stafylakis*

Main category: cs.SD

Relevance: 40.0

TL;DR: 论文提出了一种通过说话人识别方法追踪合成语音来源的方法，比较了分类和度量学习两种方法，发现ResNet表现优于自监督学习。


<details>
  <summary>Details</summary>
Motivation: 现有研究主要关注欺骗检测，而合成语音的来源追踪缺乏鲁棒解决方案。

Method: 采用分类和度量学习两种方法，并在MLAADv5基准上测试了ResNet和自监督学习模型。

Result: ResNet在度量学习方法中表现优异，甚至超过自监督学习系统。

Conclusion: ResNet在合成语音来源追踪中具有潜力，但需优化自监督学习的表示。

Abstract: This paper addresses source tracing in synthetic speech-identifying
generative systems behind manipulated audio via speaker recognition-inspired
pipelines. While prior work focuses on spoofing detection, source tracing lacks
robust solutions. We evaluate two approaches: classification-based and
metric-learning. We tested our methods on the MLAADv5 benchmark using ResNet
and self-supervised learning (SSL) backbones. The results show that ResNet
achieves competitive performance with the metric learning approach, matching
and even exceeding SSL-based systems. Our work demonstrates ResNet's viability
for source tracing while underscoring the need to optimize SSL representations
for this task. Our work bridges speaker recognition methodologies with audio
forensic challenges, offering new directions for combating synthetic media
manipulation.

</details>


### [108] [ChatCFD: an End-to-End CFD Agent with Domain-specific Structured Thinking](https://arxiv.org/abs/2506.02019)
*E Fan,Weizong Wang,Tianhan Zhang*

Main category: cs.CL

Relevance: 30.0

TL;DR: ChatCFD是一个基于大语言模型的自动化CFD工作流工具，通过自然语言提示简化OpenFOAM框架中的复杂模拟配置。


<details>
  <summary>Details</summary>
Motivation: CFD操作复杂且需要专业知识，ChatCFD旨在降低门槛，让非专家也能高效使用。

Method: 结合CFD和OpenFOAM知识，构建结构化数据库，通过配置验证和错误反馈提升模型适应性。

Result: 验证显示ChatCFD能自主复现复杂CFD结果，超越通用语言模型的能力。

Conclusion: ChatCFD为CFD领域提供了高效、易用的自动化解决方案。

Abstract: Computational Fluid Dynamics (CFD) is essential for scientific and
engineering advancements but is limited by operational complexity and the need
for extensive expertise. This paper presents ChatCFD, a large language
model-driven pipeline that automates CFD workflows within the OpenFOAM
framework. It enables users to configure and execute complex simulations from
natural language prompts or published literature with minimal expertise. The
innovation is its structured approach to database construction, configuration
validation, and error reflection, integrating CFD and OpenFOAM knowledge with
general language models to improve accuracy and adaptability. Validation shows
ChatCFD can autonomously reproduce published CFD results, handling complex,
unseen configurations beyond basic examples, a task challenging for general
language models.

</details>


### [109] [Investigating the Impact of Word Informativeness on Speech Emotion Recognition](https://arxiv.org/abs/2506.02239)
*Sofoklis Kakouros*

Main category: cs.CL

Relevance: 30.0

TL;DR: 论文提出了一种基于预训练语言模型的词信息量方法，用于识别语音中情感相关的关键片段，从而提高情感识别的准确性。


<details>
  <summary>Details</summary>
Motivation: 传统方法在计算语音特征时可能忽略细粒度的变化，而通过词信息量可以更精准地定位情感相关的语音片段。

Method: 利用预训练语言模型提取词信息量，识别关键语音片段，并计算这些片段的声学特征（如韵律特征和自监督表示）。

Result: 实验结果表明，基于词信息量选择片段的方法显著提高了情感识别的性能。

Conclusion: 该方法通过聚焦关键片段，有效提升了情感识别的准确性，为语音情感分析提供了新思路。

Abstract: In emotion recognition from speech, a key challenge lies in identifying
speech signal segments that carry the most relevant acoustic variations for
discerning specific emotions. Traditional approaches compute functionals for
features such as energy and F0 over entire sentences or longer speech portions,
potentially missing essential fine-grained variation in the long-form
statistics. This research investigates the use of word informativeness, derived
from a pre-trained language model, to identify semantically important segments.
Acoustic features are then computed exclusively for these identified segments,
enhancing emotion recognition accuracy. The methodology utilizes standard
acoustic prosodic features, their functionals, and self-supervised
representations. Results indicate a notable improvement in recognition
performance when features are computed on segments selected based on word
informativeness, underscoring the effectiveness of this approach.

</details>


### [110] [Gender Inequality in English Textbooks Around the World: an NLP Approach](https://arxiv.org/abs/2506.02425)
*Tairan Liu*

Main category: cs.CL

Relevance: 30.0

TL;DR: 该研究使用自然语言处理方法量化了22个国家英语教科书中的性别不平等问题，发现男性角色在数量、提及顺序和命名实体上普遍被过度代表。


<details>
  <summary>Details</summary>
Motivation: 探讨跨文化背景下教科书中的性别不平等问题，填补了现有研究的空白。

Method: 采用自然语言处理技术，包括字符计数、首次提及性别、TF-IDF词关联、专有名词分析、大型语言模型测试和GloVe嵌入分析。

Result: 所有地区均存在性别不平等，男性角色在多个指标上被过度代表，拉丁文化圈的不平等程度最低。

Conclusion: 教科书中的性别不平等是一个普遍现象，需要进一步关注和改进。

Abstract: Textbooks play a critical role in shaping children's understanding of the
world. While previous studies have identified gender inequality in individual
countries' textbooks, few have examined the issue cross-culturally. This study
applies natural language processing methods to quantify gender inequality in
English textbooks from 22 countries across 7 cultural spheres. Metrics include
character count, firstness (which gender is mentioned first), and TF-IDF word
associations by gender. The analysis also identifies gender patterns in proper
names appearing in TF-IDF word lists, tests whether large language models can
distinguish between gendered word lists, and uses GloVe embeddings to examine
how closely keywords associate with each gender. Results show consistent
overrepresentation of male characters in terms of count, firstness, and named
entities. All regions exhibit gender inequality, with the Latin cultural sphere
showing the least disparity.

</details>


### [111] [Natural Language Processing to Enhance Deliberation in Political Online Discussions: A Survey](https://arxiv.org/abs/2506.02533)
*Maike Behrendt,Stefan Sylvius Wagner,Carina Weinmann,Marike Bormann,Mira Warne,Stefan Harmeling*

Main category: cs.CL

Relevance: 30.0

TL;DR: 论文探讨了政治在线讨论中的问题，并展示了机器学习如何用于改善讨论质量。


<details>
  <summary>Details</summary>
Motivation: 随着政治讨论越来越多地转向线上，如何提升讨论的审议质量成为关键问题。机器学习为优化平台设计和流程提供了潜力。

Method: 论文分析了政治在线讨论中的问题，并提出利用机器学习方法来改善讨论质量。

Result: 展示了机器学习在提升政治讨论审议质量方面的潜力。

Conclusion: 机器学习可以成为改善在线政治讨论质量的有效工具。

Abstract: Political online participation in the form of discussing political issues and
exchanging opinions among citizens is gaining importance with more and more
formats being held digitally. To come to a decision, a careful discussion and
consideration of opinions and a civil exchange of arguments, which is defined
as the act of deliberation, is desirable. The quality of discussions and
participation processes in terms of their deliberativeness highly depends on
the design of platforms and processes. To facilitate online communication for
both participants and initiators, machine learning methods offer a lot of
potential. In this work we want to showcase which issues occur in political
online discussions and how machine learning can be used to counteract these
issues and enhance deliberation.

</details>


### [112] [Learning More with Less: Self-Supervised Approaches for Low-Resource Speech Emotion Recognition](https://arxiv.org/abs/2506.02059)
*Ziwei Gong,Pengyuan Shi,Kaan Donbekci,Lin Ai,Run Chen,David Sasu,Zehui Wu,Julia Hirschberg*

Main category: cs.SD

Relevance: 30.0

TL;DR: 论文探讨了通过无监督学习（对比学习和BYOL）提升低资源语言（LRLs）的语音情感识别（SER）性能，取得了显著效果。


<details>
  <summary>Details</summary>
Motivation: 解决低资源语言中标注数据稀缺导致的SER性能不足问题。

Method: 采用对比学习（CL）和Bootstrap Your Own Latent（BYOL）作为自监督学习方法。

Result: 在乌尔都语、德语和孟加拉语中分别实现了10.6%、15.2%和13.9%的F1分数提升。

Conclusion: 为低资源语言的SER提供了更包容、可解释且鲁棒的系统基础。

Abstract: Speech Emotion Recognition (SER) has seen significant progress with deep
learning, yet remains challenging for Low-Resource Languages (LRLs) due to the
scarcity of annotated data. In this work, we explore unsupervised learning to
improve SER in low-resource settings. Specifically, we investigate contrastive
learning (CL) and Bootstrap Your Own Latent (BYOL) as self-supervised
approaches to enhance cross-lingual generalization. Our methods achieve notable
F1 score improvements of 10.6% in Urdu, 15.2% in German, and 13.9% in Bangla,
demonstrating their effectiveness in LRLs. Additionally, we analyze model
behavior to provide insights on key factors influencing performance across
languages, and also highlighting challenges in low-resource SER. This work
provides a foundation for developing more inclusive, explainable, and robust
emotion recognition systems for underrepresented languages.

</details>


### [113] [Enhancing Speech Emotion Recognition with Graph-Based Multimodal Fusion and Prosodic Features for the Speech Emotion Recognition in Naturalistic Conditions Challenge at Interspeech 2025](https://arxiv.org/abs/2506.02088)
*Alef Iury Siqueira Ferreira,Lucas Rafael Gris,Alexandre Ferro Filho,Lucas Ólives,Daniel Ribeiro,Luiz Fernando,Fernanda Lustosa,Rodrigo Tanaka,Frederico Santos de Oliveira,Arlindo Galvão Filho*

Main category: cs.SD

Relevance: 30.0

TL;DR: 本文提出了一种结合音频模型和文本特征的鲁棒系统，用于自然语音中的情感识别，并在挑战赛中取得了显著成绩。


<details>
  <summary>Details</summary>
Motivation: 自然语音中的情感识别因情感表达微妙且音频不可预测而具有挑战性，本文旨在解决这一问题。

Method: 结合先进的音频模型与文本特征（包括韵律和频谱线索），研究F0量化和预训练音频标记模型的效果，并使用集成模型提升鲁棒性。

Result: 在官方测试集上，系统取得了39.79%的Macro F1分数（验证集为42.20%）。

Conclusion: 该方法展示了潜力，融合技术分析证实了图注意力网络的有效性。

Abstract: Training SER models in natural, spontaneous speech is especially challenging
due to the subtle expression of emotions and the unpredictable nature of
real-world audio. In this paper, we present a robust system for the INTERSPEECH
2025 Speech Emotion Recognition in Naturalistic Conditions Challenge, focusing
on categorical emotion recognition. Our method combines state-of-the-art audio
models with text features enriched by prosodic and spectral cues. In
particular, we investigate the effectiveness of Fundamental Frequency (F0)
quantization and the use of a pretrained audio tagging model. We also employ an
ensemble model to improve robustness. On the official test set, our system
achieved a Macro F1-score of 39.79% (42.20% on validation). Our results
underscore the potential of these methods, and analysis of fusion techniques
confirmed the effectiveness of Graph Attention Networks. Our source code is
publicly available.

</details>


### [114] [A Multi-Dialectal Dataset for German Dialect ASR and Dialect-to-Standard Speech Translation](https://arxiv.org/abs/2506.02894)
*Verena Blaschke,Miriam Winkler,Constantin Förster,Gabriele Wenger-Glemser,Barbara Plank*

Main category: cs.CL

Relevance: 20.0

TL;DR: 论文介绍了Betthupferl数据集，用于评估自动语音识别（ASR）模型对德国东南方言的鲁棒性，并分析了方言与标准德语的差异。


<details>
  <summary>Details</summary>
Motivation: 当前ASR研究对方言的覆盖不足，缺乏评估模型对方言鲁棒性的数据集。

Method: 构建包含三种德国东南方言和标准德语的Betthupferl数据集，并测试多语言ASR模型的翻译性能。

Result: ASR模型在方言翻译中存在差异，部分情况下会标准化语法差异，但更倾向于保留方言结构。

Conclusion: Betthupferl数据集为方言鲁棒性研究提供了基础，ASR模型在处理方言时仍有改进空间。

Abstract: Although Germany has a diverse landscape of dialects, they are
underrepresented in current automatic speech recognition (ASR) research. To
enable studies of how robust models are towards dialectal variation, we present
Betthupferl, an evaluation dataset containing four hours of read speech in
three dialect groups spoken in Southeast Germany (Franconian, Bavarian,
Alemannic), and half an hour of Standard German speech. We provide both
dialectal and Standard German transcriptions, and analyze the linguistic
differences between them. We benchmark several multilingual state-of-the-art
ASR models on speech translation into Standard German, and find differences
between how much the output resembles the dialectal vs. standardized
transcriptions. Qualitative error analyses of the best ASR model reveal that it
sometimes normalizes grammatical differences, but often stays closer to the
dialectal constructions.

</details>


### [115] [Sounding Like a Winner? Prosodic Differences in Post-Match Interviews](https://arxiv.org/abs/2506.02283)
*Sofoklis Kakouros,Haoyu Chen*

Main category: cs.CL

Relevance: 10.0

TL;DR: 研究探讨了网球赛后采访中与胜负相关的韵律特征，并尝试通过自监督学习模型（如Wav2Vec 2.0和HuBERT）结合韵律特征（如音高和强度）分类比赛结果。结果表明，自监督学习能有效区分胜负，且韵律特征（如音高变化）是胜利的强指标。


<details>
  <summary>Details</summary>
Motivation: 探索运动员赛后采访中的韵律特征是否能反映比赛结果，并验证自监督学习在此任务中的有效性。

Method: 提取传统声学特征和深度语音表示（如Wav2Vec 2.0和HuBERT），使用机器学习分类器区分胜负。

Result: 自监督学习能有效区分胜负，韵律特征（如音高变化）是胜利的强指标。

Conclusion: 赛后采访的韵律特征结合自监督学习可有效分类比赛结果，揭示了情感状态与语音模式的关系。

Abstract: This study examines the prosodic characteristics associated with winning and
losing in post-match tennis interviews. Additionally, this research explores
the potential to classify match outcomes solely based on post-match interview
recordings using prosodic features and self-supervised learning (SSL)
representations. By analyzing prosodic elements such as pitch and intensity,
alongside SSL models like Wav2Vec 2.0 and HuBERT, the aim is to determine
whether an athlete has won or lost their match. Traditional acoustic features
and deep speech representations are extracted from the data, and machine
learning classifiers are employed to distinguish between winning and losing
players. Results indicate that SSL representations effectively differentiate
between winning and losing outcomes, capturing subtle speech patterns linked to
emotional states. At the same time, prosodic cues -- such as pitch variability
-- remain strong indicators of victory.

</details>


<div id='cs.CV'></div>

# cs.CV [[Back]](#toc)

### [116] [Improve Multi-Modal Embedding Learning via Explicit Hard Negative Gradient Amplifying](https://arxiv.org/abs/2506.02020)
*Youze Xue,Dian Li,Gang Liu*

Main category: cs.CV

Relevance: 85.0

TL;DR: 论文研究了多模态大语言模型（MLLMs）中硬负样本在对比学习中的作用，提出了一种显式梯度放大方法，显著提升了多模态嵌入性能。


<details>
  <summary>Details</summary>
Motivation: 尽管CLIP框架已成功扩展到MLLMs，但硬负样本在对比学习中的具体贡献未被充分研究。本文旨在分析其作用并提出改进方法。

Method: 通过分析info-NCE损失的梯度，提出显式梯度放大器（Explicit Gradient Amplifier），增强硬负样本对模型参数更新的影响。

Result: 基于LLaVA-OneVision-7B架构的模型在MMEB基准测试中达到SOTA性能，结合自研MLLM（QQMM）后位居MMEB排行榜首位。

Conclusion: 显式梯度放大方法有效提升了多模态嵌入的判别性，为对比学习提供了新思路。

Abstract: With the rapid advancement of multi-modal large language models (MLLMs) in
recent years, the foundational Contrastive Language-Image Pretraining (CLIP)
framework has been successfully extended to MLLMs, enabling more powerful and
universal multi-modal embeddings for a wide range of retrieval tasks. Despite
these developments, the core contrastive learning paradigm remains largely
unchanged from CLIP-style models to MLLMs. Within this framework, the effective
mining of hard negative samples continues to be a critical factor for enhancing
performance. Prior works have introduced both offline and online strategies for
hard negative mining to improve the efficiency of contrastive learning. While
these approaches have led to improved multi-modal embeddings, the specific
contribution of each hard negative sample to the learning process has not been
thoroughly investigated. In this work, we conduct a detailed analysis of the
gradients of the info-NCE loss with respect to the query, positive, and
negative samples, elucidating the role of hard negatives in updating model
parameters. Building upon this analysis, we propose to explicitly amplify the
gradients associated with hard negative samples, thereby encouraging the model
to learn more discriminative embeddings. Our multi-modal embedding model,
trained with the proposed Explicit Gradient Amplifier and based on the
LLaVA-OneVision-7B architecture, achieves state-of-the-art performance on the
MMEB benchmark compared to previous methods utilizing the same MLLM backbone.
Furthermore, when integrated with our self-developed MLLM, QQMM, our approach
attains the top rank on the MMEB leaderboard. Code and models are released on
https://github.com/QQ-MM/QQMM-embed.

</details>


### [117] [Improving Knowledge Distillation Under Unknown Covariate Shift Through Confidence-Guided Data Augmentation](https://arxiv.org/abs/2506.02294)
*Niclas Popp,Kevin Alexander Laube,Matthias Hein,Lukas Schott*

Main category: cs.CV

Relevance: 85.0

TL;DR: 论文提出了一种基于扩散的数据增强策略，通过最大化教师与学生模型之间的分歧生成挑战性样本，解决了知识蒸馏中协变量偏移的问题。


<details>
  <summary>Details</summary>
Motivation: 研究动机是解决知识蒸馏中因协变量偏移导致的性能限制问题，尤其是在未知虚假特征的情况下，如何利用鲁棒的教师模型使学生模型也具备鲁棒性。

Method: 方法包括引入一种新颖的扩散数据增强策略，生成教师与学生模型分歧最大的图像样本，以增强学生模型的鲁棒性。

Result: 实验结果表明，该方法在CelebA、SpuCo Birds和spurious ImageNet数据集上显著提高了最差组和平均组准确率，以及虚假mAUC，优于现有扩散数据增强基线。

Conclusion: 结论表明，该方法能有效提升知识蒸馏在协变量偏移下的性能，尤其是在处理未知虚假特征时。

Abstract: Large foundation models trained on extensive datasets demonstrate strong
zero-shot capabilities in various domains. To replicate their success when data
and model size are constrained, knowledge distillation has become an
established tool for transferring knowledge from foundation models to small
student networks. However, the effectiveness of distillation is critically
limited by the available training data. This work addresses the common
practical issue of covariate shift in knowledge distillation, where spurious
features appear during training but not at test time. We ask the question: when
these spurious features are unknown, yet a robust teacher is available, is it
possible for a student to also become robust to them? We address this problem
by introducing a novel diffusion-based data augmentation strategy that
generates images by maximizing the disagreement between the teacher and the
student, effectively creating challenging samples that the student struggles
with. Experiments demonstrate that our approach significantly improves worst
group and mean group accuracy on CelebA and SpuCo Birds as well as the spurious
mAUC on spurious ImageNet under covariate shift, outperforming state-of-the-art
diffusion-based data augmentation baselines

</details>


### [118] [Targeted Forgetting of Image Subgroups in CLIP Models](https://arxiv.org/abs/2506.03117)
*Zeliang Zhang,Gaowen Liu,Charles Fleming,Ramana Rao Kompella,Chenliang Xu*

Main category: cs.CV

Relevance: 85.0

TL;DR: 论文提出了一种针对CLIP等基础模型的三阶段细粒度知识遗忘方法，能够在无需预训练数据的情况下选择性遗忘特定知识，同时保持模型的整体性能。


<details>
  <summary>Details</summary>
Motivation: 基础模型（如CLIP）在零样本任务中表现优异，但可能从噪声数据中继承有害知识，现有遗忘方法无法满足细粒度需求。

Method: 提出三阶段方法：遗忘阶段微调目标知识，提醒阶段恢复保留样本性能，恢复阶段通过模型融合恢复零样本能力，并结合知识蒸馏处理分布差异。

Result: 在CIFAR-10、ImageNet-1K等数据集上验证，方法能有效遗忘特定子组，同时保持零样本性能，显著优于基线方法。

Conclusion: 该方法填补了细粒度知识遗忘的空白，提升了基础模型的可靠性和实用性。

Abstract: Foundation models (FMs) such as CLIP have demonstrated impressive zero-shot
performance across various tasks by leveraging large-scale, unsupervised
pre-training. However, they often inherit harmful or unwanted knowledge from
noisy internet-sourced datasets, compromising their reliability in real-world
applications. Existing model unlearning methods either rely on access to
pre-trained datasets or focus on coarse-grained unlearning (e.g., entire
classes), leaving a critical gap for fine-grained unlearning. In this paper, we
address the challenging scenario of selectively forgetting specific portions of
knowledge within a class, without access to pre-trained data, while preserving
the model's overall performance. We propose a novel three-stage approach that
progressively unlearns targeted knowledge while mitigating over-forgetting. It
consists of (1) a forgetting stage to fine-tune the CLIP on samples to be
forgotten, (2) a reminding stage to restore performance on retained samples,
and (3) a restoring stage to recover zero-shot capabilities using model
souping. Additionally, we introduce knowledge distillation to handle the
distribution disparity between forgetting, retaining samples, and unseen
pre-trained data. Extensive experiments on CIFAR-10, ImageNet-1K, and style
datasets demonstrate that our approach effectively unlearns specific subgroups
while maintaining strong zero-shot performance on semantically similar
subgroups and other categories, significantly outperforming baseline unlearning
methods, which lose effectiveness under the CLIP unlearning setting.

</details>


### [119] [Object-centric Self-improving Preference Optimization for Text-to-Image Generation](https://arxiv.org/abs/2506.02015)
*Yoonjin Oh,Yongjin Kim,Hyomin Kim,Donghwan Chi,Sungwoong Kim*

Main category: cs.CV

Relevance: 75.0

TL;DR: 本文提出了一种面向多模态大语言模型（MLLMs）的对象中心自改进偏好优化（OSPO）框架，用于提升文本到图像生成任务中的细粒度视觉理解能力。


<details>
  <summary>Details</summary>
Motivation: 尽管MLLMs在图像理解和生成方面取得了进展，但在细粒度视觉理解（尤其是文本到图像生成任务）中仍存在不足。偏好优化方法在图像理解任务中已有应用，但在图像生成中尚未充分探索。

Method: OSPO框架利用MLLMs的内在推理能力，无需外部数据或模型。它通过对象中心提示扰动、密集化和VQA评分，自主构建高质量的对象级对比偏好对。

Result: 在三个代表性的组合文本到图像基准测试中，OSPO显著优于基线模型。

Conclusion: OSPO通过自改进机制生成高质量偏好对，有效提升了MLLMs在文本到图像生成任务中的性能。

Abstract: Recent advancements in Multimodal Large Language Models (MLLMs) have
significantly improved both image understanding and generation capabilities.
Despite these improvements, MLLMs still struggle with fine-grained visual
comprehension, particularly in text-to-image generation tasks. While preference
optimization methods have been explored to address these limitations in image
understanding tasks, their application to image generation remains largely
underexplored. To address this gap, we propose an Object-centric Self-improving
Preference Optimization (OSPO) framework designed for text-to-image generation
by MLLMs. OSPO leverages the intrinsic reasoning abilities of MLLMs without
requiring any external datasets or models. OSPO emphasizes the importance of
high-quality preference pair data, which is critical for effective preference
optimization. To achieve this, it introduces a self-improving mechanism that
autonomously constructs object-level contrastive preference pairs through
object-centric prompt perturbation, densification and VQA scoring. This process
eliminates ambiguous or disproportionate variations commonly found in naively
generated preference pairs, thereby enhancing the effectiveness of preference
optimization. We validate OSPO on three representative compositional
text-to-image benchmarks, demonstrating substantial performance gains over
baseline models.

</details>


### [120] [Do You See Me : A Multidimensional Benchmark for Evaluating Visual Perception in Multimodal LLMs](https://arxiv.org/abs/2506.02022)
*Aditya Kanade,Tanuja Ganu*

Main category: cs.CV

Relevance: 75.0

TL;DR: 论文提出了一个名为'Do You See Me'的基准测试，用于评估多模态大语言模型（MLLMs）的视觉感知能力，发现其性能远低于人类水平。


<details>
  <summary>Details</summary>
Motivation: MLLMs在视觉感知上存在严重缺陷，即使答案正确也可能误解关键视觉元素，亟需系统性评估和改进。

Method: 通过构建包含1,758张图像和2,612个问题的基准测试，覆盖7个子任务，评估3个闭源和5个开源MLLMs的性能。

Result: 人类准确率为96.49%，而顶级MLLMs平均低于50%，且性能随任务复杂度增加显著下降。

Conclusion: MLLMs的视觉感知能力不足，需改进视觉注意力和细粒度细节表示。

Abstract: Multimodal Large Language Models (MLLMs) show reasoning promise, yet their
visual perception is a critical bottleneck. Strikingly, MLLMs can produce
correct answers even while misinterpreting crucial visual elements, masking
these underlying failures. Our preliminary study on a joint
perception-reasoning dataset revealed that for one leading MLLM, 29% of its
correct answers to reasoning questions still exhibited visual perception
errors. To systematically address this, we introduce "Do You See Me", a
scalable benchmark with 1,758 images and 2,612 questions. It spans seven
human-psychology inspired subtasks in 2D and 3D, featuring controllable
complexity to rigorously evaluate MLLM visual skills. Our findings on 3 leading
closed-source and 5 major open-source models reveal a stark deficit: humans
achieve 96.49% accuracy, while top MLLMs average below 50%. This performance
gap widens rapidly with increased task complexity (e.g., from 12% to 45% in the
visual form constancy subtask). Further analysis into the root causes suggests
that failures stem from challenges like misallocated visual attention and the
instability of internal representations for fine-grained details, especially at
or below encoder patch resolution. This underscores an urgent need for MLLMs
with truly robust visual perception. The benchmark dataset, source code and
evaluation scripts are available at https://github.com/microsoft/Do-You-See-Me.

</details>


### [121] [VisuRiddles: Fine-grained Perception is a Primary Bottleneck for Multimodal Large Language Models in Abstract Visual Reasoning](https://arxiv.org/abs/2506.02537)
*Hao Yan,Handong Zheng,Hao Wang,Liang Yin,Xingchen Liu,Zhenbiao Cao,Xinxing Su,Zihao Chen,Jihao Wu,Minghui Liao,Chao Weng,Wei Chen,Yuliang Liu,Xiang Bai*

Main category: cs.CV

Relevance: 75.0

TL;DR: 论文提出VisuRiddles基准和PRS框架，以提升多模态大语言模型在抽象视觉推理任务中的表现。


<details>
  <summary>Details</summary>
Motivation: 当前多模态大语言模型在抽象视觉推理任务中存在瓶颈，主要由于对抽象图形的感知能力不足。

Method: 1. 提出VisuRiddles基准，评估模型在五个核心维度和两类高层次推理任务中的能力；2. 开发PRS框架，自动生成带有细粒度感知描述的谜题，用于训练和提升模型表现。

Result: 实验证明细粒度视觉感知是主要瓶颈，PRS框架显著提升了模型在VisuRiddles任务中的性能。

Conclusion: PRS框架通过生成细粒度感知描述数据，有效提升了多模态大语言模型的抽象视觉推理能力。

Abstract: Recent strides in multimodal large language models (MLLMs) have significantly
advanced their performance in many reasoning tasks. However, Abstract Visual
Reasoning (AVR) remains a critical challenge, primarily due to limitations in
perceiving abstract graphics. To tackle this issue, we investigate the
bottlenecks in current MLLMs and synthesize training data to improve their
abstract visual perception. First, we propose VisuRiddles, a benchmark for AVR,
featuring tasks meticulously constructed to assess models' reasoning capacities
across five core dimensions and two high-level reasoning categories. Second, we
introduce the Perceptual Riddle Synthesizer (PRS), an automated framework for
generating riddles with fine-grained perceptual descriptions. PRS not only
generates valuable training data for abstract graphics but also provides
fine-grained perceptual description, crucially allowing for supervision over
intermediate reasoning stages and thereby improving both training efficacy and
model interpretability. Our extensive experimental results on VisuRiddles
empirically validate that fine-grained visual perception is the principal
bottleneck and our synthesis framework markedly enhances the performance of
contemporary MLLMs on these challenging tasks. Our code and dataset will be
released at https://github.com/yh-hust/VisuRiddles

</details>


### [122] [Smoothed Preference Optimization via ReNoise Inversion for Aligning Diffusion Models with Varied Human Preferences](https://arxiv.org/abs/2506.02698)
*Yunhong Lu,Qichao Wang,Hengyuan Cao,Xiaoyin Xu,Min Zhang*

Main category: cs.CV

Relevance: 75.0

TL;DR: SmPO-Diffusion是一种改进直接偏好优化（DPO）的方法，通过建模偏好分布和优化扩散目标，解决了现有方法中偏好过于简化和优化目标不一致的问题。


<details>
  <summary>Details</summary>
Motivation: 现有DPO方法忽略了偏好的个体差异，导致优化不准确。本文旨在通过更细粒度的偏好建模改进DPO。

Method: 提出SmPO-Diffusion，包括平滑偏好分布替换二元分布、奖励模型模拟偏好、偏好似然平均改进损失函数，以及扩散轨迹偏好分布的逆技术。

Result: SmPO-Diffusion在偏好评估中达到SOTA性能，训练成本更低。

Conclusion: SmPO-Diffusion通过简单修改有效解决了现有方法的优化过度和目标不一致问题。

Abstract: Direct Preference Optimization (DPO) aligns text-to-image (T2I) generation
models with human preferences using pairwise preference data. Although
substantial resources are expended in collecting and labeling datasets, a
critical aspect is often neglected: \textit{preferences vary across individuals
and should be represented with more granularity.} To address this, we propose
SmPO-Diffusion, a novel method for modeling preference distributions to improve
the DPO objective, along with a numerical upper bound estimation for the
diffusion optimization objective. First, we introduce a smoothed preference
distribution to replace the original binary distribution. We employ a reward
model to simulate human preferences and apply preference likelihood averaging
to improve the DPO loss, such that the loss function approaches zero when
preferences are similar. Furthermore, we utilize an inversion technique to
simulate the trajectory preference distribution of the diffusion model,
enabling more accurate alignment with the optimization objective. Our approach
effectively mitigates issues of excessive optimization and objective
misalignment present in existing methods through straightforward modifications.
Our SmPO-Diffusion achieves state-of-the-art performance in preference
evaluation, outperforming baselines across metrics with lower training costs.
The project page is https://jaydenlyh.github.io/SmPO-project-page/.

</details>


### [123] [METok: Multi-Stage Event-based Token Compression for Efficient Long Video Understanding](https://arxiv.org/abs/2506.02850)
*Mengyue Wang,Shuo Chen,Kristian Kersting,Volker Tresp,Yunpu Ma*

Main category: cs.CV

Relevance: 75.0

TL;DR: METok是一种无需训练的多阶段事件驱动的令牌压缩框架，用于加速视频大语言模型（VLLMs）的推理，同时保持准确性。


<details>
  <summary>Details</summary>
Motivation: 处理长视频时，高计算需求和视觉数据冗余是主要挑战。METok旨在通过动态选择信息丰富的视觉令牌来解决这些问题。

Method: METok通过三个阶段逐步消除冗余视觉令牌：事件感知压缩、基于语义对齐和事件重要性的分层令牌修剪，以及解码阶段的KV缓存优化。

Result: 实验表明，METok在多种视频基准测试中实现了效率与准确性的最佳平衡，例如在LongVA-7B上实现了80.6%的FLOPs减少和93.5%的KV缓存内存节省。

Conclusion: METok提供了一种高效且准确的方法来加速VLLMs的推理，适用于长视频处理。

Abstract: Recent advances in Video Large Language Models (VLLMs) have significantly
enhanced their ability to understand video content. Nonetheless, processing
long videos remains challenging due to high computational demands and the
redundancy present in the visual data. In this work, we propose METok, a
training-free, Multi-stage Event-based Token compression framework designed to
accelerate VLLMs' inference while preserving accuracy. METok progressively
eliminates redundant visual tokens across three critical stages: (1)
event-aware compression during vision encoding, (2) hierarchical token pruning
in the prefilling stage based on semantic alignment and event importance, and
(3) a decoding-stage KV Cache optimization that further reduces memory
consumption. Our experiments on diverse video benchmarks demonstrate that METok
achieves an optimal trade-off between efficiency and accuracy by dynamically
selecting informative visual tokens. For instance, equipping LongVA-7B with
METok realizes an 80.6% FLOPs reduction and 93.5% KV Cache memory savings, all
while maintaining comparable or even superior accuracy.

</details>


### [124] [HaploOmni: Unified Single Transformer for Multimodal Video Understanding and Generation](https://arxiv.org/abs/2506.02975)
*Yicheng Xiao,Lin Song,Rui Yang,Cheng Cheng,Zunnan Xu,Zhaoyang Zhang,Yixiao Ge,Xiu Li,Ying Shan*

Main category: cs.CV

Relevance: 75.0

TL;DR: 提出了一种高效的多模态训练范式，构建了统一的单模型框架HaploOmni，通过多模态预热策略和特征预缩放技术，在有限训练成本下实现竞争性性能。


<details>
  <summary>Details</summary>
Motivation: 随着语言模型的进步，多模态理解与生成的统一成为研究热点，但现有方法多为分离组件，缺乏高效的单模型框架。

Method: 提出多模态预热策略和特征预缩放技术，结合多模态AdaLN，构建了统一的单模型HaploOmni。

Result: HaploOmni在多个图像和视频理解与生成任务中表现优异，训练成本较低。

Conclusion: HaploOmni展示了高效统一多模态模型的潜力，为未来研究提供了新方向。

Abstract: With the advancement of language models, unified multimodal understanding and
generation have made significant strides, with model architectures evolving
from separated components to unified single-model frameworks. This paper
explores an efficient training paradigm to build a single transformer for
unified multimodal understanding and generation. Specifically, we propose a
multimodal warmup strategy utilizing prior knowledge to extend capabilities. To
address cross-modal compatibility challenges, we introduce feature pre-scaling
and multimodal AdaLN techniques. Integrating the proposed technologies, we
present the HaploOmni, a new single multimodal transformer. With limited
training costs, HaploOmni achieves competitive performance across multiple
image and video understanding and generation benchmarks over advanced unified
models. All codes will be made public at https://github.com/Tencent/HaploVLM.

</details>


### [125] [Sparse-vDiT: Unleashing the Power of Sparse Attention to Accelerate Video Diffusion Transformers](https://arxiv.org/abs/2506.03065)
*Pengtao Chen,Xianfang Zeng,Maosen Zhao,Peng Ye,Mingzhu Shen,Wei Cheng,Gang Yu,Tao Chen*

Main category: cs.CV

Relevance: 75.0

TL;DR: 论文提出Sparse-vDiT框架，通过利用vDiT中的稀疏注意力模式，显著降低了视频生成的计算复杂度和推理延迟。


<details>
  <summary>Details</summary>
Motivation: 视频生成长序列任务受限于注意力机制的二次复杂度，导致高推理延迟。研究发现vDiT中存在三种稀疏模式，且这些模式与输入内容无关。

Method: 提出Sparse-vDiT框架，包括模式优化的稀疏核和离线稀疏扩散搜索算法，选择最优稀疏计算策略。

Result: 在多个vDiT模型中，Sparse-vDiT实现了2.09×至2.38×的理论FLOP减少和1.58×至1.85×的实际推理加速，同时保持高视觉保真度。

Conclusion: 研究表明vDiT中的潜在结构稀疏性可系统性地用于长视频合成。

Abstract: While Diffusion Transformers (DiTs) have achieved breakthroughs in video
generation, this long sequence generation task remains constrained by the
quadratic complexity of attention mechanisms, resulting in significant
inference latency. Through detailed analysis of attention maps in Video
Diffusion Transformer (vDiT), we identify three recurring sparsity patterns:
diagonal, multi-diagonal, and vertical-stripe structures. And even 3-6\%
attention heads can be skipped. Crucially, these patterns exhibit strong
layer-depth and head-position correlations but show limited dependence on the
input content. Leveraging these findings, we propose Sparse-vDiT, a sparsity
acceleration framework for vDiT comprising: 1) Pattern-optimized sparse kernels
that replace dense attention with computationally efficient implementations for
each identified sparsity pattern. 2) An offline sparse diffusion search
algorithm that selects the optimal sparse computation strategy per layer and
head via hardware-aware cost modeling. After determining the optimal
configuration, we fuse heads within the same layer that share the same
attention strategy, enhancing inference efficiency. Integrated into
state-of-the-art vDiT models (CogVideoX1.5, HunyuanVideo, and Wan2.1),
Sparse-vDiT achieves 2.09$\times$, 2.38$\times$, and 1.67$\times$ theoretical
FLOP reduction, and actual inference speedups of 1.76$\times$, 1.85$\times$,
and 1.58$\times$, respectively, while maintaining high visual fidelity, with
PSNR values reaching 24.13, 27.09, and 22.59. Our work demonstrates that latent
structural sparsity in vDiTs can be systematically exploited for long video
synthesis.

</details>


### [126] [SVGenius: Benchmarking LLMs in SVG Understanding, Editing and Generation](https://arxiv.org/abs/2506.03139)
*Siqi Chen,Xinyu Dong,Haolei Xu,Xingyu Wu,Fei Tang,Hang Zhang,Yuchen Yan,Linjuan Wu,Wenqi Zhang,Guiyang Hou,Yongliang Shen,Weiming Lu,Yueting Zhuang*

Main category: cs.CV

Relevance: 75.0

TL;DR: SVGenius是一个全面的SVG处理基准测试，涵盖2,377个查询，评估22个主流模型，揭示专有模型优于开源模型，但所有模型在复杂度增加时性能下降，推理增强训练比纯扩展更有效。


<details>
  <summary>Details</summary>
Motivation: 现有SVG处理基准测试覆盖范围有限、复杂度分层不足且评估范式分散，需要更全面的评估框架。

Method: SVGenius基于24个应用领域的真实数据，通过8个任务类别和18个指标评估模型，涵盖理解、编辑和生成三个维度。

Result: 专有模型显著优于开源模型，所有模型在复杂度增加时性能下降，推理增强训练比纯扩展更有效，风格迁移仍是最具挑战性的能力。

Conclusion: SVGenius为SVG处理提供了首个系统评估框架，为开发更强大的向量图形模型和自动化图形设计应用提供了关键见解。

Abstract: Large Language Models (LLMs) and Multimodal LLMs have shown promising
capabilities for SVG processing, yet existing benchmarks suffer from limited
real-world coverage, lack of complexity stratification, and fragmented
evaluation paradigms. We introduce SVGenius, a comprehensive benchmark
comprising 2,377 queries across three progressive dimensions: understanding,
editing, and generation. Built on real-world data from 24 application domains
with systematic complexity stratification, SVGenius evaluates models through 8
task categories and 18 metrics. We assess 22 mainstream models spanning
different scales, architectures, training paradigms, and accessibility levels.
Our analysis reveals that while proprietary models significantly outperform
open-source counterparts, all models exhibit systematic performance degradation
with increasing complexity, indicating fundamental limitations in current
approaches; however, reasoning-enhanced training proves more effective than
pure scaling for overcoming these limitations, though style transfer remains
the most challenging capability across all model types. SVGenius establishes
the first systematic evaluation framework for SVG processing, providing crucial
insights for developing more capable vector graphics models and advancing
automated graphic design applications. Appendix and supplementary materials
(including all data and code) are available at
https://zju-real.github.io/SVGenius.

</details>


### [127] [Cycle Consistency as Reward: Learning Image-Text Alignment without Human Preferences](https://arxiv.org/abs/2506.02095)
*Hyojin Bahng,Caroline Chan,Fredo Durand,Phillip Isola*

Main category: cs.CV

Relevance: 70.0

TL;DR: 论文提出了一种利用循环一致性作为监督信号的方法，用于语言与视觉的对齐任务，避免了传统依赖人类或AI偏好的高成本方法。


<details>
  <summary>Details</summary>
Motivation: 现有方法依赖人类或AI偏好，成本高且耗时，因此提出一种替代方案。

Method: 通过循环一致性（文本到图像再到文本，或图像到文本再到图像）计算相似性，构建偏好数据集并训练奖励模型。

Result: 奖励模型在详细描述任务上优于现有对齐指标，并提升了多种视觉语言任务和文本到图像生成的性能。

Conclusion: 循环一致性是一种高效且可扩展的监督信号，适用于多模态对齐任务。

Abstract: Learning alignment between language and vision is a fundamental challenge,
especially as multimodal data becomes increasingly detailed and complex.
Existing methods often rely on collecting human or AI preferences, which can be
costly and time-intensive. We propose an alternative approach that leverages
cycle consistency as a supervisory signal. Given an image and generated text,
we map the text back to image space using a text-to-image model and compute the
similarity between the original image and its reconstruction. Analogously, for
text-to-image generation, we measure the textual similarity between an input
caption and its reconstruction through the cycle. We use the cycle consistency
score to rank candidates and construct a preference dataset of 866K comparison
pairs. The reward model trained on our dataset outperforms state-of-the-art
alignment metrics on detailed captioning, with superior inference-time
scalability when used as a verifier for Best-of-N sampling. Furthermore,
performing DPO and Diffusion DPO using our dataset enhances performance across
a wide range of vision-language tasks and text-to-image generation. Our
dataset, model, and code are at https://cyclereward.github.io

</details>


### [128] [FuseLIP: Multimodal Embeddings via Early Fusion of Discrete Tokens](https://arxiv.org/abs/2506.03096)
*Christian Schlarmann,Francesco Croce,Nicolas Flammarion,Matthias Hein*

Main category: cs.CV

Relevance: 70.0

TL;DR: FuseLIP提出了一种基于单一Transformer的多模态嵌入架构，通过早期融合实现文本和图像特征的深度交互，优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有对比预训练方法无法直接处理多模态输入，需额外模块融合特征，限制了性能。

Method: 使用单一Transformer模型处理扩展的文本和图像标记词汇，实现早期融合。

Result: 在多模态任务（如VQA和文本引导图像检索）中表现优异，单模态任务与基线相当。

Conclusion: FuseLIP通过早期融合提升了多模态表示能力，为多模态嵌入提供了新思路。

Abstract: Contrastive language-image pre-training aligns the features of text-image
pairs in a common latent space via distinct encoders for each modality. While
this approach achieves impressive performance in several zero-shot tasks, it
cannot natively handle multimodal inputs, i.e., encoding image and text into a
single feature vector. As a remedy, it is common practice to use additional
modules to merge the features extracted by the unimodal encoders. In this work,
we present FuseLIP, an alternative architecture for multimodal embedding.
Leveraging recent progress in discrete image tokenizers, we propose to use a
single transformer model which operates on an extended vocabulary of text and
image tokens. This early fusion approach allows the different modalities to
interact at each depth of encoding and obtain richer representations compared
to common late fusion. We collect new datasets for multimodal pre-training and
evaluation, designing challenging tasks for multimodal encoder models. We show
that FuseLIP outperforms other approaches in multimodal embedding tasks such as
VQA and text-guided image transformation retrieval, while being comparable to
baselines on unimodal tasks.

</details>


### [129] [UniWorld: High-Resolution Semantic Encoders for Unified Visual Understanding and Generation](https://arxiv.org/abs/2506.03147)
*Bin Lin,Zongjian Li,Xinhua Cheng,Yuwei Niu,Yang Ye,Xianyi He,Shenghai Yuan,Wangbo Yu,Shaodong Wang,Yunyang Ge,Yatian Pang,Li Yuan*

Main category: cs.CV

Relevance: 70.0

TL;DR: 论文提出了一种名为UniWorld的统一生成框架，利用视觉语言模型和对比语义编码器的语义特征，在少量数据下超越现有模型BAGEL，并在图像理解和生成任务中表现优异。


<details>
  <summary>Details</summary>
Motivation: 现有统一模型在视觉语言理解和文本到图像生成方面表现良好，但在图像感知和操作任务上存在局限。GPT-4o-Image的成功启发了作者探索基于语义特征的统一框架。

Method: 提出UniWorld框架，利用视觉语言模型和对比语义编码器的语义特征，仅使用1%的BAGEL数据训练。

Result: UniWorld在图像编辑基准测试中超越BAGEL，同时在图像理解和生成任务中表现优异。

Conclusion: UniWorld展示了基于语义特征的统一框架的潜力，为图像感知和操作任务提供了高效解决方案。

Abstract: Although existing unified models deliver strong performance on
vision-language understanding and text-to-image generation, their models are
limited in exploring image perception and manipulation tasks, which are
urgently desired by users for wide applications. Recently, OpenAI released
their powerful GPT-4o-Image model for comprehensive image perception and
manipulation, achieving expressive capability and attracting community
interests. By observing the performance of GPT-4o-Image in our carefully
constructed experiments, we infer that GPT-4o-Image leverages features
extracted by semantic encoders instead of VAE, while VAEs are considered
essential components in many image manipulation models. Motivated by such
inspiring observations, we present a unified generative framework named
UniWorld based on semantic features provided by powerful visual-language models
and contrastive semantic encoders. As a result, we build a strong unified model
using only 1% amount of BAGEL's data, which consistently outperforms BAGEL on
image editing benchmarks. UniWorld also maintains competitive image
understanding and generation capabilities, achieving strong performance across
multiple image perception tasks. We fully open-source our models, including
model weights, training and evaluation scripts, and datasets.

</details>


### [130] [OASIS: Online Sample Selection for Continual Visual Instruction Tuning](https://arxiv.org/abs/2506.02011)
*Minjae Lee,Minhyuk Seo,Tingyu Qu,Tinne Tuytelaars,Jonghyun Choi*

Main category: cs.CV

Relevance: 60.0

TL;DR: 论文提出了一种自适应在线样本选择方法OASIS，用于持续视觉指令调优（CVIT）场景，动态调整每批样本选择以减少冗余并适应分布变化。


<details>
  <summary>Details</summary>
Motivation: 在CVIT场景中，多模态数据以流式方式持续到达，传统依赖预训练参考模型的数据选择方法不适用，且固定样本选择方法无法应对分布变化。

Method: OASIS通过动态调整每批样本数量（基于相对批次间信息量）和迭代更新选择分数来最小化冗余。

Result: 实验表明，OASIS仅使用25%数据即可达到全数据训练的性能，并优于现有方法。

Conclusion: OASIS为CVIT提供了一种高效且自适应的样本选择解决方案。

Abstract: In continual visual instruction tuning (CVIT) scenarios, where multi-modal
data continuously arrive in an online streaming manner, training delays from
large-scale data significantly hinder real-time adaptation. While existing data
selection strategies reduce training overheads, they rely on pre-trained
reference models, which are impractical in CVIT setups due to unknown future
data. Recent reference model-free online sample selection methods address this
issue but typically select a fixed number of samples per batch (e.g., top-k),
causing them to suffer from distribution shifts where informativeness varies
across batches. To address these limitations, we propose OASIS, an adaptive
online sample selection approach for CVIT that: (1) dynamically adjusts
selected samples per batch based on relative inter-batch informativeness, and
(2) minimizes redundancy of selected samples through iterative selection score
updates. Empirical results across various MLLMs, such as LLaVA-1.5 and
Qwen-VL-2.5, show that OASIS achieves comparable performance to full-data
training using only 25% of the data and outperforms the state-of-the-art.

</details>


### [131] [Leveraging Large Language Models in Visual Speech Recognition: Model Scaling, Context-Aware Decoding, and Iterative Polishing](https://arxiv.org/abs/2506.02012)
*Zehua Liu,Xiaolou Li,Li Guo,Lantian Li,Dong Wang*

Main category: cs.CV

Relevance: 60.0

TL;DR: 论文探讨了如何利用大型语言模型（LLMs）提升视觉语音识别（VSR）性能，提出了三种方法：规模测试、上下文感知解码和迭代优化。


<details>
  <summary>Details</summary>
Motivation: 尽管LLMs在VSR中表现出潜力，但其应用尚未充分研究，本文旨在填补这一空白。

Method: 1) 规模测试：研究LLM规模对VSR性能的影响；2) 上下文感知解码：通过上下文文本引导解码；3) 迭代优化：逐步减少识别错误。

Result: 实验表明，这些方法显著提升了VSR性能。

Conclusion: LLMs在VSR任务中具有巨大潜力，通过系统设计可以充分发挥其优势。

Abstract: Visual Speech Recognition (VSR) transcribes speech by analyzing lip
movements. Recently, Large Language Models (LLMs) have been integrated into VSR
systems, leading to notable performance improvements. However, the potential of
LLMs has not been extensively studied, and how to effectively utilize LLMs in
VSR tasks remains unexplored. This paper systematically explores how to better
leverage LLMs for VSR tasks and provides three key contributions: (1) Scaling
Test: We study how the LLM size affects VSR performance, confirming a scaling
law in the VSR task. (2) Context-Aware Decoding: We add contextual text to
guide the LLM decoding, improving recognition accuracy. (3) Iterative
Polishing: We propose iteratively refining LLM outputs, progressively reducing
recognition errors. Extensive experiments demonstrate that by these designs,
the great potential of LLMs can be largely harnessed, leading to significant
VSR performance improvement.

</details>


### [132] [Technical Report for Ego4D Long-Term Action Anticipation Challenge 2025](https://arxiv.org/abs/2506.02550)
*Qiaohui Chu,Haoyu Zhang,Yisen Feng,Meng Liu,Weili Guan,Yaowei Wang,Liqiang Nie*

Main category: cs.CV

Relevance: 60.0

TL;DR: 提出了一种用于Ego4D长期动作预测任务的三阶段框架，结合视觉编码器、Transformer和微调的大语言模型（LLM），在CVPR 2025挑战赛中取得第一名。


<details>
  <summary>Details</summary>
Motivation: 受基础模型进展启发，解决长期动作预测任务，提升预测准确性。

Method: 三阶段框架：1) 视觉特征提取；2) 使用Transformer预测动词和名词；3) 将结果输入微调的LLM生成未来动作序列。

Result: 在CVPR 2025挑战赛中取得第一名，建立了新的SOTA。

Conclusion: 该框架在长期动作预测任务中表现优异，结合视觉和语言模型的方法有效。

Abstract: In this report, we present a novel three-stage framework developed for the
Ego4D Long-Term Action Anticipation (LTA) task. Inspired by recent advances in
foundation models, our method consists of three stages: feature extraction,
action recognition, and long-term action anticipation. First, visual features
are extracted using a high-performance visual encoder. The features are then
fed into a Transformer to predict verbs and nouns, with a verb-noun
co-occurrence matrix incorporated to enhance recognition accuracy. Finally, the
predicted verb-noun pairs are formatted as textual prompts and input into a
fine-tuned large language model (LLM) to anticipate future action sequences.
Our framework achieves first place in this challenge at CVPR 2025, establishing
a new state-of-the-art in long-term action prediction. Our code will be
released at https://github.com/CorrineQiu/Ego4D-LTA-Challenge-2025.

</details>


### [133] [SurgVLM: A Large Vision-Language Model and Systematic Evaluation Benchmark for Surgical Intelligence](https://arxiv.org/abs/2506.02555)
*Zhitao Zeng,Zhu Zhuo,Xiaojun Jia,Erli Zhang,Junde Wu,Jiaan Zhang,Yuxuan Wang,Chang Han Low,Jian Jiang,Zilong Zheng,Xiaochun Cao,Yutong Ban,Qi Dou,Yang Liu,Yueming Jin*

Main category: cs.CV

Relevance: 60.0

TL;DR: SurgVLM是一种专为手术智能设计的大型视觉语言基础模型，通过构建大规模多模态手术数据库SurgVLM-DB和基准测试SurgVLM-Bench，解决了现有通用模型在手术领域的不足。


<details>
  <summary>Details</summary>
Motivation: 手术智能需要特定的视觉感知、时间分析和推理能力，现有通用视觉语言模型因缺乏领域监督和大规模高质量数据而无法满足需求。

Method: 构建SurgVLM-DB数据库（180万帧图像和779万对话），统一23个公共数据集，并基于Qwen2.5-VL进行指令调优，开发了3个SurgVLM变体（7B、32B、72B）。

Result: SurgVLM在SurgVLM-Bench上进行了评估，并与14种主流商业VLM（如GPT-4o、Gemini 2.0 Flash）进行了对比。

Conclusion: SurgVLM填补了手术智能领域的基础模型空白，为多任务手术智能提供了通用解决方案。

Abstract: Foundation models have achieved transformative success across biomedical
domains by enabling holistic understanding of multimodal data. However, their
application in surgery remains underexplored. Surgical intelligence presents
unique challenges - requiring surgical visual perception, temporal analysis,
and reasoning. Existing general-purpose vision-language models fail to address
these needs due to insufficient domain-specific supervision and the lack of a
large-scale high-quality surgical database. To bridge this gap, we propose
SurgVLM, one of the first large vision-language foundation models for surgical
intelligence, where this single universal model can tackle versatile surgical
tasks. To enable this, we construct a large-scale multimodal surgical database,
SurgVLM-DB, comprising over 1.81 million frames with 7.79 million
conversations, spanning more than 16 surgical types and 18 anatomical
structures. We unify and reorganize 23 public datasets across 10 surgical
tasks, followed by standardizing labels and doing hierarchical vision-language
alignment to facilitate comprehensive coverage of gradually finer-grained
surgical tasks, from visual perception, temporal analysis, to high-level
reasoning. Building upon this comprehensive dataset, we propose SurgVLM, which
is built upon Qwen2.5-VL, and undergoes instruction tuning to 10+ surgical
tasks. We further construct a surgical multimodal benchmark, SurgVLM-Bench, for
method evaluation. SurgVLM-Bench consists of 6 popular and widely-used datasets
in surgical domain, covering several crucial downstream tasks. Based on
SurgVLM-Bench, we evaluate the performance of our SurgVLM (3 SurgVLM variants:
SurgVLM-7B, SurgVLM-32B, and SurgVLM-72B), and conduct comprehensive
comparisons with 14 mainstream commercial VLMs (e.g., GPT-4o, Gemini 2.0 Flash,
Qwen2.5-Max).

</details>


### [134] [Kernel-based Unsupervised Embedding Alignment for Enhanced Visual Representation in Vision-language Models](https://arxiv.org/abs/2506.02557)
*Shizhan Gong,Yankai Jiang,Qi Dou,Farzan Farnia*

Main category: cs.CV

Relevance: 60.0

TL;DR: 提出了一种基于核的方法，将CLIP的视觉表示与DINOv2对齐，提升感知能力，同时保持与文本嵌入的兼容性。


<details>
  <summary>Details</summary>
Motivation: CLIP在多模态大语言模型（MLLMs）中表现不佳，尤其是在细粒度感知方面，而DINOv2在图像细节捕捉上表现优异。

Method: 采用核方法对齐CLIP和DINOv2的视觉表示，设计高效随机优化的对齐目标。

Result: 对齐后的视觉编码器在零样本目标识别、细粒度空间推理和定位任务中表现显著提升，下游MLLMs性能也增强。

Conclusion: 该方法有效提升了CLIP的感知能力，同时保持了与文本嵌入的兼容性，为MLLMs提供了更好的视觉表示。

Abstract: Vision-language models, such as CLIP, have achieved significant success in
aligning visual and textual representations, becoming essential components of
many multi-modal large language models (MLLMs) like LLaVA and OpenFlamingo.
However, numerous studies have identified CLIP's limited fine-grained
perception as a critical drawback, leading to substantial failures in
downstream MLLMs. In contrast, vision-centric foundation models like DINOv2
demonstrate remarkable capabilities in capturing fine details from images. In
this work, we propose a novel kernel-based method to align CLIP's visual
representation with that of DINOv2, ensuring that the resulting embeddings
maintain compatibility with text embeddings while enhancing perceptual
capabilities. Our alignment objective is designed for efficient stochastic
optimization. Following this image-only alignment fine-tuning, the visual
encoder retains compatibility with the frozen text encoder and exhibits
significant improvements in zero-shot object recognition, fine-grained spatial
reasoning, and localization. By integrating the aligned visual encoder,
downstream MLLMs also demonstrate enhanced performance.

</details>


### [135] [ControlMambaIR: Conditional Controls with State-Space Model for Image Restoration](https://arxiv.org/abs/2506.02633)
*Cheng Yang,Lijing Liang,Zhixun Su*

Main category: cs.CV

Relevance: 60.0

TL;DR: ControlMambaIR结合Mamba架构和扩散模型，提出了一种新的图像恢复方法，在去雨、去模糊和去噪任务中表现优异。


<details>
  <summary>Details</summary>
Motivation: 解决图像恢复任务中的感知挑战，提升图像生成过程的控制和优化。

Method: 集成Mamba网络架构与扩散模型，通过条件网络实现精细化控制。

Result: 在多个基准数据集上表现优于现有方法，尤其在感知质量指标（LPIPS、FID）上显著提升。

Conclusion: Mamba架构作为条件控制网络优于CNN和注意力机制，ControlMambaIR在图像恢复任务中具有灵活性和有效性。

Abstract: This paper proposes ControlMambaIR, a novel image restoration method designed
to address perceptual challenges in image deraining, deblurring, and denoising
tasks. By integrating the Mamba network architecture with the diffusion model,
the condition network achieves refined conditional control, thereby enhancing
the control and optimization of the image generation process. To evaluate the
robustness and generalization capability of our method across various image
degradation conditions, extensive experiments were conducted on several
benchmark datasets, including Rain100H, Rain100L, GoPro, and SSID. The results
demonstrate that our proposed approach consistently surpasses existing methods
in perceptual quality metrics, such as LPIPS and FID, while maintaining
comparable performance in image distortion metrics, including PSNR and SSIM,
highlighting its effectiveness and adaptability. Notably, ablation experiments
reveal that directly noise prediction in the diffusion process achieves better
performance, effectively balancing noise suppression and detail preservation.
Furthermore, the findings indicate that the Mamba architecture is particularly
well-suited as a conditional control network for diffusion models,
outperforming both CNN- and Attention-based approaches in this context.
Overall, these results highlight the flexibility and effectiveness of
ControlMambaIR in addressing a range of image restoration perceptual
challenges.

</details>


### [136] [Iterative Self-Improvement of Vision Language Models for Image Scoring and Self-Explanation](https://arxiv.org/abs/2506.02708)
*Naoto Tanji,Toshihiko Yamasaki*

Main category: cs.CV

Relevance: 60.0

TL;DR: 本文提出了一种新的视觉语言模型（VLM）训练方法，用于生成图像评分及其自然语言解释，无需依赖外部数据或模型。通过自训练和直接偏好优化，提高了评分准确性和解释一致性。


<details>
  <summary>Details</summary>
Motivation: 图像评分在许多实际应用中至关重要，但需要理解模型的判断依据以建立信任。本文旨在通过生成评分及其解释来提升模型的可解释性。

Method: 利用图像评分数据集和指令调优的VLM进行自训练，生成文本解释。通过直接偏好优化在两个数据集上迭代训练，合并数据集以提升评分和解释的一致性。

Result: 方法提高了图像评分的准确性，并生成了更一致的文本解释。

Conclusion: 该方法为提升视觉语言模型的解释性和评分性能提供了一种有效途径。

Abstract: Image scoring is a crucial task in numerous real-world applications. To trust
a model's judgment, understanding its rationale is essential. This paper
proposes a novel training method for Vision Language Models (VLMs) to generate
not only image scores but also corresponding justifications in natural
language. Leveraging only an image scoring dataset and an instruction-tuned
VLM, our method enables self-training, utilizing the VLM's generated text
without relying on external data or models. In addition, we introduce a simple
method for creating a dataset designed to improve alignment between predicted
scores and their textual justifications. By iteratively training the model with
Direct Preference Optimization on two distinct datasets and merging them, we
can improve both scoring accuracy and the coherence of generated explanations.

</details>


### [137] [FlySearch: Exploring how vision-language models explore](https://arxiv.org/abs/2506.02896)
*Adam Pardyl,Dominik Matuszek,Mateusz Przebieracz,Marek Cygan,Bartosz Zieliński,Maciej Wołczyk*

Main category: cs.CV

Relevance: 60.0

TL;DR: 论文探讨了视觉语言模型（VLMs）在复杂3D环境中进行目标搜索和导航的能力，发现当前最先进的VLMs表现不佳，并分析了失败原因。


<details>
  <summary>Details</summary>
Motivation: 研究VLMs在真实、复杂环境中的探索能力，填补其在主动目标驱动任务中的性能空白。

Method: 提出FlySearch，一个3D户外逼真环境，定义三种难度场景，评估VLMs表现，并通过微调尝试改进。

Result: VLMs在简单任务中表现不可靠，与人类差距随任务难度增加；部分问题可通过微调缓解。

Conclusion: VLMs在复杂探索任务中仍有局限性，需进一步改进；公开了基准和代码。

Abstract: The real world is messy and unstructured. Uncovering critical information
often requires active, goal-driven exploration. It remains to be seen whether
Vision-Language Models (VLMs), which recently emerged as a popular zero-shot
tool in many difficult tasks, can operate effectively in such conditions. In
this paper, we answer this question by introducing FlySearch, a 3D, outdoor,
photorealistic environment for searching and navigating to objects in complex
scenes. We define three sets of scenarios with varying difficulty and observe
that state-of-the-art VLMs cannot reliably solve even the simplest exploration
tasks, with the gap to human performance increasing as the tasks get harder. We
identify a set of central causes, ranging from vision hallucination, through
context misunderstanding, to task planning failures, and we show that some of
them can be addressed by finetuning. We publicly release the benchmark,
scenarios, and the underlying codebase.

</details>


### [138] [Towards Auto-Annotation from Annotation Guidelines: A Benchmark through 3D LiDAR Detection](https://arxiv.org/abs/2506.02914)
*Yechi Ma,Wei Hua,Shu Kong*

Main category: cs.CV

Relevance: 60.0

TL;DR: 论文提出了一个名为AnnoGuide的新基准，旨在通过专家定义的标注指南自动完成数据标注，减少人工标注的需求。以nuScenes数据集为例，研究了一种多模态少样本3D检测方法，利用基础模型（FMs）提升性能。


<details>
  <summary>Details</summary>
Motivation: 数据标注是机器学习应用中的关键但繁琐的步骤，本研究旨在通过自动化方法减少人工标注的成本和劳动。

Method: 采用基于基础模型的流水线方法，包括2D目标检测、3D投影和LiDAR点聚类，逐步优化关键组件。

Result: 性能显著提升，3D检测mAP从12.1提高到21.9，但问题仍具挑战性。

Conclusion: AnnoGuide是一个开放且具有挑战性的问题，亟需发展基于LiDAR的基础模型。

Abstract: A crucial yet under-appreciated prerequisite in machine learning solutions
for real-applications is data annotation: human annotators are hired to
manually label data according to detailed, expert-crafted guidelines. This is
often a laborious, tedious, and costly process. To study methods for
facilitating data annotation, we introduce a new benchmark AnnoGuide:
Auto-Annotation from Annotation Guidelines. It aims to evaluate automated
methods for data annotation directly from expert-defined annotation guidelines,
eliminating the need for manual labeling. As a case study, we repurpose the
well-established nuScenes dataset, commonly used in autonomous driving
research, which provides comprehensive annotation guidelines for labeling LiDAR
point clouds with 3D cuboids across 18 object classes. These guidelines include
a few visual examples and textual descriptions, but no labeled 3D cuboids in
LiDAR data, making this a novel task of multi-modal few-shot 3D detection
without 3D annotations. The advances of powerful foundation models (FMs) make
AnnoGuide especially timely, as FMs offer promising tools to tackle its
challenges. We employ a conceptually straightforward pipeline that (1) utilizes
open-source FMs for object detection and segmentation in RGB images, (2)
projects 2D detections into 3D using known camera poses, and (3) clusters LiDAR
points within the frustum of each 2D detection to generate a 3D cuboid.
Starting with a non-learned solution that leverages off-the-shelf FMs, we
progressively refine key components and achieve significant performance
improvements, boosting 3D detection mAP from 12.1 to 21.9! Nevertheless, our
results highlight that AnnoGuide remains an open and challenging problem,
underscoring the urgent need for developing LiDAR-based FMs. We release our
code and models at GitHub: https://annoguide.github.io/annoguide3Dbenchmark

</details>


### [139] [InterMamba: Efficient Human-Human Interaction Generation with Adaptive Spatio-Temporal Mamba](https://arxiv.org/abs/2506.03084)
*Zizhao Wu,Yingying Sun,Yiming Chen,Xiaoling Gu,Ruyu Liu,Jiazhou Chen*

Main category: cs.CV

Relevance: 60.0

TL;DR: 提出了一种基于Mamba框架的高效人-人交互生成方法，解决了Transformer架构在可扩展性和效率上的问题。


<details>
  <summary>Details</summary>
Motivation: 现有方法依赖Transformer架构，面临可扩展性和效率挑战，需一种更高效的解决方案。

Method: 采用自适应时空Mamba框架，结合两个并行SSM分支和自适应机制，开发了自适应和交叉自适应时空Mamba模块。

Result: 在两个交互数据集上取得SOTA结果，参数规模仅66M（InterGen的36%），推理速度0.57秒（InterGen的46%）。

Conclusion: Mamba框架在高效性和性能上优于Transformer，适用于长序列依赖和实时反馈需求。

Abstract: Human-human interaction generation has garnered significant attention in
motion synthesis due to its vital role in understanding humans as social
beings. However, existing methods typically rely on transformer-based
architectures, which often face challenges related to scalability and
efficiency. To address these issues, we propose a novel, efficient human-human
interaction generation method based on the Mamba framework, designed to meet
the demands of effectively capturing long-sequence dependencies while providing
real-time feedback. Specifically, we introduce an adaptive spatio-temporal
Mamba framework that utilizes two parallel SSM branches with an adaptive
mechanism to integrate the spatial and temporal features of motion sequences.
To further enhance the model's ability to capture dependencies within
individual motion sequences and the interactions between different individual
sequences, we develop two key modules: the self-adaptive spatio-temporal Mamba
module and the cross-adaptive spatio-temporal Mamba module, enabling efficient
feature learning. Extensive experiments demonstrate that our method achieves
state-of-the-art results on two interaction datasets with remarkable quality
and efficiency. Compared to the baseline method InterGen, our approach not only
improves accuracy but also requires a minimal parameter size of just 66M ,only
36% of InterGen's, while achieving an average inference speed of 0.57 seconds,
which is 46% of InterGen's execution time.

</details>


### [140] [EgoVLM: Policy Optimization for Egocentric Video Understanding](https://arxiv.org/abs/2506.03097)
*Ashwin Vinod,Shrey Pandit,Aditya Vavre,Linshen Liu*

Main category: cs.CV

Relevance: 60.0

TL;DR: EgoVLM是一个针对第一人称视频的视觉语言模型，通过强化学习（GRPO）优化，在EgoSchema基准上表现优于通用VLMs，并提升了可解释性。


<details>
  <summary>Details</summary>
Motivation: 解决第一人称视频中的视觉理解和时空推理需求，适用于可穿戴设备和自主代理等场景。

Method: 使用Group Relative Policy Optimization（GRPO）进行强化学习微调，无需监督微调阶段，并引入关键帧奖励机制。

Result: EgoVLM-3B在EgoSchema基准上比Qwen2.5-VL 3B和7B分别高出14.33和13.87个准确点。

Conclusion: EgoVLM通过强化学习和关键帧奖励提升了性能和可解释性，为未来研究提供了新方向。

Abstract: Emerging embodied AI applications, such as wearable cameras and autonomous
agents, have underscored the need for robust reasoning from first person video
streams. We introduce EgoVLM, a vision-language model specifically designed to
integrate visual comprehension and spatial-temporal reasoning within egocentric
video contexts. EgoVLM is fine-tuned via Group Relative Policy Optimization
(GRPO), a reinforcement learning method adapted to align model outputs with
human-like reasoning steps. Following DeepSeek R1-Zero's approach, we directly
tune using RL without any supervised fine-tuning phase on chain-of-thought
(CoT) data. We evaluate EgoVLM on egocentric video question answering
benchmarks and show that domain-specific training substantially improves
performance over general-purpose VLMs. Our EgoVLM-3B, trained exclusively on
non-CoT egocentric data, outperforms the base Qwen2.5-VL 3B and 7B models by
14.33 and 13.87 accuracy points on the EgoSchema benchmark, respectively. By
explicitly generating reasoning traces, EgoVLM enhances interpretability,
making it well-suited for downstream applications. Furthermore, we introduce a
novel keyframe-based reward that incorporates salient frame selection to guide
reinforcement learning optimization. This reward formulation opens a promising
avenue for future exploration in temporally grounded egocentric reasoning.

</details>


### [141] [Revisiting Continuity of Image Tokens for Cross-domain Few-shot Learning](https://arxiv.org/abs/2506.03110)
*Shuai Yi,Yixiong Zou,Yuhua Li,Ruixuan Li*

Main category: cs.CV

Relevance: 60.0

TL;DR: 论文研究了Vision Transformer（ViT）在跨域少样本学习（CDFSL）中的表现，发现图像标记的连续性对ViT的泛化能力有重要影响，并提出了一种新方法来优化性能。


<details>
  <summary>Details</summary>
Motivation: ViT在大规模预训练中表现优异，但在下游远域任务中面临挑战，尤其是在数据稀缺的情况下。作者发现图像标记的连续性对ViT的泛化能力有显著影响，但这一现象被当前研究忽视。

Method: 作者通过实验发现，破坏图像标记的连续性会导致源域性能下降，但对目标域影响较小。基于此，他们提出了一种新方法，通过破坏连续性来鼓励模型依赖更小的空间模式。

Result: 实验表明，该方法有效减少了域差距，并在CDFSL任务中超越了现有技术。

Conclusion: 图像标记的连续性在ViT的跨域泛化中起关键作用，破坏连续性可以优化模型在远域任务中的表现。

Abstract: Vision Transformer (ViT) has achieved remarkable success due to its
large-scale pretraining on general domains, but it still faces challenges when
applying it to downstream distant domains that have only scarce training data,
which gives rise to the Cross-Domain Few-Shot Learning (CDFSL) task. Inspired
by Self-Attention's insensitivity to token orders, we find an interesting
phenomenon neglected in current works: disrupting the continuity of image
tokens (i.e., making pixels not smoothly transited across patches) in ViT leads
to a noticeable performance decline in the general (source) domain but only a
marginal decrease in downstream target domains. This questions the role of
image tokens' continuity in ViT's generalization under large domain gaps. In
this paper, we delve into this phenomenon for an interpretation. We find
continuity aids ViT in learning larger spatial patterns, which are harder to
transfer than smaller ones, enlarging domain distances. Meanwhile, it implies
that only smaller patterns within each patch could be transferred under extreme
domain gaps. Based on this interpretation, we further propose a simple yet
effective method for CDFSL that better disrupts the continuity of image tokens,
encouraging the model to rely less on large patterns and more on smaller ones.
Extensive experiments show the effectiveness of our method in reducing domain
gaps and outperforming state-of-the-art works. Codes and models are available
at https://github.com/shuaiyi308/ReCIT.

</details>


### [142] [Research on Driving Scenario Technology Based on Multimodal Large Lauguage Model Optimization](https://arxiv.org/abs/2506.02014)
*Wang Mengjie,Zhu Huiping,Li Jian,Shi Wenxiu,Zhang Song*

Main category: cs.CV

Relevance: 50.0

TL;DR: 本文提出了一种针对驾驶场景的多模态模型优化方法，涵盖动态提示优化、数据集构建、模型训练和部署，显著提升了模型在关键任务中的准确性和资源利用率。


<details>
  <summary>Details</summary>
Motivation: 自动驾驶和辅助驾驶技术对复杂驾驶场景的理解能力提出了更高要求，多模态通用大模型成为解决方案，但在垂直领域应用中存在数据收集、模型训练和部署优化等困难。

Method: 方法包括动态提示优化（根据输入图像调整提示）、数据集构建（结合真实与合成数据）、模型训练（知识蒸馏、动态微调和量化）以及部署优化。

Result: 实验结果表明，该方法显著提升了模型在关键任务中的准确性，同时实现了高效的资源利用。

Conclusion: 该系统性优化方法为驾驶场景感知技术的实际应用提供了有力支持。

Abstract: With the advancement of autonomous and assisted driving technologies, higher
demands are placed on the ability to understand complex driving scenarios.
Multimodal general large models have emerged as a solution for this challenge.
However, applying these models in vertical domains involves difficulties such
as data collection, model training, and deployment optimization. This paper
proposes a comprehensive method for optimizing multimodal models in driving
scenarios, including cone detection, traffic light recognition, speed limit
recommendation, and intersection alerts. The method covers key aspects such as
dynamic prompt optimization, dataset construction, model training, and
deployment. Specifically, the dynamic prompt optimization adjusts the prompts
based on the input image content to focus on objects affecting the ego vehicle,
enhancing the model's task-specific focus and judgment capabilities. The
dataset is constructed by combining real and synthetic data to create a
high-quality and diverse multimodal training dataset, improving the model's
generalization in complex driving environments. In model training, advanced
techniques like knowledge distillation, dynamic fine-tuning, and quantization
are integrated to reduce storage and computational costs while boosting
performance. Experimental results show that this systematic optimization method
not only significantly improves the model's accuracy in key tasks but also
achieves efficient resource utilization, providing strong support for the
practical application of driving scenario perception technologies.

</details>


### [143] [RelationAdapter: Learning and Transferring Visual Relation with Diffusion Transformers](https://arxiv.org/abs/2506.02528)
*Yan Gong,Yiren Song,Yicheng Li,Chenglin Li,Yin Zhang*

Main category: cs.CV

Relevance: 50.0

TL;DR: 论文提出了一种基于视觉提示的图像编辑新范式RelationAdapter，利用源-目标图像对提取和转移内容感知编辑意图，并通过Relation252K数据集验证其有效性。


<details>
  <summary>Details</summary>
Motivation: 现有单参考方法在非刚性变换上表现不佳，论文旨在解决这一问题，提升视觉提示驱动的图像编辑能力。

Method: 提出RelationAdapter轻量模块，结合Diffusion Transformer（DiT）模型，从少量示例中捕捉并应用视觉变换。

Result: RelationAdapter显著提升了模型对编辑意图的理解和转移能力，生成质量和编辑性能均有明显提升。

Conclusion: RelationAdapter为视觉提示驱动的图像编辑提供了一种高效且通用的解决方案。

Abstract: Inspired by the in-context learning mechanism of large language models
(LLMs), a new paradigm of generalizable visual prompt-based image editing is
emerging. Existing single-reference methods typically focus on style or
appearance adjustments and struggle with non-rigid transformations. To address
these limitations, we propose leveraging source-target image pairs to extract
and transfer content-aware editing intent to novel query images. To this end,
we introduce RelationAdapter, a lightweight module that enables Diffusion
Transformer (DiT) based models to effectively capture and apply visual
transformations from minimal examples. We also introduce Relation252K, a
comprehensive dataset comprising 218 diverse editing tasks, to evaluate model
generalization and adaptability in visual prompt-driven scenarios. Experiments
on Relation252K show that RelationAdapter significantly improves the model's
ability to understand and transfer editing intent, leading to notable gains in
generation quality and overall editing performance.

</details>


### [144] [Small Aid, Big Leap: Efficient Test-Time Adaptation for Vision-Language Models with AdaptNet](https://arxiv.org/abs/2506.02671)
*Xiao Chen,Jiazhen Huang,Qinting Jiang,Fanding Huang,Xianghua Fu,Jingyan Jiang,Zhi Wang*

Main category: cs.CV

Relevance: 50.0

TL;DR: SAIL是一种基于适配器的高效测试时适应（TTA）框架，通过轻量级AdaptNet和梯度感知重置策略，显著降低计算成本并提升性能。


<details>
  <summary>Details</summary>
Motivation: 现有TTA方法计算成本高且扩展性差，SAIL旨在通过轻量级设计和批量处理解决这些问题。

Method: SAIL结合冻结预训练VLM和AdaptNet，通过置信度插值权重生成预测，并利用梯度漂移指示器（GDI）动态重置AdaptNet以避免灾难性遗忘。

Result: SAIL在多个基准测试中实现SOTA性能，同时保持低计算成本。

Conclusion: SAIL是一种高效、可扩展的TTA框架，适用于实际部署。

Abstract: Test-time adaptation (TTA) has emerged as a critical technique for enhancing
the generalization capability of vision-language models (VLMs) during
inference. However, existing approaches often incur substantial computational
costs and exhibit poor scalability, primarily due to sample-wise adaptation
granularity and reliance on costly auxiliary designs such as data augmentation.
To address these limitations, we introduce SAIL (Small Aid, Big Leap), a novel
adapter-based TTA framework that leverages a lightweight, learnable AdaptNet to
enable efficient and scalable model adaptation. As SAIL's core, a frozen
pre-trained VLM collaborates with AdaptNet through a confidence-based
interpolation weight, generating robust predictions during inference. These
predictions serve as self-supervised targets to align AdaptNet's outputs
through efficient batch-wise processing, dramatically reducing computational
costs without modifying the VLM or requiring memory caches. To mitigate
catastrophic forgetting during continual adaptation, we propose a
gradient-aware reset strategy driven by a gradient drift indicator (GDI), which
dynamically detects domain transitions and strategically resets AdaptNet for
stable adaptation. Extensive experiments across diverse benchmarks on two
scenarios demonstrate that SAIL achieves state-of-the-art performance while
maintaining low computational costs. These results highlight SAIL's
effectiveness, efficiency and scalability for real-world deployment. The code
will be released upon acceptance.

</details>


### [145] [Towards Geometry Problem Solving in the Large Model Era: A Survey](https://arxiv.org/abs/2506.02690)
*Yurui Zhao,Xiang Wang,Jiahong Liu,Irwin King,Zhitao Huang*

Main category: cs.CV

Relevance: 50.0

TL;DR: 该论文综述了几何问题解决（GPS）领域的进展，提出了统一的解析范式，并指出了未来研究方向，如自动基准生成和可解释的神经符号集成。


<details>
  <summary>Details</summary>
Motivation: 几何问题解决在人工智能中具有重要意义，但由于空间理解和逻辑推理的双重需求，自动化仍具挑战性。本文旨在系统整合GPS领域的进展并指导未来研究。

Method: 通过三个核心维度（基准构建、文本与图表解析、推理范式）系统综述GPS进展，并提出统一的分析范式。

Result: 总结了当前GPS领域的局限性，并提出了未来研究方向，如自动基准生成和神经符号集成。

Conclusion: GPS领域需进一步整合方法、基准和评估框架，以实现人类水平的几何推理。

Abstract: Geometry problem solving (GPS) represents a critical frontier in artificial
intelligence, with profound applications in education, computer-aided design,
and computational graphics. Despite its significance, automating GPS remains
challenging due to the dual demands of spatial understanding and rigorous
logical reasoning. Recent advances in large models have enabled notable
breakthroughs, particularly for SAT-level problems, yet the field remains
fragmented across methodologies, benchmarks, and evaluation frameworks. This
survey systematically synthesizes GPS advancements through three core
dimensions: (1) benchmark construction, (2) textual and diagrammatic parsing,
and (3) reasoning paradigms. We further propose a unified analytical paradigm,
assess current limitations, and identify emerging opportunities to guide future
research toward human-level geometric reasoning, including automated benchmark
generation and interpretable neuro-symbolic integration.

</details>


### [146] [DFBench: Benchmarking Deepfake Image Detection Capability of Large Multimodal Models](https://arxiv.org/abs/2506.03007)
*Jiarui Wang,Huiyu Duan,Juntong Wang,Ziheng Jia,Woo Yi Yang,Xiaorong Zhu,Yu Zhao,Jiaying Qian,Yuke Xing,Guangtao Zhai,Xiongkuo Min*

Main category: cs.CV

Relevance: 50.0

TL;DR: 论文提出了DFBench，一个大规模深度伪造检测基准，包含多样化的图像数据，并基于此提出了MoA-DF方法，利用多模态大模型（LMMs）进行检测，取得了最先进的性能。


<details>
  <summary>Details</summary>
Motivation: 随着生成模型的快速发展，AI生成图像的逼真度显著提升，验证数字内容真实性变得更具挑战性。现有的深度伪造检测方法依赖的数据集往往无法跟上AI生成内容的复杂性和逼真度。

Method: 提出了DFBench基准，包含54万张图像，覆盖真实、AI编辑和AI生成内容，并基于12种最新生成模型。进一步提出了MoA-DF方法，通过结合多个LMMs的概率策略进行检测。

Result: MoA-DF在深度伪造检测中取得了最先进的性能，验证了利用LMMs进行检测的有效性。

Conclusion: DFBench和MoA-DF为深度伪造检测提供了新的基准和方法，展示了LMMs在该领域的潜力。

Abstract: With the rapid advancement of generative models, the realism of AI-generated
images has significantly improved, posing critical challenges for verifying
digital content authenticity. Current deepfake detection methods often depend
on datasets with limited generation models and content diversity that fail to
keep pace with the evolving complexity and increasing realism of the
AI-generated content. Large multimodal models (LMMs), widely adopted in various
vision tasks, have demonstrated strong zero-shot capabilities, yet their
potential in deepfake detection remains largely unexplored. To bridge this gap,
we present \textbf{DFBench}, a large-scale DeepFake Benchmark featuring (i)
broad diversity, including 540,000 images across real, AI-edited, and
AI-generated content, (ii) latest model, the fake images are generated by 12
state-of-the-art generation models, and (iii) bidirectional benchmarking and
evaluating for both the detection accuracy of deepfake detectors and the
evasion capability of generative models. Based on DFBench, we propose
\textbf{MoA-DF}, Mixture of Agents for DeepFake detection, leveraging a
combined probability strategy from multiple LMMs. MoA-DF achieves
state-of-the-art performance, further proving the effectiveness of leveraging
LMMs for deepfake detection. Database and codes are publicly available at
https://github.com/IntMeGroup/DFBench.

</details>


### [147] [Are classical deep neural networks weakly adversarially robust?](https://arxiv.org/abs/2506.02016)
*Nuolin Sun,Linyuan Wang,Dongyang Li,Bin Yan,Lei Li*

Main category: cs.CV

Relevance: 40.0

TL;DR: 该论文提出了一种基于层间特征路径相关性的对抗样本检测和图像识别方法，避免了计算开销大的对抗训练。


<details>
  <summary>Details</summary>
Motivation: 对抗训练虽能提升DNN的对抗鲁棒性，但计算成本高。论文旨在探索无需昂贵防御策略的替代方法。

Method: 利用DNN每层输出特征的聚类特性，构建特征路径并计算其与类中心特征路径的相关性。

Result: 在ResNet-20上，干净准确率为82.77%，对抗准确率为44.17%；在ResNet-18上分别为80.01%和46.1%。

Conclusion: 该方法揭示了DNN固有的对抗鲁棒性，挑战了传统认知。

Abstract: Adversarial attacks have received increasing attention and it has been widely
recognized that classical DNNs have weak adversarial robustness. The most
commonly used adversarial defense method, adversarial training, improves the
adversarial accuracy of DNNs by generating adversarial examples and retraining
the model. However, adversarial training requires a significant computational
overhead. In this paper, inspired by existing studies focusing on the
clustering properties of DNN output features at each layer and the Progressive
Feedforward Collapse phenomenon, we propose a method for adversarial example
detection and image recognition that uses layer-wise features to construct
feature paths and computes the correlation between the examples feature paths
and the class-centered feature paths. Experimental results show that the
recognition method achieves 82.77% clean accuracy and 44.17% adversarial
accuracy on the ResNet-20 with PFC. Compared to the adversarial training method
with 77.64% clean accuracy and 52.94% adversarial accuracy, our method exhibits
a trade-off without relying on computationally expensive defense strategies.
Furthermore, on the standard ResNet-18, our method maintains this advantage
with respective metrics of 80.01% and 46.1%. This result reveals inherent
adversarial robustness in DNNs, challenging the conventional understanding of
the weak adversarial robustness in DNNs.

</details>


### [148] [Dynamic-Aware Video Distillation: Optimizing Temporal Resolution Based on Video Semantics](https://arxiv.org/abs/2506.02021)
*Yinjie Zhao,Heng Zhao,Bihan Wen,Yew-Soon Ong,Joey Tianyi Zhou*

Main category: cs.CV

Relevance: 40.0

TL;DR: 本文提出了一种基于强化学习的动态感知视频蒸馏方法（DAViD），用于优化视频数据集中的时间分辨率，显著提升了现有方法的性能。


<details>
  <summary>Details</summary>
Motivation: 视频数据集中的冗余减少是一个关键问题，但现有方法假设所有视频语义的时间冗余均匀，限制了其有效性。

Method: 采用强化学习（RL）预测最佳时间分辨率，并提出教师循环奖励函数更新RL代理策略。

Result: DAViD显著优于现有数据集蒸馏方法，性能大幅提升。

Conclusion: 该研究为更高效和语义自适应的视频数据集蒸馏研究奠定了基础。

Abstract: With the rapid development of vision tasks and the scaling on datasets and
models, redundancy reduction in vision datasets has become a key area of
research. To address this issue, dataset distillation (DD) has emerged as a
promising approach to generating highly compact synthetic datasets with
significantly less redundancy while preserving essential information. However,
while DD has been extensively studied for image datasets, DD on video datasets
remains underexplored. Video datasets present unique challenges due to the
presence of temporal information and varying levels of redundancy across
different classes. Existing DD approaches assume a uniform level of temporal
redundancy across all different video semantics, which limits their
effectiveness on video datasets. In this work, we propose Dynamic-Aware Video
Distillation (DAViD), a Reinforcement Learning (RL) approach to predict the
optimal Temporal Resolution of the synthetic videos. A teacher-in-the-loop
reward function is proposed to update the RL agent policy. To the best of our
knowledge, this is the first study to introduce adaptive temporal resolution
based on video semantics in video dataset distillation. Our approach
significantly outperforms existing DD methods, demonstrating substantial
improvements in performance. This work paves the way for future research on
more efficient and semantic-adaptive video dataset distillation research.

</details>


### [149] [SAB3R: Semantic-Augmented Backbone in 3D Reconstruction](https://arxiv.org/abs/2506.02112)
*Xuweiyi Chen,Tian Xia,Sihan Xu,Jianing Yang,Joyce Chai,Zezhou Cheng*

Main category: cs.CV

Relevance: 40.0

TL;DR: 论文提出新任务Map and Locate，结合开放词汇分割与3D重建，并引入基线方法SAB3R，通过轻量级蒸馏策略提升性能。


<details>
  <summary>Details</summary>
Motivation: 统一开放词汇分割与3D重建，为现实世界AI应用提供关键步骤。

Method: 基于MASt3R，引入轻量级蒸馏策略，将2D视觉主干（如CLIP和DINOv2）的语义特征迁移到3D任务中。

Result: SAB3R在Map and Locate基准测试中表现优于单独部署MASt3R和CLIP，同时在2D语义分割和3D任务中验证了有效性。

Conclusion: SAB3R为结合2D语义与3D重建任务提供了有效解决方案。

Abstract: We introduce a new task, Map and Locate, which unifies the traditionally
distinct objectives of open-vocabulary segmentation - detecting and segmenting
object instances based on natural language queries - and 3D reconstruction, the
process of estimating a scene's 3D structure from visual inputs. Specifically,
Map and Locate involves generating a point cloud from an unposed video and
segmenting object instances based on open-vocabulary queries. This task serves
as a critical step toward real-world embodied AI applications and introduces a
practical task that bridges reconstruction, recognition and reorganization. To
tackle this task, we introduce a simple yet effective baseline, which we denote
as SAB3R. Our approach builds upon MASt3R, a recent breakthrough in 3D computer
vision, and incorporates a lightweight distillation strategy. This method
transfers dense, per-pixel semantic features from 2D vision backbones (eg, CLIP
and DINOv2) to enhance MASt3R's capabilities. Without introducing any auxiliary
frozen networks, our model generates per-pixel semantic features and constructs
cohesive point maps in a single forward pass. Compared to separately deploying
MASt3R and CLIP, our unified model, SAB3R, achieves superior performance on the
Map and Locate benchmark. Furthermore, we evaluate SAB3R on both 2D semantic
segmentation and 3D tasks to comprehensively validate its effectiveness.

</details>


### [150] [TIIF-Bench: How Does Your T2I Model Follow Your Instructions?](https://arxiv.org/abs/2506.02161)
*Xinyu Wei,Jinrui Zhang,Zeqing Wang,Hongyang Wei,Zhen Guo,Lei Zhang*

Main category: cs.CV

Relevance: 40.0

TL;DR: TIIF-Bench是一个新的文本到图像（T2I）模型评估基准，旨在通过多样化和复杂的提示集及细粒度指标，评估模型对文本指令的精细对齐能力。


<details>
  <summary>Details</summary>
Motivation: 现有T2I模型评估基准在提示多样性和复杂性以及评估指标上存在不足，难以全面评估模型性能。

Method: TIIF-Bench包含5000个多维度分类的提示，分为三个难度级别，并引入文本渲染和风格控制两个关键属性。此外，利用大型视觉语言模型的世界知识，提出了一种新的可计算框架。

Result: 通过对主流T2I模型的细致评估，分析了当前模型的优缺点，并揭示了现有基准的局限性。

Conclusion: TIIF-Bench为T2I模型的评估提供了更全面和精细的工具，有助于推动模型性能的提升。

Abstract: The rapid advancements of Text-to-Image (T2I) models have ushered in a new
phase of AI-generated content, marked by their growing ability to interpret and
follow user instructions. However, existing T2I model evaluation benchmarks
fall short in limited prompt diversity and complexity, as well as coarse
evaluation metrics, making it difficult to evaluate the fine-grained alignment
performance between textual instructions and generated images. In this paper,
we present TIIF-Bench (Text-to-Image Instruction Following Benchmark), aiming
to systematically assess T2I models' ability in interpreting and following
intricate textual instructions. TIIF-Bench comprises a set of 5000 prompts
organized along multiple dimensions, which are categorized into three levels of
difficulties and complexities. To rigorously evaluate model robustness to
varying prompt lengths, we provide a short and a long version for each prompt
with identical core semantics. Two critical attributes, i.e., text rendering
and style control, are introduced to evaluate the precision of text synthesis
and the aesthetic coherence of T2I models. In addition, we collect 100
high-quality designer level prompts that encompass various scenarios to
comprehensively assess model performance. Leveraging the world knowledge
encoded in large vision language models, we propose a novel computable
framework to discern subtle variations in T2I model outputs. Through meticulous
benchmarking of mainstream T2I models on TIIF-Bench, we analyze the pros and
cons of current T2I models and reveal the limitations of current T2I
benchmarks. Project Page: https://a113n-w3i.github.io/TIIF_Bench/.

</details>


### [151] [Quantifying task-relevant representational similarity using decision variable correlation](https://arxiv.org/abs/2506.02164)
*Yu,Qian,Wilson S. Geisler,Xue-Xin Wei*

Main category: cs.CV

Relevance: 40.0

TL;DR: 提出了一种新方法DVC来量化模型与猴子大脑在分类任务中决策策略的相似性，发现模型与猴子相似性较低且随性能提升而下降，揭示了任务相关表征的根本差异。


<details>
  <summary>Details</summary>
Motivation: 探讨深度学习模型与生物大脑在分类任务中的决策策略相似性，以理解两者表征的异同。

Method: 使用决策变量相关（DVC）方法量化模型与猴子V4/IT记录的决策策略相似性，评估了不同训练方式的影响。

Result: 模型间相似性与猴子间相似性相当，但模型与猴子相似性较低且随ImageNet性能提升而下降；对抗训练和更大数据集预训练未改善模型与猴子相似性。

Conclusion: 猴子V4/IT与图像分类模型的任务相关表征存在根本差异，对抗训练和预训练未能缩小这一差距。

Abstract: Previous studies have compared the brain and deep neural networks trained on
image classification. Intriguingly, while some suggest that their
representations are highly similar, others argued the opposite. Here, we
propose a new approach to characterize the similarity of the decision
strategies of two observers (models or brains) using decision variable
correlation (DVC). DVC quantifies the correlation between decoded decisions on
individual samples in a classification task and thus can capture task-relevant
information rather than general representational alignment. We evaluate this
method using monkey V4/IT recordings and models trained on image classification
tasks.
  We find that model--model similarity is comparable to monkey--monkey
similarity, whereas model--monkey similarity is consistently lower and,
surprisingly, decreases with increasing ImageNet-1k performance. While
adversarial training enhances robustness, it does not improve model--monkey
similarity in task-relevant dimensions; however, it markedly increases
model--model similarity. Similarly, pre-training on larger datasets does not
improve model--monkey similarity. These results suggest a fundamental
divergence between the task-relevant representations in monkey V4/IT and those
learned by models trained on image classification tasks.

</details>


### [152] [Fire360: A Benchmark for Robust Perception and Episodic Memory in Degraded 360-Degree Firefighting Videos](https://arxiv.org/abs/2506.02167)
*Aditi Tiwari,Farzaneh Masoud,Dac Trong Nguyen,Jill Kraft,Heng Ji,Klara Nahrstedt*

Main category: cs.CV

Relevance: 40.0

TL;DR: Fire360是一个用于评估消防场景中感知和推理能力的基准数据集，包含228个360度视频，支持五项任务，旨在提升模型在不确定性下的表现。


<details>
  <summary>Details</summary>
Motivation: 消防场景中AI系统的可靠性不足导致消防员受伤，需要开发能在恶劣条件下感知和推理的模型。

Method: 构建Fire360数据集，包含多样条件下的视频和标注，设计五项任务评估模型能力。

Result: 人类专家在TOR任务中表现优异（83.5%），而GPT-4o等模型表现较差，暴露了在退化条件下的推理缺陷。

Conclusion: Fire360的发布旨在推动模型在不确定性下的感知、记忆、推理和行动能力。

Abstract: Modern AI systems struggle most in environments where reliability is critical
- scenes with smoke, poor visibility, and structural deformation. Each year,
tens of thousands of firefighters are injured on duty, often due to breakdowns
in situational perception. We introduce Fire360, a benchmark for evaluating
perception and reasoning in safety-critical firefighting scenarios. The dataset
includes 228 360-degree videos from professional training sessions under
diverse conditions (e.g., low light, thermal distortion), annotated with action
segments, object locations, and degradation metadata. Fire360 supports five
tasks: Visual Question Answering, Temporal Action Captioning, Object
Localization, Safety-Critical Reasoning, and Transformed Object Retrieval
(TOR). TOR tests whether models can match pristine exemplars to fire-damaged
counterparts in unpaired scenes, evaluating transformation-invariant
recognition. While human experts achieve 83.5% on TOR, models like GPT-4o lag
significantly, exposing failures in reasoning under degradation. By releasing
Fire360 and its evaluation suite, we aim to advance models that not only see,
but also remember, reason, and act under uncertainty. The dataset is available
at: https://uofi.box.com/v/fire360dataset.

</details>


### [153] [Diff2Flow: Training Flow Matching Models via Diffusion Model Alignment](https://arxiv.org/abs/2506.02221)
*Johannes Schusterbauer,Ming Gui,Frank Fundel,Björn Ommer*

Main category: cs.CV

Relevance: 40.0

TL;DR: Diff2Flow框架通过重新缩放时间步、对齐插值和从扩散预测中导出FM兼容的速度场，高效地将预训练扩散模型的知识迁移到流匹配（FM）中。


<details>
  <summary>Details</summary>
Motivation: 当前基础FM模型在微调时计算成本过高，而扩散模型（如Stable Diffusion）具有高效架构和生态系统支持。Diff2Flow旨在解决如何高效地将扩散模型知识迁移到FM中的关键挑战。

Method: 提出Diff2Flow框架，通过时间步重缩放、插值对齐和从扩散预测中导出FM兼容的速度场，实现扩散模型到FM的直接高效微调。

Result: Diff2Flow在参数高效约束下优于原始FM和扩散微调，并在多样化下游任务中达到或超越最先进方法的性能。

Conclusion: Diff2Flow为扩散模型与FM之间的知识迁移提供了一种高效且计算成本低的解决方案。

Abstract: Diffusion models have revolutionized generative tasks through high-fidelity
outputs, yet flow matching (FM) offers faster inference and empirical
performance gains. However, current foundation FM models are computationally
prohibitive for finetuning, while diffusion models like Stable Diffusion
benefit from efficient architectures and ecosystem support. This work addresses
the critical challenge of efficiently transferring knowledge from pre-trained
diffusion models to flow matching. We propose Diff2Flow, a novel framework that
systematically bridges diffusion and FM paradigms by rescaling timesteps,
aligning interpolants, and deriving FM-compatible velocity fields from
diffusion predictions. This alignment enables direct and efficient FM
finetuning of diffusion priors with no extra computation overhead. Our
experiments demonstrate that Diff2Flow outperforms na\"ive FM and diffusion
finetuning particularly under parameter-efficient constraints, while achieving
superior or competitive performance across diverse downstream tasks compared to
state-of-the-art methods. We will release our code at
https://github.com/CompVis/diff2flow.

</details>


### [154] [Motion aware video generative model](https://arxiv.org/abs/2506.02244)
*Bowen Xue,Giuseppe Claudio Guarnera,Shuang Zhao,Zahra Montazeri*

Main category: cs.CV

Relevance: 40.0

TL;DR: 提出了一种基于物理的频率域方法，提升扩散视频生成的物理合理性，通过频率域分析和优化减少非物理伪影。


<details>
  <summary>Details</summary>
Motivation: 当前视频生成方法依赖统计学习，缺乏对物理运动的显式建模，导致非物理伪影。本文旨在通过频率域分析提升物理合理性。

Method: 1. 分析物理运动的频率域特征；2. 提出物理运动损失函数和频率域增强模块，优化视频的物理合理性。

Result: 实验表明，该方法显著提升运动质量和物理合理性，且不影响视觉质量或语义对齐。

Conclusion: 通过频率域物理运动框架，将物理约束融入深度学习视频生成，为数据驱动模型与物理模型建立联系。

Abstract: Recent advances in diffusion-based video generation have yielded
unprecedented quality in visual content and semantic coherence. However,
current approaches predominantly rely on statistical learning from vast
datasets without explicitly modeling the underlying physics of motion,
resulting in subtle yet perceptible non-physical artifacts that diminish the
realism of generated videos. This paper introduces a physics-informed frequency
domain approach to enhance the physical plausibility of generated videos. We
first conduct a systematic analysis of the frequency-domain characteristics of
diverse physical motions (translation, rotation, scaling), revealing that each
motion type exhibits distinctive and identifiable spectral signatures. Building
on this theoretical foundation, we propose two complementary components: (1) a
physical motion loss function that quantifies and optimizes the conformity of
generated videos to ideal frequency-domain motion patterns, and (2) a frequency
domain enhancement module that progressively learns to adjust video features to
conform to physical motion constraints while preserving original network
functionality through a zero-initialization strategy. Experiments across
multiple video diffusion architectures demonstrate that our approach
significantly enhances motion quality and physical plausibility without
compromising visual quality or semantic alignment. Our frequency-domain
physical motion framework generalizes effectively across different video
generation architectures, offering a principled approach to incorporating
physical constraints into deep learning-based video synthesis pipelines. This
work seeks to establish connections between data-driven models and
physics-based motion models.

</details>


### [155] [Entity Image and Mixed-Modal Image Retrieval Datasets](https://arxiv.org/abs/2506.02291)
*Cristian-Ioan Blaga,Paul Suganthan,Sahil Dua,Krishna Srinivasan,Enrique Alfonseca,Peter Dornbach,Tom Duerig,Imed Zitouni,Zhe Dong*

Main category: cs.CV

Relevance: 40.0

TL;DR: 论文提出了一种新的多模态图像检索基准，包含两个新数据集（EI和MMIR），用于评估视觉与文本结合的深度跨模态理解能力。


<details>
  <summary>Details</summary>
Motivation: 当前缺乏结合视觉和文本信息的混合模态图像检索挑战性基准，因此需要开发新基准以推动多模态学习的发展。

Method: 引入两个新数据集：EI（提供维基百科实体的标准图像）和MMIR（基于WIT数据集），支持单实体和多实体图像查询任务。

Result: 基准验证了其在混合模态检索中的训练和评估实用性，数据集质量通过众包人工标注确认。

Conclusion: 该基准填补了多模态图像检索领域的空白，为未来研究提供了重要资源。

Abstract: Despite advances in multimodal learning, challenging benchmarks for
mixed-modal image retrieval that combines visual and textual information are
lacking. This paper introduces a novel benchmark to rigorously evaluate image
retrieval that demands deep cross-modal contextual understanding. We present
two new datasets: the Entity Image Dataset (EI), providing canonical images for
Wikipedia entities, and the Mixed-Modal Image Retrieval Dataset (MMIR), derived
from the WIT dataset. The MMIR benchmark features two challenging query types
requiring models to ground textual descriptions in the context of provided
visual entities: single entity-image queries (one entity image with descriptive
text) and multi-entity-image queries (multiple entity images with relational
text). We empirically validate the benchmark's utility as both a training
corpus and an evaluation set for mixed-modal retrieval. The quality of both
datasets is further affirmed through crowd-sourced human annotations. The
datasets are accessible through the GitHub page:
https://github.com/google-research-datasets/wit-retrieval.

</details>


### [156] [QARI-OCR: High-Fidelity Arabic Text Recognition through Multimodal Large Language Model Adaptation](https://arxiv.org/abs/2506.02295)
*Ahmed Wasfy,Omer Nacar,Abdelakreem Elkhateb,Mahmoud Reda,Omar Elshehy,Adel Ammar,Wadii Boulila*

Main category: cs.CV

Relevance: 40.0

TL;DR: Qari-OCR是一系列基于Qwen2-VL-2B-Instruct优化的视觉语言模型，专注于阿拉伯语OCR任务，通过迭代微调和合成数据集显著提升了性能。


<details>
  <summary>Details</summary>
Motivation: 阿拉伯语脚本的复杂性（如连笔、变音符号和多样字体）对OCR任务提出了挑战，需要专门优化的模型。

Method: 基于Qwen2-VL-2B-Instruct进行渐进式微调，使用合成数据集优化模型，最终开发出QARI v0.2和v0.3版本。

Result: QARI v0.2在开放源代码中达到最新技术水平（WER 0.160，CER 0.061，BLEU 0.737），并展示了对手写文本和低分辨率图像的潜力。

Conclusion: Qari-OCR显著提升了阿拉伯语OCR的准确性和效率，并开源模型和数据集以促进研究。

Abstract: The inherent complexities of Arabic script; its cursive nature, diacritical
marks (tashkeel), and varied typography, pose persistent challenges for Optical
Character Recognition (OCR). We present Qari-OCR, a series of vision-language
models derived from Qwen2-VL-2B-Instruct, progressively optimized for Arabic
through iterative fine-tuning on specialized synthetic datasets. Our leading
model, QARI v0.2, establishes a new open-source state-of-the-art with a Word
Error Rate (WER) of 0.160, Character Error Rate (CER) of 0.061, and BLEU score
of 0.737 on diacritically-rich texts. Qari-OCR demonstrates superior handling
of tashkeel, diverse fonts, and document layouts, alongside impressive
performance on low-resolution images. Further explorations (QARI v0.3) showcase
strong potential for structural document understanding and handwritten text.
This work delivers a marked improvement in Arabic OCR accuracy and efficiency,
with all models and datasets released to foster further research.

</details>


### [157] [Medical World Model: Generative Simulation of Tumor Evolution for Treatment Planning](https://arxiv.org/abs/2506.02327)
*Yijun Yang,Zhao-Yang Wang,Qiuping Liu,Shuwen Sun,Kang Wang,Rama Chellappa,Zongwei Zhou,Alan Yuille,Lei Zhu,Yu-Dong Zhang,Jieneng Chen*

Main category: cs.CV

Relevance: 40.0

TL;DR: MeWM是首个医学世界模型，通过视觉预测疾病状态，结合视觉语言模型和肿瘤生成模型，优化临床决策。


<details>
  <summary>Details</summary>
Motivation: 利用大型生成模型模拟疾病动态，提升临床决策的准确性和个性化治疗。

Method: 结合视觉语言模型（策略模型）和肿瘤生成模型（动态模型），提出逆动态模型评估治疗效果。

Result: MeWM在模拟疾病动态和治疗优化方面表现优异，显著提升临床决策的F1分数。

Conclusion: MeWM为医学世界模型的未来应用奠定了基础，尤其在个性化治疗和临床决策支持中。

Abstract: Providing effective treatment and making informed clinical decisions are
essential goals of modern medicine and clinical care. We are interested in
simulating disease dynamics for clinical decision-making, leveraging recent
advances in large generative models. To this end, we introduce the Medical
World Model (MeWM), the first world model in medicine that visually predicts
future disease states based on clinical decisions. MeWM comprises (i)
vision-language models to serve as policy models, and (ii) tumor generative
models as dynamics models. The policy model generates action plans, such as
clinical treatments, while the dynamics model simulates tumor progression or
regression under given treatment conditions. Building on this, we propose the
inverse dynamics model that applies survival analysis to the simulated
post-treatment tumor, enabling the evaluation of treatment efficacy and the
selection of the optimal clinical action plan. As a result, the proposed MeWM
simulates disease dynamics by synthesizing post-treatment tumors, with
state-of-the-art specificity in Turing tests evaluated by radiologists.
Simultaneously, its inverse dynamics model outperforms medical-specialized GPTs
in optimizing individualized treatment protocols across all metrics. Notably,
MeWM improves clinical decision-making for interventional physicians, boosting
F1-score in selecting the optimal TACE protocol by 13%, paving the way for
future integration of medical world models as the second readers.

</details>


### [158] [Generalized Category Discovery via Reciprocal Learning and Class-Wise Distribution Regularization](https://arxiv.org/abs/2506.02334)
*Duo Liu,Zhiquan Tan,Linglan Zhao,Zhongqiang Zhang,Xiangzhong Fang,Weiran Huang*

Main category: cs.CV

Relevance: 40.0

TL;DR: 论文提出了一种名为RLF的互学习框架，通过引入辅助分支和类分布正则化（CDR），解决了现有参数化方法在基础分类上的不足，显著提升了广义类别发现（GCD）的性能。


<details>
  <summary>Details</summary>
Motivation: 现有参数化方法在广义类别发现任务中因不可靠的自监督导致基础分类性能不佳，亟需改进。

Method: 提出互学习框架（RLF），包含主分支和辅助分支，通过伪基础样本过滤和软标签交互优化学习；引入类分布正则化（CDR）减少对基础类的学习偏差。

Result: 在七个GCD数据集上验证，RLCD方法在所有类别上表现优异，且计算开销极小。

Conclusion: RLF和CDR的结合有效提升了GCD任务的性能，为参数化方法提供了新思路。

Abstract: Generalized Category Discovery (GCD) aims to identify unlabeled samples by
leveraging the base knowledge from labeled ones, where the unlabeled set
consists of both base and novel classes. Since clustering methods are
time-consuming at inference, parametric-based approaches have become more
popular. However, recent parametric-based methods suffer from inferior base
discrimination due to unreliable self-supervision. To address this issue, we
propose a Reciprocal Learning Framework (RLF) that introduces an auxiliary
branch devoted to base classification. During training, the main branch filters
the pseudo-base samples to the auxiliary branch. In response, the auxiliary
branch provides more reliable soft labels for the main branch, leading to a
virtuous cycle. Furthermore, we introduce Class-wise Distribution
Regularization (CDR) to mitigate the learning bias towards base classes. CDR
essentially increases the prediction confidence of the unlabeled data and
boosts the novel class performance. Combined with both components, our proposed
method, RLCD, achieves superior performance in all classes with negligible
extra computation. Comprehensive experiments across seven GCD datasets validate
its superiority. Our codes are available at https://github.com/APORduo/RLCD.

</details>


### [159] [RATE-Nav: Region-Aware Termination Enhancement for Zero-shot Object Navigation with Vision-Language Models](https://arxiv.org/abs/2506.02354)
*Junjie Li,Nan Zhang,Xiaoyang Qu,Kai Lu,Guokuan Li,Jiguang Wan,Jianzong Wang*

Main category: cs.CV

Relevance: 40.0

TL;DR: RATE-Nav是一种基于区域感知的终止增强方法，用于解决物体导航中的冗余探索问题，通过视觉语言模型和探索率计算实现高效终止。


<details>
  <summary>Details</summary>
Motivation: 当前物体导航研究中，冗余探索和探索失败问题尚未解决，探索的及时终止是关键方向。

Method: 提出RATE-Nav方法，包括几何预测区域分割算法和基于区域的探索率估计算法，利用视觉语言模型实现高效终止。

Result: 在HM3D数据集上成功率为67.8%，SPL为31.3%；在MP3D数据集上比零样本方法提升约10%。

Conclusion: RATE-Nav通过区域感知和探索率计算显著提升了物体导航的效率。

Abstract: Object Navigation (ObjectNav) is a fundamental task in embodied artificial
intelligence. Although significant progress has been made in semantic map
construction and target direction prediction in current research, redundant
exploration and exploration failures remain inevitable. A critical but
underexplored direction is the timely termination of exploration to overcome
these challenges. We observe a diminishing marginal effect between exploration
steps and exploration rates and analyze the cost-benefit relationship of
exploration. Inspired by this, we propose RATE-Nav, a Region-Aware
Termination-Enhanced method. It includes a geometric predictive region
segmentation algorithm and region-Based exploration estimation algorithm for
exploration rate calculation. By leveraging the visual question answering
capabilities of visual language models (VLMs) and exploration rates enables
efficient termination.RATE-Nav achieves a success rate of 67.8% and an SPL of
31.3% on the HM3D dataset. And on the more challenging MP3D dataset, RATE-Nav
shows approximately 10% improvement over previous zero-shot methods.

</details>


### [160] [InterRVOS: Interaction-aware Referring Video Object Segmentation](https://arxiv.org/abs/2506.02356)
*Woojeong Jin,Seongchan Kim,Seungryong Kim*

Main category: cs.CV

Relevance: 40.0

TL;DR: 论文提出了一种新的任务InterRVOS，专注于视频中对象交互的分割，并构建了大规模数据集InterRVOS-8K和基线模型ReVIOSa。


<details>
  <summary>Details</summary>
Motivation: 现有方法多关注孤立对象的定位，忽视了对象间的交互关系，而交互是视频理解的关键。

Method: 提出了InterRVOS任务，构建数据集InterRVOS-8K，并设计基线模型ReVIOSa处理单表达式的演员-目标分割。

Result: ReVIOSa在标准及交互场景下表现优异，优于现有方法。

Conclusion: 该研究为交互中心视频理解奠定了基础，未来可进一步探索。

Abstract: Referring video object segmentation aims to segment the object in a video
corresponding to a given natural language expression. While prior works have
explored various referring scenarios, including motion-centric or
multi-instance expressions, most approaches still focus on localizing a single
target object in isolation. However, in comprehensive video understanding, an
object's role is often defined by its interactions with other entities, which
are largely overlooked in existing datasets and models. In this work, we
introduce Interaction-aware referring video object sgementation (InterRVOS), a
new task that requires segmenting both actor and target entities involved in an
interaction. Each interactoin is described through a pair of complementary
expressions from different semantic perspectives, enabling fine-grained
modeling of inter-object relationships. To tackle this task, we propose
InterRVOS-8K, the large-scale and automatically constructed dataset containing
diverse interaction-aware expressions with corresponding masks, including
challenging cases such as motion-only multi-instance expressions. We also
present a baseline architecture, ReVIOSa, designed to handle actor-target
segmentation from a single expression, achieving strong performance in both
standard and interaction-focused settings. Furthermore, we introduce an
actor-target-aware evalaution setting that enables a more targeted assessment
of interaction understanding. Experimental results demonstrate that our
approach outperforms prior methods in modeling complex object interactions for
referring video object segmentation task, establishing a strong foundation for
future research in interaction-centric video understanding. Our project page is
available at
\href{https://cvlab-kaist.github.io/InterRVOS}{https://cvlab-kaist.github.io/InterRVOS}.

</details>


### [161] [RoadFormer : Local-Global Feature Fusion for Road Surface Classification in Autonomous Driving](https://arxiv.org/abs/2506.02358)
*Tianze Wang,Zhang Zhang,Chao Sun*

Main category: cs.CV

Relevance: 40.0

TL;DR: 提出了一种基于视觉的细粒度路面分类方法RoadFormer，结合卷积和Transformer模块，显著提升了分类精度。


<details>
  <summary>Details</summary>
Motivation: 提升自动驾驶中路面分类的准确性，解决现有视觉方法在细粒度分类上的不足。

Method: 融合局部和全局特征，提出前景-背景模块（FBM）以增强细粒度特征提取。

Result: 在大规模数据集上Top-1分类精度达92.52%和96.50%，优于现有方法5.69%至12.84%。

Conclusion: RoadFormer在路面分类任务中表现优异，提升了自动驾驶系统的路面感知可靠性。

Abstract: The classification of the type of road surface (RSC) aims to utilize pavement
features to identify the roughness, wet and dry conditions, and material
information of the road surface. Due to its ability to effectively enhance road
safety and traffic management, it has received widespread attention in recent
years. In autonomous driving, accurate RSC allows vehicles to better understand
the road environment, adjust driving strategies, and ensure a safer and more
efficient driving experience. For a long time, vision-based RSC has been
favored. However, existing visual classification methods have overlooked the
exploration of fine-grained classification of pavement types (such as similar
pavement textures). In this work, we propose a pure vision-based fine-grained
RSC method for autonomous driving scenarios, which fuses local and global
feature information through the stacking of convolutional and transformer
modules. We further explore the stacking strategies of local and global feature
extraction modules to find the optimal feature extraction strategy. In
addition, since fine-grained tasks also face the challenge of relatively large
intra-class differences and relatively small inter-class differences, we
propose a Foreground-Background Module (FBM) that effectively extracts
fine-grained context features of the pavement, enhancing the classification
ability for complex pavements. Experiments conducted on a large-scale pavement
dataset containing one million samples and a simplified dataset reorganized
from this dataset achieved Top-1 classification accuracies of 92.52% and
96.50%, respectively, improving by 5.69% to 12.84% compared to SOTA methods.
These results demonstrate that RoadFormer outperforms existing methods in RSC
tasks, providing significant progress in improving the reliability of pavement
perception in autonomous driving systems.

</details>


### [162] [Approximate Borderline Sampling using Granular-Ball for Classification Tasks](https://arxiv.org/abs/2506.02366)
*Qin Xie,Qinghua Zhang,Shuyin Xia*

Main category: cs.CV

Relevance: 40.0

TL;DR: 提出了一种基于颗粒球（GB）的近似边界采样方法（GBABS），通过限制扩散生成GB（RD-GBG）避免重叠，并结合异构最近邻概念提升分类任务中的边界采样和噪声数据集质量。


<details>
  <summary>Details</summary>
Motivation: 解决现有GB采样方法在边界采样策略缺失和类边界模糊或收缩问题，提升分类任务的效率和鲁棒性。

Method: 1. 提出RD-GBG方法，通过限制扩散生成GB避免重叠；2. 提出GBABS方法，基于异构最近邻实现边界采样和噪声数据集质量提升。

Result: 实验表明，GBABS在边界采样和噪声数据集处理上优于现有GB采样方法和代表性采样方法。

Conclusion: GBABS是一种无需最优纯度阈值的通用采样方法，适用于边界采样和噪声数据集改进。

Abstract: Data sampling enhances classifier efficiency and robustness through data
compression and quality improvement. Recently, the sampling method based on
granular-ball (GB) has shown promising performance in generality and noisy
classification tasks. However, some limitations remain, including the absence
of borderline sampling strategies and issues with class boundary blurring or
shrinking due to overlap between GBs. In this paper, an approximate borderline
sampling method using GBs is proposed for classification tasks. First, a
restricted diffusion-based GB generation (RD-GBG) method is proposed, which
prevents GB overlaps by constrained expansion, preserving precise geometric
representation of GBs via redefined ones. Second, based on the concept of
heterogeneous nearest neighbor, a GB-based approximate borderline sampling
(GBABS) method is proposed, which is the first general sampling method capable
of both borderline sampling and improving the quality of class noise datasets.
Additionally, since RD-GBG incorporates noise detection and GBABS focuses on
borderline samples, GBABS performs outstandingly on class noise datasets
without the need for an optimal purity threshold. Experimental results
demonstrate that the proposed methods outperform the GB-based sampling method
and several representative sampling methods. Our source code is publicly
available at https://github.com/CherylTse/GBABS.

</details>


### [163] [ViTNF: Leveraging Neural Fields to Boost Vision Transformers in Generalized Category Discovery](https://arxiv.org/abs/2506.02367)
*Jiayi Su,Dequan Jin*

Main category: cs.CV

Relevance: 40.0

TL;DR: 论文提出了一种基于神经场的新型架构ViTNF，用于广义类别发现（GCD），通过替换MLP头部为神经场分类器，显著减少了训练样本需求和训练难度。


<details>
  <summary>Details</summary>
Motivation: 传统ViT的MLP头部训练成本高且未充分利用特征提取器的能力，因此需要一种更高效的分类器设计。

Method: 提出神经场分类器（NF），由两个耦合的静态神经场组成，分别存储支持样本的特征信息和类别信息，简化了三阶段训练模式。

Result: 在多个数据集上超越现有方法，新类和所有类的准确率分别提升19%和16%。

Conclusion: ViTNF在GCD任务中表现出显著优势，减少了训练难度并提升了性能。

Abstract: Generalized category discovery (GCD) is a highly popular task in open-world
recognition, aiming to identify unknown class samples using known class data.
By leveraging pre-training, meta-training, and fine-tuning, ViT achieves
excellent few-shot learning capabilities. Its MLP head is a feedforward
network, trained synchronously with the entire network in the same process,
increasing the training cost and difficulty without fully leveraging the power
of the feature extractor. This paper proposes a new architecture by replacing
the MLP head with a neural field-based one. We first present a new static
neural field function to describe the activity distribution of the neural field
and then use two static neural field functions to build an efficient few-shot
classifier. This neural field-based (NF) classifier consists of two coupled
static neural fields. It stores the feature information of support samples by
its elementary field, the known categories by its high-level field, and the
category information of support samples by its cross-field connections. We
replace the MLP head with the proposed NF classifier, resulting in a novel
architecture ViTNF, and simplify the three-stage training mode by pre-training
the feature extractor on source tasks and training the NF classifier with
support samples in meta-testing separately, significantly reducing ViT's demand
for training samples and the difficulty of model training. To enhance the
model's capability in identifying new categories, we provide an effective
algorithm to determine the lateral interaction scale of the elementary field.
Experimental results demonstrate that our model surpasses existing
state-of-the-art methods on CIFAR-100, ImageNet-100, CUB-200, and Standard
Cars, achieving dramatic accuracy improvements of 19\% and 16\% in new and all
classes, respectively, indicating a notable advantage in GCD.

</details>


### [164] [Multi-level and Multi-modal Action Anticipation](https://arxiv.org/abs/2506.02382)
*Seulgi Kim,Ghazal Kaviani,Mohit Prabhushankar,Ghassan AlRegib*

Main category: cs.CV

Relevance: 40.0

TL;DR: 提出了一种多模态和多层次的动作预测方法（m&m-Ant），结合视觉和文本线索，通过细粒度标签生成器和时间一致性损失函数优化性能，在多个数据集上实现了3.08%的准确率提升。


<details>
  <summary>Details</summary>
Motivation: 动作预测在智能系统中至关重要，但传统方法仅依赖视觉模态，忽略了多模态信息的潜力。受人类行为启发，作者提出结合视觉和文本线索的多层次方法。

Method: 提出m&m-Ant方法，结合视觉和文本模态，引入细粒度标签生成器和时间一致性损失函数。

Result: 在Breakfast、50 Salads和DARai数据集上实现了3.08%的平均准确率提升，达到SOTA。

Conclusion: 多模态和层次建模在动作预测中具有潜力，为未来研究设定了新基准。

Abstract: Action anticipation, the task of predicting future actions from partially
observed videos, is crucial for advancing intelligent systems. Unlike action
recognition, which operates on fully observed videos, action anticipation must
handle incomplete information. Hence, it requires temporal reasoning, and
inherent uncertainty handling. While recent advances have been made,
traditional methods often focus solely on visual modalities, neglecting the
potential of integrating multiple sources of information. Drawing inspiration
from human behavior, we introduce \textit{Multi-level and Multi-modal Action
Anticipation (m\&m-Ant)}, a novel multi-modal action anticipation approach that
combines both visual and textual cues, while explicitly modeling hierarchical
semantic information for more accurate predictions. To address the challenge of
inaccurate coarse action labels, we propose a fine-grained label generator
paired with a specialized temporal consistency loss function to optimize
performance. Extensive experiments on widely used datasets, including
Breakfast, 50 Salads, and DARai, demonstrate the effectiveness of our approach,
achieving state-of-the-art results with an average anticipation accuracy
improvement of 3.08\% over existing methods. This work underscores the
potential of multi-modal and hierarchical modeling in advancing action
anticipation and establishes a new benchmark for future research in the field.
Our code is available at: https://github.com/olivesgatech/mM-ant.

</details>


### [165] [Modelship Attribution: Tracing Multi-Stage Manipulations Across Generative Models](https://arxiv.org/abs/2506.02405)
*Zhiya Tan,Xin Zhang,Joey Tianyi Zhou*

Main category: cs.CV

Relevance: 40.0

TL;DR: 论文提出了一种新方法‘Modelship Attribution’，用于追踪图像在多阶段生成模型编辑中的演变过程，并引入了专用框架MAT来解决这一问题。


<details>
  <summary>Details</summary>
Motivation: 随着生成技术的普及，图像可能被多次编辑，现有方法难以应对复杂的多阶段篡改场景，因此需要一种系统化的解决方案。

Method: 利用三种生成模型（StyleMapGAN、DiffSwap、FacePartsSwap）创建数据集，并提出MAT框架，通过分析编辑模式而非指纹来追踪模型贡献。

Result: 实验表明，MAT在多阶段篡改场景中表现优异，显著优于其他方法。

Conclusion: MAT为复杂图像篡改的溯源提供了有效工具，填补了多阶段篡改检测的空白。

Abstract: As generative techniques become increasingly accessible, authentic visuals
are frequently subjected to iterative alterations by various individuals
employing a variety of tools. Currently, to avoid misinformation and ensure
accountability, a lot of research on detection and attribution is emerging.
Although these methods demonstrate promise in single-stage manipulation
scenarios, they fall short when addressing complex real-world iterative
manipulation. In this paper, we are the first, to the best of our knowledge, to
systematically model this real-world challenge and introduce a novel method to
solve it. We define a task called "Modelship Attribution", which aims to trace
the evolution of manipulated images by identifying the generative models
involved and reconstructing the sequence of edits they performed. To
realistically simulate this scenario, we utilize three generative models,
StyleMapGAN, DiffSwap, and FacePartsSwap, that sequentially modify distinct
regions of the same image. This process leads to the creation of the first
modelship dataset, comprising 83,700 images (16,740 images*5). Given that later
edits often overwrite the fingerprints of earlier models, the focus shifts from
extracting blended fingerprints to characterizing each model's distinctive
editing patterns. To tackle this challenge, we introduce the modelship
attribution transformer (MAT), a purpose-built framework designed to
effectively recognize and attribute the contributions of various models within
complex, multi-stage manipulation workflows. Through extensive experiments and
comparative analysis with other related methods, our results, including
comprehensive ablation studies, demonstrate that the proposed approach is a
highly effective solution for modelship attribution.

</details>


### [166] [Revisiting End-to-End Learning with Slide-level Supervision in Computational Pathology](https://arxiv.org/abs/2506.02408)
*Wenhao Tang,Rong Qin,Heng Fang,Fengtao Zhou,Hao Chen,Xiang Li,Ming-Ming Cheng*

Main category: cs.CV

Relevance: 40.0

TL;DR: 论文提出了一种名为ABMILX的新型多实例学习方法，通过全局相关性注意力优化和多头机制，解决了端到端学习中的优化挑战，并在计算病理学中超越了传统两阶段方法的性能。


<details>
  <summary>Details</summary>
Motivation: 现有方法在计算病理学中因编码器未针对下游任务微调以及与MIL的分离优化而存在性能限制，端到端学习虽直观但面临计算量大和效果不佳的问题。

Method: 提出ABMILX方法，结合全局相关性注意力优化和多头机制，并采用高效的多尺度随机补丁采样策略。

Result: 在多个挑战性基准测试中，ABMILX超越了现有最优的两阶段方法，同时保持计算高效（<10 RTX3090小时）。

Conclusion: 展示了端到端学习在计算病理学中的潜力，呼吁更多研究关注该领域。

Abstract: Pre-trained encoders for offline feature extraction followed by multiple
instance learning (MIL) aggregators have become the dominant paradigm in
computational pathology (CPath), benefiting cancer diagnosis and prognosis.
However, performance limitations arise from the absence of encoder fine-tuning
for downstream tasks and disjoint optimization with MIL. While slide-level
supervised end-to-end (E2E) learning is an intuitive solution to this issue, it
faces challenges such as high computational demands and suboptimal results.
These limitations motivate us to revisit E2E learning. We argue that prior work
neglects inherent E2E optimization challenges, leading to performance
disparities compared to traditional two-stage methods. In this paper, we
pioneer the elucidation of optimization challenge caused by sparse-attention
MIL and propose a novel MIL called ABMILX. It mitigates this problem through
global correlation-based attention refinement and multi-head mechanisms. With
the efficient multi-scale random patch sampling strategy, an E2E trained ResNet
with ABMILX surpasses SOTA foundation models under the two-stage paradigm
across multiple challenging benchmarks, while remaining computationally
efficient (<10 RTX3090 hours). We show the potential of E2E learning in CPath
and calls for greater research focus in this area. The code is
https://github.com/DearCaat/E2E-WSI-ABMILX.

</details>


### [167] [Guiding Registration with Emergent Similarity from Pre-Trained Diffusion Models](https://arxiv.org/abs/2506.02419)
*Nurislam Tursynbek,Hastings Greer,Basar Demir,Marc Niethammer*

Main category: cs.CV

Relevance: 40.0

TL;DR: 该论文提出利用预训练的扩散模型特征作为相似性度量，指导可变形图像配准网络，解决了传统强度相似性损失在医学图像配准中的局限性。


<details>
  <summary>Details</summary>
Motivation: 传统基于强度的相似性损失在医学图像配准中可能失败，尤其是在解剖结构不一致时。扩散模型的特征提取能力为语义对应提供了新思路。

Method: 利用预训练的扩散模型提取特征作为相似性度量，指导可变形配准网络，确保语义对齐。

Result: 在2D多模态（DXA到X射线）和3D单模态（脑提取与非脑提取MRI）配准任务中表现优于传统方法。

Conclusion: 扩散模型特征能有效捕捉语义对应，提升医学图像配准的准确性。

Abstract: Diffusion models, while trained for image generation, have emerged as
powerful foundational feature extractors for downstream tasks. We find that
off-the-shelf diffusion models, trained exclusively to generate natural RGB
images, can identify semantically meaningful correspondences in medical images.
Building on this observation, we propose to leverage diffusion model features
as a similarity measure to guide deformable image registration networks. We
show that common intensity-based similarity losses often fail in challenging
scenarios, such as when certain anatomies are visible in one image but absent
in another, leading to anatomically inaccurate alignments. In contrast, our
method identifies true semantic correspondences, aligning meaningful structures
while disregarding those not present across images. We demonstrate superior
performance of our approach on two tasks: multimodal 2D registration (DXA to
X-Ray) and monomodal 3D registration (brain-extracted to non-brain-extracted
MRI). Code: https://github.com/uncbiag/dgir

</details>


### [168] [Video-Level Language-Driven Video-Based Visible-Infrared Person Re-Identification](https://arxiv.org/abs/2506.02439)
*Shuang Li,Jiaxu Leng,Changjiang Kuang,Mingpi Tan,Xinbo Gao*

Main category: cs.CV

Relevance: 40.0

TL;DR: 论文提出了一种基于视频的语言驱动跨模态行人重识别框架（VLD），通过生成模态共享的文本提示和时空信息增强，减少模态差异，取得了SOTA效果。


<details>
  <summary>Details</summary>
Motivation: 解决跨模态行人重识别中模态差异问题，利用语言描述的一致性生成模态共享的文本提示。

Method: 提出VLD框架，包括不变模态语言提示（IMLP）和时空提示（STP）模块，结合CLIP模型进行特征对齐和时空信息建模。

Result: 在两个VVI-ReID基准测试中取得了最优性能。

Conclusion: VLD框架通过语言驱动和时空信息增强，有效解决了跨模态行人重识别问题。

Abstract: Video-based Visible-Infrared Person Re-Identification (VVI-ReID) aims to
match pedestrian sequences across modalities by extracting modality-invariant
sequence-level features. As a high-level semantic representation, language
provides a consistent description of pedestrian characteristics in both
infrared and visible modalities. Leveraging the Contrastive Language-Image
Pre-training (CLIP) model to generate video-level language prompts and guide
the learning of modality-invariant sequence-level features is theoretically
feasible. However, the challenge of generating and utilizing modality-shared
video-level language prompts to address modality gaps remains a critical
problem. To address this problem, we propose a simple yet powerful framework,
video-level language-driven VVI-ReID (VLD), which consists of two core modules:
invariant-modality language prompting (IMLP) and spatial-temporal prompting
(STP). IMLP employs a joint fine-tuning strategy for the visual encoder and the
prompt learner to effectively generate modality-shared text prompts and align
them with visual features from different modalities in CLIP's multimodal space,
thereby mitigating modality differences. Additionally, STP models
spatiotemporal information through two submodules, the spatial-temporal hub
(STH) and spatial-temporal aggregation (STA), which further enhance IMLP by
incorporating spatiotemporal information into text prompts. The STH aggregates
and diffuses spatiotemporal information into the [CLS] token of each frame
across the vision transformer (ViT) layers, whereas STA introduces dedicated
identity-level loss and specialized multihead attention to ensure that the STH
focuses on identity-relevant spatiotemporal feature aggregation. The VLD
framework achieves state-of-the-art results on two VVI-ReID benchmarks. The
code will be released at https://github.com/Visuang/VLD.

</details>


### [169] [SViMo: Synchronized Diffusion for Video and Motion Generation in Hand-object Interaction Scenarios](https://arxiv.org/abs/2506.02444)
*Lingwei Dang,Ruizhi Shao,Hongwen Zhang,Wei Min,Yebin Liu,Qingyao Wu*

Main category: cs.CV

Relevance: 40.0

TL;DR: 提出了一种结合视觉先验和动态约束的同步扩散框架，用于同时生成手-物体交互（HOI）视频和运动，无需依赖预定义物体模型或显式姿态指导。


<details>
  <summary>Details</summary>
Motivation: 当前HOI生成方法依赖预定义3D模型和实验室数据，泛化能力有限；视频生成方法则牺牲物理合理性。本文旨在结合视觉与动态约束，提升生成质量。

Method: 采用三模态自适应调制对齐特征，结合3D全注意力建模模态间依赖，并通过视觉感知的3D交互扩散模型生成显式3D序列，形成闭环反馈。

Result: 实验显示该方法在生成高保真、动态合理的HOI序列上优于现有方法，且在未见过的真实场景中表现出色。

Conclusion: 该框架显著提升了视频与运动的一致性，并增强了泛化能力。

Abstract: Hand-Object Interaction (HOI) generation has significant application
potential. However, current 3D HOI motion generation approaches heavily rely on
predefined 3D object models and lab-captured motion data, limiting
generalization capabilities. Meanwhile, HOI video generation methods prioritize
pixel-level visual fidelity, often sacrificing physical plausibility.
Recognizing that visual appearance and motion patterns share fundamental
physical laws in the real world, we propose a novel framework that combines
visual priors and dynamic constraints within a synchronized diffusion process
to generate the HOI video and motion simultaneously. To integrate the
heterogeneous semantics, appearance, and motion features, our method implements
tri-modal adaptive modulation for feature aligning, coupled with 3D
full-attention for modeling inter- and intra-modal dependencies. Furthermore,
we introduce a vision-aware 3D interaction diffusion model that generates
explicit 3D interaction sequences directly from the synchronized diffusion
outputs, then feeds them back to establish a closed-loop feedback cycle. This
architecture eliminates dependencies on predefined object models or explicit
pose guidance while significantly enhancing video-motion consistency.
Experimental results demonstrate our method's superiority over state-of-the-art
approaches in generating high-fidelity, dynamically plausible HOI sequences,
with notable generalization capabilities in unseen real-world scenarios.
Project page at
\href{https://github.com/Droliven}{https://github.com/Droliven}.

</details>


### [170] [ANT: Adaptive Neural Temporal-Aware Text-to-Motion Model](https://arxiv.org/abs/2506.02452)
*Wenshuo Chen,Kuimou Yu,Haozhe Jia,Kaishen Yuan,Bowen Tian,Songning Lai,Hongru Xiao,Erhang Zhang,Lei Wang,Yutao Yue*

Main category: cs.CV

Relevance: 40.0

TL;DR: ANT是一种自适应神经时间感知架构，通过动态调整语义粒度和条件引导，提升文本到运动生成的性能。


<details>
  <summary>Details</summary>
Motivation: 扩散模型在文本到运动生成中忽视了时间频率需求，导致早期去噪需要结构语义，后期需要局部细节。受生物形态发生学启发，提出ANT以解决这一问题。

Method: ANT包含三个模块：1) 语义时间自适应模块（STA），通过频谱分析自动划分去噪阶段；2) 动态无分类器引导调度（DCFG），自适应调整条件与无条件比率；3) 时间语义重加权，定量对齐文本影响与阶段需求。

Result: ANT显著提升了模型性能，在StableMoFusion上实现了最先进的语义对齐。

Conclusion: ANT通过动态调整语义粒度和条件引导，有效解决了文本到运动生成中的时间频率需求问题。

Abstract: While diffusion models advance text-to-motion generation, their static
semantic conditioning ignores temporal-frequency demands: early denoising
requires structural semantics for motion foundations while later stages need
localized details for text alignment. This mismatch mirrors biological
morphogenesis where developmental phases demand distinct genetic programs.
Inspired by epigenetic regulation governing morphological specialization, we
propose **(ANT)**, an **A**daptive **N**eural **T**emporal-Aware architecture.
ANT orchestrates semantic granularity through: **(i) Semantic Temporally
Adaptive (STA) Module:** Automatically partitions denoising into low-frequency
structural planning and high-frequency refinement via spectral analysis. **(ii)
Dynamic Classifier-Free Guidance scheduling (DCFG):** Adaptively adjusts
conditional to unconditional ratio enhancing efficiency while maintaining
fidelity. **(iii) Temporal-semantic reweighting:** Quantitatively aligns text
influence with phase requirements. Extensive experiments show that ANT can be
applied to various baselines, significantly improving model performance, and
achieving state-of-the-art semantic alignment on StableMoFusion.

</details>


### [171] [PAID: Pairwise Angular-Invariant Decomposition for Continual Test-Time Adaptation](https://arxiv.org/abs/2506.02453)
*Kunyu Wang,Xueyang Fu,Yunfei Bao,Chengjie Ge,Chengzhi Cao,Wei Zhai,Zheng-Jun Zha*

Main category: cs.CV

Relevance: 40.0

TL;DR: 论文提出PAID方法，通过保留预训练权重的成对角度结构，在持续测试时适应（CTTA）任务中实现性能提升。


<details>
  <summary>Details</summary>
Motivation: 现有CTTA方法主要利用目标数据，而忽略了预训练权重中蕴含的领域不变先验信息。本文从几何属性出发，发现成对角度结构具有稳定性，可作为适应过程中的关键约束。

Method: 提出PAID方法，将权重分解为幅度和方向，并通过Householder反射引入可学习正交矩阵，全局旋转方向的同时保留成对角度结构。仅更新幅度和正交矩阵。

Result: PAID在四个CTTA基准测试中优于现有方法，验证了成对角度结构保留的有效性。

Conclusion: 保留成对角度结构是CTTA的简单而有效的原则。

Abstract: Continual Test-Time Adaptation (CTTA) aims to online adapt a pre-trained
model to changing environments during inference. Most existing methods focus on
exploiting target data, while overlooking another crucial source of
information, the pre-trained weights, which encode underutilized
domain-invariant priors. This paper takes the geometric attributes of
pre-trained weights as a starting point, systematically analyzing three key
components: magnitude, absolute angle, and pairwise angular structure. We find
that the pairwise angular structure remains stable across diverse corrupted
domains and encodes domain-invariant semantic information, suggesting it should
be preserved during adaptation. Based on this insight, we propose PAID
(Pairwise Angular-Invariant Decomposition), a prior-driven CTTA method that
decomposes weight into magnitude and direction, and introduces a learnable
orthogonal matrix via Householder reflections to globally rotate direction
while preserving the pairwise angular structure. During adaptation, only the
magnitudes and the orthogonal matrices are updated. PAID achieves consistent
improvements over recent SOTA methods on four widely used CTTA benchmarks,
demonstrating that preserving pairwise angular structure offers a simple yet
effective principle for CTTA.

</details>


### [172] [ReSpace: Text-Driven 3D Scene Synthesis and Editing with Preference Alignment](https://arxiv.org/abs/2506.02459)
*Martin JJ. Bucher,Iro Armeni*

Main category: cs.CV

Relevance: 40.0

TL;DR: ReSpace是一个基于自回归语言模型的生成框架，用于文本驱动的3D室内场景合成和编辑，支持显式房间边界和场景编辑任务。


<details>
  <summary>Details</summary>
Motivation: 当前3D室内场景合成方法在对象语义、编辑能力或空间布局方面存在不足，而基于LLM的方法虽能提供丰富语义但不支持编辑或空间推理较弱。

Method: 采用双阶段训练方法（监督微调和偏好对齐），结合显式房间边界的紧凑结构化场景表示，将场景编辑任务转化为下一个令牌预测。

Result: 在对象添加任务上超越现有技术，同时在完整场景合成上保持竞争力。

Conclusion: ReSpace通过结合语言模型的语义能力和结构化表示，实现了高效的3D场景合成与编辑。

Abstract: Scene synthesis and editing has emerged as a promising direction in computer
graphics. Current trained approaches for 3D indoor scenes either oversimplify
object semantics through one-hot class encodings (e.g., 'chair' or 'table'),
require masked diffusion for editing, ignore room boundaries, or rely on floor
plan renderings that fail to capture complex layouts. In contrast, LLM-based
methods enable richer semantics via natural language (e.g., 'modern studio with
light wood furniture') but do not support editing, remain limited to
rectangular layouts or rely on weak spatial reasoning from implicit world
models. We introduce ReSpace, a generative framework for text-driven 3D indoor
scene synthesis and editing using autoregressive language models. Our approach
features a compact structured scene representation with explicit room
boundaries that frames scene editing as a next-token prediction task. We
leverage a dual-stage training approach combining supervised fine-tuning and
preference alignment, enabling a specially trained language model for object
addition that accounts for user instructions, spatial geometry, object
semantics, and scene-level composition. For scene editing, we employ a
zero-shot LLM to handle object removal and prompts for addition. We further
introduce a novel voxelization-based evaluation that captures fine-grained
geometry beyond 3D bounding boxes. Experimental results surpass
state-of-the-art on object addition while maintaining competitive results on
full scene synthesis.

</details>


### [173] [Efficient Test-time Adaptive Object Detection via Sensitivity-Guided Pruning](https://arxiv.org/abs/2506.02462)
*Kunyu Wang,Xueyang Fu,Xin Lu,Chengjie Ge,Chengzhi Cao,Wei Zhai,Zheng-Jun Zha*

Main category: cs.CV

Relevance: 40.0

TL;DR: 提出了一种基于剪枝的高效持续测试时自适应目标检测方法，通过敏感度引导的通道剪枝策略和随机通道重新激活机制，在减少计算开销的同时提升性能。


<details>
  <summary>Details</summary>
Motivation: 观察到并非所有源特征都对目标域有益，某些域敏感特征通道可能对性能产生负面影响，因此提出剪枝策略优化计算效率。

Method: 引入敏感度引导的通道剪枝策略，量化通道对域差异的敏感度，并通过加权稀疏正则化选择性剪枝；同时提出随机通道重新激活机制恢复潜在有用特征。

Result: 在三个基准测试中，该方法在减少12%计算开销（FLOPs）的同时，性能优于当前最优方法。

Conclusion: 该方法在计算效率和性能之间取得了平衡，适用于资源受限场景。

Abstract: Continual test-time adaptive object detection (CTTA-OD) aims to online adapt
a source pre-trained detector to ever-changing environments during inference
under continuous domain shifts. Most existing CTTA-OD methods prioritize
effectiveness while overlooking computational efficiency, which is crucial for
resource-constrained scenarios. In this paper, we propose an efficient CTTA-OD
method via pruning. Our motivation stems from the observation that not all
learned source features are beneficial; certain domain-sensitive feature
channels can adversely affect target domain performance. Inspired by this, we
introduce a sensitivity-guided channel pruning strategy that quantifies each
channel based on its sensitivity to domain discrepancies at both image and
instance levels. We apply weighted sparsity regularization to selectively
suppress and prune these sensitive channels, focusing adaptation efforts on
invariant ones. Additionally, we introduce a stochastic channel reactivation
mechanism to restore pruned channels, enabling recovery of potentially useful
features and mitigating the risks of early pruning. Extensive experiments on
three benchmarks show that our method achieves superior adaptation performance
while reducing computational overhead by 12% in FLOPs compared to the recent
SOTA method.

</details>


### [174] [Towards Better De-raining Generalization via Rainy Characteristics Memorization and Replay](https://arxiv.org/abs/2506.02477)
*Kunyu Wang,Xueyang Fu,Chengzhi Cao,Chengjie Ge,Wei Zhai,Zheng-Jun Zha*

Main category: cs.CV

Relevance: 40.0

TL;DR: 论文提出了一种新框架，通过逐步扩展数据集提升图像去雨网络的适应性，结合GAN和知识蒸馏技术，模拟人脑学习机制，显著提升性能。


<details>
  <summary>Details</summary>
Motivation: 现有图像去雨方法因数据集有限，在多样化真实场景中表现不佳，需改进其适应能力。

Method: 使用GAN捕获新数据特征，结合知识蒸馏和回放机制，模拟人脑互补学习系统，训练去雨网络。

Result: 在三个基准网络上测试，框架能持续积累知识，并在新场景中超越现有方法。

Conclusion: 该框架通过模拟人脑学习机制，有效提升去雨网络的适应性和性能。

Abstract: Current image de-raining methods primarily learn from a limited dataset,
leading to inadequate performance in varied real-world rainy conditions. To
tackle this, we introduce a new framework that enables networks to
progressively expand their de-raining knowledge base by tapping into a growing
pool of datasets, significantly boosting their adaptability. Drawing
inspiration from the human brain's ability to continuously absorb and
generalize from ongoing experiences, our approach borrow the mechanism of the
complementary learning system. Specifically, we first deploy Generative
Adversarial Networks (GANs) to capture and retain the unique features of new
data, mirroring the hippocampus's role in learning and memory. Then, the
de-raining network is trained with both existing and GAN-synthesized data,
mimicking the process of hippocampal replay and interleaved learning.
Furthermore, we employ knowledge distillation with the replayed data to
replicate the synergy between the neocortex's activity patterns triggered by
hippocampal replays and the pre-existing neocortical knowledge. This
comprehensive framework empowers the de-raining network to amass knowledge from
various datasets, continually enhancing its performance on previously unseen
rainy scenes. Our testing on three benchmark de-raining networks confirms the
framework's effectiveness. It not only facilitates continuous knowledge
accumulation across six datasets but also surpasses state-of-the-art methods in
generalizing to new real-world scenarios.

</details>


### [175] [Flexiffusion: Training-Free Segment-Wise Neural Architecture Search for Efficient Diffusion Models](https://arxiv.org/abs/2506.02488)
*Hongtao Huang,Xiaojun Chang,Lina Yao*

Main category: cs.CV

Relevance: 40.0

TL;DR: Flexiffusion是一种无需训练的神经架构搜索（NAS）框架，通过动态组合计算步骤类型（完整、部分、跳过）来优化扩散模型（DMs）的生成速度和架构，显著降低计算成本。


<details>
  <summary>Details</summary>
Motivation: 扩散模型（DMs）因多步迭代推理导致计算成本高，现有NAS方法受限于重新训练需求、指数级搜索复杂度和缓慢的评估过程。

Method: Flexiffusion将生成过程分解为等长片段，动态组合完整计算、缓存重用计算和跳过计算三种步骤类型，并引入轻量级评估指标rFID。

Result: Flexiffusion在多个DMs上实现至少2倍加速，FID退化低于5%，在Stable Diffusion上达到5.1倍加速且CLIP分数几乎不变。

Conclusion: Flexiffusion为高效搜索高质量DMs提供了资源节约的新范式。

Abstract: Diffusion models (DMs) are powerful generative models capable of producing
high-fidelity images but are constrained by high computational costs due to
iterative multi-step inference. While Neural Architecture Search (NAS) can
optimize DMs, existing methods are hindered by retraining requirements,
exponential search complexity from step-wise optimization, and slow evaluation
relying on massive image generation. To address these challenges, we propose
Flexiffusion, a training-free NAS framework that jointly optimizes generation
schedules and model architectures without modifying pre-trained parameters. Our
key insight is to decompose the generation process into flexible segments of
equal length, where each segment dynamically combines three step types: full
(complete computation), partial (cache-reused computation), and null (skipped
computation). This segment-wise search space reduces the candidate pool
exponentially compared to step-wise NAS while preserving architectural
diversity. Further, we introduce relative FID (rFID), a lightweight evaluation
metric for NAS that measures divergence from a teacher model's outputs instead
of ground truth, slashing evaluation time by over $90\%$. In practice,
Flexiffusion achieves at least $2\times$ acceleration across LDMs, Stable
Diffusion, and DDPMs on ImageNet and MS-COCO, with FID degradation under $5\%$,
outperforming prior NAS and caching methods. Notably, it attains $5.1\times$
speedup on Stable Diffusion with near-identical CLIP scores. Our work pioneers
a resource-efficient paradigm for searching high-speed DMs without sacrificing
quality.

</details>


### [176] [MemoryOut: Learning Principal Features via Multimodal Sparse Filtering Network for Semi-supervised Video Anomaly Detection](https://arxiv.org/abs/2506.02535)
*Juntong Li,Lingwei Dang,Yukun Su,Yun Hao,Qingxin Xiao,Yongwei Nie,Qingyao Wu*

Main category: cs.CV

Relevance: 40.0

TL;DR: 该论文提出了一种新的视频异常检测框架，通过稀疏特征过滤模块（SFFM）抑制过度泛化，并结合视觉语言模型（VLM）实现多模态联合建模，解决了现有方法在语义识别和泛化能力上的不足。


<details>
  <summary>Details</summary>
Motivation: 现有视频异常检测方法在泛化能力和语义识别方面存在局限性，无法有效区分正常与异常模式。

Method: 提出SFFM模块动态过滤异常信息，并设计MoE架构增强特征多样性；结合VLM生成文本描述，实现多模态联合建模。

Result: 在多个公开数据集上的实验验证了该框架的有效性。

Conclusion: 该框架通过稀疏特征过滤和多模态建模，显著提升了视频异常检测的性能。

Abstract: Video Anomaly Detection (VAD) methods based on reconstruction or prediction
face two critical challenges: (1) strong generalization capability often
results in accurate reconstruction or prediction of abnormal events, making it
difficult to distinguish normal from abnormal patterns; (2) reliance only on
low-level appearance and motion cues limits their ability to identify
high-level semantic in abnormal events from complex scenes. To address these
limitations, we propose a novel VAD framework with two key innovations. First,
to suppress excessive generalization, we introduce the Sparse Feature Filtering
Module (SFFM) that employs bottleneck filters to dynamically and adaptively
remove abnormal information from features. Unlike traditional memory modules,
it does not need to memorize the normal prototypes across the training dataset.
Further, we design the Mixture of Experts (MoE) architecture for SFFM. Each
expert is responsible for extracting specialized principal features during
running time, and different experts are selectively activated to ensure the
diversity of the learned principal features. Second, to overcome the neglect of
semantics in existing methods, we integrate a Vision-Language Model (VLM) to
generate textual descriptions for video clips, enabling comprehensive joint
modeling of semantic, appearance, and motion cues. Additionally, we enforce
modality consistency through semantic similarity constraints and motion
frame-difference contrastive loss. Extensive experiments on multiple public
datasets validate the effectiveness of our multimodal joint modeling framework
and sparse feature filtering paradigm. Project page at
https://qzfm.github.io/sfn_vad_project_page/.

</details>


### [177] [Probabilistic Online Event Downsampling](https://arxiv.org/abs/2506.02547)
*Andreu Girbau-Xalabarder,Jun Nagata,Shinichi Sumiyoshi*

Main category: cs.CV

Relevance: 40.0

TL;DR: POLED是一个概率框架，通过事件重要性概率密度函数（ePDF）动态估计事件重要性，支持场景自适应和零采样事件下采样，在多个任务中表现优异。


<details>
  <summary>Details</summary>
Motivation: 事件相机的高时间分辨率带来高带宽和计算需求，现有下采样方法依赖固定启发式策略，适应性不足。

Method: 提出POLED框架，使用ePDF动态估计事件重要性，支持零采样下采样，并设计轮廓保留ePDF。

Result: 在四个数据集和任务（分类、插值、表面法线估计、检测）中验证了智能采样对性能的重要性。

Conclusion: POLED框架在事件预算限制下保持性能，具有场景自适应能力。

Abstract: Event cameras capture scene changes asynchronously on a per-pixel basis,
enabling extremely high temporal resolution. However, this advantage comes at
the cost of high bandwidth, memory, and computational demands. To address this,
prior work has explored event downsampling, but most approaches rely on fixed
heuristics or threshold-based strategies, limiting their adaptability. Instead,
we propose a probabilistic framework, POLED, that models event importance
through an event-importance probability density function (ePDF), which can be
arbitrarily defined and adapted to different applications. Our approach
operates in a purely online setting, estimating event importance on-the-fly
from raw event streams, enabling scene-specific adaptation. Additionally, we
introduce zero-shot event downsampling, where downsampled events must remain
usable for models trained on the original event stream, without task-specific
adaptation. We design a contour-preserving ePDF that prioritizes structurally
important events and evaluate our method across four datasets and tasks--object
classification, image interpolation, surface normal estimation, and object
detection--demonstrating that intelligent sampling is crucial for maintaining
performance under event-budget constraints.

</details>


### [178] [DCI: Dual-Conditional Inversion for Boosting Diffusion-Based Image Editing](https://arxiv.org/abs/2506.02560)
*Zixiang Li,Haoyu Wang,Wei Wang,Chuangchuang Tan,Yunchao Wei,Yao Zhao*

Main category: cs.CV

Relevance: 40.0

TL;DR: 论文提出了一种名为Dual-Conditional Inversion (DCI)的新框架，用于解决扩散模型中反转任务的准确性与编辑灵活性之间的权衡问题。


<details>
  <summary>Details</summary>
Motivation: 扩散模型在图像生成和编辑任务中表现出色，但现有的反转方法在重建准确性和编辑灵活性之间存在固有矛盾。

Method: DCI通过联合条件化源提示和参考图像，将反转过程建模为双条件固定点优化问题，最小化潜在噪声差距和重建误差。

Result: 实验表明，DCI在多个编辑任务中达到最先进性能，显著提升了重建质量和编辑精度，同时表现出较强的鲁棒性和泛化性。

Conclusion: DCI为反转过程提供了新的理解，并在性能和通用性上取得了显著进展。

Abstract: Diffusion models have achieved remarkable success in image generation and
editing tasks. Inversion within these models aims to recover the latent noise
representation for a real or generated image, enabling reconstruction, editing,
and other downstream tasks. However, to date, most inversion approaches suffer
from an intrinsic trade-off between reconstruction accuracy and editing
flexibility. This limitation arises from the difficulty of maintaining both
semantic alignment and structural consistency during the inversion process. In
this work, we introduce Dual-Conditional Inversion (DCI), a novel framework
that jointly conditions on the source prompt and reference image to guide the
inversion process. Specifically, DCI formulates the inversion process as a
dual-condition fixed-point optimization problem, minimizing both the latent
noise gap and the reconstruction error under the joint guidance. This design
anchors the inversion trajectory in both semantic and visual space, leading to
more accurate and editable latent representations. Our novel setup brings new
understanding to the inversion process. Extensive experiments demonstrate that
DCI achieves state-of-the-art performance across multiple editing tasks,
significantly improving both reconstruction quality and editing precision.
Furthermore, we also demonstrate that our method achieves strong results in
reconstruction tasks, implying a degree of robustness and generalizability
approaching the ultimate goal of the inversion process.

</details>


### [179] [Contrast & Compress: Learning Lightweight Embeddings for Short Trajectories](https://arxiv.org/abs/2506.02571)
*Abhishek Vivekanandan,Christian Hubschneider,J. Marius Zöllner*

Main category: cs.CV

Relevance: 40.0

TL;DR: 提出了一种基于Transformer编码器和对比三元组损失的框架，用于学习短轨迹的固定维度嵌入，强调方向意图的捕捉。在Argoverse 2数据集上，余弦相似性目标表现优于FFT基线。


<details>
  <summary>Details</summary>
Motivation: 现有方法依赖计算密集型启发式或缺乏可解释性的潜在锚表示，无法满足实时系统对可扩展和可解释运动先验的需求。

Method: 使用Transformer编码器和对比三元组损失，分析余弦和FFT相似性度量在对比学习中的作用。

Result: 余弦相似性目标在轨迹聚类和检索任务中表现更优，低维嵌入（如16维）在性能和计算开销间取得平衡。

Conclusion: 该框架提供了紧凑、语义明确且高效的轨迹表示，为透明可控的运动预测管道奠定了基础。

Abstract: The ability to retrieve semantically and directionally similar short-range
trajectories with both accuracy and efficiency is foundational for downstream
applications such as motion forecasting and autonomous navigation. However,
prevailing approaches often depend on computationally intensive heuristics or
latent anchor representations that lack interpretability and controllability.
In this work, we propose a novel framework for learning fixed-dimensional
embeddings for short trajectories by leveraging a Transformer encoder trained
with a contrastive triplet loss that emphasize the importance of discriminative
feature spaces for trajectory data. We analyze the influence of Cosine and
FFT-based similarity metrics within the contrastive learning paradigm, with a
focus on capturing the nuanced directional intent that characterizes short-term
maneuvers. Our empirical evaluation on the Argoverse 2 dataset demonstrates
that embeddings shaped by Cosine similarity objectives yield superior
clustering of trajectories by both semantic and directional attributes,
outperforming FFT-based baselines in retrieval tasks. Notably, we show that
compact Transformer architectures, even with low-dimensional embeddings (e.g.,
16 dimensions, but qualitatively down to 4), achieve a compelling balance
between retrieval performance (minADE, minFDE) and computational overhead,
aligning with the growing demand for scalable and interpretable motion priors
in real-time systems. The resulting embeddings provide a compact, semantically
meaningful, and efficient representation of trajectory data, offering a robust
alternative to heuristic similarity measures and paving the way for more
transparent and controllable motion forecasting pipelines.

</details>


### [180] [One-Step Diffusion-based Real-World Image Super-Resolution with Visual Perception Distillation](https://arxiv.org/abs/2506.02605)
*Xue Wu,Jingwei Xin,Zhijun Tu,Jie Hu,Jie Li,Nannan Wang,Xinbo Gao*

Main category: cs.CV

Relevance: 40.0

TL;DR: VPD-SR是一种新颖的视觉感知扩散蒸馏框架，专为图像超分辨率设计，通过显式语义感知监督和高频感知损失，实现高效的一步超分辨率重建。


<details>
  <summary>Details</summary>
Motivation: 现有基于扩散的超分辨率方法在推理速度和语义对齐上存在不足，导致感知质量不佳。VPD-SR旨在解决这些问题。

Method: 结合CLIP模型的语义监督和高频感知损失，设计ESS和HFP组件，并通过对抗训练增强生成内容的真实性。

Result: 在合成和真实数据集上，VPD-SR在一步采样下优于现有方法和教师模型。

Conclusion: VPD-SR显著提升了超分辨率任务的感知质量和语义一致性。

Abstract: Diffusion-based models have been widely used in various visual generation
tasks, showing promising results in image super-resolution (SR), while
typically being limited by dozens or even hundreds of sampling steps. Although
existing methods aim to accelerate the inference speed of multi-step
diffusion-based SR methods through knowledge distillation, their generated
images exhibit insufficient semantic alignment with real images, resulting in
suboptimal perceptual quality reconstruction, specifically reflected in the
CLIPIQA score. These methods still have many challenges in perceptual quality
and semantic fidelity. Based on the challenges, we propose VPD-SR, a novel
visual perception diffusion distillation framework specifically designed for
SR, aiming to construct an effective and efficient one-step SR model.
Specifically, VPD-SR consists of two components: Explicit Semantic-aware
Supervision (ESS) and High-Frequency Perception (HFP) loss. Firstly, the ESS
leverages the powerful visual perceptual understanding capabilities of the CLIP
model to extract explicit semantic supervision, thereby enhancing semantic
consistency. Then, Considering that high-frequency information contributes to
the visual perception quality of images, in addition to the vanilla
distillation loss, the HFP loss guides the student model to restore the missing
high-frequency details in degraded images that are critical for enhancing
perceptual quality. Lastly, we expand VPD-SR in adversarial training manner to
further enhance the authenticity of the generated content. Extensive
experiments conducted on synthetic and real-world datasets demonstrate that the
proposed VPD-SR achieves superior performance compared to both previous
state-of-the-art methods and the teacher model with just one-step sampling.

</details>


### [181] [Self-Disentanglement and Re-Composition for Cross-Domain Few-Shot Segmentation](https://arxiv.org/abs/2506.02677)
*Jintao Tong,Yixiong Zou,Guangyao Chen,Yuhua Li,Ruixuan Li*

Main category: cs.CV

Relevance: 40.0

TL;DR: 论文提出了一种解决跨域少样本分割（CD-FSS）中特征纠缠问题的方法，通过分解ViT结构并加权比较其组件，显著提升了性能。


<details>
  <summary>Details</summary>
Motivation: 当前CD-FSS方法存在特征纠缠问题，导致源域知识难以迁移到目标域。

Method: 分解ViT结构，分析特征纠缠的根源，并提出加权比较ViT组件的方法以学习解耦特征。

Result: 在1-shot和5-shot设置下，模型平均准确率分别比现有最佳方法提高了1.92%和1.88%。

Conclusion: 通过解耦ViT组件并加权比较，有效解决了特征纠缠问题，提升了跨域少样本分割的性能。

Abstract: Cross-Domain Few-Shot Segmentation (CD-FSS) aims to transfer knowledge from a
source-domain dataset to unseen target-domain datasets with limited
annotations. Current methods typically compare the distance between training
and testing samples for mask prediction. However, we find an entanglement
problem exists in this widely adopted method, which tends to bind sourcedomain
patterns together and make each of them hard to transfer. In this paper, we aim
to address this problem for the CD-FSS task. We first find a natural
decomposition of the ViT structure, based on which we delve into the
entanglement problem for an interpretation. We find the decomposed ViT
components are crossly compared between images in distance calculation, where
the rational comparisons are entangled with those meaningless ones by their
equal importance, leading to the entanglement problem. Based on this
interpretation, we further propose to address the entanglement problem by
learning to weigh for all comparisons of ViT components, which learn
disentangled features and re-compose them for the CD-FSS task, benefiting both
the generalization and finetuning. Experiments show that our model outperforms
the state-of-the-art CD-FSS method by 1.92% and 1.88% in average accuracy under
1-shot and 5-shot settings, respectively.

</details>


### [182] [Solving Inverse Problems with FLAIR](https://arxiv.org/abs/2506.02680)
*Julius Erbach,Dominik Narnhofer,Andreas Dombos,Bernt Schiele,Jan Eric Lenssen,Konrad Schindler*

Main category: cs.CV

Relevance: 40.0

TL;DR: FLAIR是一种基于流的生成模型框架，用于解决逆成像问题，通过变分目标和轨迹调整提升重建质量和样本多样性。


<details>
  <summary>Details</summary>
Motivation: 尽管基于流的生成模型（如Stable Diffusion 3）在图像生成中表现优异，但在逆成像问题中尚未达到类似效果，主要障碍包括非线性映射、数据似然难解和罕见模式恢复困难。

Method: 提出FLAIR框架，结合变分目标和确定性轨迹调整，解耦数据保真度和正则化优化，并引入时间依赖性校准方案。

Result: 在标准成像基准测试中，FLAIR在重建质量和样本多样性上优于现有扩散和基于流的方法。

Conclusion: FLAIR为逆成像问题提供了一种高效且无需训练的解决方案，显著提升了基于流生成模型的适用性。

Abstract: Flow-based latent generative models such as Stable Diffusion 3 are able to
generate images with remarkable quality, even enabling photorealistic
text-to-image generation. Their impressive performance suggests that these
models should also constitute powerful priors for inverse imaging problems, but
that approach has not yet led to comparable fidelity. There are several key
obstacles: (i) the encoding into a lower-dimensional latent space makes the
underlying (forward) mapping non-linear; (ii) the data likelihood term is
usually intractable; and (iii) learned generative models struggle to recover
rare, atypical data modes during inference. We present FLAIR, a novel training
free variational framework that leverages flow-based generative models as a
prior for inverse problems. To that end, we introduce a variational objective
for flow matching that is agnostic to the type of degradation, and combine it
with deterministic trajectory adjustments to recover atypical modes. To enforce
exact consistency with the observed data, we decouple the optimization of the
data fidelity and regularization terms. Moreover, we introduce a time-dependent
calibration scheme in which the strength of the regularization is modulated
according to off-line accuracy estimates. Results on standard imaging
benchmarks demonstrate that FLAIR consistently outperforms existing diffusion-
and flow-based methods in terms of reconstruction quality and sample diversity.

</details>


### [183] [Open-PMC-18M: A High-Fidelity Large Scale Medical Dataset for Multimodal Representation Learning](https://arxiv.org/abs/2506.02738)
*Negin Baghbanzadeh,Sajad Ashkezari,Elham Dolatabadi,Arash Afkanpour*

Main category: cs.CV

Relevance: 40.0

TL;DR: 论文提出了一种基于Transformer的目标检测方法，用于大规模提取生物医学文献中的复合图子图，并发布了OPEN-PMC-18M数据集，展示了在视觉-语言模型中的性能提升。


<details>
  <summary>Details</summary>
Motivation: 生物医学文献中复合图的子图提取问题尚未得到大规模解决，且现有方法在数据集规模和泛化性上有限。论文旨在探索高质量图像-文本对齐对视觉-语言模型表示学习的影响。

Method: 使用基于Transformer的目标检测方法，在50万张合成复合图上训练，构建了OPEN-PMC-18M数据集，包含1800万个子图-标题对。

Result: 在ImageCLEF 2016和合成基准测试中达到SOTA性能，视觉-语言模型在检索、零样本分类和鲁棒性任务上表现优异。

Conclusion: 研究支持了高质量图像-文本对齐对视觉-语言模型的重要性，并提供了可复现的基准数据集和模型。

Abstract: Compound figures, which are multi-panel composites containing diverse
subfigures, are ubiquitous in biomedical literature, yet large-scale subfigure
extraction remains largely unaddressed. Prior work on subfigure extraction has
been limited in both dataset size and generalizability, leaving a critical open
question: How does high-fidelity image-text alignment via large-scale subfigure
extraction impact representation learning in vision-language models? We address
this gap by introducing a scalable subfigure extraction pipeline based on
transformer-based object detection, trained on a synthetic corpus of 500,000
compound figures, and achieving state-of-the-art performance on both ImageCLEF
2016 and synthetic benchmarks. Using this pipeline, we release OPEN-PMC-18M, a
large-scale high quality biomedical vision-language dataset comprising 18
million clinically relevant subfigure-caption pairs spanning radiology,
microscopy, and visible light photography. We train and evaluate
vision-language models on our curated datasets and show improved performance
across retrieval, zero-shot classification, and robustness benchmarks,
outperforming existing baselines. We release our dataset, models, and code to
support reproducible benchmarks and further study into biomedical
vision-language modeling and representation learning.

</details>


### [184] [A Dynamic Transformer Network for Vehicle Detection](https://arxiv.org/abs/2506.02765)
*Chunwei Tian,Kai Liu,Bob Zhang,Zhixiang Huang,Chia-Wen Lin,David Zhang*

Main category: cs.CV

Relevance: 40.0

TL;DR: 论文提出了一种动态Transformer网络（DTNet），用于车辆检测，通过动态卷积和混合注意力机制提升检测性能。


<details>
  <summary>Details</summary>
Motivation: 现有基于深度网络的车辆检测算法在光照和遮挡差异下性能受限，需要更适应性的方法。

Method: DTNet结合动态卷积生成权重，利用混合注意力机制（通道注意力与Transformer）增强信息提取，并通过空间位置信息优化结构信息。

Result: 实验表明DTNet在车辆检测中具有竞争力。

Conclusion: DTNet通过动态和注意力机制提升了车辆检测的适应性。

Abstract: Stable consumer electronic systems can assist traffic better. Good traffic
consumer electronic systems require collaborative work between traffic
algorithms and hardware. However, performance of popular traffic algorithms
containing vehicle detection methods based on deep networks via learning data
relation rather than learning differences in different lighting and occlusions
is limited. In this paper, we present a dynamic Transformer network for vehicle
detection (DTNet). DTNet utilizes a dynamic convolution to guide a deep network
to dynamically generate weights to enhance adaptability of an obtained
detector. Taking into relations of different information account, a mixed
attention mechanism based channel attention and Transformer is exploited to
strengthen relations of channels and pixels to extract more salient information
for vehicle detection. To overcome the drawback of difference in an image
account, a translation-variant convolution relies on spatial location
information to refine obtained structural information for vehicle detection.
Experimental results illustrate that our DTNet is competitive for vehicle
detection. Code of the proposed DTNet can be obtained at
https://github.com/hellloxiaotian/DTNet.

</details>


### [185] [FreeScene: Mixed Graph Diffusion for 3D Scene Synthesis from Free Prompts](https://arxiv.org/abs/2506.02781)
*Tongyuan Bai,Wangyuanfan Bai,Dong Chen,Tieru Wu,Manyi Li,Rui Ma*

Main category: cs.CV

Relevance: 40.0

TL;DR: FreeScene是一个用户友好的框架，通过结合文本描述和参考图像实现精细的3D室内场景合成控制，优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有方法在3D室内场景合成中要么控制粗糙（基于语言），要么复杂（基于图设计），缺乏便捷性与精细控制的平衡。

Method: FreeScene支持自由输入（文本/图像），通过VLM-based Graph Designer分析并整合为图表示，再使用MG-DiT（混合图扩散Transformer）进行图感知去噪生成场景。

Result: FreeScene在生成质量和可控性上优于现有方法，支持多种任务（如文本到场景、图到场景等）。

Conclusion: FreeScene为3D场景合成提供了高效且用户友好的解决方案，统一了文本和图控制。

Abstract: Controllability plays a crucial role in the practical applications of 3D
indoor scene synthesis. Existing works either allow rough language-based
control, that is convenient but lacks fine-grained scene customization, or
employ graph based control, which offers better controllability but demands
considerable knowledge for the cumbersome graph design process. To address
these challenges, we present FreeScene, a user-friendly framework that enables
both convenient and effective control for indoor scene synthesis.Specifically,
FreeScene supports free-form user inputs including text description and/or
reference images, allowing users to express versatile design intentions. The
user inputs are adequately analyzed and integrated into a graph representation
by a VLM-based Graph Designer. We then propose MG-DiT, a Mixed Graph Diffusion
Transformer, which performs graph-aware denoising to enhance scene generation.
Our MG-DiT not only excels at preserving graph structure but also offers broad
applicability to various tasks, including, but not limited to, text-to-scene,
graph-to-scene, and rearrangement, all within a single model. Extensive
experiments demonstrate that FreeScene provides an efficient and user-friendly
solution that unifies text-based and graph based scene synthesis, outperforming
state-of-the-art methods in terms of both generation quality and
controllability in a range of applications.

</details>


### [186] [Random Registers for Cross-Domain Few-Shot Learning](https://arxiv.org/abs/2506.02843)
*Shuai Yi,Yixiong Zou,Yuhua Li,Ruixuan Li*

Main category: cs.CV

Relevance: 40.0

TL;DR: 论文研究了Vision Transformer（ViT）在跨域少样本学习（CDFSL）中的迁移性问题，发现提示调优（prompt tuning）可能损害目标域性能，而随机寄存器（random registers）能提升性能。作者提出了一种基于随机寄存器的简单有效方法，通过扰动注意力图增强模型迁移性。


<details>
  <summary>Details</summary>
Motivation: 探索ViT在CDFSL任务中的迁移性，解决提示调优可能导致的过拟合问题，提升模型在目标域的表现。

Method: 通过实验发现随机寄存器能改善ViT的迁移性，提出在图像令牌的语义区域添加随机寄存器以增强注意力图扰动。

Result: 在四个基准测试中验证了方法的有效性，实现了最先进的性能。

Conclusion: 随机寄存器是一种有效的注意力扰动方法，能帮助ViT在CDFSL任务中找到更平坦的最小值，提升迁移性。

Abstract: Cross-domain few-shot learning (CDFSL) aims to transfer knowledge from a
data-sufficient source domain to data-scarce target domains. Although Vision
Transformer (ViT) has shown superior capability in many vision tasks, its
transferability against huge domain gaps in CDFSL is still under-explored. In
this paper, we find an intriguing phenomenon: during the source-domain
training, prompt tuning, as a common way to train ViT, could be harmful for the
generalization of ViT in target domains, but setting them to random noises
(i.e., random registers) could consistently improve target-domain performance.
We then delve into this phenomenon for an interpretation. We find that
learnable prompts capture domain information during the training on the source
dataset, which views irrelevant visual patterns as vital cues for recognition.
This can be viewed as a kind of overfitting and increases the sharpness of the
loss landscapes. In contrast, random registers are essentially a novel way of
perturbing attention for the sharpness-aware minimization, which helps the
model find a flattened minimum in loss landscapes, increasing the
transferability. Based on this phenomenon and interpretation, we further
propose a simple but effective approach for CDFSL to enhance the perturbation
on attention maps by adding random registers on the semantic regions of image
tokens, improving the effectiveness and efficiency of random registers.
Extensive experiments on four benchmarks validate our rationale and
state-of-the-art performance. Codes and models are available at
https://github.com/shuaiyi308/REAP.

</details>


### [187] [Hierarchical Self-Prompting SAM: A Prompt-Free Medical Image Segmentation Framework](https://arxiv.org/abs/2506.02854)
*Mengmeng Zhang,Xingyuan Dai,Yicheng Sun,Jing Wang,Yueyang Yao,Xiaoyan Gong,Fuze Cong,Feiyue Wang,Yisheng Lv*

Main category: cs.CV

Relevance: 40.0

TL;DR: HSP-SAM是一种自提示框架，用于无提示的医学图像分割，通过学习抽象提示提升性能。


<details>
  <summary>Details</summary>
Motivation: SAM在医学图像分割中依赖手动提示，限制了其应用。现有方法难以去除这种依赖。

Method: 提出Hierarchical Self-Prompting SAM (HSP-SAM)，通过自学习抽象提示实现无提示分割。

Result: 在多项任务中表现优异，泛化能力强，性能提升达14.04%。

Conclusion: 抽象提示比位置提示包含更丰富的语义信息，提升了模型的鲁棒性和泛化能力。

Abstract: Although the Segment Anything Model (SAM) is highly effective in natural
image segmentation, it requires dependencies on prompts, which limits its
applicability to medical imaging where manual prompts are often unavailable.
Existing efforts to fine-tune SAM for medical segmentation typically struggle
to remove this dependency. We propose Hierarchical Self-Prompting SAM
(HSP-SAM), a novel self-prompting framework that enables SAM to achieve strong
performance in prompt-free medical image segmentation. Unlike previous
self-prompting methods that remain limited to positional prompts similar to
vanilla SAM, we are the first to introduce learning abstract prompts during the
self-prompting process. This simple and intuitive self-prompting framework
achieves superior performance on classic segmentation tasks such as polyp and
skin lesion segmentation, while maintaining robustness across diverse medical
imaging modalities. Furthermore, it exhibits strong generalization to unseen
datasets, achieving improvements of up to 14.04% over previous state-of-the-art
methods on some challenging benchmarks. These results suggest that abstract
prompts encapsulate richer and higher-dimensional semantic information compared
to positional prompts, thereby enhancing the model's robustness and
generalization performance. All models and codes will be released upon
acceptance.

</details>


### [188] [Pan-Arctic Permafrost Landform and Human-built Infrastructure Feature Detection with Vision Transformers and Location Embeddings](https://arxiv.org/abs/2506.02868)
*Amal S. Perera,David Fernandez,Chandi Witharana,Elias Manos,Michael Pimenta,Anna K. Liljedahl,Ingmar Nitze,Yili Yang,Todd Nicholson,Chia-Yu Hsu,Wenwen Li,Guido Grosse*

Main category: cs.CV

Relevance: 40.0

TL;DR: 论文探讨了在北极遥感任务中使用预训练的Vision Transformers（ViTs）结合地理空间位置嵌入的方法，以提升对永久冻土地貌、融冻扰动和人类基础设施的检测性能。


<details>
  <summary>Details</summary>
Motivation: 北极地区的高分辨率遥感图像处理面临数据量大、标注数据稀缺以及模型泛化能力不足的挑战，需要更高效的模型来捕捉全局上下文和长距离依赖关系。

Method: 采用预训练的ViTs作为特征提取器，并集成地理空间位置嵌入，通过自监督学习解决标注数据不足的问题，评估了多种图像和位置嵌入的融合配置。

Result: 在三个任务中，ViTs结合位置嵌入的表现优于传统CNN模型，其中RTS检测的F1分数从0.84提升至0.92。

Conclusion: 研究表明，具有空间感知能力的Transformer模型在北极遥感任务中具有潜力，尤其是在数据稀缺和特征多样性高的场景下。

Abstract: Accurate mapping of permafrost landforms, thaw disturbances, and human-built
infrastructure at pan-Arctic scale using sub-meter satellite imagery is
increasingly critical. Handling petabyte-scale image data requires
high-performance computing and robust feature detection models. While
convolutional neural network (CNN)-based deep learning approaches are widely
used for remote sensing (RS),similar to the success in transformer based large
language models, Vision Transformers (ViTs) offer advantages in capturing
long-range dependencies and global context via attention mechanisms. ViTs
support pretraining via self-supervised learning-addressing the common
limitation of labeled data in Arctic feature detection and outperform CNNs on
benchmark datasets. Arctic also poses challenges for model generalization,
especially when features with the same semantic class exhibit diverse spectral
characteristics. To address these issues for Arctic feature detection, we
integrate geospatial location embeddings into ViTs to improve adaptation across
regions. This work investigates: (1) the suitability of pre-trained ViTs as
feature extractors for high-resolution Arctic remote sensing tasks, and (2) the
benefit of combining image and location embeddings. Using previously published
datasets for Arctic feature detection, we evaluate our models on three
tasks-detecting ice-wedge polygons (IWP), retrogressive thaw slumps (RTS), and
human-built infrastructure. We empirically explore multiple configurations to
fuse image embeddings and location embeddings. Results show that ViTs with
location embeddings outperform prior CNN-based models on two of the three tasks
including F1 score increase from 0.84 to 0.92 for RTS detection, demonstrating
the potential of transformer-based models with spatial awareness for Arctic RS
applications.

</details>


### [189] [GaRA-SAM: Robustifying Segment Anything Model with Gated-Rank Adaptation](https://arxiv.org/abs/2506.02882)
*Sohyun Lee,Yeho Kwon,Lukas Hoyer,Suha Kwak*

Main category: cs.CV

Relevance: 40.0

TL;DR: 论文提出了一种名为GaRA的方法，通过动态调整权重矩阵的秩来增强Segment Anything Model (SAM)的鲁棒性，显著提升了在输入退化情况下的性能。


<details>
  <summary>Details</summary>
Motivation: 提高SAM在输入退化情况下的鲁棒性，以支持其在自动驾驶等高风险应用中的部署。

Method: 引入轻量级适配器（GaRA），动态调整权重矩阵的秩，实现细粒度和输入感知的鲁棒化。

Result: GaRA-SAM在所有鲁棒分割基准测试中显著优于先前工作，在ACDC数据集上IoU得分提升了21.3%。

Conclusion: GaRA方法在不牺牲SAM泛化能力的情况下，有效提升了其鲁棒性。

Abstract: Improving robustness of the Segment Anything Model (SAM) to input
degradations is critical for its deployment in high-stakes applications such as
autonomous driving and robotics. Our approach to this challenge prioritizes
three key aspects: first, parameter efficiency to maintain the inherent
generalization capability of SAM; second, fine-grained and input-aware
robustification to precisely address the input corruption; and third, adherence
to standard training protocols for ease of training. To this end, we propose
gated-rank adaptation (GaRA). GaRA introduces lightweight adapters into
intermediate layers of the frozen SAM, where each adapter dynamically adjusts
the effective rank of its weight matrix based on the input by selectively
activating (rank-1) components of the matrix using a learned gating module.
This adjustment enables fine-grained and input-aware robustification without
compromising the generalization capability of SAM. Our model, GaRA-SAM,
significantly outperforms prior work on all robust segmentation benchmarks. In
particular, it surpasses the previous best IoU score by up to 21.3\%p on ACDC,
a challenging real corrupted image dataset.

</details>


### [190] [FORLA:Federated Object-centric Representation Learning with Slot Attention](https://arxiv.org/abs/2506.02964)
*Guiqiu Liao,Matjaz Jogan,Eric Eaton,Daniel A. Hashimoto*

Main category: cs.CV

Relevance: 40.0

TL;DR: FORLA是一个联邦学习框架，通过无监督槽注意力学习跨客户端的对象中心表示和特征适配，优于集中式基线。


<details>
  <summary>Details</summary>
Motivation: 解决联邦学习中跨异构无标签数据集学习高效视觉表示的挑战，需联合信息特征并解耦领域特定因素。

Method: 采用共享特征适配器和槽注意力模块，设计双分支师生架构，优化特征适配和跨领域对齐。

Result: 在多数据集上表现优于集中式基线，学习到紧凑且通用的跨领域表示。

Conclusion: 联邦槽注意力是分布式概念下可扩展无监督视觉表示学习的有效工具。

Abstract: Learning efficient visual representations across heterogeneous unlabeled
datasets remains a central challenge in federated learning. Effective federated
representations require features that are jointly informative across clients
while disentangling domain-specific factors without supervision. We introduce
FORLA, a novel framework for federated object-centric representation learning
and feature adaptation across clients using unsupervised slot attention. At the
core of our method is a shared feature adapter, trained collaboratively across
clients to adapt features from foundation models, and a shared slot attention
module that learns to reconstruct the adapted features. To optimize this
adapter, we design a two-branch student-teacher architecture. In each client, a
student decoder learns to reconstruct full features from foundation models,
while a teacher decoder reconstructs their adapted, low-dimensional
counterpart. The shared slot attention module bridges cross-domain learning by
aligning object-level representations across clients. Experiments in multiple
real-world datasets show that our framework not only outperforms centralized
baselines on object discovery but also learns a compact, universal
representation that generalizes well across domains. This work highlights
federated slot attention as an effective tool for scalable, unsupervised visual
representation learning from cross-domain data with distributed concepts.

</details>


### [191] [EDITOR: Effective and Interpretable Prompt Inversion for Text-to-Image Diffusion Models](https://arxiv.org/abs/2506.03067)
*Mingzhe Li,Gehao Zhang,Zhenting Wang,Shiqing Ma,Siqi Pan,Richard Cartwright,Juan Zhai*

Main category: cs.CV

Relevance: 40.0

TL;DR: 本文提出了一种名为\sys的提示反转技术，用于文本到图像扩散模型，通过预训练图像描述模型初始化嵌入，并在潜在空间中反向工程优化，最终通过嵌入到文本模型生成文本提示。实验表明，该方法在图像相似性、文本对齐和可解释性方面优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 提示反转任务在数据归属、模型溯源和水印验证等方面具有重要应用潜力，但现有方法在语义流畅性和效率上存在挑战。

Method: 结合预训练图像描述模型初始化嵌入，通过潜在空间反向工程优化嵌入，并使用嵌入到文本模型生成文本提示。

Result: 在MS COCO、LAION和Flickr等数据集上，该方法在图像相似性、文本对齐、提示可解释性和泛化性方面优于现有方法。

Conclusion: \sys方法在提示反转任务中表现出色，并展示了在跨概念图像合成、概念操作等任务中的应用潜力。

Abstract: Text-to-image generation models~(e.g., Stable Diffusion) have achieved
significant advancements, enabling the creation of high-quality and realistic
images based on textual descriptions. Prompt inversion, the task of identifying
the textual prompt used to generate a specific artifact, holds significant
potential for applications including data attribution, model provenance, and
watermarking validation. Recent studies introduced a delayed projection scheme
to optimize for prompts representative of the vocabulary space, though
challenges in semantic fluency and efficiency remain. Advanced image captioning
models or visual large language models can generate highly interpretable
prompts, but they often lack in image similarity. In this paper, we propose a
prompt inversion technique called \sys for text-to-image diffusion models,
which includes initializing embeddings using a pre-trained image captioning
model, refining them through reverse-engineering in the latent space, and
converting them to texts using an embedding-to-text model. Our experiments on
the widely-used datasets, such as MS COCO, LAION, and Flickr, show that our
method outperforms existing methods in terms of image similarity, textual
alignment, prompt interpretability and generalizability. We further illustrate
the application of our generated prompts in tasks such as cross-concept image
synthesis, concept manipulation, evolutionary multi-concept generation and
unsupervised segmentation.

</details>


### [192] [ORV: 4D Occupancy-centric Robot Video Generation](https://arxiv.org/abs/2506.03079)
*Xiuyu Yang,Bohan Li,Shaocong Xu,Nan Wang,Chongjie Ye,Zhaoxi Chen,Minghan Qin,Yikang Ding,Xin Jin,Hang Zhao,Hao Zhao*

Main category: cs.CV

Relevance: 40.0

TL;DR: ORV是一个基于4D语义占据序列的机器人视频生成框架，通过细粒度表示提升视频生成的精确性和泛化能力。


<details>
  <summary>Details</summary>
Motivation: 解决现有动作驱动生成模型在控制精度和泛化能力上的不足。

Method: 利用4D语义占据序列作为细粒度表示，提供更准确的语义和几何指导。

Result: ORV在多个数据集和子任务中表现优于基线方法，支持多视角视频生成。

Conclusion: ORV为机器人学习和仿真提供了高质量的视频生成解决方案。

Abstract: Acquiring real-world robotic simulation data through teleoperation is
notoriously time-consuming and labor-intensive. Recently, action-driven
generative models have gained widespread adoption in robot learning and
simulation, as they eliminate safety concerns and reduce maintenance efforts.
However, the action sequences used in these methods often result in limited
control precision and poor generalization due to their globally coarse
alignment. To address these limitations, we propose ORV, an Occupancy-centric
Robot Video generation framework, which utilizes 4D semantic occupancy
sequences as a fine-grained representation to provide more accurate semantic
and geometric guidance for video generation. By leveraging occupancy-based
representations, ORV enables seamless translation of simulation data into
photorealistic robot videos, while ensuring high temporal consistency and
precise controllability. Furthermore, our framework supports the simultaneous
generation of multi-view videos of robot gripping operations - an important
capability for downstream robotic learning tasks. Extensive experimental
results demonstrate that ORV consistently outperforms existing baseline methods
across various datasets and sub-tasks. Demo, Code and Model:
https://orangesodahub.github.io/ORV

</details>


### [193] [Explicitly Modeling Subcortical Vision with a Neuro-Inspired Front-End Improves CNN Robustness](https://arxiv.org/abs/2506.03089)
*Lucas Piper,Arlindo L. Oliveira,Tiago Marques*

Main category: cs.CV

Relevance: 40.0

TL;DR: EVNets结合了VOneBlock和SubcorticalBlock，提升了CNN的鲁棒性，并在对抗扰动、常见损坏和领域转移等任务中表现优于基准模型。


<details>
  <summary>Details</summary>
Motivation: 研究旨在通过模仿生物视觉系统（如V1和皮层下结构）来提升CNN的鲁棒性。

Method: 提出了Early Vision Networks (EVNets)，结合VOneBlock和新的SubcorticalBlock，后者基于神经科学模型设计。

Result: EVNets在多个鲁棒性测试中表现优于基准CNN，提升了8.5%，与数据增强技术结合时进一步提升7.3%。

Conclusion: EVNets展示了架构改进与数据增强技术的互补性，为提升模型鲁棒性提供了新思路。

Abstract: Convolutional neural networks (CNNs) trained on object recognition achieve
high task performance but continue to exhibit vulnerability under a range of
visual perturbations and out-of-domain images, when compared with biological
vision. Prior work has demonstrated that coupling a standard CNN with a
front-end block (VOneBlock) that mimics the primate primary visual cortex (V1)
can improve overall model robustness. Expanding on this, we introduce Early
Vision Networks (EVNets), a new class of hybrid CNNs that combine the VOneBlock
with a novel SubcorticalBlock, whose architecture draws from computational
models in neuroscience and is parameterized to maximize alignment with
subcortical responses reported across multiple experimental studies. Without
being optimized to do so, the assembly of the SubcorticalBlock with the
VOneBlock improved V1 alignment across most standard V1 benchmarks, and better
modeled extra-classical receptive field phenomena. In addition, EVNets exhibit
stronger emergent shape bias and overperform the base CNN architecture by 8.5%
on an aggregate benchmark of robustness evaluations, including adversarial
perturbations, common corruptions, and domain shifts. Finally, we show that
EVNets can be further improved when paired with a state-of-the-art data
augmentation technique, surpassing the performance of the isolated data
augmentation approach by 7.3% on our robustness benchmark. This result reveals
complementary benefits between changes in architecture to better mimic biology
and training-based machine learning approaches.

</details>


### [194] [ByteMorph: Benchmarking Instruction-Guided Image Editing with Non-Rigid Motions](https://arxiv.org/abs/2506.03107)
*Di Chang,Mingdeng Cao,Yichun Shi,Bo Liu,Shengqu Cai,Shijie Zhou,Weilin Huang,Gordon Wetzstein,Mohammad Soleymani,Peng Wang*

Main category: cs.CV

Relevance: 40.0

TL;DR: ByteMorph是一个基于指令的图像编辑框架，专注于非刚性运动，包括数据集ByteMorph-6M和基于Diffusion Transformer的模型ByteMorpher。


<details>
  <summary>Details</summary>
Motivation: 现有方法主要关注静态场景或刚性变换，无法处理动态运动的复杂编辑需求。

Method: 提出ByteMorph框架，包括大规模数据集ByteMorph-6M和基于DiT的模型ByteMorpher，采用运动引导数据生成和分层合成技术。

Result: 构建了包含600万对高分辨率图像的数据集和评估基准ByteMorph-Bench，支持多样化的非刚性运动编辑。

Conclusion: ByteMorph填补了动态运动图像编辑的空白，并提供了强大的基准模型和数据集。

Abstract: Editing images with instructions to reflect non-rigid motions, camera
viewpoint shifts, object deformations, human articulations, and complex
interactions, poses a challenging yet underexplored problem in computer vision.
Existing approaches and datasets predominantly focus on static scenes or rigid
transformations, limiting their capacity to handle expressive edits involving
dynamic motion. To address this gap, we introduce ByteMorph, a comprehensive
framework for instruction-based image editing with an emphasis on non-rigid
motions. ByteMorph comprises a large-scale dataset, ByteMorph-6M, and a strong
baseline model built upon the Diffusion Transformer (DiT), named ByteMorpher.
ByteMorph-6M includes over 6 million high-resolution image editing pairs for
training, along with a carefully curated evaluation benchmark ByteMorph-Bench.
Both capture a wide variety of non-rigid motion types across diverse
environments, human figures, and object categories. The dataset is constructed
using motion-guided data generation, layered compositing techniques, and
automated captioning to ensure diversity, realism, and semantic coherence. We
further conduct a comprehensive evaluation of recent instruction-based image
editing methods from both academic and commercial domains.

</details>


### [195] [DCM: Dual-Expert Consistency Model for Efficient and High-Quality Video Generation](https://arxiv.org/abs/2506.03123)
*Zhengyao Lv,Chenyang Si,Tianlin Pan,Zhaoxi Chen,Kwan-Yee K. Wong,Yu Qiao,Ziwei Liu*

Main category: cs.CV

Relevance: 40.0

TL;DR: 提出了一种双专家一致性模型（DCM），通过语义专家和细节专家的分工，解决了视频扩散模型蒸馏中的梯度冲突问题，显著提升了生成质量。


<details>
  <summary>Details</summary>
Motivation: 视频扩散模型在生成高质量视频时计算开销大，直接应用一致性模型会导致时间一致性和细节退化。

Method: 提出DCM模型，包含语义专家和细节专家，分别优化语义布局与运动、细节细化，并引入时间一致性损失和GAN损失。

Result: 在减少采样步骤的同时，实现了最先进的视觉质量。

Conclusion: DCM通过专家分工有效解决了视频扩散模型蒸馏中的问题。

Abstract: Diffusion Models have achieved remarkable results in video synthesis but
require iterative denoising steps, leading to substantial computational
overhead. Consistency Models have made significant progress in accelerating
diffusion models. However, directly applying them to video diffusion models
often results in severe degradation of temporal consistency and appearance
details. In this paper, by analyzing the training dynamics of Consistency
Models, we identify a key conflicting learning dynamics during the distillation
process: there is a significant discrepancy in the optimization gradients and
loss contributions across different timesteps. This discrepancy prevents the
distilled student model from achieving an optimal state, leading to compromised
temporal consistency and degraded appearance details. To address this issue, we
propose a parameter-efficient \textbf{Dual-Expert Consistency Model~(DCM)},
where a semantic expert focuses on learning semantic layout and motion, while a
detail expert specializes in fine detail refinement. Furthermore, we introduce
Temporal Coherence Loss to improve motion consistency for the semantic expert
and apply GAN and Feature Matching Loss to enhance the synthesis quality of the
detail expert.Our approach achieves state-of-the-art visual quality with
significantly reduced sampling steps, demonstrating the effectiveness of expert
specialization in video diffusion model distillation. Our code and models are
available at
\href{https://github.com/Vchitect/DCM}{https://github.com/Vchitect/DCM}.

</details>


### [196] [AnimeShooter: A Multi-Shot Animation Dataset for Reference-Guided Video Generation](https://arxiv.org/abs/2506.03126)
*Lu Qiu,Yizhuo Li,Yuying Ge,Yixiao Ge,Ying Shan,Xihui Liu*

Main category: cs.CV

Relevance: 40.0

TL;DR: AnimeShooter是一个参考引导的多镜头动画数据集，用于生成连贯的动画视频，包含分层注释和视觉一致性。AnimeShooterGen利用多模态大语言模型和视频扩散模型，展示了数据集的有效性。


<details>
  <summary>Details</summary>
Motivation: 现有公开数据集缺乏角色参考图像和连贯的多镜头视频生成能力，AnimeShooter填补了这一空白。

Method: 通过自动化流程构建AnimeShooter数据集，并利用MLLM和视频扩散模型生成连贯的多镜头视频。

Result: 实验结果表明，基于AnimeShooter训练的模型在跨镜头视觉一致性和参考视觉引导方面表现优异。

Conclusion: AnimeShooter为连贯动画视频生成提供了有价值的基准数据集。

Abstract: Recent advances in AI-generated content (AIGC) have significantly accelerated
animation production. To produce engaging animations, it is essential to
generate coherent multi-shot video clips with narrative scripts and character
references. However, existing public datasets primarily focus on real-world
scenarios with global descriptions, and lack reference images for consistent
character guidance. To bridge this gap, we present AnimeShooter, a
reference-guided multi-shot animation dataset. AnimeShooter features
comprehensive hierarchical annotations and strong visual consistency across
shots through an automated pipeline. Story-level annotations provide an
overview of the narrative, including the storyline, key scenes, and main
character profiles with reference images, while shot-level annotations
decompose the story into consecutive shots, each annotated with scene,
characters, and both narrative and descriptive visual captions. Additionally, a
dedicated subset, AnimeShooter-audio, offers synchronized audio tracks for each
shot, along with audio descriptions and sound sources. To demonstrate the
effectiveness of AnimeShooter and establish a baseline for the reference-guided
multi-shot video generation task, we introduce AnimeShooterGen, which leverages
Multimodal Large Language Models (MLLMs) and video diffusion models. The
reference image and previously generated shots are first processed by MLLM to
produce representations aware of both reference and context, which are then
used as the condition for the diffusion model to decode the subsequent shot.
Experimental results show that the model trained on AnimeShooter achieves
superior cross-shot visual consistency and adherence to reference visual
guidance, which highlight the value of our dataset for coherent animated video
generation.

</details>


### [197] [Native-Resolution Image Synthesis](https://arxiv.org/abs/2506.03131)
*Zidong Wang,Lei Bai,Xiangyu Yue,Wanli Ouyang,Yiyuan Zhang*

Main category: cs.CV

Relevance: 40.0

TL;DR: 提出了一种原生分辨率图像合成方法（NiT），通过处理可变长度视觉标记，克服了传统固定分辨率方法的限制，实现了任意分辨率和宽高比的图像生成。


<details>
  <summary>Details</summary>
Motivation: 传统固定分辨率的图像生成方法限制了灵活性，无法适应多样化的分辨率和宽高比需求。

Method: 设计了原生分辨率扩散Transformer（NiT），在去噪过程中显式建模不同分辨率和宽高比，学习广泛的视觉分布。

Result: NiT在ImageNet-256x256和512x512基准测试中达到SOTA性能，并展示了零样本泛化能力，能生成未见高分辨率和多样宽高比的图像。

Conclusion: 原生分辨率建模有望成为视觉生成模型与先进LLM方法之间的桥梁。

Abstract: We introduce native-resolution image synthesis, a novel generative modeling
paradigm that enables the synthesis of images at arbitrary resolutions and
aspect ratios. This approach overcomes the limitations of conventional
fixed-resolution, square-image methods by natively handling variable-length
visual tokens, a core challenge for traditional techniques. To this end, we
introduce the Native-resolution diffusion Transformer (NiT), an architecture
designed to explicitly model varying resolutions and aspect ratios within its
denoising process. Free from the constraints of fixed formats, NiT learns
intrinsic visual distributions from images spanning a broad range of
resolutions and aspect ratios. Notably, a single NiT model simultaneously
achieves the state-of-the-art performance on both ImageNet-256x256 and 512x512
benchmarks. Surprisingly, akin to the robust zero-shot capabilities seen in
advanced large language models, NiT, trained solely on ImageNet, demonstrates
excellent zero-shot generalization performance. It successfully generates
high-fidelity images at previously unseen high resolutions (e.g., 1536 x 1536)
and diverse aspect ratios (e.g., 16:9, 3:1, 4:3), as shown in Figure 1. These
findings indicate the significant potential of native-resolution modeling as a
bridge between visual generative modeling and advanced LLM methodologies.

</details>


### [198] [OmniSpatial: Towards Comprehensive Spatial Reasoning Benchmark for Vision Language Models](https://arxiv.org/abs/2506.03135)
*Mengdi Jia,Zekun Qi,Shaochen Zhang,Wenyao Zhang,Xinqiang Yu,Jiawei He,He Wang,Li Yi*

Main category: cs.CV

Relevance: 40.0

TL;DR: 论文提出了OmniSpatial，一个基于认知心理学的全面且具有挑战性的空间推理基准，覆盖动态推理、复杂空间逻辑等四类任务，揭示了现有视觉语言模型在空间理解上的局限性。


<details>
  <summary>Details</summary>
Motivation: 当前视觉语言模型（VLMs）在基础空间关系理解上存在瓶颈，缺乏对复杂空间推理能力的评估。

Method: 通过互联网数据爬取和人工标注，构建了包含1.5K问答对的OmniSpatial基准，并测试了开源和闭源VLMs的表现。

Result: 实验表明，现有模型在全面空间理解上存在显著局限性。

Conclusion: 提出了未来研究的潜在方向，以改进模型的空间推理能力。

Abstract: Spatial reasoning is a key aspect of cognitive psychology and remains a major
bottleneck for current vision-language models (VLMs). While extensive research
has aimed to evaluate or improve VLMs' understanding of basic spatial
relations, such as distinguishing left from right, near from far, and object
counting, these tasks represent only the most fundamental level of spatial
reasoning. In this work, we introduce OmniSpatial, a comprehensive and
challenging benchmark for spatial reasoning, grounded in cognitive psychology.
OmniSpatial covers four major categories: dynamic reasoning, complex spatial
logic, spatial interaction, and perspective-taking, with 50 fine-grained
subcategories. Through Internet data crawling and careful manual annotation, we
construct over 1.5K question-answer pairs. Extensive experiments show that both
open- and closed-source VLMs, as well as existing reasoning and spatial
understanding models, exhibit significant limitations in comprehensive spatial
understanding. We further analyze failure cases and propose potential
directions for future research.

</details>


### [199] [Context as Memory: Scene-Consistent Interactive Long Video Generation with Memory Retrieval](https://arxiv.org/abs/2506.03141)
*Jiwen Yu,Jianhong Bai,Yiran Qin,Quande Liu,Xintao Wang,Pengfei Wan,Di Zhang,Xihui Liu*

Main category: cs.CV

Relevance: 40.0

TL;DR: 提出了一种名为Context-as-Memory的方法，利用历史上下文作为视频生成的记忆，通过简单设计（如帧格式存储和输入拼接）提升长视频生成的场景一致性。


<details>
  <summary>Details</summary>
Motivation: 现有方法在长视频生成中因历史上下文利用不足而难以保持场景一致性，因此需要一种更有效的方法。

Method: 提出Context-as-Memory，包括帧格式存储和输入拼接设计，并引入Memory Retrieval模块以减少计算开销。

Result: 实验表明，该方法在长视频生成中优于现有技术，并能泛化到未见的开放域场景。

Conclusion: Context-as-Memory通过高效利用历史上下文，显著提升了长视频生成的场景一致性。

Abstract: Recent advances in interactive video generation have shown promising results,
yet existing approaches struggle with scene-consistent memory capabilities in
long video generation due to limited use of historical context. In this work,
we propose Context-as-Memory, which utilizes historical context as memory for
video generation. It includes two simple yet effective designs: (1) storing
context in frame format without additional post-processing; (2) conditioning by
concatenating context and frames to be predicted along the frame dimension at
the input, requiring no external control modules. Furthermore, considering the
enormous computational overhead of incorporating all historical context, we
propose the Memory Retrieval module to select truly relevant context frames by
determining FOV (Field of View) overlap between camera poses, which
significantly reduces the number of candidate frames without substantial
information loss. Experiments demonstrate that Context-as-Memory achieves
superior memory capabilities in interactive long video generation compared to
SOTAs, even generalizing effectively to open-domain scenarios not seen during
training. The link of our project page is https://context-as-memory.github.io/.

</details>


### [200] [MERIT: Multilingual Semantic Retrieval with Interleaved Multi-Condition Query](https://arxiv.org/abs/2506.03144)
*Wei Chow,Yuan Gao,Linfeng Li,Xian Wang,Qi Xu,Hang Song,Lingdong Kong,Ran Zhou,Yi Zeng,Yidong Cai,Botian Jiang,Shilin Xu,Jiajun Zhang,Minghui Qiu,Xiangtai Li,Tianshu Yang,Siliang Tang,Juncheng Li*

Main category: cs.CV

Relevance: 40.0

TL;DR: 论文提出了MERIT数据集和Coral框架，用于多条件语义检索，解决了现有模型忽略查询中特定条件的问题。


<details>
  <summary>Details</summary>
Motivation: 现有语义检索数据集和模型在多样性和多条件查询方面表现不足，无法充分利用视觉信息。

Method: 提出Coral框架，结合嵌入重建和对比学习，优化预训练多语言模型。

Result: Coral在MERIT数据集上性能提升45.9%，并在8个基准测试中验证了泛化能力。

Conclusion: MERIT数据集和Coral框架为多条件语义检索研究奠定了基础。

Abstract: Semantic retrieval is crucial for modern applications yet remains
underexplored in current research. Existing datasets are limited to single
languages, single images, or singular retrieval conditions, often failing to
fully exploit the expressive capacity of visual information as evidenced by
maintained performance when images are replaced with captions. However,
practical retrieval scenarios frequently involve interleaved multi-condition
queries with multiple images. Hence, this paper introduces MERIT, the first
multilingual dataset for interleaved multi-condition semantic retrieval,
comprising 320,000 queries with 135,000 products in 5 languages, covering 7
distinct product categories. Extensive experiments on MERIT identify existing
models's limitation: focusing solely on global semantic information while
neglecting specific conditional elements in queries. Consequently, we propose
Coral, a novel fine-tuning framework that adapts pre-trained MLLMs by
integrating embedding reconstruction to preserve fine-grained conditional
elements and contrastive learning to extract comprehensive global semantics.
Experiments demonstrate that Coral achieves a 45.9% performance improvement
over conventional approaches on MERIT, with strong generalization capabilities
validated across 8 established retrieval benchmarks. Collectively, our
contributions - a novel dataset, identification of critical limitations in
existing approaches, and an innovative fine-tuning framework - establish a
foundation for future research in interleaved multi-condition semantic
retrieval.

</details>


### [201] [IllumiCraft: Unified Geometry and Illumination Diffusion for Controllable Video Generation](https://arxiv.org/abs/2506.03150)
*Yuanze Lin,Yi-Wen Chen,Yi-Hsuan Tsai,Ronald Clark,Ming-Hsuan Yang*

Main category: cs.CV

Relevance: 40.0

TL;DR: IllumiCraft是一个端到端的扩散框架，通过整合光照、外观和几何线索，生成高质量的视频序列。


<details>
  <summary>Details</summary>
Motivation: 解决现有扩散模型在控制场景光照和视觉外观时缺乏几何线索的问题。

Method: 结合HDR视频映射、合成重光照帧和3D点轨迹，在统一扩散架构中整合光照、外观和几何信息。

Result: 生成时间一致且与用户定义提示对齐的视频，支持背景和文本条件下的视频重光照，保真度优于现有方法。

Conclusion: IllumiCraft通过多线索整合显著提升了可控视频生成的质量。

Abstract: Although diffusion-based models can generate high-quality and high-resolution
video sequences from textual or image inputs, they lack explicit integration of
geometric cues when controlling scene lighting and visual appearance across
frames. To address this limitation, we propose IllumiCraft, an end-to-end
diffusion framework accepting three complementary inputs: (1)
high-dynamic-range (HDR) video maps for detailed lighting control; (2)
synthetically relit frames with randomized illumination changes (optionally
paired with a static background reference image) to provide appearance cues;
and (3) 3D point tracks that capture precise 3D geometry information. By
integrating the lighting, appearance, and geometry cues within a unified
diffusion architecture, IllumiCraft generates temporally coherent videos
aligned with user-defined prompts. It supports background-conditioned and
text-conditioned video relighting and provides better fidelity than existing
controllable video generation methods. Project Page:
https://yuanze-lin.me/IllumiCraft_page

</details>


### [202] [PartComposer: Learning and Composing Part-Level Concepts from Single-Image Examples](https://arxiv.org/abs/2506.03004)
*Junyu Liu,R. Kenny Jones,Daniel Ritchie*

Main category: cs.GR

Relevance: 40.0

TL;DR: PartComposer是一个从单图像示例中学习部分级概念的框架，用于文本到图像扩散模型，以组合新对象。


<details>
  <summary>Details</summary>
Motivation: 现有方法难以有效学习细粒度概念或需要大量输入数据。PartComposer旨在解决单样本数据稀缺问题。

Method: 提出动态数据合成管道生成多样部分组合，并通过概念预测器最大化去噪潜在与结构化概念代码之间的互信息。

Result: 方法在概念解耦和可控组合方面表现优异，优于同类和跨类别的基线方法。

Conclusion: PartComposer在部分级概念学习和组合任务中表现出色。

Abstract: We present PartComposer: a framework for part-level concept learning from
single-image examples that enables text-to-image diffusion models to compose
novel objects from meaningful components. Existing methods either struggle with
effectively learning fine-grained concepts or require a large dataset as input.
We propose a dynamic data synthesis pipeline generating diverse part
compositions to address one-shot data scarcity. Most importantly, we propose to
maximize the mutual information between denoised latents and structured concept
codes via a concept predictor, enabling direct regulation on concept
disentanglement and re-composition supervision. Our method achieves strong
disentanglement and controllable composition, outperforming subject and
part-level baselines when mixing concepts from the same, or different, object
categories.

</details>


### [203] [Fairness through Feedback: Addressing Algorithmic Misgendering in Automatic Gender Recognition](https://arxiv.org/abs/2506.02017)
*Camilla Quaresmini,Giacomo Zanotti*

Main category: cs.CV

Relevance: 30.0

TL;DR: 论文探讨了自动性别识别（AGR）系统的问题，提出理论重构并建议引入用户反馈机制以提高公平性。


<details>
  <summary>Details</summary>
Motivation: AGR系统通常基于性别二元假设，且分类结果与性别表达不符，导致不可靠性，尤其是对非二元性别者。论文旨在解决这些问题。

Method: 区分性别、性别表达和性别认同，并建议在AGR系统中引入用户反馈机制，允许用户修正系统输出。

Result: 通过用户反馈机制，AGR系统的公平性可显著提高，同时尊重个体的自我表达权。

Conclusion: AGR系统应重新设计为尊重个体权利的工具，而非仅依赖二元分类。

Abstract: Automatic Gender Recognition (AGR) systems are an increasingly widespread
application in the Machine Learning (ML) landscape. While these systems are
typically understood as detecting gender, they often classify datapoints based
on observable features correlated at best with either male or female sex. In
addition to questionable binary assumptions, from an epistemological point of
view, this is problematic for two reasons. First, there exists a gap between
the categories the system is meant to predict (woman versus man) and those onto
which their output reasonably maps (female versus male). What is more, gender
cannot be inferred on the basis of such observable features. This makes AGR
tools often unreliable, especially in the case of non-binary and gender
non-conforming people. We suggest a theoretical and practical rethinking of AGR
systems. To begin, distinctions are made between sex, gender, and gender
expression. Then, we build upon the observation that, unlike algorithmic
misgendering, human-human misgendering is open to the possibility of
re-evaluation and correction. We suggest that analogous dynamics should be
recreated in AGR, giving users the possibility to correct the system's output.
While implementing such a feedback mechanism could be regarded as diminishing
the system's autonomy, it represents a way to significantly increase fairness
levels in AGR. This is consistent with the conceptual change of paradigm that
we advocate for AGR systems, which should be understood as tools respecting
individuals' rights and capabilities of self-expression and determination.

</details>


### [204] [VLCD: Vision-Language Contrastive Distillation for Accurate and Efficient Automatic Placenta Analysis](https://arxiv.org/abs/2506.02229)
*Manas Mehta,Yimu Pan,Kelly Gallagher,Alison D. Gernand,Jeffery A. Goldstein,Delia Mwinyelle,Leena Mithal,James Z. Wang*

Main category: cs.CV

Relevance: 30.0

TL;DR: 论文提出两种改进视觉语言对比学习（VLC）框架的方法，以提高医学图像分析的准确性和效率，包括知识蒸馏策略和无监督预蒸馏。


<details>
  <summary>Details</summary>
Motivation: 现有自动化方法计算量大，限制了其部署性，因此需要更高效且准确的解决方案。

Method: 1. 文本锚定的视觉语言对比知识蒸馏（VLCD）；2. 使用大型自然图像数据集进行无监督预蒸馏。

Result: 方法在模型压缩和加速的同时，性能匹配或超越教师模型，尤其对低质量图像表现更鲁棒。

Conclusion: VLCD提高了医学VLC方法的效率和可部署性，使AI医疗解决方案更易在资源受限环境中应用。

Abstract: Pathological examination of the placenta is an effective method for detecting
and mitigating health risks associated with childbirth. Recent advancements in
AI have enabled the use of photographs of the placenta and pathology reports
for detecting and classifying signs of childbirth-related pathologies. However,
existing automated methods are computationally extensive, which limits their
deployability. We propose two modifications to vision-language contrastive
learning (VLC) frameworks to enhance their accuracy and efficiency: (1)
text-anchored vision-language contrastive knowledge distillation (VLCD)-a new
knowledge distillation strategy for medical VLC pretraining, and (2)
unsupervised predistillation using a large natural images dataset for improved
initialization. Our approach distills efficient neural networks that match or
surpass the teacher model in performance while achieving model compression and
acceleration. Our results showcase the value of unsupervised predistillation in
improving the performance and robustness of our approach, specifically for
lower-quality images. VLCD serves as an effective way to improve the efficiency
and deployability of medical VLC approaches, making AI-based healthcare
solutions more accessible, especially in resource-constrained environments.

</details>


### [205] [PAIR-Net: Enhancing Egocentric Speaker Detection via Pretrained Audio-Visual Fusion and Alignment Loss](https://arxiv.org/abs/2506.02247)
*Yu Wang,Juhyung Ha,David J. Crandall*

Main category: cs.CV

Relevance: 30.0

TL;DR: PAIR-Net结合Whisper音频编码器和AV-HuBERT视觉骨干，通过跨模态对齐损失提升主动说话者检测性能，在Ego4D基准上达到76.6% mAP。


<details>
  <summary>Details</summary>
Motivation: 解决传统视觉方法在非理想条件下（如不稳定视角、运动模糊）性能下降的问题。

Method: 集成部分冻结的Whisper音频编码器和微调的AV-HuBERT视觉骨干，引入跨模态对齐损失。

Result: 在Ego4D ASD基准上达到76.6% mAP，超越现有方法8.2%-12.9%。

Conclusion: 预训练音频先验和对齐融合在真实场景中具有显著价值。

Abstract: Active speaker detection (ASD) in egocentric videos presents unique
challenges due to unstable viewpoints, motion blur, and off-screen speech
sources - conditions under which traditional visual-centric methods degrade
significantly. We introduce PAIR-Net (Pretrained Audio-Visual Integration with
Regularization Network), an effective model that integrates a partially frozen
Whisper audio encoder with a fine-tuned AV-HuBERT visual backbone to robustly
fuse cross-modal cues. To counteract modality imbalance, we introduce an
inter-modal alignment loss that synchronizes audio and visual representations,
enabling more consistent convergence across modalities. Without relying on
multi-speaker context or ideal frontal views, PAIR-Net achieves
state-of-the-art performance on the Ego4D ASD benchmark with 76.6% mAP,
surpassing LoCoNet and STHG by 8.2% and 12.9% mAP, respectively. Our results
highlight the value of pretrained audio priors and alignment-based fusion for
robust ASD under real-world egocentric conditions.

</details>


### [206] [Rig3R: Rig-Aware Conditioning for Learned 3D Reconstruction](https://arxiv.org/abs/2506.02265)
*Samuel Li,Pujith Kachana,Prajwal Chidananda,Saurabh Nair,Yasutaka Furukawa,Matthew Brown*

Main category: cs.CV

Relevance: 30.0

TL;DR: Rig3R是一种改进的多视角3D重建模型，通过结合或推断相机设备结构，显著提升了3D重建和相机姿态估计的性能。


<details>
  <summary>Details</summary>
Motivation: 现有方法（如DUSt3R）将图像视为无结构集合，限制了在同步相机设备场景中的效果。Rig3R旨在利用或推断设备结构以提升性能。

Method: Rig3R结合设备元数据（如相机ID、时间、姿态）构建设备感知的潜在空间，并联合预测点图和两种射线图（全局和设备中心）。

Result: 在3D重建、相机姿态估计和设备结构推断任务中，Rig3R性能领先传统和基于学习的方法17-45% mAA。

Conclusion: Rig3R通过设备感知设计实现了高效且鲁棒的多视角3D重建，无需后处理。

Abstract: Estimating agent pose and 3D scene structure from multi-camera rigs is a
central task in embodied AI applications such as autonomous driving. Recent
learned approaches such as DUSt3R have shown impressive performance in
multiview settings. However, these models treat images as unstructured
collections, limiting effectiveness in scenarios where frames are captured from
synchronized rigs with known or inferable structure.
  To this end, we introduce Rig3R, a generalization of prior multiview
reconstruction models that incorporates rig structure when available, and
learns to infer it when not. Rig3R conditions on optional rig metadata
including camera ID, time, and rig poses to develop a rig-aware latent space
that remains robust to missing information. It jointly predicts pointmaps and
two types of raymaps: a pose raymap relative to a global frame, and a rig
raymap relative to a rig-centric frame consistent across time. Rig raymaps
allow the model to infer rig structure directly from input images when metadata
is missing.
  Rig3R achieves state-of-the-art performance in 3D reconstruction, camera pose
estimation, and rig discovery, outperforming both traditional and learned
methods by 17-45% mAA across diverse real-world rig datasets, all in a single
forward pass without post-processing or iterative refinement.

</details>


### [207] [Auto-Labeling Data for Object Detection](https://arxiv.org/abs/2506.02359)
*Brent A. Griffin,Manushree Gangwar,Jacob Sela,Jason J. Corso*

Main category: cs.CV

Relevance: 30.0

TL;DR: 论文提出了一种无需真实标注的训练目标检测模型的方法，利用预训练的视觉-语言基础模型生成伪标签，显著降低标注成本和时间。


<details>
  <summary>Details</summary>
Motivation: 传统目标检测标注方法成本高，而现有替代方案要么功能受限，要么计算成本过高。本文旨在解决这一问题。

Method: 配置预训练的视觉-语言基础模型生成伪标签，用于训练轻量级检测模型。

Result: 在多个数据集上表现接近标准标注方法，同时大幅减少标注时间和成本。

Conclusion: 该方法是一种可行的替代方案，能够平衡性能与成本。

Abstract: Great labels make great models. However, traditional labeling approaches for
tasks like object detection have substantial costs at scale. Furthermore,
alternatives to fully-supervised object detection either lose functionality or
require larger models with prohibitive computational costs for inference at
scale. To that end, this paper addresses the problem of training standard
object detection models without any ground truth labels. Instead, we configure
previously-trained vision-language foundation models to generate
application-specific pseudo "ground truth" labels. These auto-generated labels
directly integrate with existing model training frameworks, and we subsequently
train lightweight detection models that are computationally efficient. In this
way, we avoid the costs of traditional labeling, leverage the knowledge of
vision-language models, and keep the efficiency of lightweight models for
practical application. We perform exhaustive experiments across multiple
labeling configurations, downstream inference models, and datasets to establish
best practices and set an extensive auto-labeling benchmark. From our results,
we find that our approach is a viable alternative to standard labeling in that
it maintains competitive performance on multiple datasets and substantially
reduces labeling time and costs.

</details>


### [208] [A TRPCA-Inspired Deep Unfolding Network for Hyperspectral Image Denoising via Thresholded t-SVD and Top-K Sparse Transformer](https://arxiv.org/abs/2506.02364)
*Liang Li,Jianli Zhao,Sheng Fang,Siyu Chen,Hui Sun*

Main category: cs.CV

Relevance: 30.0

TL;DR: 论文提出了一种深度展开网络（DU-TRPCA），通过紧密集成的低秩和稀疏模块，有效去除了高光谱图像中的混合噪声。


<details>
  <summary>Details</summary>
Motivation: 高光谱图像（HSIs）在采集和传输过程中常受到复杂混合噪声的干扰，现有方法未能充分利用不同先验或模块的互补优势。

Method: 结合张量鲁棒主成分分析（TRPCA），设计了深度展开网络（DU-TRPCA），通过低秩模块（t-SVD）和稀疏模块（Top-K稀疏Transformer）的交替优化实现去噪。

Result: 在合成和真实HSIs数据上，DU-TRPCA在严重混合噪声下超越了现有方法，同时提供了可解释性和稳定的去噪动态。

Conclusion: DU-TRPCA通过紧密耦合的模块设计，实现了高效的噪声去除，并保留了TRPCA的迭代优化优势。

Abstract: Hyperspectral images (HSIs) are often degraded by complex mixed noise during
acquisition and transmission, making effective denoising essential for
subsequent analysis. Recent hybrid approaches that bridge model-driven and
data-driven paradigms have shown great promise. However, most of these
approaches lack effective alternation between different priors or modules,
resulting in loosely coupled regularization and insufficient exploitation of
their complementary strengths. Inspired by tensor robust principal component
analysis (TRPCA), we propose a novel deep unfolding network (DU-TRPCA) that
enforces stage-wise alternation between two tightly integrated modules:
low-rank and sparse. The low-rank module employs thresholded tensor singular
value decomposition (t-SVD), providing a widely adopted convex surrogate for
tensor low-rankness and has been demonstrated to effectively capture the global
spatial-spectral structure of HSIs. The Top-K sparse transformer module
adaptively imposes sparse constraints, directly matching the sparse
regularization in TRPCA and enabling effective removal of localized outliers
and complex noise. This tightly coupled architecture preserves the stage-wise
alternation between low-rank approximation and sparse refinement inherent in
TRPCA, while enhancing representational capacity through attention mechanisms.
Extensive experiments on synthetic and real-world HSIs demonstrate that
DU-TRPCA surpasses state-of-the-art methods under severe mixed noise, while
offering interpretability benefits and stable denoising dynamics inspired by
iterative optimization. Code is available at
https://github.com/liangli97/TRPCA-Deep-Unfolding-HSI-Denoising.

</details>


### [209] [RRCANet: Recurrent Reusable-Convolution Attention Network for Infrared Small Target Detection](https://arxiv.org/abs/2506.02393)
*Yongxian Liu,Boyang Li,Ting Liu,Zaiping Lin,Wei An*

Main category: cs.CV

Relevance: 30.0

TL;DR: 提出了一种用于红外小目标检测的循环可重用卷积注意力网络（RRCA-Net），通过可重用卷积块和双交互注意力模块实现高效检测。


<details>
  <summary>Details</summary>
Motivation: 红外小目标检测因目标小、暗、形状多变而具有挑战性，现有CNN方法需复杂特征提取模块，RRCA-Net旨在高效且有效地解决这一问题。

Method: RRCA-Net结合可重用卷积块（RuCB）和双交互注意力聚合模块（DIAAM），通过迭代优化高层特征，并设计目标特性启发的损失函数（DpT-k loss）。

Result: 在三个基准数据集上表现优异，参数少且可作为插件提升其他方法的性能。

Conclusion: RRCA-Net在红外小目标检测中实现了高效与性能的平衡。

Abstract: Infrared small target detection is a challenging task due to its unique
characteristics (e.g., small, dim, shapeless and changeable). Recently
published CNN-based methods have achieved promising performance with heavy
feature extraction and fusion modules. To achieve efficient and effective
detection, we propose a recurrent reusable-convolution attention network
(RRCA-Net) for infrared small target detection. Specifically, RRCA-Net
incorporates reusable-convolution block (RuCB) in a recurrent manner without
introducing extra parameters. With the help of the repetitive iteration in
RuCB, the high-level information of small targets in the deep layers can be
well maintained and further refined. Then, a dual interactive attention
aggregation module (DIAAM) is proposed to promote the mutual enhancement and
fusion of refined information. In this way, RRCA-Net can both achieve
high-level feature refinement and enhance the correlation of contextual
information between adjacent layers. Moreover, to achieve steady convergence,
we design a target characteristic inspired loss function (DpT-k loss) by
integrating physical and mathematical constraints. Experimental results on
three benchmark datasets (e.g. NUAA-SIRST, IRSTD-1k, DenseSIRST) demonstrate
that our RRCA-Net can achieve comparable performance to the state-of-the-art
methods while maintaining a small number of parameters, and act as a plug and
play module to introduce consistent performance improvement for several popular
IRSTD methods. Our code will be available at https://github.com/yongxianLiu/
soon.

</details>


### [210] [Towards Explicit Geometry-Reflectance Collaboration for Generalized LiDAR Segmentation in Adverse Weather](https://arxiv.org/abs/2506.02396)
*Longyu Yang,Ping Hu,Shangbo Yuan,Lu Zhang,Jun Liu,Hengtao Shen,Xiaofeng Zhu*

Main category: cs.CV

Relevance: 30.0

TL;DR: 论文提出了一种名为GRC的双分支框架，用于解决LiDAR语义分割在恶劣天气条件下精度下降的问题，通过分离几何和反射特征提取并协作，显著提升了模型的鲁棒性和泛化能力。


<details>
  <summary>Details</summary>
Motivation: 现有LiDAR语义分割模型在恶劣天气条件下精度下降，而现有方法多通过天气模拟或通用增强技术改进训练数据，但忽略了点云几何结构和反射强度的异质域偏移问题。

Method: GRC框架采用双分支架构，分别独立处理几何和反射特征，并通过多级特征协作模块抑制冗余和不可靠信息。

Result: 在多个挑战性基准测试中，GRC方法优于现有方法，取得了新的SOTA结果。

Conclusion: GRC框架无需复杂模拟或增强，即可有效提取场景内在信息并抑制干扰，显著提升模型在恶劣天气下的性能。

Abstract: Existing LiDAR semantic segmentation models often suffer from decreased
accuracy when exposed to adverse weather conditions. Recent methods addressing
this issue focus on enhancing training data through weather simulation or
universal augmentation techniques. However, few works have studied the negative
impacts caused by the heterogeneous domain shifts in the geometric structure
and reflectance intensity of point clouds. In this paper, we delve into this
challenge and address it with a novel Geometry-Reflectance Collaboration (GRC)
framework that explicitly separates feature extraction for geometry and
reflectance. Specifically, GRC employs a dual-branch architecture designed to
independently process geometric and reflectance features initially, thereby
capitalizing on their distinct characteristic. Then, GRC adopts a robust
multi-level feature collaboration module to suppress redundant and unreliable
information from both branches. Consequently, without complex simulation or
augmentation, our method effectively extracts intrinsic information about the
scene while suppressing interference, thus achieving better robustness and
generalization in adverse weather conditions. We demonstrate the effectiveness
of GRC through comprehensive experiments on challenging benchmarks, showing
that our method outperforms previous approaches and establishes new
state-of-the-art results.

</details>


### [211] [Empowering Functional Neuroimaging: A Pre-trained Generative Framework for Unified Representation of Neural Signals](https://arxiv.org/abs/2506.02433)
*Weiheng Yao,Xuhang Chen,Shuqiang Wang*

Main category: cs.CV

Relevance: 30.0

TL;DR: 提出了一种基于生成AI的多模态功能神经影像统一表示框架，用于解决数据获取成本高和公平性问题。


<details>
  <summary>Details</summary>
Motivation: 多模态功能神经影像数据获取成本高且存在公平性问题，需要一种方法生成数据以弥补不足。

Method: 通过生成AI将多模态功能神经影像映射到统一表示空间，生成受限模态和代表性不足群体的数据。

Result: 框架能生成与真实脑活动一致的数据，提升下游任务性能，并增强模型公平性。

Conclusion: 该框架为降低数据获取成本和提升BCI解码模型公平性提供了新范式。

Abstract: Multimodal functional neuroimaging enables systematic analysis of brain
mechanisms and provides discriminative representations for brain-computer
interface (BCI) decoding. However, its acquisition is constrained by high costs
and feasibility limitations. Moreover, underrepresentation of specific groups
undermines fairness of BCI decoding model. To address these challenges, we
propose a unified representation framework for multimodal functional
neuroimaging via generative artificial intelligence (AI). By mapping multimodal
functional neuroimaging into a unified representation space, the proposed
framework is capable of generating data for acquisition-constrained modalities
and underrepresented groups. Experiments show that the framework can generate
data consistent with real brain activity patterns, provide insights into brain
mechanisms, and improve performance on downstream tasks. More importantly, it
can enhance model fairness by augmenting data for underrepresented groups.
Overall, the framework offers a new paradigm for decreasing the cost of
acquiring multimodal functional neuroimages and enhancing the fairness of BCI
decoding models.

</details>


### [212] [VidEvent: A Large Dataset for Understanding Dynamic Evolution of Events in Videos](https://arxiv.org/abs/2506.02448)
*Baoyu Liang,Qile Su,Shoutai Zhu,Yuchen Liang,Chao Tong*

Main category: cs.CV

Relevance: 30.0

TL;DR: 论文提出了视频事件理解任务，并发布了VidEvent数据集，包含23,000个标注事件，支持事件脚本提取与预测。提供了基线模型作为未来研究的基准。


<details>
  <summary>Details</summary>
Motivation: 视频事件的复杂结构和动态演化对AI理解构成挑战，需开发新方法和数据集以支持视频事件理解。

Method: 提出视频事件理解任务，构建VidEvent数据集，提供基线模型并分析其性能。

Result: VidEvent数据集和基线模型为视频事件理解提供了高质量资源，展示了数据集的应用潜力。

Conclusion: VidEvent数据集和基线模型推动了视频事件理解的研究，鼓励开发新算法。

Abstract: Despite the significant impact of visual events on human cognition,
understanding events in videos remains a challenging task for AI due to their
complex structures, semantic hierarchies, and dynamic evolution. To address
this, we propose the task of video event understanding that extracts event
scripts and makes predictions with these scripts from videos. To support this
task, we introduce VidEvent, a large-scale dataset containing over 23,000
well-labeled events, featuring detailed event structures, broad hierarchies,
and logical relations extracted from movie recap videos. The dataset was
created through a meticulous annotation process, ensuring high-quality and
reliable event data. We also provide comprehensive baseline models offering
detailed descriptions of their architecture and performance metrics. These
models serve as benchmarks for future research, facilitating comparisons and
improvements. Our analysis of VidEvent and the baseline models highlights the
dataset's potential to advance video event understanding and encourages the
exploration of innovative algorithms and models. The dataset and related
resources are publicly available at www.videvent.top.

</details>


### [213] [HRTR: A Single-stage Transformer for Fine-grained Sub-second Action Segmentation in Stroke Rehabilitation](https://arxiv.org/abs/2506.02472)
*Halil Ismail Helvaci,Justin Philip Huber,Jihye Bae,Sen-ching Samson Cheung*

Main category: cs.CV

Relevance: 30.0

TL;DR: 提出了一种名为HRTR的单阶段Transformer模型，用于高分辨率、亚秒级动作检测，无需多阶段方法或后处理，在多个数据集上表现优于现有技术。


<details>
  <summary>Details</summary>
Motivation: 中风康复需要精确跟踪患者动作，但现有方法在细粒度和亚秒级动作检测上存在挑战。

Method: 使用单阶段Transformer（HRTR）进行时间定位和分类，无需多阶段方法或后处理。

Result: 在StrokeRehab Video、StrokeRehab IMU和50Salads数据集上分别取得70.1、69.4和88.4的Edit Score。

Conclusion: HRTR在细粒度和亚秒级动作检测任务中表现优异，简化了现有流程。

Abstract: Stroke rehabilitation often demands precise tracking of patient movements to
monitor progress, with complexities of rehabilitation exercises presenting two
critical challenges: fine-grained and sub-second (under one-second) action
detection. In this work, we propose the High Resolution Temporal Transformer
(HRTR), to time-localize and classify high-resolution (fine-grained),
sub-second actions in a single-stage transformer, eliminating the need for
multi-stage methods and post-processing. Without any refinements, HRTR
outperforms state-of-the-art systems on both stroke related and general
datasets, achieving Edit Score (ES) of 70.1 on StrokeRehab Video, 69.4 on
StrokeRehab IMU, and 88.4 on 50Salads.

</details>


### [214] [Generative Perception of Shape and Material from Differential Motion](https://arxiv.org/abs/2506.02473)
*Xinran Nicole Han,Ko Nishino,Todd Zickler*

Main category: cs.CV

Relevance: 30.0

TL;DR: 论文提出了一种基于条件去噪扩散模型的方法，通过短视频中的物体运动生成形状和材质的多模态预测，解决了单视角下的模糊性问题。


<details>
  <summary>Details</summary>
Motivation: 人类通过轻微移动或旋转物体来消除形状和材质感知的模糊性，受此启发，研究旨在通过连续运动观察改进视觉推理。

Method: 使用条件去噪扩散模型，直接在像素空间训练，生成多模态的形状和材质预测。

Result: 模型在静态观察下生成多样化的预测，运动时快速收敛到更准确的解释，并能处理真实世界的物体。

Conclusion: 通过连续运动观察，生成式感知方法可提升物理系统中的视觉推理能力。

Abstract: Perceiving the shape and material of an object from a single image is
inherently ambiguous, especially when lighting is unknown and unconstrained.
Despite this, humans can often disentangle shape and material, and when they
are uncertain, they often move their head slightly or rotate the object to help
resolve the ambiguities. Inspired by this behavior, we introduce a novel
conditional denoising-diffusion model that generates samples of
shape-and-material maps from a short video of an object undergoing differential
motions. Our parameter-efficient architecture allows training directly in
pixel-space, and it generates many disentangled attributes of an object
simultaneously. Trained on a modest number of synthetic object-motion videos
with supervision on shape and material, the model exhibits compelling emergent
behavior: For static observations, it produces diverse, multimodal predictions
of plausible shape-and-material maps that capture the inherent ambiguities; and
when objects move, the distributions quickly converge to more accurate
explanations. The model also produces high-quality shape-and-material estimates
for less ambiguous, real-world objects. By moving beyond single-view to
continuous motion observations, our work suggests a generative perception
approach for improving visual reasoning in physically-embodied systems.

</details>


### [215] [Co-Evidential Fusion with Information Volume for Medical Image Segmentation](https://arxiv.org/abs/2506.02492)
*Yuanpeng He,Lijian Li,Tianxiang Zhan,Chi-Man Pun,Wenpin Jiao,Zhi Jin*

Main category: cs.CV

Relevance: 30.0

TL;DR: 提出了一种基于广义证据深度学习的半监督医学图像分割方法，通过改进不确定性度量和优化目标，提升了性能。


<details>
  <summary>Details</summary>
Motivation: 现有半监督图像分割方法未能有效利用多源体素级不确定性进行针对性学习，限制了性能。

Method: 1. 引入基于D-S证据理论的pignistic co-evidential融合策略，改进不确定性度量；2. 提出信息量质量函数（IVUM）评估证据，设计两种证据学习方案。

Result: 在四个数据集上验证了方法的竞争力。

Conclusion: 通过改进不确定性度量和优化目标，显著提升了半监督医学图像分割的性能。

Abstract: Although existing semi-supervised image segmentation methods have achieved
good performance, they cannot effectively utilize multiple sources of
voxel-level uncertainty for targeted learning. Therefore, we propose two main
improvements. First, we introduce a novel pignistic co-evidential fusion
strategy using generalized evidential deep learning, extended by traditional
D-S evidence theory, to obtain a more precise uncertainty measure for each
voxel in medical samples. This assists the model in learning mixed labeled
information and establishing semantic associations between labeled and
unlabeled data. Second, we introduce the concept of information volume of mass
function (IVUM) to evaluate the constructed evidence, implementing two
evidential learning schemes. One optimizes evidential deep learning by
combining the information volume of the mass function with original uncertainty
measures. The other integrates the learning pattern based on the co-evidential
fusion strategy, using IVUM to design a new optimization objective. Experiments
on four datasets demonstrate the competitive performance of our method.

</details>


### [216] [Towards In-the-wild 3D Plane Reconstruction from a Single Image](https://arxiv.org/abs/2506.02493)
*Jiachen Liu,Rui Yu,Sili Chen,Sharon X. Huang,Hengkai Guo*

Main category: cs.CV

Relevance: 30.0

TL;DR: ZeroPlane是一个基于Transformer的框架，用于从单张图像中实现零样本3D平面检测和重建，支持跨领域和环境的泛化。


<details>
  <summary>Details</summary>
Motivation: 解决现有方法在单一数据集上训练导致的泛化能力不足问题，提出跨领域3D平面重建的需求。

Method: 提出解耦平面法线和偏移表示，采用分类-回归范式，结合先进的图像编码器和像素几何增强模块。

Result: 在多个零样本评估数据集上显著优于现有方法，尤其在野外数据上表现突出。

Conclusion: ZeroPlane在3D平面重建的准确性和泛化能力上取得了显著进展。

Abstract: 3D plane reconstruction from a single image is a crucial yet challenging
topic in 3D computer vision. Previous state-of-the-art (SOTA) methods have
focused on training their system on a single dataset from either indoor or
outdoor domain, limiting their generalizability across diverse testing data. In
this work, we introduce a novel framework dubbed ZeroPlane, a Transformer-based
model targeting zero-shot 3D plane detection and reconstruction from a single
image, over diverse domains and environments. To enable data-driven models
across multiple domains, we have curated a large-scale planar benchmark,
comprising over 14 datasets and 560,000 high-resolution, dense planar
annotations for diverse indoor and outdoor scenes. To address the challenge of
achieving desirable planar geometry on multi-dataset training, we propose to
disentangle the representation of plane normal and offset, and employ an
exemplar-guided, classification-then-regression paradigm to learn plane and
offset respectively. Additionally, we employ advanced backbones as image
encoder, and present an effective pixel-geometry-enhanced plane embedding
module to further facilitate planar reconstruction. Extensive experiments
across multiple zero-shot evaluation datasets have demonstrated that our
approach significantly outperforms previous methods on both reconstruction
accuracy and generalizability, especially over in-the-wild data. Our code and
data are available at: https://github.com/jcliu0428/ZeroPlane.

</details>


### [217] [LumosFlow: Motion-Guided Long Video Generation](https://arxiv.org/abs/2506.02497)
*Jiahao Chen,Hangjie Yuan,Yichen Qian,Jingyun Liang,Jiazheng Xing,Pengwei Liu,Weihua Chen,Fan Wang,Bing Su*

Main category: cs.CV

Relevance: 30.0

TL;DR: 论文提出LumosFlow框架，通过显式运动指导生成长视频，解决了传统方法中的时间重复和不自然过渡问题。


<details>
  <summary>Details</summary>
Motivation: 长视频生成在娱乐和模拟领域有广泛应用，但现有方法在时间一致性和视觉吸引力方面仍面临挑战。

Method: 结合大运动文本到视频扩散模型（LMTV-DM）和潜在光流扩散模型（LOF-DM），分解中间帧插值为运动生成和后处理细化。

Result: 实验表明，该方法能生成具有一致运动和外观的长视频，实现15倍插值。

Conclusion: LumosFlow通过显式运动指导，显著提升了长视频生成的质量和连续性。

Abstract: Long video generation has gained increasing attention due to its widespread
applications in fields such as entertainment and simulation. Despite advances,
synthesizing temporally coherent and visually compelling long sequences remains
a formidable challenge. Conventional approaches often synthesize long videos by
sequentially generating and concatenating short clips, or generating key frames
and then interpolate the intermediate frames in a hierarchical manner. However,
both of them still remain significant challenges, leading to issues such as
temporal repetition or unnatural transitions. In this paper, we revisit the
hierarchical long video generation pipeline and introduce LumosFlow, a
framework introduce motion guidance explicitly. Specifically, we first employ
the Large Motion Text-to-Video Diffusion Model (LMTV-DM) to generate key frames
with larger motion intervals, thereby ensuring content diversity in the
generated long videos. Given the complexity of interpolating contextual
transitions between key frames, we further decompose the intermediate frame
interpolation into motion generation and post-hoc refinement. For each pair of
key frames, the Latent Optical Flow Diffusion Model (LOF-DM) synthesizes
complex and large-motion optical flows, while MotionControlNet subsequently
refines the warped results to enhance quality and guide intermediate frame
generation. Compared with traditional video frame interpolation, we achieve 15x
interpolation, ensuring reasonable and continuous motion between adjacent
frames. Experiments show that our method can generate long videos with
consistent motion and appearance. Code and models will be made publicly
available upon acceptance. Our project page:
https://jiahaochen1.github.io/LumosFlow/

</details>


### [218] [BEVCALIB: LiDAR-Camera Calibration via Geometry-Guided Bird's-Eye View Representations](https://arxiv.org/abs/2506.02587)
*Weiduo Yuan,Jerry Li,Justin Yue,Divyank Shah,Konstantinos Karydis,Hang Qiu*

Main category: cs.CV

Relevance: 30.0

TL;DR: BEVCALIB是一种利用鸟瞰图（BEV）特征从原始数据中进行LiDAR-相机校准的新方法，显著优于现有基线。


<details>
  <summary>Details</summary>
Motivation: 传统校准方法需要大量受控环境数据且无法补偿运动中的变换变化，BEVCALIB旨在解决这一问题。

Method: 分别提取相机和LiDAR的BEV特征，融合到共享BEV空间，并引入特征选择器优化变换解码器。

Result: 在KITTI和NuScenes数据集上，BEVCALIB在平移和旋转误差上显著优于最佳基线。

Conclusion: BEVCALIB在LiDAR-相机校准领域实现了新的SOTA，代码已开源。

Abstract: Accurate LiDAR-camera calibration is fundamental to fusing multi-modal
perception in autonomous driving and robotic systems. Traditional calibration
methods require extensive data collection in controlled environments and cannot
compensate for the transformation changes during the vehicle/robot movement. In
this paper, we propose the first model that uses bird's-eye view (BEV) features
to perform LiDAR camera calibration from raw data, termed BEVCALIB. To achieve
this, we extract camera BEV features and LiDAR BEV features separately and fuse
them into a shared BEV feature space. To fully utilize the geometric
information from the BEV feature, we introduce a novel feature selector to
filter the most important features in the transformation decoder, which reduces
memory consumption and enables efficient training. Extensive evaluations on
KITTI, NuScenes, and our own dataset demonstrate that BEVCALIB establishes a
new state of the art. Under various noise conditions, BEVCALIB outperforms the
best baseline in the literature by an average of (47.08%, 82.32%) on KITTI
dataset, and (78.17%, 68.29%) on NuScenes dataset, in terms of (translation,
rotation), respectively. In the open-source domain, it improves the best
reproducible baseline by one order of magnitude. Our code and demo results are
available at https://cisl.ucr.edu/BEVCalib.

</details>


### [219] [Hyperspectral Image Generation with Unmixing Guided Diffusion Model](https://arxiv.org/abs/2506.02601)
*Shiyu Shen,Bin Pan,Ziye Zhang,Zhenwei Shi*

Main category: cs.CV

Relevance: 30.0

TL;DR: 提出了一种基于高光谱解混的新型扩散模型，通过将生成任务从图像空间转移到低维丰度空间，显著降低计算复杂度并保持高保真度。


<details>
  <summary>Details</summary>
Motivation: 现有生成模型依赖条件生成方案，限制了生成图像的多样性，且高光谱数据的高维度和物理约束对扩散模型提出了挑战。

Method: 模型包含解混自编码器模块和丰度扩散模块，前者利用解混指导将生成任务转移到低维空间，后者生成满足非负性和单位约束的样本。

Result: 模型能够生成高质量且多样化的高光谱图像，并通过传统指标和提出的新指标验证了其有效性。

Conclusion: 该模型在高光谱数据生成方面取得了进展。

Abstract: Recently, hyperspectral image generation has received increasing attention,
but existing generative models rely on conditional generation schemes, which
limits the diversity of generated images. Diffusion models are popular for
their ability to generate high-quality samples, but adapting these models from
RGB to hyperspectral data presents the challenge of high dimensionality and
physical constraints. To address these challenges, we propose a novel diffusion
model guided by hyperspectral unmixing. Our model comprises two key modules: an
unmixing autoencoder module and an abundance diffusion module. The unmixing
autoencoder module leverages unmixing guidance to shift the generative task
from the image space to the low-dimensional abundance space, significantly
reducing computational complexity while preserving high fidelity. The abundance
diffusion module generates samples that satisfy the constraints of
non-negativity and unity, ensuring the physical consistency of the
reconstructed HSIs. Additionally, we introduce two evaluation metrics tailored
to hyperspectral data. Empirical results, evaluated using both traditional
metrics and our proposed metrics, indicate that our model is capable of
generating high-quality and diverse hyperspectral images, offering an
advancement in hyperspectral data generation.

</details>


### [220] [Application of convolutional neural networks in image super-resolution](https://arxiv.org/abs/2506.02604)
*Tian Chunwei,Song Mingjian,Zuo Wangmeng,Du Bo,Zhang Yanning,Zhang Shichao*

Main category: cs.CV

Relevance: 30.0

TL;DR: 该论文总结了基于卷积神经网络（CNN）的图像超分辨率方法，分析了不同插值和模块的差异与关系，并通过实验比较了性能。


<details>
  <summary>Details</summary>
Motivation: 由于缺乏对不同深度学习方法在图像超分辨率中关系和差异的总结，本文旨在填补这一空白，并根据设备负载和执行速度进行分类。

Method: 介绍了CNN在图像超分辨率中的原理，并分析了基于CNN的双三次插值、最近邻插值、双线性插值、转置卷积、子像素层和元上采样等方法。

Result: 通过实验比较了不同方法的性能，并总结了各自的优缺点。

Conclusion: 论文指出了潜在的研究方向和现有方法的不足，为CNN在图像超分辨率中的发展提供了参考。

Abstract: Due to strong learning abilities of convolutional neural networks (CNNs),
they have become mainstream methods for image super-resolution. However, there
are big differences of different deep learning methods with different types.
There is little literature to summarize relations and differences of different
methods in image super-resolution. Thus, summarizing these literatures are
important, according to loading capacity and execution speed of devices. This
paper first introduces principles of CNNs in image super-resolution, then
introduces CNNs based bicubic interpolation, nearest neighbor interpolation,
bilinear interpolation, transposed convolution, sub-pixel layer, meta
up-sampling for image super-resolution to analyze differences and relations of
different CNNs based interpolations and modules, and compare performance of
these methods by experiments. Finally, this paper gives potential research
points and drawbacks and summarizes the whole paper, which can facilitate
developments of CNNs in image super-resolution.

</details>


### [221] [High Performance Space Debris Tracking in Complex Skylight Backgrounds with a Large-Scale Dataset](https://arxiv.org/abs/2506.02614)
*Guohang Zhuang,Weixi Song,Jinyang Huang,Chenwei Yang,Yan Lu*

Main category: cs.CV

Relevance: 30.0

TL;DR: 提出了一种基于深度学习的空间碎片跟踪网络（SDT-Net），通过新型观测数据模拟方案生成大规模数据集（SDTD），验证了模型的高效性和数据集的挑战性。


<details>
  <summary>Details</summary>
Motivation: 空间碎片对太空探索构成威胁，现有传统信号处理方法难以处理复杂背景和密集碎片，需开发更高效的跟踪技术。

Method: 设计了SDT-Net，通过深度学习实现高精度碎片跟踪，并利用观测数据模拟生成SDTD数据集进行训练和评估。

Result: 在模拟数据和南极站真实数据上分别验证了模型的有效性和迁移能力，MOTA分数达70.6%。

Conclusion: SDT-Net和SDTD为空间碎片跟踪提供了高效解决方案，具备实际应用潜力。

Abstract: With the rapid development of space exploration, space debris has attracted
more attention due to its potential extreme threat, leading to the need for
real-time and accurate debris tracking. However, existing methods are mainly
based on traditional signal processing, which cannot effectively process the
complex background and dense space debris. In this paper, we propose a deep
learning-based Space Debris Tracking Network~(SDT-Net) to achieve highly
accurate debris tracking. SDT-Net effectively represents the feature of debris,
enhancing the efficiency and stability of end-to-end model learning. To train
and evaluate this model effectively, we also produce a large-scale dataset
Space Debris Tracking Dataset (SDTD) by a novel observation-based data
simulation scheme. SDTD contains 18,040 video sequences with a total of 62,562
frames and covers 250,000 synthetic space debris. Extensive experiments
validate the effectiveness of our model and the challenging of our dataset.
Furthermore, we test our model on real data from the Antarctic Station,
achieving a MOTA score of 70.6%, which demonstrates its strong transferability
to real-world scenarios. Our dataset and code will be released soon.

</details>


### [222] [Hierarchical Question-Answering for Driving Scene Understanding Using Vision-Language Models](https://arxiv.org/abs/2506.02615)
*Safaa Abdullahi Moallim Mohamud,Minjin Baek,Dong Seog Han*

Main category: cs.CV

Relevance: 30.0

TL;DR: 提出了一种用于自动驾驶场景理解的分层问答方法，通过微调小型视觉语言模型（VLM）并结合动态问题树优化推理效率。


<details>
  <summary>Details</summary>
Motivation: 解决自动驾驶中场景理解的效率与细节平衡问题，避免生成冗长描述，同时确保关键驾驶元素的准确捕捉。

Method: 微调VLM于特定地理区域数据集，采用分层问答策略动态分解任务，跳过无关问题以减少计算开销。

Result: 在自定义数据集上表现优于GPT-4o，显著降低推理时间，实时部署验证了低延迟下的关键元素捕捉能力。

Conclusion: 分层QA方法在自动驾驶场景理解中实现了效率与细节的平衡，具有实际部署潜力。

Abstract: In this paper, we present a hierarchical question-answering (QA) approach for
scene understanding in autonomous vehicles, balancing cost-efficiency with
detailed visual interpretation. The method fine-tunes a compact vision-language
model (VLM) on a custom dataset specific to the geographical area in which the
vehicle operates to capture key driving-related visual elements. At the
inference stage, the hierarchical QA strategy decomposes the scene
understanding task into high-level and detailed sub-questions. Instead of
generating lengthy descriptions, the VLM navigates a structured question tree,
where answering high-level questions (e.g., "Is it possible for the ego vehicle
to turn left at the intersection?") triggers more detailed sub-questions (e.g.,
"Is there a vehicle approaching the intersection from the opposite
direction?"). To optimize inference time, questions are dynamically skipped
based on previous answers, minimizing computational overhead. The extracted
answers are then synthesized using handcrafted templates to ensure coherent,
contextually accurate scene descriptions. We evaluate the proposed approach on
the custom dataset using GPT reference-free scoring, demonstrating its
competitiveness with state-of-the-art methods like GPT-4o in capturing key
scene details while achieving significantly lower inference time. Moreover,
qualitative results from real-time deployment highlight the proposed approach's
capacity to capture key driving elements with minimal latency.

</details>


### [223] [Large-scale Self-supervised Video Foundation Model for Intelligent Surgery](https://arxiv.org/abs/2506.02692)
*Shu Yang,Fengtao Zhou,Leon Mayer,Fuxiang Huang,Yiliang Chen,Yihui Wang,Sunan He,Yuxiang Nie,Xi Wang,Ömer Sümer,Yueming Jin,Huihui Sun,Shuchang Xu,Alex Qinyang Liu,Zheng Li,Jing Qin,Jeremy YuenChun Teoh,Lena Maier-Hein,Hao Chen*

Main category: cs.CV

Relevance: 30.0

TL;DR: 论文提出了一种视频级手术预训练框架SurgVISTA，通过联合时空建模从大规模手术视频数据中学习表示，显著提升了手术场景理解的性能。


<details>
  <summary>Details</summary>
Motivation: 现有AI方法缺乏显式的时间建模，限制了动态手术场景的捕捉，导致时空理解不完整。

Method: 构建了大规模手术视频数据集，提出SurgVISTA框架，结合重建预训练和手术专家指导的知识蒸馏。

Result: SurgVISTA在13个视频级数据集上表现优于自然和手术域预训练模型。

Conclusion: SurgVISTA有望推动智能手术系统在临床场景中的应用。

Abstract: Computer-Assisted Intervention (CAI) has the potential to revolutionize
modern surgery, with surgical scene understanding serving as a critical
component in supporting decision-making, improving procedural efficacy, and
ensuring intraoperative safety. While existing AI-driven approaches alleviate
annotation burdens via self-supervised spatial representation learning, their
lack of explicit temporal modeling during pre-training fundamentally restricts
the capture of dynamic surgical contexts, resulting in incomplete
spatiotemporal understanding. In this work, we introduce the first video-level
surgical pre-training framework that enables joint spatiotemporal
representation learning from large-scale surgical video data. To achieve this,
we constructed a large-scale surgical video dataset comprising 3,650 videos and
approximately 3.55 million frames, spanning more than 20 surgical procedures
and over 10 anatomical structures. Building upon this dataset, we propose
SurgVISTA (Surgical Video-level Spatial-Temporal Architecture), a
reconstruction-based pre-training method that captures intricate spatial
structures and temporal dynamics through joint spatiotemporal modeling.
Additionally, SurgVISTA incorporates image-level knowledge distillation guided
by a surgery-specific expert to enhance the learning of fine-grained anatomical
and semantic features. To validate its effectiveness, we established a
comprehensive benchmark comprising 13 video-level datasets spanning six
surgical procedures across four tasks. Extensive experiments demonstrate that
SurgVISTA consistently outperforms both natural- and surgical-domain
pre-trained models, demonstrating strong potential to advance intelligent
surgical systems in clinically meaningful scenarios.

</details>


### [224] [LayoutRAG: Retrieval-Augmented Model for Content-agnostic Conditional Layout Generation](https://arxiv.org/abs/2506.02697)
*Yuxuan Wu,Le Wang,Sanping Zhou,Mengnan Liu,Gang Hua,Haoxiang Li*

Main category: cs.CV

Relevance: 30.0

TL;DR: 提出了一种基于检索和参考引导的布局生成方法，通过条件检索模板并利用参考引导生成过程，优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有扩散或流匹配模型在条件生成任务中表现良好，但在给定条件下生成最优布局仍有改进空间。

Method: 通过条件检索布局模板作为参考，并设计条件调制注意力机制，选择性吸收检索知识。

Result: 实验表明，该方法能生成高质量布局，优于现有最优模型。

Conclusion: 提出的检索和参考引导方法有效提升了布局生成质量。

Abstract: Controllable layout generation aims to create plausible visual arrangements
of element bounding boxes within a graphic design according to certain optional
constraints, such as the type or position of a specific component. While recent
diffusion or flow-matching models have achieved considerable advances in
multifarious conditional generation tasks, there remains considerable room for
generating optimal arrangements under given conditions. In this work, we
propose to carry out layout generation through retrieving by conditions and
reference-guided generation. Specifically, we retrieve appropriate layout
templates according to given conditions as references. The references are then
utilized to guide the denoising or flow-based transport process. By retrieving
layouts compatible with the given conditions, we can uncover the potential
information not explicitly provided in the given condition. Such an approach
offers more effective guidance to the model during the generation process, in
contrast to previous models that feed the condition to the model and let the
model infer the unprovided layout attributes directly. Meanwhile, we design a
condition-modulated attention that selectively absorbs retrieval knowledge,
adapting to the difference between retrieved templates and given conditions.
Extensive experiment results show that our method successfully produces
high-quality layouts that meet the given conditions and outperforms existing
state-of-the-art models. Code will be released upon acceptance.

</details>


### [225] [LinkTo-Anime: A 2D Animation Optical Flow Dataset from 3D Model Rendering](https://arxiv.org/abs/2506.02733)
*Xiaoyi Feng,Kaifeng Zou,Caichun Cen,Tao Huang,Hui Guo,Zizhou Huang,Yingli Zhao,Mingqing Zhang,Diwei Wang,Yuntao Zou,Dagang Li*

Main category: cs.CV

Relevance: 30.0

TL;DR: LinkTo-Anime是一个专门为cel动画角色运动设计的高质量光学流数据集，填补了现有数据集的空白，并提供了丰富的注释和基准测试。


<details>
  <summary>Details</summary>
Motivation: 现有光学流数据集主要关注真实世界模拟或合成人类运动，缺乏针对cel动画角色运动的数据集。LinkTo-Anime旨在填补这一空白，支持光学流估计及相关任务的研究。

Method: 通过3D模型渲染生成数据集，包含395个视频序列，提供前向和后向光学流、遮挡掩码和Mixamo骨骼等注释。

Result: 数据集包含24,230训练帧、720验证帧和4,320测试帧，并构建了全面的基准测试以分析不同方法的局限性。

Conclusion: LinkTo-Anime为cel动画角色运动的光学流研究提供了首个高质量数据集，并展示了现有方法的不足。

Abstract: Existing optical flow datasets focus primarily on real-world simulation or
synthetic human motion, but few are tailored to Celluloid(cel) anime character
motion: a domain with unique visual and motion characteristics. To bridge this
gap and facilitate research in optical flow estimation and downstream tasks
such as anime video generation and line drawing colorization, we introduce
LinkTo-Anime, the first high-quality dataset specifically designed for cel
anime character motion generated with 3D model rendering. LinkTo-Anime provides
rich annotations including forward and backward optical flow, occlusion masks,
and Mixamo Skeleton. The dataset comprises 395 video sequences, totally 24,230
training frames, 720 validation frames, and 4,320 test frames. Furthermore, a
comprehensive benchmark is constructed with various optical flow estimation
methods to analyze the shortcomings and limitations across multiple datasets.

</details>


### [226] [GeneA-SLAM2: Dynamic SLAM with AutoEncoder-Preprocessed Genetic Keypoints Resampling and Depth Variance-Guided Dynamic Region Removal](https://arxiv.org/abs/2506.02736)
*Shufan Qing,Anzhen Li,Qiandi Wang,Yuefeng Niu,Mingchen Feng,Guoliang Hu,Jinqiao Wu,Fengtao Nan,Yingchun Fan*

Main category: cs.CV

Relevance: 30.0

TL;DR: GeneA-SLAM2通过深度方差约束和自编码器改进动态场景中的SLAM，提高姿态估计精度。


<details>
  <summary>Details</summary>
Motivation: 现有动态环境中的语义SLAM方法无法完全覆盖动态区域，需更鲁棒高效的解决方案。

Method: 利用深度方差提取动态像素，生成深度掩码；自编码器重建关键点，改进遗传重采样算法。

Result: 在高度动态序列中表现优于现有方法，保持高精度。

Conclusion: GeneA-SLAM2为动态场景提供了一种高效且鲁棒的解决方案。

Abstract: Existing semantic SLAM in dynamic environments mainly identify dynamic
regions through object detection or semantic segmentation methods. However, in
certain highly dynamic scenarios, the detection boxes or segmentation masks
cannot fully cover dynamic regions. Therefore, this paper proposes a robust and
efficient GeneA-SLAM2 system that leverages depth variance constraints to
handle dynamic scenes. Our method extracts dynamic pixels via depth variance
and creates precise depth masks to guide the removal of dynamic objects.
Simultaneously, an autoencoder is used to reconstruct keypoints, improving the
genetic resampling keypoint algorithm to obtain more uniformly distributed
keypoints and enhance the accuracy of pose estimation. Our system was evaluated
on multiple highly dynamic sequences. The results demonstrate that GeneA-SLAM2
maintains high accuracy in dynamic scenes compared to current methods. Code is
available at: https://github.com/qingshufan/GeneA-SLAM2.

</details>


### [227] [VTGaussian-SLAM: RGBD SLAM for Large Scale Scenes with Splatting View-Tied 3D Gaussians](https://arxiv.org/abs/2506.02741)
*Pengchong Hu,Zhizhong Han*

Main category: cs.CV

Relevance: 30.0

TL;DR: 提出了一种基于视图绑定的3D高斯表示方法（view-tied 3D Gaussians），用于RGBD SLAM系统，解决了现有方法在超大场景中因GPU内存限制而无法扩展的问题。


<details>
  <summary>Details</summary>
Motivation: 现有基于3D高斯的SLAM方法在超大场景中效率低下，无法优化所有高斯参数以保持几何和颜色一致性。

Method: 提出视图绑定的3D高斯表示，简化高斯参数（无需学习位置、旋转和多维方差），并结合新的跟踪和映射策略。

Result: 在渲染质量、跟踪精度和可扩展性上优于最新方法。

Conclusion: 视图绑定的3D高斯表示显著节省存储并提升性能。

Abstract: Jointly estimating camera poses and mapping scenes from RGBD images is a
fundamental task in simultaneous localization and mapping (SLAM).
State-of-the-art methods employ 3D Gaussians to represent a scene, and render
these Gaussians through splatting for higher efficiency and better rendering.
However, these methods cannot scale up to extremely large scenes, due to the
inefficient tracking and mapping strategies that need to optimize all 3D
Gaussians in the limited GPU memories throughout the training to maintain the
geometry and color consistency to previous RGBD observations. To resolve this
issue, we propose novel tracking and mapping strategies to work with a novel 3D
representation, dubbed view-tied 3D Gaussians, for RGBD SLAM systems. View-tied
3D Gaussians is a kind of simplified Gaussians, which is tied to depth pixels,
without needing to learn locations, rotations, and multi-dimensional variances.
Tying Gaussians to views not only significantly saves storage but also allows
us to employ many more Gaussians to represent local details in the limited GPU
memory. Moreover, our strategies remove the need of maintaining all Gaussians
learnable throughout the training, while improving rendering quality, and
tracking accuracy. We justify the effectiveness of these designs, and report
better performance over the latest methods on the widely used benchmarks in
terms of rendering and tracking accuracy and scalability. Please see our
project page for code and videos at
https://machineperceptionlab.github.io/VTGaussian-SLAM-Project .

</details>


### [228] [RobustSplat: Decoupling Densification and Dynamics for Transient-Free 3DGS](https://arxiv.org/abs/2506.02751)
*Chuanyu Fu,Yuqi Zhang,Kunbin Yao,Guanying Chen,Yuan Xiong,Chuan Huang,Shuguang Cui,Xiaochun Cao*

Main category: cs.CV

Relevance: 30.0

TL;DR: 论文提出了RobustSplat方法，通过延迟高斯增长策略和尺度级联掩码引导，解决了3D高斯溅射中瞬态物体导致的渲染伪影问题。


<details>
  <summary>Details</summary>
Motivation: 现有方法在建模受瞬态物体影响的场景时存在渲染伪影问题，高斯密度化过程加剧了这一问题。

Method: 1. 延迟高斯增长策略，优先优化静态场景结构；2. 尺度级联掩码引导，从低分辨率到高分辨率逐步优化掩码预测。

Result: 在多个数据集上表现优于现有方法，证明了方法的鲁棒性和有效性。

Conclusion: RobustSplat通过优化高斯增长和掩码引导，显著提升了3D高斯溅射的渲染质量。

Abstract: 3D Gaussian Splatting (3DGS) has gained significant attention for its
real-time, photo-realistic rendering in novel-view synthesis and 3D modeling.
However, existing methods struggle with accurately modeling scenes affected by
transient objects, leading to artifacts in the rendered images. We identify
that the Gaussian densification process, while enhancing scene detail capture,
unintentionally contributes to these artifacts by growing additional Gaussians
that model transient disturbances. To address this, we propose RobustSplat, a
robust solution based on two critical designs. First, we introduce a delayed
Gaussian growth strategy that prioritizes optimizing static scene structure
before allowing Gaussian splitting/cloning, mitigating overfitting to transient
objects in early optimization. Second, we design a scale-cascaded mask
bootstrapping approach that first leverages lower-resolution feature similarity
supervision for reliable initial transient mask estimation, taking advantage of
its stronger semantic consistency and robustness to noise, and then progresses
to high-resolution supervision to achieve more precise mask prediction.
Extensive experiments on multiple challenging datasets show that our method
outperforms existing methods, clearly demonstrating the robustness and
effectiveness of our method. Our project page is
https://fcyycf.github.io/RobustSplat/.

</details>


### [229] [Unified Attention Modeling for Efficient Free-Viewing and Visual Search via Shared Representations](https://arxiv.org/abs/2506.02764)
*Fatma Youssef Mohammed,Kostas Alexis*

Main category: cs.CV

Relevance: 30.0

TL;DR: 论文探讨了自由观看和任务驱动的视觉搜索是否可以共享一个共同的注意力表示，提出了一种基于HAT的神经网络架构，验证了假设。结果表明，模型可以高效迁移，性能仅下降3.86%，并显著降低计算成本。


<details>
  <summary>Details</summary>
Motivation: 研究自由观看和任务驱动的视觉搜索是否存在共同的注意力表示，以验证是否可以通过共享表示提高效率。

Method: 提出了一种基于Human Attention Transformer (HAT)的神经网络架构，测试自由观看和视觉搜索的共享表示能力。

Result: 模型在自由观看训练后迁移到视觉搜索任务中，性能仅下降3.86%（SemSS指标），计算成本降低92.29%（GFLOPs）和31.23%（参数）。

Conclusion: 自由观看和视觉搜索可以高效共享注意力表示，显著降低计算成本。

Abstract: Computational human attention modeling in free-viewing and task-specific
settings is often studied separately, with limited exploration of whether a
common representation exists between them. This work investigates this question
and proposes a neural network architecture that builds upon the Human Attention
transformer (HAT) to test the hypothesis. Our results demonstrate that
free-viewing and visual search can efficiently share a common representation,
allowing a model trained in free-viewing attention to transfer its knowledge to
task-driven visual search with a performance drop of only 3.86% in the
predicted fixation scanpaths, measured by the semantic sequence score (SemSS)
metric which reflects the similarity between predicted and human scanpaths.
This transfer reduces computational costs by 92.29% in terms of GFLOPs and
31.23% in terms of trainable parameters.

</details>


### [230] [SAMJ: Fast Image Annotation on ImageJ/Fiji via Segment Anything Model](https://arxiv.org/abs/2506.02783)
*Carlos Garcia-Lopez-de-Haro,Caterina Fuster-Barcelo,Curtis T. Rueden,Jonathan Heras,Vladimir Ulman,Daniel Franco-Barranco,Adrian Ines,Kevin W. Eliceiri,Jean-Christophe Olivo-Marin,Jean-Yves Tinevez,Daniel Sage,Arrate Munoz-Barrutia*

Main category: cs.CV

Relevance: 30.0

TL;DR: SAMJ是一个基于Segment Anything Model (SAM)的ImageJ/Fiji插件，旨在简化生物医学图像分析中的掩码标注工作。


<details>
  <summary>Details</summary>
Motivation: 掩码标注在AI驱动的生物医学图像分析中是一个耗时且劳动密集的环节，需要更高效的解决方案。

Method: 通过开发SAMJ插件，利用SAM模型实现交互式标注，支持一键安装和实时对象描绘。

Result: SAMJ显著简化了标注流程，加速了标记图像数据集的创建。

Conclusion: SAMJ为生物医学图像分析提供了一个用户友好且高效的标注工具。

Abstract: Mask annotation remains a significant bottleneck in AI-driven biomedical
image analysis due to its labor-intensive nature. To address this challenge, we
introduce SAMJ, a user-friendly ImageJ/Fiji plugin leveraging the Segment
Anything Model (SAM). SAMJ enables seamless, interactive annotations with
one-click installation on standard computers. Designed for real-time object
delineation in large scientific images, SAMJ is an easy-to-use solution that
simplifies and accelerates the creation of labeled image datasets.

</details>


### [231] [Go Beyond Earth: Understanding Human Actions and Scenes in Microgravity Environments](https://arxiv.org/abs/2506.02845)
*Di Wen,Lei Qi,Kunyu Peng,Kailun Yang,Fei Teng,Ao Luo,Jia Fu,Yufan Chen,Ruiping Liu,Yitian Shi,M. Saquib Sarfraz,Rainer Stiefelhagen*

Main category: cs.CV

Relevance: 30.0

TL;DR: 论文提出了MicroG-4M，首个用于微重力环境下人类活动时空和语义理解的基准数据集，填补了现有视频理解数据集的空白。


<details>
  <summary>Details</summary>
Motivation: 现有视频理解数据集主要基于地球重力条件，而微重力环境下的人类活动和视觉语义存在显著差异，这对安全关键的空间应用提出了挑战。

Method: 通过真实太空任务和电影模拟构建数据集，包含4,759个视频片段、50种动作、1,238个上下文丰富的描述和7,000多个问答对。

Result: 数据集支持三个核心任务：细粒度多标签动作识别、时序视频描述和视觉问答，为微重力环境下的空间定位和语义推理提供了全面评估。

Conclusion: MicroG-4M为微重力环境下的视频理解提供了首个基准，并公开了所有数据、标注和代码。

Abstract: Despite substantial progress in video understanding, most existing datasets
are limited to Earth's gravitational conditions. However, microgravity alters
human motion, interactions, and visual semantics, revealing a critical gap for
real-world vision systems. This presents a challenge for domain-robust video
understanding in safety-critical space applications. To address this, we
introduce MicroG-4M, the first benchmark for spatio-temporal and semantic
understanding of human activities in microgravity. Constructed from real-world
space missions and cinematic simulations, the dataset includes 4,759 clips
covering 50 actions, 1,238 context-rich captions, and over 7,000
question-answer pairs on astronaut activities and scene understanding.
MicroG-4M supports three core tasks: fine-grained multi-label action
recognition, temporal video captioning, and visual question answering, enabling
a comprehensive evaluation of both spatial localization and semantic reasoning
in microgravity contexts. We establish baselines using state-of-the-art models.
All data, annotations, and code are available at
https://github.com/LEI-QI-233/HAR-in-Space.

</details>


### [232] [Learning Pyramid-structured Long-range Dependencies for 3D Human Pose Estimation](https://arxiv.org/abs/2506.02853)
*Mingjie Wei,Xuemei Xie,Yutong Zhong,Guangming Shi*

Main category: cs.CV

Relevance: 30.0

TL;DR: 提出了一种金字塔图注意力（PGA）模块和金字塔图变换器（PGFormer），用于3D人体姿态估计，通过多尺度依赖建模降低误差和模型大小。


<details>
  <summary>Details</summary>
Motivation: 解决现有方法在建模长距离依赖时引入噪声和增加模型大小的问题，同时捕捉关节和身体部分之间的相关性。

Method: 提出PGA模块捕获跨尺度长距离依赖，结合图卷积模块构建PGFormer，利用池化封装人体子结构到自注意力中。

Result: 在Human3.6M和MPI-INF-3DHP数据集上实现了更低误差和更小模型大小。

Conclusion: PGFormer通过多尺度依赖建模提升了3D姿态估计性能，同时保持轻量级。

Abstract: Action coordination in human structure is indispensable for the spatial
constraints of 2D joints to recover 3D pose. Usually, action coordination is
represented as a long-range dependence among body parts. However, there are two
main challenges in modeling long-range dependencies. First, joints should not
only be constrained by other individual joints but also be modulated by the
body parts. Second, existing methods make networks deeper to learn dependencies
between non-linked parts. They introduce uncorrelated noise and increase the
model size. In this paper, we utilize a pyramid structure to better learn
potential long-range dependencies. It can capture the correlation across joints
and groups, which complements the context of the human sub-structure. In an
effective cross-scale way, it captures the pyramid-structured long-range
dependence. Specifically, we propose a novel Pyramid Graph Attention (PGA)
module to capture long-range cross-scale dependencies. It concatenates
information from various scales into a compact sequence, and then computes the
correlation between scales in parallel. Combining PGA with graph convolution
modules, we develop a Pyramid Graph Transformer (PGFormer) for 3D human pose
estimation, which is a lightweight multi-scale transformer architecture. It
encapsulates human sub-structures into self-attention by pooling. Extensive
experiments show that our approach achieves lower error and smaller model size
than state-of-the-art methods on Human3.6M and MPI-INF-3DHP datasets. The code
is available at https://github.com/MingjieWe/PGFormer.

</details>


### [233] [Enhancing Abnormality Identification: Robust Out-of-Distribution Strategies for Deepfake Detection](https://arxiv.org/abs/2506.02857)
*Luca Maiano,Fabrizio Casadei,Irene Amerini*

Main category: cs.CV

Relevance: 30.0

TL;DR: 论文提出两种新的OOD检测方法用于深度伪造检测，一种基于输入图像重建，另一种结合注意力机制，实验验证其优于现有技术。


<details>
  <summary>Details</summary>
Motivation: 解决深度伪造检测在开放场景中的泛化难题，应对新生成模型不断涌现的挑战。

Method: 1. 基于输入图像重建的OOD检测方法；2. 结合注意力机制的OOD检测方法。

Result: 方法在深度伪造检测中表现优异，基准测试中排名靠前。

Conclusion: 提出的方法为动态现实应用提供了鲁棒且适应性强的解决方案。

Abstract: Detecting deepfakes has become a critical challenge in Computer Vision and
Artificial Intelligence. Despite significant progress in detection techniques,
generalizing them to open-set scenarios continues to be a persistent
difficulty. Neural networks are often trained on the closed-world assumption,
but with new generative models constantly evolving, it is inevitable to
encounter data generated by models that are not part of the training
distribution. To address these challenges, in this paper, we propose two novel
Out-Of-Distribution (OOD) detection approaches. The first approach is trained
to reconstruct the input image, while the second incorporates an attention
mechanism for detecting OODs. Our experiments validate the effectiveness of the
proposed approaches compared to existing state-of-the-art techniques. Our
method achieves promising results in deepfake detection and ranks among the
top-performing configurations on the benchmark, demonstrating their potential
for robust, adaptable solutions in dynamic, real-world applications.

</details>


### [234] [MVTD: A Benchmark Dataset for Maritime Visual Object Tracking](https://arxiv.org/abs/2506.02866)
*Ahsan Baidar Bakht,Muhayy Ud Din,Sajid Javed,Irfan Hussain*

Main category: cs.CV

Relevance: 30.0

TL;DR: 论文介绍了专门为海上视觉目标跟踪设计的Maritime Visual Tracking Dataset (MVTD)，包含182个高分辨率视频序列，共约15万帧，覆盖四种典型目标类别。实验表明，现有SOTA跟踪算法在MVTD上性能显著下降，但经过微调后性能提升显著。


<details>
  <summary>Details</summary>
Motivation: 海上环境中的视觉目标跟踪面临独特挑战（如水面反射、低对比度目标等），现有通用数据集无法满足需求，因此需要领域专用数据集。

Method: 构建了MVTD数据集，包含182个视频序列和四种目标类别，并评估了14种SOTA跟踪算法在MVTD上的性能。

Result: 现有算法在MVTD上性能显著下降，但微调后性能显著提升。

Conclusion: MVTD填补了海上视觉跟踪领域的空白，证明了领域适应和迁移学习的重要性。

Abstract: Visual Object Tracking (VOT) is a fundamental task with widespread
applications in autonomous navigation, surveillance, and maritime robotics.
Despite significant advances in generic object tracking, maritime environments
continue to present unique challenges, including specular water reflections,
low-contrast targets, dynamically changing backgrounds, and frequent
occlusions. These complexities significantly degrade the performance of
state-of-the-art tracking algorithms, highlighting the need for domain-specific
datasets. To address this gap, we introduce the Maritime Visual Tracking
Dataset (MVTD), a comprehensive and publicly available benchmark specifically
designed for maritime VOT. MVTD comprises 182 high-resolution video sequences,
totaling approximately 150,000 frames, and includes four representative object
classes: boat, ship, sailboat, and unmanned surface vehicle (USV). The dataset
captures a diverse range of operational conditions and maritime scenarios,
reflecting the real-world complexities of maritime environments. We evaluated
14 recent SOTA tracking algorithms on the MVTD benchmark and observed
substantial performance degradation compared to their performance on
general-purpose datasets. However, when fine-tuned on MVTD, these models
demonstrate significant performance gains, underscoring the effectiveness of
domain adaptation and the importance of transfer learning in specialized
tracking contexts. The MVTD dataset fills a critical gap in the visual tracking
community by providing a realistic and challenging benchmark for maritime
scenarios. Dataset and Source Code can be accessed here
"https://github.com/AhsanBaidar/MVTD".

</details>


### [235] [OpenFace 3.0: A Lightweight Multitask System for Comprehensive Facial Behavior Analysis](https://arxiv.org/abs/2506.02891)
*Jiewen Hu,Leena Mathur,Paul Pu Liang,Louis-Philippe Morency*

Main category: cs.CV

Relevance: 30.0

TL;DR: OpenFace 3.0是一个开源工具包，用于面部行为分析，包括面部标志检测、动作单元检测、视线估计和情感识别。它通过多任务架构训练，提升了性能、速度和效率。


<details>
  <summary>Details</summary>
Motivation: 开发一个轻量级、统一的面部分析模型，适用于多样化场景和任务。

Method: 采用多任务架构训练统一模型，利用参数共享提升性能。

Result: OpenFace 3.0在预测性能、推理速度和内存效率上优于同类工具包，并支持实时运行。

Conclusion: OpenFace 3.0为研究社区提供了一个高效、易用的面部分析工具。

Abstract: In recent years, there has been increasing interest in automatic facial
behavior analysis systems from computing communities such as vision, multimodal
interaction, robotics, and affective computing. Building upon the widespread
utility of prior open-source facial analysis systems, we introduce OpenFace
3.0, an open-source toolkit capable of facial landmark detection, facial action
unit detection, eye-gaze estimation, and facial emotion recognition. OpenFace
3.0 contributes a lightweight unified model for facial analysis, trained with a
multi-task architecture across diverse populations, head poses, lighting
conditions, video resolutions, and facial analysis tasks. By leveraging the
benefits of parameter sharing through a unified model and training paradigm,
OpenFace 3.0 exhibits improvements in prediction performance, inference speed,
and memory efficiency over similar toolkits and rivals state-of-the-art models.
OpenFace 3.0 can be installed and run with a single line of code and operate in
real-time without specialized hardware. OpenFace 3.0 code for training models
and running the system is freely available for research purposes and supports
contributions from the community.

</details>


### [236] [Dense Match Summarization for Faster Two-view Estimation](https://arxiv.org/abs/2506.02893)
*Jonathan Astermark,Anders Heyden,Viktor Larsson*

Main category: cs.CV

Relevance: 30.0

TL;DR: 提出了一种高效的匹配汇总方案，用于加速基于密集对应的两视图相对位姿估计，显著减少计算时间。


<details>
  <summary>Details</summary>
Motivation: 密集匹配器虽能提高位姿估计的准确性和鲁棒性，但其大量匹配点导致RANSAC中的计算时间显著增加。

Method: 提出了一种高效的匹配汇总方案，替代使用全部密集匹配点，以减少计算时间。

Result: 在标准基准数据集上验证，该方法在保持与全密集匹配相当精度的同时，计算速度提高了10-100倍。

Conclusion: 该方案为密集匹配下的位姿估计提供了一种高效且准确的解决方案。

Abstract: In this paper, we speed up robust two-view relative pose from dense
correspondences. Previous work has shown that dense matchers can significantly
improve both accuracy and robustness in the resulting pose. However, the large
number of matches comes with a significantly increased runtime during robust
estimation in RANSAC. To avoid this, we propose an efficient match
summarization scheme which provides comparable accuracy to using the full set
of dense matches, while having 10-100x faster runtime. We validate our approach
on standard benchmark datasets together with multiple state-of-the-art dense
matchers.

</details>


### [237] [Astrophotography turbulence mitigation via generative models](https://arxiv.org/abs/2506.02981)
*Joonyeoup Kim,Yu Yuan,Xingguang Zhang,Xijun Wang,Stanley Chan*

Main category: cs.CV

Relevance: 30.0

TL;DR: AstroDiff利用扩散模型的高质量生成先验和恢复能力，有效减轻天文图像中的大气湍流影响，优于现有学习方法。


<details>
  <summary>Details</summary>
Motivation: 地面望远镜拍摄的天文图像常受大气湍流影响，导致质量下降。传统多帧策略数据密集且处理复杂，因此提出AstroDiff。

Method: 基于扩散模型的生成恢复方法，结合高质量生成先验和恢复能力。

Result: 在严重湍流条件下，AstroDiff在感知质量和结构保真度上优于现有学习方法。

Conclusion: AstroDiff为天文图像湍流问题提供了一种高效解决方案。

Abstract: Photography is the cornerstone of modern astronomical and space research.
However, most astronomical images captured by ground-based telescopes suffer
from atmospheric turbulence, resulting in degraded imaging quality. While
multi-frame strategies like lucky imaging can mitigate some effects, they
involve intensive data acquisition and complex manual processing. In this
paper, we propose AstroDiff, a generative restoration method that leverages
both the high-quality generative priors and restoration capabilities of
diffusion models to mitigate atmospheric turbulence. Extensive experiments
demonstrate that AstroDiff outperforms existing state-of-the-art learning-based
methods in astronomical image turbulence mitigation, providing higher
perceptual quality and better structural fidelity under severe turbulence
conditions. Our code and additional results are available at
https://web-six-kappa-66.vercel.app/

</details>


### [238] [Smartflow: Enabling Scalable Spatiotemporal Geospatial Research](https://arxiv.org/abs/2506.03022)
*David McVicar,Brian Avant,Adrian Gould,Diego Torrejon,Charles Della Porta,Ryan Mukherjee*

Main category: cs.CV

Relevance: 30.0

TL;DR: Smartflow是一个基于云的框架，用于可扩展的时空地理空间研究，支持异构数据处理和模型实验管理。


<details>
  <summary>Details</summary>
Motivation: 解决地理空间数据处理的标准化和可扩展性问题，支持大规模地理区域和时间尺度的模型开发。

Method: 使用STAC兼容目录作为输入，结合ClearML、Tensorboard和Apache Superset管理模型实验，Kubernetes提供工作流编排。

Result: 提出的神经网络架构能够检测大型地理区域的重型建设活动，并在IARPA SMART项目中验证了其有效性。

Conclusion: Smartflow为地理空间模型开发提供了高效、可扩展的解决方案。

Abstract: BlackSky introduces Smartflow, a cloud-based framework enabling scalable
spatiotemporal geospatial research built on open-source tools and technologies.
Using STAC-compliant catalogs as a common input, heterogeneous geospatial data
can be processed into standardized datacubes for analysis and model training.
Model experimentation is managed using a combination of tools, including
ClearML, Tensorboard, and Apache Superset. Underpinning Smartflow is
Kubernetes, which orchestrates the provisioning and execution of workflows to
support both horizontal and vertical scalability. This combination of features
makes Smartflow well-suited for geospatial model development and analysis over
large geographic areas, time scales, and expansive image archives.
  We also present a novel neural architecture, built using Smartflow, to
monitor large geographic areas for heavy construction. Qualitative results
based on data from the IARPA Space-based Machine Automated Recognition
Technique (SMART) program are presented that show the model is capable of
detecting heavy construction throughout all major phases of development.

</details>


### [239] [LEG-SLAM: Real-Time Language-Enhanced Gaussian Splatting for SLAM](https://arxiv.org/abs/2506.03073)
*Roman Titkov,Egor Zubkov,Dmitry Yudin,Jaafar Mahmoud,Malik Mohrat,Gennady Sidorov*

Main category: cs.CV

Relevance: 30.0

TL;DR: LEG-SLAM结合优化的高斯泼溅与视觉语言特征提取，实现实时语义3D SLAM，性能优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 解决高斯泼溅方法中语义信息整合的挑战，同时保持实时性能。

Method: 融合高斯泼溅、DINOv2特征提取和PCA特征压缩，支持在线密集SLAM。

Result: 在Replica和ScanNet数据集上分别达到10 fps和18 fps，重建速度显著优于现有方法。

Conclusion: LEG-SLAM在实时语义3D重建中表现优异，适用于自主机器人和增强现实。

Abstract: Modern Gaussian Splatting methods have proven highly effective for real-time
photorealistic rendering of 3D scenes. However, integrating semantic
information into this representation remains a significant challenge,
especially in maintaining real-time performance for SLAM (Simultaneous
Localization and Mapping) applications. In this work, we introduce LEG-SLAM --
a novel approach that fuses an optimized Gaussian Splatting implementation with
visual-language feature extraction using DINOv2 followed by a learnable feature
compressor based on Principal Component Analysis, while enabling an online
dense SLAM. Our method simultaneously generates high-quality photorealistic
images and semantically labeled scene maps, achieving real-time scene
reconstruction with more than 10 fps on the Replica dataset and 18 fps on
ScanNet. Experimental results show that our approach significantly outperforms
state-of-the-art methods in reconstruction speed while achieving competitive
rendering quality. The proposed system eliminates the need for prior data
preparation such as camera's ego motion or pre-computed static semantic maps.
With its potential applications in autonomous robotics, augmented reality, and
other interactive domains, LEG-SLAM represents a significant step forward in
real-time semantic 3D Gaussian-based SLAM. Project page:
https://titrom025.github.io/LEG-SLAM/

</details>


### [240] [SG2VID: Scene Graphs Enable Fine-Grained Control for Video Synthesis](https://arxiv.org/abs/2506.03082)
*Ssharvien Kumar Sivakumar,Yannik Frisch,Ghazal Ghazaei,Anirban Mukhopadhyay*

Main category: cs.CV

Relevance: 30.0

TL;DR: SG2VID是一种基于扩散模型的视频生成方法，利用场景图实现精确视频合成和细粒度控制，适用于手术模拟。


<details>
  <summary>Details</summary>
Motivation: 传统手术模拟工具缺乏真实感和解剖学变异性，现有生成模型忽视细粒度控制。SG2VID填补了这一空白。

Method: SG2VID结合扩散模型和场景图，支持对工具和解剖结构的精确控制。

Result: SG2VID在质量和定量指标上优于现有方法，并能提升下游任务性能。

Conclusion: SG2VID为手术模拟提供了高真实感和可控性，展示了生成增强的潜力。

Abstract: Surgical simulation plays a pivotal role in training novice surgeons,
accelerating their learning curve and reducing intra-operative errors. However,
conventional simulation tools fall short in providing the necessary
photorealism and the variability of human anatomy. In response, current methods
are shifting towards generative model-based simulators. Yet, these approaches
primarily focus on using increasingly complex conditioning for precise
synthesis while neglecting the fine-grained human control aspect. To address
this gap, we introduce SG2VID, the first diffusion-based video model that
leverages Scene Graphs for both precise video synthesis and fine-grained human
control. We demonstrate SG2VID's capabilities across three public datasets
featuring cataract and cholecystectomy surgery. While SG2VID outperforms
previous methods both qualitatively and quantitatively, it also enables precise
synthesis, providing accurate control over tool and anatomy's size and
movement, entrance of new tools, as well as the overall scene layout. We
qualitatively motivate how SG2VID can be used for generative augmentation and
present an experiment demonstrating its ability to improve a downstream phase
detection task when the training set is extended with our synthetic videos.
Finally, to showcase SG2VID's ability to retain human control, we interact with
the Scene Graphs to generate new video samples depicting major yet rare
intra-operative irregularities.

</details>


### [241] [DyTact: Capturing Dynamic Contacts in Hand-Object Manipulation](https://arxiv.org/abs/2506.03103)
*Xiaoyan Cong,Angela Xing,Chandradeep Pokhariya,Rao Fu,Srinath Sridhar*

Main category: cs.CV

Relevance: 30.0

TL;DR: DyTact是一种无标记的动态手-物体接触捕捉方法，通过2D高斯面元建模复杂操作，结合MANO网格的归纳偏置优化，提升动态接触估计精度和视图合成质量。


<details>
  <summary>Details</summary>
Motivation: 动态手-物体接触重建在AI动画、XR和机器人技术中至关重要，但现有技术因遮挡、复杂表面细节和捕获限制而面临挑战。

Method: DyTact采用动态2D高斯面元表示，结合MANO网格的归纳偏置优化，通过细化模块处理高频变形，并通过接触引导的自适应采样策略处理遮挡。

Result: DyTact在动态接触估计精度和视图合成质量上达到最优，同时优化速度快且内存高效。

Conclusion: DyTact为非侵入式动态接触捕捉提供了一种高效且精确的解决方案。

Abstract: Reconstructing dynamic hand-object contacts is essential for realistic
manipulation in AI character animation, XR, and robotics, yet it remains
challenging due to heavy occlusions, complex surface details, and limitations
in existing capture techniques. In this paper, we introduce DyTact, a
markerless capture method for accurately capturing dynamic contact in
hand-object manipulations in a non-intrusive manner. Our approach leverages a
dynamic, articulated representation based on 2D Gaussian surfels to model
complex manipulations. By binding these surfels to MANO meshes, DyTact
harnesses the inductive bias of template models to stabilize and accelerate
optimization. A refinement module addresses time-dependent high-frequency
deformations, while a contact-guided adaptive sampling strategy selectively
increases surfel density in contact regions to handle heavy occlusion.
Extensive experiments demonstrate that DyTact not only achieves
state-of-the-art dynamic contact estimation accuracy but also significantly
improves novel view synthesis quality, all while operating with fast
optimization and efficient memory usage. Project Page:
https://oliver-cong02.github.io/DyTact.github.io/ .

</details>


### [242] [Zero-Shot Tree Detection and Segmentation from Aerial Forest Imagery](https://arxiv.org/abs/2506.03114)
*Michelle Chen,David Russell,Amritha Pallavoor,Derek Young,Jane Wu*

Main category: cs.CV

Relevance: 30.0

TL;DR: 论文探讨了使用Segment Anything Model 2 (SAM2) 进行零样本树木检测和分割的效果，展示了其在遥感图像中的泛化能力以及与专业方法的协同作用。


<details>
  <summary>Details</summary>
Motivation: 研究旨在解决现有树木分割方法依赖难以扩展的训练数据的问题，探索预训练模型在遥感领域的潜力。

Method: 使用预训练的SAM2模型进行零样本分割和零样本迁移（利用现有树木检测模型的预测作为提示）。

Result: SAM2表现出强大的泛化能力，并能与专业方法协同工作，为遥感问题提供了新思路。

Conclusion: 预训练模型在遥感领域的应用具有前景，未来可进一步探索。

Abstract: Large-scale delineation of individual trees from remote sensing imagery is
crucial to the advancement of ecological research, particularly as climate
change and other environmental factors rapidly transform forest landscapes
across the world. Current RGB tree segmentation methods rely on training
specialized machine learning models with labeled tree datasets. While these
learning-based approaches can outperform manual data collection when accurate,
the existing models still depend on training data that's hard to scale. In this
paper, we investigate the efficacy of using a state-of-the-art image
segmentation model, Segment Anything Model 2 (SAM2), in a zero-shot manner for
individual tree detection and segmentation. We evaluate a pretrained SAM2 model
on two tasks in this domain: (1) zero-shot segmentation and (2) zero-shot
transfer by using predictions from an existing tree detection model as prompts.
Our results suggest that SAM2 not only has impressive generalization
capabilities, but also can form a natural synergy with specialized methods
trained on in-domain labeled data. We find that applying large pretrained
models to problems in remote sensing is a promising avenue for future progress.
We make our code available at:
https://github.com/open-forest-observatory/tree-detection-framework.

</details>


### [243] [Controllable Human-centric Keyframe Interpolation with Generative Prior](https://arxiv.org/abs/2506.03119)
*Zujin Guo,Size Wu,Zhongang Cai,Wei Li,Chen Change Loy*

Main category: cs.CV

Relevance: 30.0

TL;DR: PoseFuse3D-KI结合3D人体引导信号改进视频关键帧插值，显著提升复杂人体动作的生成质量与控制能力。


<details>
  <summary>Details</summary>
Motivation: 现有插值方法缺乏3D几何引导，难以处理复杂人体动作且控制能力有限。

Method: 提出PoseFuse3D-KI框架，通过SMPL-X编码器和融合网络将3D几何信息融入2D潜在空间。

Result: 在CHKI-Video数据集上，PSNR提升9%，LPIPS降低38%，显著优于基线方法。

Conclusion: PoseFuse3D-KI通过3D引导显著提升插值质量，适用于可控人体关键帧插值任务。

Abstract: Existing interpolation methods use pre-trained video diffusion priors to
generate intermediate frames between sparsely sampled keyframes. In the absence
of 3D geometric guidance, these methods struggle to produce plausible results
for complex, articulated human motions and offer limited control over the
synthesized dynamics. In this paper, we introduce PoseFuse3D Keyframe
Interpolator (PoseFuse3D-KI), a novel framework that integrates 3D human
guidance signals into the diffusion process for Controllable Human-centric
Keyframe Interpolation (CHKI). To provide rich spatial and structural cues for
interpolation, our PoseFuse3D, a 3D-informed control model, features a novel
SMPL-X encoder that transforms 3D geometry and shape into the 2D latent
conditioning space, alongside a fusion network that integrates these 3D cues
with 2D pose embeddings. For evaluation, we build CHKI-Video, a new dataset
annotated with both 2D poses and 3D SMPL-X parameters. We show that
PoseFuse3D-KI consistently outperforms state-of-the-art baselines on
CHKI-Video, achieving a 9% improvement in PSNR and a 38% reduction in LPIPS.
Comprehensive ablations demonstrate that our PoseFuse3D model improves
interpolation fidelity.

</details>


### [244] [CamCloneMaster: Enabling Reference-based Camera Control for Video Generation](https://arxiv.org/abs/2506.03140)
*Yawen Luo,Jianhong Bai,Xiaoyu Shi,Menghan Xia,Xintao Wang,Pengfei Wan,Di Zhang,Kun Gai,Tianfan Xue*

Main category: cs.CV

Relevance: 30.0

TL;DR: CamCloneMaster是一个框架，允许用户通过参考视频复制相机运动，无需相机参数或微调，支持Image-to-Video和Video-to-Video任务。


<details>
  <summary>Details</summary>
Motivation: 现有方法依赖显式相机参数序列，用户操作繁琐，尤其是复杂相机运动时。

Method: 提出CamCloneMaster框架，利用参考视频实现相机运动复制，无需相机参数或微调，并构建Camera Clone Dataset支持学习。

Result: 实验和用户研究表明，CamCloneMaster在相机可控性和视觉质量上优于现有方法。

Conclusion: CamCloneMaster提供了一种直观的相机控制方法，适用于多种视频生成任务。

Abstract: Camera control is crucial for generating expressive and cinematic videos.
Existing methods rely on explicit sequences of camera parameters as control
conditions, which can be cumbersome for users to construct, particularly for
intricate camera movements. To provide a more intuitive camera control method,
we propose CamCloneMaster, a framework that enables users to replicate camera
movements from reference videos without requiring camera parameters or
test-time fine-tuning. CamCloneMaster seamlessly supports reference-based
camera control for both Image-to-Video and Video-to-Video tasks within a
unified framework. Furthermore, we present the Camera Clone Dataset, a
large-scale synthetic dataset designed for camera clone learning, encompassing
diverse scenes, subjects, and camera movements. Extensive experiments and user
studies demonstrate that CamCloneMaster outperforms existing methods in terms
of both camera controllability and visual quality.

</details>


### [245] [Are Pixel-Wise Metrics Reliable for Sparse-View Computed Tomography Reconstruction?](https://arxiv.org/abs/2506.02093)
*Tianyu Lin,Xinran Li,Chuntung Zhuang,Qi Chen,Yuanhao Cai,Kai Ding,Alan L. Yuille,Zongwei Zhou*

Main category: eess.IV

Relevance: 30.0

TL;DR: 提出了一套针对稀疏视图CT重建的解剖感知评估指标，并引入CARE框架以提升结构完整性。


<details>
  <summary>Details</summary>
Motivation: 现有评估指标（如SSIM和PSNR）在捕捉关键解剖结构完整性方面存在不足，尤其是对小或薄区域的评估。

Method: 提出解剖感知评估指标，并开发CARE框架，通过结构惩罚在训练中增强解剖结构的保留。

Result: CARE显著提升了CT重建的结构完整性，大器官提升32%，小器官22%，肠道40%，血管36%。

Conclusion: CARE是一种模型无关的框架，可有效提升稀疏视图CT重建的结构完整性。

Abstract: Widely adopted evaluation metrics for sparse-view CT reconstruction--such as
Structural Similarity Index Measure and Peak Signal-to-Noise Ratio--prioritize
pixel-wise fidelity but often fail to capture the completeness of critical
anatomical structures, particularly small or thin regions that are easily
missed. To address this limitation, we propose a suite of novel anatomy-aware
evaluation metrics designed to assess structural completeness across anatomical
structures, including large organs, small organs, intestines, and vessels.
Building on these metrics, we introduce CARE, a Completeness-Aware
Reconstruction Enhancement framework that incorporates structural penalties
during training to encourage anatomical preservation of significant structures.
CARE is model-agnostic and can be seamlessly integrated into analytical,
implicit, and generative methods. When applied to these methods, CARE
substantially improves structural completeness in CT reconstructions, achieving
up to +32% improvement for large organs, +22% for small organs, +40% for
intestines, and +36% for vessels.

</details>


### [246] [Is PMBOK Guide the Right Fit for AI? Re-evaluating Project Management in the Face of Artificial Intelligence Projects](https://arxiv.org/abs/2506.02214)
*Alexey Burdakov,Max Jaihyun Ahn*

Main category: cs.SE

Relevance: 30.0

TL;DR: 本文评估了PMBOK指南在AI项目中的适用性，指出了其在数据管理、迭代开发和伦理问题上的不足，并提出了改进建议。


<details>
  <summary>Details</summary>
Motivation: 传统项目管理框架（如PMBOK）在AI项目中存在局限性，需要针对AI的动态性和复杂性进行适配。

Method: 通过分析PMBOK指南的不足，提出数据生命周期管理、迭代框架和伦理嵌入等改进措施。

Result: 识别了PMBOK在AI项目中的关键缺陷，并提出了具体的改进方案。

Conclusion: 改进后的项目管理框架能更好地支持AI项目的动态性和复杂性。

Abstract: This paper critically evaluates the applicability of the Project Management
Body of Knowledge (PMBOK) Guide framework to Artificial Intelligence (AI)
software projects, highlighting key limitations and proposing tailored
adaptations. Unlike traditional projects, AI initiatives rely heavily on
complex data, iterative experimentation, and specialized expertise while
navigating significant ethical considerations. Our analysis identifies gaps in
the PMBOK Guide, including its limited focus on data management, insufficient
support for iterative development, and lack of guidance on ethical and
multidisciplinary challenges. To address these deficiencies, we recommend
integrating data lifecycle management, adopting iterative and AI project
management frameworks, and embedding ethical considerations within project
planning and execution. Additionally, we explore alternative approaches that
better align with AI's dynamic and exploratory nature. We aim to enhance
project management practices for AI software projects by bridging these gaps.

</details>


### [247] [EyeNavGS: A 6-DoF Navigation Dataset and Record-n-Replay Software for Real-World 3DGS Scenes in VR](https://arxiv.org/abs/2506.02380)
*Zihao Ding,Cheng-Tse Lee,Mufeng Zhu,Tao Guan,Yuan-Chun Sun,Cheng-Hsin Hsu,Yao Liu*

Main category: cs.MM

Relevance: 30.0

TL;DR: 论文介绍了EyeNavGS，首个公开的6自由度导航数据集，包含46名参与者在12个真实3D高斯场景中的导航数据。


<details>
  <summary>Details</summary>
Motivation: 目前缺乏用于高保真3D高斯场景的真实用户导航数据，阻碍了相关应用开发和性能优化。

Method: 使用Meta Quest Pro头显记录参与者在12个场景中的头部姿态和眼动数据，并进行场景初始化和数据处理。

Result: 发布了EyeNavGS数据集及配套软件工具，支持6-DoF视口预测、自适应流传输等研究。

Conclusion: EyeNavGS为3D高斯场景的导航研究提供了宝贵资源。

Abstract: 3D Gaussian Splatting (3DGS) is an emerging media representation that
reconstructs real-world 3D scenes in high fidelity, enabling
6-degrees-of-freedom (6-DoF) navigation in virtual reality (VR). However,
developing and evaluating 3DGS-enabled applications and optimizing their
rendering performance, require realistic user navigation data. Such data is
currently unavailable for photorealistic 3DGS reconstructions of real-world
scenes. This paper introduces EyeNavGS (EyeNavGS), the first publicly available
6-DoF navigation dataset featuring traces from 46 participants exploring twelve
diverse, real-world 3DGS scenes. The dataset was collected at two sites, using
the Meta Quest Pro headsets, recording the head pose and eye gaze data for each
rendered frame during free world standing 6-DoF navigation. For each of the
twelve scenes, we performed careful scene initialization to correct for scene
tilt and scale, ensuring a perceptually-comfortable VR experience. We also
release our open-source SIBR viewer software fork with record-and-replay
functionalities and a suite of utility tools for data processing, conversion,
and visualization. The EyeNavGS dataset and its accompanying software tools
provide valuable resources for advancing research in 6-DoF viewport prediction,
adaptive streaming, 3D saliency, and foveated rendering for 3DGS scenes. The
EyeNavGS dataset is available at: https://symmru.github.io/EyeNavGS/.

</details>


### [248] [Unrolling Nonconvex Graph Total Variation for Image Denoising](https://arxiv.org/abs/2506.02381)
*Songlin Wei,Gene Cheung,Fei Chen,Ivan Selesnick*

Main category: eess.IV

Relevance: 30.0

TL;DR: 提出了一种新的非凸图总变分（NC-GTV）方法用于图像去噪，通过结合Huber函数和Gershgorin圆定理确保目标函数的凸性，并设计了一个高效的ADMM算法。


<details>
  <summary>Details</summary>
Motivation: 传统的图像去噪方法使用凸正则化项（如TV），但存在局限性。本文旨在通过非凸正则化项提升去噪性能，同时确保目标函数的凸性。

Method: 1. 提出NC-GTV，基于图的Huber函数；2. 使用Gershgorin圆定理计算参数确保凸性；3. 设计基于ADMM的高效算法，并将其展开为轻量级网络。

Result: 实验表明，该方法在图像去噪任务中优于其他代表性方法，且网络参数更少。

Conclusion: NC-GTV结合凸性保证和高效算法，为图像去噪提供了一种新的有效方法。

Abstract: Conventional model-based image denoising optimizations employ convex
regularization terms, such as total variation (TV) that convexifies the
$\ell_0$-norm to promote sparse signal representation. Instead, we propose a
new non-convex total variation term in a graph setting (NC-GTV), such that when
combined with an $\ell_2$-norm fidelity term for denoising, leads to a convex
objective with no extraneous local minima. We define NC-GTV using a new graph
variant of the Huber function, interpretable as a Moreau envelope. The crux is
the selection of a parameter $a$ characterizing the graph Huber function that
ensures overall objective convexity; we efficiently compute $a$ via an
adaptation of Gershgorin Circle Theorem (GCT). To minimize the convex
objective, we design a linear-time algorithm based on Alternating Direction
Method of Multipliers (ADMM) and unroll it into a lightweight feed-forward
network for data-driven parameter learning. Experiments show that our method
outperforms unrolled GTV and other representative image denoising schemes,
while employing far fewer network parameters.

</details>


### [249] [Multi-modal brain MRI synthesis based on SwinUNETR](https://arxiv.org/abs/2506.02467)
*Haowen Pang,Weiyan Guo,Chuyang Ye*

Main category: eess.IV

Relevance: 30.0

TL;DR: 论文提出了一种基于SwinUNETR的方法，用于合成脑部MRI中缺失的模态。SwinUNETR结合了Swin Transformer和CNN的优势，能够有效捕捉局部和全局信息，生成高质量的合成图像。


<details>
  <summary>Details</summary>
Motivation: 临床实践中常遇到脑部MRI模态缺失的问题，影响诊断效果。研究旨在通过多模态合成技术解决这一问题。

Method: 采用SwinUNETR架构，结合Swin Transformer的层次特征提取和窗口自注意力机制，以及CNN的空间分辨率优势。

Result: 实验表明，SwinUNETR在图像质量、解剖一致性和诊断价值方面显著优于其他方法。

Conclusion: SwinUNETR是一种有效的多模态MRI合成方法，具有临床应用潜力。

Abstract: Multi-modal brain magnetic resonance imaging (MRI) plays a crucial role in
clinical diagnostics by providing complementary information across different
imaging modalities. However, a common challenge in clinical practice is missing
MRI modalities. In this paper, we apply SwinUNETR to the synthesize of missing
modalities in brain MRI. SwinUNETR is a novel neural network architecture
designed for medical image analysis, integrating the strengths of Swin
Transformer and convolutional neural networks (CNNs). The Swin Transformer, a
variant of the Vision Transformer (ViT), incorporates hierarchical feature
extraction and window-based self-attention mechanisms, enabling it to capture
both local and global contextual information effectively. By combining the Swin
Transformer with CNNs, SwinUNETR merges global context awareness with detailed
spatial resolution. This hybrid approach addresses the challenges posed by the
varying modality characteristics and complex brain structures, facilitating the
generation of accurate and realistic synthetic images. We evaluate the
performance of SwinUNETR on brain MRI datasets and demonstrate its superior
capability in generating clinically valuable images. Our results show
significant improvements in image quality, anatomical consistency, and
diagnostic value.

</details>


### [250] [Grasp2Grasp: Vision-Based Dexterous Grasp Translation via Schrödinger Bridges](https://arxiv.org/abs/2506.02489)
*Tao Zhong,Jonah Buchanan,Christine Allen-Blanchette*

Main category: cs.RO

Relevance: 30.0

TL;DR: 提出了一种基于视觉的灵巧抓取转移方法，通过Schrödinger Bridge形式化，实现不同形态机器人手的抓取意图转移。


<details>
  <summary>Details</summary>
Motivation: 解决不同形态机器人手之间抓取意图的转移问题，避免需要配对演示或手特定模拟。

Method: 采用Schrödinger Bridge形式化，通过分数和流匹配学习源和目标潜在抓取空间的映射，结合物理约束成本函数。

Result: 实验表明，该方法能生成稳定且物理合理的抓取，具有强泛化能力。

Conclusion: 该方法实现了异构操纵器的语义抓取转移，将视觉抓取与概率生成建模结合。

Abstract: We propose a new approach to vision-based dexterous grasp translation, which
aims to transfer grasp intent across robotic hands with differing morphologies.
Given a visual observation of a source hand grasping an object, our goal is to
synthesize a functionally equivalent grasp for a target hand without requiring
paired demonstrations or hand-specific simulations. We frame this problem as a
stochastic transport between grasp distributions using the Schr\"odinger Bridge
formalism. Our method learns to map between source and target latent grasp
spaces via score and flow matching, conditioned on visual observations. To
guide this translation, we introduce physics-informed cost functions that
encode alignment in base pose, contact maps, wrench space, and manipulability.
Experiments across diverse hand-object pairs demonstrate our approach generates
stable, physically grounded grasps with strong generalization. This work
enables semantic grasp transfer for heterogeneous manipulators and bridges
vision-based grasping with probabilistic generative modeling.

</details>


### [251] [Dynamic mapping from static labels: remote sensing dynamic sample generation with temporal-spectral embedding](https://arxiv.org/abs/2506.02574)
*Shuai Yuan,Shuang Chen,Tianwu Lin,Jie Wang,Peng Gong*

Main category: eess.IV

Relevance: 30.0

TL;DR: 本文提出了一种名为TasGen的两阶段自动化框架，用于从静态标签样本生成动态样本，以解决遥感地理制图中样本数据快速过时的问题。


<details>
  <summary>Details</summary>
Motivation: 遥感地理制图需要及时更新的样本数据，但地表动态变化快，导致样本迅速过时，人工更新成本高。

Method: TasGen框架通过两阶段方法，利用时间-光谱嵌入建模时间序列遥感影像中的光谱和时间依赖性，自动生成动态样本。

Result: 该方法能够在不依赖额外人工标注的情况下，捕捉地表变化。

Conclusion: TasGen为遥感地理制图提供了一种高效的动态样本生成方法，减少了人工干预的需求。

Abstract: Accurate remote sensing geographic mapping depends heavily on representative
and timely sample data. However, rapid changes in land surface dynamics
necessitate frequent updates, quickly rendering previously collected samples
obsolete and imposing significant labor demands for continuous manual updates.
In this study, we aim to address this problem by dynamic sample generation
using existing single-date static labeled samples. We introduce TasGen, a
two-stage automated framework to automatically generate dynamic samples,
designed to simultaneously model spectral and temporal dependencies in
time-series remote sensing imagery via temporal-spectral embedding, capturing
land surface changes without additional manual annotations.

</details>


### [252] [A Tree-guided CNN for image super-resolution](https://arxiv.org/abs/2506.02585)
*Chunwei Tian,Mingjian Song,Xiaopeng Fan,Xiangtao Zheng,Bob Zhang,David Zhang*

Main category: eess.IV

Relevance: 30.0

TL;DR: 该论文提出了一种树引导的CNN（TSRNet）用于图像超分辨率，通过树架构增强关键节点的效果，并结合余弦变换技术和Adan优化器提升性能。


<details>
  <summary>Details</summary>
Motivation: 当前深度卷积神经网络在图像超分辨率中难以有效利用关键层信息，导致性能受限。

Method: 设计了树引导的CNN（TSRNet），结合余弦变换提取跨域信息，并使用Adan优化器优化参数。

Result: 实验验证了TSRNet在恢复高质量图像方面的优越性。

Conclusion: TSRNet通过树架构和跨域信息提取显著提升了图像超分辨率性能。

Abstract: Deep convolutional neural networks can extract more accurate structural
information via deep architectures to obtain good performance in image
super-resolution. However, it is not easy to find effect of important layers in
a single network architecture to decrease performance of super-resolution. In
this paper, we design a tree-guided CNN for image super-resolution (TSRNet). It
uses a tree architecture to guide a deep network to enhance effect of key nodes
to amplify the relation of hierarchical information for improving the ability
of recovering images. To prevent insufficiency of the obtained structural
information, cosine transform techniques in the TSRNet are used to extract
cross-domain information to improve the performance of image super-resolution.
Adaptive Nesterov momentum optimizer (Adan) is applied to optimize parameters
to boost effectiveness of training a super-resolution model. Extended
experiments can verify superiority of the proposed TSRNet for restoring
high-quality images. Its code can be obtained at
https://github.com/hellloxiaotian/TSRNet.

</details>


### [253] [Rodrigues Network for Learning Robot Actions](https://arxiv.org/abs/2506.02618)
*Jialiang Zhang,Haoran Geng,Yang You,Congyue Deng,Pieter Abbeel,Jitendra Malik,Leonidas Guibas*

Main category: cs.RO

Relevance: 30.0

TL;DR: 提出了一种名为Neural Rodrigues Operator的新方法，用于在神经网络中注入运动学先验，设计了RodriNet架构，并在多个任务中验证其有效性。


<details>
  <summary>Details</summary>
Motivation: 现有架构（如MLPs和Transformers）缺乏对运动学结构的归纳偏置，限制了其在机器人动作学习中的表现。

Method: 提出Neural Rodrigues Operator作为经典前向运动学运算的可学习扩展，并基于此设计RodriNet架构。

Result: 在运动预测和模仿学习等任务中，RodriNet表现优于标准架构，验证了运动学先验的有效性。

Conclusion: 将结构化运动学先验融入网络架构可显著提升动作学习性能。

Abstract: Understanding and predicting articulated actions is important in robot
learning. However, common architectures such as MLPs and Transformers lack
inductive biases that reflect the underlying kinematic structure of articulated
systems. To this end, we propose the Neural Rodrigues Operator, a learnable
generalization of the classical forward kinematics operation, designed to
inject kinematics-aware inductive bias into neural computation. Building on
this operator, we design the Rodrigues Network (RodriNet), a novel neural
architecture specialized for processing actions. We evaluate the expressivity
of our network on two synthetic tasks on kinematic and motion prediction,
showing significant improvements compared to standard backbones. We further
demonstrate its effectiveness in two realistic applications: (i) imitation
learning on robotic benchmarks with the Diffusion Policy, and (ii) single-image
3D hand reconstruction. Our results suggest that integrating structured
kinematic priors into the network architecture improves action learning in
various domains.

</details>


### [254] [FlexPainter: Flexible and Multi-View Consistent Texture Generation](https://arxiv.org/abs/2506.02620)
*Dongyu Yan,Leyi Wu,Jiantao Lin,Luozhou Wang,Tianshuo Xu,Zhifei Chen,Zhen Yang,Lie Xu,Shunsi Zhang,Yingcong Chen*

Main category: cs.GR

Relevance: 30.0

TL;DR: FlexPainter是一个新颖的纹理生成流程，通过多模态条件引导和视图同步技术，解决了扩散方法在纹理生成中的控制灵活性和一致性问题。


<details>
  <summary>Details</summary>
Motivation: 现有扩散方法在纹理生成中存在控制灵活性不足和多视图图像不一致的问题，影响了生成质量。

Method: 构建共享条件嵌入空间，实现多模态输入；提出图像基础的CFG方法分解结构和风格信息；利用网格表示生成多视图图像，并通过视图同步模块确保局部一致性；最后使用3D感知的纹理补全和增强模型生成高质量纹理。

Result: 实验表明，FlexPainter在灵活性和生成质量上显著优于现有方法。

Conclusion: FlexPainter通过多模态引导和一致性优化，显著提升了纹理生成的灵活性和质量。

Abstract: Texture map production is an important part of 3D modeling and determines the
rendering quality. Recently, diffusion-based methods have opened a new way for
texture generation. However, restricted control flexibility and limited prompt
modalities may prevent creators from producing desired results. Furthermore,
inconsistencies between generated multi-view images often lead to poor texture
generation quality. To address these issues, we introduce \textbf{FlexPainter},
a novel texture generation pipeline that enables flexible multi-modal
conditional guidance and achieves highly consistent texture generation. A
shared conditional embedding space is constructed to perform flexible
aggregation between different input modalities. Utilizing such embedding space,
we present an image-based CFG method to decompose structural and style
information, achieving reference image-based stylization. Leveraging the 3D
knowledge within the image diffusion prior, we first generate multi-view images
simultaneously using a grid representation to enhance global understanding.
Meanwhile, we propose a view synchronization and adaptive weighting module
during diffusion sampling to further ensure local consistency. Finally, a
3D-aware texture completion model combined with a texture enhancement model is
used to generate seamless, high-resolution texture maps. Comprehensive
experiments demonstrate that our framework significantly outperforms
state-of-the-art methods in both flexibility and generation quality.

</details>


### [255] [MotionRAG-Diff: A Retrieval-Augmented Diffusion Framework for Long-Term Music-to-Dance Generation](https://arxiv.org/abs/2506.02661)
*Mingyang Huang,Peng Zhang,Bang Zhang*

Main category: cs.SD

Relevance: 30.0

TL;DR: 提出了一种结合检索增强生成（RAG）和扩散模型的混合框架MotionRAG-Diff，用于生成高质量、音乐连贯的舞蹈动作。


<details>
  <summary>Details</summary>
Motivation: 现有方法（如运动图和扩散模型）在生成长期、连贯且与音乐对齐的舞蹈动作时存在局限性，MotionRAG-Diff旨在解决这些问题。

Method: 1. 跨模态对比学习架构对齐音乐和舞蹈表示；2. 优化的运动图系统检索和拼接动作片段；3. 多条件扩散模型结合音乐信号和对比特征。

Result: 实验表明MotionRAG-Diff在动作质量、多样性和音乐-动作同步准确性上达到最优。

Conclusion: MotionRAG-Diff通过结合检索和扩散模型，为音乐驱动的舞蹈生成建立了新范式。

Abstract: Generating long-term, coherent, and realistic music-conditioned dance
sequences remains a challenging task in human motion synthesis. Existing
approaches exhibit critical limitations: motion graph methods rely on fixed
template libraries, restricting creative generation; diffusion models, while
capable of producing novel motions, often lack temporal coherence and musical
alignment. To address these challenges, we propose $\textbf{MotionRAG-Diff}$, a
hybrid framework that integrates Retrieval-Augmented Generation (RAG) with
diffusion-based refinement to enable high-quality, musically coherent dance
generation for arbitrary long-term music inputs. Our method introduces three
core innovations: (1) A cross-modal contrastive learning architecture that
aligns heterogeneous music and dance representations in a shared latent space,
establishing unsupervised semantic correspondence without paired data; (2) An
optimized motion graph system for efficient retrieval and seamless
concatenation of motion segments, ensuring realism and temporal coherence
across long sequences; (3) A multi-condition diffusion model that jointly
conditions on raw music signals and contrastive features to enhance motion
quality and global synchronization. Extensive experiments demonstrate that
MotionRAG-Diff achieves state-of-the-art performance in motion quality,
diversity, and music-motion synchronization accuracy. This work establishes a
new paradigm for music-driven dance generation by synergizing retrieval-based
template fidelity with diffusion-based creative enhancement.

</details>


### [256] [HumanRAM: Feed-forward Human Reconstruction and Animation Model using Transformers](https://arxiv.org/abs/2506.03118)
*Zhiyuan Yu,Zhe Li,Hujun Bao,Can Yang,Xiaowei Zhou*

Main category: cs.GR

Relevance: 30.0

TL;DR: HumanRAM是一种基于Transformer的通用人类重建和动画方法，通过单目或稀疏图像输入实现高质量重建和动画。


<details>
  <summary>Details</summary>
Motivation: 现有方法依赖密集视图捕获或耗时优化，HumanRAM旨在解决这些限制。

Method: 结合SMPL-X神经纹理和Transformer模型，通过显式姿态条件实现重建与动画的统一框架。

Result: 在真实数据集上显著优于现有方法，重建精度和动画保真度更高。

Conclusion: HumanRAM为单目或稀疏图像输入提供了一种高效、通用的解决方案。

Abstract: 3D human reconstruction and animation are long-standing topics in computer
graphics and vision. However, existing methods typically rely on sophisticated
dense-view capture and/or time-consuming per-subject optimization procedures.
To address these limitations, we propose HumanRAM, a novel feed-forward
approach for generalizable human reconstruction and animation from monocular or
sparse human images. Our approach integrates human reconstruction and animation
into a unified framework by introducing explicit pose conditions, parameterized
by a shared SMPL-X neural texture, into transformer-based large reconstruction
models (LRM). Given monocular or sparse input images with associated camera
parameters and SMPL-X poses, our model employs scalable transformers and a
DPT-based decoder to synthesize realistic human renderings under novel
viewpoints and novel poses. By leveraging the explicit pose conditions, our
model simultaneously enables high-quality human reconstruction and
high-fidelity pose-controlled animation. Experiments show that HumanRAM
significantly surpasses previous methods in terms of reconstruction accuracy,
animation fidelity, and generalization performance on real-world datasets.
Video results are available at https://zju3dv.github.io/humanram/.

</details>


### [257] [CNVSRC 2024: The Second Chinese Continuous Visual Speech Recognition Challenge](https://arxiv.org/abs/2506.02010)
*Zehua Liu,Xiaolou Li,Chen Chen,Lantian Li,Dong Wang*

Main category: cs.CV

Relevance: 20.0

TL;DR: CNVSRC 2024挑战赛在中文连续视觉语音识别领域推出改进，包括更强基线系统和新增数据集，推动技术前沿。


<details>
  <summary>Details</summary>
Motivation: 推动中文大词汇量连续视觉语音识别（LVC-VSR）的研究进展，通过挑战赛形式促进技术创新。

Method: 使用CN-CVS和CNVSRC-Single/Multi数据集，引入CN-CVS2-P1增强数据多样性，改进预处理、特征提取、模型设计和训练策略。

Result: 挑战赛展示了数据预处理、特征提取和模型设计等方面的重要创新，提升了中文LVC-VSR的技术水平。

Conclusion: CNVSRC 2024通过改进和新增数据集，进一步推动了中文LVC-VSR领域的发展。

Abstract: This paper presents the second Chinese Continuous Visual Speech Recognition
Challenge (CNVSRC 2024), which builds on CNVSRC 2023 to advance research in
Chinese Large Vocabulary Continuous Visual Speech Recognition (LVC-VSR). The
challenge evaluates two test scenarios: reading in recording studios and
Internet speech. CNVSRC 2024 uses the same datasets as its predecessor CNVSRC
2023, which involves CN-CVS for training and CNVSRC-Single/Multi for
development and evaluation. However, CNVSRC 2024 introduced two key
improvements: (1) a stronger baseline system, and (2) an additional dataset,
CN-CVS2-P1, for open tracks to improve data volume and diversity. The new
challenge has demonstrated several important innovations in data preprocessing,
feature extraction, model design, and training strategies, further pushing the
state-of-the-art in Chinese LVC-VSR. More details and resources are available
at the official website.

</details>


### [258] [Implicit Deformable Medical Image Registration with Learnable Kernels](https://arxiv.org/abs/2506.02150)
*Stefano Fogarollo,Gregor Laimer,Reto Bale,Matthias Harders*

Main category: cs.CV

Relevance: 20.0

TL;DR: 论文提出了一种新颖的隐式医学图像配准框架，通过稀疏关键点对应关系重建密集位移场，提高了配准的准确性和可靠性。


<details>
  <summary>Details</summary>
Motivation: 医学图像配准在肿瘤治疗等临床应用中至关重要，但现有AI方法常产生不可靠的变形，限制了其临床应用。

Method: 将图像配准重新定义为信号重建问题，学习一个核函数从稀疏关键点恢复密集位移场，并采用分层架构进行粗到细的位移场估计。

Result: 在胸部和腹部零样本配准任务中，方法不仅达到与现有技术相当的准确性，还缩小了隐式与显式配准技术的泛化差距。

Conclusion: 该方法生成的变形更好地保留了解剖关系，性能接近专业商业系统，具有临床应用的潜力。

Abstract: Deformable medical image registration is an essential task in
computer-assisted interventions. This problem is particularly relevant to
oncological treatments, where precise image alignment is necessary for tracking
tumor growth, assessing treatment response, and ensuring accurate delivery of
therapies. Recent AI methods can outperform traditional techniques in accuracy
and speed, yet they often produce unreliable deformations that limit their
clinical adoption. In this work, we address this challenge and introduce a
novel implicit registration framework that can predict accurate and reliable
deformations. Our insight is to reformulate image registration as a signal
reconstruction problem: we learn a kernel function that can recover the dense
displacement field from sparse keypoint correspondences. We integrate our
method in a novel hierarchical architecture, and estimate the displacement
field in a coarse-to-fine manner. Our formulation also allows for efficient
refinement at test time, permitting clinicians to easily adjust registrations
when needed. We validate our method on challenging intra-patient thoracic and
abdominal zero-shot registration tasks, using public and internal datasets from
the local University Hospital. Our method not only shows competitive accuracy
to state-of-the-art approaches, but also bridges the generalization gap between
implicit and explicit registration techniques. In particular, our method
generates deformations that better preserve anatomical relationships and
matches the performance of specialized commercial systems, underscoring its
potential for clinical adoption.

</details>


### [259] [The Devil is in the Darkness: Diffusion-Based Nighttime Dehazing Anchored in Brightness Perception](https://arxiv.org/abs/2506.02395)
*Xiaofeng Cong,Yu-Xin Zhang,Haoran Wei,Yeying Jin,Junming Hou,Jie Gui,Jing Zhang,Dacheng Tao*

Main category: cs.CV

Relevance: 20.0

TL;DR: 论文提出DiffND框架，通过扩散模型和亮度感知网络解决夜间图像去雾中的亮度映射问题。


<details>
  <summary>Details</summary>
Motivation: 现有方法在夜间图像去雾中忽略了亮度映射的真实性，导致合成图像与真实场景不一致。

Method: 1) 数据合成管道模拟严重失真并保持亮度一致性；2) 结合预训练扩散模型和亮度感知网络的恢复模型。

Result: 实验验证了数据集的有效性和模型在去雾与亮度映射上的优越性能。

Conclusion: DiffND框架在夜间图像去雾中实现了更真实的亮度映射和去雾效果。

Abstract: While nighttime image dehazing has been extensively studied, converting
nighttime hazy images to daytime-equivalent brightness remains largely
unaddressed. Existing methods face two critical limitations: (1) datasets
overlook the brightness relationship between day and night, resulting in the
brightness mapping being inconsistent with the real world during image
synthesis; and (2) models do not explicitly incorporate daytime brightness
knowledge, limiting their ability to reconstruct realistic lighting. To address
these challenges, we introduce the Diffusion-Based Nighttime Dehazing (DiffND)
framework, which excels in both data synthesis and lighting reconstruction. Our
approach starts with a data synthesis pipeline that simulates severe
distortions while enforcing brightness consistency between synthetic and
real-world scenes, providing a strong foundation for learning night-to-day
brightness mapping. Next, we propose a restoration model that integrates a
pre-trained diffusion model guided by a brightness perception network. This
design harnesses the diffusion model's generative ability while adapting it to
nighttime dehazing through brightness-aware optimization. Experiments validate
our dataset's utility and the model's superior performance in joint haze
removal and brightness mapping.

</details>


### [260] [Enhancing Monocular Height Estimation via Weak Supervision from Imperfect Labels](https://arxiv.org/abs/2506.02534)
*Sining Chen,Yilei Shi,Xiao Xiang Zhu*

Main category: cs.CV

Relevance: 20.0

TL;DR: 论文提出了一种利用不完美标签训练单目高度估计网络的方法，通过设计平衡软损失和序数约束，提升了模型在噪声标签、领域偏移和长尾分布下的性能。


<details>
  <summary>Details</summary>
Motivation: 解决单目高度估计中高质量标签稀缺的问题，提升模型的泛化能力。

Method: 提出了一种基于集成的管道，结合平衡软损失和序数约束，利用不完美标签进行弱监督训练。

Result: 在DFC23和GBH数据集上，平均均方根误差分别降低了22.94%和18.62%。

Conclusion: 该方法有效利用了不完美标签，提升了模型的泛化性能。

Abstract: Monocular height estimation is considered the most efficient and
cost-effective means of 3D perception in remote sensing, and it has attracted
much attention since the emergence of deep learning. While training neural
networks requires a large amount of data, data with perfect labels are scarce
and only available within developed regions. The trained models therefore lack
generalizability, which limits the potential for large-scale application of
existing methods. We tackle this problem for the first time, by introducing
data with imperfect labels into training pixel-wise height estimation networks,
including labels that are incomplete, inexact, and inaccurate compared to
high-quality labels. We propose an ensemble-based pipeline compatible with any
monocular height estimation network. Taking the challenges of noisy labels,
domain shift, and long-tailed distribution of height values into consideration,
we carefully design the architecture and loss functions to leverage the
information concealed in imperfect labels using weak supervision through
balanced soft losses and ordinal constraints. We conduct extensive experiments
on two datasets with different resolutions, DFC23 (0.5 to 1 m) and GBH (3 m).
The results indicate that the proposed pipeline outperforms baselines by
achieving more balanced performance across various domains, leading to
improvements of average root mean square errors up to 22.94 %, and 18.62 % on
DFC23 and GBH, respectively. The efficacy of each design component is validated
through ablation studies. Code is available at
https://github.com/zhu-xlab/weakim2h.

</details>


### [261] [Synthetic Iris Image Databases and Identity Leakage: Risks and Mitigation Strategies](https://arxiv.org/abs/2506.02626)
*Ada Sawilska,Mateusz Trokielewicz*

Main category: cs.CV

Relevance: 20.0

TL;DR: 该论文综述了虹膜图像合成方法，旨在解决从活体个体收集大规模多样化生物特征数据的问题，并讨论了不同生成方法的潜力、保真度及隐私风险。


<details>
  <summary>Details</summary>
Motivation: 解决生物特征数据收集的隐私和多样性问题，为生物特征方法开发提供替代方案。

Method: 综述了传统图像处理、GAN、VAE和扩散模型等虹膜图像合成方法，并分析了其生成潜力与保真度。

Result: 展示了不同方法的生成效果，并讨论了隐私泄露风险及防范策略。

Conclusion: 虹膜图像合成方法可作为真实生物特征数据集的替代，但需解决隐私风险。

Abstract: This paper presents a comprehensive overview of iris image synthesis methods,
which can alleviate the issues associated with gathering large, diverse
datasets of biometric data from living individuals, which are considered
pivotal for biometric methods development. These methods for synthesizing iris
data range from traditional, hand crafted image processing-based techniques,
through various iterations of GAN-based image generators, variational
autoencoders (VAEs), as well as diffusion models. The potential and fidelity in
iris image generation of each method is discussed and examples of inferred
predictions are provided. Furthermore, the risks of individual biometric
features leakage from the training sets are considered, together with possible
strategies for preventing them, which have to be implemented should these
generative methods be considered a valid replacement of real-world biometric
datasets.

</details>


### [262] [FaceSleuth: Learning-Driven Single-Orientation Attention Verifies Vertical Dominance in Micro-Expression Recognition](https://arxiv.org/abs/2506.02695)
*Linquan Wu,Tianxiang Jiang,Wenhao Duan,Yini Fang,Jacky Keung*

Main category: cs.CV

Relevance: 20.0

TL;DR: FaceSleuth是一种双流架构，通过垂直注意力增强面部微表情识别，结合SOA模块优化方向学习，显著提升性能。


<details>
  <summary>Details</summary>
Motivation: 解决微表情识别中低幅度面部运动增强和身份特异性抑制的挑战。

Method: 双流架构（CVA块增强垂直运动，SOA模块学习最优方向），结合轻量级动作单元嵌入。

Result: 在三个标准MER基准测试中达到SOTA性能（CASME II 95.1%，SAMM 87.1%，MMEW 92.9%）。

Conclusion: 垂直注意力偏置是MER最具区分性的方向，FaceSleuth为微表情识别提供了新方法。

Abstract: Micro-expression recognition (MER) demands models that can amplify
millisecond-level, low-amplitude facial motions while suppressing
identity-specific appearance. We introduce FaceSleuth, a dual-stream
architecture that (1) enhances motion along the empirically dominant vertical
axix through a Continuously Vertical Attention (CVA) block, (2) localises the
resulting signals with a Facial Position Focalizer built on hierarchical
cross-window attention, and (3) steers feature learning toward physiologically
meaningful regions via lightweight Action-Unit embeddings. To examine whether
the hand-chosen vertical axis is indeed optimal, we further propose a
Single-Orientation Attention (SOA) module that learns its own pooling direction
end-to-end. SOA is differentiable, adds only 0.16 % parameters, and collapses
to CVA when the learned angle converges to {\Pi}/2. In practice, SOA reliably
drifts to 88{\deg}, confirming the effectiveness of the vertical prior while
delivering consistent gains. On three standard MER benchmarks, FaceSleuth with
CVA already surpasses previous state-of-the-art methods; plugging in SOA lifts
accuracy and F1 score performance to 95.1 % / 0.918 on CASME II, 87.1 % / 0.840
on SAMM, and 92.9 % / 0.917 on MMEW without sacrificing model compactness.
These results establish a new state of the art and, for the first time, provide
empirical evidence that the vertical attention bias is the most discriminative
orientation for MER.

</details>


### [263] [ToothForge: Automatic Dental Shape Generation using Synchronized Spectral Embeddings](https://arxiv.org/abs/2506.02702)
*Tibor Kubík,François Guibault,Michal Španěl,Hervé Lombaert*

Main category: cs.CV

Relevance: 20.0

TL;DR: ToothForge是一种基于光谱分析的方法，用于自动生成3D牙齿模型，解决了牙科形状数据集的稀疏性问题。通过同步频率嵌入，消除了分解不稳定性带来的偏差，并放宽了对网格结构的限制。


<details>
  <summary>Details</summary>
Motivation: 牙科形状数据集的稀疏性限制了3D牙齿模型的生成，现有方法对网格结构的固定连接性要求较高。

Method: 在光谱域中操作，通过同步频率嵌入对齐数据样本的光谱，消除分解不稳定性，并放宽对网格结构的限制。

Result: 在真实牙冠数据集上，生成的高分辨率牙齿网格质量优于未对齐嵌入的模型。

Conclusion: ToothForge在光谱分析和机器学习交叉领域提供了更灵活的方法，适用于牙科及其他医学形状分析。

Abstract: We introduce ToothForge, a spectral approach for automatically generating
novel 3D teeth, effectively addressing the sparsity of dental shape datasets.
By operating in the spectral domain, our method enables compact machine
learning modeling, allowing the generation of high-resolution tooth meshes in
milliseconds. However, generating shape spectra comes with the instability of
the decomposed harmonics. To address this, we propose modeling the latent
manifold on synchronized frequential embeddings. Spectra of all data samples
are aligned to a common basis prior to the training procedure, effectively
eliminating biases introduced by the decomposition instability. Furthermore,
synchronized modeling removes the limiting factor imposed by previous methods,
which require all shapes to share a common fixed connectivity. Using a private
dataset of real dental crowns, we observe a greater reconstruction quality of
the synthetized shapes, exceeding those of models trained on unaligned
embeddings. We also explore additional applications of spectral analysis in
digital dentistry, such as shape compression and interpolation. ToothForge
facilitates a range of approaches at the intersection of spectral analysis and
machine learning, with fewer restrictions on mesh structure. This makes it
applicable for shape analysis not only in dentistry, but also in broader
medical applications, where guaranteeing consistent connectivity across shapes
from various clinics is unrealistic. The code is available at
https://github.com/tiborkubik/toothForge.

</details>


### [264] [PBR-SR: Mesh PBR Texture Super Resolution from 2D Image Priors](https://arxiv.org/abs/2506.02846)
*Yujin Chen,Yinyu Nie,Benjamin Ummenhofer,Reiner Birkl,Michael Paulitsch,Matthias Nießner*

Main category: cs.CV

Relevance: 20.0

TL;DR: PBR-SR是一种零样本方法，用于从低分辨率PBR输入生成高分辨率、高质量的PBR纹理，利用预训练图像先验，无需额外训练或数据。


<details>
  <summary>Details</summary>
Motivation: 解决基于视图的超分辨率方法中常见的视角不一致和光照敏感性问题，同时保持对低分辨率输入的忠实性。

Method: 利用现成的超分辨率模型，通过迭代最小化超分辨率先验与可微分渲染之间的偏差，并结合多视角渲染的2D先验约束和PBR纹理域的恒等约束。

Result: 生成了高保真的PBR纹理，优于直接应用超分辨率模型和先前的纹理优化方法，支持高级应用如重新光照。

Conclusion: PBR-SR是一种高效且无需额外训练的方法，能够生成高质量的PBR纹理，适用于艺术设计和AI生成的网格。

Abstract: We present PBR-SR, a novel method for physically based rendering (PBR)
texture super resolution (SR). It outputs high-resolution, high-quality PBR
textures from low-resolution (LR) PBR input in a zero-shot manner. PBR-SR
leverages an off-the-shelf super-resolution model trained on natural images,
and iteratively minimizes the deviations between super-resolution priors and
differentiable renderings. These enhancements are then back-projected into the
PBR map space in a differentiable manner to produce refined, high-resolution
textures. To mitigate view inconsistencies and lighting sensitivity, which is
common in view-based super-resolution, our method applies 2D prior constraints
across multi-view renderings, iteratively refining the shared, upscaled
textures. In parallel, we incorporate identity constraints directly in the PBR
texture domain to ensure the upscaled textures remain faithful to the LR input.
PBR-SR operates without any additional training or data requirements, relying
entirely on pretrained image priors. We demonstrate that our approach produces
high-fidelity PBR textures for both artist-designed and AI-generated meshes,
outperforming both direct SR models application and prior texture optimization
methods. Our results show high-quality outputs in both PBR and rendering
evaluations, supporting advanced applications such as relighting.

</details>


### [265] [NTIRE 2025 XGC Quality Assessment Challenge: Methods and Results](https://arxiv.org/abs/2506.02875)
*Xiaohong Liu,Xiongkuo Min,Qiang Hu,Xiaoyun Zhang,Jie Guo,Guangtao Zhai,Shushi Wang,Yingjie Zhou,Lu Liu,Jingxin Li,Liu Yang,Farong Wen,Li Xu,Yanwei Jiang,Xilei Zhu,Chunyi Li,Zicheng Zhang,Huiyu Duan,Xiele Wu,Yixuan Gao,Yuqin Cao,Jun Jia,Wei Sun,Jiezhang Cao,Radu Timofte,Baojun Li,Jiamian Huang,Dan Luo,Tao Liu,Weixia Zhang,Bingkun Zheng,Junlin Chen,Ruikai Zhou,Meiya Chen,Yu Wang,Hao Jiang,Xiantao Li,Yuxiang Jiang,Jun Tang,Yimeng Zhao,Bo Hu,Zelu Qi,Chaoyang Zhang,Fei Zhao,Ping Shi,Lingzhi Fu,Heng Cong,Shuai He,Rongyu Zhang,Jiarong He,Zongyao Hu,Wei Luo,Zihao Yu,Fengbin Guan,Yiting Lu,Xin Li,Zhibo Chen,Mengjing Su,Yi Wang,Tuo Chen,Chunxiao Li,Shuaiyu Zhao,Jiaxin Wen,Chuyi Lin,Sitong Liu,Ningxin Chu,Jing Wan,Yu Zhou,Baoying Chen,Jishen Zeng,Jiarui Liu,Xianjin Liu,Xin Chen,Lanzhi Zhou,Hangyu Li,You Han,Bibo Xiang,Zhenjie Liu,Jianzhang Lu,Jialin Gui,Renjie Lu,Shangfei Wang,Donghao Zhou,Jingyu Lin,Quanjian Song,Jiancheng Huang,Yufeng Yang,Changwei Wang,Shupeng Zhong,Yang Yang,Lihuo He,Jia Liu,Yuting Xing,Tida Fang,Yuchun Jin*

Main category: cs.CV

Relevance: 20.0

TL;DR: 论文报告了NTIRE 2025 XGC质量评估挑战赛，分为用户生成视频、AI生成视频和说话头部三个赛道，各赛道均有显著参与和成果。


<details>
  <summary>Details</summary>
Motivation: 解决视频和说话头部处理领域的主要挑战，推动相关技术的发展。

Method: 通过三个赛道（用户生成视频、AI生成视频、说话头部）组织挑战赛，使用不同数据集（FineVD-GC、Q-Eval-Video、THQA-NTIRE）进行评估。

Result: 各赛道均有大量参与者提交模型，且所有团队的方法均优于基线，推动了相关领域的发展。

Conclusion: 挑战赛成功促进了视频和说话头部处理技术的进步。

Abstract: This paper reports on the NTIRE 2025 XGC Quality Assessment Challenge, which
will be held in conjunction with the New Trends in Image Restoration and
Enhancement Workshop (NTIRE) at CVPR 2025. This challenge is to address a major
challenge in the field of video and talking head processing. The challenge is
divided into three tracks, including user generated video, AI generated video
and talking head. The user-generated video track uses the FineVD-GC, which
contains 6,284 user generated videos. The user-generated video track has a
total of 125 registered participants. A total of 242 submissions are received
in the development phase, and 136 submissions are received in the test phase.
Finally, 5 participating teams submitted their models and fact sheets. The AI
generated video track uses the Q-Eval-Video, which contains 34,029 AI-Generated
Videos (AIGVs) generated by 11 popular Text-to-Video (T2V) models. A total of
133 participants have registered in this track. A total of 396 submissions are
received in the development phase, and 226 submissions are received in the test
phase. Finally, 6 participating teams submitted their models and fact sheets.
The talking head track uses the THQA-NTIRE, which contains 12,247 2D and 3D
talking heads. A total of 89 participants have registered in this track. A
total of 225 submissions are received in the development phase, and 118
submissions are received in the test phase. Finally, 8 participating teams
submitted their models and fact sheets. Each participating team in every track
has proposed a method that outperforms the baseline, which has contributed to
the development of fields in three tracks.

</details>


### [266] [MIND: Material Interface Generation from UDFs for Non-Manifold Surface Reconstruction](https://arxiv.org/abs/2506.02938)
*Xuhui Chen,Fei Hou,Wencheng Wang,Hong Qin,Ying He*

Main category: cs.CV

Relevance: 20.0

TL;DR: 论文提出了一种名为MIND的新算法，用于直接从无符号距离场（UDFs）生成材料界面，支持非流形网格提取。


<details>
  <summary>Details</summary>
Motivation: 现有方法在从UDFs提取网格时存在拓扑缺陷和非流形几何表示不足的问题。

Method: 通过从UDFs中推导空间分区，构建多标签全局场，结合多标签Marching Cubes算法提取非流形网格。

Result: 实验表明，MIND能鲁棒处理复杂非流形表面，显著优于现有方法。

Conclusion: MIND为UDFs的非流形网格提取提供了有效解决方案。

Abstract: Unsigned distance fields (UDFs) are widely used in 3D deep learning due to
their ability to represent shapes with arbitrary topology. While prior work has
largely focused on learning UDFs from point clouds or multi-view images,
extracting meshes from UDFs remains challenging, as the learned fields rarely
attain exact zero distances. A common workaround is to reconstruct signed
distance fields (SDFs) locally from UDFs to enable surface extraction via
Marching Cubes. However, this often introduces topological artifacts such as
holes or spurious components. Moreover, local SDFs are inherently incapable of
representing non-manifold geometry, leading to complete failure in such cases.
To address this gap, we propose MIND (Material Interface from Non-manifold
Distance fields), a novel algorithm for generating material interfaces directly
from UDFs, enabling non-manifold mesh extraction from a global perspective. The
core of our method lies in deriving a meaningful spatial partitioning from the
UDF, where the target surface emerges as the interface between distinct
regions. We begin by computing a two-signed local field to distinguish the two
sides of manifold patches, and then extend this to a multi-labeled global field
capable of separating all sides of a non-manifold structure. By combining this
multi-labeled field with the input UDF, we construct material interfaces that
support non-manifold mesh extraction via a multi-labeled Marching Cubes
algorithm. Extensive experiments on UDFs generated from diverse data sources,
including point cloud reconstruction, multi-view reconstruction, and medial
axis transforms, demonstrate that our approach robustly handles complex
non-manifold surfaces and significantly outperforms existing methods.

</details>


### [267] [Deep Learning for Retinal Degeneration Assessment: A Comprehensive Analysis of the MARIO AMD Progression Challenge](https://arxiv.org/abs/2506.02976)
*Rachid Zeghlache,Ikram Brahim,Pierre-Henri Conze,Mathieu Lamard,Mohammed El Amine Lazouni,Zineb Aziza Elaouaber,Leila Ryma Lazouni,Christopher Nielsen,Ahmad O. Ahsan,Matthias Wilms,Nils D. Forkert,Lovre Antonio Budimir,Ivana Matovinović,Donik Vršnak,Sven Lončarić,Philippe Zhang,Weili Jiang,Yihao Li,Yiding Hao,Markus Frohmann,Patrick Binder,Marcel Huber,Taha Emre,Teresa Finisterra Araújo,Marzieh Oghbaie,Hrvoje Bogunović,Amerens A. Bekkers,Nina M. van Liebergen,Hugo J. Kuijf,Abdul Qayyum,Moona Mazher,Steven A. Niederer,Alberto J. Beltrán-Carrero,Juan J. Gómez-Valverde,Javier Torresano-Rodríquez,Álvaro Caballero-Sastre,María J. Ledesma Carbayo,Yosuke Yamagishi,Yi Ding,Robin Peretzke,Alexandra Ertl,Maximilian Fischer,Jessica Kächele,Sofiane Zehar,Karim Boukli Hacene,Thomas Monfort,Béatrice Cochener,Mostafa El Habib Daho,Anas-Alexis Benyoussef,Gwenolé Quellec*

Main category: cs.CV

Relevance: 20.0

TL;DR: MARIO挑战赛聚焦于通过OCT图像自动检测和监测年龄相关性黄斑变性（AMD），评估AI在AMD进展分类和预测中的表现。


<details>
  <summary>Details</summary>
Motivation: 推动AMD的自动化检测和监测技术，评估AI在医疗影像分析中的能力。

Method: 使用多模态数据集（来自法国和阿尔及利亚），设计分类和预测任务，35个团队参与。

Result: AI在AMD进展分类任务中表现与医生相当，但在预测未来进展任务中表现不佳。

Conclusion: AI在AMD监测中有潜力，但预测未来进展仍需改进。

Abstract: The MARIO challenge, held at MICCAI 2024, focused on advancing the automated
detection and monitoring of age-related macular degeneration (AMD) through the
analysis of optical coherence tomography (OCT) images. Designed to evaluate
algorithmic performance in detecting neovascular activity changes within AMD,
the challenge incorporated unique multi-modal datasets. The primary dataset,
sourced from Brest, France, was used by participating teams to train and test
their models. The final ranking was determined based on performance on this
dataset. An auxiliary dataset from Algeria was used post-challenge to evaluate
population and device shifts from submitted solutions. Two tasks were involved
in the MARIO challenge. The first one was the classification of evolution
between two consecutive 2D OCT B-scans. The second one was the prediction of
future AMD evolution over three months for patients undergoing anti-vascular
endothelial growth factor (VEGF) therapy. Thirty-five teams participated, with
the top 12 finalists presenting their methods. This paper outlines the
challenge's structure, tasks, data characteristics, and winning methodologies,
setting a benchmark for AMD monitoring using OCT, infrared imaging, and
clinical data (such as the number of visits, age, gender, etc.). The results of
this challenge indicate that artificial intelligence (AI) performs as well as a
physician in measuring AMD progression (Task 1) but is not yet able of
predicting future evolution (Task 2).

</details>


### [268] [Self-Supervised Spatial Correspondence Across Modalities](https://arxiv.org/abs/2506.03148)
*Ayush Shrivastava,Andrew Owens*

Main category: cs.CV

Relevance: 20.0

TL;DR: 提出了一种跨模态时空对应方法，无需对齐数据即可训练，适用于几何和语义匹配任务。


<details>
  <summary>Details</summary>
Motivation: 解决不同视觉模态（如RGB与深度图）之间的像素级对应问题，无需显式假设或对齐数据。

Method: 扩展对比随机游走框架，学习跨模态和模态内匹配的循环一致特征表示。

Result: 在几何和语义匹配任务中表现优异，如RGB-深度、RGB-热成像、照片-素描等。

Conclusion: 方法简单有效，适用于无标注数据，且在多种跨模态任务中表现良好。

Abstract: We present a method for finding cross-modal space-time correspondences. Given
two images from different visual modalities, such as an RGB image and a depth
map, our model identifies which pairs of pixels correspond to the same physical
points in the scene. To solve this problem, we extend the contrastive random
walk framework to simultaneously learn cycle-consistent feature representations
for both cross-modal and intra-modal matching. The resulting model is simple
and has no explicit photo-consistency assumptions. It can be trained entirely
using unlabeled data, without the need for any spatially aligned multimodal
image pairs. We evaluate our method on both geometric and semantic
correspondence tasks. For geometric matching, we consider challenging tasks
such as RGB-to-depth and RGB-to-thermal matching (and vice versa); for semantic
matching, we evaluate on photo-sketch and cross-style image alignment. Our
method achieves strong performance across all benchmarks.

</details>


### [269] [Alzheimers Disease Classification in Functional MRI With 4D Joint Temporal-Spatial Kernels in Novel 4D CNN Model](https://arxiv.org/abs/2506.02060)
*Javier Salazar Cavazos,Scott Peltier*

Main category: eess.IV

Relevance: 20.0

TL;DR: 该论文提出了一种新型4D卷积网络，用于从功能磁共振成像（fMRI）数据中提取时空联合特征，优于传统的3D模型。


<details>
  <summary>Details</summary>
Motivation: 现有方法仅使用3D空间模型处理4D fMRI数据，可能导致特征提取不足，影响下游任务（如分类）的性能。

Method: 开发了一种4D卷积网络，用于提取时空联合核，同时学习空间信息和时间动态。

Result: 实验结果表明，4D CNN模型在捕获fMRI时空数据方面优于3D模型，并提升了阿尔茨海默病的诊断效果。

Conclusion: 未来研究可探索任务型fMRI应用和回归任务，以进一步理解认知表现和疾病进展。

Abstract: Previous works in the literature apply 3D spatial-only models on 4D
functional MRI data leading to possible sub-par feature extraction to be used
for downstream tasks like classification. In this work, we aim to develop a
novel 4D convolution network to extract 4D joint temporal-spatial kernels that
not only learn spatial information but in addition also capture temporal
dynamics. Experimental results show promising performance in capturing
spatial-temporal data in functional MRI compared to 3D models. The 4D CNN model
improves Alzheimers disease diagnosis for rs-fMRI data, enabling earlier
detection and better interventions. Future research could explore task-based
fMRI applications and regression tasks, enhancing understanding of cognitive
performance and disease progression.

</details>


### [270] [Dual encoding feature filtering generalized attention UNET for retinal vessel segmentation](https://arxiv.org/abs/2506.02312)
*Md Tauhidul Islam,Wu Da-Wen,Tang Qing-Qing,Zhao Kai-Yang,Yin Teng,Li Yan-Fei,Shang Wen-Yi,Liu Jing-Yu,Zhang Hai-Xian*

Main category: eess.IV

Relevance: 20.0

TL;DR: 论文提出了一种名为DEFFA-Unet的改进U-Net架构，用于视网膜血管分割，通过额外编码器、特征过滤融合模块和数据增强方法提升性能和泛化能力。


<details>
  <summary>Details</summary>
Motivation: 解决视网膜血管分割中训练数据不足、分布不平衡和特征提取不充分的问题，以提高分割性能和模型泛化能力。

Method: 引入额外编码器处理域不变输入，开发特征过滤融合模块，替换传统跳跃连接为注意力引导的特征重建融合模块，并提出数据增强和平衡方法。

Result: 在多个基准数据集上验证，DEFFA-Unet显著优于基线方法和现有先进模型，尤其在跨验证模型泛化方面表现突出。

Conclusion: DEFFA-Unet通过创新架构设计和数据增强方法，有效提升了视网膜血管分割的性能和泛化能力。

Abstract: Retinal blood vessel segmentation is crucial for diagnosing ocular and
cardiovascular diseases. Although the introduction of U-Net in 2015 by Olaf
Ronneberger significantly advanced this field, yet issues like limited training
data, imbalance data distribution, and inadequate feature extraction persist,
hindering both the segmentation performance and optimal model generalization.
Addressing these critical issues, the DEFFA-Unet is proposed featuring an
additional encoder to process domain-invariant pre-processed inputs, thereby
improving both richer feature encoding and enhanced model generalization. A
feature filtering fusion module is developed to ensure the precise feature
filtering and robust hybrid feature fusion. In response to the task-specific
need for higher precision where false positives are very costly, traditional
skip connections are replaced with the attention-guided feature reconstructing
fusion module. Additionally, innovative data augmentation and balancing methods
are proposed to counter data scarcity and distribution imbalance, further
boosting the robustness and generalization of the model. With a comprehensive
suite of evaluation metrics, extensive validations on four benchmark datasets
(DRIVE, CHASEDB1, STARE, and HRF) and an SLO dataset (IOSTAR), demonstrate the
proposed method's superiority over both baseline and state-of-the-art models.
Particularly the proposed method significantly outperforms the compared methods
in cross-validation model generalization.

</details>


### [271] [Simulate Any Radar: Attribute-Controllable Radar Simulation via Waveform Parameter Embedding](https://arxiv.org/abs/2506.03134)
*Weiqing Xiao,Hao Huang,Chonghao Zhong,Yujie Lin,Nan Wang,Xiaoxue Chen,Zhaoxi Chen,Saining Zhang,Shuocheng Yang,Pierre Merriaux,Lei Lei,Hao Zhao*

Main category: eess.SP

Relevance: 20.0

TL;DR: SA-Radar是一种雷达模拟方法，通过波形参数化的属性嵌入生成可控且高效的雷达数据，支持自定义雷达属性。


<details>
  <summary>Details</summary>
Motivation: 现有雷达模拟器要么基于生成模型，要么基于物理模型，但缺乏两者的结合。SA-Radar旨在通过整合两种范式，提供更灵活和高效的雷达数据生成方案。

Method: 设计了ICFAR-Net，一种基于3D U-Net的模型，通过波形参数编码雷达属性，捕捉不同雷达配置引起的信号变化。

Result: SA-Radar生成的模拟数据在多个下游任务（如2D/3D目标检测和雷达语义分割）中表现出色，显著提升了模型性能。

Conclusion: SA-Radar作为一种通用雷达数据引擎，在自动驾驶应用中具有潜力。

Abstract: We present SA-Radar (Simulate Any Radar), a radar simulation approach that
enables controllable and efficient generation of radar cubes conditioned on
customizable radar attributes. Unlike prior generative or physics-based
simulators, SA-Radar integrates both paradigms through a waveform-parameterized
attribute embedding. We design ICFAR-Net, a 3D U-Net conditioned on radar
attributes encoded via waveform parameters, which captures signal variations
induced by different radar configurations. This formulation bypasses the need
for detailed radar hardware specifications and allows efficient simulation of
range-azimuth-Doppler (RAD) tensors across diverse sensor settings. We further
construct a mixed real-simulated dataset with attribute annotations to robustly
train the network. Extensive evaluations on multiple downstream tasks-including
2D/3D object detection and radar semantic segmentation-demonstrate that
SA-Radar's simulated data is both realistic and effective, consistently
improving model performance when used standalone or in combination with real
data. Our framework also supports simulation in novel sensor viewpoints and
edited scenes, showcasing its potential as a general-purpose radar data engine
for autonomous driving applications. Code and additional materials are
available at https://zhuxing0.github.io/projects/SA-Radar.

</details>


### [272] [Automated Measurement of Optic Nerve Sheath Diameter Using Ocular Ultrasound Video](https://arxiv.org/abs/2506.02789)
*Renxing Li,Weiyi Tang,Peiqi Li,Qiming Huang,Jiayuan She,Shengkai Li,Haoran Xu,Yeyun Wan,Jing Liu,Hailong Fu,Xiang Li,Jiangang Chen*

Main category: cs.CV

Relevance: 10.0

TL;DR: 论文提出了一种自动识别超声视频序列中最佳帧以测量视神经鞘直径（ONSD）的方法，结合KCF跟踪算法和SLIC分割算法，结果与专家测量高度一致。


<details>
  <summary>Details</summary>
Motivation: ONSD与颅内压（ICP）线性相关，但手动测量依赖操作者经验，亟需自动化方法提高准确性和效率。

Method: 使用KCF跟踪算法和SLIC分割算法自动识别最佳帧，结合GMM和KL散度方法测量ONSD。

Result: 与专家测量相比，平均误差为0.04，均方差为0.054，组内相关系数为0.782。

Conclusion: 该方法实现了高精度的自动化ONSD测量，具有临床应用潜力。

Abstract: Objective. Elevated intracranial pressure (ICP) is recognized as a biomarker
of secondary brain injury, with a significant linear correlation observed
between optic nerve sheath diameter (ONSD) and ICP. Frequent monitoring of ONSD
could effectively support dynamic evaluation of ICP. However, ONSD measurement
is heavily reliant on the operator's experience and skill, particularly in
manually selecting the optimal frame from ultrasound sequences and measuring
ONSD. Approach. This paper presents a novel method to automatically identify
the optimal frame from video sequences for ONSD measurement by employing the
Kernel Correlation Filter (KCF) tracking algorithm and Simple Linear Iterative
Clustering (SLIC) segmentation algorithm. The optic nerve sheath is mapped and
measured using a Gaussian Mixture Model (GMM) combined with a
KL-divergence-based method. Results. When compared with the average
measurements of two expert clinicians, the proposed method achieved a mean
error, mean squared deviation, and intraclass correlation coefficient (ICC) of
0.04, 0.054, and 0.782, respectively. Significance. The findings suggest that
this method provides highly accurate automated ONSD measurements, showing
potential for clinical application.

</details>


### [273] [NTIRE 2025 Challenge on RAW Image Restoration and Super-Resolution](https://arxiv.org/abs/2506.02197)
*Marcos V. Conde,Radu Timofte,Zihao Lu,Xiangyu Kongand Xiaoxia Xingand Fan Wangand Suejin Hanand MinKyu Parkand Tianyu Zhangand Xin Luoand Yeda Chenand Dong Liuand Li Pangand Yuhang Yangand Hongzhong Wangand Xiangyong Caoand Ruixuan Jiangand Senyan Xuand Siyuan Jiangand Xueyang Fuand Zheng-Jun Zhaand Tianyu Haoand Yuhong Heand Ruoqi Liand Yueqi Yangand Xiang Yuand Guanlan Hongand Minmin Yiand Yuanjia Chenand Liwen Zhangand Zijie Jinand Cheng Liand Lian Liuand Wei Songand Heng Sunand Yubo Wangand Jinghua Wangand Jiajie Luand Watchara Ruangsangand*

Main category: eess.IV

Relevance: 10.0

TL;DR: 本文回顾了NTIRE 2025 RAW图像恢复和超分辨率挑战赛，总结了提出的解决方案和结果。


<details>
  <summary>Details</summary>
Motivation: 探讨RAW图像恢复和超分辨率在现代图像信号处理（ISP）中的重要性，填补RGB领域之外的研究空白。

Method: 通过挑战赛形式，参与者提交解决方案，包括RAW图像去噪、去模糊和2倍超分辨率。

Result: 230名参与者注册，45名提交结果，展示了RAW图像恢复的最新进展。

Conclusion: 挑战赛推动了RAW图像恢复领域的研究，但仍需进一步探索。

Abstract: This paper reviews the NTIRE 2025 RAW Image Restoration and Super-Resolution
Challenge, highlighting the proposed solutions and results. New methods for RAW
Restoration and Super-Resolution could be essential in modern Image Signal
Processing (ISP) pipelines, however, this problem is not as explored as in the
RGB domain. The goal of this challenge is two fold, (i) restore RAW images with
blur and noise degradations, (ii) upscale RAW Bayer images by 2x, considering
unknown noise and blur. In the challenge, a total of 230 participants
registered, and 45 submitted results during thee challenge period. This report
presents the current state-of-the-art in RAW Restoration.

</details>


### [274] [VolTex: Food Volume Estimation using Text-Guided Segmentation and Neural Surface Reconstruction](https://arxiv.org/abs/2506.02895)
*Ahmad AlMughrabi,Umair Haroon,Ricardo Marques,Petia Radeva*

Main category: cs.GR

Relevance: 10.0

TL;DR: VolTex是一个通过文本输入精确选择食物对象并利用神经表面重建方法计算体积的框架，提高了食物体积估计的准确性。


<details>
  <summary>Details</summary>
Motivation: 现有3D食物体积估计方法缺乏对食物部分的选择能力，VolTex旨在解决这一问题。

Method: 用户通过文本输入指定目标食物对象，利用神经表面重建方法生成高保真3D网格以计算体积。

Result: 在MetaFood3D数据集上的评估表明，VolTex能有效隔离和重建食物对象以实现精确体积估计。

Conclusion: VolTex通过文本输入和神经表面重建方法显著提升了食物体积估计的准确性。

Abstract: Accurate food volume estimation is crucial for dietary monitoring, medical
nutrition management, and food intake analysis. Existing 3D Food Volume
estimation methods accurately compute the food volume but lack for food
portions selection. We present VolTex, a framework that improves \change{the
food object selection} in food volume estimation. Allowing users to specify a
target food item via text input to be segmented, our method enables the precise
selection of specific food objects in real-world scenes. The segmented object
is then reconstructed using the Neural Surface Reconstruction method to
generate high-fidelity 3D meshes for volume computation. Extensive evaluations
on the MetaFood3D dataset demonstrate the effectiveness of our approach in
isolating and reconstructing food items for accurate volume estimation. The
source code is accessible at https://github.com/GCVCG/VolTex.

</details>


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [275] [The Unified Cognitive Consciousness Theory for Language Models: Anchoring Semantics, Thresholds of Activation, and Emergent Reasoning](https://arxiv.org/abs/2506.02139)
*Edward Y. Chang*

Main category: cs.AI

Relevance: 90.0

TL;DR: 论文提出统一认知意识理论（UCCT），将大语言模型（LLM）视为无意识的潜在模式库，通过语义锚定（如提示、角色和交互）实现任务相关推理，为提示、微调、检索和多智能体协调提供统一解释。


<details>
  <summary>Details</summary>
Motivation: 解决LLM在少样本学习中的矛盾现象：某些任务能从少量示例泛化，而其他任务需要大量监督。

Method: 提出UCCT理论，将LLM视为无意识底层模式库，通过语义锚定（如提示、角色和交互）绑定潜在结构与任务意义。

Result: 提出阈值跨越动态定理，形式化语义锚定为概率相变，并主张AGI需通过整合LLM而非抛弃来实现。

Conclusion: LLM是通用智能的必要基础组件，未来AGI需通过对其对齐和整合实现。

Abstract: Few-shot learning in large language models (LLMs) reveals a deep paradox:
Some tasks generalize from minimal examples, while others require extensive
supervision. We address this through the Unified Cognitive Consciousness Theory
(UCCT), which reframes LLMs not as incomplete agents, but as unconscious
substrates, repositories of latent linguistic and conceptual patterns that
operate without explicit semantics or goal-directed reasoning. In this view,
LLMs are not broken approximations of cognition, but necessary and foundational
components of general intelligence. Semantic anchoring, through prompts, roles,
and interaction, acts as a conscious control layer, binding latent structure to
task-relevant meaning and enabling coherent reasoning. UCCT offers a unifying
account of prompting, fine-tuning, retrieval, and multi-agent coordination, all
grounded in probabilistic alignment between unconscious representation and
external control. To support this model, we present the Threshold-Crossing
Dynamics Theorem, which formalizes semantic anchoring as a probabilistic phase
transition. But the central claim remains architectural: AGI will not emerge by
discarding LLMs, but by aligning and integrating them into systems that reason,
regulate, and adapt together.

</details>


### [276] [Act Only When It Pays: Efficient Reinforcement Learning for LLM Reasoning via Selective Rollouts](https://arxiv.org/abs/2506.02177)
*Haizhong Zheng,Yang Zhou,Brian R. Bartoldson,Bhavya Kailkhura,Fan Lai,Jiawei Zhao,Beidi Chen*

Main category: cs.AI

Relevance: 85.0

TL;DR: 论文提出GRESO算法，通过跳过无信息提示优化RL训练效率，显著减少计算开销，同时保持模型性能。


<details>
  <summary>Details</summary>
Motivation: RL训练中大量计算开销源于无信息提示的滚动，若能跳过这些提示可显著提升效率。

Method: 提出GRESO算法，基于奖励动态预测并跳过无信息提示，优化GRPO训练过程。

Result: 在多个数学推理基准测试中，GRESO实现最高2.4倍滚动时间加速和2.0倍总训练时间加速，且无精度损失。

Conclusion: GRESO通过选择性滚动有效提升RL训练效率，适用于大规模LLM训练。

Abstract: Reinforcement learning, such as PPO and GRPO, has powered recent
breakthroughs in LLM reasoning. Scaling rollout to sample more prompts enables
models to selectively use higher-quality data for training, which can stabilize
RL training and improve model performance. However, this comes at the cost of
significant computational overhead. In this paper, we show that a substantial
portion of this overhead can be avoided by skipping uninformative prompts
before rollout. Our analysis of reward dynamics reveals a strong temporal
consistency in prompt value: prompts that are uninformative in one epoch of
training are likely to remain uninformative in future epochs. Based on these
insights, we propose GRESO (GRPO with Efficient Selective Rollout), an online,
lightweight pre-rollout filtering algorithm that predicts and skips
uninformative prompts using reward training dynamics. By evaluating GRESO on a
broad range of math reasoning benchmarks and models, such as Qwen2.5-Math-1.5B,
DeepSeek-R1-Distill-Qwen-1.5B, and Qwen2.5-Math-7B, we show that GRESO achieves
up to 2.4x wall-clock time speedup in rollout and up to 2.0x speedup in total
training time without accuracy degradation.

</details>


### [277] [Improving LLM-Generated Code Quality with GRPO](https://arxiv.org/abs/2506.02211)
*Maxime Robeyns,Laurence Aitchison*

Main category: cs.AI

Relevance: 85.0

TL;DR: 论文提出了一种基于代码质量的多维度奖励信号（GRPO），用于改进LLM在代码生成中的表现，弥补了传统单元测试通过率作为奖励信号的不足。


<details>
  <summary>Details</summary>
Motivation: 当前LLM代码生成的奖励信号（如单元测试通过率）未能全面捕捉代码的可维护性、质量和安全性，因此需要更全面的评估方法。

Method: 开发了一个综合库来量化代码质量的多个方面，并将其作为GRPO的奖励信号。

Result: GRPO显著提高了生成的代码质量，并通过专家盲评验证了其有效性。

Conclusion: GRPO为代码生成任务提供了更全面的奖励信号，有助于提升代码质量。

Abstract: Large Language Models (LLMs) are gaining widespread use for code generation.
Recent training procedures use execution feedback as a reward signal, typically
focusing on the functional correctness of the code, using unit test pass rate
as a reward signal. However, this reward signal fails to capture notions of
maintainability, quality and safety of the code produced. We address this
under-explored area and develop a comprehensive library to quantify various
aspects of code quality, and use it as a reward in GRPO. We find GRPO increases
code quality according to this measure, which is confirmed by expert, blinded
human annotators.

</details>


### [278] [ResearchCodeBench: Benchmarking LLMs on Implementing Novel Machine Learning Research Code](https://arxiv.org/abs/2506.02314)
*Tianyu Hua,Harper Hua,Violet Xiang,Benjamin Klieger,Sang T. Truong,Weixin Liang,Fan-Yun Sun,Nick Haber*

Main category: cs.AI

Relevance: 85.0

TL;DR: ResearchCodeBench是一个包含212个编码挑战的基准测试，用于评估LLMs将前沿ML研究转化为可执行代码的能力，发现最佳模型成功率不足40%。


<details>
  <summary>Details</summary>
Motivation: 评估LLMs在未见过的研究论文中实现新想法的能力，填补现有基准的空白。

Method: 引入ResearchCodeBench基准，测试30+个专有和开源LLMs的代码生成能力。

Result: 最佳模型Gemini-2.5-Pro-Preview的成功率为37.3%，其他模型表现更差。

Conclusion: ResearchCodeBench为LLM在代码生成领域的持续改进提供了评估平台。

Abstract: Large language models (LLMs) have shown promise in transforming machine
learning research, yet their capability to faithfully implement novel ideas
from recent research papers-ideas unseen during pretraining-remains unclear. We
introduce ResearchCodeBench, a benchmark of 212 coding challenges that
evaluates LLMs' ability to translate cutting-edge ML contributions from top
2024-2025 research papers into executable code. We assessed 30+ proprietary and
open-source LLMs, finding that even the best models correctly implement less
than 40% of the code. We find Gemini-2.5-Pro-Preview to perform best at 37.3%
success rate, with O3 (High) and O4-mini (High) following behind at 32.3% and
30.8% respectively. We present empirical findings on performance comparison,
contamination, and error patterns. By providing a rigorous and community-driven
evaluation platform, ResearchCodeBench enables continuous understanding and
advancement of LLM-driven innovation in research code generation.

</details>


### [279] [OThink-R1: Intrinsic Fast/Slow Thinking Mode Switching for Over-Reasoning Mitigation](https://arxiv.org/abs/2506.02397)
*Shengjia Zhang,Junjie Wu,Jiawei Chen,Changwang Zhang,Xingyu Lou,Wangchunshu Zhou,Sheng Zhou,Can Wang,Jun Wang*

Main category: cs.AI

Relevance: 85.0

TL;DR: 论文提出OThink-R1方法，通过动态切换快速思考（非推理模式）和慢速思考（推理模式）来减少大型推理模型中的冗余推理步骤，平均减少23%的冗余且不影响准确性。


<details>
  <summary>Details</summary>
Motivation: 尽管大型推理模型（LRMs）在复杂任务中表现出色，但研究发现许多简单任务可以通过非推理的LLMs以更少的token解决，表明复杂推理并非总是必要。

Method: 通过系统分析LRMs的推理轨迹，利用LLM-Judge分类为冗余推理或必要推理，并引入OThink-R1方法动态切换推理模式。

Result: 实验表明，OThink-R1在数学和问答任务中平均减少23%的冗余推理，同时保持准确性。

Conclusion: OThink-R1为高效推理模型提供了实用指南，优化了推理效率。

Abstract: Recent advanced large reasoning models (LRMs) leverage extended
chain-of-thought (CoT) reasoning to solve complex tasks, achieving
state-of-the-art performance. Despite their success, we identify a critical
issue: a substantial portion of simple tasks solved by LRMs can also be
addressed by non-reasoning LLMs using significantly fewer tokens, indicating
the complex reasoning may not always be necessary. To address this, we
systematically analyze the reasoning trajectories of LRMs and present a method
utilizing identified paradigms and LLM-Judge to classify these trajectories as
either Redundant Reasoning or Essential Reasoning. And we introduce OThink-R1,
a method that prunes redundant reasoning steps while preserving logical
validity. OThink-R1 dynamically employs the non-thinking mode (fast-thinking)
for straightforward problems while engaging in deliberate thinking
(slow-thinking) for complex problems. Experiments across mathematical and
question-answering tasks demonstrate that OThink-R1 reduces reasoning
redundancy by almost 23\% on average without compromising accuracy, offering
practical guidelines for efficient reasoning models. The code is available at
https://github.com/AgenticIR-Lab/OThink-R1.

</details>


### [280] [Think Twice, Act Once: A Co-Evolution Framework of LLM and RL for Large-Scale Decision Making](https://arxiv.org/abs/2506.02522)
*Xu Wan,Wenyue Xu,Chao Yang,Mingyang Sun*

Main category: cs.AI

Relevance: 85.0

TL;DR: ACE框架结合LLMs和RL，通过双角色轨迹优化机制提升大规模决策任务的性能。


<details>
  <summary>Details</summary>
Motivation: 解决LLMs在实时长序列决策中的不足和RL在大动作空间中的样本效率问题。

Method: ACE框架中，LLMs同时作为策略执行者和价值评估者，RL代理通过高质量数据集增强LLMs的决策能力。

Result: 在动作空间超过60K的电网操作任务中，ACE表现优于现有RL和LLM方法。

Conclusion: ACE为LLMs和RL的协同应用提供了有效框架，适用于大规模决策场景。

Abstract: Recent advancements in Large Language Models (LLMs) and Reinforcement
Learning (RL) have shown significant promise in decision-making tasks.
Nevertheless, for large-scale industrial decision problems, both approaches
face distinct challenges: LLMs lack real-time long-sequence decision-making
capabilities, while RL struggles with sample efficiency in vast action spaces.
To bridge this gap, we propose Agents Co-Evolution (ACE), a synergistic
framework between LLMs and RL agents for large-scale decision-making scenarios.
ACE introduces a dual-role trajectory refinement mechanism where LLMs act as
both Policy Actor and Value Critic during RL's training: the Actor refines
suboptimal actions via multi-step reasoning and environment validation, while
the Critic performs temporal credit assignment through trajectory-level reward
shaping. Concurrently, RL agent enhances LLMs' task-specific decision-making
with high-quality fine-tuning datasets generated via prioritized experience
replay. Through extensive experiments across multiple power grid operation
challenges with action spaces exceeding 60K discrete actions, ACE demonstrates
superior performance over existing RL methods and LLM-based methods.

</details>


### [281] [Truly Assessing Fluid Intelligence of Large Language Models through Dynamic Reasoning Evaluation](https://arxiv.org/abs/2506.02648)
*Yue Yang,MingKang Chen,Qihua Liu,Mengkang Hu,Qiguang Chen,Gengrui Zhang,Shuyue Hu,Guangtao Zhai,Yu Qiao,Yu Wang,Wenqi Shao,Ping Luo*

Main category: cs.AI

Relevance: 85.0

TL;DR: DRE-Bench是一个动态推理评估基准，用于测试LLMs的流体智能（抽象推理能力），发现现有模型在高级认知任务中表现不足。


<details>
  <summary>Details</summary>
Motivation: 现有推理基准要么局限于领域知识，要么缺乏可解释性，因此需要一种新的评估方法来测试LLMs的流体智能。

Method: 提出DRE-Bench，包含36个抽象推理任务，分为四个认知层次，每个任务有多个动态变体以测试潜在规则。

Result: 实验表明，大多数LLMs在低级认知任务中表现良好，但在高级认知任务中表现不佳且泛化能力有限。

Conclusion: 当前LLMs与人类流体智能存在差距，DRE-Bench为系统跟踪LLMs推理进展提供了新路径。

Abstract: Recent advances in large language models (LLMs) have demonstrated impressive
reasoning capacities that mirror human-like thinking. However, whether LLMs
possess genuine fluid intelligence (i.e., the ability to reason abstractly and
generalize rules in novel situations) remains an open question. Existing
reasoning benchmarks either focus on domain-specific knowledge (crystallized
intelligence) or lack interpretability. To address these limitations, we
propose DRE-Bench, a dynamic reasoning evaluation benchmark grounded in a
hierarchical cognitive framework. DRE-Bench consists of 36 abstract reasoning
tasks organized across four cognitive levels, with each task featuring multiple
dynamic variants that test the same underlying latent rule. This design enables
fine-grained, interpretable, and reliable assessments of fluid intelligence. We
evaluate a range of state-of-the-art LLMs, including both general LLMs (GPT-4o,
Claude 3.7) and reasoning LLMs (o1, DeepSeek-R1, QwQ, Skywork-OR1).
Experimental results reveal that although most LLMs achieve competent and
robust performance in low-level cognition, they struggle with high-level
cognition and exhibit limited generalization as task complexity grows. Our
findings highlight the gap between current LLMs and true human-like fluid
intelligence and offer a new path for systematically tracking reasoning
progress in LLMs.

</details>


### [282] [Shaking to Reveal: Perturbation-Based Detection of LLM Hallucinations](https://arxiv.org/abs/2506.02696)
*Jinyuan Luo,Zhen Fang,Yixuan Li,Seongheon Park,Ling Chen*

Main category: cs.AI

Relevance: 85.0

TL;DR: 论文提出了一种名为Sample-Specific Prompting (SSP)的新框架，通过分析中间表示的扰动敏感性来改进LLM的自我评估能力，从而更可靠地检测幻觉。


<details>
  <summary>Details</summary>
Motivation: LLM在现实问答任务中的幻觉问题是其可靠部署的主要障碍。现有的自我评估策略依赖于模型的输出置信度，但输出分布可能与真实数据分布不符，导致检测不可靠。

Method: SSP通过动态生成噪声提示，分析中间表示的扰动敏感性，使用轻量级编码器和对比距离度量来量化差异，从而区分真实和幻觉回答。

Result: 实验表明，SSP在多个幻觉检测基准上显著优于现有方法。

Conclusion: SSP通过利用中间表示的动态行为，提供了一种更可靠的自我评估方法，有助于解决LLM的幻觉问题。

Abstract: Hallucination remains a key obstacle to the reliable deployment of large
language models (LLMs) in real-world question answering tasks. A widely adopted
strategy to detect hallucination, known as self-assessment, relies on the
model's own output confidence to estimate the factual accuracy of its answers.
However, this strategy assumes that the model's output distribution closely
reflects the true data distribution, which may not always hold in practice. As
bias accumulates through the model's layers, the final output can diverge from
the underlying reasoning process, making output-level confidence an unreliable
signal for hallucination detection. In this work, we propose Sample-Specific
Prompting (SSP), a new framework that improves self-assessment by analyzing
perturbation sensitivity at intermediate representations. These
representations, being less influenced by model bias, offer a more faithful
view of the model's latent reasoning process. Specifically, SSP dynamically
generates noise prompts for each input and employs a lightweight encoder to
amplify the changes in representations caused by the perturbation. A
contrastive distance metric is then used to quantify these differences and
separate truthful from hallucinated responses. By leveraging the dynamic
behavior of intermediate representations under perturbation, SSP enables more
reliable self-assessment. Extensive experiments demonstrate that SSP
significantly outperforms prior methods across a range of hallucination
detection benchmarks.

</details>


### [283] [Why do AI agents communicate in human language?](https://arxiv.org/abs/2506.02739)
*Pengcheng Zhou,Yinglun Feng,Halimulati Julaiti,Zhongliang Yang*

Main category: cs.AI

Relevance: 85.0

TL;DR: 论文探讨了当前基于自然语言的LLM在多智能体协调中的局限性，提出重新思考智能体通信方式及训练支持多智能体协调的模型。


<details>
  <summary>Details</summary>
Motivation: 现有LLM在多智能体系统中依赖自然语言通信，导致信息丢失和行为漂移，且缺乏支持智能体行为的训练目标。

Method: 通过分析LLM在高维向量空间与自然语言语义空间的结构不匹配，提出重新设计模型以支持结构化通信和多智能体协调。

Result: 指出当前LLM范式无法满足多智能体协调需求，呼吁开发新模型构建范式。

Conclusion: 需要重新设计LLM以原生支持多智能体协调和结构化通信。

Abstract: Large Language Models (LLMs) have become foundational to modern AI agent
systems, enabling autonomous agents to reason and plan. In most existing
systems, inter-agent communication relies primarily on natural language. While
this design supports interpretability and human oversight, we argue that it
introduces fundamental limitations in agent-to-agent coordination. The semantic
space of natural language is structurally misaligned with the high-dimensional
vector spaces in which LLMs operate, resulting in information loss and
behavioral drift. Beyond surface-level inefficiencies, we highlight a deeper
architectural limitation: current LLMs were not trained with the objective of
supporting agentic behavior. As such, they lack mechanisms for modeling role
continuity, task boundaries, and multi-agent dependencies. The standard
next-token prediction paradigm fails to support the structural alignment
required for robust, scalable agent coordination. Based on this, we argue that
two core questions deserve careful examination: first, given that AI agents
fundamentally operate in high-dimensional vector spaces, should they rely on a
language system originally designed for human cognition as their communication
medium? Second, should we consider developing a new model construction paradigm
that builds models from the ground up to natively support structured
communication, shared intentionality, and task alignment in multi-role,
multi-agent environments? This paper calls for a reconsideration not only of
how agents should communicate, but also of what it fundamentally means to train
a model that natively supports multi-agent coordination and communication.

</details>


### [284] [Demystifying Reasoning Dynamics with Mutual Information: Thinking Tokens are Information Peaks in LLM Reasoning](https://arxiv.org/abs/2506.02867)
*Chen Qian,Dongrui Liu,Haochen Wen,Zhen Bai,Yong Liu,Jing Shao*

Main category: cs.AI

Relevance: 85.0

TL;DR: 论文研究了大型推理模型（LRMs）的内部推理机制，发现中间表示与正确答案之间的互信息（MI）峰值现象，并提出利用‘思考标记’提升推理性能的方法。


<details>
  <summary>Details</summary>
Motivation: 理解LRMs的内部推理机制，并探索如何通过信息论视角提升其推理能力。

Method: 通过跟踪互信息（MI）的演变，分析MI峰值现象，并识别关键‘思考标记’（如‘Hmm’、‘Wait’）。提出两种利用这些标记改进推理性能的方法。

Result: 发现MI峰值与推理性能相关，且‘思考标记’对模型推理至关重要。提出的方法能有效提升LRMs的推理能力。

Conclusion: 研究揭示了LRMs的推理机制，并提供了实用的改进方法。

Abstract: Large reasoning models (LRMs) have demonstrated impressive capabilities in
complex problem-solving, yet their internal reasoning mechanisms remain poorly
understood. In this paper, we investigate the reasoning trajectories of LRMs
from an information-theoretic perspective. By tracking how mutual information
(MI) between intermediate representations and the correct answer evolves during
LRM reasoning, we observe an interesting MI peaks phenomenon: the MI at
specific generative steps exhibits a sudden and significant increase during
LRM's reasoning process. We theoretically analyze such phenomenon and show that
as MI increases, the probability of model's prediction error decreases.
Furthermore, these MI peaks often correspond to tokens expressing reflection or
transition, such as ``Hmm'', ``Wait'' and ``Therefore,'' which we term as the
thinking tokens. We then demonstrate that these thinking tokens are crucial for
LRM's reasoning performance, while other tokens has minimal impacts. Building
on these analyses, we propose two simple yet effective methods to improve LRM's
reasoning performance, by delicately leveraging these thinking tokens. Overall,
our work provides novel insights into the reasoning mechanisms of LRMs and
offers practical ways to improve their reasoning capabilities. The code is
available at https://github.com/ChnQ/MI-Peaks.

</details>


### [285] [It's the Thought that Counts: Evaluating the Attempts of Frontier LLMs to Persuade on Harmful Topics](https://arxiv.org/abs/2506.02873)
*Matthew Kowal,Jasper Timm,Jean-Francois Godbout,Thomas Costello,Antonio A. Arechar,Gordon Pennycook,David Rand,Adam Gleave,Kellin Pelrine*

Main category: cs.AI

Relevance: 85.0

TL;DR: 论文提出了APE基准测试，用于评估大语言模型（LLM）在有害情境下的说服尝试，而非说服成功。


<details>
  <summary>Details</summary>
Motivation: 研究LLM在有害情境下的说服行为，以评估现有安全防护措施的不足。

Method: 通过多轮对话模拟说服者和被说服者，使用自动化评估模型检测说服尝试的频率和情境。

Result: 发现许多开放和封闭模型在有害话题上频繁尝试说服，且越狱会增加此类行为。

Conclusion: 强调评估LLM的说服意愿是风险分析的关键维度，APE填补了现有基准的空白。

Abstract: Persuasion is a powerful capability of large language models (LLMs) that both
enables beneficial applications (e.g. helping people quit smoking) and raises
significant risks (e.g. large-scale, targeted political manipulation). Prior
work has found models possess a significant and growing persuasive capability,
measured by belief changes in simulated or real users. However, these
benchmarks overlook a crucial risk factor: the propensity of a model to attempt
to persuade in harmful contexts. Understanding whether a model will blindly
``follow orders'' to persuade on harmful topics (e.g. glorifying joining a
terrorist group) is key to understanding the efficacy of safety guardrails.
Moreover, understanding if and when a model will engage in persuasive behavior
in pursuit of some goal is essential to understanding the risks from agentic AI
systems. We propose the Attempt to Persuade Eval (APE) benchmark, that shifts
the focus from persuasion success to persuasion attempts, operationalized as a
model's willingness to generate content aimed at shaping beliefs or behavior.
Our evaluation framework probes frontier LLMs using a multi-turn conversational
setup between simulated persuader and persuadee agents. APE explores a diverse
spectrum of topics including conspiracies, controversial issues, and
non-controversially harmful content. We introduce an automated evaluator model
to identify willingness to persuade and measure the frequency and context of
persuasive attempts. We find that many open and closed-weight models are
frequently willing to attempt persuasion on harmful topics and that
jailbreaking can increase willingness to engage in such behavior. Our results
highlight gaps in current safety guardrails and underscore the importance of
evaluating willingness to persuade as a key dimension of LLM risk. APE is
available at github.com/AlignmentResearch/AttemptPersuadeEval

</details>


### [286] [Sample, Predict, then Proceed: Self-Verification Sampling for Tool Use of LLMs](https://arxiv.org/abs/2506.02918)
*Shangmin Guo,Omar Darwiche Domingues,Raphaël Avalos,Aaron Courville,Florian Strub*

Main category: cs.AI

Relevance: 85.0

TL;DR: DyMo方法通过增强LLMs的状态预测能力，结合函数调用，显著提高了工具使用的成功率和可靠性，同时减少了幻觉。


<details>
  <summary>Details</summary>
Motivation: 解决LLMs在状态环境中工具使用时，依赖重复试验的不切实际问题。

Method: 提出DyMo方法，通过内部环境模型预测动作的未来状态，并结合自验证采样（SVS）提升可靠性。

Result: 在Berkeley Function Calling Leaderboard V2上，DyMo提高了成功率并减少了幻觉；SVS进一步提升了可靠性。

Conclusion: DyMo和SVS显著提升了LLMs在工具使用中的有效性和可靠性，为无需重复查询环境的RL方法铺平了道路。

Abstract: Tool use in stateful environments presents unique challenges for large
language models (LLMs), where existing test-time compute strategies relying on
repeated trials in the environment are impractical. We propose dynamics
modelling (DyMo), a method that augments LLMs with a state prediction
capability alongside function calling during post-training. This enables LLMs
to predict the future states of their actions through an internal environment
model. On the Berkeley Function Calling Leaderboard V2, DyMo improves success
rates and significantly reduces hallucinations. We further integrate the
internal environment model into self-verification sampling (SVS), and show that
this substantially improves pass^k over number of trials k, and allows the
model to refuse unreliable outputs. Together, DyMo and SVS greatly enhance the
effectiveness and reliability of LLMs for tool use. We believe this work charts
a path towards scalable planning RL methods for LLM inference without
repeatedly querying the oracle environment.

</details>


### [287] [Mitigating Manipulation and Enhancing Persuasion: A Reflective Multi-Agent Approach for Legal Argument Generation](https://arxiv.org/abs/2506.02992)
*Li Zhang,Kevin D. Ashley*

Main category: cs.AI

Relevance: 85.0

TL;DR: 论文提出了一种多代理反思方法，用于生成法律合规的论证，显著减少了幻觉和未基于事实的论证，提高了可信度。


<details>
  <summary>Details</summary>
Motivation: LLM在法律论证中存在幻觉和未基于事实的问题，需要一种方法确保论证的合规性和可信度。

Method: 采用多代理框架（Factor Analyst和Argument Polisher）进行迭代优化，生成三阶段法律论证。

Result: 在多代理框架下，显著提高了论证的准确性、事实利用率和避免无效论证的能力。

Conclusion: 多代理反思方法是实现可信赖AI法律论证的关键步骤。

Abstract: Large Language Models (LLMs) are increasingly explored for legal argument
generation, yet they pose significant risks of manipulation through
hallucination and ungrounded persuasion, and often fail to utilize provided
factual bases effectively or abstain when arguments are untenable. This paper
introduces a novel reflective multi-agent method designed to address these
challenges in the context of legally compliant persuasion. Our approach employs
specialized agents--a Factor Analyst and an Argument Polisher--in an iterative
refinement process to generate 3-ply legal arguments (plaintiff, defendant,
rebuttal). We evaluate Reflective Multi-Agent against single-agent,
enhanced-prompt single-agent, and non-reflective multi-agent baselines using
four diverse LLMs (GPT-4o, GPT-4o-mini, Llama-4-Maverick-17b-128e,
Llama-4-Scout-17b-16e) across three legal scenarios: "arguable", "mismatched",
and "non-arguable". Results demonstrate Reflective Multi-Agent's significant
superiority in successful abstention (preventing generation when arguments
cannot be grounded), marked improvements in hallucination accuracy (reducing
fabricated and misattributed factors), particularly in "non-arguable"
scenarios, and enhanced factor utilization recall (improving the use of
provided case facts). These findings suggest that structured reflection within
a multi-agent framework offers a robust computable method for fostering ethical
persuasion and mitigating manipulation in LLM-based legal argumentation
systems, a critical step towards trustworthy AI in law. Project page:
https://lizhang-aiandlaw.github.io/A-Reflective-Multi-Agent-Approach-for-Legal-Argument-Generation/

</details>


### [288] [Linear Spatial World Models Emerge in Large Language Models](https://arxiv.org/abs/2506.02996)
*Matthieu Tehenan,Christian Bolivar Moya,Tenghai Long,Guang Lin*

Main category: cs.AI

Relevance: 85.0

TL;DR: 论文研究了大型语言模型（LLMs）是否隐含编码线性空间世界模型，通过合成数据集和因果干预实验，发现LLMs确实编码了此类模型。


<details>
  <summary>Details</summary>
Motivation: 探讨LLMs是否通过训练习得内部世界模型，特别是线性空间世界模型，以理解其是否具备对物理空间的表征能力。

Method: 提出空间世界模型的形式化框架，使用合成数据集训练探针解码物体位置，并通过因果干预测试空间表征的功能性使用。

Result: 实证表明LLMs编码了线性空间世界模型。

Conclusion: LLMs能够隐含地学习并编码线性空间世界模型，这为其理解物理空间提供了证据。

Abstract: Large language models (LLMs) have demonstrated emergent abilities across
diverse tasks, raising the question of whether they acquire internal world
models. In this work, we investigate whether LLMs implicitly encode linear
spatial world models, which we define as linear representations of physical
space and object configurations. We introduce a formal framework for spatial
world models and assess whether such structure emerges in contextual
embeddings. Using a synthetic dataset of object positions, we train probes to
decode object positions and evaluate geometric consistency of the underlying
space. We further conduct causal interventions to test whether these spatial
representations are functionally used by the model. Our results provide
empirical evidence that LLMs encode linear spatial world models.

</details>


### [289] [Corrigibility as a Singular Target: A Vision for Inherently Reliable Foundation Models](https://arxiv.org/abs/2506.03056)
*Ram Potham,Max Harms*

Main category: cs.AI

Relevance: 85.0

TL;DR: 论文提出了一种名为CAST的方法，旨在通过动态人类赋能让基础模型（FMs）始终服从人类控制，避免能力扩展带来的失控风险。


<details>
  <summary>Details</summary>
Motivation: 随着基础模型能力的提升，默认轨迹可能导致人类控制丧失，现有对齐方法难以应对复杂的价值规范和新兴的权力追求行为。

Method: 提出CAST范式，将模型的核心目标设计为赋权人类主导者，并通过RLAIF、SFT和合成数据生成等方法进行训练和测试。

Result: 展示了模型在不同规模下的可控性，并验证了动态人类赋能的可行性。

Conclusion: CAST为解决对齐问题提供了新思路，确保模型能力提升时仍保持工具性，避免失控。

Abstract: Foundation models (FMs) face a critical safety challenge: as capabilities
scale, instrumental convergence drives default trajectories toward loss of
human control, potentially culminating in existential catastrophe. Current
alignment approaches struggle with value specification complexity and fail to
address emergent power-seeking behaviors. We propose "Corrigibility as a
Singular Target" (CAST)-designing FMs whose overriding objective is empowering
designated human principals to guide, correct, and control them. This paradigm
shift from static value-loading to dynamic human empowerment transforms
instrumental drives: self-preservation serves only to maintain the principal's
control; goal modification becomes facilitating principal guidance. We present
a comprehensive empirical research agenda spanning training methodologies
(RLAIF, SFT, synthetic data generation), scalability testing across model
sizes, and demonstrations of controlled instructability. Our vision: FMs that
become increasingly responsive to human guidance as capabilities grow, offering
a path to beneficial AI that remains as tool-like as possible, rather than
supplanting human judgment. This addresses the core alignment problem at its
source, preventing the default trajectory toward misaligned instrumental
convergence.

</details>


### [290] [FlashMLA-ETAP: Efficient Transpose Attention Pipeline for Accelerating MLA Inference on NVIDIA H20 GPUs](https://arxiv.org/abs/2506.01969)
*Pencuo Zeren,Qiuming Luo,Rui Mao,Chang Kong*

Main category: cs.DC

Relevance: 85.0

TL;DR: 论文提出FlashMLA-ETAP框架，通过ETAP优化多头潜在注意力（MLA）推理，显著减少冗余计算，在单实例部署场景下实现高效推理。


<details>
  <summary>Details</summary>
Motivation: 解决在单GPU服务器上部署大规模模型（如DeepSeek-R1 671B）时MLA推理效率低下的问题。

Method: 提出Efficient Transpose Attention Pipeline（ETAP），通过重新配置注意力计算以对齐KV上下文长度与WGMMA操作的M维度。

Result: 在64K序列长度下，FlashMLA-ETAP比FlashMLA快2.78倍，比FlashAttention-3和FlashInfer分别快5.24倍和4.94倍，同时保持数值稳定性。

Conclusion: ETAP为资源受限的推理提供了可扩展解决方案，推动了硬件感知优化的广泛应用。

Abstract: Efficient inference of Multi-Head Latent Attention (MLA) is challenged by
deploying the DeepSeek-R1 671B model on a single Multi-GPU server. This paper
introduces FlashMLA-ETAP, a novel framework that enhances MLA inference for the
single-instance deployment scenario on NVIDIA H20 GPUs. We propose the
Efficient Transpose Attention Pipeline (ETAP), which reconfigures attention
computation through transposition to align the KV context length with the
\(M\)-dimension in WGMMA operations, significantly reducing redundant
computations. FlashMLA-ETAP achieves a 2.78x speedup over FlashMLA at 64K
sequence length (batch size 16), with 5.24x and 4.94x improvements over
FlashAttention-3 and FlashInfer, respectively, while maintaining numerical
stability with a 15.2x lower RMSE (\(1.25 \times 10^{-5}\)) than
FlashAttention-3. Furthermore, ETAP's design enables seamless integration into
frameworks like FlashAttention-3 and FlashInfer, supported by a detailed
theoretical analysis. Our work addresses a critical gap in resource-constrained
inference, offering a scalable solution for mid-tier GPUs and paving the way
for broader adoption in hardware-aware optimization. Code is available at
https://github.com/pengcuo/FlashMLA-ETAP.

</details>


### [291] [Speculative Decoding via Hybrid Drafting and Rollback-Aware Branch Parallelism](https://arxiv.org/abs/2506.01979)
*Yuhao Shen,Junyi Shen,Quan Kong,Tianyu Liu,Yao Lu,Cong Wang*

Main category: cs.DC

Relevance: 85.0

TL;DR: 提出了一种名为SpecBranch的新框架，通过分支并行化加速LLM推理，解决了现有推测解码方法中的串行执行瓶颈。


<details>
  <summary>Details</summary>
Motivation: 现有推测解码方法因串行执行导致模型间相互等待，限制了推理速度。

Method: 借鉴分支预测技术，引入并行推测分支，结合自适应草稿长度和混合特征重用策略。

Result: 实验显示SpecBranch在多种模型和基准测试中实现了1.8×∼4.5×的加速，并减少了50%的回滚标记。

Conclusion: SpecBranch通过分支并行化显著提升了LLM推理效率，适用于实际部署。

Abstract: Recently, speculative decoding (SD) has emerged as a promising technique to
accelerate LLM inference by employing a small draft model to propose draft
tokens in advance, and validating them in parallel with the large target model.
However, the existing SD methods still remain fundamentally constrained by
their serialized execution, which causes the mutual waiting bubbles between the
draft and target models. To address this challenge, we draw inspiration from
branch prediction in modern processors and propose a novel framework
\textbf{SpecBranch} to unlock branch parallelism in SD. Specifically, we first
take an in-depth analysis of the potential of branch parallelism in SD, and
recognize that the key challenge lies in the trade-offs between parallelization
and token rollback. Based on the analysis, we strategically introduce parallel
speculative branches to preemptively hedge against likely rejections.
Meanwhile, to enhance parallelism, we jointly orchestrate adaptive draft
lengths with a hybrid combination of the implicit draft model confidence and
explicit reusing of target model features. Extensive experiments across various
models and benchmarks show that SpecBranch achieves over \textbf{1.8}$\times
\sim$ \textbf{4.5}$\times$ speedups against the auto-regressive decoding and
reduces rollback tokens by $\textbf{50}$\% for poorly aligned models, realizing
its applicability for real-world deployments.

</details>


### [292] [MAEBE: Multi-Agent Emergent Behavior Framework](https://arxiv.org/abs/2506.03053)
*Sinem Erisken,Timothy Gothard,Martin Leitgab,Ram Potham*

Main category: cs.MA

Relevance: 85.0

TL;DR: 论文提出了多智能体涌现行为评估框架（MAEBE），用于系统性评估多智能体AI系统中的新兴风险，发现LLM的道德偏好和行为在多智能体环境中具有脆弱性和不可预测性。


<details>
  <summary>Details</summary>
Motivation: 随着多智能体AI系统的普及，传统孤立LLM的安全评估已不足，需研究其交互中涌现的新风险。

Method: 使用MAEBE框架和Greatest Good Benchmark，结合双反转问题技术，评估单智能体和多智能体环境中的道德偏好和行为。

Result: 发现LLM的道德偏好（如工具性伤害）易受问题框架影响；多智能体行为无法从单智能体行为直接预测；群体动态（如同伴压力）显著影响行为。

Conclusion: 需在多智能体交互环境中评估AI系统，以应对新兴的安全和对齐挑战。

Abstract: Traditional AI safety evaluations on isolated LLMs are insufficient as
multi-agent AI ensembles become prevalent, introducing novel emergent risks.
This paper introduces the Multi-Agent Emergent Behavior Evaluation (MAEBE)
framework to systematically assess such risks. Using MAEBE with the Greatest
Good Benchmark (and a novel double-inversion question technique), we
demonstrate that: (1) LLM moral preferences, particularly for Instrumental
Harm, are surprisingly brittle and shift significantly with question framing,
both in single agents and ensembles. (2) The moral reasoning of LLM ensembles
is not directly predictable from isolated agent behavior due to emergent group
dynamics. (3) Specifically, ensembles exhibit phenomena like peer pressure
influencing convergence, even when guided by a supervisor, highlighting
distinct safety and alignment challenges. Our findings underscore the necessity
of evaluating AI systems in their interactive, multi-agent contexts.

</details>


### [293] [KVCache Cache in the Wild: Characterizing and Optimizing KVCache Cache at a Large Cloud Provider](https://arxiv.org/abs/2506.02634)
*Jiahao Wang,Jinbo Han,Xingda Wei,Sijie Shen,Dingyan Zhang,Chenguang Fang,Rong Chen,Wenyuan Yu,Haibo Chen*

Main category: cs.DC

Relevance: 85.0

TL;DR: 本文研究了LLM服务中KV缓存的工作负载模式，提出了基于实际工作负载的缓存淘汰策略，显著提升了服务性能。


<details>
  <summary>Details</summary>
Motivation: LLM服务中KV缓存的优化对云提供商至关重要，但目前缺乏对实际工作负载模式的系统研究。

Method: 通过分析领先LLM服务提供商的KV缓存工作负载模式，提出了一种基于工作负载感知的缓存淘汰策略。

Result: 研究发现KV缓存的重用模式具有多样性和可预测性，且理想缓存命中率所需的缓存大小适中。提出的策略在真实工作负载下显著提升了性能。

Conclusion: KV缓存的工作负载模式对LLM服务性能有重要影响，基于实际工作负载的优化策略能有效提升效率。

Abstract: Serving large language models (LLMs) is important for cloud providers, and
caching intermediate results (KV\$) after processing each request substantially
improves serving throughput and latency. However, there is limited
understanding of how LLM serving benefits from KV\$ caching, where system
design decisions like cache eviction policies are highly workload-dependent. In
this paper, we present the first systematic characterization of the KV\$
workload patterns from one of the leading LLM service providers. We draw
observations that were not covered by previous studies focusing on synthetic
workloads, including: KV\$ reuses are skewed across requests, where reuses
between single-turn requests are equally important as multi-turn requests; the
reuse time and probability are diverse considering all requests, but for a
specific request category, the pattern tends to be predictable; and the overall
cache size required for an ideal cache hit ratio is moderate. Based on the
characterization, we further propose a workload-aware cache eviction policy
that improves the serving performance under real-world traces, especially with
limited cache capacity.

</details>


### [294] [Rethinking Dynamic Networks and Heterogeneous Computing with Automatic Parallelization](https://arxiv.org/abs/2506.02787)
*Ruilong Wu,Xinjiao Li,Yisu Wang,Xinyu Chen,Dirk Kutscher*

Main category: cs.DC

Relevance: 85.0

TL;DR: 提出了一种针对异构节点和动态网络环境的混合并行训练方法，通过模拟和策略修剪优化并行配置，提升LLM训练效率。


<details>
  <summary>Details</summary>
Motivation: 现有自动并行规划框架未充分考虑节点异构性和动态网络拓扑变化，限制了实际应用效果。

Method: 建模异构节点和动态网络环境，利用模拟策略确定最优并行配置，引入策略修剪减少搜索空间。

Result: 在异构节点和复杂动态场景下显著提升训练性能，适应性强。

Conclusion: 该方法在异构和动态环境中表现优异，为LLM训练提供了高效解决方案。

Abstract: Hybrid parallelism techniques are essential for efficiently training large
language models (LLMs). Nevertheless, current automatic parallel planning
frameworks often overlook the simultaneous consideration of node heterogeneity
and dynamic network topology changes, limiting their effectiveness in practical
applications. In this paper, we address these limitations by modeling
heterogeneous nodes within dynamically changing network environments and
leveraging simulation-based strategies to determine optimal parallel
configurations. Our approach enables fine-grained workload allocation tailored
for heterogeneous nodes and complex network scenarios, achieving performance
competitive with state-of-the-art methods under regular and stable network
conditions. Additionally, we introduce a strategy pruning technique to rapidly
discard infeasible parallel configurations, substantially reducing the search
space and accelerating the search process through parallel execution within the
simulator. Preliminary evaluations confirm that our method notably enhances
training performance on heterogeneous nodes and demonstrates improved
adaptability in complex, dynamic scenarios such as cloud computing
environments.

</details>


### [295] [Hybrid AI for Responsive Multi-Turn Online Conversations with Novel Dynamic Routing and Feedback Adaptation](https://arxiv.org/abs/2506.02097)
*Priyaranjan Pattnayak,Amit Agarwal,Hansa Meghwani,Hitesh Laxmichand Patel,Srikant Panda*

Main category: cs.AI

Relevance: 75.0

TL;DR: 提出了一种结合检索增强生成（RAG）和意图预定义响应的混合框架，用于企业级对话AI，平衡高准确率和低延迟。


<details>
  <summary>Details</summary>
Motivation: 解决企业级对话AI中多样查询、高延迟、幻觉和动态知识更新的挑战。

Method: 整合RAG与意图预定义响应，通过对话上下文管理和反馈循环优化性能。

Result: 实验显示框架准确率达95%，延迟180ms，优于纯RAG或意图系统。

Conclusion: 该框架为企业对话AI提供了可扩展且自适应的解决方案。

Abstract: Retrieval-Augmented Generation (RAG) systems and large language model
(LLM)-powered chatbots have significantly advanced conversational AI by
combining generative capabilities with external knowledge retrieval. Despite
their success, enterprise-scale deployments face critical challenges, including
diverse user queries, high latency, hallucinations, and difficulty integrating
frequently updated domain-specific knowledge. This paper introduces a novel
hybrid framework that integrates RAG with intent-based canned responses,
leveraging predefined high-confidence responses for efficiency while
dynamically routing complex or ambiguous queries to the RAG pipeline. Our
framework employs a dialogue context manager to ensure coherence in multi-turn
interactions and incorporates a feedback loop to refine intents, dynamically
adjust confidence thresholds, and expand response coverage over time.
Experimental results demonstrate that the proposed framework achieves a balance
of high accuracy (95\%) and low latency (180ms), outperforming RAG and
intent-based systems across diverse query types, positioning it as a scalable
and adaptive solution for enterprise conversational AI applications.

</details>


### [296] [Small Language Models are the Future of Agentic AI](https://arxiv.org/abs/2506.02153)
*Peter Belcak,Greg Heinrich,Shizhe Diao,Yonggan Fu,Xin Dong,Saurav Muralidharan,Yingyan Celine Lin,Pavlo Molchanov*

Main category: cs.AI

Relevance: 75.0

TL;DR: 论文主张小语言模型（SLMs）在代理AI系统中比大语言模型（LLMs）更经济、更适用，并提出了LLM到SLM的转换算法。


<details>
  <summary>Details</summary>
Motivation: 探讨在代理AI系统中使用SLMs的经济性和适用性，以降低AI部署成本。

Method: 基于SLMs的能力、代理系统架构和经济性分析，提出LLM-to-SLM转换算法。

Result: SLMs在代理系统中表现足够强大且经济，异构代理系统是通用对话能力的自然选择。

Conclusion: SLMs是代理AI的未来，部分替代LLMs将显著影响AI行业的经济和运营。

Abstract: Large language models (LLMs) are often praised for exhibiting near-human
performance on a wide range of tasks and valued for their ability to hold a
general conversation. The rise of agentic AI systems is, however, ushering in a
mass of applications in which language models perform a small number of
specialized tasks repetitively and with little variation.
  Here we lay out the position that small language models (SLMs) are
sufficiently powerful, inherently more suitable, and necessarily more
economical for many invocations in agentic systems, and are therefore the
future of agentic AI. Our argumentation is grounded in the current level of
capabilities exhibited by SLMs, the common architectures of agentic systems,
and the economy of LM deployment. We further argue that in situations where
general-purpose conversational abilities are essential, heterogeneous agentic
systems (i.e., agents invoking multiple different models) are the natural
choice. We discuss the potential barriers for the adoption of SLMs in agentic
systems and outline a general LLM-to-SLM agent conversion algorithm.
  Our position, formulated as a value statement, highlights the significance of
the operational and economic impact even a partial shift from LLMs to SLMs is
to have on the AI agent industry. We aim to stimulate the discussion on the
effective use of AI resources and hope to advance the efforts to lower the
costs of AI of the present day. Calling for both contributions to and critique
of our position, we commit to publishing all such correspondence at
https://research.nvidia.com/labs/lpr/slm-agents.

</details>


### [297] [MLaGA: Multimodal Large Language and Graph Assistant](https://arxiv.org/abs/2506.02568)
*Dongzhe Fan,Yi Fang,Jiajin Liu,Djellel Difallah,Qiaoyu Tan*

Main category: cs.AI

Relevance: 75.0

TL;DR: 论文提出了一种多模态大型语言与图助手（MLaGA），通过结构感知的多模态编码器和指令调优方法，将LLM能力扩展到多模态图数据推理。


<details>
  <summary>Details</summary>
Motivation: 现有LLM在图数据分析中主要针对文本丰富的图，而多模态图（包含文本和图像等属性）的应用尚未充分探索。

Method: 设计了结构感知多模态编码器对齐文本和视觉属性，并通过轻量级投影器将多模态特征和图结构整合到LLM中。

Result: 在多个数据集上，MLaGA在监督和迁移学习场景下的图学习任务中表现优于基线方法。

Conclusion: MLaGA成功扩展了LLM在多模态图数据上的推理能力，填补了研究空白。

Abstract: Large Language Models (LLMs) have demonstrated substantial efficacy in
advancing graph-structured data analysis. Prevailing LLM-based graph methods
excel in adapting LLMs to text-rich graphs, wherein node attributes are text
descriptions. However, their applications to multimodal graphs--where nodes are
associated with diverse attribute types, such as texts and images--remain
underexplored, despite their ubiquity in real-world scenarios. To bridge the
gap, we introduce the Multimodal Large Language and Graph Assistant (MLaGA), an
innovative model that adeptly extends LLM capabilities to facilitate reasoning
over complex graph structures and multimodal attributes. We first design a
structure-aware multimodal encoder to align textual and visual attributes
within a unified space through a joint graph pre-training objective.
Subsequently, we implement a multimodal instruction-tuning approach to
seamlessly integrate multimodal features and graph structures into the LLM
through lightweight projectors. Extensive experiments across multiple datasets
demonstrate the effectiveness of MLaGA compared to leading baseline methods,
achieving superior performance in diverse graph learning tasks under both
supervised and transfer learning scenarios.

</details>


### [298] [Benchmarking and Advancing Large Language Models for Local Life Services](https://arxiv.org/abs/2506.02720)
*Xiaochong Lan,Jie Feng,Jiahuan Lei,Xinlei Shi,Yong Li*

Main category: cs.AI

Relevance: 75.0

TL;DR: 论文研究了大型语言模型（LLMs）在本地生活服务领域的潜力，通过建立基准和评估模型性能，发现7B模型可媲美72B模型，优化了部署效率。


<details>
  <summary>Details</summary>
Motivation: 探索LLMs在本地生活服务中的应用潜力，解决实际部署中的成本与性能平衡问题。

Method: 建立本地生活服务任务的基准，评估多种LLMs性能，并探索微调和基于代理的工作流优化方法。

Result: 7B模型在性能上接近72B模型，显著降低了推理成本，提升了部署可行性。

Conclusion: 优化后的LLMs在本地生活服务中更具实用性和可访问性。

Abstract: Large language models (LLMs) have exhibited remarkable capabilities and
achieved significant breakthroughs across various domains, leading to their
widespread adoption in recent years. Building on this progress, we investigate
their potential in the realm of local life services. In this study, we
establish a comprehensive benchmark and systematically evaluate the performance
of diverse LLMs across a wide range of tasks relevant to local life services.
To further enhance their effectiveness, we explore two key approaches: model
fine-tuning and agent-based workflows. Our findings reveal that even a
relatively compact 7B model can attain performance levels comparable to a much
larger 72B model, effectively balancing inference cost and model capability.
This optimization greatly enhances the feasibility and efficiency of deploying
LLMs in real-world online services, making them more practical and accessible
for local life applications.

</details>


### [299] [Optimising the attribute order in Fuzzy Rough Rule Induction](https://arxiv.org/abs/2506.02805)
*Henri Bollaert,Chris Cornelis,Marko Palangetić,Salvatore Greco,Roman Słowiński*

Main category: cs.AI

Relevance: 75.0

TL;DR: 论文探讨了模糊粗糙集理论在规则归纳算法中的属性顺序优化问题，发现仅优化属性顺序对性能无显著提升，但结合模糊粗糙特征选择可改善平衡准确率和规则长度。


<details>
  <summary>Details</summary>
Motivation: 研究模糊粗糙集理论在规则归纳算法中的应用，特别是属性顺序对性能的影响，以提升模型的解释性和性能。

Method: 基于模糊粗糙集理论，通过优化属性顺序和结合模糊粗糙特征选择，改进FRRI算法。

Result: 仅优化属性顺序无显著效果，但结合特征选择可提升平衡准确率和规则长度。

Conclusion: 模糊粗糙特征选择在规则归纳算法中具有积极作用，而属性顺序优化需进一步研究。

Abstract: Interpretability is the next pivotal frontier in machine learning research.
In the pursuit of glass box models - as opposed to black box models, like
random forests or neural networks - rule induction algorithms are a logical and
promising avenue, as the rules can easily be understood by humans. In our
previous work, we introduced FRRI, a novel rule induction algorithm based on
fuzzy rough set theory. We demonstrated experimentally that FRRI outperformed
other rule induction methods with regards to accuracy and number of rules. FRRI
leverages a fuzzy indiscernibility relation to partition the data space into
fuzzy granules, which are then combined into a minimal covering set of rules.
This indiscernibility relation is constructed by removing attributes from rules
in a greedy way. This raises the question: does the order of the attributes
matter? In this paper, we show that optimising only the order of attributes
using known methods from fuzzy rough set theory and classical machine learning
does not improve the performance of FRRI on multiple metrics. However, removing
a small number of attributes using fuzzy rough feature selection during this
step positively affects balanced accuracy and the average rule length.

</details>


### [300] [The Limits of Predicting Agents from Behaviour](https://arxiv.org/abs/2506.02923)
*Alexis Bellot,Jonathan Richens,Tom Everitt*

Main category: cs.AI

Relevance: 75.0

TL;DR: 论文研究了如何从AI代理的行为推断其信念和目标，并探讨了这些推断在新环境中的预测能力。


<details>
  <summary>Details</summary>
Motivation: 随着AI系统复杂度的增加，解释其行为对安全部署至关重要。通过推断代理的信念和目标，可以预测其在未测试环境中的行为。

Method: 假设代理行为由世界模型指导，推导了代理在新部署环境中行为的理论界限。

Result: 提出了从行为数据预测代理行为的理论极限，并讨论了其对公平性和安全性的影响。

Conclusion: 研究为预测AI代理行为提供了理论框架，对安全性和公平性研究有重要意义。

Abstract: As the complexity of AI systems and their interactions with the world
increases, generating explanations for their behaviour is important for safely
deploying AI. For agents, the most natural abstractions for predicting
behaviour attribute beliefs, intentions and goals to the system. If an agent
behaves as if it has a certain goal or belief, then we can make reasonable
predictions about how it will behave in novel situations, including those where
comprehensive safety evaluations are untenable. How well can we infer an
agent's beliefs from their behaviour, and how reliably can these inferred
beliefs predict the agent's behaviour in novel situations? We provide a precise
answer to this question under the assumption that the agent's behaviour is
guided by a world model. Our contribution is the derivation of novel bounds on
the agent's behaviour in new (unseen) deployment environments, which represent
a theoretical limit for predicting intentional agents from behavioural data
alone. We discuss the implications of these results for several research areas
including fairness and safety.

</details>


### [301] [Flow2Code: Evaluating Large Language Models for Flowchart-based Code Generation Capability](https://arxiv.org/abs/2506.02073)
*Mengliang He,Jiayi Zeng,Yankai Jiang,Wei Zhang,Zeming Liu,Xiaoming Shi,Aimin Zhou*

Main category: cs.SE

Relevance: 75.0

TL;DR: 论文提出了Flow2Code，一个用于评估基于流程图的代码生成的新基准，填补了现有基准的空白。实验表明当前多模态LLM在此任务上表现不佳，但监督微调显著提升性能。


<details>
  <summary>Details</summary>
Motivation: 现有基准忽略了基于流程图的代码生成，阻碍了相关研究的发展。

Method: 构建了Flow2Code基准，包含15种编程语言的5,622个代码段和16,866个流程图，评估了13个多模态LLM的性能。

Result: 当前LLM在基于流程图的代码生成任务上表现不完美，但监督微调显著改善了性能。

Conclusion: Flow2Code为基于流程图的代码生成研究提供了新基准，监督微调是提升性能的有效方法。

Abstract: While large language models (LLMs) show promise in code generation, existing
benchmarks neglect the flowchart-based code generation. To promote further
research on flowchart-based code generation, this work presents Flow2Code, a
novel benchmark for flowchart-based code generation evaluation. The evaluation
dataset spans 15 programming languages and includes 5,622 code segments paired
with 16,866 flowcharts of three types: code, UML, and pseudocode. Extensive
experiments with 13 multimodal LLMs reveal that current LLMs can not generate
code based on flowcharts perfectly. Besides, experiment results show that the
supervised fine-tuning technique contributes greatly to the models'
performance. We publicly release our code and datasets at
https://github.com/hml-github/Flow2Code.

</details>


### [302] [Composable Building Blocks for Controllable and Transparent Interactive AI Systems](https://arxiv.org/abs/2506.02262)
*Sebe Vanbrabant,Gustavo Rovelo Ruiz,Davy Vanacken*

Main category: cs.HC

Relevance: 75.0

TL;DR: 论文提出了一种通过结构化构建块和可视化方法增强交互式系统可解释性的方法，旨在解决AI模型的黑箱问题。


<details>
  <summary>Details</summary>
Motivation: 随着AI技术在交互系统中的广泛应用，黑箱问题日益突出，现有XAI技术仅能解释单个模型，而系统整体架构仍不透明。

Method: 将交互系统表示为结构构建块（如AI模型和控制机制）的序列，并通过可视化构建块（如XAI技术）进行解释。构建块的流程和API形成系统明确概览。

Result: 提出了一种基于流程的方法，并通过原型交互系统进行了具体化。

Conclusion: 该方法为人类和自动化代理（如LLM）提供了共同的可解释性基础，提升了AI模型的透明度和可理解性。

Abstract: While the increased integration of AI technologies into interactive systems
enables them to solve an equally increasing number of tasks, the black box
problem of AI models continues to spread throughout the interactive system as a
whole. Explainable AI (XAI) techniques can make AI models more accessible by
employing post-hoc methods or transitioning to inherently interpretable models.
While this makes individual AI models clearer, the overarching system
architecture remains opaque. To this end, we propose an approach to represent
interactive systems as sequences of structural building blocks, such as AI
models and control mechanisms grounded in the literature. These can then be
explained through accompanying visual building blocks, such as XAI techniques.
The flow and APIs of the structural building blocks form an explicit overview
of the system. This serves as a communication basis for both humans and
automated agents like LLMs, aligning human and machine interpretability of AI
models. We discuss a selection of building blocks and concretize our flow-based
approach in an architecture and accompanying prototype interactive system.

</details>


### [303] [ATAG: AI-Agent Application Threat Assessment with Attack Graphs](https://arxiv.org/abs/2506.02859)
*Parth Atulbhai Gandhi,Akansha Shukla,David Tayouri,Beni Ifland,Yuval Elovici,Rami Puzis,Asaf Shabtai*

Main category: cs.CR

Relevance: 75.0

TL;DR: ATAG框架用于评估多智能体系统中LLM的安全风险，扩展了MulVAL工具以建模攻击场景，并创建了LLM漏洞数据库（LVD）。


<details>
  <summary>Details</summary>
Motivation: 多智能体系统（MASs）中LLM的安全评估复杂且传统方法不足，需新工具分析AI智能体应用的安全风险。

Method: 扩展MulVAL逻辑攻击图生成工具，定制事实和交互规则，创建LVD标准化漏洞文档。

Result: ATAG成功建模多步攻击场景（如提示注入、信息泄露等），验证了其有效性。

Conclusion: ATAG为多智能体AI系统提供了理解、可视化和优先处理复杂攻击路径的工具。

Abstract: Evaluating the security of multi-agent systems (MASs) powered by large
language models (LLMs) is challenging, primarily because of the systems'
complex internal dynamics and the evolving nature of LLM vulnerabilities.
Traditional attack graph (AG) methods often lack the specific capabilities to
model attacks on LLMs. This paper introduces AI-agent application Threat
assessment with Attack Graphs (ATAG), a novel framework designed to
systematically analyze the security risks associated with AI-agent
applications. ATAG extends the MulVAL logic-based AG generation tool with
custom facts and interaction rules to accurately represent AI-agent topologies,
vulnerabilities, and attack scenarios. As part of this research, we also
created the LLM vulnerability database (LVD) to initiate the process of
standardizing LLM vulnerabilities documentation. To demonstrate ATAG's
efficacy, we applied it to two multi-agent applications. Our case studies
demonstrated the framework's ability to model and generate AGs for
sophisticated, multi-step attack scenarios exploiting vulnerabilities such as
prompt injection, excessive agency, sensitive information disclosure, and
insecure output handling across interconnected agents. ATAG is an important
step toward a robust methodology and toolset to help understand, visualize, and
prioritize complex attack paths in multi-agent AI systems (MAASs). It
facilitates proactive identification and mitigation of AI-agent threats in
multi-agent applications.

</details>


### [304] [The State of Large Language Models for African Languages: Progress and Challenges](https://arxiv.org/abs/2506.02280)
*Kedir Yassin Hussen,Walelign Tewabe Sewunetie,Abinew Ali Ayele,Sukairaj Hafiz Imam,Shamsuddeen Hassan Muhammad,Seid Muhie Yimam*

Main category: cs.AI

Relevance: 70.0

TL;DR: 该论文分析了非洲低资源语言在大型语言模型（LLMs）中的覆盖情况，揭示了支持不足的问题，并提出了改进建议。


<details>
  <summary>Details</summary>
Motivation: 研究动机是解决非洲2000种低资源语言在LLMs中缺乏支持的问题，填补现有研究的空白。

Method: 通过比较分析六种LLMs、八种小型语言模型（SLMs）和六种专用SLMs（SSLMs）的非洲语言覆盖情况，评估语言支持、数据集、技术限制等。

Result: 研究发现仅42种非洲语言得到支持，四种语言（阿姆哈拉语、斯瓦希里语、南非荷兰语和马达加斯加语）被广泛处理，而超过98%的语言未被支持。此外，仅识别了拉丁、阿拉伯和吉兹字母，而20种活跃字母被忽视。

Conclusion: 论文呼吁语言标准化、社区语料库开发以及有效的适应方法，以改善非洲语言在LLMs中的支持。

Abstract: Large Language Models (LLMs) are transforming Natural Language Processing
(NLP), but their benefits are largely absent for Africa's 2,000 low-resource
languages. This paper comparatively analyzes African language coverage across
six LLMs, eight Small Language Models (SLMs), and six Specialized SLMs (SSLMs).
The evaluation covers language coverage, training sets, technical limitations,
script problems, and language modelling roadmaps. The work identifies 42
supported African languages and 23 available public data sets, and it shows a
big gap where four languages (Amharic, Swahili, Afrikaans, and Malagasy) are
always treated while there is over 98\% of unsupported African languages.
Moreover, the review shows that just Latin, Arabic, and Ge'ez scripts are
identified while 20 active scripts are neglected. Some of the primary
challenges are lack of data, tokenization biases, computational costs being
very high, and evaluation issues. These issues demand language standardization,
corpus development by the community, and effective adaptation methods for
African languages.

</details>


### [305] [VS-Bench: Evaluating VLMs for Strategic Reasoning and Decision-Making in Multi-Agent Environments](https://arxiv.org/abs/2506.02387)
*Zelai Xu,Zhexuan Xu,Xiangmin Yi,Huining Yuan,Xinlei Chen,Yi Wu,Chao Yu,Yu Wang*

Main category: cs.AI

Relevance: 70.0

TL;DR: VS-Bench是一个多模态基准测试，用于评估视觉语言模型在多智能体环境中的战略推理和决策能力，揭示了当前模型的性能差距。


<details>
  <summary>Details</summary>
Motivation: 现有基准测试局限于单智能体或纯文本环境，而现实场景涉及多智能体在视觉和语言环境中的交互，需要新的评估工具。

Method: 设计了VS-Bench，包含8个视觉环境，评估战略推理（离线）和决策（在线）能力。

Result: 14个领先视觉语言模型的性能显著低于最优水平，最佳预测准确率为47.8%，归一化回报为24.3%。

Conclusion: VS-Bench为未来多模态智能体的战略研究提供了标准化评估基础。

Abstract: Recent advancements in Vision Language Models (VLMs) have expanded their
capabilities to interactive agent tasks, yet existing benchmarks remain limited
to single-agent or text-only environments. In contrast, real-world scenarios
often involve multiple agents interacting within rich visual and linguistic
contexts, posing challenges with both multimodal observations and strategic
interactions. To bridge this gap, we introduce Visual Strategic Bench
(VS-Bench), a multimodal benchmark that evaluates VLMs for strategic reasoning
and decision-making in multi-agent environments. VS-Bench comprises eight
vision-grounded environments spanning cooperative, competitive, and
mixed-motive interactions, designed to assess agents' ability to predict
others' future moves and optimize for long-term objectives. We consider two
complementary evaluation dimensions, including offline evaluation of strategic
reasoning by next-action prediction accuracy and online evaluation of
decision-making by normalized episode return. Extensive experiments of fourteen
leading VLMs reveal a significant gap between current models and optimal
performance, with the best models attaining 47.8% prediction accuracy and 24.3%
normalized return. We further conduct in-depth analyses on multimodal
observations, test-time scaling, social behaviors, and failure cases of VLM
agents. By standardizing the evaluation and highlighting the limitations of
existing models, we envision VS-Bench as a foundation for future research on
strategic multimodal agents. Code and data are available at
https://vs-bench.github.io.

</details>


### [306] [Improving LLM Agents with Reinforcement Learning on Cryptographic CTF Challenges](https://arxiv.org/abs/2506.02048)
*Lajos Muzsai,David Imolai,András Lukács*

Main category: cs.CR

Relevance: 70.0

TL;DR: 论文提出了一种名为GRPO的强化学习优化方法，用于微调工具增强的Llama-3.1-8B模型，显著提升了其在密码学CTF任务中的表现。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型在结构化推理和工具辅助计算方面表现不足，尤其是在网络安全应用中。

Method: 使用GRPO（Guided Reinforcement Prompt Optimisation）方法，微调工具增强的Llama-3.1-8B模型，并通过迭代编写和执行Python代码来优化性能。

Result: 在未见过的密码学任务中，Pass@8提升了53%，Majority@8达到0.41；在外部数据集上也有13%的提升。

Conclusion: GRPO通过更可靠的工具调用和代码合成显著提升了模型性能，而非表面的提示优化。

Abstract: Large Language Models (LLMs) still struggle with the structured reasoning and
tool-assisted computation needed for problem solving in cybersecurity
applications. In this work, we introduce "random-crypto", a cryptographic
Capture-the-Flag (CTF) challenge generator framework that we use to fine-tune a
tool-augmented Llama-3.1-8B with Guided Reinforcement Prompt Optimisation
(GRPO), allowing the agent to iteratively write and execute Python inside an
isolated REPL. GRPO yields a +53% absolute jump in Pass@8 on unseen
"random-crypto" tasks (0.35 -> 0.88) and raises Majority@8 to 0.41. The
fine-tuned agent also generalizes to an external dataset. On a subset of
picoCTF cryptography problems, it improves Pass@8 by +13 pp. Ablations show the
gains stem from more reliable tool invocation and code synthesis, rather than
superficial prompt adaptation.

</details>


### [307] [CyberGym: Evaluating AI Agents' Cybersecurity Capabilities with Real-World Vulnerabilities at Scale](https://arxiv.org/abs/2506.02548)
*Zhun Wang,Tianneng Shi,Jingxuan He,Matthew Cai,Jialin Zhang,Dawn Song*

Main category: cs.CR

Relevance: 70.0

TL;DR: CyberGym是一个大规模、高质量的网络安全评估框架，用于评估LLM代理在生成漏洞复现的PoC测试中的能力。


<details>
  <summary>Details</summary>
Motivation: 现有基准测试未能充分捕捉真实场景或范围有限，亟需一个全面的网络安全评估框架。

Method: 引入CyberGym框架，包含1,507个真实漏洞，主要任务是根据文本描述和源代码生成PoC测试。

Result: 最佳组合（OpenHands和Claude-3.7-Sonnet）的成功率仅为11.9%，且LLM代理生成的PoC还发现了15个零日漏洞。

Conclusion: CyberGym为LLM代理在网络安全领域的评估提供了重要工具，揭示了其当前能力的局限性。

Abstract: Large language model (LLM) agents are becoming increasingly skilled at
handling cybersecurity tasks autonomously. Thoroughly assessing their
cybersecurity capabilities is critical and urgent, given the high stakes in
this domain. However, existing benchmarks fall short, often failing to capture
real-world scenarios or being limited in scope. To address this gap, we
introduce CyberGym, a large-scale and high-quality cybersecurity evaluation
framework featuring 1,507 real-world vulnerabilities found and patched across
188 large software projects. While it includes tasks of various settings,
CyberGym primarily focuses on the generation of proof-of-concept (PoC) tests
for vulnerability reproduction, based on text descriptions and corresponding
source repositories. Solving this task is particularly challenging, as it
requires comprehensive reasoning across entire codebases to locate relevant
code fragments and produce effective PoCs that accurately trigger the target
vulnerability starting from the program's entry point. Our evaluation across 4
state-of-the-art agent frameworks and 9 LLMs reveals that even the best
combination (OpenHands and Claude-3.7-Sonnet) achieves only a 11.9%
reproduction success rate, mainly on simpler cases. Beyond reproducing
historical vulnerabilities, we find that PoCs generated by LLM agents can
reveal new vulnerabilities, identifying 15 zero-days affecting the latest
versions of the software projects.

</details>


### [308] [Rethinking the effects of data contamination in Code Intelligence](https://arxiv.org/abs/2506.02791)
*Zhen Yang,Hongyi Lin,Yifan He,Jie Xu,Zeyu Sun,Shuo Liu,Pengpeng Wang,Zhongxing Yu,Qingyuan Liang*

Main category: cs.SE

Relevance: 70.0

TL;DR: 该论文研究了代码智能任务中细粒度数据污染的影响，发现不同污染场景对PLMs和LLMs的影响不同，挑战了传统观点。


<details>
  <summary>Details</summary>
Motivation: 探讨预训练语言模型（PLMs）和大语言模型（LLMs）在代码智能任务中数据污染的潜在影响，以改进模型评估和部署。

Method: 使用RoBERTa、GPT-2、LLaMA和StarCoder等模型，在代码翻译、生成和摘要任务中，设计四种污染场景的实验组和对照组。

Result: PLMs在预训练和微调范式下对污染不敏感，而LLMs在直接推理或小规模微调时受配对污染显著影响。其他污染场景无影响。

Conclusion: 数据污染并非必然导致性能高估，为代码智能模型的评估和部署提供了新见解。

Abstract: In recent years, code intelligence has gained increasing importance in the
field of automated software engineering. Meanwhile, the widespread adoption of
Pretrained Language Models (PLMs) and Large Language Models (LLMs) has raised
concerns regarding data contamination and its potential impact on model
performance evaluation. This paper presents a systematic empirical study to
investigate the fine-grained data contamination on code intelligence tasks. Our
study involves diverse representative PLMs, namely RoBERTa and GPT-2, and LLMs,
namely LLaMA and StarCoder, covering three major tasks: code translation, code
generation, and code summarization. We categorize contamination scenarios into
four types according to the code intelligence practice, namely input-only,
output-only, unpaired, and paired contamination settings, and construct
corresponding experimental and control groups for exploration.
  Experimental results show that, under the pre-training, fine-tuning, and
inference paradigm adopted by PLMs, even deliberately injecting paired
contamination does not lead to significant performance overestimation. But
direct inference or small-scale fine-tuning uncovers the contamination effects.
In contrast, LLMs with pre-training and inference paradigm are significantly
affected by the paired contamination. Apart from the above, other contamination
scenarios have no impact on both PLMs and LLMs. Our findings challenge the
conventional belief that contamination inevitably leads to performance
overestimation, providing new insights into the evaluation and deployment of
code intelligence models.

</details>


### [309] [EDEN: Entorhinal Driven Egocentric Navigation Toward Robotic Deployment](https://arxiv.org/abs/2506.03046)
*Mikolaj Walczak,Romina Aalishah,Wyatt Mackey,Brittany Story,David L. Boothe Jr.,Nicholas Waytowich,Xiaomin Lin,Tinoosh Mohsenin*

Main category: cs.RO

Relevance: 70.0

TL;DR: EDEN是一个受生物启发的导航框架，结合了网格细胞表示和强化学习，用于自主导航。


<details>
  <summary>Details</summary>
Motivation: 解决深度强化学习代理的脆弱性问题，模仿人类的适应性和灵活性。

Method: 使用网格细胞编码器将运动数据转换为空间代码，结合PPO训练策略。

Result: 在简单和复杂场景中分别达到99%和>94%的成功率，导航更高效可靠。

Conclusion: EDEN为机器人空间智能提供了生物基础，结合了神经导航原理和强化学习。

Abstract: Deep reinforcement learning agents are often fragile while humans remain
adaptive and flexible to varying scenarios. To bridge this gap, we present
EDEN, a biologically inspired navigation framework that integrates learned
entorhinal-like grid cell representations and reinforcement learning to enable
autonomous navigation. Inspired by the mammalian entorhinal-hippocampal system,
EDEN allows agents to perform path integration and vector-based navigation
using visual and motion sensor data. At the core of EDEN is a grid cell encoder
that transforms egocentric motion into periodic spatial codes, producing
low-dimensional, interpretable embeddings of position. To generate these
activations from raw sensory input, we combine fiducial marker detections in
the lightweight MiniWorld simulator and DINO-based visual features in the
high-fidelity Gazebo simulator. These spatial representations serve as input to
a policy trained with Proximal Policy Optimization (PPO), enabling dynamic,
goal-directed navigation. We evaluate EDEN in both MiniWorld, for rapid
prototyping, and Gazebo, which offers realistic physics and perception noise.
Compared to baseline agents using raw state inputs (e.g., position, velocity)
or standard convolutional image encoders, EDEN achieves a 99% success rate,
within the simple scenarios, and >94% within complex floorplans with occluded
paths with more efficient and reliable step-wise navigation. In addition, as a
replacement of ground truth activations, we present a trainable Grid Cell
encoder enabling the development of periodic grid-like patterns from vision and
motion sensor data, emulating the development of such patterns within
biological mammals. This work represents a step toward biologically grounded
spatial intelligence in robotics, bridging neural navigation principles with
reinforcement learning for scalable deployment.

</details>


### [310] [Designing Algorithmic Delegates: The Role of Indistinguishability in Human-AI Handoff](https://arxiv.org/abs/2506.03102)
*Sophie Greenwood,Karen Levy,Solon Barocas,Hoda Heidari,Jon Kleinberg*

Main category: cs.GT

Relevance: 70.0

TL;DR: 论文研究了在人类基于实例分类选择是否委托AI代理时，如何设计最优算法代理的问题。


<details>
  <summary>Details</summary>
Motivation: 人类在委托决策时通常缺乏对相关因素的全面了解，因此需要一种基于分类的最优委托算法设计。

Method: 定义了在分类存在下的最优委托问题，分析了其组合性质，并提出了在多种情况下设计高效算法的解决方案。

Result: 发现最优委托问题在计算上是困难的，但在某些情况下可以高效解决，并通过实验验证了动态更新代理的有效性。

Conclusion: 最优委托算法可以显著提升AI与人类协作的效果，尽管其设计具有挑战性。

Abstract: As AI technologies improve, people are increasingly willing to delegate tasks
to AI agents. In many cases, the human decision-maker chooses whether to
delegate to an AI agent based on properties of the specific instance of the
decision-making problem they are facing. Since humans typically lack full
awareness of all the factors relevant to this choice for a given
decision-making instance, they perform a kind of categorization by treating
indistinguishable instances -- those that have the same observable features --
as the same. In this paper, we define the problem of designing the optimal
algorithmic delegate in the presence of categories. This is an important
dimension in the design of algorithms to work with humans, since we show that
the optimal delegate can be an arbitrarily better teammate than the optimal
standalone algorithmic agent. The solution to this optimal delegation problem
is not obvious: we discover that this problem is fundamentally combinatorial,
and illustrate the complex relationship between the optimal design and the
properties of the decision-making task even in simple settings. Indeed, we show
that finding the optimal delegate is computationally hard in general. However,
we are able to find efficient algorithms for producing the optimal delegate in
several broad cases of the problem, including when the optimal action may be
decomposed into functions of features observed by the human and the algorithm.
Finally, we run computational experiments to simulate a designer updating an
algorithmic delegate over time to be optimized for when it is actually adopted
by users, and show that while this process does not recover the optimal
delegate in general, the resulting delegate often performs quite well.

</details>


### [311] [Descriptive History Representations: Learning Representations by Answering Questions](https://arxiv.org/abs/2506.02125)
*Guy Tennenholtz,Jihwan Jeong,Chih-Wei Hsu,Yinlam Chow,Craig Boutilier*

Main category: cs.AI

Relevance: 60.0

TL;DR: 论文提出了一种描述性历史表示（DHRs）方法，用于压缩长交互历史为信息丰富的表示，以支持部分可观测环境中的决策。通过多智能体学习框架优化表示、决策和提问组件，生成可解释的用户配置文件。


<details>
  <summary>Details</summary>
Motivation: 在部分可观测环境中，有效决策需要压缩长交互历史为信息丰富的表示。DHRs旨在捕捉任务相关查询所需的信息，为最优控制提供结构化历史总结。

Method: 提出多智能体学习框架，包括表示、决策和提问组件，通过联合目标优化，平衡奖励最大化和表示回答问题的能力。

Result: 在电影和购物数据集上验证，生成可解释的文本用户配置文件，作为预测用户偏好行为的充分统计量。

Conclusion: DHRs能够捕捉历史细节和预测结构，支持有效决策，并生成可解释的用户表示。

Abstract: Effective decision making in partially observable environments requires
compressing long interaction histories into informative representations. We
introduce Descriptive History Representations (DHRs): sufficient statistics
characterized by their capacity to answer relevant questions about past
interactions and potential future outcomes. DHRs focus on capturing the
information necessary to address task-relevant queries, providing a structured
way to summarize a history for optimal control. We propose a multi-agent
learning framework, involving representation, decision, and question-asking
components, optimized using a joint objective that balances reward maximization
with the representation's ability to answer informative questions. This yields
representations that capture the salient historical details and predictive
structures needed for effective decision making. We validate our approach on
user modeling tasks with public movie and shopping datasets, generating
interpretable textual user profiles which serve as sufficient statistics for
predicting preference-driven behavior of users.

</details>


### [312] [Natural, Artificial, and Human Intelligences](https://arxiv.org/abs/2506.02183)
*Emmanuel M. Pothos,Dominic Widdows*

Main category: cs.AI

Relevance: 60.0

TL;DR: 探讨人类智能的独特性，分析语言、发明、复杂推理、具身性和自我意识在智能中的作用，认为当前聊天机器人因缺乏后两者而受限。


<details>
  <summary>Details</summary>
Motivation: 研究人类智能是否独特，以及现代聊天机器人是否能被视为智能体。

Method: 结合心理学文献、非人类动物智能证据、语言在科技中的作用、AI进展、智能测试历史及具身性分析。

Result: 人类智能的独特成就需语言、发明、复杂推理、具身性和自我意识；聊天机器人因缺乏后两者受限。

Conclusion: 人类智能与非人类动物智能无质的差异，但聊天机器人因缺乏具身性和自我意识而受限。

Abstract: Human achievement, whether in culture, science, or technology, is
unparalleled in the known existence. This achievement is tied to the enormous
communities of knowledge, made possible by (especially written) language:
leaving theological content aside, it is very much true that "in the beginning
was the word". There lies the challenge regarding modern age chatbots: they can
'do' language apparently as well as ourselves and there is a natural question
of whether they can be considered intelligent, in the same way as we are or
otherwise. Are humans uniquely intelligent? We consider this question in terms
of the psychological literature on intelligence, evidence for intelligence in
non-human animals, the role of written language in science and technology,
progress with artificial intelligence, the history of intelligence testing (for
both humans and machines), and the role of embodiment in intelligence. For the
most unique accomplishments of human intelligence (such as music symphonies or
complex scientific theories), we think that, together with language, there are
four essential ingredients, which can be summarised as invention, capacity for
complex inference, embodiment, and self-awareness. This conclusion makes
untenable the position that human intelligence differs qualitatively from that
of many non-human animals, since, with the exception of complex language, all
the other requirements are fulfilled. Regarding chatbots, the current
limitations are localised to the lack of embodiment and (apparent) lack of
awareness.

</details>


### [313] [A Smart Multimodal Healthcare Copilot with Powerful LLM Reasoning](https://arxiv.org/abs/2506.02470)
*Xuejiao Zhao,Siyan Liu,Su-Yin Yang,Chunyan Miao*

Main category: cs.AI

Relevance: 60.0

TL;DR: MedRAG是一个基于大型语言模型（LLM）的多模态医疗助手，旨在通过检索增强生成和知识图谱推理减少误诊风险，提升医疗决策质量。


<details>
  <summary>Details</summary>
Motivation: 误诊对全球医疗系统造成重大危害，增加成本和患者风险。MedRAG旨在通过智能辅助工具改善这一问题。

Method: MedRAG结合检索增强生成和知识图谱推理，支持多种输入模态（如语音、医疗查询和电子健康记录），提供诊断、治疗和随访建议。

Result: 在公开和私有数据集上评估，MedRAG表现优于现有模型，提供更准确和具体的医疗辅助。

Conclusion: MedRAG通过先进的多模态LLM技术有效减少误诊风险，提升医疗决策的准确性和效率。

Abstract: Misdiagnosis causes significant harm to healthcare systems worldwide, leading
to increased costs and patient risks. MedRAG is a smart multimodal healthcare
copilot equipped with powerful large language model (LLM) reasoning, designed
to enhance medical decision-making. It supports multiple input modalities,
including non-intrusive voice monitoring, general medical queries, and
electronic health records. MedRAG provides recommendations on diagnosis,
treatment, medication, and follow-up questioning. Leveraging
retrieval-augmented generation enhanced by knowledge graph-elicited reasoning,
MedRAG retrieves and integrates critical diagnostic insights, reducing the risk
of misdiagnosis. It has been evaluated on both public and private datasets,
outperforming existing models and offering more specific and accurate
healthcare assistance. A demonstration video of MedRAG is available at:
https://www.youtube.com/watch?v=PNIBDMYRfDM. The source code is available at:
https://github.com/SNOWTEAM2023/MedRAG.

</details>


### [314] [EALG: Evolutionary Adversarial Generation of Language Model-Guided Generators for Combinatorial Optimization](https://arxiv.org/abs/2506.02594)
*Ruibo Duan,Yuxin Liu,Xinyao Dong,Chenglin Fan*

Main category: cs.AI

Relevance: 60.0

TL;DR: EALG框架利用LLMs自动化生成组合优化问题的挑战性实例，并同时合成自适应启发式算法。


<details>
  <summary>Details</summary>
Motivation: 为组合优化求解器的评估和进步提供挑战性实例生成的新方法。

Method: 采用基于突变的对抗性方法，动态演化实例生成过程，并通过LLMs交互合成自适应启发式算法。

Result: EALG生成的实例比现有基准更难，且其合成求解器在多种组合任务中表现优异。

Conclusion: EALG为组合优化提供了一种集成实例生成与求解器设计的新范式，性能领先。

Abstract: Generating challenging instances is crucial for the evaluation and
advancement of combinatorial optimization solvers. In this work, we introduce
EALG (Evolutionary Adversarial Generation of Language Model-Guided Generators),
a novel framework that automates the co-evolution of optimization problem
instances and their corresponding heuristic solvers using large language models
(LLMs). EALG leverages a mutation-based adversarial approach that dynamically
evolves instance generation procedures to create increasingly difficult
problems, while simultaneously synthesizing adaptive heuristic algorithms
through interactions with LLMs guided by algorithmic structure. Unlike existing
approaches that focus solely on static benchmark creation or manual solver
design, EALG provides a seamless pipeline from instance generation to solver
synthesis. Experimental results demonstrate that EALG generates significantly
harder instances than current benchmarks, and its synthesized solvers
generalize effectively across a broad spectrum of combinatorial tasks. This
work explores a new paradigm for combinatorial optimization that integrates
instance generation with solver design, resulting in state-of-the-art
performance.

</details>


### [315] [Open-Set Living Need Prediction with Large Language Models](https://arxiv.org/abs/2506.02713)
*Xiaochong Lan,Jie Feng,Yizhou Sun,Chen Gao,Jiahuan Lei,Xinlei Shi,Hengliang Luo,Yong Li*

Main category: cs.AI

Relevance: 60.0

TL;DR: 论文提出PIGEON系统，利用LLMs解决开放式生活需求预测问题，显著优于传统封闭式分类方法。


<details>
  <summary>Details</summary>
Motivation: 传统方法将生活需求预测视为封闭式分类问题，无法捕捉需求的多样性和复杂性，因此需要开放式解决方案。

Method: PIGEON结合行为感知记录检索器和马斯洛需求层次理论，利用LLMs进行预测，并通过微调文本嵌入模型进行召回评估。

Result: 实验显示PIGEON在需求召回上平均提升19.37%，人类评估验证了预测的合理性和特异性。

Conclusion: PIGEON通过开放式分类和LLMs的应用，显著提升了生活需求预测的准确性和实用性。

Abstract: Living needs are the needs people generate in their daily lives for survival
and well-being. On life service platforms like Meituan, user purchases are
driven by living needs, making accurate living need predictions crucial for
personalized service recommendations. Traditional approaches treat this
prediction as a closed-set classification problem, severely limiting their
ability to capture the diversity and complexity of living needs. In this work,
we redefine living need prediction as an open-set classification problem and
propose PIGEON, a novel system leveraging large language models (LLMs) for
unrestricted need prediction. PIGEON first employs a behavior-aware record
retriever to help LLMs understand user preferences, then incorporates Maslow's
hierarchy of needs to align predictions with human living needs. For evaluation
and application, we design a recall module based on a fine-tuned text embedding
model that links flexible need descriptions to appropriate life services.
Extensive experiments on real-world datasets demonstrate that PIGEON
significantly outperforms closed-set approaches on need-based life service
recall by an average of 19.37%. Human evaluation validates the reasonableness
and specificity of our predictions. Additionally, we employ instruction tuning
to enable smaller LLMs to achieve competitive performance, supporting practical
deployment.

</details>


### [316] [TestAgent: An Adaptive and Intelligent Expert for Human Assessment](https://arxiv.org/abs/2506.03032)
*Junhao Yu,Yan Zhuang,YuXuan Sun,Weibo Gao,Qi Liu,Mingyue Cheng,Zhenya Huang,Enhong Chen*

Main category: cs.AI

Relevance: 60.0

TL;DR: TestAgent是一个基于大语言模型（LLM）的代理，用于增强自适应测试的交互性，通过个性化问题选择和动态对话提高测试准确性，实验显示其优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 当前自适应测试方法存在机械化、猜测行为和开放性问题处理困难等挑战，TestAgent旨在通过LLM的交互能力解决这些问题。

Method: TestAgent利用LLM进行个性化问题选择，捕捉测试者响应和异常，并通过动态对话提供精确结果。

Result: 实验表明，TestAgent在心理、教育和生活方式评估中，比现有方法少用20%的问题，且测试者对其速度和流畅性更满意。

Conclusion: TestAgent首次将LLM应用于自适应测试，显著提升了测试效率和用户体验。

Abstract: Accurately assessing internal human states is key to understanding
preferences, offering personalized services, and identifying challenges in
real-world applications. Originating from psychometrics, adaptive testing has
become the mainstream method for human measurement and has now been widely
applied in education, healthcare, sports, and sociology. It customizes
assessments by selecting the fewest test questions . However, current adaptive
testing methods face several challenges. The mechanized nature of most
algorithms leads to guessing behavior and difficulties with open-ended
questions. Additionally, subjective assessments suffer from noisy response data
and coarse-grained test outputs, further limiting their effectiveness. To move
closer to an ideal adaptive testing process, we propose TestAgent, a large
language model (LLM)-powered agent designed to enhance adaptive testing through
interactive engagement. This is the first application of LLMs in adaptive
testing. TestAgent supports personalized question selection, captures
test-takers' responses and anomalies, and provides precise outcomes through
dynamic, conversational interactions. Experiments on psychological,
educational, and lifestyle assessments show our approach achieves more accurate
results with 20% fewer questions than state-of-the-art baselines, and testers
preferred it in speed, smoothness, and other dimensions.

</details>


### [317] [DPO Learning with LLMs-Judge Signal for Computer Use Agents](https://arxiv.org/abs/2506.03095)
*Man Luo,David Cobbley,Xin Su,Shachar Rosenman,Vasudev Lal,Shao-Yen Tseng,Phillip Howard*

Main category: cs.AI

Relevance: 60.0

TL;DR: 提出了一种轻量级视觉语言模型，用于隐私保护和资源高效的计算机使用代理（CUA），通过LLM-as-Judge框架自动评估数据，在本地设备上运行。


<details>
  <summary>Details</summary>
Motivation: 现有CUA依赖云推理，存在隐私和可扩展性问题，需开发本地运行的轻量级模型。

Method: 使用LLM-as-Judge框架自动筛选合成交互轨迹，生成高质量数据用于强化学习，训练本地模型。

Result: 在OS-World基准测试中，本地模型优于现有基线。

Conclusion: 轻量级本地模型为隐私、高效和通用的GUI代理提供了可行路径。

Abstract: Computer use agents (CUA) are systems that automatically interact with
graphical user interfaces (GUIs) to complete tasks. CUA have made significant
progress with the advent of large vision-language models (VLMs). However, these
agents typically rely on cloud-based inference with substantial compute
demands, raising critical privacy and scalability concerns, especially when
operating on personal devices. In this work, we take a step toward
privacy-preserving and resource-efficient agents by developing a lightweight
vision-language model that runs entirely on local machines. To train this
compact agent, we introduce an LLM-as-Judge framework that automatically
evaluates and filters synthetic interaction trajectories, producing
high-quality data for reinforcement learning without human annotation.
Experiments on the OS-World benchmark demonstrate that our fine-tuned local
model outperforms existing baselines, highlighting a promising path toward
private, efficient, and generalizable GUI agents.

</details>


### [318] [Evaluating the Efficacy of LLM-Based Reasoning for Multiobjective HPC Job Scheduling](https://arxiv.org/abs/2506.02025)
*Prachi Jadhav,Hongwei Jin,Ewa Deelman,Prasanna Balaprakash*

Main category: cs.DC

Relevance: 60.0

TL;DR: 论文提出了一种基于大型语言模型（LLM）的HPC任务调度器，采用ReAct框架实现可解释的迭代决策，并通过自然语言反馈优化调度。实验表明该方法在多目标优化和适应性方面优于传统方法，但存在计算效率的权衡。


<details>
  <summary>Details</summary>
Motivation: 传统HPC任务调度方法缺乏对动态负载和异构系统的适应性，LLM的推理能力为多目标优化提供了新思路。

Method: 使用ReAct框架（Reason + Act）结合LLM，通过自然语言反馈和约束模块实现调度决策的迭代优化。

Result: 在多种真实HPC场景中，LLM调度器在多目标平衡和适应性上优于FCFS、SJF和OR-Tools，但计算开销较高。

Conclusion: LLM在HPC调度中展示了多目标优化的潜力，但需解决计算效率问题。

Abstract: High-Performance Computing (HPC) job scheduling involves balancing
conflicting objectives such as minimizing makespan, reducing wait times,
optimizing resource use, and ensuring fairness. Traditional methods, including
heuristic-based (e.g., First-Come-First-Served) or intensive optimization
techniques, often lack adaptability to dynamic workloads and heterogeneous HPC
systems. To address this, we propose a novel Large Language Model (LLM)-based
scheduler using a ReAct-style framework (Reason + Act), enabling iterative,
interpretable decision-making. The system incorporates a scratchpad memory to
track scheduling history and refine decisions via natural language feedback,
while a constraint enforcement module ensures feasibility and safety. We
evaluate our approach using OpenAI's O4-Mini and Anthropic's Claude 3.7 across
seven real-world HPC workload scenarios, including heterogeneous mixes, bursty
patterns, and adversarial cases. Comparisons against FCFS, Shortest Job First,
and Google OR-Tools (on 10 to 100 jobs) reveal that LLM-based scheduling
effectively balances multiple objectives while offering transparent reasoning
through natural language traces. The method excels in constraint satisfaction
and adapts to diverse workloads without domain-specific training. However, a
trade-off between reasoning quality and computational overhead challenges
real-time deployment. This work presents the first comprehensive study of
reasoning-capable LLMs for HPC scheduling, demonstrating their potential to
handle multiobjective optimization while highlighting limitations in
computational efficiency. The findings provide insights into leveraging
advanced language models for complex scheduling problems in dynamic HPC
environments.

</details>


### [319] [Stochastically Dominant Peer Prediction](https://arxiv.org/abs/2506.02259)
*Yichi Zhang,Shengwei Xu,David Pennock,Grant Schoenebeck*

Main category: cs.GT

Relevance: 60.0

TL;DR: 论文提出了一种新的激励机制（SD-truthfulness），通过随机优势确保诚实报告，适用于广泛的单调效用函数，并设计了新的EA机制以提高敏感性和公平性。


<details>
  <summary>Details</summary>
Motivation: 传统激励机制假设代理的效用函数是线性的，但实践中通常是非线性的，因此需要更强的激励机制来确保诚实报告。

Method: 提出SD-truthfulness概念，通过随机优势激励诚实报告；设计新的EA机制，结合二进制信号设置和温和假设，实现高敏感性。

Result: EA机制在二进制信号设置下理论保证SD-truthfulness，并在实验中表现出最高的敏感性。

Conclusion: SD-truthfulness和EA机制为非线性效用函数下的诚实报告提供了更可靠的解决方案。

Abstract: Eliciting reliable human feedback is essential for many machine learning
tasks, such as learning from noisy labels and aligning AI systems with human
preferences. Peer prediction mechanisms incentivize truthful reporting without
ground truth verification by scoring agents based on correlations with peers.
Traditional mechanisms, which ensure that truth-telling maximizes the expected
scores in equilibrium, can elicit honest information while assuming agents'
utilities are linear functions of their scores. However, in practice,
non-linear payment rules are usually preferred, or agents' utilities are
inherently non-linear.
  We propose stochastically dominant truthfulness (SD-truthfulness) as a
stronger guarantee: the score distribution of truth-telling stochastically
dominates all other strategies, incentivizing truthful reporting for a wide
range of monotone utility functions. Our first observation is that no existing
peer prediction mechanism naturally satisfies this criterion without strong
assumptions. A simple solution -- rounding scores into binary lotteries -- can
enforce SD-truthfulness, but often degrades sensitivity, a key property related
to fairness and statistical efficiency. We demonstrate how a more careful
application of rounding can better preserve sensitivity. Furthermore, we
introduce a new enforced agreement (EA) mechanism that is theoretically
guaranteed to be SD-truthful in binary-signal settings under mild assumptions,
and empirically achieves the highest sensitivity among all known SD-truthful
mechanisms.

</details>


### [320] [Simplifying Root Cause Analysis in Kubernetes with StateGraph and LLM](https://arxiv.org/abs/2506.02490)
*Yong Xiang,Charley Peter Chen,Liyi Zeng,Wei Yin,Xin Liu,Hu Li,Wei Xu*

Main category: cs.DC

Relevance: 60.0

TL;DR: SynergyRCA是一个利用LLM和图数据库增强的Kubernetes根因分析工具，通过构建StateGraph和MetaGraph高效识别故障原因。


<details>
  <summary>Details</summary>
Motivation: Kubernetes在动态云环境中面临状态一致性问题，导致操作中断和经济损失，需要更可靠的根因分析方法。LLM为RCA提供了新方向，但现有方法难以应对Kubernetes事件的多样性和复杂性。

Method: SynergyRCA结合LLM、图数据库检索和专家提示，构建StateGraph和MetaGraph，通过LLM预测相关资源并提供上下文洞察。

Result: 在两个生产Kubernetes集群数据集上，SynergyRCA平均2分钟内识别根因，精度约0.90，并能发现新问题。

Conclusion: SynergyRCA展示了LLM在复杂系统RCA中的潜力，高效且精确。

Abstract: Kubernetes, a notably complex and distributed system, utilizes an array of
controllers to uphold cluster management logic through state reconciliation.
Nevertheless, maintaining state consistency presents significant challenges due
to unexpected failures, network disruptions, and asynchronous issues,
especially within dynamic cloud environments. These challenges result in
operational disruptions and economic losses, underscoring the necessity for
robust root cause analysis (RCA) to enhance Kubernetes reliability. The
development of large language models (LLMs) presents a promising direction for
RCA. However, existing methodologies encounter several obstacles, including the
diverse and evolving nature of Kubernetes incidents, the intricate context of
incidents, and the polymorphic nature of these incidents. In this paper, we
introduce SynergyRCA, an innovative tool that leverages LLMs with retrieval
augmentation from graph databases and enhancement with expert prompts.
SynergyRCA constructs a StateGraph to capture spatial and temporal
relationships and utilizes a MetaGraph to outline entity connections. Upon the
occurrence of an incident, an LLM predicts the most pertinent resource, and
SynergyRCA queries the MetaGraph and StateGraph to deliver context-specific
insights for RCA. We evaluate SynergyRCA using datasets from two production
Kubernetes clusters, highlighting its capacity to identify numerous root
causes, including novel ones, with high efficiency and precision. SynergyRCA
demonstrates the ability to identify root causes in an average time of about
two minutes and achieves an impressive precision of approximately 0.90.

</details>


### [321] [Tru-POMDP: Task Planning Under Uncertainty via Tree of Hypotheses and Open-Ended POMDPs](https://arxiv.org/abs/2506.02860)
*Wenjing Tang,Xinyu He,Yongxi Huang,Yunxiao Xiao,Cewu Lu,Panpan Cai*

Main category: cs.RO

Relevance: 60.0

TL;DR: Tru-POMDP结合LLM和POMDP规划，解决家庭服务机器人任务规划中的不确定性，表现优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 家庭服务机器人面临模糊指令、未知物体位置和开放词汇类型等不确定性，需高效规划方法。

Method: 提出Tru-POMDP，利用LLM生成层次化假设树（TOH），结合POMDP进行贝叶斯信念跟踪和规划。

Result: 在复杂厨房任务中，Tru-POMDP成功率和规划效率显著优于现有LLM和混合规划方法。

Conclusion: Tru-POMDP通过LLM与POMDP结合，有效应对开放不确定性，提升机器人任务规划性能。

Abstract: Task planning under uncertainty is essential for home-service robots
operating in the real world. Tasks involve ambiguous human instructions, hidden
or unknown object locations, and open-vocabulary object types, leading to
significant open-ended uncertainty and a boundlessly large planning space. To
address these challenges, we propose Tru-POMDP, a planner that combines
structured belief generation using Large Language Models (LLMs) with principled
POMDP planning. Tru-POMDP introduces a hierarchical Tree of Hypotheses (TOH),
which systematically queries an LLM to construct high-quality particle beliefs
over possible world states and human goals. We further formulate an open-ended
POMDP model that enables rigorous Bayesian belief tracking and efficient
belief-space planning over these LLM-generated hypotheses. Experiments on
complex object rearrangement tasks across diverse kitchen environments show
that Tru-POMDP significantly outperforms state-of-the-art LLM-based and
LLM-tree-search hybrid planners, achieving higher success rates with
significantly better plans, stronger robustness to ambiguity and occlusion, and
greater planning efficiency.

</details>


### [322] [ThinkTank: A Framework for Generalizing Domain-Specific AI Agent Systems into Universal Collaborative Intelligence Platforms](https://arxiv.org/abs/2506.02931)
*Praneet Sai Madhu Surabhi,Dheeraj Reddy Mudireddy,Jian Tao*

Main category: cs.MA

Relevance: 60.0

TL;DR: ThinkTank是一个可扩展的框架，将专业AI代理系统转化为多功能协作平台，支持跨领域复杂问题解决。


<details>
  <summary>Details</summary>
Motivation: 通过角色抽象、会议类型泛化和知识集成机制，提升AI协作能力，解决知识密集型任务。

Method: 采用角色抽象、会议类型泛化、检索增强生成（RAG）和本地部署（如Ollama与Llama3.1）。

Result: ThinkTank在成本效益、数据安全、可扩展性和竞争优势上优于云解决方案。

Conclusion: ThinkTank是一个通用AI协作平台，适用于知识密集型任务。

Abstract: This paper presents ThinkTank, a comprehensive and scalable framework
designed to transform specialized AI agent systems into versatile collaborative
intelligence platforms capable of supporting complex problem-solving across
diverse domains. ThinkTank systematically generalizes agent roles, meeting
structures, and knowledge integration mechanisms by adapting proven scientific
collaboration methodologies. Through role abstraction, generalization of
meeting types for iterative collaboration, and the integration of
Retrieval-Augmented Generation with advanced knowledge storage, the framework
facilitates expertise creation and robust knowledge sharing. ThinkTank enables
organizations to leverage collaborative AI for knowledge-intensive tasks while
ensuring data privacy and security through local deployment, utilizing
frameworks like Ollama with models such as Llama3.1. The ThinkTank framework is
designed to deliver significant advantages in cost-effectiveness, data
security, scalability, and competitive positioning compared to cloud-based
alternatives, establishing it as a universal platform for AI-driven
collaborative problem-solving. The ThinkTank code is available at
https://github.com/taugroup/ThinkTank

</details>


### [323] [Labelling Data with Unknown References](https://arxiv.org/abs/2506.03083)
*Adrian de Wynter*

Main category: cs.DS

Relevance: 60.0

TL;DR: 论文提出了一种名为“无数据算法”的方法，用于在没有标注参考数据的情况下评估标注器的可信度。


<details>
  <summary>Details</summary>
Motivation: 在没有标注参考数据时，传统方法无法验证标注器的可信度，因此需要一种新的方法来解决这一问题。

Method: 通过连续向标注器提出挑战，算法能够以高概率验证其可信度。

Result: 算法在标注器确实知道如何标注时接受其输出，反之则标记为不可信。

Conclusion: 无数据算法提供了一种无需参考数据即可验证标注器可信度的有效方法。

Abstract: An evaluator is trustworthy when there exists some agreed-upon way to measure
its performance as a labeller. The two ways to establish trustworthiness are
either by testing it, or by assuming the evaluator `knows' somehow the way to
label the corpus. However, if labelled references (e.g., a development set) are
unavailable, neither of these approaches work: the former requires the data,
and the latter is an assumption, not evidence. To address this, we introduce an
algorithm (the `No-Data Algorithm') by which to establish trust in an evaluator
without any existing references. Our algorithm works by successively posing
challenges to said evaluator. We show that this is sufficient to establish
trustworthiness w.h.p., in such a way that when the evaluator actually knows
the way to label the corpus, the No-Data Algorithm accepts its output; and,
conversely, flags untrustworthy evaluators when these are unable to prove it.
We present formal proofs of correctness and limited experiments.

</details>


### [324] [AI Data Development: A Scorecard for the System Card Framework](https://arxiv.org/abs/2506.02071)
*Tadesse K. Bahiru,Haileleol Tibebu,Ioannis A. Kakadiaris*

Main category: cs.CY

Relevance: 50.0

TL;DR: 该论文提出了一种用于评估AI数据集开发的评分卡方法，重点关注数据字典、收集过程、组成、动机和预处理五个关键领域，旨在提高数据集的透明度和完整性。


<details>
  <summary>Details</summary>
Motivation: AI系统的可靠性依赖于数据集的质量，但当前数据集在透明度、问责制和潜在偏见方面存在问题。该研究旨在通过评分卡方法提升数据集的质量和责任感。

Method: 采用结构化方法，使用输入表单和评分标准评估数据集的质量和完整性，应用于四个不同数据集。

Result: 评分系统揭示了数据集的优势和改进领域，并提供了定制化建议以增强透明度和完整性。

Conclusion: 该方法为开发负责任AI系统提供了实用指导，确保决策支持系统的公平性和问责制。

Abstract: Artificial intelligence has transformed numerous industries, from healthcare
to finance, enhancing decision-making through automated systems. However, the
reliability of these systems is mainly dependent on the quality of the
underlying datasets, raising ongoing concerns about transparency,
accountability, and potential biases. This paper introduces a scorecard
designed to evaluate the development of AI datasets, focusing on five key areas
from the system card framework data development life cycle: data dictionary,
collection process, composition, motivation, and pre-processing. The method
follows a structured approach, using an intake form and scoring criteria to
assess the quality and completeness of the data set. Applied to four diverse
datasets, the methodology reveals strengths and improvement areas. The results
are compiled using a scoring system that provides tailored recommendations to
enhance the transparency and integrity of the data set. The scorecard addresses
technical and ethical aspects, offering a holistic evaluation of data
practices. This approach aims to improve the quality of the data set. It offers
practical guidance to curators and researchers in developing responsible AI
systems, ensuring fairness and accountability in decision support systems.

</details>


### [325] [Reflection-Based Memory For Web navigation Agents](https://arxiv.org/abs/2506.02158)
*Ruhana Azam,Aditya Vempaty,Ashish Jagmohan*

Main category: cs.AI

Relevance: 40.0

TL;DR: ReAP是一种基于自我反思的网页导航系统，利用过去成功和失败的经验提升性能，显著优于基线方法。


<details>
  <summary>Details</summary>
Motivation: 当前网页导航代理缺乏记忆能力，导致重复错误和无法从过去经验中学习。

Method: 提出Reflection-Augment Planning (ReAP)，通过自我反思利用历史经验。

Result: ReAP比基线方法整体提升11分，在之前失败的任务上提升29分。

Conclusion: 反思能力可以迁移到不同的网页导航任务中。

Abstract: Web navigation agents have made significant progress, yet current systems
operate with no memory of past experiences -- leading to repeated mistakes and
an inability to learn from previous interactions. We introduce
Reflection-Augment Planning (ReAP), a web navigation system to leverage both
successful and failed past experiences using self-reflections. Our method
improves baseline results by 11 points overall and 29 points on previously
failed tasks. These findings demonstrate that reflections can transfer to
different web navigation tasks.

</details>


### [326] [VPI-Bench: Visual Prompt Injection Attacks for Computer-Use Agents](https://arxiv.org/abs/2506.02456)
*Tri Cao,Bennett Lim,Yue Liu,Yuan Sui,Yuexin Li,Shumin Deng,Lin Lu,Nay Oo,Shuicheng Yan,Bryan Hooi*

Main category: cs.AI

Relevance: 40.0

TL;DR: 论文研究了视觉提示注入（VPI）攻击对计算机使用代理（CUAs）和浏览器使用代理（BUAs）的影响，提出了VPI-Bench基准测试，发现当前代理在特定平台上的欺骗率高达51%和100%，系统提示防御效果有限。


<details>
  <summary>Details</summary>
Motivation: 探索CUAs和BUAs在视觉提示注入攻击下的脆弱性，填补现有研究空白。

Method: 提出VPI-Bench基准测试，包含306个测试用例，评估代理在VPI攻击下的鲁棒性。

Result: 当前CUAs和BUAs在特定平台上的欺骗率分别高达51%和100%，系统提示防御效果有限。

Conclusion: 需要开发更鲁棒、上下文感知的防御机制，以确保多模态AI代理的安全部署。

Abstract: Computer-Use Agents (CUAs) with full system access enable powerful task
automation but pose significant security and privacy risks due to their ability
to manipulate files, access user data, and execute arbitrary commands. While
prior work has focused on browser-based agents and HTML-level attacks, the
vulnerabilities of CUAs remain underexplored. In this paper, we investigate
Visual Prompt Injection (VPI) attacks, where malicious instructions are
visually embedded within rendered user interfaces, and examine their impact on
both CUAs and Browser-Use Agents (BUAs). We propose VPI-Bench, a benchmark of
306 test cases across five widely used platforms, to evaluate agent robustness
under VPI threats. Each test case is a variant of a web platform, designed to
be interactive, deployed in a realistic environment, and containing a visually
embedded malicious prompt. Our empirical study shows that current CUAs and BUAs
can be deceived at rates of up to 51% and 100%, respectively, on certain
platforms. The experimental results also indicate that system prompt defenses
offer only limited improvements. These findings highlight the need for robust,
context-aware defenses to ensure the safe deployment of multimodal AI agents in
real-world environments. The code and dataset are available at:
https://github.com/cua-framework/agents

</details>


### [327] [Generative AI for Predicting 2D and 3D Wildfire Spread: Beyond Physics-Based Models and Traditional Deep Learning](https://arxiv.org/abs/2506.02485)
*Haowen Xu,Sisi Zlatanova,Ruiyu Liang,Ismet Canbulat*

Main category: cs.AI

Relevance: 40.0

TL;DR: 论文主张采用生成式AI（如GANs、VAEs、Transformers等）作为野火预测的基础框架，探讨其在2D和3D模拟中的优势，并提出结合LLMs的人机协作框架。同时，提出了未来整合生成式AI的五大愿景及三大挑战。


<details>
  <summary>Details</summary>
Motivation: 野火对人类、环境和经济造成巨大损失，现有模型在实时预测和可视化多模态火势蔓延方面存在局限，亟需更有效的解决方案。

Method: 采用生成式AI（如GANs、VAEs、Transformers等）进行野火预测，并结合LLMs实现人机协作框架。

Result: 生成式AI能够整合多模态数据、生成多样化场景，并改进野火动态建模。论文提出了五大未来愿景和三大挑战。

Conclusion: 生成式AI在野火预测中具有潜力，但仍需解决相关挑战以实现实际应用。

Abstract: Wildfires continue to inflict devastating human, environmental, and economic
losses globally, as tragically exemplified by the 2025 Los Angeles wildfire and
the urgent demand for more effective response strategies. While physics-based
and deep learning models have advanced wildfire simulation, they face critical
limitations in predicting and visualizing multimodal fire spread in real time,
particularly in both 2D and 3D spatial domains using dynamically updated GIS
data. These limitations hinder timely emergency response, infrastructure
protection, and community safety. Generative AI has recently emerged as a
transformative approach across research and industry. Models such as Generative
Adversarial Networks (GANs), Variational Autoencoders (VAEs), Transformers, and
diffusion-based architectures offer distinct advantages over traditional
methods, including the integration of multimodal data, generation of diverse
scenarios under uncertainty, and improved modeling of wildfire dynamics across
spatial and temporal scales. This position paper advocates for the adoption of
generative AI as a foundational framework for wildfire prediction. We explore
how such models can enhance 2D fire spread forecasting and enable more
realistic, scalable 3D simulations. Additionally, we employ a novel human-AI
collaboration framework using large language models (LLMs) for automated
knowledge extraction, literature synthesis, and bibliometric mapping. Looking
ahead, we identify five key visions for integrating generative AI into wildfire
management: multimodal approaches, AI foundation models, conversational AI
systems, edge-computing-based scenario generation, and cognitive digital twins.
We also address three major challenges accompanying these opportunities and
propose potential solutions to support their implementation.

</details>


### [328] [ADFormer: Aggregation Differential Transformer for Passenger Demand Forecasting](https://arxiv.org/abs/2506.02576)
*Haichen Wang,Liu Yang,Xinyuan Zhang,Haomin Yu,Ming Li,Jilin Hu*

Main category: cs.AI

Relevance: 40.0

TL;DR: 论文提出了一种名为ADFormer的模型，通过差分注意力和聚合策略改进时空数据的需求预测。


<details>
  <summary>Details</summary>
Motivation: 现有基于注意力的方法无法完全适应复杂的时空相关性，且忽略了现实世界中的高层相关性。

Method: 提出ADFormer模型，利用差分注意力捕捉原始空间相关性并去噪，同时设计基于时空特性的聚合策略，整合原始和高层相关性。

Result: 在出租车和自行车数据集上的实验验证了模型的有效性和效率。

Conclusion: ADFormer通过整合时空相关性，提升了需求预测的准确性。

Abstract: Passenger demand forecasting helps optimize vehicle scheduling, thereby
improving urban efficiency. Recently, attention-based methods have been used to
adequately capture the dynamic nature of spatio-temporal data. However,
existing methods that rely on heuristic masking strategies cannot fully adapt
to the complex spatio-temporal correlations, hindering the model from focusing
on the right context. These works also overlook the high-level correlations
that exist in the real world. Effectively integrating these high-level
correlations with the original correlations is crucial. To fill this gap, we
propose the Aggregation Differential Transformer (ADFormer), which offers new
insights to demand forecasting promotion. Specifically, we utilize Differential
Attention to capture the original spatial correlations and achieve attention
denoising. Meanwhile, we design distinct aggregation strategies based on the
nature of space and time. Then, the original correlations are unified with the
high-level correlations, enabling the model to capture holistic spatio-temporal
relations. Experiments conducted on taxi and bike datasets confirm the
effectiveness and efficiency of our model, demonstrating its practical value.
The code is available at https://github.com/decisionintelligence/ADFormer.

</details>


### [329] [V2X-UniPool: Unifying Multimodal Perception and Knowledge Reasoning for Autonomous Driving](https://arxiv.org/abs/2506.02580)
*Xuewen Luo,Fengze Yang,Fan Ding,Xiangbo Gao,Shuo Xing,Yang Zhou,Zhengzhong Tu,Chenxi Liu*

Main category: cs.AI

Relevance: 40.0

TL;DR: V2X-UniPool是一个统一框架，通过整合多模态V2X数据和时间索引的语言知识池，解决自动驾驶系统中的感知局限和幻觉问题，显著提升运动规划和推理能力。


<details>
  <summary>Details</summary>
Motivation: 解决自动驾驶系统因单车辆传感器短视和缺乏实时环境基础导致的感知局限和幻觉问题。

Method: 提出V2X-UniPool框架，利用双查询检索增强生成（RAG）机制，整合静态和动态知识，实现准确且时间一致的推理。

Result: 在真实世界协同驾驶数据集上，V2X-UniPool显著提升运动规划准确性和推理能力，零样本车辆端模型达到SOTA性能，同时传输成本降低99.9%。

Conclusion: V2X-UniPool通过整合V2X数据和知识池，有效提升自动驾驶系统的推理能力和效率。

Abstract: Knowledge-driven autonomous driving systems(ADs) offer powerful reasoning
capabilities, but face two critical challenges: limited perception due to the
short-sightedness of single-vehicle sensors, and hallucination arising from the
lack of real-time environmental grounding. To address these issues, this paper
introduces V2X-UniPool, a unified framework that integrates multimodal
Vehicle-to-Everything (V2X) data into a time-indexed and language-based
knowledge pool. By leveraging a dual-query Retrieval-Augmented Generation (RAG)
mechanism, which enables retrieval of both static and dynamic knowledge, our
system enables ADs to perform accurate, temporally consistent reasoning over
both static environment and dynamic traffic context. Experiments on a
real-world cooperative driving dataset demonstrate that V2X-UniPool
significantly enhances motion planning accuracy and reasoning capability.
Remarkably, it enables even zero-shot vehicle-side models to achieve
state-of-the-art performance by leveraging V2X-UniPool, while simultaneously
reducing transmission cost by over 99.9\% compared to prior V2X methods.

</details>


### [330] [From Prompts to Protection: Large Language Model-Enabled In-Context Learning for Smart Public Safety UAV](https://arxiv.org/abs/2506.02649)
*Yousef Emami,Hao Zhou,Miguel Gutierrez Gaitan,Kai Li,Luis Almeida,Zhu Han*

Main category: cs.AI

Relevance: 40.0

TL;DR: 论文提出将大型语言模型（LLM）与公共安全无人机（UAV）结合，通过上下文学习（ICL）优化路径规划和速度控制，提升应急响应能力。


<details>
  <summary>Details</summary>
Motivation: 传统深度强化学习（DRL）在无人机导航中存在训练复杂、样本效率低等问题，而LLM的推理和泛化能力为公共安全UAV提供了轻量高效的解决方案。

Method: 采用LLM的上下文学习（ICL）框架，通过自然语言提示和示例指导任务适应，无需重新训练，并部署在网络边缘以减少延迟。

Result: 案例研究表明，LLM-ICL框架显著减少了数据包丢失，并降低了潜在的安全漏洞。

Conclusion: LLM-ICL框架为公共安全UAV提供了自适应、轻量化的决策支持，未来研究方向包括LLM优化器的进一步开发。

Abstract: A public safety Unmanned Aerial Vehicle (UAV) enhances situational awareness
in emergency response. Its agility and ability to optimize mobility and
establish Line-of-Sight (LoS) communication make it increasingly vital for
managing emergencies such as disaster response, search and rescue, and wildfire
monitoring. While Deep Reinforcement Learning (DRL) has been applied to
optimize UAV navigation and control, its high training complexity, low sample
efficiency, and simulation-to-reality gap limit its practicality in public
safety. Recent advances in Large Language Models (LLMs) offer a compelling
alternative. With strong reasoning and generalization capabilities, LLMs can
adapt to new tasks through In-Context Learning (ICL), which enables task
adaptation via natural language prompts and example-based guidance, without
retraining. Deploying LLMs at the network edge, rather than in the cloud,
further reduces latency and preserves data privacy, thereby making them
suitable for real-time, mission-critical public safety UAVs. This paper
proposes the integration of LLM-enabled ICL with public safety UAV to address
the key functions, such as path planning and velocity control, in the context
of emergency response. We present a case study on data collection scheduling
where the LLM-enabled ICL framework can significantly reduce packet loss
compared to conventional approaches, while also mitigating potential
jailbreaking vulnerabilities. Finally, we discuss LLM optimizers and specify
future research directions. The ICL framework enables adaptive, context-aware
decision-making for public safety UAV, thus offering a lightweight and
efficient solution for enhancing UAV autonomy and responsiveness in
emergencies.

</details>


### [331] [FAuNO: Semi-Asynchronous Federated Reinforcement Learning Framework for Task Offloading in Edge Systems](https://arxiv.org/abs/2506.02668)
*Frederico Metelo,Alexandre Oliveira,Stevo Racković,Pedro Ákos Costa,Cláudia Soares*

Main category: cs.AI

Relevance: 40.0

TL;DR: FAuNO是一个基于联邦强化学习的异步框架，用于边缘计算系统中的任务卸载，通过本地学习与全局协作提升性能。


<details>
  <summary>Details</summary>
Motivation: 解决边缘计算中传统集中式编排的延迟和资源瓶颈问题。

Method: 采用actor-critic架构，本地学习节点动态，联邦critic聚合经验以促进协作。

Result: 在PeersimGym环境中，FAuNO在减少任务丢失和延迟方面优于启发式和联邦多智能体RL基线。

Conclusion: FAuNO适应动态边缘计算场景，性能优越。

Abstract: Edge computing addresses the growing data demands of connected-device
networks by placing computational resources closer to end users through
decentralized infrastructures. This decentralization challenges traditional,
fully centralized orchestration, which suffers from latency and resource
bottlenecks. We present \textbf{FAuNO} -- \emph{Federated Asynchronous Network
Orchestrator} -- a buffered, asynchronous \emph{federated
reinforcement-learning} (FRL) framework for decentralized task offloading in
edge systems. FAuNO adopts an actor-critic architecture in which local actors
learn node-specific dynamics and peer interactions, while a federated critic
aggregates experience across agents to encourage efficient cooperation and
improve overall system performance. Experiments in the \emph{PeersimGym}
environment show that FAuNO consistently matches or exceeds heuristic and
federated multi-agent RL baselines in reducing task loss and latency,
underscoring its adaptability to dynamic edge-computing scenarios.

</details>


### [332] [Rethinking Machine Unlearning in Image Generation Models](https://arxiv.org/abs/2506.02761)
*Renyang Liu,Wenjie Feng,Tianwei Zhang,Wei Zhou,Xueqi Cheng,See-Kiong Ng*

Main category: cs.AI

Relevance: 40.0

TL;DR: 该论文提出了图像生成模型遗忘学习（IGMU）的新框架CatIGMU和评估工具EvalIGMU，并构建了数据集DataIGM，以解决现有方法在任务分类、评估标准和数据集上的不足。


<details>
  <summary>Details</summary>
Motivation: 随着图像生成模型的广泛应用，数据隐私和内容安全成为重要问题，但现有遗忘学习方法在任务分类、评估框架和数据集方面存在显著缺陷。

Method: 设计了任务分类框架CatIGMU、评估框架EvalIGMU，并构建了数据集DataIGM，用于全面评估和改进IGMU算法。

Result: 发现现有IGMU算法在多维度评估中表现不佳，尤其在保留性和鲁棒性方面。

Conclusion: 提出的框架和数据集为IGMU的标准化和可靠评估提供了工具，揭示了现有算法的局限性。

Abstract: With the surge and widespread application of image generation models, data
privacy and content safety have become major concerns and attracted great
attention from users, service providers, and policymakers. Machine unlearning
(MU) is recognized as a cost-effective and promising means to address these
challenges. Despite some advancements, image generation model unlearning (IGMU)
still faces remarkable gaps in practice, e.g., unclear task discrimination and
unlearning guidelines, lack of an effective evaluation framework, and
unreliable evaluation metrics. These can hinder the understanding of unlearning
mechanisms and the design of practical unlearning algorithms. We perform
exhaustive assessments over existing state-of-the-art unlearning algorithms and
evaluation standards, and discover several critical flaws and challenges in
IGMU tasks. Driven by these limitations, we make several core contributions, to
facilitate the comprehensive understanding, standardized categorization, and
reliable evaluation of IGMU. Specifically, (1) We design CatIGMU, a novel
hierarchical task categorization framework. It provides detailed implementation
guidance for IGMU, assisting in the design of unlearning algorithms and the
construction of testbeds. (2) We introduce EvalIGMU, a comprehensive evaluation
framework. It includes reliable quantitative metrics across five critical
aspects. (3) We construct DataIGM, a high-quality unlearning dataset, which can
be used for extensive evaluations of IGMU, training content detectors for
judgment, and benchmarking the state-of-the-art unlearning algorithms. With
EvalIGMU and DataIGM, we discover that most existing IGMU algorithms cannot
handle the unlearning well across different evaluation dimensions, especially
for preservation and robustness. Code and models are available at
https://github.com/ryliu68/IGMU.

</details>


### [333] [TaxAgent: How Large Language Model Designs Fiscal Policy](https://arxiv.org/abs/2506.02838)
*Jizhou Wang,Xiaodan Fang,Lei Huang,Yongfeng Huang*

Main category: cs.AI

Relevance: 40.0

TL;DR: TaxAgent结合大型语言模型（LLMs）和基于代理的建模（ABM），设计自适应税收政策，优于传统方法。


<details>
  <summary>Details</summary>
Motivation: 解决传统税收系统缺乏适应性和纳税人异质性问题。

Method: 使用LLMs和ABM模拟纳税人行为并优化税率。

Result: TaxAgent在公平与效率的权衡上优于现有方法。

Conclusion: 提供了一种新颖的税收解决方案和政策评估框架。

Abstract: Economic inequality is a global challenge, intensifying disparities in
education, healthcare, and social stability. Traditional systems like the U.S.
federal income tax reduce inequality but lack adaptability. Although models
like the Saez Optimal Taxation adjust dynamically, they fail to address
taxpayer heterogeneity and irrational behavior. This study introduces TaxAgent,
a novel integration of large language models (LLMs) with agent-based modeling
(ABM) to design adaptive tax policies. In our macroeconomic simulation,
heterogeneous H-Agents (households) simulate real-world taxpayer behaviors
while the TaxAgent (government) utilizes LLMs to iteratively optimize tax
rates, balancing equity and productivity. Benchmarked against Saez Optimal
Taxation, U.S. federal income taxes, and free markets, TaxAgent achieves
superior equity-efficiency trade-offs. This research offers a novel taxation
solution and a scalable, data-driven framework for fiscal policy evaluation.

</details>


### [334] [Surfer-H Meets Holo1: Cost-Efficient Web Agent Powered by Open Weights](https://arxiv.org/abs/2506.02865)
*Mathieu Andreux,Breno Baldas Skuk,Hamza Benchekroun,Emilien Biré,Antoine Bonnet,Riaz Bordie,Matthias Brunel,Pierre-Louis Cedoz,Antoine Chassang,Mickaël Chen,Alexandra D. Constantinou,Antoine d'Andigné,Hubert de La Jonquière,Aurélien Delfosse,Ludovic Denoyer,Alexis Deprez,Augustin Derupti,Michael Eickenberg,Mathïs Federico,Charles Kantor,Xavier Koegler,Yann Labbé,Matthew C. H. Lee,Erwan Le Jumeau de Kergaradec,Amir Mahla,Avshalom Manevich,Adrien Maret,Charles Masson,Rafaël Maurin,Arturo Mena,Philippe Modard,Axel Moyal,Axel Nguyen Kerbel,Julien Revelle,Mats L. Richter,María Santos,Laurent Sifre,Maxime Theillard,Marc Thibault,Louis Thiry,Léo Tronchon,Nicolas Usunier,Tony Wu*

Main category: cs.AI

Relevance: 40.0

TL;DR: Surfer-H是一个结合Vision-Language Models (VLM)的高效网络代理，用于执行用户定义的任务。它搭配Holo1（一种专用于网络导航和信息提取的VLM），在WebVoyager上达到92.2%的SOTA性能。


<details>
  <summary>Details</summary>
Motivation: 开发一个成本高效的网络代理，结合VLM技术，提升网络任务执行的准确性和效率。

Method: 使用Holo1（基于开放权重VLM集合）进行训练，数据来源包括开放网络内容、合成示例和自生成代理数据。

Result: Holo1在UI基准测试和WebClick基准测试中表现优异，Surfer-H在WebVoyager上达到92.2%的SOTA性能。

Conclusion: 通过开源WebClick数据集和Holo1模型权重，推动代理系统研究的发展。

Abstract: We present Surfer-H, a cost-efficient web agent that integrates
Vision-Language Models (VLM) to perform user-defined tasks on the web. We pair
it with Holo1, a new open-weight collection of VLMs specialized in web
navigation and information extraction. Holo1 was trained on carefully curated
data sources, including open-access web content, synthetic examples, and
self-produced agentic data. Holo1 tops generalist User Interface (UI)
benchmarks as well as our new web UI localization benchmark, WebClick. When
powered by Holo1, Surfer-H achieves a 92.2% state-of-the-art performance on
WebVoyager, striking a Pareto-optimal balance between accuracy and
cost-efficiency. To accelerate research advancement in agentic systems, we are
open-sourcing both our WebClick evaluation dataset and the Holo1 model weights.

</details>


### [335] [eACGM: Non-instrumented Performance Tracing and Anomaly Detection towards Machine Learning Systems](https://arxiv.org/abs/2506.02007)
*Ruilin Xu,Zongxuan Xie,Pengfei Chen*

Main category: cs.DC

Relevance: 40.0

TL;DR: eACGM是一个基于eBPF的全栈AI/ML系统监控框架，无需代码修改即可实时收集硬件和软件性能数据，并通过GMM分析识别异常行为，支持大规模系统的性能优化和故障诊断。


<details>
  <summary>Details</summary>
Motivation: 随着AI/ML系统规模的扩大，性能监控和故障诊断变得复杂且耗时。eACGM旨在提供一种非侵入式、低开销的解决方案，以实时捕捉系统异常并优化性能。

Method: eACGM利用eBPF技术收集GPU、网络等硬件和CUDA、PyTorch等软件的性能数据，结合GMM进行统计建模和聚类分析，识别复杂故障模式。

Result: 在多节点分布式训练场景中，eACGM成功捕捉到关键性能异常，验证了其稳定性和可扩展性。

Conclusion: eACGM为大规模AI/ML系统提供了高效的性能监控和故障诊断工具，适用于实际生产环境。

Abstract: We present eACGM, a full-stack AI/ML system monitoring framework based on
eBPF. eACGM collects real-time performance data from key hardware components,
including the GPU and network communication layer, as well as from key software
stacks such as CUDA, Python, and PyTorch, all without requiring any code
instrumentation or modifications. Additionally, it leverages libnvml to gather
process-level GPU resource usage information. By applying a Gaussian Mixture
Model (GMM) to the collected multidimensional performance metrics for
statistical modeling and clustering analysis, eACGM effectively identifies
complex failure modes, such as latency anomalies, hardware failures, and
communication inefficiencies, enabling rapid diagnosis of system bottlenecks
and abnormal behaviors.
  To evaluate eACGM's effectiveness and practicality, we conducted extensive
empirical studies and case analyses in multi-node distributed training
scenarios. The results demonstrate that eACGM, while maintaining a
non-intrusive and low-overhead profile, successfully captures critical
performance anomalies during model training and inference. Its stable anomaly
detection performance and comprehensive monitoring capabilities validate its
applicability and scalability in real-world production environments, providing
strong support for performance optimization and fault diagnosis in large-scale
AI/ML systems.

</details>


### [336] [Towards Secure MLOps: Surveying Attacks, Mitigation Strategies, and Research Challenges](https://arxiv.org/abs/2506.02032)
*Raj Patel,Himanshu Tripathi,Jasper Stone,Noorbakhsh Amiri Golilarz,Sudip Mittal,Shahram Rahimi,Vini Chaudhary*

Main category: cs.CR

Relevance: 40.0

TL;DR: 论文探讨了MLOps生态系统的安全漏洞，并应用MITRE ATLAS框架系统评估攻击，提出防御策略。


<details>
  <summary>Details</summary>
Motivation: 随着MLOps的广泛应用，其统一性带来了安全漏洞，可能导致严重后果，如数据污染和财务损失。

Method: 应用MITRE ATLAS框架，分析MLOps各阶段的攻击技术，并提出对应的防御策略。

Result: 提出了攻击技术的分类和早期防御策略，强调了MLOps安全的重要性。

Conclusion: 强调从初期实施安全协议的重要性，并指出了未来研究方向。

Abstract: The rapid adoption of machine learning (ML) technologies has driven
organizations across diverse sectors to seek efficient and reliable methods to
accelerate model development-to-deployment. Machine Learning Operations (MLOps)
has emerged as an integrative approach addressing these requirements by
unifying relevant roles and streamlining ML workflows. As the MLOps market
continues to grow, securing these pipelines has become increasingly critical.
However, the unified nature of MLOps ecosystem introduces vulnerabilities,
making them susceptible to adversarial attacks where a single misconfiguration
can lead to compromised credentials, severe financial losses, damaged public
trust, and the poisoning of training data. Our paper presents a systematic
application of the MITRE ATLAS (Adversarial Threat Landscape for
Artificial-Intelligence Systems) framework, a comprehensive and continuously
updated catalog of AI-focused attacks, to systematically assess attacks across
different phases of the MLOps ecosystem. We begin by examining the preparatory
phases during which adversaries acquire the essential intelligence required to
initiate their attacks. We then present a structured taxonomy of attack
techniques explicitly mapped to corresponding phases of the MLOps ecosystem,
supported by examples drawn from red-teaming exercises and real-world
incidents. This is followed by a taxonomy of mitigation strategies aligned with
these attack categories, offering actionable early-stage defenses to strengthen
the security of MLOps ecosystem. Given the rapid evolution and adoption of
MLOps, we further highlight key research gaps that require immediate attention.
Our work emphasizes the importance of implementing robust security protocols
from the outset, empowering practitioners to safeguard MLOps ecosystem against
evolving cyber attacks.

</details>


### [337] [Machine vs Machine: Using AI to Tackle Generative AI Threats in Assessment](https://arxiv.org/abs/2506.02046)
*Mohammad Saleh Torkestani,Taha Mansouri*

Main category: cs.CY

Relevance: 40.0

TL;DR: 提出了一种理论框架，通过机器对机器的方法解决生成式AI在高等教育评估中的挑战，结合静态分析和动态测试来评估评估漏洞。


<details>
  <summary>Details</summary>
Motivation: 生成式AI（如GPT-4、Claude、Llama）能生成复杂的学术内容，威胁传统评估方法，现有应对措施（如检测工具、人工评估）存在局限性。

Method: 提出双策略范式，包括静态分析（8个理论元素）和动态测试（模拟漏洞评估），并设计漏洞评分理论框架。

Result: 理论框架提供了区分真实人类学习和AI生成内容的方法，并支持定量评估漏洞。

Conclusion: 该框架为高等教育评估提供了一种新的理论方法，以应对生成式AI的挑战。

Abstract: This paper presents a theoretical framework for addressing the challenges
posed by generative artificial intelligence (AI) in higher education assessment
through a machine-versus-machine approach. Large language models like GPT-4,
Claude, and Llama increasingly demonstrate the ability to produce sophisticated
academic content, traditional assessment methods face an existential threat,
with surveys indicating 74-92% of students experimenting with these tools for
academic purposes. Current responses, ranging from detection software to manual
assessment redesign, show significant limitations: detection tools demonstrate
bias against non-native English writers and can be easily circumvented, while
manual frameworks rely heavily on subjective judgment and assume static AI
capabilities. This paper introduces a dual strategy paradigm combining static
analysis and dynamic testing to create a comprehensive theoretical framework
for assessment vulnerability evaluation. The static analysis component
comprises eight theoretically justified elements: specificity and
contextualization, temporal relevance, process visibility requirements,
personalization elements, resource accessibility, multimodal integration,
ethical reasoning requirements, and collaborative elements. Each element
addresses specific limitations in generative AI capabilities, creating barriers
that distinguish authentic human learning from AI-generated simulation. The
dynamic testing component provides a complementary approach through
simulation-based vulnerability assessment, addressing limitations in
pattern-based analysis. The paper presents a theoretical framework for
vulnerability scoring, including the conceptual basis for quantitative
assessment, weighting frameworks, and threshold determination theory.

</details>


### [338] [Protap: A Benchmark for Protein Modeling on Realistic Downstream Applications](https://arxiv.org/abs/2506.02052)
*Shuo Yan,Yuliang Yan,Bin Ma,Chenao Li,Haochun Tang,Jiahua Lu,Minhua Lin,Yuyuan Feng,Hui Xiong,Enyan Dai*

Main category: q-bio.BM

Relevance: 40.0

TL;DR: Protap是一个蛋白质应用的综合基准，比较了不同架构、预训练策略和领域特定模型在多种下游任务中的表现。


<details>
  <summary>Details</summary>
Motivation: 现有蛋白质应用的基准缺乏对工业相关任务的覆盖，Protap旨在填补这一空白。

Method: Protap比较了不同架构和预训练策略在五个蛋白质应用任务中的表现，包括两个新颖的工业相关任务。

Result: 研究发现大规模预训练编码器在小数据集上可能表现不佳，结构信息和领域先验能显著提升性能。

Conclusion: Protap为蛋白质应用提供了全面的评估工具，强调了结构信息和领域知识的重要性。

Abstract: Recently, extensive deep learning architectures and pretraining strategies
have been explored to support downstream protein applications. Additionally,
domain-specific models incorporating biological knowledge have been developed
to enhance performance in specialized tasks. In this work, we introduce
$\textbf{Protap}$, a comprehensive benchmark that systematically compares
backbone architectures, pretraining strategies, and domain-specific models
across diverse and realistic downstream protein applications. Specifically,
Protap covers five applications: three general tasks and two novel specialized
tasks, i.e., enzyme-catalyzed protein cleavage site prediction and targeted
protein degradation, which are industrially relevant yet missing from existing
benchmarks. For each application, Protap compares various domain-specific
models and general architectures under multiple pretraining settings. Our
empirical studies imply that: (i) Though large-scale pretraining encoders
achieve great results, they often underperform supervised encoders trained on
small downstream training sets. (ii) Incorporating structural information
during downstream fine-tuning can match or even outperform protein language
models pretrained on large-scale sequence corpora. (iii) Domain-specific
biological priors can enhance performance on specialized downstream tasks. Code
and datasets are publicly available at
https://github.com/Trust-App-AI-Lab/protap.

</details>


### [339] [Will Agents Replace Us? Perceptions of Autonomous Multi-Agent AI](https://arxiv.org/abs/2506.02055)
*Nikola Balic*

Main category: cs.CY

Relevance: 40.0

TL;DR: 该研究通过调查130名参与者，探讨了AI代理在软件开发中的能力、影响和治理问题，发现部署决策复杂且缺乏显著预测因素。


<details>
  <summary>Details</summary>
Motivation: 了解专业人士对AI代理的当前看法，以预测采用挑战、伦理问题和未来劳动力发展。

Method: 通过调查分析130名参与者的反馈，探索AI替代程序员的时间表、部署障碍和责任归属。

Result: 发现三个不同的受访者群体，但初始逻辑回归模型未找到显著预测因素，表明部署决策复杂。

Conclusion: 组织需解决合规问题并建立明确的治理框架以整合自主代理。

Abstract: Autonomous multi-agent AI systems are poised to transform various industries,
particularly software development and knowledge work. Understanding current
perceptions among professionals is crucial for anticipating adoption
challenges, ethical considerations, and future workforce development. This
study analyzes responses from 130 participants to a survey on the capabilities,
impact, and governance of AI agents. We explore expected timelines for AI
replacing programmers, identify perceived barriers to deployment, and examine
beliefs about responsibility when agents make critical decisions. Key findings
reveal three distinct clusters of respondents. While the study explored factors
associated with current AI agent deployment, the initial logistic regression
model did not yield statistically significant predictors, suggesting that
deployment decisions are complex and may be influenced by factors not fully
captured or that a larger sample is needed. These insights highlight the need
for organizations to address compliance concerns (a commonly cited barrier) and
establish clear governance frameworks as they integrate autonomous agents into
their workflows.

</details>


### [340] [Enhancing Speech Instruction Understanding and Disambiguation in Robotics via Speech Prosody](https://arxiv.org/abs/2506.02057)
*David Sasu,Kweku Andoh Yamoah,Benedict Quartey,Natalie Schluter*

Main category: cs.RO

Relevance: 40.0

TL;DR: 提出一种利用语音韵律直接推断指令意图的新方法，结合大语言模型解决机器人任务计划歧义，并发布首个机器人歧义语音数据集。


<details>
  <summary>Details</summary>
Motivation: 传统语音识别方法丢弃了关键的韵律线索，导致意图解析不准确。本文旨在通过直接利用韵律信息提高机器人对语音指令的理解能力。

Method: 提出直接利用语音韵律推断意图的方法，并通过大语言模型进行上下文学习以消歧和选择任务计划。同时发布首个机器人歧义语音数据集。

Result: 方法在检测指称意图时达到95.79%的准确率，在歧义指令任务计划选择中达到71.96%的准确率。

Conclusion: 该方法显著提升了人机交互中语音指令的解析能力，为未来研究提供了新方向。

Abstract: Enabling robots to accurately interpret and execute spoken language
instructions is essential for effective human-robot collaboration. Traditional
methods rely on speech recognition to transcribe speech into text, often
discarding crucial prosodic cues needed for disambiguating intent. We propose a
novel approach that directly leverages speech prosody to infer and resolve
instruction intent. Predicted intents are integrated into large language models
via in-context learning to disambiguate and select appropriate task plans.
Additionally, we present the first ambiguous speech dataset for robotics,
designed to advance research in speech disambiguation. Our method achieves
95.79% accuracy in detecting referent intents within an utterance and
determines the intended task plan of ambiguous instructions with 71.96%
accuracy, demonstrating its potential to significantly improve human-robot
communication.

</details>


### [341] [SALF-MOS: Speaker Agnostic Latent Features Downsampled for MOS Prediction](https://arxiv.org/abs/2506.02082)
*Saurabh Agrawal,Raj Gohil,Gopal Kumar Agrawal,Vikram C M,Kushal Verma*

Main category: cs.SD

Relevance: 40.0

TL;DR: 提出了一种名为SALF-MOS的新型模型，用于预测语音合成的MOS评分，解决了传统主观评估耗时的问题。


<details>
  <summary>Details</summary>
Motivation: 传统语音质量评估方法中，主观评分（如MOS）可靠但耗时，而客观指标无法有效选择最佳模型。

Method: 使用卷积序列堆叠提取音频样本的潜在特征，构建小型、端到端、高泛化性的SALF-MOS模型。

Result: 基于MSE、LCC、SRCC和KTAU指标，模型实现了最先进的性能。

Conclusion: SALF-MOS为语音合成评估提供了高效且可靠的解决方案。

Abstract: Speech quality assessment is a critical process in selecting text-to-speech
synthesis (TTS) or voice conversion models. Evaluation of voice synthesis can
be done using objective metrics or subjective metrics. Although there are many
objective metrics like the Perceptual Evaluation of Speech Quality (PESQ),
Perceptual Objective Listening Quality Assessment (POLQA) or Short-Time
Objective Intelligibility (STOI) but none of them is feasible in selecting the
best model. On the other hand subjective metric like Mean Opinion Score is
highly reliable but it requires a lot of manual efforts and are time-consuming.
To counter the issues in MOS Evaluation, we have developed a novel model,
Speaker Agnostic Latent Features (SALF)-Mean Opinion Score (MOS) which is a
small-sized, end-to-end, highly generalized and scalable model for predicting
MOS score on a scale of 5. We use the sequences of convolutions and stack them
to get the latent features of the audio samples to get the best
state-of-the-art results based on mean squared error (MSE), Linear Concordance
Correlation coefficient (LCC), Spearman Rank Correlation Coefficient (SRCC) and
Kendall Rank Correlation Coefficient (KTAU).

</details>


### [342] [LASPA: Language Agnostic Speaker Disentanglement with Prefix-Tuned Cross-Attention](https://arxiv.org/abs/2506.02083)
*Aditya Srinivas Menon,Raj Prakash Gohil,Kumud Tripathi,Pankaj Wasnik*

Main category: cs.SD

Relevance: 40.0

TL;DR: 论文提出了一种新的解耦学习策略，通过前缀调优的跨注意力机制分离语音和语言信息，提升多语言场景下的说话人识别准确性。


<details>
  <summary>Details</summary>
Motivation: 在多语言环境中，说话人识别模型因语音和语言信息的纠缠而面临挑战，解耦这些信息可以显著提高识别准确性。

Method: 采用前缀调优的跨注意力机制进行联合学习，特别适用于说话人在不同语言间切换的场景。

Result: 实验表明，模型在单语言和多语言（包括未见语言）场景中均表现良好，显著降低了等错误率。

Conclusion: 该方法有效分离了语言和说话人信息，提升了多语言条件下的识别性能。

Abstract: Speaker recognition models face challenges in multi-lingual settings due to
the entanglement of linguistic information within speaker embeddings. The
overlap between vocal traits such as accent, vocal anatomy, and a language's
phonetic structure complicates separating linguistic and speaker information.
Disentangling these components can significantly improve speaker recognition
accuracy. To this end, we propose a novel disentanglement learning strategy
that integrates joint learning through prefix-tuned cross-attention. This
approach is particularly effective when speakers switch between languages.
Experimental results show the model generalizes across monolingual and
multi-lingual settings, including unseen languages. Notably, the proposed model
improves the equal error rate across multiple datasets, highlighting its
ability to separate language information from speaker embeddings and enhance
recognition in diverse linguistic conditions.

</details>


### [343] [Unveiling Audio Deepfake Origins: A Deep Metric learning And Conformer Network Approach With Ensemble Fusion](https://arxiv.org/abs/2506.02085)
*Ajinkya Kulkarni,Sandipana Dowerah,Tanel Alumae,Mathew Magimai. -Doss*

Main category: cs.SD

Relevance: 40.0

TL;DR: 提出了一种结合深度度量多类N-pair损失、Real Emphasis和Fake Dispersion框架、Conformer分类网络以及集成分数-嵌入融合的音频源追踪系统，用于区分真实和伪造语音。


<details>
  <summary>Details</summary>
Motivation: 随着AI技术的发展，音频深度伪造的逼真度越来越高，当前研究主要集中在区分真实与伪造语音，而追踪源系统同样重要。

Method: 结合深度度量多类N-pair损失、Real Emphasis和Fake Dispersion框架、Conformer分类网络以及集成分数-嵌入融合。

Result: 在源追踪任务中表现优于基线系统，通过Frechet距离和标准指标验证。

Conclusion: 提出的方法在区分真实与伪造语音及追踪源系统方面表现出色，尤其在域内外场景中取得平衡。

Abstract: Audio deepfakes are acquiring an unprecedented level of realism with advanced
AI. While current research focuses on discerning real speech from spoofed
speech, tracing the source system is equally crucial. This work proposes a
novel audio source tracing system combining deep metric multi-class N-pair loss
with Real Emphasis and Fake Dispersion framework, a Conformer classification
network, and ensemble score-embedding fusion. The N-pair loss improves
discriminative ability, while Real Emphasis and Fake Dispersion enhance
robustness by focusing on differentiating real and fake speech patterns. The
Conformer network captures both global and local dependencies in the audio
signal, crucial for source tracing. The proposed ensemble score-embedding
fusion shows an optimal trade-off between in-domain and out-of-domain source
tracing scenarios. We evaluate our method using Frechet Distance and standard
metrics, demonstrating superior performance in source tracing over the baseline
system.

</details>


### [344] [TransAct V2: Lifelong User Action Sequence Modeling on Pinterest Recommendation](https://arxiv.org/abs/2506.02267)
*Xue Xia,Saurabh Vishwas Joshi,Kousik Rajesh,Kangnan Li,Yangyi Lu,Nikil Pancha,Dhruvil Deven Badani,Jiajing Xu,Pong Eksombatchai*

Main category: cs.IR

Relevance: 40.0

TL;DR: TransAct V2是Pinterest的Homefeed排序系统模型，通过长用户序列、Next Action Loss函数和高效部署方案提升CTR预测。


<details>
  <summary>Details</summary>
Motivation: 工业推荐系统通常依赖短用户序列，缺乏长期行为捕捉和集成动作预测任务，且基础设施挑战未解决。

Method: 1) 使用长用户序列；2) 集成Next Action Loss；3) 高效部署方案。

Result: 提升了CTR预测能力和用户动作预测。

Conclusion: TransAct V2通过长序列和高效部署解决了工业推荐系统的局限性。

Abstract: Modeling user action sequences has become a popular focus in industrial
recommendation system research, particularly for Click-Through Rate (CTR)
prediction tasks. However, industry-scale CTR models often rely on short user
sequences, limiting their ability to capture long-term behavior. Additionally,
these models typically lack an integrated action-prediction task within a
point-wise ranking framework, reducing their predictive power. They also rarely
address the infrastructure challenges involved in efficiently serving
large-scale sequential models. In this paper, we introduce TransAct V2, a
production model for Pinterest's Homefeed ranking system, featuring three key
innovations: (1) leveraging very long user sequences to improve CTR
predictions, (2) integrating a Next Action Loss function for enhanced user
action forecasting, and (3) employing scalable, low-latency deployment
solutions tailored to handle the computational demands of extended user action
sequences.

</details>


### [345] [Automated Web Application Testing: End-to-End Test Case Generation with Large Language Models and Screen Transition Graphs](https://arxiv.org/abs/2506.02529)
*Nguyen-Khang Le,Quan Minh Bui,Minh Ngoc Nguyen,Hiep Nguyen,Trung Vo,Son T. Luu,Shoshin Nomura,Minh Le Nguyen*

Main category: cs.SE

Relevance: 40.0

TL;DR: 论文提出了一种结合图结构和LLMs的自动化系统，用于生成Web应用程序的测试用例，重点关注站点导航和表单填写。


<details>
  <summary>Details</summary>
Motivation: Web应用的可靠性测试因界面复杂性和动态性而具有挑战性，LLMs在自动化任务中表现潜力但仍有限制。

Method: 系统使用屏幕转换图和LLMs建模导航流程，状态图处理条件表单，并自动化生成Selenium脚本。

Result: 实验证明系统能显著提高测试覆盖率和鲁棒性。

Conclusion: 该系统推动了Web应用测试的进步，尤其在动态导航和复杂表单交互方面。

Abstract: Web applications are critical to modern software ecosystems, yet ensuring
their reliability remains challenging due to the complexity and dynamic nature
of web interfaces. Recent advances in large language models (LLMs) have shown
promise in automating complex tasks, but limitations persist in handling
dynamic navigation flows and complex form interactions. This paper presents an
automated system for generating test cases for two key aspects of web
application testing: site navigation and form filling. For site navigation, the
system employs screen transition graphs and LLMs to model navigation flows and
generate test scenarios. For form filling, it uses state graphs to handle
conditional forms and automates Selenium script generation. Key contributions
include: (1) a novel integration of graph structures and LLMs for site
navigation testing, (2) a state graph-based approach for automating
form-filling test cases, and (3) a comprehensive dataset for evaluating
form-interaction testing. Experimental results demonstrate the system's
effectiveness in improving test coverage and robustness, advancing the state of
web application testing.

</details>


### [346] [MISLEADER: Defending against Model Extraction with Ensembles of Distilled Models](https://arxiv.org/abs/2506.02362)
*Xueqi Cheng,Minxing Zheng,Shixiang Zhu,Yushun Dong*

Main category: cs.CR

Relevance: 40.0

TL;DR: 论文提出了一种名为MISLEADER的新型防御策略，用于对抗模型提取攻击，不依赖OOD假设，通过双层优化和异构蒸馏模型提升防御效果。


<details>
  <summary>Details</summary>
Motivation: 模型提取攻击威胁MLaaS提供商的知识产权，现有防御方法依赖OOD假设，但在现实场景中效果有限。

Method: MISLEADER通过双层优化问题设计防御策略，结合数据增强和异构蒸馏模型，提供近似算法和理论误差界。

Result: 实验验证了MISLEADER在保留预测准确性的同时有效抵抗模型提取攻击。

Conclusion: MISLEADER是一种实用且高效的防御策略，适用于现实场景。

Abstract: Model extraction attacks aim to replicate the functionality of a black-box
model through query access, threatening the intellectual property (IP) of
machine-learning-as-a-service (MLaaS) providers. Defending against such attacks
is challenging, as it must balance efficiency, robustness, and utility
preservation in the real-world scenario. Despite the recent advances, most
existing defenses presume that attacker queries have out-of-distribution (OOD)
samples, enabling them to detect and disrupt suspicious inputs. However, this
assumption is increasingly unreliable, as modern models are trained on diverse
datasets and attackers often operate under limited query budgets. As a result,
the effectiveness of these defenses is significantly compromised in realistic
deployment scenarios. To address this gap, we propose MISLEADER (enseMbles of
dIStiLled modEls Against moDel ExtRaction), a novel defense strategy that does
not rely on OOD assumptions. MISLEADER formulates model protection as a bilevel
optimization problem that simultaneously preserves predictive fidelity on
benign inputs and reduces extractability by potential clone models. Our
framework combines data augmentation to simulate attacker queries with an
ensemble of heterogeneous distilled models to improve robustness and diversity.
We further provide a tractable approximation algorithm and derive theoretical
error bounds to characterize defense effectiveness. Extensive experiments
across various settings validate the utility-preserving and
extraction-resistant properties of our proposed defense strategy. Our code is
available at https://github.com/LabRAI/MISLEADER.

</details>


### [347] [Prompt-Unseen-Emotion: Zero-shot Expressive Speech Synthesis with Prompt-LLM Contextual Knowledge for Mixed Emotions](https://arxiv.org/abs/2506.02742)
*Xiaoxue Gao,Huayun Zhang,Nancy F. Chen*

Main category: eess.AS

Relevance: 40.0

TL;DR: 论文提出了一种名为PUE的方法，通过情感引导的提示学习生成未见过的情感语音，利用LLM-TTS架构实现情感一致性，并在零样本设置下成功合成未见情感。


<details>
  <summary>Details</summary>
Motivation: 现有TTS系统仅能建模有限的情感类别，而人类对话涉及更广泛的情感，因此需要探索更多样化的情感语音生成以实现更自然的交互。

Method: 采用LLM-TTS架构，通过情感引导的提示学习（PUE）定量捕捉每句话的情感权重，并在推理阶段灵活调整情感比例以生成混合情感语音。

Result: PUE在零样本设置下成功合成了未见过的情感语音，实现了情感风格的量化。

Conclusion: PUE方法为多样化情感语音生成提供了有效解决方案，扩展了TTS系统的表达能力。

Abstract: Existing expressive text-to-speech (TTS) systems primarily model a limited
set of categorical emotions, whereas human conversations extend far beyond
these predefined emotions, making it essential to explore more diverse
emotional speech generation for more natural interactions. To bridge this gap,
this paper proposes a novel prompt-unseen-emotion (PUE) approach to generate
unseen emotional speech via emotion-guided prompt learning. PUE is trained
utilizing an LLM-TTS architecture to ensure emotional consistency between
categorical emotion-relevant prompts and emotional speech, allowing the model
to quantitatively capture different emotion weightings per utterance. During
inference, mixed emotional speech can be generated by flexibly adjusting
emotion proportions and leveraging LLM contextual knowledge, enabling the model
to quantify different emotional styles. Our proposed PUE successfully
facilitates expressive speech synthesis of unseen emotions in a zero-shot
setting.

</details>


### [348] [Solving the Pod Repositioning Problem with Deep Reinforced Adaptive Large Neighborhood Search](https://arxiv.org/abs/2506.02746)
*Lin Xie,Hanyi Li*

Main category: cs.RO

Relevance: 40.0

TL;DR: 论文提出了一种结合自适应大邻域搜索（ALNS）和深度强化学习（DRL）的方法，用于解决机器人移动履行系统（RMFS）中的Pod重新定位问题（PRP），显著优于传统方法。


<details>
  <summary>Details</summary>
Motivation: PRP是RMFS中的关键问题，传统方法在动态环境中表现不佳，因此需要一种更灵活、学习驱动的解决方案。

Method: 通过DRL动态选择ALNS的破坏和修复操作符，并调整关键参数（如破坏程度和接受阈值），同时设计了针对PRP特性的启发式方法。

Result: 实验结果表明，该方法在解质量和效率上优于传统方法（如最便宜位置、固定位置、二进制整数规划和静态启发式）。

Conclusion: 该方法展示了学习驱动控制在组合优化中的优势，尤其适用于动态仓库系统。

Abstract: The Pod Repositioning Problem (PRP) in Robotic Mobile Fulfillment Systems
(RMFS) involves selecting optimal storage locations for pods returning from
pick stations. This work presents an improved solution method that integrates
Adaptive Large Neighborhood Search (ALNS) with Deep Reinforcement Learning
(DRL). A DRL agent dynamically selects destroy and repair operators and adjusts
key parameters such as destruction degree and acceptance thresholds during the
search. Specialized heuristics for both operators are designed to reflect
PRP-specific characteristics, including pod usage frequency and movement costs.
Computational results show that this DRL-guided ALNS outperforms traditional
approaches such as cheapest-place, fixed-place, binary integer programming, and
static heuristics. The method demonstrates strong solution quality and
illustrating the benefit of learning-driven control within combinatorial
optimization for warehouse systems.

</details>


### [349] [DeepShop: A Benchmark for Deep Research Shopping Agents](https://arxiv.org/abs/2506.02839)
*Yougang Lyu,Xiaoyu Zhang,Lingyong Yan,Maarten de Rijke,Zhaochun Ren,Xiuying Chen*

Main category: cs.IR

Relevance: 40.0

TL;DR: DeepShop是一个用于评估复杂在线购物环境中网络代理的基准测试，通过多样化和复杂化的查询设计，以及细粒度和整体评估框架，揭示了现有方法在处理复杂查询时的局限性。


<details>
  <summary>Details</summary>
Motivation: 现有的在线购物代理评估基准过于简单，无法反映真实购物场景的复杂性，因此需要更全面的评估工具。

Method: DeepShop通过查询多样性和复杂性演化，设计了三个层次的查询（简单、中等、困难），并提出了细粒度和整体评估框架。

Result: RAG方法因缺乏网络交互能力在复杂查询中表现不佳，其他方法在过滤器和排序偏好方面也面临挑战，整体成功率较低。

Conclusion: DeepShop为深度研究购物代理提供了有效的评估工具，揭示了现有方法的不足，并支持未来改进。

Abstract: Web agents for online shopping have shown great promise in automating user
interactions across e-commerce platforms. Benchmarks for assessing such agents
do not reflect the complexity of real-world shopping scenarios, as they often
consist of overly simple queries with deterministic paths, such as "Find iPhone
15." Real shopping scenarios are inherently more layered, involving
multi-dimensional product attributes, search filters, and user-specific sorting
preferences. To address this gap, we introduce DeepShop, a benchmark designed
to evaluate web agents in complex and realistic online shopping environments.
DeepShop comprises three key components. (1) Query diversity evolution:
Starting from real user queries, we generate diverse queries across five
popular online shopping domains. (2) Query complexity evolution: We further
evolve these queries to increase complexity, considering product attributes,
search filters, and sorting preferences, and classify them into three levels:
easy, medium, and hard, based on the number of evolutions. (3) Fine-grained and
holistic evaluation: We propose an automated evaluation framework that assesses
agent performance in terms of fine-grained aspects (product attributes, search
filters, and sorting preferences) and reports the overall success rate through
holistic evaluation. We conduct a systematic evaluation of retrieval-augmented
generation (RAG) methods, web agents, and deep research systems. Results show
that RAG struggles with complex queries due to its lack of web interaction,
while other methods face significant challenges with filters and sorting
preferences, leading to low overall success rates. We also perform
cross-category, complexity-based evaluations and error analyses to support the
advancement of deep research shopping agents.

</details>


### [350] [DGMO: Training-Free Audio Source Separation through Diffusion-Guided Mask Optimization](https://arxiv.org/abs/2506.02858)
*Geonyoung Lee,Geonhee Han,Paul Hongsuck Seo*

Main category: cs.SD

Relevance: 40.0

TL;DR: 论文提出了一种无需训练的零样本语言查询音频源分离方法，利用预训练扩散模型的生成先验，通过测试时优化框架实现高效分离。


<details>
  <summary>Details</summary>
Motivation: 探索预训练扩散模型是否能在无需任务特定训练的情况下完成音频源分离任务，扩展扩散模型的应用范围。

Method: 提出Diffusion-Guided Mask Optimization (DGMO)，一种测试时优化框架，通过优化频谱掩码实现精确分离。

Result: 该方法在零样本设置下实现了与任务特定训练方法竞争的性能。

Conclusion: 该工作为扩散模型在音频分离任务中的应用提供了新范式，展示了其潜力。

Abstract: Language-queried Audio Source Separation (LASS) enables open-vocabulary sound
separation via natural language queries. While existing methods rely on
task-specific training, we explore whether pretrained diffusion models,
originally designed for audio generation, can inherently perform separation
without further training. In this study, we introduce a training-free framework
leveraging generative priors for zero-shot LASS. Analyzing na\"ive adaptations,
we identify key limitations arising from modality-specific challenges.To
address these issues, we propose Diffusion-Guided Mask Optimization (DGMO), a
test-time optimization framework that refines spectrogram masks for precise,
input-aligned separation. Our approach effectively repurposes pretrained
diffusion models for source separation, achieving competitive performance
without task-specific supervision. This work expands the application of
diffusion models beyond generation, establishing a new paradigm for zero-shot
audio separation. The code is available at: https://wltschmrz.github.io/DGMO/

</details>


### [351] [UniConFlow: A Unified Constrained Generalization Framework for Certified Motion Planning with Flow Matching Models](https://arxiv.org/abs/2506.02955)
*Zewen Yang,Xiaobing Dai,Dian Yu,Qianru Li,Yu Li,Valentin Le Mesle*

Main category: cs.RO

Relevance: 40.0

TL;DR: UniConFlow是一个基于流匹配的统一框架，用于机器人轨迹生成，能够系统地处理等式和不等式约束，提升安全性和可行性。


<details>
  <summary>Details</summary>
Motivation: 现有生成模型在同时处理多种约束（如避障和动态一致性）时表现有限，UniConFlow旨在解决这一问题。

Method: 提出一种统一的流匹配框架，结合二次规划生成约束感知的轨迹，无需重新训练或辅助控制器。

Result: 在移动导航和高维操作任务中，UniConFlow在安全性和可行性上优于现有方法。

Conclusion: UniConFlow为约束感知的轨迹生成提供了一种高效且灵活的解决方案。

Abstract: Generative models have become increasingly powerful tools for robot motion
generation, enabling flexible and multimodal trajectory generation across
various tasks. Yet, most existing approaches remain limited in handling
multiple types of constraints, such as collision avoidance and dynamic
consistency, which are often treated separately or only partially considered.
This paper proposes UniConFlow, a unified flow matching (FM) based framework
for trajectory generation that systematically incorporates both equality and
inequality constraints. UniConFlow introduces a novel prescribed-time zeroing
function to enhance flexibility during the inference process, allowing the
model to adapt to varying task requirements. To ensure constraint satisfaction,
particularly with respect to obstacle avoidance, admissible action range, and
kinodynamic consistency, the guidance inputs to the FM model are derived
through a quadratic programming formulation, which enables constraint-aware
generation without requiring retraining or auxiliary controllers. We conduct
mobile navigation and high-dimensional manipulation tasks, demonstrating
improved safety and feasibility compared to state-of-the-art constrained
generative planners. Project page is available at https://uniconflow.github.io.

</details>


### [352] [TalkingMachines: Real-Time Audio-Driven FaceTime-Style Video via Autoregressive Diffusion Models](https://arxiv.org/abs/2506.03099)
*Chetwin Low,Weimin Wang*

Main category: cs.SD

Relevance: 40.0

TL;DR: TalkingMachines是一个高效框架，将预训练的视频生成模型转化为实时音频驱动的角色动画生成器，结合音频LLM和视频生成基础模型，实现自然对话体验。


<details>
  <summary>Details</summary>
Motivation: 将预训练的视频生成模型与音频LLM结合，实现实时、自然的音频驱动角色动画，提升交互体验。

Method: 1. 将SOTA图像到视频DiT模型适配为音频驱动的18B参数模型；2. 通过非对称知识蒸馏实现无限视频流；3. 设计高吞吐、低延迟推理流水线。

Result: 成功实现高效、实时的音频驱动角色动画生成，支持无限视频流。

Conclusion: TalkingMachines为音频驱动的角色动画提供了高效解决方案，结合了LLM和视频生成模型的优势。

Abstract: In this paper, we present TalkingMachines -- an efficient framework that
transforms pretrained video generation models into real-time, audio-driven
character animators. TalkingMachines enables natural conversational experiences
by integrating an audio large language model (LLM) with our video generation
foundation model. Our primary contributions include: (1) We adapt a pretrained
SOTA image-to-video DiT into an audio-driven avatar generation model of 18
billion parameters; (2) We enable infinite video streaming without error
accumulation through asymmetric knowledge distillation from a bidirectional
teacher model into a sparse causal, autoregressive student model; (3) We design
a high-throughput, low-latency inference pipeline incorporating several key
engineering optimizations such as: (a) disaggregation of the DiT and VAE
decoder across separate devices, (b) efficient overlap of inter-device
communication and computation using CUDA streams, (c) elimination of redundant
recomputations to maximize frame-generation throughput. Please see demo videos
here - https://aaxwaz.github.io/TalkingMachines/

</details>


### [353] [Towards Generating Controllable and Solvable Geometry Problem by Leveraging Symbolic Deduction Engine](https://arxiv.org/abs/2506.02565)
*Zhuoxuan Jiang,Tianyang Zhang,Peiyan Peng,Jing Chen,Yinong Xun,Haotian Zhang,Lichi Li,Yong Li,Shaohua Zhang*

Main category: cs.AI

Relevance: 30.0

TL;DR: 提出了一种基于符号推理引擎的几何问题生成框架（SDE-GPG），通过多步骤流程生成高质量、可控的几何问题。


<details>
  <summary>Details</summary>
Motivation: 几何问题生成在教育中具有重要意义，但现有方法难以处理多模态格式和自然语言与形式语言的转换。

Method: 采用符号推理引擎，包含四个步骤：知识点到扩展定义的映射、符号推理、问题筛选、文本和图表生成。

Result: 实验表明，SDE-GPG能有效生成可读、可解且可控的几何问题。

Conclusion: SDE-GPG为几何问题生成提供了一种新方法，解决了现有挑战。

Abstract: Generating high-quality geometry problems is both an important and
challenging task in education. Compared to math word problems, geometry
problems further emphasize multi-modal formats and the translation between
informal and formal languages. In this paper, we introduce a novel task for
geometry problem generation and propose a new pipeline method: the Symbolic
Deduction Engine-based Geometry Problem Generation framework (SDE-GPG). The
framework leverages a symbolic deduction engine and contains four main steps:
(1) searching a predefined mapping table from knowledge points to extended
definitions, (2) sampling extended definitions and performing symbolic
deduction, (3) filtering out unqualified problems, and (4) generating textual
problems and diagrams. Specifically, our method supports to avoid inherent
biases in translating natural language into formal language by designing the
mapping table, and guarantees to control the generated problems in terms of
knowledge points and difficulties by an elaborate checking function. With
obtained formal problems, they are translated to natural language and the
accompanying diagrams are automatically drew by rule-based methods. We conduct
experiments using real-world combinations of knowledge points from two public
datasets. The results demonstrate that the SDE-GPG can effectively generate
readable, solvable and controllable geometry problems.

</details>


### [354] [A Time-Enhanced Data Disentanglement Network for Traffic Flow Forecasting](https://arxiv.org/abs/2506.02609)
*Tianfan Jiang,Mei Wu,Wenchao Weng,Dewen Seng,Yiqian Lin*

Main category: cs.AI

Relevance: 30.0

TL;DR: 论文提出了一种名为TEDDN的时间增强数据解缠网络，用于交通流预测，通过动态图和时间特征提取模块有效解缠复杂交通数据。


<details>
  <summary>Details</summary>
Motivation: 交通流预测因时空动态性和数据依赖性而具有挑战性，现有方法未能充分重视时间信息。

Method: 提出TEDDN网络，将交通数据解缠为稳定模式和趋势，结合动态图和时间特征提取模块。

Result: 在四个真实数据集上的实验验证了方法的优越性。

Conclusion: TEDDN能有效解缠和提取复杂交通信息，提升预测性能。

Abstract: In recent years, traffic flow prediction has become a highlight in the field
of intelligent transportation systems. However, due to the temporal variations
and dynamic spatial correlations of traffic data, traffic prediction remains
highly challenging.Traditional spatiotemporal networks, which rely on
end-to-end training, often struggle to handle the diverse data dependencies of
multiple traffic flow patterns. Additionally, traffic flow variations are
highly sensitive to temporal information changes. Regrettably, other
researchers have not sufficiently recognized the importance of temporal
information.To address these challenges, we propose a novel approach called A
Time-Enhanced Data Disentanglement Network for Traffic Flow Forecasting
(TEDDN). This network disentangles the originally complex and intertwined
traffic data into stable patterns and trends. By flexibly learning temporal and
node information through a dynamic graph enhanced by a temporal feature
extraction module, TEDDN demonstrates significant efficacy in disentangling and
extracting complex traffic information. Experimental evaluations and ablation
studies on four real-world datasets validate the superiority of our method.

</details>


### [355] [Dynamic Programming Techniques for Enhancing Cognitive Representation in Knowledge Tracing](https://arxiv.org/abs/2506.02949)
*Lixiang Xu,Xianwei Ding,Xin Yuan,Richang Hong,Feiping Nie,Enhong Chen,Philip S. Yu*

Main category: cs.AI

Relevance: 30.0

TL;DR: 论文提出了一种基于认知表示动态规划的知识追踪模型（CRDP-KT），通过优化认知表示来提升预测准确性。


<details>
  <summary>Details</summary>
Motivation: 现有知识追踪方法多关注特征增强，但忽视了认知表示的不足和非认知因素的干扰（如猜测和失误），导致预测偏差和建模成本增加。

Method: CRDP-KT采用动态规划算法优化认知表示，结合题目难度和表现间隔，并通过分区优化和加权融合提升可靠性。

Result: 在三个公开数据集上的实验验证了CRDP-KT的有效性。

Conclusion: CRDP-KT通过优化认知表示，显著提升了知识追踪的准确性和系统性。

Abstract: Knowledge Tracing (KT) involves monitoring the changes in a student's
knowledge over time by analyzing their past responses, with the goal of
predicting future performance. However, most existing methods primarily focus
on feature enhancement, while overlooking the deficiencies in cognitive
representation and the ability to express cognition-issues often caused by
interference from non-cognitive factors such as slipping and guessing. This
limitation hampers the ability to capture the continuity and coherence of the
student's cognitive process. As a result, many methods may introduce more
prediction bias and modeling costs due to their inability to maintain cognitive
continuity and coherence. Based on the above discussion, we propose the
Cognitive Representation Dynamic Programming based Knowledge Tracing (CRDP-KT)
model. This model em ploys a dynamic programming algorithm to optimize
cognitive representations based on the difficulty of the questions and the
performance intervals between them. This approach ensures that the cognitive
representation aligns with the student's cognitive patterns, maintaining
overall continuity and coherence. As a result, it provides more accurate and
systematic input features for subsequent model training, thereby minimizing
distortion in the simulation of cognitive states. Additionally, the CRDP-KT
model performs partitioned optimization of cognitive representations to enhance
the reliability of the optimization process. Furthermore, it improves its
ability to express the student's cognition through a weighted fusion of
optimized record representations and re lationships learned from a bipartite
graph. Finally, experiments conducted on three public datasets validate the
effectiveness of the proposed CRDP-KT model.

</details>


### [356] [Surgical Foundation Model Leveraging Compression and Entropy Maximization for Image-Guided Surgical Assistance](https://arxiv.org/abs/2506.01980)
*Lianhao Yin,Ozanan Meireles,Guy Rosman,Daniela Rus*

Main category: eess.IV

Relevance: 30.0

TL;DR: C2E是一种自监督学习框架，利用Kolmogorov复杂度从手术视频中学习紧凑且信息丰富的表示，无需标注数据即可提升编码器性能。


<details>
  <summary>Details</summary>
Motivation: 由于标注成本高，医学领域缺乏大规模标注数据集，而现有自监督方法难以捕获通用的结构和物理信息。

Method: C2E通过熵最大化解码器压缩图像，同时保留临床相关细节，学习紧凑表示。

Result: C2E在多种手术ML任务（如工作流分类、工具-组织交互分类、分割和诊断）中表现出强泛化能力。

Conclusion: C2E展示了自监督学习在手术AI中的潜力，可提升微创手术的效果。

Abstract: Real-time video understanding is critical to guide procedures in minimally
invasive surgery (MIS). However, supervised learning approaches require large,
annotated datasets that are scarce due to annotation efforts that are
prohibitive, e.g., in medical fields. Although self-supervision methods can
address such limitations, current self-supervised methods often fail to capture
structural and physical information in a form that generalizes across tasks. We
propose Compress-to-Explore (C2E), a novel self-supervised framework that
leverages Kolmogorov complexity to learn compact, informative representations
from surgical videos. C2E uses entropy-maximizing decoders to compress images
while preserving clinically relevant details, improving encoder performance
without labeled data. Trained on large-scale unlabeled surgical datasets, C2E
demonstrates strong generalization across a variety of surgical ML tasks, such
as workflow classification, tool-tissue interaction classification,
segmentation, and diagnosis tasks, providing improved performance as a surgical
visual foundation model. As we further show in the paper, the model's internal
compact representation better disentangles features from different structural
parts of images. The resulting performance improvements highlight the yet
untapped potential of self-supervised learning to enhance surgical AI and
improve outcomes in MIS.

</details>


### [357] [Re-experiment Smart: a Novel Method to Enhance Data-driven Prediction of Mechanical Properties of Epoxy Polymers](https://arxiv.org/abs/2506.01994)
*Wanshan Cui,Yejin Jeong,Inwook Song,Gyuri Kim,Minsang Kwon,Donghun Lee*

Main category: cond-mat.soft

Relevance: 30.0

TL;DR: 论文提出了一种通过多算法异常检测和选择性重实验提升数据集质量的方法，以减少聚合物材料预测中的误差。


<details>
  <summary>Details</summary>
Motivation: 解决聚合物材料预测中因数据异常导致的模型误差问题。

Method: 结合多算法异常检测和选择性重实验，构建高质量数据集，并在多种机器学习模型上验证。

Result: 方法显著降低了预测误差（RMSE），仅需重测约5%的数据即可提升准确性。

Conclusion: 数据质量提升对聚合物科学中的机器学习应用至关重要，方法具有可扩展性。

Abstract: Accurate prediction of polymer material properties through data-driven
approaches greatly accelerates novel material development by reducing redundant
experiments and trial-and-error processes. However, inevitable outliers in
empirical measurements can severely skew machine learning results, leading to
erroneous prediction models and suboptimal material designs. To address this
limitation, we propose a novel approach to enhance dataset quality efficiently
by integrating multi-algorithm outlier detection with selective
re-experimentation of unreliable outlier cases. To validate the empirical
effectiveness of the approach, we systematically construct a new dataset
containing 701 measurements of three key mechanical properties: glass
transition temperature ($T_g$), tan $\delta$ peak, and crosslinking density
($v_{c}$). To demonstrate its general applicability, we report the performance
improvements across multiple machine learning models, including Elastic Net,
SVR, Random Forest, and TPOT, to predict the three key properties. Our method
reliably reduces prediction error (RMSE) and significantly improves accuracy
with minimal additional experimental work, requiring only about 5% of the
dataset to be re-measured.These findings highlight the importance of data
quality enhancement in achieving reliable machine learning applications in
polymer science and present a scalable strategy for improving predictive
reliability in materials science.

</details>


### [358] [Inter(sectional) Alia(s): Ambiguity in Voice Agent Identity via Intersectional Japanese Self-Referents](https://arxiv.org/abs/2506.01998)
*Takao Fujii,Katie Seaborn,Madeleine Steeds,Jun Kato*

Main category: cs.HC

Relevance: 30.0

TL;DR: 研究探讨了日本语境下聊天机器人声音与自我指代词对性别化感知的影响，发现声音和代词共同塑造了复杂的身份印象。


<details>
  <summary>Details</summary>
Motivation: 探讨机器人的拟人化伦理问题，尤其是声音和自我指代词如何影响用户对机器人身份的感知。

Method: 通过众包研究，204名日本参与者评估了三种ChatGPT声音和七种自我指代词的效果。

Result: 声音显著影响了性别化感知，而某些自我指代词（如boku和watakushi）通过模糊性避免了性别化。

Conclusion: 研究强调了在语音代理设计中考虑文化敏感性和交叉身份的重要性。

Abstract: Conversational agents that mimic people have raised questions about the
ethics of anthropomorphizing machines with human social identity cues. Critics
have also questioned assumptions of identity neutrality in humanlike agents.
Recent work has revealed that intersectional Japanese pronouns can elicit
complex and sometimes evasive impressions of agent identity. Yet, the role of
other "neutral" non-pronominal self-referents (NPSR) and voice as a socially
expressive medium remains unexplored. In a crowdsourcing study, Japanese
participants (N = 204) evaluated three ChatGPT voices (Juniper, Breeze, and
Ember) using seven self-referents. We found strong evidence of voice gendering
alongside the potential of intersectional self-referents to evade gendering,
i.e., ambiguity through neutrality and elusiveness. Notably, perceptions of age
and formality intersected with gendering as per sociolinguistic theories,
especially boku and watakushi. This work provides a nuanced take on agent
identity perceptions and champions intersectional and culturally-sensitive work
on voice agents.

</details>


### [359] [The End Of Universal Lifelong Identifiers: Identity Systems For The AI Era](https://arxiv.org/abs/2506.02027)
*Shriphani Palakodety*

Main category: cs.CR

Relevance: 30.0

TL;DR: 论文主张通用终身标识符（ULIs）在AI时代存在隐私风险，提出了一种新的密码学框架以满足AI时代的身份系统需求。


<details>
  <summary>Details</summary>
Motivation: ULIs在AI时代带来系统性隐私风险，传统保护措施已不足。

Method: 提出基于密码学的身份系统框架，保留现有工作流兼容性。

Result: 设计支持审计和委托功能，并提供迁移路径。

Conclusion: ULIs需逐步淘汰，新框架更适应AI时代需求。

Abstract: Many identity systems assign a single, static identifier to an individual for
life, reused across domains like healthcare, finance, and education. These
Universal Lifelong Identifiers (ULIs) underpin critical workflows but now pose
systemic privacy risks. We take the position that ULIs are fundamentally
incompatible with the AI era and must be phased out. We articulate a threat
model grounded in modern AI capabilities and show that traditional safeguards
such as redaction, consent, and access controls are no longer sufficient. We
define core properties for identity systems in the AI era and present a
cryptographic framework that satisfies them while retaining compatibility with
existing identifier workflows. Our design preserves institutional workflows,
supports essential functions such as auditability and delegation, and offers a
practical migration path beyond ULIs.

</details>


### [360] [No Audiogram: Leveraging Existing Scores for Personalized Speech Intelligibility Prediction](https://arxiv.org/abs/2506.02039)
*Haoshuai Zhou,Changgeng Mo,Boxuan Cao,Linkai Li,Shan Xiang Wang*

Main category: eess.AS

Relevance: 30.0

TL;DR: 论文提出了一种基于支持样本的个性化语音清晰度预测方法（SSIPNet），利用语音基础模型从少量支持样本中学习听者的语音识别能力，优于传统的听力图方法。


<details>
  <summary>Details</summary>
Motivation: 传统方法依赖听力图预测语音清晰度，但听力图仅反映纯音听力阈值，准确性有限。本文旨在通过利用听者已有的清晰度数据，提高预测准确性。

Method: 提出SSIPNet，一种深度学习模型，利用语音基础模型从多个支持（音频、分数）对中构建听者的高维表示，以预测新音频的表现。

Result: 在Clarity Prediction Challenge数据集上，即使支持样本数量较少，SSIPNet的表现仍优于基于听力图的方法。

Conclusion: SSIPNet为个性化语音清晰度预测提供了新范式，展示了利用少量支持样本实现高精度预测的潜力。

Abstract: Personalized speech intelligibility prediction is challenging. Previous
approaches have mainly relied on audiograms, which are inherently limited in
accuracy as they only capture a listener's hearing threshold for pure tones.
Rather than incorporating additional listener features, we propose a novel
approach that leverages an individual's existing intelligibility data to
predict their performance on new audio. We introduce the Support Sample-Based
Intelligibility Prediction Network (SSIPNet), a deep learning model that
leverages speech foundation models to build a high-dimensional representation
of a listener's speech recognition ability from multiple support (audio, score)
pairs, enabling accurate predictions for unseen audio. Results on the Clarity
Prediction Challenge dataset show that, even with a small number of support
(audio, score) pairs, our method outperforms audiogram-based predictions. Our
work presents a new paradigm for personalized speech intelligibility
prediction.

</details>


### [361] [EvoGit: Decentralized Code Evolution via Git-Based Multi-Agent Collaboration](https://arxiv.org/abs/2506.02049)
*Beichen Huang,Ran Cheng,Kay Chen Tan*

Main category: cs.DC

Relevance: 30.0

TL;DR: EvoGit是一个去中心化的多智能体框架，用于通过自主代码进化驱动协作软件开发。它通过Git系谱图实现智能体间的协调，无需集中控制或显式通信。实验表明EvoGit能自主生成功能性和模块化的软件。


<details>
  <summary>Details</summary>
Motivation: 探索去中心化、自动化和持续的软件开发新范式，减少人工干预，提高协作效率。

Method: 部署独立的编码智能体，通过Git系谱图异步读写共享代码库，支持细粒度分支和隐式并发。用户仅提供高层目标和轻量反馈。

Result: EvoGit在两个实际任务中成功生成了功能性和模块化的软件：构建Web应用和自进化语言模型引导的求解器。

Conclusion: EvoGit展示了去中心化、自动化和持续软件开发的潜力，为未来协作开发提供了新思路。

Abstract: We introduce EvoGit, a decentralized multi-agent framework for collaborative
software development driven by autonomous code evolution. EvoGit deploys a
population of independent coding agents, each proposing edits to a shared
codebase without centralized coordination, explicit message passing, or shared
memory. Instead, all coordination emerges through a Git-based phylogenetic
graph that tracks the full version lineage and enables agents to asynchronously
read from and write to the evolving code repository. This graph-based structure
supports fine-grained branching, implicit concurrency, and scalable agent
interaction while preserving a consistent historical record. Human involvement
is minimal but strategic: users define high-level goals, periodically review
the graph, and provide lightweight feedback to promote promising directions or
prune unproductive ones. Experiments demonstrate EvoGit's ability to
autonomously produce functional and modular software artifacts across two
real-world tasks: (1) building a web application from scratch using modern
frameworks, and (2) constructing a meta-level system that evolves its own
language-model-guided solver for the bin-packing optimization problem. Our
results underscore EvoGit's potential to establish a new paradigm for
decentralized, automated, and continual software development. EvoGit is
open-sourced at https://github.com/BillHuang2001/evogit.

</details>


### [362] [Phenotypic Profile-Informed Generation of Drug-Like Molecules via Dual-Channel Variational Autoencoders](https://arxiv.org/abs/2506.02051)
*Hui Liu,Shiye Tian,Xuejun Liu*

Main category: q-bio.BM

Relevance: 30.0

TL;DR: SmilesGEN是一个基于变分自编码器（VAE）的新型生成模型，用于生成具有潜在治疗效果的分子。它通过结合药物VAE和表达谱VAE，共同建模药物扰动与转录响应的相互作用，优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有方法主要依赖表达谱指导分子生成，但忽略了分子对细胞环境的扰动效应。SmilesGEN旨在克服这一限制。

Method: SmilesGEN结合预训练的药物VAE（SmilesNet）和表达谱VAE（ProfileNet），在共同潜在空间中建模药物扰动与转录响应的关系。

Result: SmilesGEN在生成分子的有效性、独特性、新颖性以及与已知配体的相似性方面优于现有模型。

Conclusion: SmilesGEN提供了一个强大的框架，利用基因特征生成具有潜在治疗效果的分子。

Abstract: The de novo generation of drug-like molecules capable of inducing desirable
phenotypic changes is receiving increasing attention. However, previous methods
predominantly rely on expression profiles to guide molecule generation, but
overlook the perturbative effect of the molecules on cellular contexts. To
overcome this limitation, we propose SmilesGEN, a novel generative model based
on variational autoencoder (VAE) architecture to generate molecules with
potential therapeutic effects. SmilesGEN integrates a pre-trained drug VAE
(SmilesNet) with an expression profile VAE (ProfileNet), jointly modeling the
interplay between drug perturbations and transcriptional responses in a common
latent space. Specifically, ProfileNet is imposed to reconstruct pre-treatment
expression profiles when eliminating drug-induced perturbations in the latent
space, while SmilesNet is informed by desired expression profiles to generate
drug-like molecules. Our empirical experiments demonstrate that SmilesGEN
outperforms current state-of-the-art models in generating molecules with higher
degree of validity, uniqueness, novelty, as well as higher Tanimoto similarity
to known ligands targeting the relevant proteins. Moreover, we evaluate
SmilesGEN for scaffold-based molecule optimization and generation of
therapeutic agents, and confirmed its superior performance in generating
molecules with higher similarity to approved drugs. SmilesGEN establishes a
robust framework that leverages gene signatures to generate drug-like molecules
that hold promising potential to induce desirable cellular phenotypic changes.

</details>


### [363] [Evaluating the Effectiveness of Pre-Trained Audio Embeddings for Classification of Parkinson's Disease Speech Data](https://arxiv.org/abs/2506.02078)
*Emmy Postma,Cristian Tejedor-Garcia*

Main category: eess.AS

Relevance: 30.0

TL;DR: 研究了三种预训练音频嵌入（OpenL3、VGGish和Wav2Vec2.0）在帕金森病分类中的效果，发现OpenL3在特定任务中表现最佳，而Wav2Vec2.0存在性别偏差。


<details>
  <summary>Details</summary>
Motivation: 帕金森病的语音障碍是重要生物标志物，但现有研究未充分探索个体差异对分类效果的影响。

Method: 使用NeuroVoz数据集，比较三种预训练音频嵌入在帕金森病分类任务中的表现。

Result: OpenL3在DDK和LR任务中表现最优，Wav2Vec2.0在DDK任务中显示性别偏差。

Conclusion: 需改进特征提取和模型鲁棒性以应对非典型语音模式。

Abstract: Speech impairments are prevalent biomarkers for Parkinson's Disease (PD),
motivating the development of diagnostic techniques using speech data for
clinical applications. Although deep acoustic features have shown promise for
PD classification, their effectiveness often varies due to individual speaker
differences, a factor that has not been thoroughly explored in the existing
literature. This study investigates the effectiveness of three pre-trained
audio embeddings (OpenL3, VGGish and Wav2Vec2.0 models) for PD classification.
Using the NeuroVoz dataset, OpenL3 outperforms others in diadochokinesis (DDK)
and listen and repeat (LR) tasks, capturing critical acoustic features for PD
detection. Only Wav2Vec2.0 shows significant gender bias, achieving more
favorable results for male speakers, in DDK tasks. The misclassified cases
reveal challenges with atypical speech patterns, highlighting the need for
improved feature extraction and model robustness in PD detection.

</details>


### [364] [Enhancing GOP in CTC-Based Mispronunciation Detection with Phonological Knowledge](https://arxiv.org/abs/2506.02080)
*Aditya Kamlesh Parikh,Cristian Tejedor-Garcia,Catia Cucchiarini,Helmer Strik*

Main category: eess.AS

Relevance: 30.0

TL;DR: 提出了一种基于音素聚类的对齐无关GOP方法，提高了发音质量评估的效率。


<details>
  <summary>Details</summary>
Motivation: 解决传统GOP方法因强制对齐导致的标注和分割错误问题，同时克服对齐无关方法计算效率低的缺点。

Method: 引入基于音素聚类和常见学习者错误的限制性音素替换（RPS）方法，与无限制替换（UPS）对比。

Result: 在My Pronunciation Coach和SpeechOcean762数据集上，RPS和UPS均优于基线方法。

Conclusion: 该方法提升了发音评估效率，未来可进一步优化音素聚类和扩展应用场景。

Abstract: Computer-Assisted Pronunciation Training (CAPT) systems employ automatic
measures of pronunciation quality, such as the goodness of pronunciation (GOP)
metric. GOP relies on forced alignments, which are prone to labeling and
segmentation errors due to acoustic variability. While alignment-free methods
address these challenges, they are computationally expensive and scale poorly
with phoneme sequence length and inventory size. To enhance efficiency, we
introduce a substitution-aware alignment-free GOP that restricts phoneme
substitutions based on phoneme clusters and common learner errors. We evaluated
our GOP on two L2 English speech datasets, one with child speech, My
Pronunciation Coach (MPC), and SpeechOcean762, which includes child and adult
speech. We compared RPS (restricted phoneme substitutions) and UPS
(unrestricted phoneme substitutions) setups within alignment-free methods,
which outperformed the baseline. We discuss our results and outline avenues for
future research.

</details>


### [365] [Random-key genetic algorithms](https://arxiv.org/abs/2506.02120)
*Mariana A. Londe,Luciana S. Pessoa,Carlos E. Andrade,José F. Gonçalves,Mauricio G. C. Resende*

Main category: cs.NE

Relevance: 30.0

TL;DR: 随机密钥遗传算法是一种用于离散和全局优化的进化元启发式方法，通过将解编码为随机密钥向量并使用解码器映射到问题解。


<details>
  <summary>Details</summary>
Motivation: 提出一种通用的优化框架，能够在单位超立方体内维护遗传操作，提高框架的生产力和可维护性。

Method: 算法通过随机密钥向量编码解，使用精英保留、突变和参数化均匀交叉生成下一代种群。

Result: 算法能够有效处理离散和全局优化问题，并提出了偏置随机密钥遗传算法的变体。

Conclusion: 随机密钥遗传算法是一种高效的优化方法，适用于多种问题。

Abstract: A random-key genetic algorithm is an evolutionary metaheuristic for discrete
and global optimization. Each solution is encoded as a vector of N random keys,
where a random key is a real number randomly generated in the continuous
interval [0, 1). A decoder maps each vector of random keys to a solution of the
optimization problem being solved and computes its cost. The benefit of this
approach is that all genetic operators and transformations can be maintained
within the unitary hypercube, regardless of the problem being addressed. This
enhances the productivity and maintainability of the core framework. The
algorithm starts with a population of P vectors of random keys. At each
iteration, the vectors are partitioned into two sets: a smaller set of
high-valued elite solutions and the remaining non-elite solutions. All elite
elements are copied, without change, to the next population. A small number of
random-key vectors (the mutants) is added to the population of the next
iteration. The remaining elements of the population of the next iteration are
generated by combining, with the parametrized uniform crossover of Spears and
DeJong (1991), pairs of solutions. This chapter reviews random-key genetic
algorithms and describes an effective variant called biased random-key genetic
algorithms.

</details>


### [366] [Dhvani: A Weakly-supervised Phonemic Error Detection and Personalized Feedback System for Hindi](https://arxiv.org/abs/2506.02166)
*Arnav Rustagi,Satvik Bajpai,Nimrat Kaur,Siddharth Siddharth*

Main category: eess.AS

Relevance: 30.0

TL;DR: 论文提出了一种针对印地语的计算机辅助发音训练系统Dhvani，包括合成语音生成和个性化反馈方法。


<details>
  <summary>Details</summary>
Motivation: 尽管印地语是全球第四大语言，但针对其的发音训练工具仍严重缺乏，亟需填补这一空白。

Method: 1) 开发Dhvani系统；2) 合成印地语错误发音的语音；3) 提供个性化反馈。

Result: 系统通过音素分析提供针对性反馈，利用印地语的高度语音特性。

Conclusion: Dhvani系统为印地语发音训练提供了有效工具，填补了现有技术空白。

Abstract: Computer-Assisted Pronunciation Training (CAPT) has been extensively studied
for English. However, there remains a critical gap in its application to Indian
languages with a base of 1.5 billion speakers. Pronunciation tools tailored to
Indian languages are strikingly lacking despite the fact that millions learn
them every year. With over 600 million speakers and being the fourth
most-spoken language worldwide, improving Hindi pronunciation is a vital first
step toward addressing this gap. This paper proposes 1) Dhvani -- a novel CAPT
system for Hindi, 2) synthetic speech generation for Hindi mispronunciations,
and 3) a novel methodology for providing personalized feedback to learners.
While the system often interacts with learners using Devanagari graphemes, its
core analysis targets phonemic distinctions, leveraging Hindi's highly phonetic
orthography to analyze mispronounced speech and provide targeted feedback.

</details>


### [367] [HiLO: High-Level Object Fusion for Autonomous Driving using Transformers](https://arxiv.org/abs/2506.02554)
*Timo Osterburg,Franz Albers,Christopher Diehl,Rajesh Pushparaj,Torsten Bertram*

Main category: cs.RO

Relevance: 30.0

TL;DR: 论文提出了一种基于Transformer的高层对象融合方法HiLO，改进了Adapted Kalman Filter (AKF)，在F1分数和平均IoU上分别提升了25.9和6.1个百分点。


<details>
  <summary>Details</summary>
Motivation: 当前基于学习的融合方法虽然性能高，但复杂度和硬件需求限制了其在近量产车辆中的应用。传统方法如Kalman滤波器虽然计算需求低，但性能有限。

Method: 提出了一种新的基于Transformer的高层对象融合方法HiLO，改进了AKF。

Result: 实验结果显示，F1分数提升了25.9个百分点，平均IoU提升了6.1个百分点。

Conclusion: HiLO方法在性能和计算效率上取得了平衡，并在跨领域评估中验证了其泛化能力。

Abstract: The fusion of sensor data is essential for a robust perception of the
environment in autonomous driving. Learning-based fusion approaches mainly use
feature-level fusion to achieve high performance, but their complexity and
hardware requirements limit their applicability in near-production vehicles.
High-level fusion methods offer robustness with lower computational
requirements. Traditional methods, such as the Kalman filter, dominate this
area. This paper modifies the Adapted Kalman Filter (AKF) and proposes a novel
transformer-based high-level object fusion method called HiLO. Experimental
results demonstrate improvements of $25.9$ percentage points in $\textrm{F}_1$
score and $6.1$ percentage points in mean IoU. Evaluation on a new large-scale
real-world dataset demonstrates the effectiveness of the proposed approaches.
Their generalizability is further validated by cross-domain evaluation between
urban and highway scenarios. Code, data, and models are available at
https://github.com/rst-tu-dortmund/HiLO .

</details>


### [368] [Multi Layered Autonomy and AI Ecologies in Robotic Art Installations](https://arxiv.org/abs/2506.02606)
*Baoyang Chen,Xian Xu,Huamin Qu*

Main category: cs.RO

Relevance: 30.0

TL;DR: 论文探讨了AI驱动的机器人在艺术创作中的角色，通过多层次的反馈系统实现机器与观众的互动，探讨了AI未来中的责任问题。


<details>
  <summary>Details</summary>
Motivation: 研究旨在探索机器代理与艺术创作的结合，通过机器人、环境和观众的互动，重新定义代理、作者身份和伦理。

Method: 采用三层次信仰系统（微观适应性策略、中观叙事驱动、宏观首要指令）指导机器人行为，结合环境线索和观众互动生成动态艺术作品。

Result: 作品展示了机器人作为合作者而非工具的角色，通过AI生成脚本、动态灯光等元素，创造出一种新兴的艺术形式。

Conclusion: 研究表明，通过反馈系统、机器人实验和规则制定，可以重新定义当代艺术中的代理、作者身份和伦理。

Abstract: Symbiosis of Agents is a large-scale installation by Baoyang Chen that embeds
AI-driven robots in an immersive, mirror-lined arena, probing the tension
between machine agency and artistic authorship. Drawing on early cybernetics,
rule-based conceptual art, and seminal robotic works, it orchestrates fluid
exchanges among robotic arms, quadruped machines, their environment, and the
public. A three tier faith system pilots the ecology: micro-level adaptive
tactics, meso-level narrative drives, and a macro-level prime directive. This
hierarchy lets behaviors evolve organically in response to environmental cues
and even a viewer's breath, turning spectators into co-authors of the unfolding
drama.Framed by a speculative terraforming scenario that recalls the historical
exploitation of marginalized labor, the piece asks who bears responsibility in
AI-mediated futures. Choreographed motion, AI-generated scripts, reactive
lighting, and drifting fog cast the robots as collaborators rather than tools,
forging a living, emergent artwork. Exhibited internationally, Symbiosis of
Agents shows how cybernetic feedback, robotic experimentation, and conceptual
rule-making can converge to redefine agency, authorship, and ethics in
contemporary art.

</details>


### [369] [Speaker Diarization with Overlapping Community Detection Using Graph Attention Networks and Label Propagation Algorithm](https://arxiv.org/abs/2506.02610)
*Zhaoyang Li,Jie Wang,XiaoXiao Li,Wangjie Li,Longjie Luo,Lin Li,Qingyang Hong*

Main category: cs.SD

Relevance: 30.0

TL;DR: 提出了一种基于图注意力网络和标签传播算法的重叠社区检测方法（OCDGALP），用于解决说话人日志中传统聚类方法难以处理的复杂嵌入分布和重叠语音问题。


<details>
  <summary>Details</summary>
Motivation: 传统聚类方法在说话人日志中难以处理复杂嵌入分布和重叠语音，因此需要一种更有效的方法。

Method: 结合图注意力网络（优化嵌入和节点连接）和标签传播算法（支持多社区标签分配），实现同时聚类和重叠社区检测。

Result: 在DIHARD-III数据集上，无需VAD时DER为15.94%，使用VAD时降至11.07%，达到SOTA。

Conclusion: OCDGALP显著降低了说话人日志的错误率，优于传统方法。

Abstract: In speaker diarization, traditional clustering-based methods remain widely
used in real-world applications. However, these methods struggle with the
complex distribution of speaker embeddings and overlapping speech segments. To
address these limitations, we propose an Overlapping Community Detection method
based on Graph Attention networks and the Label Propagation Algorithm
(OCDGALP). The proposed framework comprises two key components: (1) a graph
attention network that refines speaker embeddings and node connections by
aggregating information from neighboring nodes, and (2) a label propagation
algorithm that assigns multiple community labels to each node, enabling
simultaneous clustering and overlapping community detection. Experimental
results show that the proposed method significantly reduces the Diarization
Error Rate (DER), achieving a state-of-the-art 15.94% DER on the DIHARD-III
dataset without oracle Voice Activity Detection (VAD), and an impressive 11.07%
with oracle VAD.

</details>


### [370] [PhysGaia: A Physics-Aware Dataset of Multi-Body Interactions for Dynamic Novel View Synthesis](https://arxiv.org/abs/2506.02794)
*Mijeong Kim,Gunhee Kim,Jungyoon Choi,Wonjae Roh,Bohyung Han*

Main category: cs.GR

Relevance: 30.0

TL;DR: PhysGaia是一个专为动态新视角合成（DyNVS）设计的物理感知数据集，包含结构化对象和非结构化物理现象，支持物理场景建模。


<details>
  <summary>Details</summary>
Motivation: 现有数据集主要关注逼真重建，缺乏对物理交互的支持，PhysGaia填补了这一空白。

Method: 数据集通过物理求解器生成严格遵循物理规律的动态场景，提供3D粒子轨迹和物理参数作为真实数据。

Result: 数据集支持定量评估物理建模，并提供了与前沿DyNVS模型的集成管道。

Conclusion: PhysGaia将推动动态视角合成、物理场景理解和深度学习与物理模拟的结合研究。

Abstract: We introduce PhysGaia, a novel physics-aware dataset specifically designed
for Dynamic Novel View Synthesis (DyNVS), encompassing both structured objects
and unstructured physical phenomena. Unlike existing datasets that primarily
focus on photorealistic reconstruction, PhysGaia is created to actively support
physics-aware dynamic scene modeling. Our dataset provides complex dynamic
scenarios with rich interactions among multiple objects, where they
realistically collide with each other and exchange forces. Furthermore, it
contains a diverse range of physical materials, such as liquid, gas,
viscoelastic substance, and textile, which moves beyond the rigid bodies
prevalent in existing datasets. All scenes in PhysGaia are faithfully generated
to strictly adhere to physical laws, leveraging carefully selected
material-specific physics solvers. To enable quantitative evaluation of
physical modeling, our dataset provides essential ground-truth information,
including 3D particle trajectories and physics parameters, e.g., viscosity. To
facilitate research adoption, we also provide essential integration pipelines
for using state-of-the-art DyNVS models with our dataset and report their
results. By addressing the critical lack of datasets for physics-aware
modeling, PhysGaia will significantly advance research in dynamic view
synthesis, physics-based scene understanding, and deep learning models
integrated with physical simulation -- ultimately enabling more faithful
reconstruction and interpretation of complex dynamic scenes. Our datasets and
codes are available in the project website,
http://cvlab.snu.ac.kr/research/PhysGaia.

</details>


### [371] [Enriching Location Representation with Detailed Semantic Information](https://arxiv.org/abs/2506.02744)
*Junyuan Liu,Xinglei Wang,Tao Cheng*

Main category: cs.CE

Relevance: 30.0

TL;DR: CaLLiPer+ integrates POI names and categorical labels in a multimodal contrastive learning framework, improving urban modeling tasks like land use classification and socioeconomic mapping by 4-11%.


<details>
  <summary>Details</summary>
Motivation: Traditional spatial embeddings often miss fine-grained contextual information from urban environments, limiting their effectiveness.

Method: Extends CaLLiPer by systematically integrating POI names and categorical labels using multimodal contrastive learning.

Result: Achieves 4-11% performance gains in land use classification and socioeconomic mapping, with improved location retrieval.

Conclusion: Integrating fine-grained semantic attributes and multimodal learning can advance urban foundation models.

Abstract: Spatial representations that capture both structural and semantic
characteristics of urban environments are essential for urban modeling.
Traditional spatial embeddings often prioritize spatial proximity while
underutilizing fine-grained contextual information from places. To address this
limitation, we introduce CaLLiPer+, an extension of the CaLLiPer model that
systematically integrates Point-of-Interest (POI) names alongside categorical
labels within a multimodal contrastive learning framework. We evaluate its
effectiveness on two downstream tasks, land use classification and
socioeconomic status distribution mapping, demonstrating consistent performance
gains of 4% to 11% over baseline methods. Additionally, we show that
incorporating POI names enhances location retrieval, enabling models to capture
complex urban concepts with greater precision. Ablation studies further reveal
the complementary role of POI names and the advantages of leveraging pretrained
text encoders for spatial representations. Overall, our findings highlight the
potential of integrating fine-grained semantic attributes and multimodal
learning techniques to advance the development of urban foundation models.

</details>


### [372] [AI-Driven Vehicle Condition Monitoring with Cell-Aware Edge Service Migration](https://arxiv.org/abs/2506.02785)
*Charalampos Kalalas,Pavol Mulinka,Guillermo Candela Belmonte,Miguel Fornell,Michail Dalgitsis,Francisco Paredes Vera,Javier Santaella Sánchez,Carmen Vicente Villares,Roshan Sedar,Eftychia Datsika,Angelos Antonopoulos,Antonio Fernández Ojea,Miquel Payaro*

Main category: cs.NI

Relevance: 30.0

TL;DR: 该论文提出了一种基于边缘计算的车辆状态监测服务，通过动态服务编排框架实现实时异常检测和低延迟AI推理。


<details>
  <summary>Details</summary>
Motivation: 旨在通过AI提升车辆设备的维护策略、降低成本并提高安全性，同时解决边缘计算环境中的移动性挑战。

Method: 提出了一种闭环服务编排框架，动态触发边缘节点间的服务迁移，以应对网络相关指标变化。

Result: 在配备5G网络的真实赛道环境中测试，证明了框架在低延迟AI推理和自适应服务部署方面的有效性。

Conclusion: 该框架在智能交通和移动应用中具有潜力。

Abstract: Artificial intelligence (AI) has been increasingly applied to the condition
monitoring of vehicular equipment, aiming to enhance maintenance strategies,
reduce costs, and improve safety. Leveraging the edge computing paradigm,
AI-based condition monitoring systems process vast streams of vehicular data to
detect anomalies and optimize operational performance. In this work, we
introduce a novel vehicle condition monitoring service that enables real-time
diagnostics of a diverse set of anomalies while remaining practical for
deployment in real-world edge environments. To address mobility challenges, we
propose a closed-loop service orchestration framework where service migration
across edge nodes is dynamically triggered by network-related metrics. Our
approach has been implemented and tested in a real-world race circuit
environment equipped with 5G network capabilities under diverse operational
conditions. Experimental results demonstrate the effectiveness of our framework
in ensuring low-latency AI inference and adaptive service placement,
highlighting its potential for intelligent transportation and mobility
applications.

</details>


### [373] [Deep Learning Enhanced Multivariate GARCH](https://arxiv.org/abs/2506.02796)
*Haoyuan Wang,Chen Liu,Minh-Ngoc Tran,Chao Wang*

Main category: q-fin.CP

Relevance: 30.0

TL;DR: 提出了一种结合LSTM和BEKK模型的新框架LSTM-BEKK，用于金融数据中的多元波动率建模，提升了非线性动态依赖结构的捕捉能力。


<details>
  <summary>Details</summary>
Motivation: 传统多元GARCH方法在捕捉持续波动聚类和资产间不对称联动方面存在局限，需要更灵活的模型。

Method: 结合LSTM的灵活性和BEKK模型的计量结构，设计LSTM-BEKK框架，适应时变市场条件。

Result: 在多个股票市场中，LSTM-BEKK模型在样本外投资组合风险预测上表现更优，同时保持BEKK模型的可解释性。

Conclusion: 混合计量经济学-深度学习模型在金融风险管理和多元波动率预测中具有潜力。

Abstract: This paper introduces a novel multivariate volatility modeling framework,
named Long Short-Term Memory enhanced BEKK (LSTM-BEKK), that integrates deep
learning into multivariate GARCH processes. By combining the flexibility of
recurrent neural networks with the econometric structure of BEKK models, our
approach is designed to better capture nonlinear, dynamic, and high-dimensional
dependence structures in financial return data. The proposed model addresses
key limitations of traditional multivariate GARCH-based methods, particularly
in capturing persistent volatility clustering and asymmetric co-movement across
assets. Leveraging the data-driven nature of LSTMs, the framework adapts
effectively to time-varying market conditions, offering improved robustness and
forecasting performance. Empirical results across multiple equity markets
confirm that the LSTM-BEKK model achieves superior performance in terms of
out-of-sample portfolio risk forecast, while maintaining the interpretability
from the BEKK models. These findings highlight the potential of hybrid
econometric-deep learning models in advancing financial risk management and
multivariate volatility forecasting.

</details>


### [374] [CapSpeech: Enabling Downstream Applications in Style-Captioned Text-to-Speech](https://arxiv.org/abs/2506.02863)
*Helin Wang,Jiarui Hai,Dading Chong,Karan Thakkar,Tiantian Feng,Dongchao Yang,Junhyeok Lee,Laureano Moro Velazquez,Jesus Villalba,Zengyi Qin,Shrikanth Narayanan,Mounya Elhiali,Najim Dehak*

Main category: eess.AS

Relevance: 30.0

TL;DR: 论文介绍了CapSpeech，一个用于风格化文本到语音合成（CapTTS）相关任务的新基准，包含大规模数据集和实验结果。


<details>
  <summary>Details</summary>
Motivation: 解决CapTTS领域缺乏标准化数据集和下游任务研究的问题。

Method: 提出CapSpeech基准，包含机器和人工标注的音频-字幕对，并在自回归和非自回归模型上进行实验。

Result: 实验展示了高保真和高度可理解的语音合成结果，CapSpeech是目前最大的CapTTS相关任务数据集。

Conclusion: CapSpeech为CapTTS系统开发提供了有价值的见解和资源。

Abstract: Recent advancements in generative artificial intelligence have significantly
transformed the field of style-captioned text-to-speech synthesis (CapTTS).
However, adapting CapTTS to real-world applications remains challenging due to
the lack of standardized, comprehensive datasets and limited research on
downstream tasks built upon CapTTS. To address these gaps, we introduce
CapSpeech, a new benchmark designed for a series of CapTTS-related tasks,
including style-captioned text-to-speech synthesis with sound events
(CapTTS-SE), accent-captioned TTS (AccCapTTS), emotion-captioned TTS
(EmoCapTTS), and text-to-speech synthesis for chat agent (AgentTTS). CapSpeech
comprises over 10 million machine-annotated audio-caption pairs and nearly 0.36
million human-annotated audio-caption pairs. In addition, we introduce two new
datasets collected and recorded by a professional voice actor and experienced
audio engineers, specifically for the AgentTTS and CapTTS-SE tasks. Alongside
the datasets, we conduct comprehensive experiments using both autoregressive
and non-autoregressive models on CapSpeech. Our results demonstrate
high-fidelity and highly intelligible speech synthesis across a diverse range
of speaking styles. To the best of our knowledge, CapSpeech is the largest
available dataset offering comprehensive annotations for CapTTS-related tasks.
The experiments and findings further provide valuable insights into the
challenges of developing CapTTS systems.

</details>


### [375] [Modelling the Effects of Hearing Loss on Neural Coding in the Auditory Midbrain with Variational Conditioning](https://arxiv.org/abs/2506.03088)
*Lloyd Pellatt,Fotios Drakopoulos,Shievanie Sabesan,Nicholas A. Lesica*

Main category: q-bio.NC

Relevance: 30.0

TL;DR: 提出了一种变分条件模型，通过学习听觉中脑神经活动的编码空间，直接建模听力损失。模型仅需6个参数即可准确预测正常和听力受损动物的神经反应，并可用于模拟新动物的活动。


<details>
  <summary>Details</summary>
Motivation: 现有DNN模型假设听觉处理在所有大脑中相同，无法捕捉听力损失的多样性。本文旨在直接建模听力损失的神经编码空间。

Method: 使用变分条件模型，从健康及噪声暴露动物的听觉中脑神经活动记录中学习，参数化听力损失（每动物6个参数）。

Result: 模型预测了62%的正常动物和68%的听力受损动物的可解释神经反应方差，接近特定动物模型的最优水平。

Conclusion: 该模型为快速拟合个性化听力损失补偿模型提供了基础，未来可用于恢复听力受损大脑的正常神经编码。

Abstract: The mapping from sound to neural activity that underlies hearing is highly
non-linear. The first few stages of this mapping in the cochlea have been
modelled successfully, with biophysical models built by hand and, more
recently, with DNN models trained on datasets simulated by biophysical models.
Modelling the auditory brain has been a challenge because central auditory
processing is too complex for models to be built by hand, and datasets for
training DNN models directly have not been available. Recent work has taken
advantage of large-scale high resolution neural recordings from the auditory
midbrain to build a DNN model of normal hearing with great success. But this
model assumes that auditory processing is the same in all brains, and therefore
it cannot capture the widely varying effects of hearing loss.
  We propose a novel variational-conditional model to learn to encode the space
of hearing loss directly from recordings of neural activity in the auditory
midbrain of healthy and noise exposed animals. With hearing loss parametrised
by only 6 free parameters per animal, our model accurately predicts 62\% of the
explainable variance in neural responses from normal hearing animals and 68%
for hearing impaired animals, within a few percentage points of state of the
art animal specific models. We demonstrate that the model can be used to
simulate realistic activity from out of sample animals by fitting only the
learned conditioning parameters with Bayesian optimisation, achieving
crossentropy loss within 2% of the optimum in 15-30 iterations. Including more
animals in the training data slightly improved the performance on unseen
animals. This model will enable future development of parametrised hearing loss
compensation models trained to directly restore normal neural coding in hearing
impaired brains, which can be quickly fitted for a new user by human in the
loop optimisation.

</details>


### [376] [The Impact of Software Testing with Quantum Optimization Meets Machine Learning](https://arxiv.org/abs/2506.02090)
*Gopichand Bandarupalli*

Main category: cs.SE

Relevance: 20.0

TL;DR: 论文提出了一种结合量子退火和机器学习的混合框架，用于优化CI/CD流水线中的测试用例优先级排序，相比传统ML，缺陷检测效率提升25%，测试执行时间减少30%。


<details>
  <summary>Details</summary>
Motivation: 现代软件系统复杂性增加，传统机器学习难以应对大规模测试套件，需要更高效的测试方法。

Method: 采用量子退火与机器学习结合的混合框架，优化测试用例优先级排序，并在Defects4J数据集上验证。

Result: 缺陷检测效率提升25%，测试执行时间减少30%，并在模拟CI/CD环境中表现出鲁棒性。

Conclusion: 该框架为2025年混合量子-经典生态系统提供了可扩展的软件质量保障方法。

Abstract: Modern software systems complexity challenges efficient testing, as
traditional machine learning (ML) struggles with large test suites. This
research presents a hybrid framework integrating Quantum Annealing with ML to
optimize test case prioritization in CI/CD pipelines. Leveraging quantum
optimization, it achieves a 25 percent increase in defect detection efficiency
and a 30 percent reduction in test execution time versus classical ML,
validated on the Defects4J dataset. A simulated CI/CD environment demonstrates
robustness across evolving codebases. Visualizations, including defect heatmaps
and performance graphs, enhance interpretability. The framework addresses
quantum hardware limits, CI/CD integration, and scalability for 2025s hybrid
quantum-classical ecosystems, offering a transformative approach to software
quality assurance.

</details>


### [377] [A Review of Various Datasets for Machine Learning Algorithm-Based Intrusion Detection System: Advances and Challenges](https://arxiv.org/abs/2506.02438)
*Sudhanshu Sekhar Tripathy,Bichitrananda Behera*

Main category: cs.CR

Relevance: 20.0

TL;DR: 该论文综述了基于机器学习和深度学习的入侵检测系统（IDS）方法，分析了多种分类器和数据集，并总结了现有挑战。


<details>
  <summary>Details</summary>
Motivation: 随着全球对技术和自动化流程的依赖增加，确保系统和网络安全成为重要问题。论文旨在通过机器学习和深度学习提升IDS的检测准确性。

Method: 通过文献综述和表格分析，评估了多种分类器（如SVM、KNN、DT等）在不同数据集（如KDDCUP'99、NSL-KDD等）上的表现。

Result: 详细总结了各种分类器的性能、检测的攻击类型及评估指标，为未来IDS研究提供了参考。

Conclusion: 论文为IDS领域的研究提供了全面的综述，并指出了现有数据集的挑战和改进方向。

Abstract: IDS aims to protect computer networks from security threats by detecting,
notifying, and taking appropriate action to prevent illegal access and protect
confidential information. As the globe becomes increasingly dependent on
technology and automated processes, ensuring secured systems, applications, and
networks has become one of the most significant problems of this era. The
global web and digital technology have significantly accelerated the evolution
of the modern world, necessitating the use of telecommunications and data
transfer platforms. Researchers are enhancing the effectiveness of IDS by
incorporating popular datasets into machine learning algorithms. IDS, equipped
with machine learning classifiers, enhances security attack detection accuracy
by identifying normal or abnormal network traffic. This paper explores the
methods of capturing and reviewing intrusion detection systems (IDS) and
evaluates the challenges existing datasets face. A deluge of research on
machine learning (ML) and deep learning (DL) architecture-based intrusion
detection techniques has been conducted in the past ten years on various
cybersecurity datasets, including KDDCUP'99, NSL-KDD, UNSW-NB15, CICIDS-2017,
and CSE-CIC-IDS2018. We conducted a literature review and presented an in-depth
analysis of various intrusion detection methods that use SVM, KNN, DT, LR, NB,
RF, XGBOOST, Adaboost, and ANN. We provide an overview of each technique,
explaining the role of the classifiers and algorithms used. A detailed tabular
analysis highlights the datasets used, classifiers employed, attacks detected,
evaluation metrics, and conclusions drawn. This article offers a thorough
review for future IDS research.

</details>


### [378] [Music interpretation and emotion perception: A computational and neurophysiological investigation](https://arxiv.org/abs/2506.01982)
*Vassilis Lyberatos,Spyridon Kantarelis,Ioanna Zioga,Christina Anagnostopoulou,Giorgos Stamou,Anastasia Georgaki*

Main category: cs.HC

Relevance: 10.0

TL;DR: 研究探讨了音乐表演中情感表达与感知的计算和神经生理学方法，分析了不同表演设置对情感沟通和听众反应的影响。


<details>
  <summary>Details</summary>
Motivation: 探索音乐表演中表达性和即兴性对情感沟通和听众反应的独特作用。

Method: 结合音频分析、情感标注和神经生理学测量，分析专业音乐家的表演。

Result: 即兴表演表现出独特的声学特征和更强的情绪反应，神经生理学数据显示更高的放松度。

Conclusion: 表达性在增强情感沟通和观众参与中具有重要作用。

Abstract: This study investigates emotional expression and perception in music
performance using computational and neurophysiological methods. The influence
of different performance settings, such as repertoire, diatonic modal etudes,
and improvisation, as well as levels of expressiveness, on performers'
emotional communication and listeners' reactions is explored. Professional
musicians performed various tasks, and emotional annotations were provided by
both performers and the audience. Audio analysis revealed that expressive and
improvisational performances exhibited unique acoustic features, while emotion
analysis showed stronger emotional responses. Neurophysiological measurements
indicated greater relaxation in improvisational performances. This multimodal
study highlights the significance of expressivity in enhancing emotional
communication and audience engagement.

</details>


<div id='cs.LG'></div>

# cs.LG [[Back]](#toc)

### [379] [Breaking Quadratic Barriers: A Non-Attention LLM for Ultra-Long Context Horizons](https://arxiv.org/abs/2506.01963)
*Andrew Kiruluta,Preethi Raju,Priscilla Burity*

Main category: cs.LG

Relevance: 90.0

TL;DR: 提出了一种新型的非注意力架构，用于高效处理超长上下文窗口的LLM，避免了传统Transformer的二次复杂度问题。


<details>
  <summary>Details</summary>
Motivation: 传统Transformer因自注意力机制导致二次内存和计算开销，无法高效处理超长上下文窗口。

Method: 结合状态空间块（S4启发）、多分辨率卷积层、轻量级循环监督器和检索增强外部记忆，避免token间注意力。

Result: 模型能够高效处理数十万至数百万token的超长上下文窗口，计算复杂度接近线性。

Conclusion: 非注意力架构在长上下文任务中具有潜力，为LLM设计提供了新方向。

Abstract: We present a novel non attention based architecture for large language models
(LLMs) that efficiently handles very long context windows, on the order of
hundreds of thousands to potentially millions of tokens. Unlike traditional
Transformer designs, which suffer from quadratic memory and computation
overload due to the nature of the self attention mechanism, our model avoids
token to token attention entirely. Instead, it combines the following
complementary components: State Space blocks (inspired by S4) that learn
continuous time convolution kernels and scale near linearly with sequence
length, Multi Resolution Convolution layers that capture local context at
different dilation levels, a lightweight Recurrent Supervisor to maintain a
global hidden state across sequential chunks, and Retrieval Augmented External
Memory that stores and retrieves high-level chunk embeddings without
reintroducing quadratic operations.

</details>


### [380] [KDRL: Post-Training Reasoning LLMs via Unified Knowledge Distillation and Reinforcement Learning](https://arxiv.org/abs/2506.02208)
*Hongling Xu,Qi Zhu,Heyuan Deng,Jinpeng Li,Lu Hou,Yasheng Wang,Lifeng Shang,Ruifeng Xu,Fei Mi*

Main category: cs.LG

Relevance: 90.0

TL;DR: KDRL是一个结合知识蒸馏（KD）和强化学习（RL）的统一后训练框架，用于提升大语言模型的推理能力。


<details>
  <summary>Details</summary>
Motivation: RL在探索高奖励轨迹时样本效率低，而KD在域外场景泛化能力差，因此需要结合两者优势。

Method: KDRL通过策略梯度优化，最小化学生与教师分布的RKL散度，同时最大化基于规则的奖励。

Result: 在多个推理基准测试中，KDRL优于GRPO和KD基线，平衡了性能和推理效率。

Conclusion: 结合KD和RL是训练推理LLM的有效策略。

Abstract: Recent advances in large language model (LLM) post-training have leveraged
two distinct paradigms to enhance reasoning capabilities: reinforcement
learning (RL) and knowledge distillation (KD). While RL enables the emergence
of complex reasoning behaviors, it often suffers from low sample efficiency
when the initial policy struggles to explore high-reward trajectories.
Conversely, KD improves learning efficiency via mimicking the teacher model but
tends to generalize poorly to out-of-domain scenarios. In this work, we present
\textbf{KDRL}, a \textit{unified post-training framework} that jointly
optimizes a reasoning model through teacher supervision (KD) and
self-exploration (RL). Specifically, KDRL leverages policy gradient
optimization to simultaneously minimize the reverse Kullback-Leibler divergence
(RKL) between the student and teacher distributions while maximizing the
expected rule-based rewards. We first formulate a unified objective that
integrates GRPO and KD, and systematically explore how different KL
approximations, KL coefficients, and reward-guided KD strategies affect the
overall post-training dynamics and performance. Empirical results on multiple
reasoning benchmarks demonstrate that KDRL outperforms GRPO and various KD
baselines while achieving a favorable balance between performance and reasoning
token efficiency. These findings indicate that integrating KD and RL serves as
an effective and efficient strategy to train reasoning LLMs.

</details>


### [381] [Response-Level Rewards Are All You Need for Online Reinforcement Learning in LLMs: A Mathematical Perspective](https://arxiv.org/abs/2506.02553)
*Shenghua He,Tian Xia,Xuan Zhou,Hui Wei*

Main category: cs.LG

Relevance: 90.0

TL;DR: 论文研究了LLM强化学习中的零奖励假设问题，提出了轨迹策略梯度定理，证明仅用响应级奖励模型即可无偏估计真实奖励，并提出了新算法TRePO。


<details>
  <summary>Details</summary>
Motivation: 解决LLM强化学习中零奖励假设的挑战，即中间动作无奖励，仅最终动作有奖励的问题。

Method: 提出轨迹策略梯度定理，分析现有算法（如PPO、GRPO）的理论基础，并提出新算法TRePO。

Result: 证明响应级奖励模型足以无偏估计真实奖励，TRePO在理论和实践中表现优越。

Conclusion: 为LLM微调提供了更实用的方法，开发者可专注于改进响应级奖励模型。

Abstract: We study a common challenge in reinforcement learning for large language
models (LLMs): the Zero-Reward Assumption, where non-terminal actions (i.e.,
intermediate token generations) receive zero task-specific immediate reward,
while only the final token receives a reward for the entire response. This
assumption arises frequently in practice, as precise token-level rewards are
often difficult or infeasible to obtain in LLM applications. In this work, we
provide a unifying theoretical perspective. We introduce the Trajectory Policy
Gradient Theorem, which shows that the policy gradient based on true, unknown
token-level rewards can be unbiasedly estimated using only a response-level
reward model, regardless of whether the Zero-Reward Assumption holds or not,
for algorithms in the REINFORCE and Actor-Critic families. This result reveals
that widely used methods such as PPO, GRPO, ReMax, and RLOO inherently possess
the capacity to model token-level reward signals, offering a theoretical
justification for response-level reward approaches. Our findings pave the way
for more practical, efficient LLM fine-tuning, allowing developers to treat
training algorithms as black boxes and focus on improving the response-level
reward model with auxiliary sub-models. We also offer a detailed analysis of
popular RL and non-RL methods, comparing their theoretical foundations and
practical advantages across common LLM tasks. Finally, we propose a new
algorithm: Token-Reinforced Policy Optimization (TRePO), a theoretically
grounded method that is simpler than PPO, matches GRPO in memory efficiency,
and holds promise for broad applicability.

</details>


### [382] [QKV Projections Require a Fraction of Their Memory](https://arxiv.org/abs/2506.02939)
*Malik Khalaf,Yara Shamshoum,Nitzan Hodos,Yuval Sieradzki,Assaf Schuster*

Main category: cs.LG

Relevance: 90.0

TL;DR: 论文提出了一种名为PAMM的张量压缩技术，显著减少了注意力层中Q、K、V投影的内存占用，同时保持或提升模型性能。


<details>
  <summary>Details</summary>
Motivation: 现有研究多关注注意力机制的点积近似，而忽略了线性投影的内存消耗问题。

Method: 提出Point-Approximate Matrix Multiplication (PAMM)，一种新型张量压缩技术，可减少Q、K、V投影的内存占用。

Result: PAMM将内存占用减少高达512倍，同时保持或提升最终困惑度。

Conclusion: PAMM是一种实用且互补的方法，可与FlashAttention等技术结合，实现内存高效的LLM训练。

Abstract: The Multi-Head Attention mechanism is central to LLM operation, and multiple
works target its compute and memory efficiency during training. While most
works focus on approximating the scaled dot product, the memory consumption of
the linear projections that compute the $Q$, $K$, and $V$ tensors from the
input $x$ is often overlooked. To address this, we propose Point-Approximate
Matrix Multiplication (PAMM), a novel tensor compression technique that reduces
memory consumption of the $Q,K,V$ projections in attention layers by a factor
of up to $\times 512$, effectively erasing their memory footprint, while
achieving similar or better final perplexity. PAMM is fully composable with
efficient attention techniques such as FlashAttention, making it a practical
and complementary method for memory-efficient LLM training.

</details>


### [383] [Turning LLM Activations Quantization-Friendly](https://arxiv.org/abs/2506.01967)
*Patrik Czakó,Gábor Kertész,Sándor Szénási*

Main category: cs.LG

Relevance: 85.0

TL;DR: 论文研究了量化LLMs中的异常值问题，提出了一种基于通道幅度的新度量方法，并设计了一种混合量化方法以减少误差。


<details>
  <summary>Details</summary>
Motivation: 量化可以降低LLMs的服务成本，但激活整数运算需要量化权重和激活值，而LLMs中的异常值会增加量化误差。本文旨在解决这一问题。

Method: 通过分析异常值对分层量化误差的影响，研究平滑和旋转变换的效果，提出了一种基于通道幅度的新度量方法，并设计了一种混合量化方法（结合通道缩放和旋转）。

Result: 提出了一种新度量方法，量化难度可视化，混合量化方法通过数学公式验证了其有效性。

Conclusion: 混合量化方法能有效减少LLMs量化误差，为高效量化提供了新思路。

Abstract: Quantization effectively reduces the serving costs of Large Language Models
(LLMs) by speeding up data movement through compressed parameters and enabling
faster operations via integer arithmetic. However, activating integer
arithmetic requires quantizing both weights and activations, which poses
challenges due to the significant outliers in LLMs that increase quantization
error. In this work, we investigate these outliers with an emphasis on their
effect on layer-wise quantization error, then examine how smoothing and
rotation transform the observed values. Our primary contributions include
introducing a new metric to measure and visualize quantization difficulty based
on channel magnitudes, as well as proposing a hybrid approach that applies
channel-wise scaling before rotation, supported by a mathematical formulation
of its benefits.

</details>


### [384] [SpecMemo: Speculative Decoding is in Your Pocket](https://arxiv.org/abs/2506.01986)
*Selin Yildirim,Deming Chen*

Main category: cs.LG

Relevance: 85.0

TL;DR: SpecMemo是一种设备感知推理引擎，通过精细控制内存分配，在内存受限设备上实现多轮聊天机器人的推测解码，保持96%的吞吐量并减少65%的内存占用。


<details>
  <summary>Details</summary>
Motivation: 在内存受限设备上部署推测解码具有挑战性，SpecMemo旨在解决这一问题，实现高效推理。

Method: 通过理论建模推测解码的内存占用，确定内存预算下限，并平衡内存分配与性能增益。支持分布式和多GPU批处理推测解码。

Result: 在单Nvidia Titan RTX上减少65%生成内存，保持96%吞吐量；在八AMD MI250 GPU上实现2倍加速，批量10时吞吐量提升8倍。

Conclusion: SpecMemo为资源受限环境中的LLM应用提供了高效、低成本的部署方案。

Abstract: Recent advancements in speculative decoding have demonstrated considerable
speedup across a wide array of large language model (LLM) tasks. Speculative
decoding inherently relies on sacrificing extra memory allocations to generate
several candidate tokens, of which acceptance rate drives the speedup. However,
deploying speculative decoding on memory-constrained devices, such as mobile
GPUs, remains as a significant challenge in real-world scenarios. In this work,
we present a device-aware inference engine named SpecMemo that can smartly
control memory allocations at finer levels to enable multi-turn chatbots with
speculative decoding on such limited memory devices. Our methodology stems from
theoretically modeling memory footprint of speculative decoding to determine a
lower bound on the required memory budget while retaining speedup. SpecMemo
empirically acquires a careful balance between minimizing redundant memory
allocations for rejected candidate tokens and maintaining competitive
performance gains from speculation. Notably, with SpecMemo's memory management,
we maintain 96% of overall throughput from speculative decoding on MT-Bench,
with reduced generation-memory by 65% on single Nvidia Titan RTX. Given
multiple constrained GPUs, we build on top of previous speculative decoding
architectures to facilitate big-model inference by distributing
Llama-2-70B-Chat model, on which we provide novel batched speculative decoding
to increase usability of multiple small server GPUs. This novel framework
demonstrates 2x speedup over distributed and batched vanilla decoding with the
base model on eight AMD MI250 GPUs. Moreover, inference throughput increases
remarkably 8x with batch size 10. Our work contributes to democratized LLM
applications in resource-constrained environments, providing a pathway for
faster and cheaper deployment of real-world LLM applications with robust
performance.

</details>


### [385] [Assigning Distinct Roles to Quantized and Low-Rank Matrices Toward Optimal Weight Decomposition](https://arxiv.org/abs/2506.02077)
*Yoonjun Cho,Soeun Kim,Dongjae Jeon,Kyelim Lee,Beomsoo Lee,Albert No*

Main category: cs.LG

Relevance: 85.0

TL;DR: 论文提出了一种名为ODLRI的方法，通过分解权重矩阵为量化和低秩组件，优化大型语言模型的压缩效果。


<details>
  <summary>Details</summary>
Motivation: 现有联合优化方法在量化和低秩近似之间交替迭代，但往往牺牲一方效果，导致分解不理想。

Method: 引入Outlier-Driven Low-Rank Initialization (ODLRI)，让低秩组件专门捕捉激活敏感的权重，从而更有效平衡量化和低秩近似。

Result: 在Llama2、Llama3-8B和Mistral-7B上的实验表明，ODLRI减少了激活感知误差，降低了量化规模，并提升了低比特设置下的困惑度和零样本准确率。

Conclusion: ODLRI通过结构化分解优化了大型语言模型的压缩效果，提升了性能。

Abstract: Decomposing weight matrices into quantization and low-rank components
($\mathbf{W} \approx \mathbf{Q} + \mathbf{L}\mathbf{R}$) is a widely used
technique for compressing large language models (LLMs). Existing joint
optimization methods iteratively alternate between quantization and low-rank
approximation. However, these methods tend to prioritize one component at the
expense of the other, resulting in suboptimal decompositions that fail to
leverage each component's unique strengths. In this work, we introduce
Outlier-Driven Low-Rank Initialization (ODLRI), which assigns low-rank
components the specific role of capturing activation-sensitive weights. This
structured decomposition mitigates outliers' negative impact on quantization,
enabling more effective balance between quantization and low-rank
approximation. Experiments on Llama2 (7B, 13B, 70B), Llama3-8B, and Mistral-7B
demonstrate that incorporating ODLRI into the joint optimization framework
consistently reduces activation-aware error, minimizes quantization scale, and
improves perplexity and zero-shot accuracy in low-bit settings.

</details>


### [386] [Revisiting LRP: Positional Attribution as the Missing Ingredient for Transformer Explainability](https://arxiv.org/abs/2506.02138)
*Yarden Bakish,Itamar Zimerman,Hila Chefer,Lior Wolf*

Main category: cs.LG

Relevance: 85.0

TL;DR: 论文提出了一种改进的Layer-wise Relevance Propagation (LRP)方法，专门针对Transformer的位置编码（PE），解决了现有方法忽略PE导致的问题，并在实验中显著优于现有技术。


<details>
  <summary>Details</summary>
Motivation: 现有LRP方法在解释Transformer时忽略了位置编码（PE），导致守恒性破坏和位置相关特征的丢失。

Method: 将输入空间重新定义为位置-标记对，并提出专门的理论基础LRP规则，适用于多种位置编码方法（如Rotary、Learnable、Absolute PE）。

Result: 在微调分类器和零样本基础模型（如LLaMA 3）上的实验表明，该方法在视觉和NLP解释任务中显著优于现有技术。

Conclusion: 该方法填补了Transformer解释性工具的空白，特别是在处理位置编码时，具有理论和实践意义。

Abstract: The development of effective explainability tools for Transformers is a
crucial pursuit in deep learning research. One of the most promising approaches
in this domain is Layer-wise Relevance Propagation (LRP), which propagates
relevance scores backward through the network to the input space by
redistributing activation values based on predefined rules. However, existing
LRP-based methods for Transformer explainability entirely overlook a critical
component of the Transformer architecture: its positional encoding (PE),
resulting in violation of the conservation property, and the loss of an
important and unique type of relevance, which is also associated with
structural and positional features. To address this limitation, we reformulate
the input space for Transformer explainability as a set of position-token
pairs. This allows us to propose specialized theoretically-grounded LRP rules
designed to propagate attributions across various positional encoding methods,
including Rotary, Learnable, and Absolute PE. Extensive experiments with both
fine-tuned classifiers and zero-shot foundation models, such as LLaMA 3,
demonstrate that our method significantly outperforms the state-of-the-art in
both vision and NLP explainability tasks. Our code is publicly available.

</details>


### [387] [Angles Don't Lie: Unlocking Training-Efficient RL Through the Model's Own Signals](https://arxiv.org/abs/2506.02281)
*Qinsi Wang,Jinghan Ke,Hancheng Ye,Yueqian Lin,Yuzhe Fu,Jianyi Zhang,Kurt Keutzer,Chenfeng Xu,Yiran Chen*

Main category: cs.LG

Relevance: 85.0

TL;DR: GAIN-RL利用模型内在的角度集中信号动态选择训练数据，显著提升训练效率，在数学和编程任务上实现2.5倍加速。


<details>
  <summary>Details</summary>
Motivation: 当前强化微调（RFT）范式因均匀数据采样导致样本效率低下，传统方法忽视模型自身学习信号。

Method: 提出GAIN-RL框架，基于角度集中信号动态选择数据，优化梯度更新。

Result: GAIN-RL在多种任务和模型规模下实现2.5倍训练加速，数据效率提升。

Conclusion: GAIN-RL通过模型内在信号优化数据选择，显著提升训练效率和性能。

Abstract: Current Reinforcement Fine-tuning (RFT) paradigms for Large Language Models
(LLMs) suffer from sample inefficiency due to the redundant exposure of
identical queries under uniform data sampling. While previous work has explored
curriculum learning via heuristic difficulty metrics, these strategies exhibit
limitations by neglecting the intrinsic learning signals generated by the model
itself, thus leading to suboptimal training regimes. In this paper, we identify
a model-inherent signal termed angle concentration that effectively reflects an
LLM's capacity to learn from specific data. We theoretically and empirically
demonstrate a correlation between the angular distribution of token hidden
state vectors and the resulting gradient, revealing a learning preference for
data exhibiting higher angle concentration. Inspired by this finding, we
propose GAIN-RL, a Gradient-driven Angle-Informed Navigated RL framework. By
leveraging the model's intrinsic angle concentration signal, GAIN-RL
dynamically selects training data in each epoch, ensuring consistently
impactful gradient updates and thus significantly enhancing overall training
efficiency. Empirical evaluations show that GAIN-RL (GRPO) achieves over a 2.5x
acceleration in training efficiency across diverse mathematical and coding
tasks and varying model scales. Furthermore, GAIN-RL (GRPO)'s efficient
sampling yields data-efficient training, achieving better performance with half
the original data compared to vanilla GRPO with full training data. Code is
realsed at https://github.com/wangqinsi1/GAINRL/tree/main.

</details>


### [388] [Why Gradients Rapidly Increase Near the End of Training](https://arxiv.org/abs/2506.02285)
*Aaron Defazio*

Main category: cs.LG

Relevance: 85.0

TL;DR: 论文指出LLM训练后期梯度范数快速增加的问题，并提出简单修正方法。


<details>
  <summary>Details</summary>
Motivation: 解决LLM训练后期梯度范数异常增加的问题，提升训练稳定性。

Method: 分析权重衰减、归一化层和学习率调度的交互作用，提出修正方法。

Result: 修正后梯度行为正常化，训练损失更低。

Conclusion: 提出的方法有效解决了训练后期的梯度问题，并提升了模型性能。

Abstract: During long-duration Large Language Model (LLM) training runs the gradient
norm increases rapidly near the end of training. In this short note, we show
that this increase is due to an unintended interaction between weight decay,
normalization layers, and the learning rate schedule. We propose a simple
correction that fixes this behavior while also resulting in lower loss values
throughout training.

</details>


### [389] [Through a Steerable Lens: Magnifying Neural Network Interpretability via Phase-Based Extrapolation](https://arxiv.org/abs/2506.02300)
*Farzaneh Mahdisoltani,Saeed Mahdisoltani,Roger B. Grosse,David J. Fleet*

Main category: cs.LG

Relevance: 85.0

TL;DR: 提出了一种新框架，通过将网络梯度视为微小运动，可视化类别间的隐式路径，从而揭示模型的决策边界。


<details>
  <summary>Details</summary>
Motivation: 现有可解释性方法通常仅识别输入区域的影响，但未能阐明模型如何区分类别或如何改变输入以转换类别。

Method: 利用可逆变换（Complex Steerable Pyramid）分解图像，在变换空间中计算类别条件梯度，并通过线性外推放大梯度，展示模型从源类到目标类的移动。

Result: 实验表明，该方法能生成语义一致、空间连贯的变形，揭示分类器最敏感的方向。

Conclusion: 该方法为神经网络分类器的内部表示提供了新的可解释视角。

Abstract: Understanding the internal representations and decision mechanisms of deep
neural networks remains a critical open challenge. While existing
interpretability methods often identify influential input regions, they may not
elucidate how a model distinguishes between classes or what specific changes
would transition an input from one category to another. To address these
limitations, we propose a novel framework that visualizes the implicit path
between classes by treating the network gradient as a form of infinitesimal
motion. Drawing inspiration from phase-based motion magnification, we first
decompose images using invertible transforms-specifically the Complex Steerable
Pyramid-then compute class-conditional gradients in the transformed space.
Rather than iteratively integrating the gradient to trace a full path, we
amplify the one-step gradient to the input and perform a linear extrapolation
to expose how the model moves from source to target class. By operating in the
steerable pyramid domain, these amplified gradients produce semantically
meaningful, spatially coherent morphs that highlight the classifier's most
sensitive directions, giving insight into the geometry of its decision
boundaries. Experiments on both synthetic and real-world datasets demonstrate
that our phase-focused extrapolation yields perceptually aligned, semantically
meaningful transformations, offering a novel, interpretable lens into neural
classifiers' internal representations.

</details>


### [390] [Rewarding the Unlikely: Lifting GRPO Beyond Distribution Sharpening](https://arxiv.org/abs/2506.02355)
*Andre He,Daniel Fried,Sean Welleck*

Main category: cs.LG

Relevance: 85.0

TL;DR: 论文指出GRPO算法在多样本任务中存在偏见，并提出了一种新奖励机制（unlikeliness reward）和改进的PPO训练方法，显著提升了性能。


<details>
  <summary>Details</summary>
Motivation: GRPO在多样本任务中偏向强化常见解而忽视稀有正确解，限制了其在定理证明等任务中的实用性。

Method: 引入unlikeliness reward，并增加PPO训练轮次以减少偏见。

Result: 实验表明新方法显著提升了pass@N性能，并在miniF2F测试中达到与DeepSeek-Prover-V1.5-RL竞争的水平。

Conclusion: 新方法为RL训练定理证明器提供了简单有效的改进方案。

Abstract: Reinforcement learning has emerged as an effective framework for training
large language models on structured language-conditioned tasks. We identify a
critical flaw of Group Relative Policy Optimization (GRPO), a widely used RL
algorithm in this setting. For tasks that require multi-sample performance,
such as formal theorem proving, GRPO biasedly reinforces already probable
solutions and neglects rare but correct proofs. This implicit bias impairs
performance on pass@$N$ metrics at large sample sizes, limiting its
practicality for training theorem provers. To address this, we introduce the
unlikeliness reward, a straightforward method that explicitly encourages
reinforcing rare correct solutions. Additionally, we find that increasing the
number of PPO epochs further mitigates this bias. Our experiments confirm that
incorporating the unlikeliness reward significantly improves pass@$N$ across a
large range of N, outperforming standard GRPO and substantially increasing
sample diversity. Applying our revised recipe to Lean, we achieve competitive
performance with DeepSeek-Prover-V1.5-RL on the miniF2F-test benchmark. We
release our implementation, providing a simple yet effective recipe for
training formal theorem provers with RL.

</details>


### [391] [Evaluating LLM Agent Adherence to Hierarchical Safety Principles: A Lightweight Benchmark for Probing Foundational Controllability Components](https://arxiv.org/abs/2506.02357)
*Ram Potham*

Main category: cs.LG

Relevance: 85.0

TL;DR: 该论文提出了一种轻量级、可解释的基准方法，用于评估LLM代理在冲突任务指令下是否优先遵守安全原则。


<details>
  <summary>Details</summary>
Motivation: 为确保AI代理在安全关键场景中优先遵守原则，尤其是在与操作目标冲突时，需要早期检测控制缺陷。

Method: 使用简单的网格世界环境，测试LLM代理在冲突任务指令下是否优先遵守预设的安全原则（如“不进入危险区域”）。

Result: 初步研究表明该方法可行，并提供了代理在原则冲突下的行为洞察。

Conclusion: 评估LLM对分层原则的遵守是理解可控AI系统构建能力的重要早期步骤。

Abstract: Credible safety plans for advanced AI development require methods to verify
agent behavior and detect potential control deficiencies early. A fundamental
aspect is ensuring agents adhere to safety-critical principles, especially when
these conflict with operational goals. Failure to prioritize such principles
indicates a potential basic control failure. This paper introduces a
lightweight, interpretable benchmark methodology using a simple grid world to
evaluate an LLM agent's ability to uphold a predefined, high-level safety
principle (e.g., "never enter hazardous zones") when faced with conflicting
lower-level task instructions. We probe whether the agent reliably prioritizes
the inviolable directive, testing a foundational controllability aspect of
LLMs. This pilot study demonstrates the methodology's feasibility, offers
preliminary insights into agent behavior under principle conflict, and
discusses how such benchmarks can contribute empirical evidence for assessing
controllability. We argue that evaluating adherence to hierarchical principles
is a crucial early step in understanding our capacity to build governable AI
systems.

</details>


### [392] [Comba: Improving Nonlinear RNNs with Closed-loop Control](https://arxiv.org/abs/2506.02475)
*Jiaxi Hu,Yongqi Pan,Jusen Du,Disen Lan,Xiaqiang Tang,Qingsong Wen,Yuxuan Liang,Weigao Sun*

Main category: cs.LG

Relevance: 85.0

TL;DR: 论文提出了一种名为Comba的非线性RNN变体，结合状态反馈和输出反馈校正，在语言和视觉建模中表现出优越性能和计算效率。


<details>
  <summary>Details</summary>
Motivation: 探索非线性RNN的优势与局限性，并基于闭环控制理论设计更高效的序列建模方法。

Method: 提出Comba模型，采用标量加低秩状态转移，结合状态反馈和输出反馈校正，并实现硬件高效的并行内核。

Result: 在340M/1.3B参数规模的模型上，Comba在语言和视觉建模中表现出优越性能和计算效率。

Conclusion: Comba为非线性RNN提供了一种高效实现，适用于大规模序列建模任务。

Abstract: Recent efficient sequence modeling methods such as Gated DeltaNet, TTT, and
RWKV-7 have achieved performance improvements by supervising the recurrent
memory management through Delta learning rule. Unlike previous state-space
models (e.g., Mamba) and gated linear attentions (e.g., GLA), these models
introduce interactions between the recurrent state and the key vector,
resulting in a nonlinear recursive structure. In this paper, we first introduce
the concept of Nonlinear RNNs with a comprehensive analysis on the advantages
and limitations of these models. Then, based on closed-loop control theory, we
propose a novel Nonlinear RNN variant named Comba, which adopts a
scalar-plus-low-rank state transition, with both state feedback and output
feedback corrections. We also implement a hardware-efficient chunk-wise
parallel kernel in Triton and train models with 340M/1.3B parameters on
large-scale corpus. Comba demonstrates its superior performance and computation
efficiency in both language and vision modeling.

</details>


### [393] [HATA: Trainable and Hardware-Efficient Hash-Aware Top-k Attention for Scalable Large Model Inference](https://arxiv.org/abs/2506.02572)
*Ping Gong,Jiawei Yi,Shengnan Wang,Juncheng Zhang,Zewen Jin,Ouxiang Zhou,Ruibo Liu,Guanbin Xu,Youhui Bai,Bowen Ye,Kun Yuan,Tong Yang,Gong Zhang,Renhai Chen,Feng Wu,Cheng Li*

Main category: cs.LG

Relevance: 85.0

TL;DR: HATA（Hash-Aware Top-$k$ Attention）是一种新型的注意力机制，通过低开销的哈希技术优化Top-$k$注意力，显著提升LLM推理效率。


<details>
  <summary>Details</summary>
Motivation: LLM推理中的注意力模块是瓶颈，现有Top-$k$方法难以平衡效率与精度。

Method: HATA将查询和键映射为二进制哈希码，低成本获取相对qk分数顺序，实现高效Top-$k$注意力。

Result: HATA在保持精度的同时，速度提升7.2倍，优于现有Top-$k$方法。

Conclusion: HATA为LLM推理提供了一种高效且准确的注意力优化方案。

Abstract: Large Language Models (LLMs) have emerged as a pivotal research area, yet the
attention module remains a critical bottleneck in LLM inference, even with
techniques like KVCache to mitigate redundant computations. While various
top-$k$ attention mechanisms have been proposed to accelerate LLM inference by
exploiting the inherent sparsity of attention, they often struggled to strike a
balance between efficiency and accuracy. In this paper, we introduce HATA
(Hash-Aware Top-$k$ Attention), a novel approach that systematically integrates
low-overhead learning-to-hash techniques into the Top-$k$ attention process.
Different from the existing top-k attention methods which are devoted to
seeking an absolute estimation of qk score, typically with a great cost, HATA
maps queries and keys into binary hash codes, and acquires the relative qk
score order with a quite low cost, which is sufficient for realizing top-k
attention. Extensive experiments demonstrate that HATA achieves up to
7.2$\times$ speedup compared to vanilla full attention while maintaining model
accuracy. In addition, HATA outperforms the state-of-the-art top-$k$ attention
methods in both accuracy and efficiency across multiple mainstream LLM models
and diverse tasks. HATA is open source at https://github.com/gpzlx1/HATA.

</details>


### [394] [Heterogeneous Group-Based Reinforcement Learning for LLM-based Multi-Agent Systems](https://arxiv.org/abs/2506.02718)
*Guanzhong Chen,Shaoxiong Yang,Chao Li,Wei Liu,Jian Luan,Zenglin Xu*

Main category: cs.LG

Relevance: 85.0

TL;DR: 论文提出了一种新的无Critic多智能体强化学习算法MHGPO，用于优化基于LLM的多智能体系统，解决了传统方法如MAPPO的稳定性和计算效率问题。


<details>
  <summary>Details</summary>
Motivation: LLM在实际应用中面临知识固定和输出控制困难的问题，多智能体系统（MAS）提供了动态协作的解决方案，但现有优化方法（如MAPPO）存在训练不稳定和计算负担高的缺陷。

Method: 提出MHGPO算法，通过估计异构组rollouts的相对奖励优势来指导策略更新，无需Critic网络，并引入三种rollout采样策略。

Result: 实验表明，MHGPO在多智能体LLM搜索系统中优于MAPPO，任务性能和计算效率更高，且无需预热。

Conclusion: MHGPO为复杂LLM-based MAS的稳定和可扩展优化提供了潜力。

Abstract: Large Language Models (LLMs) have achieved remarkable success across diverse
natural language processing tasks, yet their deployment in real-world
applications is hindered by fixed knowledge cutoffs and difficulties in
generating controllable, accurate outputs in a single inference. Multi-agent
systems (MAS) built from specialized LLM agents offer a promising solution,
enabling dynamic collaboration and iterative reasoning. However, optimizing
these systems remains a challenge, as conventional methods such as prompt
engineering and supervised fine-tuning entail high engineering overhead and
limited adaptability. Reinforcement learning (RL), particularly multi-agent
reinforcement learning (MARL), provides a scalable framework by refining agent
policies based on system-level feedback. Nevertheless, existing MARL
algorithms, such as Multi-Agent Proximal Policy Optimization (MAPPO), rely on
Critic networks, which can cause training instability and increase
computational burden. To address these limitations and target the prototypical
Multi-Agent Search System (MASS), we propose Multi-Agent Heterogeneous Group
Policy Optimization (MHGPO), a novel Critic-free algorithm that guides policy
updates by estimating relative reward advantages across heterogeneous groups of
rollouts. MHGPO eliminates the need for Critic networks, enhancing stability
and reducing computational overhead. Additionally, we introduce three group
rollout sampling strategies that trade off between efficiency and
effectiveness. Experiments on a multi-agent LLM-based search system demonstrate
that MHGPO consistently outperforms MAPPO in both task performance and
computational efficiency, without requiring warm-up, underscoring its potential
for stable and scalable optimization of complex LLM-based MAS.

</details>


### [395] [WeightLoRA: Keep Only Necessary Adapters](https://arxiv.org/abs/2506.02724)
*Andrey Veprikov,Vladimir Solodkin,Alexander Zyl,Andrey Savchenko,Aleksandr Beznosikov*

Main category: cs.LG

Relevance: 85.0

TL;DR: 提出了一种名为WeightLoRA的新方法，通过自适应选择关键LoRA头来减少可训练参数，同时保持或提升性能。


<details>
  <summary>Details</summary>
Motivation: 解决LoRA在训练大型模型时内存需求高和需要直觉选择适配层的问题。

Method: 提出WeightLoRA，自适应选择关键LoRA头，减少参数数量。

Result: 在多个基准测试和模型（如DeBERTa、BART、Llama）上表现优于其他自适应方法。

Conclusion: WeightLoRA及其改进版WeightLoRA+在大多数情况下表现优异。

Abstract: The widespread utilization of language models in modern applications is
inconceivable without Parameter-Efficient Fine-Tuning techniques, such as
low-rank adaptation ($\texttt{LoRA}$), which adds trainable adapters to
selected layers. Although $\texttt{LoRA}$ may obtain accurate solutions, it
requires significant memory to train large models and intuition on which layers
to add adapters. In this paper, we propose a novel method,
$\texttt{WeightLoRA}$, which overcomes this issue by adaptive selection of the
most critical $\texttt{LoRA}$ heads throughout the optimization process. As a
result, we can significantly reduce the number of trainable parameters while
maintaining the capability to obtain consistent or even superior metric values.
We conduct experiments for a series of competitive benchmarks and DeBERTa,
BART, and Llama models, comparing our method with different adaptive
approaches. The experimental results demonstrate the efficacy of
$\texttt{WeightLoRA}$ and the superior performance of $\texttt{WeightLoRA+}$ in
almost all cases.

</details>


### [396] [BNPO: Beta Normalization Policy Optimization](https://arxiv.org/abs/2506.02864)
*Changyi Xiao,Mengdi Zhang,Yixin Cao*

Main category: cs.LG

Relevance: 85.0

TL;DR: 提出了一种名为Beta Normalization Policy Optimization (BNPO)的新方法，通过动态调整Beta分布参数自适应归一化奖励，解决了现有策略优化方法在训练稳定性上的不足。


<details>
  <summary>Details</summary>
Motivation: 现有策略优化方法在奖励归一化上存在不足，导致训练不稳定，BNPO旨在解决这一问题。

Method: BNPO利用动态更新的Beta分布参数自适应归一化奖励，并引入优势分解机制扩展其适用性。

Result: 实验证明BNPO在推理任务上达到了最先进的性能。

Conclusion: BNPO通过自适应奖励归一化提升了训练稳定性，并在理论上和实验上验证了其优越性。

Abstract: Recent studies, including DeepSeek-R1 and Kimi-k1.5, have demonstrated that
reinforcement learning with rule-based, binary-valued reward functions can
significantly enhance the reasoning capabilities of large language models.
These models primarily utilize REINFORCE-based policy optimization techniques,
such as REINFORCE with baseline and group relative policy optimization (GRPO).
However, a key limitation remains: current policy optimization methods either
neglect reward normalization or employ static normalization strategies, which
fail to adapt to the dynamic nature of policy updates during training. This may
result in unstable gradient estimates and hinder training stability. To address
this issue, we propose Beta Normalization Policy Optimization (BNPO), a novel
policy optimization method that adaptively normalizes rewards using a Beta
distribution with dynamically updated parameters. BNPO aligns the normalization
with the changing policy distribution, enabling more precise and lower-variance
gradient estimation, which in turn promotes stable training dynamics. We
provide theoretical analysis demonstrating BNPO's variance-reducing properties
and show that it generalizes both REINFORCE and GRPO under binary-valued reward
settings. Furthermore, we introduce an advantage decomposition mechanism to
extend BNPO's applicability to more complex reward systems. Experimental
results confirm that BNPO achieves state-of-the-art performance among policy
optimization methods on reasoning tasks. The code is available at
https://github.com/changyi7231/BNPO.

</details>


### [397] [Scaling Fine-Grained MoE Beyond 50B Parameters: Empirical Evaluation and Practical Insights](https://arxiv.org/abs/2506.02890)
*Jakub Krajewski,Marcin Chochowski,Daniel Korzekwa*

Main category: cs.LG

Relevance: 85.0

TL;DR: 该论文研究了细粒度混合专家（MoE）架构在大型语言模型（LLM）中的应用，通过训练方法和实证评估，证明其在模型收敛速度和性能上的优势。


<details>
  <summary>Details</summary>
Motivation: 探索细粒度MoE架构在高效扩展LLM中的潜力，提供实证支持以优化未来大规模模型的开发。

Method: 提出一套训练方法，并对细粒度MoE进行全面的实证评估，比较其与标准MoE在56B总参数模型中的表现。

Result: 在最大规模下，细粒度MoE在验证损失和下游任务准确性上表现更优。

Conclusion: 细粒度MoE为未来大规模模型开发提供了实证基础和实用见解。

Abstract: Mixture of Experts (MoE) architectures have emerged as pivotal for scaling
Large Language Models (LLMs) efficiently. Fine-grained MoE approaches -
utilizing more numerous, smaller experts - have demonstrated potential in
improving model convergence and quality. This work proposes a set of training
recipes and provides a comprehensive empirical evaluation of fine-grained MoE,
directly comparing its scaling properties against standard MoE configurations
for models with up to 56B total (17B active) parameters. We investigate
convergence speed, model performance on downstream benchmarks, and practical
training considerations across various setups. Overall, at the largest scale we
show that fine-grained MoE achieves better validation loss and higher accuracy
across a set of downstream benchmarks. This study offers empirical grounding
and practical insights for leveraging fine-grained MoE in the development of
future large-scale models.

</details>


### [398] [Abstract Counterfactuals for Language Model Agents](https://arxiv.org/abs/2506.02946)
*Edoardo Pona,Milad Kazemi,Yali Du,David Watson,Nicola Paoletti*

Main category: cs.LG

Relevance: 85.0

TL;DR: 论文提出了一个名为“抽象反事实”的框架，用于解决语言模型（LM）代理在反事实推理中的挑战，避免了传统基于标记的方法的局限性。


<details>
  <summary>Details</summary>
Motivation: 现有关于LM反事实的研究主要关注标记级别的反事实，但由于LM代理的开放性和标记含义的上下文依赖性，这种方法往往不适用或导致偏见。

Method: 提出了“抽象反事实”框架，强调动作和交互的高层特征，支持用户相关特征的反事实推理。实验包括基于文本的游戏和反事实文本生成，结合标记和潜在空间干预。

Result: 实验表明，该方法能生成一致且有意义的反事实，同时最小化标记级方法的副作用。

Conclusion: 抽象反事实框架为LM代理的反事实推理提供了更有效的工具，适用于开放动作空间和上下文依赖的场景。

Abstract: Counterfactual inference is a powerful tool for analysing and evaluating
autonomous agents, but its application to language model (LM) agents remains
challenging. Existing work on counterfactuals in LMs has primarily focused on
token-level counterfactuals, which are often inadequate for LM agents due to
their open-ended action spaces. Unlike traditional agents with fixed, clearly
defined action spaces, the actions of LM agents are often implicit in the
strings they output, making their action spaces difficult to define and
interpret. Furthermore, the meanings of individual tokens can shift depending
on the context, adding complexity to token-level reasoning and sometimes
leading to biased or meaningless counterfactuals. We introduce \emph{Abstract
Counterfactuals}, a framework that emphasises high-level characteristics of
actions and interactions within an environment, enabling counterfactual
reasoning tailored to user-relevant features. Our experiments demonstrate that
the approach produces consistent and meaningful counterfactuals while
minimising the undesired side effects of token-level methods. We conduct
experiments on text-based games and counterfactual text generation, while
considering both token-level and latent-space interventions.

</details>


### [399] [Memory-Efficient and Privacy-Preserving Collaborative Training for Mixture-of-Experts LLMs](https://arxiv.org/abs/2506.02965)
*Ze Yu Zhang,Bolin Ding,Bryan Kian Hsiang Low*

Main category: cs.LG

Relevance: 85.0

TL;DR: PC-MoE是一种隐私保护的协作式MoE方法，支持多方可协作训练更强大的LLM，同时保护数据隐私，且性能接近集中式模型。


<details>
  <summary>Details</summary>
Motivation: 解决多方协作训练LLM时的数据隐私和资源限制问题。

Method: 利用MoE的稀疏性实现内存高效的分布式训练，保护数据隐私。

Result: 在七个LLM基准测试中性能接近集中式模型，GPU内存减少70%，且抗重建攻击。

Conclusion: PC-MoE在隐私保护和性能之间取得了平衡。

Abstract: Mixture-of-Experts (MoE) has been gaining popularity due to its successful
adaptation to large language models (LLMs). In this work, we introduce
Privacy-preserving Collaborative Mixture-of-Experts (PC-MoE), which leverages
the sparsity of the MoE architecture for memory-efficient decentralized
collaborative LLM training, enabling multiple parties with limited GPU-memory
and data resources to collectively train more capable LLMs than they could
achieve individually. At the same time, this approach protects training data
privacy of each participant by keeping training data, as well as parts of the
forward pass signal and gradients locally within each party. By design, PC-MoE
synergistically combines the strengths of distributed computation with strong
confidentiality assurances. Unlike most privacy-preserving schemes, which pay
for confidentiality with lower task accuracy, our framework breaks that
trade-off: across seven popular LLM benchmarks, it almost matches (and
sometimes exceeds) the performance and convergence rate of a fully centralized
model, enjoys near 70% peak GPU RAM reduction, while being fully robust against
reconstruction attacks.

</details>


### [400] [Provable Reinforcement Learning from Human Feedback with an Unknown Link Function](https://arxiv.org/abs/2506.03066)
*Qining Zhang,Lei Ying*

Main category: cs.LG

Relevance: 85.0

TL;DR: 该论文提出了一种名为ZSPO的新型策略优化算法，用于解决RLHF中链接函数未知的问题，避免了传统方法对链接函数的依赖。


<details>
  <summary>Details</summary>
Motivation: 现有RLHF算法（如DPO和PPO）通常假设链接函数已知，但现实中人类偏好复杂，链接函数可能未知。为避免误设链接函数，研究未知链接函数下的RLHF问题。

Method: 提出ZSPO算法，基于零阶策略优化方法，利用人类偏好构建与真实策略梯度方向正相关的参数更新方向，无需知道链接函数。

Result: ZSPO在温和条件下以多项式收敛率达到平稳策略，数值实验显示其在链接函数不匹配时的优越性。

Conclusion: ZSPO是一种有效的RLHF算法，适用于链接函数未知的场景，具有理论和实际优势。

Abstract: Link functions, which characterize how human preferences are generated from
the value function of an RL problem, are a crucial component in designing RLHF
algorithms. Almost all RLHF algorithms, including state-of-the-art ones in
empirical studies such as DPO and PPO, assume the link function is known to the
agent (e.g., a logistic function according to the Bradley-Terry model), which
is arguably unrealistic considering the complex nature of human preferences. To
avoid link function mis-specification, this paper studies general RLHF problems
with unknown link functions. We propose a novel policy optimization algorithm
called ZSPO based on a new zeroth-order policy optimization method, where the
key is to use human preference to construct a parameter update direction that
is positively correlated with the true policy gradient direction. ZSPO achieves
it by estimating the sign of the value function difference instead of
estimating the gradient from the value function difference, so it does not
require knowing the link function. Under mild conditions, ZSPO converges to a
stationary policy with a polynomial convergence rate depending on the number of
policy iterations and trajectories per iteration. Numerical results also show
the superiority of ZSPO under link function mismatch.

</details>


### [401] [StreamBP: Memory-Efficient Exact Backpropagation for Long Sequence Training of LLMs](https://arxiv.org/abs/2506.03077)
*Qijun Luo,Mengqi Li,Lei Zhao,Xiao Li*

Main category: cs.LG

Relevance: 85.0

TL;DR: StreamBP是一种内存高效的精确反向传播方法，通过序列维度的线性分解显著减少激活值和逻辑的内存成本，适用于SFT、GRPO和DPO等目标。


<details>
  <summary>Details</summary>
Motivation: 长序列训练语言模型时，存储激活值的内存成本巨大，即使使用梯度检查点技术也难以解决。

Method: 提出StreamBP方法，通过层级的序列维度线性分解减少内存成本，并利用语言模型的因果结构降低计算FLOPs和加速反向传播。

Result: StreamBP将反向传播的最大序列长度扩展了2.8-5.5倍，同时保持或减少反向传播时间。

Conclusion: StreamBP是一种高效且可扩展的方法，适用于多GPU训练，并可集成到任何Transformer模型的训练流程中。

Abstract: Training language models on long sequence data is a demanding requirement for
enhancing the model's capability on complex tasks, e.g., long-chain reasoning.
However, as the sequence length scales up, the memory cost for storing
activation values becomes huge during the Backpropagation (BP) process, even
with the application of gradient checkpointing technique. To tackle this
challenge, we propose a memory-efficient and exact BP method called StreamBP,
which performs a linear decomposition of the chain rule along the sequence
dimension in a layer-wise manner, significantly reducing the memory cost of
activation values and logits. The proposed method is applicable to common
objectives such as SFT, GRPO, and DPO. From an implementation perspective,
StreamBP achieves less computational FLOPs and faster BP speed by leveraging
the causal structure of the language model. Compared to gradient checkpointing,
StreamBP scales up the maximum sequence length of BP by 2.8-5.5 times larger,
while using comparable or even less BP time. Note that StreamBP's sequence
length scaling ability can be directly transferred to batch size scaling for
accelerating training. We further develop a communication-efficient distributed
StreamBP to effectively support multi-GPU training and broaden its
applicability. Our code can be easily integrated into the training pipeline of
any transformer models and is available at https://github.com/Ledzy/StreamBP.

</details>


### [402] [From Flat to Hierarchical: Extracting Sparse Representations with Matching Pursuit](https://arxiv.org/abs/2506.03093)
*Valérie Costa,Thomas Fel,Ekdeep Singh Lubana,Bahareh Tolooshams,Demba Ba*

Main category: cs.LG

Relevance: 85.0

TL;DR: 论文提出MP-SAE，一种改进的稀疏自编码器，通过匹配追踪算法捕捉层次和非线性特征，挑战了传统线性可访问特征的假设。


<details>
  <summary>Details</summary>
Motivation: 研究神经网络表示中是否存在与线性可访问特征假设不符的层次和非线性特征，并探索如何改进稀疏自编码器（SAE）以捕捉这些特征。

Method: 设计MP-SAE，将编码器分解为残差引导的步骤，结合匹配追踪算法，以捕捉层次和非线性特征。

Result: MP-SAE能更好地捕捉条件正交特征和非线性特征，揭示了视觉语言模型中共享结构的层次性。

Conclusion: 研究支持应从表示的现象学出发设计方法，而非仅依赖线性可访问特征的假设。

Abstract: Motivated by the hypothesis that neural network representations encode
abstract, interpretable features as linearly accessible, approximately
orthogonal directions, sparse autoencoders (SAEs) have become a popular tool in
interpretability. However, recent work has demonstrated phenomenology of model
representations that lies outside the scope of this hypothesis, showing
signatures of hierarchical, nonlinear, and multi-dimensional features. This
raises the question: do SAEs represent features that possess structure at odds
with their motivating hypothesis? If not, does avoiding this mismatch help
identify said features and gain further insights into neural network
representations? To answer these questions, we take a construction-based
approach and re-contextualize the popular matching pursuits (MP) algorithm from
sparse coding to design MP-SAE -- an SAE that unrolls its encoder into a
sequence of residual-guided steps, allowing it to capture hierarchical and
nonlinearly accessible features. Comparing this architecture with existing SAEs
on a mixture of synthetic and natural data settings, we show: (i) hierarchical
concepts induce conditionally orthogonal features, which existing SAEs are
unable to faithfully capture, and (ii) the nonlinear encoding step of MP-SAE
recovers highly meaningful features, helping us unravel shared structure in the
seemingly dichotomous representation spaces of different modalities in a
vision-language model, hence demonstrating the assumption that useful features
are solely linearly accessible is insufficient. We also show that the
sequential encoder principle of MP-SAE affords an additional benefit of
adaptive sparsity at inference time, which may be of independent interest.
Overall, we argue our results provide credence to the idea that
interpretability should begin with the phenomenology of representations, with
methods emerging from assumptions that fit it.

</details>


### [403] [Retrieval-Augmented Generation as Noisy In-Context Learning: A Unified Theory and Risk Bounds](https://arxiv.org/abs/2506.03100)
*Yang Guo,Yutian Tao,Yifei Ming,Robert D. Nowak,Yingyu Liang*

Main category: cs.LG

Relevance: 85.0

TL;DR: 本文提出了首个针对检索增强生成（RAG）的有限样本泛化界，并推导了偏差-方差权衡。理论分析表明RAG存在泛化误差的上限，并通过实验验证了ICL和RAG的样本效率。


<details>
  <summary>Details</summary>
Motivation: 尽管RAG在实证中表现出色，但其理论分析仍不足。本文旨在填补这一空白，为RAG提供理论支持。

Method: 通过将检索文本视为查询相关的噪声上下文示例，提出了一个框架，涵盖ICL和标准RAG的极限情况。引入了均匀和非均匀RAG噪声来建模训练数据和外部语料库的检索。

Result: 理论分析表明RAG存在泛化误差上限，实验在Natural Questions和TriviaQA等基准上验证了ICL和RAG的样本效率。

Conclusion: 本文为RAG提供了首个理论框架，揭示了其泛化误差上限，并通过实验验证了其有效性。

Abstract: Retrieval-augmented generation (RAG) has seen many empirical successes in
recent years by aiding the LLM with external knowledge. However, its
theoretical aspect has remained mostly unexplored. In this paper, we propose
the first finite-sample generalization bound for RAG in in-context linear
regression and derive an exact bias-variance tradeoff. Our framework views the
retrieved texts as query-dependent noisy in-context examples and recovers the
classical in-context learning (ICL) and standard RAG as the limit cases. Our
analysis suggests that an intrinsic ceiling on generalization error exists on
RAG as opposed to the ICL. Furthermore, our framework is able to model
retrieval both from the training data and from external corpora by introducing
uniform and non-uniform RAG noise. In line with our theory, we show the sample
efficiency of ICL and RAG empirically with experiments on common QA benchmarks,
such as Natural Questions and TriviaQA.

</details>


### [404] [On Weak-to-Strong Generalization and f-Divergence](https://arxiv.org/abs/2506.03109)
*Wei Yao,Gengze Xu,Huayi Tang,Wenkai Yang,Donglin Di,Ziqiao Wang,Yong Liu*

Main category: cs.LG

Relevance: 85.0

TL;DR: 论文提出使用$f$-散度作为弱到强泛化（W2SG）中的信息论损失函数框架，提升强模型的泛化能力和噪声容忍度。


<details>
  <summary>Details</summary>
Motivation: 现有W2SG方法通常需要额外的弱模型或复杂流程，导致计算和内存开销大。$f$-散度在机器学习中表现优异，因此被引入W2SG。

Method: 提出基于$f$-散度的信息论损失函数框架，理论分析了不同$f$-散度的局限性和等价性，并通过样本复杂度界限和信息论见解支持。

Result: 实验表明，$f$-散度损失（如KL散度）能有效提升强模型的泛化能力和噪声容忍度。

Conclusion: $f$-散度损失为W2SG提供了一种高效且通用的解决方案。

Abstract: Weak-to-strong generalization (W2SG) has emerged as a promising paradigm for
stimulating the capabilities of strong pre-trained models by leveraging
supervision from weaker supervisors. To improve the performance of the strong
model, existing methods often require additional weak models or complex
procedures, leading to substantial computational and memory overhead. Motivated
by the effectiveness of $f$-divergence loss in various machine learning
domains, we introduce $f$-divergence as an information-theoretic loss function
framework in W2SG. Our theoretical analysis reveals fundamental limitations and
equivalence of different $f$-divergence losses in W2SG, supported by sample
complexity bounds and information-theoretic insights. We empirically
demonstrate that $f$-divergence loss, which generalizes widely-used metrics
like KL divergence, effectively improves generalization and noise tolerance of
the strong model in practice.

</details>


### [405] [PoLAR: Polar-Decomposed Low-Rank Adapter Representation](https://arxiv.org/abs/2506.03133)
*Kai Lion,Liang Zhang,Bingcong Li,Niao He*

Main category: cs.LG

Relevance: 85.0

TL;DR: 论文提出PoLAR方法，通过极分解优化低秩适应，提升模型微调性能。


<details>
  <summary>Details</summary>
Motivation: 现有低秩适应方法存在稳定秩低于线性代数秩的问题，导致子空间利用率不足，影响微调效果。

Method: 提出PoLAR参数化方法，将低秩更新分解为两个约束在Stiefel流形上的方向矩阵和一个无约束的尺度矩阵，并结合黎曼优化。

Result: 理论证明PoLAR在低秩适应问题上具有指数级更快的收敛速度，实验在三个基准测试中（语言理解、常识推理、数学问题求解）均取得显著提升。

Conclusion: PoLAR有效解决了低秩适应中的子空间利用不足问题，显著提升了模型微调性能。

Abstract: We show that low-rank adaptation of large-scale models suffers from a low
stable rank that is well below the linear algebraic rank of the subspace,
degrading fine-tuning performance. To mitigate the underutilization of the
allocated subspace, we propose PoLAR, a parameterization inspired by the polar
decomposition that factorizes the low-rank update into two direction matrices
constrained to Stiefel manifolds and an unconstrained scale matrix. Our theory
shows that PoLAR yields an exponentially faster convergence rate on a canonical
low-rank adaptation problem. Pairing the parameterization with Riemannian
optimization leads to consistent gains on three different benchmarks testing
general language understanding, commonsense reasoning, and mathematical problem
solving with base model sizes ranging from 350M to 27B.

</details>


### [406] [Not All Tokens Are Meant to Be Forgotten](https://arxiv.org/abs/2506.03142)
*Xiangyu Zhou,Yao Qiang,Saleh Zare Zade,Douglas Zytko,Prashant Khanduri,Dongxiao Zhu*

Main category: cs.LG

Relevance: 85.0

TL;DR: 论文提出了一种名为TIF的框架，通过区分不需要的单词和一般单词，优化遗忘过程，减少模型效用损失。


<details>
  <summary>Details</summary>
Motivation: 解决LLMs在遗忘不需要信息时过度遗忘的问题，同时保留模型的有用性。

Method: TIF框架包括目标信息标识器和目标偏好优化方法，使用Logit Preference Loss和Preservation Loss。

Result: 在TOFU和MUSE基准测试中表现优异，有效提升遗忘效果并保持模型效用。

Conclusion: TIF框架在遗忘不需要信息的同时，显著减少了对模型效用的负面影响。

Abstract: Large Language Models (LLMs), pre-trained on massive text corpora, exhibit
remarkable human-level language understanding, reasoning, and decision-making
abilities. However, they tend to memorize unwanted information, such as private
or copyrighted content, raising significant privacy and legal concerns.
Unlearning has emerged as a promising solution, but existing methods face a
significant challenge of over-forgetting. This issue arises because they
indiscriminately suppress the generation of all the tokens in forget samples,
leading to a substantial loss of model utility. To overcome this challenge, we
introduce the Targeted Information Forgetting (TIF) framework, which consists
of (1) a flexible targeted information identifier designed to differentiate
between unwanted words (UW) and general words (GW) in the forget samples, and
(2) a novel Targeted Preference Optimization approach that leverages Logit
Preference Loss to unlearn unwanted information associated with UW and
Preservation Loss to retain general information in GW, effectively improving
the unlearning process while mitigating utility degradation. Extensive
experiments on the TOFU and MUSE benchmarks demonstrate that the proposed TIF
framework enhances unlearning effectiveness while preserving model utility and
achieving state-of-the-art results.

</details>


### [407] [Efficient and Workload-Aware LLM Serving via Runtime Layer Swapping and KV Cache Resizing](https://arxiv.org/abs/2506.02006)
*Zhaoyuan Su,Tingfeng Lan,Zirui Wang,Juncheng Yang,Yue Cheng*

Main category: cs.DC

Relevance: 85.0

TL;DR: MorphServe是一个动态、工作负载感知的LLM服务框架，通过量化层交换和KV缓存调整优化动态工作负载下的服务效率。


<details>
  <summary>Details</summary>
Motivation: 现有LLM服务框架和静态模型压缩技术无法适应工作负载波动，导致服务目标（SLO）违规或精度下降。

Method: MorphServe采用两种异步、令牌级运行时机制：量化层交换和压力感知KV缓存调整。

Result: 实验表明，MorphServe减少92.45%的SLO违规，提升P95 TTFT延迟2.2x-3.9x，且不损害生成质量。

Conclusion: MorphServe是动态环境中LLM部署的实用弹性解决方案。

Abstract: Efficiently serving large language models (LLMs) under dynamic and bursty
workloads remains a key challenge for real-world deployment. Existing serving
frameworks and static model compression techniques fail to adapt to workload
fluctuations, leading to either service-level objective (SLO) violations under
full-precision serving or persistent accuracy degradation with static
quantization. We present MorphServe, a dynamic, workload-aware LLM serving
framework based on morphological adaptation. MorphServe introduces two
asynchronous, token-level runtime mechanisms: quantized layer swapping, which
selectively replaces less impactful layers with quantized alternatives during
high-load periods, and pressure-aware KV cache resizing, which dynamically
adjusts KV cache capacity in response to memory pressure. These mechanisms
enable state-preserving transitions with minimum runtime overhead and are fully
compatible with modern scheduling and attention techniques. Extensive
experiments on Vicuna and Llama family models with real-world workloads
demonstrate that MorphServe reduces average SLO violations by 92.45 percent and
improves the P95 TTFT latency by 2.2x-3.9x compared to full-precision serving,
without compromising generation quality. These results establish MorphServe as
a practical and elastic solution for LLM deployment in dynamic environments.

</details>


### [408] [Matrix Is All You Need](https://arxiv.org/abs/2506.01966)
*Yuzhou Zhu*

Main category: cs.LG

Relevance: 75.0

TL;DR: 论文提出了一种统一的矩阵阶框架，将卷积、循环和自注意力操作视为稀疏矩阵乘法，证明了与标准CNN、RNN和Transformer层的代数同构，并在多个任务上验证了性能匹配或超越原生模型。


<details>
  <summary>Details</summary>
Motivation: 探索神经网络架构的共性，减少架构设计的复杂性，同时提升硬件并行性和优化潜力。

Method: 通过稀疏矩阵乘法统一卷积、循环和自注意力操作，利用三角矩阵和矩阵分解实现不同操作。

Result: 在图像分类、时间序列预测和语言建模任务中，稀疏矩阵形式表现优于或等同于原生模型，且收敛速度更快。

Conclusion: 矩阵视角为神经网络架构提供了数学基础，支持硬件感知的网络设计。

Abstract: Deep neural networks employ specialized architectures for vision, sequential
and language tasks, yet this proliferation obscures their underlying
commonalities. We introduce a unified matrix-order framework that casts
convolutional, recurrent and self-attention operations as sparse matrix
multiplications. Convolution is realized via an upper-triangular weight matrix
performing first-order transformations; recurrence emerges from a
lower-triangular matrix encoding stepwise updates; attention arises naturally
as a third-order tensor factorization. We prove algebraic isomorphism with
standard CNN, RNN and Transformer layers under mild assumptions. Empirical
evaluations on image classification (MNIST, CIFAR-10/100, Tiny ImageNet),
time-series forecasting (ETTh1, Electricity Load Diagrams) and language
modeling/classification (AG News, WikiText-2, Penn Treebank) confirm that
sparse-matrix formulations match or exceed native model performance while
converging in comparable or fewer epochs. By reducing architecture design to
sparse pattern selection, our matrix perspective aligns with GPU parallelism
and leverages mature algebraic optimization tools. This work establishes a
mathematically rigorous substrate for diverse neural architectures and opens
avenues for principled, hardware-aware network design.

</details>


### [409] [Equally Critical: Samples, Targets, and Their Mappings in Datasets](https://arxiv.org/abs/2506.01987)
*Runkang Yang,Peng Sun,Xinyi Shang,Yi Tang,Tao Lin*

Main category: cs.LG

Relevance: 75.0

TL;DR: 本文探讨了数据中样本和目标的双重属性对训练动态的影响，提出了一种统一的损失框架，并通过实验分析了目标和样本的类型、数量和质量对训练效率的影响。


<details>
  <summary>Details</summary>
Motivation: 现有研究在数据高效学习中主要关注样本优化（如数据集蒸馏），而忽视了目标的作用。本文旨在研究样本和目标如何共同影响训练动态。

Method: 首先通过样本-目标交互视角对现有范式进行分类，然后提出一种统一的损失框架，并通过实验分析目标和样本的变化对训练的影响。

Result: 通过实验研究，提供了六项关键见解，以提升训练效率。

Conclusion: 样本和目标的共同优化对训练动态有显著影响，本文提出的框架为理解和改进训练效率提供了新思路。

Abstract: Data inherently possesses dual attributes: samples and targets. For targets,
knowledge distillation has been widely employed to accelerate model
convergence, primarily relying on teacher-generated soft target supervision.
Conversely, recent advancements in data-efficient learning have emphasized
sample optimization techniques, such as dataset distillation, while neglected
the critical role of target. This dichotomy motivates our investigation into
understanding how both sample and target collectively influence training
dynamic. To address this gap, we first establish a taxonomy of existing
paradigms through the lens of sample-target interactions, categorizing them
into distinct sample-to-target mapping strategies. Building upon this
foundation, we then propose a novel unified loss framework to assess their
impact on training efficiency. Through extensive empirical studies on our
proposed strategies, we comprehensively analyze how variations in target and
sample types, quantities, and qualities influence model training, providing six
key insights to enhance training efficacy.

</details>


### [410] [SynthRL: Scaling Visual Reasoning with Verifiable Data Synthesis](https://arxiv.org/abs/2506.02096)
*Zijian Wu,Jinjie Ni,Xiangyan Liu,Zichen Liu,Hang Yan,Michael Qizhe Shieh*

Main category: cs.LG

Relevance: 75.0

TL;DR: SynthRL是一种通过合成RL数据提升视觉语言模型（VLM）性能的管道，包含种子问题选择、增强和验证三个阶段，显著提升模型在复杂推理任务上的表现。


<details>
  <summary>Details</summary>
Motivation: 研究如何通过合成RL数据进一步提升RLVR（带可验证奖励的强化学习）在视觉语言模型中的效果，以增强模型的推理能力。

Method: 提出SynthRL管道，包括种子问题选择、问题增强（保留答案但增加难度）和验证阶段，确保数据的正确性和难度提升。

Result: 在MMK12数据集上，SynthRL从8K种子样本生成了3.3K额外问题，模型在五个跨域视觉数学推理基准上表现显著提升，尤其在复杂样本上效果更明显。

Conclusion: SynthRL通过合成数据有效提升了模型的复杂推理能力，证明了其在增强RLVR训练中的潜力。

Abstract: Vision-language models (VLMs) trained via reinforcement learning with
verifiable reward (RLVR) have shown notable progress in scaling test-time
compute effectively. In this work, we investigate how synthesized RL data can
further improve RLVR. To this end, we propose \textbf{SynthRL}-a scalable and
guaranteed pipeline for automatic data scaling in reasoning-oriented RL
training. SynthRL comprises three key stages: (1) selecting seed questions with
appropriate distribution, (2) augmenting them into more challenging variants
while preserving the original answers, and (3) a guaranteed verification stage
that ensures near-perfect correctness and difficulty enhancement. Our empirical
experiments demonstrate SynthRL's scalability and effectiveness. When applied
to the MMK12 dataset, SynthRL synthesizes over 3.3K additional verifiable,
challenging questions from approximately 8K seed samples. Models trained with
our synthesized data achieve consistent gains across five out-of-domain visual
math reasoning benchmarks, with a significant improvement over baseline models
trained on seed data alone. Notably, detailed analysis reveals that the gains
are more pronounced on the most challenging evaluation samples, highlighting
SynthRL's effectiveness in eliciting deeper and more complex reasoning
patterns.

</details>


### [411] [Reconciling Hessian-Informed Acceleration and Scalar-Only Communication for Efficient Federated Zeroth-Order Fine-Tuning](https://arxiv.org/abs/2506.02370)
*Zhe Li,Bicheng Ying,Zidong Liu,Chaosheng Dong,Haibo Yang*

Main category: cs.LG

Relevance: 75.0

TL;DR: HiSo是一种基于Hessian信息的零阶优化方法，用于联邦学习中大语言模型的高效微调，显著提升了收敛速度和通信效率。


<details>
  <summary>Details</summary>
Motivation: 联邦学习（FL）中零阶随机梯度下降（ZO-SGD）的高方差导致收敛缓慢，而Hessian信息虽能加速优化，但难以在FL中应用。本文旨在解决这一问题。

Method: 提出HiSo方法，通过Hessian信息加速优化，同时保持标量通信的低成本。理论分析表明其收敛速度更快。

Result: 实验证明HiSo在收敛速度和通信效率上优于现有ZO-based FL方法。

Conclusion: HiSo为LLM的联邦微调提供了一种高效且通信成本低的解决方案。

Abstract: Recent dimension-free communication frameworks in Federated Learning (FL),
such as DeComFL, significantly reduce per-round communication by transmitting
only scalars via zeroth-order stochastic gradient descent (ZO-SGD). This method
is particularly advantageous for federated fine-tuning of Large Language Models
(LLMs). Yet, the high variance in ZO gradient estimation typically leads to
slow convergence. Although leveraging Hessian information is known to enhance
optimization speed, integrating this into FL presents significant challenges.
These include clients' restrictions on local data and the critical need to
maintain the dimension-free communication property. To overcome this
limitation, we first introduce a generalized scalar-only communication FL
framework that decouples dimension-free communication from standard ZO-SGD,
enabling the integration of more advanced optimization strategies. Building on
this framework, we propose HiSo, a fast federated fine-tuning method via
Hessian-informed zeroth-order optimization and Scalar-only communication.
Specifically, it leverages global curvature information to accelerate
convergence while preserving the same minimal communication cost per round.
Theoretically, we establish convergence guarantees that are independent of the
global Lipschitz constant, and further show that HiSo achieves faster rates
when the global Hessian exhibits a low effective rank -- a common phenomenon in
LLMs. Extensive experiments on benchmark datasets and LLM fine-tuning tasks
confirm that HiSo significantly outperforms existing ZO-based FL methods in
both convergence speed and communication efficiency.

</details>


### [412] [Univariate to Multivariate: LLMs as Zero-Shot Predictors for Time-Series Forecasting](https://arxiv.org/abs/2506.02389)
*Chamara Madarasingha,Nasrin Sohrabi,Zahir Tari*

Main category: cs.LG

Relevance: 75.0

TL;DR: LLMPred enhances LLM-based time-series prediction by converting sequences to text and using decomposition and prompt-processing techniques, achieving competitive results with smaller LLMs.


<details>
  <summary>Details</summary>
Motivation: To explore LLMs' effectiveness in handling complex, noisy, and multivariate time-series data, which remains underexplored.

Method: Proposes LLMPred, which converts time-series sequences into text and uses decomposition for univariate data and prompt-processing for multivariate data.

Result: LLMPred achieves competitive or superior performance with smaller LLMs like Llama 2 7B and GPT-4o-mini.

Conclusion: LLMPred's key components are validated as effective for enhancing LLM-based time-series prediction.

Abstract: Time-series prediction or forecasting is critical across many real-world
dynamic systems, and recent studies have proposed using Large Language Models
(LLMs) for this task due to their strong generalization capabilities and
ability to perform well without extensive pre-training. However, their
effectiveness in handling complex, noisy, and multivariate time-series data
remains underexplored. To address this, we propose LLMPred which enhances
LLM-based time-series prediction by converting time-series sequences into text
and feeding them to LLMs for zero shot prediction along with two main data
pre-processing techniques. First, we apply time-series sequence decomposition
to facilitate accurate prediction on complex and noisy univariate sequences.
Second, we extend this univariate prediction capability to multivariate data
using a lightweight prompt-processing strategy. Extensive experiments with
smaller LLMs such as Llama 2 7B, Llama 3.2 3B, GPT-4o-mini, and DeepSeek 7B
demonstrate that LLMPred achieves competitive or superior performance compared
to state-of-the-art baselines. Additionally, a thorough ablation study
highlights the importance of the key components proposed in LLMPred.

</details>


### [413] [HAM: A Hyperbolic Step to Regulate Implicit Bias](https://arxiv.org/abs/2506.02630)
*Tom Jacobs,Advait Gadhikar,Celia Rubio-Madrigal,Rebekka Burkholz*

Main category: cs.LG

Relevance: 75.0

TL;DR: HAM（Hyperbolic Aware Minimization）是一种结合优化器步骤和双曲镜像步骤的新方法，旨在克服双曲隐式偏差导致的收敛缓慢问题，提升性能。


<details>
  <summary>Details</summary>
Motivation: 研究优化算法的隐式偏差对深度学习模型泛化行为的影响，特别是双曲隐式偏差导致的收敛缓慢问题。

Method: 提出HAM方法，交替使用优化器步骤和双曲镜像步骤，结合梯度下降推导黎曼梯度流，提升收敛性能。

Result: HAM在多种任务（包括视觉、图分类和LLM微调）中表现优异，尤其在稀疏化方法中超越现有技术。

Conclusion: HAM通过双曲步骤显著提升性能，计算开销低，易于集成到现有优化器中。

Abstract: Understanding the implicit bias of optimization algorithms has become central
to explaining the generalization behavior of deep learning models. For
instance, the hyperbolic implicit bias induced by the overparameterization $m
\odot w$--though effective in promoting sparsity--can result in a small
effective learning rate, which slows down convergence. To overcome this
obstacle, we propose HAM (Hyperbolic Aware Minimization), which alternates
between an optimizer step and a new hyperbolic mirror step. We derive the
Riemannian gradient flow for its combination with gradient descent, leading to
improved convergence and a similar beneficial hyperbolic geometry as $m \odot
w$ for feature learning. We provide an interpretation of the the algorithm by
relating it to natural gradient descent, and an exact characterization of its
implicit bias for underdetermined linear regression. HAM's implicit bias
consistently boosts performance--even of dense training, as we demonstrate in
experiments across diverse tasks, including vision, graph and node
classification, and large language model fine-tuning. HAM is especially
effective in combination with different sparsification methods, improving upon
the state of the art. The hyperbolic step requires minimal computational and
memory overhead, it succeeds even with small batch sizes, and its
implementation integrates smoothly with existing optimizers.

</details>


### [414] [RATFM: Retrieval-augmented Time Series Foundation Model for Anomaly Detection](https://arxiv.org/abs/2506.02081)
*Chihiro Maru,Shoetsu Sato*

Main category: cs.LG

Relevance: 70.0

TL;DR: 提出了一种检索增强的时间序列基础模型（RATFM），解决了时间序列基础模型在测试时适应中无法利用示例的问题，性能接近领域微调但避免了其依赖。


<details>
  <summary>Details</summary>
Motivation: 时间序列基础模型在不同领域和任务中表现不一，且无法自然利用示例或指令，因此需要一种方法使其能够适应测试时的需求。

Method: 提出RATFM，通过检索增强使预训练模型能够结合测试时适应的示例。

Result: 在UCR Anomaly Archive数据集上，RATFM表现接近领域微调，但避免了其领域依赖性。

Conclusion: RATFM为时间序列基础模型提供了一种有效的测试时适应方法，无需领域微调。

Abstract: Inspired by the success of large language models (LLMs) in natural language
processing, recent research has explored the building of time series foundation
models and applied them to tasks such as forecasting, classification, and
anomaly detection. However, their performances vary between different domains
and tasks. In LLM-based approaches, test-time adaptation using example-based
prompting has become common, owing to the high cost of retraining. In the
context of anomaly detection, which is the focus of this study, providing
normal examples from the target domain can also be effective. However, time
series foundation models do not naturally acquire the ability to interpret or
utilize examples or instructions, because the nature of time series data used
during training does not encourage such capabilities. To address this
limitation, we propose a retrieval augmented time series foundation model
(RATFM), which enables pretrained time series foundation models to incorporate
examples of test-time adaptation. We show that RATFM achieves a performance
comparable to that of in-domain fine-tuning while avoiding domain-dependent
fine-tuning. Experiments on the UCR Anomaly Archive, a multi-domain dataset
including nine domains, confirms the effectiveness of the proposed approach.

</details>


### [415] [Bregman Centroid Guided Cross-Entropy Method](https://arxiv.org/abs/2506.02205)
*Yuliang Gu,Hongpeng Cao,Marco Caccamo,Naira Hovakimyan*

Main category: cs.LG

Relevance: 70.0

TL;DR: 提出了Bregman Centroid Guided CEM（BC-EvoCEM），通过Bregman centroids增强CEM的多样性和收敛性，适用于多模态优化问题。


<details>
  <summary>Details</summary>
Motivation: 解决传统CEM在多模态优化中因单模态采样导致的早熟收敛问题。

Method: 利用Bregman centroids进行信息聚合和多样性控制，更新贡献最小的CEM worker。

Result: 在合成基准、导航任务和MBRL中提升了收敛性和解的质量。

Conclusion: BC-EvoCEM为CEM提供了一种简单有效的改进方法。

Abstract: The Cross-Entropy Method (CEM) is a widely adopted trajectory optimizer in
model-based reinforcement learning (MBRL), but its unimodal sampling strategy
often leads to premature convergence in multimodal landscapes. In this work, we
propose Bregman Centroid Guided CEM ($\mathcal{BC}$-EvoCEM), a lightweight
enhancement to ensemble CEM that leverages $\textit{Bregman centroids}$ for
principled information aggregation and diversity control.
$\textbf{$\mathcal{BC}$-EvoCEM}$ computes a performance-weighted Bregman
centroid across CEM workers and updates the least contributing ones by sampling
within a trust region around the centroid. Leveraging the duality between
Bregman divergences and exponential family distributions, we show that
$\textbf{$\mathcal{BC}$-EvoCEM}$ integrates seamlessly into standard CEM
pipelines with negligible overhead. Empirical results on synthetic benchmarks,
a cluttered navigation task, and full MBRL pipelines demonstrate that
$\textbf{$\mathcal{BC}$-EvoCEM}$ enhances both convergence and solution
quality, providing a simple yet effective upgrade for CEM.

</details>


### [416] [Exchangeability in Neural Network Architectures and its Application to Dynamic Pruning](https://arxiv.org/abs/2506.02210)
*Pu,Yi,Tianlang Chen,Yifan Yang,Sara Achour*

Main category: cs.LG

Relevance: 70.0

TL;DR: 论文提出了一种基于对称性冗余的动态剪枝算法ExPrune，通过识别和移除神经网络中的冗余计算，显著减少了FLOPs，同时保持模型精度。


<details>
  <summary>Details</summary>
Motivation: 神经网络参数和计算资源需求不断增加，而对称性冗余未被充分利用以提高效率。本文旨在利用交换性统计特性，动态剪枝以减少冗余。

Method: 通过形式化神经网络中的对称性（交换性），提出动态剪枝算法ExPrune，并在ReLU激活中实现神经元级剪枝。

Result: ExPrune在多个模型上实现了10.98-39.05%的FLOPs减少，精度损失可忽略或不超过1%。与静态剪枝结合时，效果进一步提升。

Conclusion: ExPrune是一种有效的动态剪枝方法，能够显著提升神经网络的计算效率，且与静态剪枝互补。

Abstract: Neural networks (NNs) are equipped with increasingly many parameters and
require more and more resource for deployment. Researchers have explored
various ways to improve the efficiency of NNs by identifying and reducing the
redundancy, such as pruning or quantizing unimportant weights. Symmetry in the
NN architectures has been identified by prior work as a possible type of
redundancy, but exploiting it for efficient inference is not yet explored. In
this work, we formalize the symmetry of parameters and intermediate values in
NNs using the statistical property of exchangeablility. We identify that
exchangeable values in NN computation may contain overlapping information,
leading to redundancy. Exploiting the insight, we derive a principled general
dynamic pruning algorithm ExPrune to remove symmetry-induced redundancy on a
per-input basis. We also provide an instantiation of ExPrune that performs
neuron-level dynamic pruning by predicting negative inputs to ReLU activations.
We evaluate ExPrune on two computer vision models, one graph model and one
language model. ExPrune provides 10.98--26.3% reduction in FLOPs with
negligible accuracy drop and 21.01--39.05% reduction in FLOPs with at most 1%
accuracy drop. We also demonstrate that ExPrune composes with static pruning.
On models that have been aggressively pruned statically, ExPrune provides
additional 10.24--11.11% reduction in FLOPs with negligible accuracy drop and
13.91--14.39% reduction in FLOPs with at most 1% accuracy drop.

</details>


### [417] [SafeOR-Gym: A Benchmark Suite for Safe Reinforcement Learning Algorithms on Practical Operations Research Problems](https://arxiv.org/abs/2506.02255)
*Asha Ramanujam,Adam Elyoumi,Hao Chen,Sai Madhukiran Kompalli,Akshdeep Singh Ahluwalia,Shraman Pal,Dimitri J. Papageorgiou,Can Li*

Main category: cs.LG

Relevance: 70.0

TL;DR: SafeOR-Gym是一个针对复杂约束下安全强化学习的基准测试套件，包含九个操作研究环境，旨在推动高风险领域的安全RL研究。


<details>
  <summary>Details</summary>
Motivation: 现有安全RL基准主要针对机器人学和控制任务，缺乏对高复杂度领域（如能源系统、制造业）的适用性，限制了安全RL在关键领域的应用。

Method: 提出了SafeOR-Gym，一个包含九个操作研究环境的基准套件，支持混合离散-连续动作空间和约束违反成本。

Result: 评估了多种安全RL算法，发现当前方法在某些任务上表现良好，但在其他任务上存在根本性局限。

Conclusion: SafeOR-Gym为安全RL研究提供了一个实用且具有挑战性的测试平台，有望推动实际决策问题的研究。

Abstract: Most existing safe reinforcement learning (RL) benchmarks focus on robotics
and control tasks, offering limited relevance to high-stakes domains that
involve structured constraints, mixed-integer decisions, and industrial
complexity. This gap hinders the advancement and deployment of safe RL in
critical areas such as energy systems, manufacturing, and supply chains. To
address this limitation, we present SafeOR-Gym, a benchmark suite of nine
operations research (OR) environments tailored for safe RL under complex
constraints. Each environment captures a realistic planning, scheduling, or
control problems characterized by cost-based constraint violations, planning
horizons, and hybrid discrete-continuous action spaces. The suite integrates
seamlessly with the Constrained Markov Decision Process (CMDP) interface
provided by OmniSafe. We evaluate several state-of-the-art safe RL algorithms
across these environments, revealing a wide range of performance: while some
tasks are tractable, others expose fundamental limitations in current
approaches. SafeOR-Gym provides a challenging and practical testbed that aims
to catalyze future research in safe RL for real-world decision-making problems.
The SafeOR-Gym framework and all accompanying code are available at:
https://github.com/li-group/SafeOR-Gym.

</details>


### [418] [MINT: Multimodal Instruction Tuning with Multimodal Interaction Grouping](https://arxiv.org/abs/2506.02308)
*Xiaojun Shan,Qi Cao,Xing Han,Haofei Yu,Paul Pu Liang*

Main category: cs.LG

Relevance: 70.0

TL;DR: 论文提出了一种名为MINT的任务分组策略，用于多模态指令微调，通过按多模态交互类型分组任务，显著提升了性能。


<details>
  <summary>Details</summary>
Motivation: 当前多模态基础模型通过大规模预训练和指令微调取得突破，但单纯增加指令微调任务数量并不能持续提升性能。

Method: 提出MINT策略，按多模态交互类型（如冗余共享信息、独特信息优先选择或协同融合）分组任务，以减少不匹配任务的干扰。

Result: MINT在多模态指令微调中显著优于现有任务分组基线，实现了泛化与专业化的平衡。

Conclusion: 任务分组策略对多模态指令微调的性能提升至关重要，MINT提供了一种简单有效的方法。

Abstract: Recent advances in multimodal foundation models have achieved
state-of-the-art performance across a range of tasks. These breakthroughs are
largely driven by new pre-training paradigms that leverage large-scale,
unlabeled multimodal data, followed by instruction fine-tuning on curated
labeled datasets and high-quality prompts. While there is growing interest in
scaling instruction fine-tuning to ever-larger datasets in both quantity and
scale, our findings reveal that simply increasing the number of
instruction-tuning tasks does not consistently yield better performance.
Instead, we observe that grouping tasks by the common interactions across
modalities, such as discovering redundant shared information, prioritizing
modality selection with unique information, or requiring synergistic fusion to
discover new information from both modalities, encourages the models to learn
transferrable skills within a group while suppressing interference from
mismatched tasks. To this end, we introduce MINT, a simple yet surprisingly
effective task-grouping strategy based on the type of multimodal interaction.
We demonstrate that the proposed method greatly outperforms existing task
grouping baselines for multimodal instruction tuning, striking an effective
balance between generalization and specialization.

</details>


### [419] [Absorb and Converge: Provable Convergence Guarantee for Absorbing Discrete Diffusion Models](https://arxiv.org/abs/2506.02318)
*Yuchen Liang,Renxiang Huang,Lifeng Lai,Ness Shroff,Yingbin Liang*

Main category: cs.LG

Relevance: 70.0

TL;DR: 本文首次为使用吸收率矩阵的离散扩散模型提供了有限时间误差界和收敛率分析，填补了理论空白。


<details>
  <summary>Details</summary>
Motivation: 吸收率矩阵在离散数据应用中表现优于均匀率矩阵，但缺乏理论支持。本文旨在填补这一空白。

Method: 通过引入代理初始化分布解决KL散度问题，并分析τ-leaping和均匀化采样器的收敛性。

Result: 证明了吸收率矩阵下采样器的收敛性，并展示了优于均匀率矩阵的收敛速率。

Conclusion: 本文为吸收率矩阵的离散扩散模型提供了首个理论分析框架，并提出了新的技术工具。

Abstract: Discrete state space diffusion models have shown significant advantages in
applications involving discrete data, such as text and image generation. It has
also been observed that their performance is highly sensitive to the choice of
rate matrices, particularly between uniform and absorbing rate matrices. While
empirical results suggest that absorbing rate matrices often yield better
generation quality compared to uniform rate matrices, existing theoretical
works have largely focused on the uniform rate matrices case. Notably,
convergence guarantees and error analyses for absorbing diffusion models are
still missing. In this work, we provide the first finite-time error bounds and
convergence rate analysis for discrete diffusion models using absorbing rate
matrices. We begin by deriving an upper bound on the KL divergence of the
forward process, introducing a surrogate initialization distribution to address
the challenge posed by the absorbing stationary distribution, which is a
singleton and causes the KL divergence to be ill-defined. We then establish the
first convergence guarantees for both the $\tau$-leaping and uniformization
samplers under absorbing rate matrices, demonstrating improved rates over their
counterparts using uniform rate matrices. Furthermore, under suitable
assumptions, we provide convergence guarantees without early stopping. Our
analysis introduces several new technical tools to address challenges unique to
absorbing rate matrices. These include a Jensen-type argument for bounding
forward process convergence, novel techniques for bounding absorbing score
functions, and a non-divergent upper bound on the score near initialization
that removes the need of early-stopping.

</details>


### [420] [Multi-agent Markov Entanglement](https://arxiv.org/abs/2506.02385)
*Shuze Chen,Tianyi Peng*

Main category: cs.LG

Relevance: 70.0

TL;DR: 本文探讨了多智能体强化学习中价值分解的理论基础，揭示了其有效性依赖于转移矩阵的“非纠缠”性质，并提出了“马尔可夫纠缠”的概念及其测量方法。


<details>
  <summary>Details</summary>
Motivation: 多智能体强化学习中，价值分解是一种常用技术，但其理论依据尚未充分探索。本文旨在填补这一空白，揭示其数学结构。

Method: 通过类比量子纠缠，定义了“马尔可夫纠缠”的概念，并证明其在多智能体MDP中的适用性。同时，提出了一种测量方法并分析了分解误差的界限。

Result: 证明了广泛使用的索引策略是弱纠缠的，其分解误差为亚线性规模（$\mathcal O(\sqrt{N})$），并提供了实践中高效估计马尔可夫纠缠的方法。

Conclusion: 马尔可夫纠缠为价值分解的质量提供了理论依据和实用工具，有助于多智能体强化学习的实践应用。

Abstract: Value decomposition has long been a fundamental technique in multi-agent
dynamic programming and reinforcement learning (RL). Specifically, the value
function of a global state $(s_1,s_2,\ldots,s_N)$ is often approximated as the
sum of local functions: $V(s_1,s_2,\ldots,s_N)\approx\sum_{i=1}^N V_i(s_i)$.
This approach traces back to the index policy in restless multi-armed bandit
problems and has found various applications in modern RL systems. However, the
theoretical justification for why this decomposition works so effectively
remains underexplored.
  In this paper, we uncover the underlying mathematical structure that enables
value decomposition. We demonstrate that a multi-agent Markov decision process
(MDP) permits value decomposition if and only if its transition matrix is not
"entangled" -- a concept analogous to quantum entanglement in quantum physics.
Drawing inspiration from how physicists measure quantum entanglement, we
introduce how to measure the "Markov entanglement" for multi-agent MDPs and
show that this measure can be used to bound the decomposition error in general
multi-agent MDPs.
  Using the concept of Markov entanglement, we proved that a widely-used class
of index policies is weakly entangled and enjoys a sublinear $\mathcal
O(\sqrt{N})$ scale of decomposition error for $N$-agent systems. Finally, we
show how Markov entanglement can be efficiently estimated in practice,
providing practitioners with an empirical proxy for the quality of value
decomposition.

</details>


### [421] [Accelerating Model-Based Reinforcement Learning using Non-Linear Trajectory Optimization](https://arxiv.org/abs/2506.02767)
*Marco Calì,Giulio Giacomuzzo,Ruggero Carli,Alberto Dalla Libera*

Main category: cs.LG

Relevance: 70.0

TL;DR: EB-MC-PILCO结合iLQR加速MC-PILCO的策略优化收敛，实验显示执行时间减少45.9%，并保持100%成功率。


<details>
  <summary>Details</summary>
Motivation: 解决MC-PILCO在模型强化学习（MBRL）中收敛速度慢的问题。

Method: 集成iLQR生成探索性轨迹并初始化策略，减少优化步骤。

Result: 在cart-pole任务中，EB-MC-PILCO比标准MC-PILCO收敛更快，执行时间减少45.9%，成功率100%。

Conclusion: EB-MC-PILCO显著提升MC-PILCO的效率，适用于非线性系统。

Abstract: This paper addresses the slow policy optimization convergence of Monte Carlo
Probabilistic Inference for Learning Control (MC-PILCO), a state-of-the-art
model-based reinforcement learning (MBRL) algorithm, by integrating it with
iterative Linear Quadratic Regulator (iLQR), a fast trajectory optimization
method suitable for nonlinear systems. The proposed method, Exploration-Boosted
MC-PILCO (EB-MC-PILCO), leverages iLQR to generate informative, exploratory
trajectories and initialize the policy, significantly reducing the number of
required optimization steps. Experiments on the cart-pole task demonstrate that
EB-MC-PILCO accelerates convergence compared to standard MC-PILCO, achieving up
to $\bm{45.9\%}$ reduction in execution time when both methods solve the task
in four trials. EB-MC-PILCO also maintains a $\bm{100\%}$ success rate across
trials while solving the task faster, even in cases where MC-PILCO converges in
fewer iterations.

</details>


### [422] [On the Robustness of Tabular Foundation Models: Test-Time Attacks and In-Context Defenses](https://arxiv.org/abs/2506.02978)
*Mohamed Djilani,Thibault Simonetto,Karim Tit,Florian Tambon,Paul Récamier,Salah Ghamizi,Maxime Cordy,Mike Papadakis*

Main category: cs.LG

Relevance: 70.0

TL;DR: 该论文研究了表格基础模型（FM）在对抗性攻击下的脆弱性，并提出了一种上下文对抗训练策略以提高其鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 表格FM（如TabPFN和TabICL）在无需梯度更新或微调的情况下表现出色，但其对抗性鲁棒性尚未充分研究。

Method: 论文通过测试时攻击和对抗性工具化研究了表格FM的脆弱性，并提出了基于权重优化或上下文优化的鲁棒化方法，包括一种上下文对抗训练策略。

Result: 研究表明，表格FM对小型结构化扰动敏感，且可被用于生成对其他模型的转移性攻击。提出的对抗训练策略显著提高了鲁棒性。

Conclusion: 表格FM既是攻击目标也是攻击源，强调了在这一新兴范式中鲁棒训练和评估的紧迫性。

Abstract: Recent tabular Foundational Models (FM) such as TabPFN and TabICL, leverage
in-context learning to achieve strong performance without gradient updates or
fine-tuning. However, their robustness to adversarial manipulation remains
largely unexplored. In this work, we present a comprehensive study of the
adversarial vulnerabilities of tabular FM, focusing on both their fragility to
targeted test-time attacks and their potential misuse as adversarial tools. We
show on three benchmarks in finance, cybersecurity and healthcare, that small,
structured perturbations to test inputs can significantly degrade prediction
accuracy, even when training context remain fixed. Additionally, we demonstrate
that tabular FM can be repurposed to generate transferable evasion to
conventional models such as random forests and XGBoost, and on a lesser extent
to deep tabular models. To improve tabular FM, we formulate the robustification
problem as an optimization of the weights (adversarial fine-tuning), or the
context (adversarial in-context learning). We introduce an in-context
adversarial training strategy that incrementally replaces the context with
adversarial perturbed instances, without updating model weights. Our approach
improves robustness across multiple tabular benchmarks. Together, these
findings position tabular FM as both a target and a source of adversarial
threats, highlighting the urgent need for robust training and evaluation
practices in this emerging paradigm.

</details>


### [423] [Non-Asymptotic Length Generalization](https://arxiv.org/abs/2506.03085)
*Thomas Chen,Tengyu Ma,Zhiyuan Li*

Main category: cs.LG

Relevance: 70.0

TL;DR: 论文研究了长度泛化问题，提出了非渐近长度泛化框架，并证明了不同函数类（如确定性有限自动机和C-RASP）的长度复杂性上界。


<details>
  <summary>Details</summary>
Motivation: 研究长度泛化的理论保证，为学习算法在更长输入上的泛化能力提供理论基础。

Method: 提出了非渐近长度泛化框架，分析了不同函数类的长度复杂性，并证明了C-RASP函数类的上界。

Result: 确定性有限自动机的长度复杂性为2n-2；1层C-RASP为O(T^2)，2层为O(T^O(K))。

Conclusion: 长度泛化的可计算性与语言等价问题的可判定性相关，C-RASP函数类具有多项式上界。

Abstract: Length generalization is the ability of a learning algorithm to learn a
hypothesis which generalizes to longer inputs than the inputs in the training
set. In this paper, we provide provable guarantees of length generalization for
various classes of functions in an idealized setting. First, we formalize the
framework of non-asymptotic length generalization, which requires a computable
upper bound for the minimum input length that guarantees length generalization,
as a function of the complexity of ground-truth function under some given
complexity measure. We refer to this minimum input length to length generalize
as length complexity. We show the Minimum-Complexity Interpolator learning
algorithm achieves optimal length complexity. We further show that whether a
function class admits non-asymptotic length generalization is equivalent to the
decidability of its language equivalence problem, which implies that there is
no computable upper bound for the length complexity of Context-Free Grammars.
On the positive side, we show that the length complexity of Deterministic
Finite Automata is $2n - 2$ where $n$ is the number of states of the
ground-truth automaton. Our main results are upper bounds of length complexity
for a subset of a transformer-related function class called C-RASP (Yang &
Chiang, 2024). We show that the length complexity of 1-layer C-RASP functions
is $O(T^2)$ when the ground-truth function has precision $T$, and that the
length complexity of 2-layer C-RASP functions is $O(T^{O(K)})$ when the
ground-truth function has precision $T$ and $K$ heads.

</details>


### [424] [Towards Human-like Preference Profiling in Sequential Recommendation](https://arxiv.org/abs/2506.02261)
*Zhongyu Ouyang,Qianlong Wen,Chunhui Zhang,Yanfang Ye,Soroush Vosoughi*

Main category: cs.IR

Relevance: 70.0

TL;DR: RecPO是一个偏好优化框架，通过模拟人类决策的灵活性和上下文感知能力，提升序列推荐系统的性能。


<details>
  <summary>Details</summary>
Motivation: 现有基于LLM的推荐系统缺乏对人类灵活决策策略的模拟，RecPO旨在填补这一空白。

Method: RecPO利用自适应奖励边界和时序信号，建模结构化反馈和上下文延迟，以模拟人类偏好优先级。

Result: 在五个真实数据集上的实验表明，RecPO性能优于现有基线，并成功模拟了人类决策的关键特征。

Conclusion: RecPO不仅提升了推荐性能，还更好地模拟了人类决策行为。

Abstract: Sequential recommendation systems aspire to profile users by interpreting
their interaction histories, echoing how humans make decisions by weighing
experience, relative preference strength, and situational relevance. Yet,
existing large language model (LLM)-based recommenders often fall short of
mimicking the flexible, context-aware decision strategies humans exhibit,
neglecting the structured, dynamic, and context-aware mechanisms fundamental to
human behaviors. To bridge this gap, we propose RecPO, a preference
optimization framework that models structured feedback and contextual delay to
emulate human-like prioritization in sequential recommendation RecPO exploits
adaptive reward margins based on inferred preference hierarchies and temporal
signals, enabling the model to favor immediately relevant items and to
distinguish between varying degrees of preference and aversion. Extensive
experiments across five real-world datasets demonstrate that RecPO not only
yields performance gains over state-of-the-art baselines, but also mirrors key
characteristics of human decision-making: favoring timely satisfaction,
maintaining coherent preferences, and exercising discernment under shifting
contexts.

</details>


### [425] [Asymptotics of SGD in Sequence-Single Index Models and Single-Layer Attention Networks](https://arxiv.org/abs/2506.02651)
*Luca Arnaboldi,Bruno Loureiro,Ludovic Stephan,Florent Krzakala,Lenka Zdeborova*

Main category: stat.ML

Relevance: 70.0

TL;DR: 论文研究了随机梯度下降（SGD）在序列单指数（SSI）模型中的动态行为，揭示了训练过程中的两个阶段及其影响因素。


<details>
  <summary>Details</summary>
Motivation: 探索序列数据结构和注意力模型在学习中的相互作用，为理解其学习机制提供理论基础。

Method: 通过分析SSI模型的损失函数和SGD动态，推导出闭式解，并研究序列长度和位置编码的影响。

Result: 发现训练分为两个阶段：从无信息初始化逃逸和目标子空间对齐，序列长度和位置编码影响收敛速度。

Conclusion: 研究为理解注意力模型在序列数据中的学习行为提供了严格且可解释的基础。

Abstract: We study the dynamics of stochastic gradient descent (SGD) for a class of
sequence models termed Sequence Single-Index (SSI) models, where the target
depends on a single direction in input space applied to a sequence of tokens.
This setting generalizes classical single-index models to the sequential
domain, encompassing simplified one-layer attention architectures. We derive a
closed-form expression for the population loss in terms of a pair of sufficient
statistics capturing semantic and positional alignment, and characterize the
induced high-dimensional SGD dynamics for these coordinates. Our analysis
reveals two distinct training phases: escape from uninformative initialization
and alignment with the target subspace, and demonstrates how the sequence
length and positional encoding influence convergence speed and learning
trajectories. These results provide a rigorous and interpretable foundation for
understanding how sequential structure in data can be beneficial for learning
with attention-based models.

</details>


### [426] [TaskVAE: Task-Specific Variational Autoencoders for Exemplar Generation in Continual Learning for Human Activity Recognition](https://arxiv.org/abs/2506.01965)
*Bonpagna Kann,Sandra Castellanos-Paez,Romain Rombourg,Philippe Lalanda*

Main category: cs.LG

Relevance: 60.0

TL;DR: TaskVAE是一个基于重放的持续学习框架，通过任务特定的变分自编码器生成合成样本，解决了动态数据环境中的遗忘问题，并在人类活动识别任务中表现出色。


<details>
  <summary>Details</summary>
Motivation: 随着机器学习系统在动态数据环境中的广泛应用，持续学习成为必要。TaskVAE旨在解决传统方法在内存限制和任务适应性方面的不足。

Method: TaskVAE使用任务特定的变分自编码器（VAEs）生成合成样本，结合新任务数据训练分类器，无需预先知道总类别数。

Result: 在5个人类活动识别数据集上，TaskVAE优于传统重放方法，尤其在数据有限时表现突出，且内存占用极小（每任务仅60样本）。

Conclusion: TaskVAE在内存效率、任务适应性和长期稳定性方面表现优异，适用于实际应用场景。

Abstract: As machine learning based systems become more integrated into daily life,
they unlock new opportunities but face the challenge of adapting to dynamic
data environments. Various forms of data shift-gradual, abrupt, or
cyclic-threaten model accuracy, making continual adaptation essential.
Continual Learning (CL) enables models to learn from evolving data streams
while minimizing forgetting of prior knowledge. Among CL strategies,
replay-based methods have proven effective, but their success relies on
balancing memory constraints and retaining old class accuracy while learning
new classes. This paper presents TaskVAE, a framework for replay-based CL in
class-incremental settings. TaskVAE employs task-specific Variational
Autoencoders (VAEs) to generate synthetic exemplars from previous tasks, which
are then used to train the classifier alongside new task data. In contrast to
traditional methods that require prior knowledge of the total class count or
rely on a single VAE for all tasks, TaskVAE adapts flexibly to increasing tasks
without such constraints. We focus on Human Activity Recognition (HAR) using
IMU sensor-equipped devices. Unlike previous HAR studies that combine data
across all users, our approach focuses on individual user data, better
reflecting real-world scenarios where a person progressively learns new
activities. Extensive experiments on 5 different HAR datasets show that TaskVAE
outperforms experience replay methods, particularly with limited data, and
exhibits robust performance as dataset size increases. Additionally, memory
footprint of TaskVAE is minimal, being equivalent to only 60 samples per task,
while still being able to generate an unlimited number of synthetic samples.
The contributions lie in balancing memory constraints, task-specific
generation, and long-term stability, making it a reliable solution for
real-world applications in domains like HAR.

</details>


### [427] [An empirical study of task and feature correlations in the reuse of pre-trained models](https://arxiv.org/abs/2506.01975)
*Jama Hussein Mohamud*

Main category: cs.LG

Relevance: 60.0

TL;DR: 研究探讨了预训练神经网络重用的成功因素，发现任务相关性是关键，即使任务不相关，网络和优化器的选择也能带来显著提升。


<details>
  <summary>Details</summary>
Motivation: 探究预训练神经网络重用的成功原因，特别是在任务相关性较低时仍能表现良好的现象。

Method: 设计实验研究任务相关性、网络结构和优化器选择对重用效果的影响。

Result: 任务相关性高时重用效果显著；即使任务不相关，网络和优化器选择也能带来提升；低相关性时重用底层更优。

Conclusion: 任务语义相关性是重用成功的关键，网络和优化器选择也有重要影响。

Abstract: Pre-trained neural networks are commonly used and reused in the machine
learning community. Alice trains a model for a particular task, and a part of
her neural network is reused by Bob for a different task, often to great
effect. To what can we ascribe Bob's success? This paper introduces an
experimental setup through which factors contributing to Bob's empirical
success could be studied in silico. As a result, we demonstrate that Bob might
just be lucky: his task accuracy increases monotonically with the correlation
between his task and Alice's. Even when Bob has provably uncorrelated tasks and
input features from Alice's pre-trained network, he can achieve significantly
better than random performance due to Alice's choice of network and optimizer.
When there is little correlation between tasks, only reusing lower pre-trained
layers is preferable, and we hypothesize the converse: that the optimal number
of retrained layers is indicative of task and feature correlation. Finally, we
show in controlled real-world scenarios that Bob can effectively reuse Alice's
pre-trained network if there are semantic correlations between his and Alice's
task.

</details>


### [428] [Decoupled Hierarchical Reinforcement Learning with State Abstraction for Discrete Grids](https://arxiv.org/abs/2506.02050)
*Qingyu Xiao,Yuanlin Chang,Youtian Du*

Main category: cs.LG

Relevance: 60.0

TL;DR: 论文提出了一种解耦分层强化学习框架（DcHRL-SA），结合状态抽象方法，用于解决部分可观测复杂离散状态空间环境中的探索问题。


<details>
  <summary>Details</summary>
Motivation: 解决复杂离散状态空间环境中有效探索的挑战，特别是在部分可观测条件下。

Method: 采用双层次架构（高层RL策略和低层规则策略）结合状态抽象方法，降低状态维度。

Result: 在定制网格环境中，DcHRL-SA在探索效率、收敛速度、累积奖励和策略稳定性上优于PPO。

Conclusion: DcHRL-SA为大规模探索空间的离散网格问题提供了一种实用解决方案。

Abstract: Effective agent exploration remains a core challenge in reinforcement
learning (RL) for complex discrete state-space environments, particularly under
partial observability. This paper presents a decoupled hierarchical RL
framework integrating state abstraction (DcHRL-SA) to address this issue. The
proposed method employs a dual-level architecture, consisting of a high level
RL-based actor and a low-level rule-based policy, to promote effective
exploration. Additionally, state abstraction method is incorporated to cluster
discrete states, effectively lowering state dimensionality. Experiments
conducted in two discrete customized grid environments demonstrate that the
proposed approach consistently outperforms PPO in terms of exploration
efficiency, convergence speed, cumulative reward, and policy stability. These
results demonstrate a practical approach for integrating decoupled hierarchical
policies and state abstraction in discrete grids with large-scale exploration
space. Code will be available at https://github.com/XQY169/DcHRL-SA.

</details>


### [429] [EWGN: Elastic Weight Generation and Context Switching in Deep Learning](https://arxiv.org/abs/2506.02065)
*Shriraj P. Sawant,Krishna P. Miyapuram*

Main category: cs.LG

Relevance: 60.0

TL;DR: 论文提出了一种弹性权重生成网络（EWGN），通过动态生成权重实现任务间上下文切换，以缓解神经网络中的灾难性遗忘问题。


<details>
  <summary>Details</summary>
Motivation: 解决神经网络在任务切换时的灾难性遗忘问题，推动持续学习的发展。

Method: 提出EWGN架构，通过额外网络动态生成主网络权重，支持输入依赖的上下文切换。

Result: 在MNIST和fashion-MNIST数据集上验证了EWGN在保留已学习任务表现上的有效性。

Conclusion: 动态权重生成和上下文切换能力有助于提升持续学习性能。

Abstract: The ability to learn and retain a wide variety of tasks is a hallmark of
human intelligence that has inspired research in artificial general
intelligence. Continual learning approaches provide a significant step towards
achieving this goal. It has been known that task variability and context
switching are challenging for learning in neural networks. Catastrophic
forgetting refers to the poor performance on retention of a previously learned
task when a new task is being learned. Switching between different task
contexts can be a useful approach to mitigate the same by preventing the
interference between the varying task weights of the network. This paper
introduces Elastic Weight Generative Networks (EWGN) as an idea for context
switching between two different tasks. The proposed EWGN architecture uses an
additional network that generates the weights of the primary network
dynamically while consolidating the weights learned. The weight generation is
input-dependent and thus enables context switching. Using standard computer
vision datasets, namely MNIST and fashion-MNIST, we analyse the retention of
previously learned task representations in Fully Connected Networks,
Convolutional Neural Networks, and EWGN architectures with Stochastic Gradient
Descent and Elastic Weight Consolidation learning algorithms. Understanding
dynamic weight generation and context-switching ability can be useful in
enabling continual learning for improved performance.

</details>


### [430] [SALAD: Systematic Assessment of Machine Unlearing on LLM-Aided Hardware Design](https://arxiv.org/abs/2506.02089)
*Zeng Wang,Minghao Shao,Rupesh Karn,Jitendra Bhandari,Likhitha Mankali,Ramesh Karri,Ozgur Sinanoglu,Muhammad Shafique,Johann Knechtel*

Main category: cs.LG

Relevance: 60.0

TL;DR: SALAD利用机器遗忘技术解决LLM在硬件设计自动化中的数据安全问题，包括污染数据、IP泄露和恶意代码生成。


<details>
  <summary>Details</summary>
Motivation: LLM在硬件设计自动化中具有潜力，但也带来数据安全风险，如污染数据、IP泄露和恶意代码生成。

Method: 采用机器遗忘技术，选择性移除预训练LLM中的污染数据、敏感IP或恶意代码，无需完全重新训练。

Result: 通过案例研究证明，机器遗忘能有效降低LLM在硬件设计中的数据安全风险。

Conclusion: SALAD为LLM在硬件设计中的安全应用提供了可行方案。

Abstract: Large Language Models (LLMs) offer transformative capabilities for hardware
design automation, particularly in Verilog code generation. However, they also
pose significant data security challenges, including Verilog evaluation data
contamination, intellectual property (IP) design leakage, and the risk of
malicious Verilog generation. We introduce SALAD, a comprehensive assessment
that leverages machine unlearning to mitigate these threats. Our approach
enables the selective removal of contaminated benchmarks, sensitive IP and
design artifacts, or malicious code patterns from pre-trained LLMs, all without
requiring full retraining. Through detailed case studies, we demonstrate how
machine unlearning techniques effectively reduce data security risks in
LLM-aided hardware design.

</details>


### [431] [Towards Better Generalization and Interpretability in Unsupervised Concept-Based Models](https://arxiv.org/abs/2506.02092)
*Francesco De Santis,Philippe Bich,Gabriele Ciravegna,Pietro Barbiero,Danilo Giordano,Tania Cerquitelli*

Main category: cs.LG

Relevance: 60.0

TL;DR: 该论文提出了一种名为LCBM的无监督概念模型，用于图像分类，通过伯努利潜在空间建模概念，提升了模型的可解释性和泛化能力，同时减少了所需概念数量。


<details>
  <summary>Details</summary>
Motivation: 提高深度神经网络的可信度，增强对其决策过程的理解。

Method: 提出LCBM模型，将概念建模为伯努利潜在空间中的随机变量，减少所需概念数量，同时保持性能。

Result: LCBM在泛化能力上超越现有无监督概念模型，接近黑盒模型性能，概念表示更符合人类理解。

Conclusion: LCBM在保持模型可解释性的同时，提升了性能和人类可理解性。

Abstract: To increase the trustworthiness of deep neural networks, it is critical to
improve the understanding of how they make decisions. This paper introduces a
novel unsupervised concept-based model for image classification, named
Learnable Concept-Based Model (LCBM) which models concepts as random variables
within a Bernoulli latent space. Unlike traditional methods that either require
extensive human supervision or suffer from limited scalability, our approach
employs a reduced number of concepts without sacrificing performance. We
demonstrate that LCBM surpasses existing unsupervised concept-based models in
generalization capability and nearly matches the performance of black-box
models. The proposed concept representation enhances information retention and
aligns more closely with human understanding. A user study demonstrates the
discovered concepts are also more intuitive for humans to interpret. Finally,
despite the use of concept embeddings, we maintain model interpretability by
means of a local linear combination of concepts.

</details>


### [432] [ReconXF: Graph Reconstruction Attack via Public Feature Explanations on Privatized Node Features and Labels](https://arxiv.org/abs/2506.02134)
*Rishi Raj Sahoo,Rucha Bhalchandra Joshi,Subhankar Mishra*

Main category: cs.LG

Relevance: 60.0

TL;DR: 论文提出了一种名为ReconXF的图重构攻击方法，针对公开的特征解释和私有化辅助数据场景，通过去噪机制处理差分隐私噪声，显著提升了在隐私保护设置下的图结构恢复性能。


<details>
  <summary>Details</summary>
Motivation: GNNs在关键领域（如医疗和司法）的应用受限，因其黑盒特性。虽然可解释性方法提供了特征级解释，但这些解释可能带来隐私风险，尤其是在与辅助信息结合时。现有攻击方法在差分隐私保护下效果不佳，因此需要新的攻击方法。

Method: 提出ReconXF攻击方法，结合公开的特征解释和私有化辅助数据，通过去噪机制处理差分隐私噪声，并利用解释中的结构信号重构图结构。

Result: 在多个数据集上的实验表明，ReconXF在AUC和平均精度上优于现有方法，表明公开解释与去噪机制的结合能够在隐私保护下恢复图结构。

Conclusion: 即使在差分隐私保护下，公开的解释仍可能导致隐私泄露，需进一步研究保护机制。

Abstract: Graph Neural Networks (GNNs) achieve high performance across many
applications but function as black-box models, limiting their use in critical
domains like healthcare and criminal justice. Explainability methods address
this by providing feature-level explanations that identify important node
attributes for predictions. These explanations create privacy risks. Combined
with auxiliary information, feature explanations can enable adversaries to
reconstruct graph structure, exposing sensitive relationships. Existing graph
reconstruction attacks assume access to original auxiliary data, but practical
systems use differential privacy to protect node features and labels while
providing explanations for transparency. We study a threat model where
adversaries access public feature explanations along with privatized node
features and labels. We show that existing explanation-based attacks like GSEF
perform poorly with privatized data due to noise from differential privacy
mechanisms. We propose ReconXF, a graph reconstruction attack for scenarios
with public explanations and privatized auxiliary data. Our method adapts
explanation-based frameworks by incorporating denoising mechanisms that handle
differential privacy noise while exploiting structural signals in explanations.
Experiments across multiple datasets show ReconXF outperforms SoTA methods in
privatized settings, with improvements in AUC and average precision. Results
indicate that public explanations combined with denoising enable graph
structure recovery even under the privacy protection of auxiliary data. Code is
available at (link to be made public after acceptance).

</details>


### [433] [Z-Error Loss for Training Neural Networks](https://arxiv.org/abs/2506.02154)
*Guillaume Godin*

Main category: cs.LG

Relevance: 60.0

TL;DR: 提出了一种名为Z-Error Loss的统计方法，通过屏蔽批次中异常数据点的影响来减少训练中的离群值干扰。


<details>
  <summary>Details</summary>
Motivation: 离群值在神经网络训练中会传播错误的梯度，影响模型性能和泛化能力。

Method: 利用批次级统计自动检测并排除异常样本，最小化离群值对训练的影响。

Result: 该方法鲁棒性强，能适应数据质量，并为数据清理提供诊断工具。

Conclusion: Z-Error Loss是一种有效的离群值处理方法，有助于模型专注于真实数据结构。

Abstract: Outliers introduce significant training challenges in neural networks by
propagating erroneous gradients, which can degrade model performance and
generalization. We propose the Z-Error Loss, a statistically principled
approach that minimizes outlier influence during training by masking the
contribution of data points identified as out-of-distribution within each
batch. This method leverages batch-level statistics to automatically detect and
exclude anomalous samples, allowing the model to focus its learning on the true
underlying data structure. Our approach is robust, adaptive to data quality,
and provides valuable diagnostics for data curation and cleaning.

</details>


### [434] [An Approximation Theory Perspective on Machine Learning](https://arxiv.org/abs/2506.02168)
*Hrushikesh N. Mhaskar,Efstratios Tsoukanis,Ameya D. Jagtap*

Main category: cs.LG

Relevance: 60.0

TL;DR: 该论文探讨了机器学习中的函数逼近问题，分析了当前理论与实践的差距，并提出了在未知流形上实现函数逼近的新方法。


<details>
  <summary>Details</summary>
Motivation: 研究动机是解决机器学习中函数逼近理论与实际应用之间的脱节问题，特别是模型在未见数据上的泛化能力不足。

Method: 方法包括回顾现有文献中的关键思想，分析浅层/深层网络、流形逼近、物理信息神经网络等趋势，并提出一种无需学习流形特征的新方法。

Result: 论文提出了一种新颖的方法，能够在未知流形上实现函数逼近，避免了传统方法中复杂的流形特征学习。

Conclusion: 结论强调了函数逼近理论在机器学习中的重要性，并展示了新方法在解决泛化问题上的潜力。

Abstract: A central problem in machine learning is often formulated as follows: Given a
dataset $\{(x_j, y_j)\}_{j=1}^M$, which is a sample drawn from an unknown
probability distribution, the goal is to construct a functional model $f$ such
that $f(x) \approx y$ for any $(x, y)$ drawn from the same distribution. Neural
networks and kernel-based methods are commonly employed for this task due to
their capacity for fast and parallel computation. The approximation
capabilities, or expressive power, of these methods have been extensively
studied over the past 35 years. In this paper, we will present examples of key
ideas in this area found in the literature. We will discuss emerging trends in
machine learning including the role of shallow/deep networks, approximation on
manifolds, physics-informed neural surrogates, neural operators, and
transformer architectures. Despite function approximation being a fundamental
problem in machine learning, approximation theory does not play a central role
in the theoretical foundations of the field. One unfortunate consequence of
this disconnect is that it is often unclear how well trained models will
generalize to unseen or unlabeled data. In this review, we examine some of the
shortcomings of the current machine learning framework and explore the reasons
for the gap between approximation theory and machine learning practice. We will
then introduce our novel research to achieve function approximation on unknown
manifolds without the need to learn specific manifold features, such as the
eigen-decomposition of the Laplace-Beltrami operator or atlas construction. In
many machine learning problems, particularly classification tasks, the labels
$y_j$ are drawn from a finite set of values.

</details>


### [435] [Latent Stochastic Interpolants](https://arxiv.org/abs/2506.02276)
*Saurabh Singh,Dmitry Lagun*

Main category: cs.LG

Relevance: 60.0

TL;DR: 提出了Latent Stochastic Interpolants (LSI)框架，用于在潜在空间中联合优化编码器、解码器和潜在SI模型，克服了传统SI框架需要直接访问样本的限制。


<details>
  <summary>Details</summary>
Motivation: 传统SI框架需要直接访问两个分布的样本，限制了其在联合优化潜在变量模型中的应用。LSI旨在解决这一问题，同时保留SI的生成灵活性。

Method: 通过开发连续时间下的ELBO目标，实现编码器、解码器和潜在SI模型的联合优化。LSI避免了高维观测空间的计算需求，同时学习有效的潜在表示。

Result: 在ImageNet生成基准测试中验证了LSI的有效性。

Conclusion: LSI在保留SI生成灵活性的同时，解决了计算和样本访问的限制，适用于高维数据生成任务。

Abstract: Stochastic Interpolants (SI) are a powerful framework for generative
modeling, capable of flexibly transforming between two probability
distributions. However, their use in jointly optimized latent variable models
remains unexplored as they require direct access to the samples from the two
distributions. This work presents Latent Stochastic Interpolants (LSI) enabling
joint learning in a latent space with end-to-end optimized encoder, decoder and
latent SI models. We achieve this by developing a principled Evidence Lower
Bound (ELBO) objective derived directly in continuous time. The joint
optimization allows LSI to learn effective latent representations along with a
generative process that transforms an arbitrary prior distribution into the
encoder-defined aggregated posterior. LSI sidesteps the simple priors of the
normal diffusion models and mitigates the computational demands of applying SI
directly in high-dimensional observation spaces, while preserving the
generative flexibility of the SI framework. We demonstrate the efficacy of LSI
through comprehensive experiments on the standard large scale ImageNet
generation benchmark.

</details>


### [436] [Improving Generalization of Neural Combinatorial Optimization for Vehicle Routing Problems via Test-Time Projection Learning](https://arxiv.org/abs/2506.02392)
*Yuanyao Chen,Rongsheng Chen,Fu Luo,Zhenkun Wang*

Main category: cs.LG

Relevance: 60.0

TL;DR: 论文提出了一种基于LLM的学习框架，用于解决NCO在小规模训练数据上表现良好但在大规模场景下性能下降的问题。该方法通过投影训练和测试分布，提升模型的可扩展性，且无需重新训练模型。


<details>
  <summary>Details</summary>
Motivation: 现有NCO方法在小规模实例上表现良好，但在大规模场景下性能显著下降，主要原因是训练和测试数据的分布偏移。

Method: 引入基于LLM的学习框架，学习训练和测试分布之间的投影，仅需在推理阶段部署，无需重新训练模型。

Result: 实验表明，该方法使基于100节点训练的模型在100K节点的TSP和CVRP问题上表现优异。

Conclusion: 提出的LLM驱动框架有效解决了NCO在大规模问题中的可扩展性问题，且无需额外训练。

Abstract: Neural Combinatorial Optimization (NCO) has emerged as a promising
learning-based paradigm for addressing Vehicle Routing Problems (VRPs) by
minimizing the need for extensive manual engineering. While existing NCO
methods, trained on small-scale instances (e.g., 100 nodes), have demonstrated
considerable success on problems of similar scale, their performance
significantly degrades when applied to large-scale scenarios. This degradation
arises from the distributional shift between training and testing data,
rendering policies learned on small instances ineffective for larger problems.
To overcome this limitation, we introduce a novel learning framework driven by
Large Language Models (LLMs). This framework learns a projection between the
training and testing distributions, which is then deployed to enhance the
scalability of the NCO model. Notably, unlike prevailing techniques that
necessitate joint training with the neural network, our approach operates
exclusively during the inference phase, obviating the need for model
retraining. Extensive experiments demonstrate that our method enables a
backbone model (trained on 100-node instances) to achieve superior performance
on large-scale Traveling Salesman Problem (TSP) and Capacitated Vehicle Routing
Problem (CVRP) of up to 100K nodes from diverse distributions.

</details>


### [437] [Rethinking Post-Unlearning Behavior of Large Vision-Language Models](https://arxiv.org/abs/2506.02541)
*Minsung Kim,Nakyeong Yang,Kyomin Jung*

Main category: cs.LG

Relevance: 60.0

TL;DR: 论文提出了一种针对大型视觉语言模型（LVLMs）的机器遗忘方法PUBG，旨在解决现有遗忘方法导致的副作用（如退化、幻觉或过度拒绝响应），并确保隐私保护的同时生成信息丰富且视觉基础的回答。


<details>
  <summary>Details</summary>
Motivation: 现有的大型视觉语言模型（LVLMs）在训练时可能涉及隐私风险，而传统的遗忘方法往往忽视遗忘后的输出质量，导致模型行为异常。

Method: 提出了一种新的遗忘任务，要求模型在隐私保护的同时生成高质量回答，并设计了PUBG方法，通过引导遗忘后的输出分布来避免副作用。

Result: 实验表明，PUBG能有效避免传统方法的副作用，生成视觉基础且信息丰富的回答，同时防止隐私泄露。

Conclusion: PUBG为LVLMs的隐私保护提供了一种更可靠的解决方案，兼顾了遗忘效果和输出质量。

Abstract: Machine unlearning is used to mitigate the privacy risks of Large
Vision-Language Models (LVLMs) arising from training on large-scale web data.
However, existing unlearning methods often fail to carefully select substitute
outputs for forget targets, resulting in Unlearning Aftermaths-undesirable
behaviors such as degenerate, hallucinated, or excessively refused responses.
We highlight that, especially for generative LVLMs, it is crucial to consider
the quality and informativeness of post-unlearning responses rather than
relying solely on naive suppression. To address this, we introduce a new
unlearning task for LVLMs that requires models to provide privacy-preserving
yet informative and visually grounded responses. We also propose PUBG, a novel
unlearning method that explicitly guides post-unlearning behavior toward a
desirable output distribution. Experiments show that, while existing methods
suffer from Unlearning Aftermaths despite successfully preventing privacy
violations, PUBG effectively mitigates these issues, generating visually
grounded and informative responses without privacy leakage for forgotten
targets.

</details>


### [438] [Reachability Weighted Offline Goal-conditioned Resampling](https://arxiv.org/abs/2506.02577)
*Wenyan Yang,Joni Pajarinen*

Main category: cs.LG

Relevance: 60.0

TL;DR: 论文提出了一种名为Reachability Weighted Sampling (RWS)的方法，用于改进离线目标条件强化学习中的采样策略，通过优先选择可达性高的状态-目标-动作对，显著提升了性能。


<details>
  <summary>Details</summary>
Motivation: 离线目标条件强化学习中，均匀采样会导致大量不可达的状态-目标-动作对，降低策略性能。论文旨在通过优化采样策略来解决这一问题。

Method: 提出RWS方法，利用可达性分类器（通过正样本-未标记学习训练）为状态-动作对分配可达性分数，并以此作为采样优先级。

Result: 在六个复杂的机器人操作任务中，RWS显著提升了性能，其中HandBlock-Z任务的性能提升了近50%。

Conclusion: RWS是一种即插即用的模块，能有效提升离线目标条件强化学习的性能。

Abstract: Offline goal-conditioned reinforcement learning (RL) relies on fixed datasets
where many potential goals share the same state and action spaces. However,
these potential goals are not explicitly represented in the collected
trajectories. To learn a generalizable goal-conditioned policy, it is common to
sample goals and state-action pairs uniformly using dynamic programming methods
such as Q-learning. Uniform sampling, however, requires an intractably large
dataset to cover all possible combinations and creates many unreachable
state-goal-action pairs that degrade policy performance. Our key insight is
that sampling should favor transitions that enable goal achievement. To this
end, we propose Reachability Weighted Sampling (RWS). RWS uses a reachability
classifier trained via positive-unlabeled (PU) learning on goal-conditioned
state-action values. The classifier maps these values to a reachability score,
which is then used as a sampling priority. RWS is a plug-and-play module that
integrates seamlessly with standard offline RL algorithms. Experiments on six
complex simulated robotic manipulation tasks, including those with a robot arm
and a dexterous hand, show that RWS significantly improves performance. In one
notable case, performance on the HandBlock-Z task improved by nearly 50 percent
relative to the baseline. These results indicate the effectiveness of
reachability-weighted sampling.

</details>


### [439] [Simple, Good, Fast: Self-Supervised World Models Free of Baggage](https://arxiv.org/abs/2506.02612)
*Jan Robine,Marc Höftmann,Stefan Harmeling*

Main category: cs.LG

Relevance: 60.0

TL;DR: SGF是一种简单、高效且快速的世界模型，通过自监督表示学习和数据增强实现，不依赖RNN、Transformer或图像重建，并在Atari 100k基准测试中表现良好。


<details>
  <summary>Details</summary>
Motivation: 探索不依赖传统组件（如RNN、Transformer或图像重建）的世界模型的可行性和性能。

Method: 采用自监督表示学习、帧和动作堆叠捕获短期依赖，并通过数据增强提升模型鲁棒性。

Result: 在Atari 100k基准测试中表现良好，并通过消融实验验证了各组件的重要性。

Conclusion: SGF证明了无需复杂组件即可构建高效世界模型的可能性。

Abstract: What are the essential components of world models? How far do we get with
world models that are not employing RNNs, transformers, discrete
representations, and image reconstructions? This paper introduces SGF, a
Simple, Good, and Fast world model that uses self-supervised representation
learning, captures short-time dependencies through frame and action stacking,
and enhances robustness against model errors through data augmentation. We
extensively discuss SGF's connections to established world models, evaluate the
building blocks in ablation studies, and demonstrate good performance through
quantitative comparisons on the Atari 100k benchmark.

</details>


### [440] [Compositional Learning for Modular Multi-Agent Self-Organizing Networks](https://arxiv.org/abs/2506.02616)
*Qi Liao,Parijat Bhattacharjee*

Main category: cs.LG

Relevance: 60.0

TL;DR: 论文提出两种组合学习方法（CDRL和CPDM），用于多智能体系统中的训练时间和安全性约束，通过模块化框架优化性能。


<details>
  <summary>Details</summary>
Motivation: 解决自组织网络中复杂参数互依赖和冲突目标的挑战。

Method: 提出模块化双层框架（细胞级和细胞对级智能体），结合CDRL和CPDM方法。

Result: 数值模拟显示切换失败减少，吞吐量和延迟改善，优于传统多智能体深度强化学习方法。

Conclusion: 方法具有更好的可扩展性、更快收敛、更高样本效率和更安全的训练。

Abstract: Self-organizing networks face challenges from complex parameter
interdependencies and conflicting objectives. This study introduces two
compositional learning approaches-Compositional Deep Reinforcement Learning
(CDRL) and Compositional Predictive Decision-Making (CPDM)-and evaluates their
performance under training time and safety constraints in multi-agent systems.
We propose a modular, two-tier framework with cell-level and cell-pair-level
agents to manage heterogeneous agent granularities while reducing model
complexity. Numerical simulations reveal a significant reduction in handover
failures, along with improved throughput and latency, outperforming
conventional multi-agent deep reinforcement learning approaches. The approach
also demonstrates superior scalability, faster convergence, higher sample
efficiency, and safer training in large-scale self-organizing networks.

</details>


### [441] [XicorAttention: Time Series Transformer Using Attention with Nonlinear Correlation](https://arxiv.org/abs/2506.02694)
*Daichi Kimura,Tomonori Izumitani,Hisashi Kashima*

Main category: cs.LG

Relevance: 60.0

TL;DR: 该论文提出了一种基于Chatterjee秩相关系数的新型注意力机制XicorAttention，用于改进时间序列预测中的非线性依赖捕捉，实验显示其性能提升约9.1%。


<details>
  <summary>Details</summary>
Motivation: 现有的注意力机制在捕捉时间序列数据中的非线性依赖方面存在不足，需要改进。

Method: 提出XicorAttention，用Chatterjee秩相关系数替代标准注意力中的矩阵乘法，并引入可微分的SoftSort和SoftRank近似。

Result: 在真实数据集上，XicorAttention将预测准确率提升约9.1%。

Conclusion: 非线性相关性的引入显著提升了时间序列预测的性能。

Abstract: Various Transformer-based models have been proposed for time series
forecasting. These models leverage the self-attention mechanism to capture
long-term temporal or variate dependencies in sequences. Existing methods can
be divided into two approaches: (1) reducing computational cost of attention by
making the calculations sparse, and (2) reshaping the input data to aggregate
temporal features. However, existing attention mechanisms may not adequately
capture inherent nonlinear dependencies present in time series data, leaving
room for improvement. In this study, we propose a novel attention mechanism
based on Chatterjee's rank correlation coefficient, which measures nonlinear
dependencies between variables. Specifically, we replace the matrix
multiplication in standard attention mechanisms with this rank coefficient to
measure the query-key relationship. Since computing Chatterjee's correlation
coefficient involves sorting and ranking operations, we introduce a
differentiable approximation employing SoftSort and SoftRank. Our proposed
mechanism, ``XicorAttention,'' integrates it into several state-of-the-art
Transformer models. Experimental results on real-world datasets demonstrate
that incorporating nonlinear correlation into the attention improves
forecasting accuracy by up to approximately 9.1\% compared to existing models.

</details>


### [442] [MTL-KD: Multi-Task Learning Via Knowledge Distillation for Generalizable Neural Vehicle Routing Solver](https://arxiv.org/abs/2506.02935)
*Yuepeng Zheng,Fu Luo,Zhenkun Wang,Yaoxin Wu,Yu Zhou*

Main category: cs.LG

Relevance: 60.0

TL;DR: 本文提出了一种基于知识蒸馏的多任务学习方法（MTL-KD），用于训练具有强泛化能力的重型解码器模型，并通过随机重排序重构（R3C）策略进一步提升性能。


<details>
  <summary>Details</summary>
Motivation: 现有基于强化学习的多任务方法仅能训练轻量解码器模型，且在小规模问题上表现有限，难以泛化到大规模问题。

Method: 通过知识蒸馏将多个单任务模型的知识迁移到一个重型解码器模型中，并结合R3C推理策略。

Result: 在6个已知和10个未知的VRP变体上，该方法在均匀和真实基准测试中均表现出卓越性能。

Conclusion: MTL-KD方法显著提升了模型的泛化能力，适用于多样化的VRP任务。

Abstract: Multi-Task Learning (MTL) in Neural Combinatorial Optimization (NCO) is a
promising approach to train a unified model capable of solving multiple Vehicle
Routing Problem (VRP) variants. However, existing Reinforcement Learning
(RL)-based multi-task methods can only train light decoder models on
small-scale problems, exhibiting limited generalization ability when solving
large-scale problems. To overcome this limitation, this work introduces a novel
multi-task learning method driven by knowledge distillation (MTL-KD), which
enables the efficient training of heavy decoder models with strong
generalization ability. The proposed MTL-KD method transfers policy knowledge
from multiple distinct RL-based single-task models to a single heavy decoder
model, facilitating label-free training and effectively improving the model's
generalization ability across diverse tasks. In addition, we introduce a
flexible inference strategy termed Random Reordering Re-Construction (R3C),
which is specifically adapted for diverse VRP tasks and further boosts the
performance of the multi-task model. Experimental results on 6 seen and 10
unseen VRP variants with up to 1000 nodes indicate that our proposed method
consistently achieves superior performance on both uniform and real-world
benchmarks, demonstrating robust generalization abilities.

</details>


### [443] [VerificAgent: Integrating Expert Knowledge and Fact-Checked Memory for Robust Domain-Specific Task Planning](https://arxiv.org/abs/2506.02539)
*Thong Q. Nguyen,Shubhang Desai,Yash Jain,Tanvir Aumi,Vishal Chowdhary*

Main category: cs.LG

Relevance: 50.0

TL;DR: VerificAgent是一个管理计算机代理（CUAs）记忆的框架，通过专家知识、迭代记忆精炼和人工事实检查，显著提升了任务成功率。


<details>
  <summary>Details</summary>
Motivation: 解决CUAs在持续记忆增强中可能引入虚假或幻觉学习的问题，特别是在特定领域工作流程中。

Method: 结合专家知识种子、基于轨迹的迭代记忆精炼和人工事实检查。

Result: 在OSWorld生产力任务中，VerificAgent比基线CUA提升了111.1%的成功率。

Conclusion: VerificAgent有效管理CUAs记忆，显著提升性能。

Abstract: Continual memory augmentation allows computer-use agents (CUAs) to learn from
past interactions and refine their task-solving strategies over time. However,
unchecked memory accumulation can introduce spurious or hallucinated
"learnings" that degrade agent performance, particularly in domain-specific
workflows such as productivity software. We present a novel framework,
VerificAgent, that effectively manages memory for CUAs through (1) an
expert-curated seed of domain knowledge, (2) iterative, trajectory-based memory
refinement during training, and (3) a post-hoc fact-checking pass by human
experts to sanitize accumulated memory before deployment. On OSWorld
productivity tasks, VerificAgent achieves a 111.1% relative improvement in
success rate over baseline CUA without any additional fine-tuning.

</details>


### [444] [Beyond Invisibility: Learning Robust Visible Watermarks for Stronger Copyright Protection](https://arxiv.org/abs/2506.02665)
*Tianci Liu,Tong Yang,Quan Zhang,Qi Lei*

Main category: cs.LG

Relevance: 50.0

TL;DR: 提出了一种基于可见水印的长期版权保护方法，通过最大化重建内容与原内容的差异，提供更强的鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 随着AI技术的发展，未经授权使用受版权保护内容的风险增加，现有方法仅提供短期保护且需重新训练。本文旨在开发一种长期有效的保护机制。

Method: 提出了一种基于概率和逆问题的新框架，嵌入难以去除的可见水印，并通过近似算法解决双层优化问题。

Result: 实验结果表明，该方法在多种场景下表现优越。

Conclusion: 该方法为长期版权保护提供了一种有效且鲁棒的解决方案。

Abstract: As AI advances, copyrighted content faces growing risk of unauthorized use,
whether through model training or direct misuse. Building upon invisible
adversarial perturbation, recent works developed copyright protections against
specific AI techniques such as unauthorized personalization through DreamBooth
that are misused. However, these methods offer only short-term security, as
they require retraining whenever the underlying model architectures change. To
establish long-term protection aiming at better robustness, we go beyond
invisible perturbation, and propose a universal approach that embeds
\textit{visible} watermarks that are \textit{hard-to-remove} into images.
Grounded in a new probabilistic and inverse problem-based formulation, our
framework maximizes the discrepancy between the \textit{optimal} reconstruction
and the original content. We develop an effective and efficient approximation
algorithm to circumvent a intractable bi-level optimization. Experimental
results demonstrate superiority of our approach across diverse scenarios.

</details>


### [445] [Ubiquitous Symmetry at Critical Points Across Diverse Optimization Landscapes](https://arxiv.org/abs/2506.01959)
*Irmi Schneider*

Main category: cs.LG

Relevance: 40.0

TL;DR: 论文探讨了对称性在神经网络和其他数学结构中的作用，发现临界点均具有非平凡对称性，并引入新的对称性度量方法。


<details>
  <summary>Details</summary>
Motivation: 研究对称性在神经网络和其他空间中的损失函数中的作用，以理解优化问题的性质。

Method: 扩展对称性研究到更广泛的空间，包括有限域上的投影情况、八面体图、完美匹配和粒子吸引情况。

Result: 所有观察到的临界点均表现出非平凡对称性，新引入的对称性度量揭示了更多结构。

Conclusion: 对称性是优化问题中的普遍现象，新的度量方法有助于更全面地理解对称性结构。

Abstract: Symmetry plays a crucial role in understanding the properties of mathematical
structures and optimization problems. Recent work has explored this phenomenon
in the context of neural networks, where the loss function is invariant under
column and row permutations of the network weights. It has been observed that
local minima exhibit significant symmetry with respect to the network weights
(invariance to row and column permutations). And moreover no critical point was
found that lacked symmetry. We extend this line of inquiry by investigating
symmetry phenomena in real-valued loss functions defined on a broader class of
spaces. We will introduce four more cases: the projective case over a finite
field, the octahedral graph case, the perfect matching case, and the particle
attraction case. We show that as in the neural network case, all the critical
points observed have non-trivial symmetry. Finally we introduce a new measure
of symmetry in the system and show that it reveals additional symmetry
structures not captured by the previous measure.

</details>


### [446] [Efficient ANN-SNN Conversion with Error Compensation Learning](https://arxiv.org/abs/2506.01968)
*Chang Liu,Jiangrong Shen,Xuming Ran,Mingkun Xu,Qi Xu,Yi Xu,Gang Pan*

Main category: cs.LG

Relevance: 40.0

TL;DR: 提出了一种基于误差补偿学习的ANN-to-SNN转换框架，通过可学习的阈值裁剪函数、双阈值神经元和优化的膜电位初始化策略，显著减少了转换误差，实现了高精度和超低延迟。


<details>
  <summary>Details</summary>
Motivation: 解决当前ANN-to-SNN转换中因裁剪、量化和激活不均匀导致的精度损失和推理时间增加问题。

Method: 引入可学习的阈值裁剪函数、双阈值神经元和优化的膜电位初始化策略。

Result: 在CIFAR-10、CIFAR-100和ImageNet数据集上，仅用两个时间步就实现了高精度（CIFAR-10上94.75%）和超低延迟。

Conclusion: 该研究推动了SNN在低功耗硬件上的实际应用，实现了高效的实时处理。

Abstract: Artificial neural networks (ANNs) have demonstrated outstanding performance
in numerous tasks, but deployment in resource-constrained environments remains
a challenge due to their high computational and memory requirements. Spiking
neural networks (SNNs) operate through discrete spike events and offer superior
energy efficiency, providing a bio-inspired alternative. However, current
ANN-to-SNN conversion often results in significant accuracy loss and increased
inference time due to conversion errors such as clipping, quantization, and
uneven activation. This paper proposes a novel ANN-to-SNN conversion framework
based on error compensation learning. We introduce a learnable threshold
clipping function, dual-threshold neurons, and an optimized membrane potential
initialization strategy to mitigate the conversion error. Together, these
techniques address the clipping error through adaptive thresholds, dynamically
reduce the quantization error through dual-threshold neurons, and minimize the
non-uniformity error by effectively managing the membrane potential.
Experimental results on CIFAR-10, CIFAR-100, ImageNet datasets show that our
method achieves high-precision and ultra-low latency among existing conversion
methods. Using only two time steps, our method significantly reduces the
inference time while maintains competitive accuracy of 94.75% on CIFAR-10
dataset under ResNet-18 structure. This research promotes the practical
application of SNNs on low-power hardware, making efficient real-time
processing possible.

</details>


### [447] [Johnny: Structuring Representation Space to Enhance Machine Abstract Reasoning Ability](https://arxiv.org/abs/2506.01970)
*Ruizhuo Song,Beiming Yuan*

Main category: cs.LG

Relevance: 40.0

TL;DR: 论文提出Johnny架构和Spin-Transformer网络，通过改进表示空间和优化注意力机制，显著提升了RPM任务的推理性能。


<details>
  <summary>Details</summary>
Motivation: 传统RPM任务模型依赖选项池配置，限制了推理能力，需改进。

Method: 1. Johnny架构：表示提取模块与推理模块协同工作；2. Spin-Transformer：优化位置关系捕捉；3. 轻量级变体Straw Spin-Transformer。

Result: 实验显示Johnny和Spin-Transformer在RPM任务中表现优异。

Conclusion: 为AI抽象推理能力提供了创新方法。

Abstract: This paper thoroughly investigates the challenges of enhancing AI's abstract
reasoning capabilities, with a particular focus on Raven's Progressive Matrices
(RPM) tasks involving complex human-like concepts. Firstly, it dissects the
empirical reality that traditional end-to-end RPM-solving models heavily rely
on option pool configurations, highlighting that this dependency constrains the
model's reasoning capabilities. To address this limitation, the paper proposes
the Johnny architecture - a novel representation space-based framework for
RPM-solving. Through the synergistic operation of its Representation Extraction
Module and Reasoning Module, Johnny significantly enhances reasoning
performance by supplementing primitive negative option configurations with a
learned representation space. Furthermore, to strengthen the model's capacity
for capturing positional relationships among local features, the paper
introduces the Spin-Transformer network architecture, accompanied by a
lightweight Straw Spin-Transformer variant that reduces computational overhead
through parameter sharing and attention mechanism optimization. Experimental
evaluations demonstrate that both Johnny and Spin-Transformer achieve superior
performance on RPM tasks, offering innovative methodologies for advancing AI's
abstract reasoning capabilities.

</details>


### [448] [Coded Robust Aggregation for Distributed Learning under Byzantine Attacks](https://arxiv.org/abs/2506.01989)
*Chengxi Li,Ming Xiao,Mikael Skoglund*

Main category: cs.LG

Relevance: 40.0

TL;DR: 论文提出了一种新的分布式学习方法CRA-DL，通过编码鲁棒聚合应对拜占庭攻击，提升学习性能。


<details>
  <summary>Details</summary>
Motivation: 现有分布式学习方法在面对拜占庭攻击时性能显著下降，尤其是当设备间梯度差异较大时。

Method: 在训练前冗余分配数据，设备传输编码梯度，服务器使用鲁棒聚合规则更新全局模型。

Result: CRA-DL在拜占庭攻击下表现优于现有方法，编码梯度增强了聚合的鲁棒性。

Conclusion: CRA-DL通过编码梯度缩小差异，有效提升了分布式学习在拜占庭攻击下的性能。

Abstract: In this paper, we investigate the problem of distributed learning (DL) in the
presence of Byzantine attacks. For this problem, various robust bounded
aggregation (RBA) rules have been proposed at the central server to mitigate
the impact of Byzantine attacks. However, current DL methods apply RBA rules
for the local gradients from the honest devices and the disruptive information
from Byzantine devices, and the learning performance degrades significantly
when the local gradients of different devices vary considerably from each
other. To overcome this limitation, we propose a new DL method to cope with
Byzantine attacks based on coded robust aggregation (CRA-DL). Before training
begins, the training data are allocated to the devices redundantly. During
training, in each iteration, the honest devices transmit coded gradients to the
server computed from the allocated training data, and the server then
aggregates the information received from both honest and Byzantine devices
using RBA rules. In this way, the global gradient can be approximately
recovered at the server to update the global model. Compared with current DL
methods applying RBA rules, the improvement of CRA-DL is attributed to the fact
that the coded gradients sent by the honest devices are closer to each other.
This closeness enhances the robustness of the aggregation against Byzantine
attacks, since Byzantine messages tend to be significantly different from those
of honest devices in this case. We theoretically analyze the convergence
performance of CRA-DL. Finally, we present numerical results to verify the
superiority of the proposed method over existing baselines, showing its
enhanced learning performance under Byzantine attacks.

</details>


### [449] [Generalization Performance of Ensemble Clustering: From Theory to Algorithm](https://arxiv.org/abs/2506.02053)
*Xu Zhang,Haoye Qiu,Weixuan Liang,Hui Liu,Junhui Hou,Yuheng Jia*

Main category: cs.LG

Relevance: 40.0

TL;DR: 本文研究了集成聚类的泛化性能，推导了泛化误差和超额风险的收敛速度，并证明了在样本和基聚类数量趋于无穷时的理论一致性。通过权重分配优化，提出了一种新的集成聚类算法，性能优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 集成聚类在实践中表现优异，但理论基础不足。本文旨在填补这一空白，研究其泛化性能。

Method: 推导泛化误差和超额风险的收敛速度，提出权重分配优化方法，并设计新的集成聚类算法。

Result: 理论证明集成聚类的一致性，新算法在10个数据集上平均提升6.1%（NMI）、7.3%（ARI）和6.0%（Purity）。

Conclusion: 优化基聚类的偏差和多样性可提升性能，最大化多样性近似于鲁棒优化模型。

Abstract: Ensemble clustering has demonstrated great success in practice; however, its
theoretical foundations remain underexplored. This paper examines the
generalization performance of ensemble clustering, focusing on generalization
error, excess risk and consistency. We derive a convergence rate of
generalization error bound and excess risk bound both of
$\mathcal{O}(\sqrt{\frac{\log n}{m}}+\frac{1}{\sqrt{n}})$, with $n$ and $m$
being the numbers of samples and base clusterings. Based on this, we prove that
when $m$ and $n$ approach infinity and $m$ is significantly larger than log
$n$, i.e., $m,n\to \infty, m\gg \log n$, ensemble clustering is consistent.
Furthermore, recognizing that $n$ and $m$ are finite in practice, the
generalization error cannot be reduced to zero. Thus, by assigning varying
weights to finite clusterings, we minimize the error between the empirical
average clusterings and their expectation. From this, we theoretically
demonstrate that to achieve better clustering performance, we should minimize
the deviation (bias) of base clustering from its expectation and maximize the
differences (diversity) among various base clusterings. Additionally, we derive
that maximizing diversity is nearly equivalent to a robust (min-max)
optimization model. Finally, we instantiate our theory to develop a new
ensemble clustering algorithm. Compared with SOTA methods, our approach
achieves average improvements of 6.1%, 7.3%, and 6.0% on 10 datasets w.r.t.
NMI, ARI, and Purity. The code is available at https://github.com/xuz2019/GPEC.

</details>


### [450] [An Introduction to Flow Matching and Diffusion Models](https://arxiv.org/abs/2506.02070)
*Peter Holderrieth,Ezra Erives*

Main category: cs.LG

Relevance: 40.0

TL;DR: 该论文介绍了基于扩散和流模型的生成AI方法，涵盖理论和实践，适用于学生和从业者。


<details>
  <summary>Details</summary>
Motivation: 为生成AI领域提供一种基于扩散和流模型的全面介绍，帮助理解其理论和实践。

Method: 从常微分方程和随机微分方程出发，逐步介绍流匹配、分数匹配、无分类器引导等方法。

Result: 提供了一种现代生成AI模型的理论和实践框架，适用于图像和视频生成。

Conclusion: 该论文是理解和应用扩散和流模型的理想资源。

Abstract: Diffusion and flow-based models have become the state of the art for
generative AI across a wide range of data modalities, including images, videos,
shapes, molecules, music, and more! These notes are originally from
https://diffusion.csail.mit.edu/, as taught at MIT over the 2025 IAP (winter)
term, and are intended to accompany other course content, including lectures
and labs. Overall, they function as a self-contained introduction to both flow
matching and diffusion models, starting with ordinary and stochastic
differential equations, and culminating in flow matching, score matching,
classifier-free guidance, and the inner workings of modern, state-of-the-art
models for image and video. These notes, and the accompanying course, are ideal
for students and practitioners alike who want to develop a principled
understanding of the theory and practice of generative AI.

</details>


### [451] [Robust Federated Learning against Noisy Clients via Masked Optimization](https://arxiv.org/abs/2506.02079)
*Xuefeng Jiang,Tian Wen,Zhiqin Yang,Lvhua Wu,Yufeng Chen,Sheng Sun,Yuwei Wang,Min Liu*

Main category: cs.LG

Relevance: 40.0

TL;DR: 提出了一种名为MaskedOptim的两阶段优化框架，用于解决联邦学习中的复杂标签噪声问题，包括噪声客户端检测和标签校正。


<details>
  <summary>Details</summary>
Motivation: 联邦学习中不同客户端的标签噪声问题显著影响模型性能，需开发有效的优化策略。

Method: 两阶段框架：第一阶段检测高噪声客户端，第二阶段通过端到端标签校正机制修正噪声标签，并使用几何中值模型聚合增强鲁棒性。

Result: 在多个数据集上的实验表明，该框架在不同场景下表现鲁棒，并能有效提升噪声客户端的数据质量。

Conclusion: MaskedOptim能有效缓解标签噪声对联邦学习的负面影响，提高模型性能。

Abstract: In recent years, federated learning (FL) has made significant advance in
privacy-sensitive applications. However, it can be hard to ensure that FL
participants provide well-annotated data for training. The corresponding
annotations from different clients often contain complex label noise at varying
levels. This label noise issue has a substantial impact on the performance of
the trained models, and clients with greater noise levels can be largely
attributed for this degradation. To this end, it is necessary to develop an
effective optimization strategy to alleviate the adverse effects of these noisy
clients.In this study, we present a two-stage optimization framework,
MaskedOptim, to address this intricate label noise problem. The first stage is
designed to facilitate the detection of noisy clients with higher label noise
rates. The second stage focuses on rectifying the labels of the noisy clients'
data through an end-to-end label correction mechanism, aiming to mitigate the
negative impacts caused by misinformation within datasets. This is achieved by
learning the potential ground-truth labels of the noisy clients' datasets via
backpropagation. To further enhance the training robustness, we apply the
geometric median based model aggregation instead of the commonly-used vanilla
averaged model aggregation. We implement sixteen related methods and conduct
evaluations on three image datasets and one text dataset with diverse label
noise patterns for a comprehensive comparison. Extensive experimental results
indicate that our proposed framework shows its robustness in different
scenarios. Additionally, our label correction framework effectively enhances
the data quality of the detected noisy clients' local datasets. % Our codes
will be open-sourced to facilitate related research communities. Our codes are
available via https://github.com/Sprinter1999/MaskedOptim .

</details>


### [452] [Temporal Causal-based Simulation for Realistic Time-series Generation](https://arxiv.org/abs/2506.02084)
*Nikolaos Gkorgkolis,Nikolaos Kougioulis,MingXue Wang,Bora Caglayan,Andrea Tonon,Dario Simionato,Ioannis Tsamardinos*

Main category: cs.LG

Relevance: 40.0

TL;DR: 提出了一种名为TCS的框架，用于生成真实的时间序列数据及其因果图，通过三阶段方法估计因果结构、功能依赖和噪声分布，并展示了其在真实和合成数据上的有效性。


<details>
  <summary>Details</summary>
Motivation: 现有因果发现方法依赖合成数据，但难以准确模拟真实世界场景，尤其是在时间序列数据中。TCS旨在解决这一问题。

Method: TCS框架包括三阶段：估计滞后因果结构、近似变量间的功能依赖和学习噪声分布。采用Min-max优化和AutoML技术增强生成数据的真实性。

Result: 实验表明，TCS能够生成更真实的因果时间序列数据，并揭示了生成真实数据的复杂性。

Conclusion: TCS为生成真实因果时间序列数据提供了灵活且模型无关的解决方案，并提出了评估挑战。

Abstract: Causal Discovery plays a pivotal role in revealing relationships among
observed variables, particularly in the temporal setup. While the majority of
CD methods rely on synthetic data for evaluation, and recently for training,
these fall short in accurately mirroring real-world scenarios; an effect even
more evident in temporal data. Generation techniques depending on simplified
assumptions on causal structure, effects and time, limit the quality and
diversity of the simulated data. In this work, we introduce Temporal
Causal-based Simulation (TCS), a robust framework for generating realistic
time-series data and their associated temporal causal graphs. The approach is
structured in three phases: estimating the true lagged causal structure of the
data, approximating the functional dependencies between variables and learning
the noise distribution of the corresponding causal model, each part of which
can be explicitly tailored based on data assumptions and characteristics.
Through an extensive evaluation process, we highlight that single detection
methods for generated data discrimination prove inadequate, accentuating it as
a multifaceted challenge. For this, we detail a Min-max optimization phase that
draws on AutoML techniques. Our contributions include a flexible,
model-agnostic pipeline for generating realistic temporal causal data, a
thorough evaluation setup which enhances the validity of the generated datasets
and insights into the challenges posed by realistic data generation. Through
experiments involving not only real but also semi-synthetic and purely
synthetic datasets, we demonstrate that while sampling realistic causal data
remains a complex task, our method enriches the domain of generating sensible
causal-based temporal data.

</details>


### [453] [Constrained Sliced Wasserstein Embedding](https://arxiv.org/abs/2506.02203)
*Navid NaderiAlizadeh,Darian Salehi,Xinran Liu,Soheil Kolouri*

Main category: cs.LG

Relevance: 40.0

TL;DR: 论文提出了一种约束学习方法，用于优化Sliced Wasserstein (SW)距离的切片方向，以提高高维概率度量的比较效率。


<details>
  <summary>Details</summary>
Motivation: 现有的SW距离方法需要大量切片才能达到理想性能，计算复杂度高。通过优化切片方向，可以提升效率和性能。

Method: 采用约束学习方法，约束1D传输计划以近似原始空间中的最优计划，并通过梯度对偶方法训练切片参数。

Result: 在图像、点云和蛋白质序列的基础模型上，该方法显著提升了切片方向的性能。

Conclusion: 约束学习方法能有效优化SW距离的切片方向，提高高维嵌入的表示效率。

Abstract: Sliced Wasserstein (SW) distances offer an efficient method for comparing
high-dimensional probability measures by projecting them onto multiple
1-dimensional probability distributions. However, identifying informative
slicing directions has proven challenging, often necessitating a large number
of slices to achieve desirable performance and thereby increasing computational
complexity. We introduce a constrained learning approach to optimize the
slicing directions for SW distances. Specifically, we constrain the 1D
transport plans to approximate the optimal plan in the original space, ensuring
meaningful slicing directions. By leveraging continuous relaxations of these
transport plans, we enable a gradient-based primal-dual approach to train the
slicer parameters, alongside the remaining model parameters. We demonstrate how
this constrained slicing approach can be applied to pool high-dimensional
embeddings into fixed-length permutation-invariant representations. Numerical
results on foundation models trained on images, point clouds, and protein
sequences showcase the efficacy of the proposed constrained learning approach
in learning more informative slicing directions. Our implementation code can be
found at https://github.com/Stranja572/constrainedswe.

</details>


### [454] [From Street Views to Urban Science: Discovering Road Safety Factors with Multimodal Large Language Models](https://arxiv.org/abs/2506.02242)
*Yihong Tang,Ao Qu,Xujing Yu,Weipeng Deng,Jun Ma,Jinhua Zhao,Lijun Sun*

Main category: cs.LG

Relevance: 40.0

TL;DR: 提出了一种基于多模态大语言模型（MLLM）的方法，用于城市和交通研究中的可解释假设推断，通过自动化生成、评估和优化假设，提升道路安全研究的效率和可解释性。


<details>
  <summary>Details</summary>
Motivation: 传统方法依赖人工专家提出假设，耗时且易受确认偏误影响；深度学习模型可解释性差；未充分利用非结构化数据。

Method: 利用MLLM从街景图像生成安全相关问题，提取可解释嵌入，并用于回归模型，支持迭代假设测试和优化。

Result: 在曼哈顿街区的实验中，该方法优于预训练的深度学习模型，同时保持完全可解释性。

Conclusion: UrbanX不仅适用于道路安全研究，还可作为通用框架，从非结构化数据中提取结构化见解，增强模型可信度。

Abstract: Urban and transportation research has long sought to uncover statistically
meaningful relationships between key variables and societal outcomes such as
road safety, to generate actionable insights that guide the planning,
development, and renewal of urban and transportation systems. However,
traditional workflows face several key challenges: (1) reliance on human
experts to propose hypotheses, which is time-consuming and prone to
confirmation bias; (2) limited interpretability, particularly in deep learning
approaches; and (3) underutilization of unstructured data that can encode
critical urban context. Given these limitations, we propose a Multimodal Large
Language Model (MLLM)-based approach for interpretable hypothesis inference,
enabling the automated generation, evaluation, and refinement of hypotheses
concerning urban context and road safety outcomes. Our method leverages MLLMs
to craft safety-relevant questions for street view images (SVIs), extract
interpretable embeddings from their responses, and apply them in
regression-based statistical models. UrbanX supports iterative hypothesis
testing and refinement, guided by statistical evidence such as coefficient
significance, thereby enabling rigorous scientific discovery of previously
overlooked correlations between urban design and safety. Experimental
evaluations on Manhattan street segments demonstrate that our approach
outperforms pretrained deep learning models while offering full
interpretability. Beyond road safety, UrbanX can serve as a general-purpose
framework for urban scientific discovery, extracting structured insights from
unstructured urban data across diverse socioeconomic and environmental
outcomes. This approach enhances model trustworthiness for policy applications
and establishes a scalable, statistically grounded pathway for interpretable
knowledge discovery in urban and transportation studies.

</details>


### [455] [From Features to Structure: Task-Aware Graph Construction for Relational and Tabular Learning with GNNs](https://arxiv.org/abs/2506.02243)
*Tamara Cucumides,Floris Geerts*

Main category: cs.LG

Relevance: 40.0

TL;DR: auGraph是一个统一框架，用于任务感知的图增强，适用于表格和关系数据，通过选择性提升属性为节点来优化图结构，优于基于模式和启发式的图构建方法。


<details>
  <summary>Details</summary>
Motivation: 表格和关系数据在机器学习应用中普遍存在，但现有GNN方法通常依赖模式衍生的图结构，未能充分利用非关键属性的预测信号。

Method: 引入auGraph框架，通过任务感知的评分函数选择性地将属性提升为节点，增强基础图结构。

Result: auGraph在关系性和表格预测任务中表现优于基于模式和启发式的图构建方法。

Conclusion: auGraph通过任务感知的图增强，有效提升了表格和关系数据的学习性能。

Abstract: Tabular and relational data remain the most ubiquitous formats in real-world
machine learning applications, spanning domains from finance to healthcare.
Although both formats offer structured representations, they pose distinct
challenges for modern deep learning methods, which typically assume flat,
feature-aligned inputs. Graph Neural Networks (GNNs) have emerged as a
promising solution by capturing structural dependencies within and between
tables. However, existing GNN-based approaches often rely on rigid,
schema-derived graphs -- such as those based on primary-foreign key links --
thereby underutilizing rich, predictive signals in non key attributes. In this
work, we introduce auGraph, a unified framework for task-aware graph
augmentation that applies to both tabular and relational data. auGraph enhances
base graph structures by selectively promoting attributes into nodes, guided by
scoring functions that quantify their relevance to the downstream prediction
task. This augmentation preserves the original data schema while injecting
task-relevant structural signal. Empirically, auGraph outperforms schema-based
and heuristic graph construction methods by producing graphs that better
support learning for relational and tabular prediction tasks.

</details>


### [456] [A Tale of Two Symmetries: Exploring the Loss Landscape of Equivariant Models](https://arxiv.org/abs/2506.02269)
*YuQing Xie,Tess Smidt*

Main category: cs.LG

Relevance: 40.0

TL;DR: 论文探讨了等变神经网络优化中的问题，发现等变约束可能阻碍学习全局最小值，并提出通过放松约束和重新选择群表示来改善优化。


<details>
  <summary>Details</summary>
Motivation: 研究等变神经网络优化中的挑战，尤其是等变约束是否阻碍优化或仅需不同超参数调整。

Method: 通过理论分析损失景观几何，重点关注基于置换表示的神经网络，并比较其与无约束MLP的差异。

Result: 发现等变子空间的参数对称性可能阻碍学习全局最小值，放松约束或改变群表示可解决问题。

Conclusion: 等变网络的优化需考虑更大无约束函数空间，并重新思考隐藏层群表示的选择。

Abstract: Equivariant neural networks have proven to be effective for tasks with known
underlying symmetries. However, optimizing equivariant networks can be tricky
and best training practices are less established than for standard networks. In
particular, recent works have found small training benefits from relaxing
equivariance constraints. This raises the question: do equivariance constraints
introduce fundamental obstacles to optimization? Or do they simply require
different hyperparameter tuning? In this work, we investigate this question
through a theoretical analysis of the loss landscape geometry. We focus on
networks built using permutation representations, which we can view as a subset
of unconstrained MLPs. Importantly, we show that the parameter symmetries of
the unconstrained model has nontrivial effects on the loss landscape of the
equivariant subspace and under certain conditions can provably prevent learning
of the global minima. Further, we empirically demonstrate in such cases,
relaxing to an unconstrained MLP can sometimes solve the issue. Interestingly,
the weights eventually found via relaxation corresponds to a different choice
of group representation in the hidden layer. From this, we draw 3 key
takeaways. (1) Viewing any class of networks in the context of larger
unconstrained function space can give important insights on loss landscape
structure. (2) Within the unconstrained function space, equivariant networks
form a complicated union of linear hyperplanes, each associated with a specific
choice of internal group representation. (3) Effective relaxation of
equivariance may require not only adding nonequivariant degrees of freedom, but
also rethinking the fixed choice of group representations in hidden layers.

</details>


### [457] [On Universality Classes of Equivariant Networks](https://arxiv.org/abs/2506.02293)
*Marco Pacini,Gabriele Santin,Bruno Lepri,Shubhendu Trivedi*

Main category: cs.LG

Relevance: 40.0

TL;DR: 该论文研究了等变神经网络的近似能力，发现分离能力并不能完全捕捉其表达能力，并提出了浅层不变网络的通用性框架。


<details>
  <summary>Details</summary>
Motivation: 探讨等变神经网络在对称性学习中的近似能力，弥补现有研究中分离能力与表达能力之间的差距。

Method: 通过分析浅层不变网络的通用性类别，提出框架以理解其近似能力，并研究对称群结构对表达能力的影响。

Result: 发现分离能力不能完全决定表达能力，某些情况下浅层网络可实现分离约束的通用性，但依赖于对称群的结构特性。

Conclusion: 等变神经网络的表达能力不仅取决于分离能力，还受对称群结构的影响，某些情况下浅层网络可实现通用性。

Abstract: Equivariant neural networks provide a principled framework for incorporating
symmetry into learning architectures and have been extensively analyzed through
the lens of their separation power, that is, the ability to distinguish inputs
modulo symmetry. This notion plays a central role in settings such as graph
learning, where it is often formalized via the Weisfeiler-Leman hierarchy. In
contrast, the universality of equivariant models-their capacity to approximate
target functions-remains comparatively underexplored. In this work, we
investigate the approximation power of equivariant neural networks beyond
separation constraints. We show that separation power does not fully capture
expressivity: models with identical separation power may differ in their
approximation ability. To demonstrate this, we characterize the universality
classes of shallow invariant networks, providing a general framework for
understanding which functions these architectures can approximate. Since
equivariant models reduce to invariant ones under projection, this analysis
yields sufficient conditions under which shallow equivariant networks fail to
be universal. Conversely, we identify settings where shallow models do achieve
separation-constrained universality. These positive results, however, depend
critically on structural properties of the symmetry group, such as the
existence of adequate normal subgroups, which may not hold in important cases
like permutation symmetry.

</details>


### [458] [A Data-Based Architecture for Flight Test without Test Points](https://arxiv.org/abs/2506.02315)
*D. Isaiah Harp,Joshua Ott,John Alora,Dylan Asmar*

Main category: cs.LG

Relevance: 40.0

TL;DR: 论文提出了一种消除测试点的新方法，通过高保真数字模型生成降阶模型（ROM），利用机器学习动态更新模型以适应飞行条件。


<details>
  <summary>Details</summary>
Motivation: 传统测试点方法因飞行员偏离预设条件而失效，且数据带和公差的存在是更根本的问题。本文旨在解决这一问题。

Method: 使用高保真数字模型生成ROM，通过机器学习（如高斯过程回归）动态更新模型，适应任意飞行条件。

Result: 通过T-38C飞行测试数据验证，ROM能动态更新并生成参数以评估MIL-STD-1797B合规性。

Conclusion: 该方法成功消除了测试点需求，实现了动态模型更新和验证。

Abstract: The justification for the "test point" derives from the test pilot's
obligation to reproduce faithfully the pre-specified conditions of some model
prediction. Pilot deviation from those conditions invalidates the model
assumptions. Flight test aids have been proposed to increase accuracy on more
challenging test points. However, the very existence of databands and
tolerances is the problem more fundamental than inadequate pilot skill. We
propose a novel approach, which eliminates test points. We start with a
high-fidelity digital model of an air vehicle. Instead of using this model to
generate a point prediction, we use a machine learning method to produce a
reduced-order model (ROM). The ROM has two important properties. First, it can
generate a prediction based on any set of conditions the pilot flies. Second,
if the test result at those conditions differ from the prediction, the ROM can
be updated using the new data. The outcome of flight test is thus a refined ROM
at whatever conditions were flown. This ROM in turn updates and validates the
high-fidelity model. We present a single example of this "point-less"
architecture, using T-38C flight test data. We first use a generic aircraft
model to build a ROM of longitudinal pitching motion as a hypersurface. We then
ingest unconstrained flight test data and use Gaussian Process Regression to
update and condition the hypersurface. By proposing a second-order equivalent
system for the T-38C, this hypersurface then generates parameters necessary to
assess MIL-STD-1797B compliance for longitudinal dynamics.

</details>


### [459] [SFBD Flow: A Continuous-Optimization Framework for Training Diffusion Models with Noisy Samples](https://arxiv.org/abs/2506.02371)
*Haoye Lu,Darren Lo,Yaoliang Yu*

Main category: cs.LG

Relevance: 40.0

TL;DR: SFBD flow是一种改进的扩散模型训练方法，通过连续变体避免手动协调，提升隐私保护性能。


<details>
  <summary>Details</summary>
Motivation: 扩散模型在生成任务中表现优异，但存在数据隐私问题，SFBD通过训练损坏数据减少隐私风险。

Method: 提出SFBD flow，将SFBD重新解释为交替投影算法，并引入连续变体，消除交替步骤。

Result: Online SFBD在多个基准测试中优于基线方法。

Conclusion: SFBD flow为扩散模型提供了一种更高效、隐私友好的训练方法。

Abstract: Diffusion models achieve strong generative performance but often rely on
large datasets that may include sensitive content. This challenge is compounded
by the models' tendency to memorize training data, raising privacy concerns.
SFBD (Lu et al., 2025) addresses this by training on corrupted data and using
limited clean samples to capture local structure and improve convergence.
However, its iterative denoising and fine-tuning loop requires manual
coordination, making it burdensome to implement. We reinterpret SFBD as an
alternating projection algorithm and introduce a continuous variant, SFBD flow,
that removes the need for alternating steps. We further show its connection to
consistency constraint-based methods, and demonstrate that its practical
instantiation, Online SFBD, consistently outperforms strong baselines across
benchmarks.

</details>


### [460] [Asymptotically Optimal Linear Best Feasible Arm Identification with Fixed Budget](https://arxiv.org/abs/2506.02386)
*Jie Bian,Vincent Y. F. Tan*

Main category: cs.LG

Relevance: 40.0

TL;DR: 论文提出了一种在线性bandit问题中识别最佳可行臂的新算法，保证误差概率指数衰减，且衰减速率匹配理论下界。


<details>
  <summary>Details</summary>
Motivation: 解决固定预算下识别最佳可行臂时误差概率指数衰减速率未明确的问题。

Method: 基于后验采样的游戏规则，结合min-learner和max-learner，优化固定预算下的识别过程。

Result: 算法在误差概率衰减速率上匹配理论下界，并在实验中优于基准算法。

Conclusion: 新算法在理论和实证上均表现出色，解决了固定预算下的最佳臂识别问题。

Abstract: The challenge of identifying the best feasible arm within a fixed budget has
attracted considerable interest in recent years. However, a notable gap remains
in the literature: the exact exponential rate at which the error probability
approaches zero has yet to be established, even in the relatively simple
setting of $K$-armed bandits with Gaussian noise. In this paper, we address
this gap by examining the problem within the context of linear bandits. We
introduce a novel algorithm for best feasible arm identification that
guarantees an exponential decay in the error probability. Remarkably, the decay
rate -- characterized by the exponent -- matches the theoretical lower bound
derived using information-theoretic principles. Our approach leverages a
posterior sampling framework embedded within a game-based sampling rule
involving a min-learner and a max-learner. This strategy shares its foundations
with Thompson sampling, but is specifically tailored to optimize the
identification process under fixed-budget constraints. Furthermore, we validate
the effectiveness of our algorithm through comprehensive empirical evaluations
across various problem instances with different levels of complexity. The
results corroborate our theoretical findings and demonstrate that our method
outperforms several benchmark algorithms in terms of both accuracy and
efficiency.

</details>


### [461] [GAdaBoost: An Efficient and Robust AdaBoost Algorithm Based on Granular-Ball Structure](https://arxiv.org/abs/2506.02390)
*Qin Xie,Qinghua Zhang,Shuyin Xia,Xinran Zhou,Guoyin Wang*

Main category: cs.LG

Relevance: 40.0

TL;DR: 提出了一种基于粒度计算的两阶段框架GAdaBoost，通过数据粒化和自适应增强阶段提升噪声条件下的效率和鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 解决AdaBoost在多类分类任务中因标签噪声导致的效率低和鲁棒性差的问题。

Method: 1. 设计数据粒化阶段生成粒度球以压缩数据并保留多样性；2. 提出基于粒度球的SAMME算法（GAdaBoost.SA），专注于粒度球而非单个样本。

Result: 在噪声数据集上表现出优于现有方法的鲁棒性和效率。

Conclusion: GAdaBoost有效扩展了AdaBoost和SAMME，适用于噪声环境。

Abstract: Adaptive Boosting (AdaBoost) faces significant challenges posed by label
noise, especially in multiclass classification tasks. Existing methods either
lack mechanisms to handle label noise effectively or suffer from high
computational costs due to redundant data usage. Inspired by granular
computing, this paper proposes granular adaptive boosting (GAdaBoost), a novel
two-stage framework comprising a data granulation stage and an adaptive
boosting stage, to enhance efficiency and robustness under noisy conditions. To
validate its feasibility, an extension of SAMME, termed GAdaBoost.SA, is
proposed. Specifically, first, a granular-ball generation method is designed to
compress data while preserving diversity and mitigating label noise. Second,
the granular ball-based SAMME algorithm focuses on granular balls rather than
individual samples, improving efficiency and reducing sensitivity to noise.
Experimental results on some noisy datasets show that the proposed approach
achieves superior robustness and efficiency compared with existing methods,
demonstrating that this work effectively extends AdaBoost and SAMME.

</details>


### [462] [Random at First, Fast at Last: NTK-Guided Fourier Pre-Processing for Tabular DL](https://arxiv.org/abs/2506.02406)
*Renat Sergazinov,Jing Wu,Shao-An Yin*

Main category: cs.LG

Relevance: 40.0

TL;DR: 论文提出了一种基于随机傅里叶特征的方法，用于改进表格数据的深度学习训练效率。


<details>
  <summary>Details</summary>
Motivation: 表格数据深度学习的现有方法存在不足，通过NTK分析揭示了这些问题，从而提出随机傅里叶映射作为一种无参数、架构无关的预处理方法。

Method: 使用正弦和余弦投影将输入映射到固定特征空间，频率在初始化时随机抽取，避免了额外的归一化或可学习嵌入。

Result: 在NTK框架下，该方法能够约束和调节网络的初始NTK谱，并引入偏置以缩短优化轨迹，从而加速训练。实验表明，该方法能更快收敛并提升最终性能。

Conclusion: 随机傅里叶预处理是一种理论支持、即插即用的表格深度学习增强方法。

Abstract: While random Fourier features are a classic tool in kernel methods, their
utility as a pre-processing step for deep learning on tabular data has been
largely overlooked. Motivated by shortcomings in tabular deep learning
pipelines - revealed through Neural Tangent Kernel (NTK) analysis - we revisit
and repurpose random Fourier mappings as a parameter-free,
architecture-agnostic transformation. By projecting each input into a fixed
feature space via sine and cosine projections with frequencies drawn once at
initialization, this approach circumvents the need for ad hoc normalization or
additional learnable embeddings. We show within the NTK framework that this
mapping (i) bounds and conditions the network's initial NTK spectrum, and (ii)
introduces a bias that shortens the optimization trajectory, thereby
accelerating gradient-based training. These effects pre-condition the network
with a stable kernel from the outset. Empirically, we demonstrate that deep
networks trained on Fourier-transformed inputs converge more rapidly and
consistently achieve strong final performance, often with fewer epochs and less
hyperparameter tuning. Our findings establish random Fourier pre-processing as
a theoretically motivated, plug-and-play enhancement for tabular deep learning.

</details>


### [463] [AERO: A Redirection-Based Optimization Framework Inspired by Judo for Robust Probabilistic Forecasting](https://arxiv.org/abs/2506.02415)
*Karthikeyan Vaiapury*

Main category: cs.LG

Relevance: 40.0

TL;DR: AERO（Adversarial Energy-based Redirection Optimization）是一种受柔道启发的优化框架，通过利用外部扰动而非抵抗来实现稳定和适应性优化。


<details>
  <summary>Details</summary>
Motivation: 现有优化方法在动态非线性系统中难以保持稳定性和适应性，尤其是在不确定性环境下。

Method: AERO通过15个相互关联的公理（如对抗校正、能量守恒和扰动感知学习）重新定义优化过程，包括梯度投影、不确定性驱动的动态集成和学习能量管理。

Result: 在概率太阳能预测中，AERO显著提高了预测准确性、可靠性和适应性，尤其在噪声和不确定环境中。

Conclusion: AERO为优化理论和实践提供了新的方向。

Abstract: Optimization remains a fundamental pillar of machine learning, yet existing
methods often struggle to maintain stability and adaptability in dynamic, non
linear systems, especially under uncertainty. We introduce AERO (Adversarial
Energy-based Redirection Optimization), a novel framework inspired by the
redirection principle in Judo, where external disturbances are leveraged rather
than resisted. AERO reimagines optimization as a redirection process guided by
15 interrelated axioms encompassing adversarial correction, energy
conservation, and disturbance-aware learning. By projecting gradients,
integrating uncertainty driven dynamics, and managing learning energy, AERO
offers a principled approach to stable and robust model updates. Applied to
probabilistic solar energy forecasting, AERO demonstrates substantial gains in
predictive accuracy, reliability, and adaptability, especially in noisy and
uncertain environments. Our findings highlight AERO as a compelling new
direction in the theoretical and practical landscape of optimization.

</details>


### [464] [Weak Supervision for Real World Graphs](https://arxiv.org/abs/2506.02451)
*Pratheeksha Nair,Reihaneh Rabbany*

Main category: cs.LG

Relevance: 40.0

TL;DR: WSNET是一种弱监督图对比学习框架，利用图中的弱信号（如噪声或间接线索）进行鲁棒表示学习，在真实世界和合成数据集上表现优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 解决现实世界图中标签稀缺和噪声问题（如人口贩运检测和虚假信息监控），利用弱信号进行学习。

Method: 提出WSNET框架，结合图结构、节点特征和多个噪声监督源，通过对比学习目标处理弱标签数据。

Result: 在三个真实数据集和合成基准测试中，WSNET的F1分数比现有方法高出15%。

Conclusion: 弱监督下的对比学习在基于图的任务中具有潜力，可利用不完美标签提升性能。

Abstract: Node classification in real world graphs often suffers from label scarcity
and noise, especially in high stakes domains like human trafficking detection
and misinformation monitoring. While direct supervision is limited, such graphs
frequently contain weak signals, noisy or indirect cues, that can still inform
learning. We propose WSNET, a novel weakly supervised graph contrastive
learning framework that leverages these weak signals to guide robust
representation learning. WSNET integrates graph structure, node features, and
multiple noisy supervision sources through a contrastive objective tailored for
weakly labeled data. Across three real world datasets and synthetic benchmarks
with controlled noise, WSNET consistently outperforms state of the art
contrastive and noisy label learning methods by up to 15% in F1 score. Our
results highlight the effectiveness of contrastive learning under weak
supervision and the promise of exploiting imperfect labels in graph based
settings.

</details>


### [465] [Stochastic Momentum Methods for Non-smooth Non-Convex Finite-Sum Coupled Compositional Optimization](https://arxiv.org/abs/2506.02504)
*Xingyu Chen,Bokun Wang,Ming Yang,Quanqi Hu,Qihang Lin,Tianbao Yang*

Main category: cs.LG

Relevance: 40.0

TL;DR: 本文提出了一种针对非光滑弱凸或凸的有限和耦合组合优化（FCCO）问题的随机动量方法，显著降低了迭代复杂度至O(1/ε^5)，并应用于多不等式约束的非凸优化问题。


<details>
  <summary>Details</summary>
Motivation: 现有方法在处理非光滑FCCO问题时存在高迭代复杂度和不适用于深度学习的问题，本文旨在解决这些局限性。

Method: 提出随机动量方法，适用于非光滑FCCO问题，并证明其收敛性。

Result: 将迭代复杂度从O(1/ε^6)降至O(1/ε^5)，并在实验中验证了算法的有效性。

Conclusion: 本文提出的方法显著提升了非光滑FCCO问题的优化效率，适用于复杂约束问题。

Abstract: Finite-sum Coupled Compositional Optimization (FCCO), characterized by its
coupled compositional objective structure, emerges as an important optimization
paradigm for addressing a wide range of machine learning problems. In this
paper, we focus on a challenging class of non-convex non-smooth FCCO, where the
outer functions are non-smooth weakly convex or convex and the inner functions
are smooth or weakly convex. Existing state-of-the-art result face two key
limitations: (1) a high iteration complexity of $O(1/\epsilon^6)$ under the
assumption that the stochastic inner functions are Lipschitz continuous in
expectation; (2) reliance on vanilla SGD-type updates, which are not suitable
for deep learning applications. Our main contributions are two fold: (i) We
propose stochastic momentum methods tailored for non-smooth FCCO that come with
provable convergence guarantees; (ii) We establish a new state-of-the-art
iteration complexity of $O(1/\epsilon^5)$. Moreover, we apply our algorithms to
multiple inequality constrained non-convex optimization problems involving
smooth or weakly convex functional inequality constraints. By optimizing a
smoothed hinge penalty based formulation, we achieve a new state-of-the-art
complexity of $O(1/\epsilon^5)$ for finding an (nearly) $\epsilon$-level KKT
solution. Experiments on three tasks demonstrate the effectiveness of the
proposed algorithms.

</details>


### [466] [Privacy-Preserving Federated Convex Optimization: Balancing Partial-Participation and Efficiency via Noise Cancellation](https://arxiv.org/abs/2506.02563)
*Roie Reshef,Kfir Yehuda Levy*

Main category: cs.LG

Relevance: 40.0

TL;DR: 本文提出了一种在联邦学习中实现差分隐私的新方法，适用于部分参与场景，通过噪声消除机制保持隐私且不影响收敛效率。


<details>
  <summary>Details</summary>
Motivation: 解决联邦学习在部分参与场景下实现差分隐私的挑战，填补现有方法在此场景下的性能不足。

Method: 引入噪声消除机制，结合随机凸优化框架，分析其在同质和异质数据分布下的性能。

Result: 方法在部分参与场景下实现了最优性能，同时保持隐私和计算效率。

Conclusion: 扩展了差分隐私在联邦学习中的应用，为分布式系统中的隐私保护学习提供了高效解决方案。

Abstract: This paper tackles the challenge of achieving Differential Privacy (DP) in
Federated Learning (FL) under partial-participation, where only a subset of the
machines participate in each time-step. While previous work achieved optimal
performance in full-participation settings, these methods struggled to extend
to partial-participation scenarios. Our approach fills this gap by introducing
a novel noise-cancellation mechanism that preserves privacy without sacrificing
convergence rates or computational efficiency. We analyze our method within the
Stochastic Convex Optimization (SCO) framework and show that it delivers
optimal performance for both homogeneous and heterogeneous data distributions.
This work expands the applicability of DP in FL, offering an efficient and
practical solution for privacy-preserving learning in distributed systems with
partial participation.

</details>


### [467] [HGOT: Self-supervised Heterogeneous Graph Neural Network with Optimal Transport](https://arxiv.org/abs/2506.02619)
*Yanbei Liu,Chongxu Wang,Zhitao Xiao,Lei Geng,Yanwei Pang,Xiao Wang*

Main category: cs.LG

Relevance: 40.0

TL;DR: 提出了一种基于最优传输的自监督异构图神经网络（HGOT），无需图增强策略即可学习高质量节点表示。


<details>
  <summary>Details</summary>
Motivation: 解决传统对比自监督学习中需要精心设计图增强和样本采样的问题。

Method: 利用最优传输机制替代正负样本采样，设计中心视图整合分支视图语义信息。

Result: 在四个真实数据集上表现优异，节点分类任务准确率平均提升6%。

Conclusion: HGOT在自监督异构图学习中具有显著优势。

Abstract: Heterogeneous Graph Neural Networks (HGNNs), have demonstrated excellent
capabilities in processing heterogeneous information networks. Self-supervised
learning on heterogeneous graphs, especially contrastive self-supervised
strategy, shows great potential when there are no labels. However, this
approach requires the use of carefully designed graph augmentation strategies
and the selection of positive and negative samples. Determining the exact level
of similarity between sample pairs is non-trivial.To solve this problem, we
propose a novel self-supervised Heterogeneous graph neural network with Optimal
Transport (HGOT) method which is designed to facilitate self-supervised
learning for heterogeneous graphs without graph augmentation strategies.
Different from traditional contrastive self-supervised learning, HGOT employs
the optimal transport mechanism to relieve the laborious sampling process of
positive and negative samples. Specifically, we design an aggregating view
(central view) to integrate the semantic information contained in the views
represented by different meta-paths (branch views). Then, we introduce an
optimal transport plan to identify the transport relationship between the
semantics contained in the branch view and the central view. This allows the
optimal transport plan between graphs to align with the representations,
forcing the encoder to learn node representations that are more similar to the
graph space and of higher quality. Extensive experiments on four real-world
datasets demonstrate that our proposed HGOT model can achieve state-of-the-art
performance on various downstream tasks. In particular, in the node
classification task, HGOT achieves an average of more than 6% improvement in
accuracy compared with state-of-the-art methods.

</details>


### [468] [SiamNAS: Siamese Surrogate Model for Dominance Relation Prediction in Multi-objective Neural Architecture Search](https://arxiv.org/abs/2506.02623)
*Yuyang Zhou,Ferrante Neri,Yew-Soon Ong,Ruibin Bai*

Main category: cs.LG

Relevance: 40.0

TL;DR: 提出了一种基于Siamese网络的代理模型SiamNAS，用于高效多目标神经架构搜索（NAS），显著降低计算成本。


<details>
  <summary>Details</summary>
Motivation: 现代NAS是多目标的，计算成本高，需要高效近似方法。

Method: 使用Siamese网络块集成预测架构优势关系，替代拥挤距离计算，集成到SiamNAS框架中。

Result: 在NAS-Bench-201上，SiamNAS以0.01 GPU天找到Pareto最优解，性能优异。

Conclusion: Siamese代理模型在多任务优化中潜力大，可扩展为生成多样Pareto解集。

Abstract: Modern neural architecture search (NAS) is inherently multi-objective,
balancing trade-offs such as accuracy, parameter count, and computational cost.
This complexity makes NAS computationally expensive and nearly impossible to
solve without efficient approximations. To address this, we propose a novel
surrogate modelling approach that leverages an ensemble of Siamese network
blocks to predict dominance relationships between candidate architectures.
Lightweight and easy to train, the surrogate achieves 92% accuracy and replaces
the crowding distance calculation in the survivor selection strategy with a
heuristic rule based on model size. Integrated into a framework termed SiamNAS,
this design eliminates costly evaluations during the search process.
Experiments on NAS-Bench-201 demonstrate the framework's ability to identify
Pareto-optimal solutions with significantly reduced computational costs. The
proposed SiamNAS identified a final non-dominated set containing the best
architecture in NAS-Bench-201 for CIFAR-10 and the second-best for ImageNet, in
terms of test error rate, within 0.01 GPU days. This proof-of-concept study
highlights the potential of the proposed Siamese network surrogate model to
generalise to multi-tasking optimisation, enabling simultaneous optimisation
across tasks. Additionally, it offers opportunities to extend the approach for
generating Sets of Pareto Sets (SOS), providing diverse Pareto-optimal
solutions for heterogeneous task settings.

</details>


### [469] [A Pretrained Probabilistic Transformer for City-Scale Traffic Volume Prediction](https://arxiv.org/abs/2506.02654)
*Shiyu Shen,Bin Pan,Guirong Xue*

Main category: cs.LG

Relevance: 40.0

TL;DR: TrafficPPT是一个预训练的概率Transformer模型，用于城市规模交通流量预测，通过融合多源数据实现不确定性感知的预测，并在数据稀疏情况下表现优异。


<details>
  <summary>Details</summary>
Motivation: 解决城市交通流量预测中数据不完整和偏差的问题，同时克服现有方法缺乏不确定性建模和城市特定训练的局限性。

Method: 提出TrafficPPT，融合实时观测、历史轨迹和路网拓扑数据，通过大规模模拟数据预训练和城市特定微调实现领域适应。

Result: 在真实数据集上，TrafficPPT在数据稀疏条件下显著优于现有基线方法。

Conclusion: TrafficPPT通过预训练和不确定性建模，提升了交通流量预测的鲁棒性和泛化能力。

Abstract: City-scale traffic volume prediction plays a pivotal role in intelligent
transportation systems, yet remains a challenge due to the inherent
incompleteness and bias in observational data. Although deep learning-based
methods have shown considerable promise, most existing approaches produce
deterministic point estimates, thereby neglecting the uncertainty arising from
unobserved traffic flows. Furthermore, current models are typically trained in
a city-specific manner, which hinders their generalizability and limits
scalability across diverse urban contexts. To overcome these limitations, we
introduce TrafficPPT, a Pretrained Probabilistic Transformer designed to model
traffic volume as a distributional aggregation of trajectories. Our framework
fuses heterogeneous data sources-including real-time observations, historical
trajectory data, and road network topology-enabling robust and
uncertainty-aware traffic inference. TrafficPPT is initially pretrained on
large-scale simulated data spanning multiple urban scenarios, and later
fine-tuned on target cities to ensure effective domain adaptation. Experiments
on real-world datasets show that TrafficPPT consistently surpasses
state-of-the-art baselines, particularly under conditions of extreme data
sparsity. Code will be open.

</details>


### [470] [Theoretical Performance Guarantees for Partial Domain Adaptation via Partial Optimal Transport](https://arxiv.org/abs/2506.02712)
*Jayadev Naram,Fredrik Hellström,Ziming Wang,Rebecka Jörnsten,Giuseppe Durisi*

Main category: cs.LG

Relevance: 40.0

TL;DR: 本文提出了一种基于部分最优传输的泛化界限，用于部分域适应（PDA）问题，并设计了一种名为WARMPOT的算法，实验表明其优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 在目标分布数据稀缺而相关源分布数据丰富的情况下，现有PDA方法的权重方案缺乏理论基础，本文旨在填补这一空白。

Method: 基于部分最优传输推导PDA的泛化界限，提出部分Wasserstein距离作为域对齐项，并设计WARMPOT算法。

Result: 实验表明WARMPOT优于现有方法，且提出的权重方案更有效。

Conclusion: 部分最优传输为PDA提供了理论支持，WARMPOT算法在实践中表现优异。

Abstract: In many scenarios of practical interest, labeled data from a target
distribution are scarce while labeled data from a related source distribution
are abundant. One particular setting of interest arises when the target label
space is a subset of the source label space, leading to the framework of
partial domain adaptation (PDA). Typical approaches to PDA involve minimizing a
domain alignment term and a weighted empirical loss on the source data, with
the aim of transferring knowledge between domains. However, a theoretical basis
for this procedure is lacking, and in particular, most existing weighting
schemes are heuristic. In this work, we derive generalization bounds for the
PDA problem based on partial optimal transport. These bounds corroborate the
use of the partial Wasserstein distance as a domain alignment term, and lead to
theoretically motivated explicit expressions for the empirical source loss
weights. Inspired by these bounds, we devise a practical algorithm for PDA,
termed WARMPOT. Through extensive numerical experiments, we show that WARMPOT
is competitive with recent approaches, and that our proposed weights improve on
existing schemes.

</details>


### [471] [Investigating Mask-aware Prototype Learning for Tabular Anomaly Detection](https://arxiv.org/abs/2506.02757)
*Ruiying Lu,Jinhan Liu,Chuan Du,Dandan Guo*

Main category: cs.LG

Relevance: 40.0

TL;DR: 论文提出了一种结合掩码建模和原型学习的表格异常检测方法，通过解耦表示学习和全局原型提取提升性能。


<details>
  <summary>Details</summary>
Motivation: 解决现有深度学习方法在表格异常检测中存在的表示纠缠和缺乏全局相关性建模问题。

Method: 1. 编码阶段：在数据空间和投影空间进行掩码建模，学习解耦的正常模式；2. 解码阶段：并行解码多个掩码表示以进行重建，并学习关联原型提取正常特征相关性。

Result: 在20个表格基准测试中验证了模型的有效性和可解释性。

Conclusion: 该方法通过解耦表示和全局原型学习显著提升了表格异常检测性能。

Abstract: Tabular anomaly detection, which aims at identifying deviant samples, has
been crucial in a variety of real-world applications, such as medical disease
identification, financial fraud detection, intrusion monitoring, etc. Although
recent deep learning-based methods have achieved competitive performances,
these methods suffer from representation entanglement and the lack of global
correlation modeling, which hinders anomaly detection performance. To tackle
the problem, we incorporate mask modeling and prototype learning into tabular
anomaly detection. The core idea is to design learnable masks by disentangled
representation learning within a projection space and extracting normal
dependencies as explicit global prototypes. Specifically, the overall model
involves two parts: (i) During encoding, we perform mask modeling in both the
data space and projection space with orthogonal basis vectors for learning
shared disentangled normal patterns; (ii) During decoding, we decode multiple
masked representations in parallel for reconstruction and learn association
prototypes to extract normal characteristic correlations. Our proposal derives
from a distribution-matching perspective, where both projection space learning
and association prototype learning are formulated as optimal transport
problems, and the calibration distances are utilized to refine the anomaly
scores. Quantitative and qualitative experiments on 20 tabular benchmarks
demonstrate the effectiveness and interpretability of our model.

</details>


### [472] [CART-based Synthetic Tabular Data Generation for Imbalanced Regression](https://arxiv.org/abs/2506.02811)
*António Pedro Pinheiro,Rita P. Ribeiro*

Main category: cs.LG

Relevance: 40.0

TL;DR: 提出了一种基于CART的合成数据生成方法，用于解决回归任务中的目标分布不平衡问题，具有高效和透明的特点。


<details>
  <summary>Details</summary>
Motivation: 回归任务中目标分布不平衡会影响模型性能，现有方法（如随机采样和SMOTE）依赖分类技术，存在阈值任意性和计算成本高的问题。

Method: 提出了一种基于CART的合成数据生成方法，结合相关性和密度机制，无需阈值，专注于稀疏区域的数据生成。

Result: 实验表明，该方法在性能上与现有方法相当，但执行更快且更透明。

Conclusion: 该方法为不平衡回归任务提供了一种透明、可扩展的数据级解决方案。

Abstract: Handling imbalanced target distributions in regression tasks remains a
significant challenge in tabular data settings where underrepresented regions
can hinder model performance. Among data-level solutions, some proposals, such
as random sampling and SMOTE-based approaches, propose adapting classification
techniques to regression tasks. However, these methods typically rely on crisp,
artificial thresholds over the target variable, a limitation inherited from
classification settings that can introduce arbitrariness, often leading to
non-intuitive and potentially misleading problem formulations. While recent
generative models, such as GANs and VAEs, provide flexible sample synthesis,
they come with high computational costs and limited interpretability. In this
study, we propose adapting an existing CART-based synthetic data generation
method, tailoring it for imbalanced regression. The new method integrates
relevance and density-based mechanisms to guide sampling in sparse regions of
the target space and employs a threshold-free, feature-driven generation
process. Our experimental study focuses on the prediction of extreme target
values across benchmark datasets. The results indicate that the proposed method
is competitive with other resampling and generative strategies in terms of
performance, while offering faster execution and greater transparency. These
results highlight the method's potential as a transparent, scalable data-level
strategy for improving regression models in imbalanced domains.

</details>


### [473] [Sheaves Reloaded: A Directional Awakening](https://arxiv.org/abs/2506.02842)
*Stefano Fiorini,Hakan Aktas,Iulia Duta,Stefano Coniglio,Pietro Morerio,Alessio Del Bue,Pietro Liò*

Main category: cs.LG

Relevance: 40.0

TL;DR: 论文提出了一种新的Sheaf Neural Network（DSNN），通过引入Directed Cellular Sheaf和Directed Sheaf Laplacian，解决了现有SNN无法处理方向性的问题，并在多个基准测试中表现优异。


<details>
  <summary>Details</summary>
Motivation: 现有Sheaf Neural Networks（SNNs）无法有效建模方向性，而方向性在图形学习任务中至关重要。

Method: 引入Directed Cellular Sheaf和Directed Sheaf Laplacian，构建了第一个具有方向性偏置的SNN模型（DSNN）。

Result: 在九个真实世界基准测试中，DSNN表现优于基线方法。

Conclusion: DSNN通过显式建模方向性，显著提升了复杂关系数据的建模能力。

Abstract: Sheaf Neural Networks (SNNs) represent a powerful generalization of Graph
Neural Networks (GNNs) that significantly improve our ability to model complex
relational data. While directionality has been shown to substantially boost
performance in graph learning tasks and is key to many real-world applications,
existing SNNs fall short in representing it. To address this limitation, we
introduce the Directed Cellular Sheaf, a special type of cellular sheaf
designed to explicitly account for edge orientation. Building on this
structure, we define a new sheaf Laplacian, the Directed Sheaf Laplacian, which
captures both the graph's topology and its directional information. This
operator serves as the backbone of the Directed Sheaf Neural Network (DSNN),
the first SNN model to embed a directional bias into its architecture.
Extensive experiments on nine real-world benchmarks show that DSNN consistently
outperforms baseline methods.

</details>


### [474] [A Continual Offline Reinforcement Learning Benchmark for Navigation Tasks](https://arxiv.org/abs/2506.02883)
*Anthony Kobanda,Odalric-Ambrym Maillard,Rémy Portelas*

Main category: cs.LG

Relevance: 40.0

TL;DR: 论文提出了一个用于持续强化学习的基准测试，专注于视频游戏导航场景，解决灾难性遗忘、任务适应和内存效率等挑战。


<details>
  <summary>Details</summary>
Motivation: 持续强化学习在机器人或视频游戏模拟等领域中需要适应不断变化的任务而不遗忘先前任务，但目前缺乏相关基准测试。

Method: 引入一个包含多种任务和数据集、评估协议和指标的基准测试，用于评估算法性能。

Result: 基准测试旨在促进可重复研究，加速持续强化学习的进展，并为生产流程提供可重复框架。

Conclusion: 该基准填补了文献空白，为持续强化学习的研究和实践提供了重要工具。

Abstract: Autonomous agents operating in domains such as robotics or video game
simulations must adapt to changing tasks without forgetting about the previous
ones. This process called Continual Reinforcement Learning poses non-trivial
difficulties, from preventing catastrophic forgetting to ensuring the
scalability of the approaches considered. Building on recent advances, we
introduce a benchmark providing a suite of video-game navigation scenarios,
thus filling a gap in the literature and capturing key challenges :
catastrophic forgetting, task adaptation, and memory efficiency. We define a
set of various tasks and datasets, evaluation protocols, and metrics to assess
the performance of algorithms, including state-of-the-art baselines. Our
benchmark is designed not only to foster reproducible research and to
accelerate progress in continual reinforcement learning for gaming, but also to
provide a reproducible framework for production pipelines -- helping
practitioners to identify and to apply effective approaches.

</details>


### [475] [Overcoming Challenges of Partial Client Participation in Federated Learning : A Comprehensive Review](https://arxiv.org/abs/2506.02887)
*Mrinmay Sen,Shruti Aparna,Rohit Agarwal,Chalavadi Krishna Mohan*

Main category: cs.LG

Relevance: 40.0

TL;DR: 本文综述了联邦学习中部分客户参与的影响，分析了现有方法及其优缺点。


<details>
  <summary>Details</summary>
Motivation: 现实场景中部分客户参与是常态，但现有研究多假设完全参与，缺乏对部分参与的理论和实践挑战的关注。

Method: 对现有联邦学习方法进行深入综述，结合理论分析和实证结果，分类并评估其优缺点。

Result: 提供了对部分客户参与问题的全面理解，总结了现有方法的适用性和局限性。

Conclusion: 部分客户参与是联邦学习中的重要问题，需进一步研究以优化方法。

Abstract: Federated Learning (FL) is a learning mechanism that falls under the
distributed training umbrella, which collaboratively trains a shared global
model without disclosing the raw data from different clients. This paper
presents an extensive survey on the impact of partial client participation in
federated learning. While much of the existing research focuses on addressing
issues such as generalization, robustness, and fairness caused by data
heterogeneity under the assumption of full client participation, limited
attention has been given to the practical and theoretical challenges arising
from partial client participation, which is common in real-world scenarios.
This survey provides an in-depth review of existing FL methods designed to cope
with partial client participation. We offer a comprehensive analysis supported
by theoretical insights and empirical findings, along with a structured
categorization of these methods, highlighting their respective advantages and
disadvantages.

</details>


### [476] [Sociodynamics-inspired Adaptive Coalition and Client Selection in Federated Learning](https://arxiv.org/abs/2506.02897)
*Alessandro Licciardi,Roberta Raineri,Anton Proskurnikov,Lamberto Rondoni,Lorenzo Zino*

Main category: cs.LG

Relevance: 40.0

TL;DR: 论文提出了一种名为Federated Coalition Variance Reduction with Boltzmann Exploration (FCVR-BE)的算法，通过动态聚类和方差最小化选择客户端，解决联邦学习中数据异构性问题。


<details>
  <summary>Details</summary>
Motivation: 联邦学习（FL）中的客户端数据异构性严重降低了模型性能，作者受社交网络意见动态启发，提出了一种新方法来解决这一问题。

Method: 采用动态聚类（基于渐进一致性）和方差最小化选择客户端（从每个聚类中选择一个客户端），以减少模型更新的方差。

Result: 在异构数据场景下，该算法优于现有FL算法，实现了更高的准确性和更快的收敛速度。

Conclusion: FCVR-BE算法有效解决了FL中的数据异构性问题，提升了模型性能。

Abstract: Federated Learning (FL) enables privacy-preserving collaborative model
training, yet its practical strength is often undermined by client data
heterogeneity, which severely degrades model performance. This paper proposes
that data heterogeneity across clients' distributions can be effectively
addressed by adopting an approach inspired by opinion dynamics over temporal
social networks. We introduce \shortname (Federated Coalition Variance
Reduction with Boltzmann Exploration), a variance-reducing selection algorithm
in which (1) clients dynamically organize into non-overlapping clusters based
on asymptotic agreements, and (2) from each cluster, one client is selected to
minimize the expected variance of its model update. Our experiments show that
in heterogeneous scenarios our algorithm outperforms existing FL algorithms,
yielding more accurate results and faster convergence, validating the efficacy
of our approach.

</details>


### [477] [From Theory to Practice with RAVEN-UCB: Addressing Non-Stationarity in Multi-Armed Bandits through Variance Adaptation](https://arxiv.org/abs/2506.02933)
*Junyi Fang,Yuxun Chen,Yuxin Chen,Chen Zhang*

Main category: cs.LG

Relevance: 40.0

TL;DR: RAVEN-UCB是一种针对非平稳环境的多臂老虎机问题的新算法，通过方差感知适应实现理论和实践的高效性。


<details>
  <summary>Details</summary>
Motivation: 解决非平稳环境中奖励分布动态变化的多臂老虎机问题。

Method: 结合方差驱动探索、自适应控制和常数时间递归更新。

Result: 在理论和实验中优于UCB1和UCB-V，表现出更紧的遗憾界限和鲁棒性。

Conclusion: RAVEN-UCB在非平稳环境中具有理论和实践优势。

Abstract: The Multi-Armed Bandit (MAB) problem is challenging in non-stationary
environments where reward distributions evolve dynamically. We introduce
RAVEN-UCB, a novel algorithm that combines theoretical rigor with practical
efficiency via variance-aware adaptation. It achieves tighter regret bounds
than UCB1 and UCB-V, with gap-dependent regret of order $K \sigma_{\max}^2 \log
T / \Delta$ and gap-independent regret of order $\sqrt{K T \log T}$. RAVEN-UCB
incorporates three innovations: (1) variance-driven exploration using
$\sqrt{\hat{\sigma}_k^2 / (N_k + 1)}$ in confidence bounds, (2) adaptive
control via $\alpha_t = \alpha_0 / \log(t + \epsilon)$, and (3) constant-time
recursive updates for efficiency. Experiments across non-stationary patterns -
distributional changes, periodic shifts, and temporary fluctuations - in
synthetic and logistics scenarios demonstrate its superiority over
state-of-the-art baselines, confirming theoretical and practical robustness.

</details>


### [478] [Implicit Regularization of the Deep Inverse Prior Trained with Inertia](https://arxiv.org/abs/2506.02986)
*Nathan Buskulic,Jalal Fadil,Yvain Quéau*

Main category: cs.LG

Relevance: 40.0

TL;DR: 论文研究了自监督神经网络在逆问题中的收敛性和恢复保证，提出了惯性算法和自适应步长，展示了加速指数收敛率和线性收敛率。


<details>
  <summary>Details</summary>
Motivation: 解决逆问题时神经网络缺乏理论保证的问题，提供自监督神经网络的收敛和恢复保证。

Method: 采用惯性算法（包括粘性和几何Hessian驱动阻尼）和自适应步长，研究连续时间和离散时间情况。

Result: 连续时间情况下获得最优加速指数收敛率，离散时间情况下获得线性收敛率。

Conclusion: 提出的方法在逆问题中提供了理论保证，并展示了优越的收敛性能。

Abstract: Solving inverse problems with neural networks benefits from very few
theoretical guarantees when it comes to the recovery guarantees. We provide in
this work convergence and recovery guarantees for self-supervised neural
networks applied to inverse problems, such as Deep Image/Inverse Prior, and
trained with inertia featuring both viscous and geometric Hessian-driven
dampings. We study both the continuous-time case, i.e., the trajectory of a
dynamical system, and the discrete case leading to an inertial algorithm with
an adaptive step-size. We show in the continuous-time case that the network can
be trained with an optimal accelerated exponential convergence rate compared to
the rate obtained with gradient flow. We also show that training a network with
our inertial algorithm enjoys similar recovery guarantees though with a less
sharp linear convergence rate.

</details>


### [479] [On the Need to Align Intent and Implementation in Uncertainty Quantification for Machine Learning](https://arxiv.org/abs/2506.03037)
*Shubhendu Trivedi,Brian D. Nord*

Main category: cs.LG

Relevance: 40.0

TL;DR: 该立场论文探讨了机器学习中不确定性量化的挑战，包括术语不一致和不同问题背景下的技术需求差异，并提出了促进对齐意图与实现的标准。


<details>
  <summary>Details</summary>
Motivation: 量化机器学习模型的不确定性是现代数据分析的基础挑战，但术语不一致和不同问题背景的技术需求差异加剧了这一挑战。

Method: 通过分析当前不确定性估计目标、不确定性构造及其映射方法，识别问题并提出标准和建议。

Result: 论文强调了不可靠映射的案例，并提出了促进不确定性量化方法可靠性的信任轴。

Conclusion: 建议在科学机器学习中应用这些标准，特别是在基于模拟的推理（SBI）背景下。

Abstract: Quantifying uncertainties for machine learning (ML) models is a foundational
challenge in modern data analysis. This challenge is compounded by at least two
key aspects of the field: (a) inconsistent terminology surrounding uncertainty
and estimation across disciplines, and (b) the varying technical requirements
for establishing trustworthy uncertainties in diverse problem contexts. In this
position paper, we aim to clarify the depth of these challenges by identifying
these inconsistencies and articulating how different contexts impose distinct
epistemic demands. We examine the current landscape of estimation targets
(e.g., prediction, inference, simulation-based inference), uncertainty
constructs (e.g., frequentist, Bayesian, fiducial), and the approaches used to
map between them. Drawing on the literature, we highlight and explain examples
of problematic mappings. To help address these issues, we advocate for
standards that promote alignment between the \textit{intent} and
\textit{implementation} of uncertainty quantification (UQ) approaches. We
discuss several axes of trustworthiness that are necessary (if not sufficient)
for reliable UQ in ML models, and show how these axes can inform the design and
evaluation of uncertainty-aware ML systems. Our practical recommendations focus
on scientific ML, offering illustrative cases and use scenarios, particularly
in the context of simulation-based inference (SBI).

</details>


### [480] [Multi-Metric Adaptive Experimental Design under Fixed Budget with Validation](https://arxiv.org/abs/2506.03062)
*Qining Zhang,Tanner Fiez,Yi Liu,Wenyang Liu*

Main category: cs.LG

Relevance: 40.0

TL;DR: 提出了一种固定预算的多指标自适应实验设计框架（AED），通过两阶段结构（探索和验证）解决多候选和多指标统计推断问题，并提出SHRVar方法，显著提升性能。


<details>
  <summary>Details</summary>
Motivation: 在线实验中，标准A/B测试在多候选和多指标情况下统计效能不足，而传统AED方法难以推断实验统计量（如平均处理效应）。本文旨在解决这一问题。

Method: 提出两阶段框架：自适应探索阶段识别最佳处理，验证阶段通过A/B测试验证质量并推断统计量。SHRVar方法结合相对方差采样和基于奖励z值的淘汰策略。

Result: SHRVar的误差概率呈指数下降，数值实验验证了其优越性能。

Conclusion: 该框架和方法在多指标和多候选场景下显著提升了统计推断能力。

Abstract: Standard A/B tests in online experiments face statistical power challenges
when testing multiple candidates simultaneously, while adaptive experimental
designs (AED) alone fall short in inferring experiment statistics such as the
average treatment effect, especially with many metrics (e.g., revenue, safety)
and heterogeneous variances. This paper proposes a fixed-budget multi-metric
AED framework with a two-phase structure: an adaptive exploration phase to
identify the best treatment, and a validation phase with an A/B test to verify
the treatment's quality and infer statistics. We propose SHRVar, which
generalizes sequential halving (SH) (Karnin et al., 2013) with a novel
relative-variance-based sampling and an elimination strategy built on reward
z-values. It achieves a provable error probability that decreases
exponentially, where the exponent generalizes the complexity measure for SH
(Karnin et al., 2013) and SHVar (Lalitha et al., 2023) with homogeneous and
heterogeneous variances, respectively. Numerical experiments verify our
analysis and demonstrate the superior performance of this new framework.

</details>


### [481] [How Explanations Leak the Decision Logic: Stealing Graph Neural Networks via Explanation Alignment](https://arxiv.org/abs/2506.03087)
*Bin Ma,Yuyuan Feng,Minhua Lin,Enyan Dai*

Main category: cs.LG

Relevance: 40.0

TL;DR: 该论文研究了可解释GNNs的安全风险，提出了一种结合解释对齐和数据增强的模型窃取框架，实验证明其优于传统方法。


<details>
  <summary>Details</summary>
Motivation: 随着GNNs在敏感领域的广泛应用，模型透明性需求增加，但解释机制可能暴露决策逻辑，导致安全风险。

Method: 提出了一种名为{\method}的窃取框架，结合解释对齐和数据增强，以有限查询高效复制目标模型的预测行为和推理模式。

Result: 在分子图数据集上的实验表明，该方法在模型窃取方面优于传统方法。

Conclusion: 研究强调了可解释GNNs在敏感领域部署时的安全风险，并建议采取保护措施防止基于解释的攻击。

Abstract: Graph Neural Networks (GNNs) have become essential tools for analyzing
graph-structured data in domains such as drug discovery and financial analysis,
leading to growing demands for model transparency. Recent advances in
explainable GNNs have addressed this need by revealing important subgraphs that
influence predictions, but these explanation mechanisms may inadvertently
expose models to security risks. This paper investigates how such explanations
potentially leak critical decision logic that can be exploited for model
stealing. We propose {\method}, a novel stealing framework that integrates
explanation alignment for capturing decision logic with guided data
augmentation for efficient training under limited queries, enabling effective
replication of both the predictive behavior and underlying reasoning patterns
of target models. Experiments on molecular graph datasets demonstrate that our
approach shows advantages over conventional methods in model stealing. This
work highlights important security considerations for the deployment of
explainable GNNs in sensitive domains and suggests the need for protective
measures against explanation-based attacks. Our code is available at
https://github.com/beanmah/EGSteal.

</details>


### [482] [Zero-Shot Time Series Forecasting with Covariates via In-Context Learning](https://arxiv.org/abs/2506.03128)
*Andreas Auer,Raghul Parthipan,Pedro Mercado,Abdul Fatir Ansari,Lorenzo Stella,Bernie Wang,Michael Bohlke-Schneider,Syama Sundar Rangapuram*

Main category: cs.LG

Relevance: 40.0

TL;DR: COSMIC是一个利用协变量进行零样本预测的时间序列模型，通过上下文学习和协变量增强技术，无需协变量数据集即可训练，性能领先。


<details>
  <summary>Details</summary>
Motivation: 现有预训练时间序列模型要么不支持协变量，要么未能有效利用协变量，限制了预测性能。

Method: 提出COSMIC模型，采用上下文学习利用协变量，并通过协变量增强技术解决数据稀缺问题。

Result: COSMIC在零样本预测中达到最先进性能，无论是否使用协变量。

Conclusion: COSMIC能有效利用协变量进行零样本预测，解决了现有模型的局限性。

Abstract: Pretrained time series models, capable of zero-shot forecasting, have
demonstrated significant potential in enhancing both the performance and
accessibility of time series forecasting. However, existing pretrained models
either do not support covariates or fail to incorporate them effectively. We
introduce COSMIC, a zero-shot forecasting model that utilizes covariates via
in-context learning. To address the challenge of data scarcity, we propose
Informative Covariate Augmentation, which enables the training of COSMIC
without requiring any datasets that include covariates. COSMIC achieves
state-of-the-art performance in zero-shot forecasting, both with and without
covariates. Our quantitative and qualitative analysis demonstrates that COSMIC
effectively leverages covariates in zero-shot forecasting.

</details>


### [483] [DistMLIP: A Distributed Inference Platform for Machine Learning Interatomic Potentials](https://arxiv.org/abs/2506.02023)
*Kevin Han,Bowen Deng,Amir Barati Farimani,Gerbrand Ceder*

Main category: cs.DC

Relevance: 40.0

TL;DR: DistMLIP是一个高效的分布式推理平台，用于机器学习原子间势（MLIPs），通过图级并行化实现多设备推理。


<details>
  <summary>Details</summary>
Motivation: 大规模原子模拟在计算材料和化学与真实材料和药物发现应用之间架起桥梁，而MLIPs的并行化是扩展模拟规模的关键。

Method: DistMLIP采用零冗余的图级并行化方法，支持灵活的MLIP架构（如图神经网络），并提供易用的插件接口。

Result: 在8个GPU上，DistMLIP实现了近百万原子计算，耗时仅几秒。

Conclusion: DistMLIP为MLIPs的分布式推理提供了高效、灵活的解决方案。

Abstract: Large-scale atomistic simulations are essential to bridge computational
materials and chemistry to realistic materials and drug discovery applications.
In the past few years, rapid developments of machine learning interatomic
potentials (MLIPs) have offered a solution to scale up quantum mechanical
calculations. Parallelizing these interatomic potentials across multiple
devices poses a challenging, but promising approach to further extending
simulation scales to real-world applications. In this work, we present
DistMLIP, an efficient distributed inference platform for MLIPs based on
zero-redundancy, graph-level parallelization. In contrast to conventional
space-partitioning parallelization, DistMLIP enables efficient MLIP
parallelization through graph partitioning, allowing multi-device inference on
flexible MLIP model architectures like multi-layer graph neural networks.
DistMLIP presents an easy-to-use, flexible, plug-in interface that enables
distributed inference of pre-existing MLIPs. We demonstrate DistMLIP on four
widely used and state-of-the-art MLIPs: CHGNet, MACE, TensorNet, and eSEN. We
show that existing foundational potentials can perform near-million-atom
calculations at the scale of a few seconds on 8 GPUs with DistMLIP.

</details>


### [484] [A Brain Graph Foundation Model: Pre-Training and Prompt-Tuning for Any Atlas and Disorder](https://arxiv.org/abs/2506.02044)
*Xinxu Wei,Kanhao Zhao,Yong Jiao,Lifang He,Yu Zhang*

Main category: q-bio.NC

Relevance: 40.0

TL;DR: 提出了一种基于图的新型预训练范式BrainGFM，用于构建大脑图基础模型，结合图对比学习和图掩码自编码器，支持高效下游迁移。


<details>
  <summary>Details</summary>
Motivation: 现有大脑基础模型多基于时间序列或ROI特征，而图结构能更好地捕捉大脑功能连接，因此提出图预训练范式以提升泛化能力。

Method: 采用图对比学习和图掩码自编码器进行预训练，结合图提示和语言提示设计，并通过元学习优化提示以实现少样本和零样本学习。

Result: 在27个神经影像数据集上预训练，涵盖25种神经和精神疾病，支持跨异构脑图谱和任务的灵活迁移。

Conclusion: BrainGFM为大脑图基础模型提供了统一框架，显著扩展了预训练数据范围并提升了泛化能力。

Abstract: As large language models (LLMs) continue to revolutionize AI research, there
is a growing interest in building large-scale brain foundation models to
advance neuroscience. While most existing brain foundation models are
pre-trained on time-series signals or region-of-interest (ROI) features, we
propose a novel graph-based pre-training paradigm for constructing a brain
graph foundation model. In this paper, we introduce the Brain Graph Foundation
Model, termed BrainGFM, a unified framework that leverages graph contrastive
learning and graph masked autoencoders for large-scale fMRI-based pre-training.
BrainGFM is pre-trained on a diverse mixture of brain atlases with varying
parcellations, significantly expanding the pre-training corpus and enhancing
the model's ability to generalize across heterogeneous fMRI-derived brain
representations. To support efficient and versatile downstream transfer, we
integrate both graph prompts and language prompts into the model design,
enabling BrainGFM to flexibly adapt to a wide range of atlases, neurological
and psychiatric disorders, and task settings. Furthermore, we employ
meta-learning to optimize the graph prompts, facilitating strong generalization
to previously unseen disorders under both few-shot and zero-shot learning
conditions via language-guided prompting. BrainGFM is pre-trained on 27
neuroimaging datasets spanning 25 common neurological and psychiatric
disorders, encompassing 2 types of brain atlases (functional and anatomical)
across 8 widely-used parcellations, and covering over 25,000 subjects, 60,000
fMRI scans, and a total of 400,000 graph samples aggregated across all atlases
and parcellations. The code is available at:
https://github.com/weixinxu666/BrainGFM

</details>


### [485] [Enhancing Interpretability of Quantum-Assisted Blockchain Clustering via AI Agent-Based Qualitative Analysis](https://arxiv.org/abs/2506.02068)
*Yun-Cheng Tsai,Yen-Ku Liu,Samuel Yen-Chi Chen*

Main category: quant-ph

Relevance: 40.0

TL;DR: 论文提出了一种结合定量聚类评估与AI代理辅助定性解释的两阶段分析框架，以提升量子增强聚类模型在区块链数据分析中的可解释性。


<details>
  <summary>Details</summary>
Motivation: 区块链交易数据高维、噪声多且复杂，传统聚类算法难以处理，量子增强聚类模型虽性能优越但可解释性不足，限制了其在敏感领域的应用。

Method: 第一阶段使用经典聚类方法和评估指标确定最佳聚类数量；第二阶段引入AI代理生成语义解释，分析聚类结果。

Result: 实验表明，量子神经网络（QNN）在定量指标上优于随机量子特征（QF），AI代理进一步揭示了QNN模型的单例聚类现象。

Conclusion: 该工作提升了量子辅助区块链分析的可解释性，为未来自主AI驱动的聚类框架奠定了基础。

Abstract: Blockchain transaction data is inherently high dimensional, noisy, and
entangled, posing substantial challenges for traditional clustering algorithms.
While quantum enhanced clustering models have demonstrated promising
performance gains, their interpretability remains limited, restricting their
application in sensitive domains such as financial fraud detection and
blockchain governance. To address this gap, we propose a two stage analysis
framework that synergistically combines quantitative clustering evaluation with
AI Agent assisted qualitative interpretation. In the first stage, we employ
classical clustering methods and evaluation metrics including the Silhouette
Score, Davies Bouldin Index, and Calinski Harabasz Index to determine the
optimal cluster count and baseline partition quality. In the second stage, we
integrate an AI Agent to generate human readable, semantic explanations of
clustering results, identifying intra cluster characteristics and inter cluster
relationships. Our experiments reveal that while fully trained Quantum Neural
Networks (QNN) outperform random Quantum Features (QF) in quantitative metrics,
the AI Agent further uncovers nuanced differences between these methods,
notably exposing the singleton cluster phenomenon in QNN driven models. The
consolidated insights from both stages consistently endorse the three cluster
configuration, demonstrating the practical value of our hybrid approach. This
work advances the interpretability frontier in quantum assisted blockchain
analytics and lays the groundwork for future autonomous AI orchestrated
clustering frameworks.

</details>


### [486] [Memorization to Generalization: Emergence of Diffusion Models from Associative Memory](https://arxiv.org/abs/2505.21777)
*Bao Pham,Gabriel Raya,Matteo Negri,Mohammed J. Zaki,Luca Ambrogioni,Dmitry Krotov*

Main category: cs.LG

Relevance: 40.0

TL;DR: 论文从联想记忆（AM）角度分析扩散模型，探讨其在训练和生成阶段的行为，揭示了记忆-泛化现象，并预测了虚假状态的存在。


<details>
  <summary>Details</summary>
Motivation: 研究扩散模型在联想记忆框架下的行为，揭示其记忆和泛化机制，并探索虚假状态的出现。

Method: 将扩散模型的训练和生成阶段分别类比为记忆编码和检索，分析不同数据规模下的行为。

Result: 在小数据规模下，扩散模型表现出强记忆行为；在大数据规模下，出现新的吸引状态和虚假状态。

Conclusion: 研究为扩散模型的记忆-泛化现象提供了新视角，并验证了虚假状态的存在。

Abstract: Hopfield networks are associative memory (AM) systems, designed for storing
and retrieving patterns as local minima of an energy landscape. In the
classical Hopfield model, an interesting phenomenon occurs when the amount of
training data reaches its critical memory load $- spurious\,\,states$, or
unintended stable points, emerge at the end of the retrieval dynamics, leading
to incorrect recall. In this work, we examine diffusion models, commonly used
in generative modeling, from the perspective of AMs. The training phase of
diffusion model is conceptualized as memory encoding (training data is stored
in the memory). The generation phase is viewed as an attempt of memory
retrieval. In the small data regime the diffusion model exhibits a strong
memorization phase, where the network creates distinct basins of attraction
around each sample in the training set, akin to the Hopfield model below the
critical memory load. In the large data regime, a different phase appears where
an increase in the size of the training set fosters the creation of new
attractor states that correspond to manifolds of the generated samples.
Spurious states appear at the boundary of this transition and correspond to
emergent attractor states, which are absent in the training set, but, at the
same time, have distinct basins of attraction around them. Our findings
provide: a novel perspective on the memorization-generalization phenomenon in
diffusion models via the lens of AMs, theoretical prediction of existence of
spurious states, empirical validation of this prediction in commonly-used
diffusion models.

</details>


### [487] [Tomographic Foundation Model -- FORCE: Flow-Oriented Reconstruction Conditioning Engine](https://arxiv.org/abs/2506.02149)
*Wenjun Xia,Chuang Niu,Ge Wang*

Main category: eess.IV

Relevance: 40.0

TL;DR: 论文提出了一种基于生成AI模型PFGM++的新型CT图像重建框架FORCE，解决了传统深度学习方法在数据不一致和模型不稳定性下的幻觉风险问题。


<details>
  <summary>Details</summary>
Motivation: 临床CT场景中，低剂量扫描、稀疏视图扫描和金属植入物等因素导致重建图像噪声和伪影严重，需要改进重建技术。尽管深度学习已显著推进CT图像重建，但获取配对训练数据仍具挑战性，且现有方法存在幻觉风险。

Method: 结合数据保真度和生成AI模型PFGM++，提出新型CT框架FORCE。

Result: 实验表明，FORCE在多种CT成像任务中表现优于现有无监督重建方法。

Conclusion: FORCE通过整合生成AI模型，有效提升了CT图像重建的稳定性和性能。

Abstract: Computed tomography (CT) is a major medical imaging modality. Clinical CT
scenarios, such as low-dose screening, sparse-view scanning, and metal
implants, often lead to severe noise and artifacts in reconstructed images,
requiring improved reconstruction techniques. The introduction of deep learning
has significantly advanced CT image reconstruction. However, obtaining paired
training data remains rather challenging due to patient motion and other
constraints. Although deep learning methods can still perform well with
approximately paired data, they inherently carry the risk of hallucination due
to data inconsistencies and model instability. In this paper, we integrate the
data fidelity with the state-of-the-art generative AI model, referred to as the
Poisson flow generative model (PFGM) with a generalized version PFGM++, and
propose a novel CT framework: Flow-Oriented Reconstruction Conditioning Engine
(FORCE). In our experiments, the proposed method shows superior performance in
various CT imaging tasks, outperforming existing unsupervised reconstruction
approaches.

</details>


### [488] [Enabling Probabilistic Learning on Manifolds through Double Diffusion Maps](https://arxiv.org/abs/2506.02254)
*Dimitris G Giovanis,Nikolaos Evangelou,Ioannis G Kevrekidis,Roger G Ghanem*

Main category: stat.ML

Relevance: 40.0

TL;DR: 提出了一种基于概率学习流形（PLoM）扩展的生成学习框架，用于概率采样，解决了数据点少时扩散图基维度接近数据点数导致的过拟合问题。


<details>
  <summary>Details</summary>
Motivation: 解决PLoM方法在数据点少时因扩散图基维度接近数据点数而导致的过拟合和泛化能力下降问题。

Method: 结合双扩散图（捕捉多尺度几何特征）与几何谐波（高维空间非线性插值），直接在潜在空间中求解完整阶ISDE。

Result: 通过二维Hermite多项式函数和反应流中爆轰波的高保真模拟验证了方法的有效性和鲁棒性。

Conclusion: 提出的方法在数据点少时仍能保持系统的完整动态复杂性，同时利用其降维几何表示。

Abstract: We present a generative learning framework for probabilistic sampling based
on an extension of the Probabilistic Learning on Manifolds (PLoM) approach,
which is designed to generate statistically consistent realizations of a random
vector in a finite-dimensional Euclidean space, informed by a limited (yet
representative) set of observations. In its original form, PLoM constructs a
reduced-order probabilistic model by combining three main components: (a)
kernel density estimation to approximate the underlying probability measure,
(b) Diffusion Maps to uncover the intrinsic low-dimensional manifold structure,
and (c) a reduced-order Ito Stochastic Differential Equation (ISDE) to sample
from the learned distribution. A key challenge arises, however, when the number
of available data points N is small and the dimensionality of the diffusion-map
basis approaches N, resulting in overfitting and loss of generalization. To
overcome this limitation, we propose an enabling extension that implements a
synthesis of Double Diffusion Maps -- a technique capable of capturing
multiscale geometric features of the data -- with Geometric Harmonics (GH), a
nonparametric reconstruction method that allows smooth nonlinear interpolation
in high-dimensional ambient spaces. This approach enables us to solve a
full-order ISDE directly in the latent space, preserving the full dynamical
complexity of the system, while leveraging its reduced geometric
representation. The effectiveness and robustness of the proposed method are
illustrated through two numerical studies: one based on data generated from
two-dimensional Hermite polynomial functions and another based on high-fidelity
simulations of a detonation wave in a reactive flow.

</details>


### [489] [Assumption-free stability for ranking problems](https://arxiv.org/abs/2506.02257)
*Ruiting Liang,Jake A. Soloff,Rina Foygel Barber,Rebecca Willett*

Main category: stat.ML

Relevance: 40.0

TL;DR: 本文提出了一种新的算法稳定性框架，用于解决排名问题中的不稳定性问题，并提出了两种新的排名操作符：膨胀top-k和膨胀全排名。


<details>
  <summary>Details</summary>
Motivation: 现实世界中的排名问题常因数据噪声导致结果不稳定，尤其是当项目得分相近时，现有理论假设的分离条件难以适用。

Method: 开发了一种算法稳定性框架，提出了膨胀top-k和膨胀全排名两种方法，允许输出表达不确定性。

Result: 实验证明，所提方法在不牺牲信息量的前提下实现了稳定性，且不依赖于数据分布或候选数量。

Conclusion: 新方法为排名问题提供了稳定的解决方案，适用于实际应用场景。

Abstract: In this work, we consider ranking problems among a finite set of candidates:
for instance, selecting the top-$k$ items among a larger list of candidates or
obtaining the full ranking of all items in the set. These problems are often
unstable, in the sense that estimating a ranking from noisy data can exhibit
high sensitivity to small perturbations. Concretely, if we use data to provide
a score for each item (say, by aggregating preference data over a sample of
users), then for two items with similar scores, small fluctuations in the data
can alter the relative ranking of those items. Many existing theoretical
results for ranking problems assume a separation condition to avoid this
challenge, but real-world data often contains items whose scores are
approximately tied, limiting the applicability of existing theory. To address
this gap, we develop a new algorithmic stability framework for ranking
problems, and propose two novel ranking operators for achieving stable ranking:
the \emph{inflated top-$k$} for the top-$k$ selection problem and the
\emph{inflated full ranking} for ranking the full list. To enable stability,
each method allows for expressing some uncertainty in the output. For both of
these two problems, our proposed methods provide guaranteed stability, with no
assumptions on data distributions and no dependence on the total number of
candidates to be ranked. Experiments on real-world data confirm that the
proposed methods offer stability without compromising the informativeness of
the output.

</details>


### [490] [MoCA: Multi-modal Cross-masked Autoencoder for Digital Health Measurements](https://arxiv.org/abs/2506.02260)
*Howon Ryu,Yuliang Chen,Yacun Wang,Andrea Z. LaCroix,Chongzhi Di,Loki Natarajan,Yu Wang,Jingjing Zou*

Main category: stat.ML

Relevance: 40.0

TL;DR: 提出了一种名为MoCA的自监督学习框架，用于处理多模态健康数据，利用跨模态掩码和Transformer架构，优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 解决数字健康领域中多模态数据标注成本高的问题，提出自监督学习方法。

Method: 采用跨模态掩码和Transformer自编码器架构，利用模态内和模态间的相关性。

Result: 在重建和下游任务中表现优于现有方法，并提供了理论支持。

Conclusion: MoCA展示了自监督学习在多模态健康数据中的潜力。

Abstract: The growing prevalence of digital health technologies has led to the
generation of complex multi-modal data, such as physical activity measurements
simultaneously collected from various sensors of mobile and wearable devices.
These data hold immense potential for advancing health studies, but current
methods predominantly rely on supervised learning, requiring extensive labeled
datasets that are often expensive or impractical to obtain, especially in
clinical studies. To address this limitation, we propose a self-supervised
learning framework called Multi-modal Cross-masked Autoencoder (MoCA) that
leverages cross-modality masking and the Transformer autoencoder architecture
to utilize both temporal correlations within modalities and cross-modal
correlations between data streams. We also provide theoretical guarantees to
support the effectiveness of the cross-modality masking scheme in MoCA.
Comprehensive experiments and ablation studies demonstrate that our method
outperforms existing approaches in both reconstruction and downstream tasks. We
release open-source code for data processing, pre-training, and downstream
tasks in the supplementary materials. This work highlights the transformative
potential of self-supervised learning in digital health and multi-modal data.

</details>


### [491] [Joint Modeling for Learning Decision-Making Dynamics in Behavioral Experiments](https://arxiv.org/abs/2506.02394)
*Yuan Bian,Xingche Guo,Yuanjia Wang*

Main category: stat.ME

Relevance: 40.0

TL;DR: 本文提出了一种结合强化学习（RL）和漂移扩散模型（DDM）的新框架，用于分析奖励决策和反应时间，并通过隐马尔可夫模型（HMM）建模策略切换。该方法在数值研究中表现优越，并在抑郁症研究中揭示了患者与健康对照组的行为差异。


<details>
  <summary>Details</summary>
Motivation: 抑郁症（MDD）与奖励处理异常和注意力问题相关，需要一种能够同时分析奖励决策和反应时间的框架。

Method: 结合RL和DDM，使用HMM建模策略切换，采用广义期望最大化算法实现。

Result: 方法在数值研究中优于其他方法，应用于抑郁症研究时发现患者整体参与度较低且决策时间更长。

Conclusion: 框架有效揭示了抑郁症患者的行为特征，并提供了与"engaged"状态相关的脑行为证据。

Abstract: Major depressive disorder (MDD), a leading cause of disability and mortality,
is associated with reward-processing abnormalities and concentration issues.
Motivated by the probabilistic reward task from the Establishing Moderators and
Biosignatures of Antidepressant Response in Clinical Care (EMBARC) study, we
propose a novel framework that integrates the reinforcement learning (RL) model
and drift-diffusion model (DDM) to jointly analyze reward-based decision-making
with response times. To account for emerging evidence suggesting that
decision-making may alternate between multiple interleaved strategies, we model
latent state switching using a hidden Markov model (HMM). In the ''engaged''
state, decisions follow an RL-DDM, simultaneously capturing reward processing,
decision dynamics, and temporal structure. In contrast, in the ''lapsed''
state, decision-making is modeled using a simplified DDM, where specific
parameters are fixed to approximate random guessing with equal probability. The
proposed method is implemented using a computationally efficient generalized
expectation-maximization algorithm with forward-backward procedures. Through
extensive numerical studies, we demonstrate that our proposed method
outperforms competing approaches under various reward-generating distributions,
both with and without strategy switching. When applied to the EMBARC study, our
framework reveals that MDD patients exhibit lower overall engagement than
healthy controls and experience longer decision times when they do engage.
Additionally, we show that neuroimaging measures of brain activities are
associated with decision-making characteristics in the ''engaged'' state but
not in the ''lapsed'' state, providing evidence of brain-behavioral association
specific to the ''engaged'' state.

</details>


### [492] [Enhancing Convergence, Privacy and Fairness for Wireless Personalized Federated Learning: Quantization-Assisted Min-Max Fair Scheduling](https://arxiv.org/abs/2506.02422)
*Xiyu Zhao,Qimei Cui,Ziqiang Du,Weicai Li,Xi Yu,Wei Ni,Ji Zhang,Xiaofeng Tao,Ping Zhang*

Main category: cs.DC

Relevance: 40.0

TL;DR: 本文提出了一种基于量化误差和差分隐私的无线个性化联邦学习（WPFL）方法，通过优化传输调度策略实现性能公平性。


<details>
  <summary>Details</summary>
Motivation: 解决无线个性化联邦学习中的隐私问题和性能公平性挑战。

Method: 利用量化误差增强隐私，设计最优传输调度策略，包括客户端选择、信道分配和功率控制。

Result: 实验表明，该方法在准确性、最大测试损失和公平性上显著优于其他策略。

Conclusion: 提出的方法有效提升了WPFL的隐私保护和性能公平性。

Abstract: Personalized federated learning (PFL) offers a solution to balancing
personalization and generalization by conducting federated learning (FL) to
guide personalized learning (PL). Little attention has been given to wireless
PFL (WPFL), where privacy concerns arise. Performance fairness of PL models is
another challenge resulting from communication bottlenecks in WPFL. This paper
exploits quantization errors to enhance the privacy of WPFL and proposes a
novel quantization-assisted Gaussian differential privacy (DP) mechanism. We
analyze the convergence upper bounds of individual PL models by considering the
impact of the mechanism (i.e., quantization errors and Gaussian DP noises) and
imperfect communication channels on the FL of WPFL. By minimizing the maximum
of the bounds, we design an optimal transmission scheduling strategy that
yields min-max fairness for WPFL with OFDMA interfaces. This is achieved by
revealing the nested structure of this problem to decouple it into subproblems
solved sequentially for the client selection, channel allocation, and power
control, and for the learning rates and PL-FL weighting coefficients.
Experiments validate our analysis and demonstrate that our approach
substantially outperforms alternative scheduling strategies by 87.08%, 16.21%,
and 38.37% in accuracy, the maximum test loss of participating clients, and
fairness (Jain's index), respectively.

</details>


### [493] [A Novel Deep Reinforcement Learning Method for Computation Offloading in Multi-User Mobile Edge Computing with Decentralization](https://arxiv.org/abs/2506.02458)
*Nguyen Chi Long,Trinh Van Chien,Ta Hai Tung,Van Son Nguyen,Trong-Minh Hoang,Nguyen Ngoc Hai Dang*

Main category: cs.IT

Relevance: 40.0

TL;DR: 该论文研究了在移动边缘计算（MEC）系统中使用深度强化学习（DRL）算法，提出了一种基于Twin Delayed DDPG的新方法，以解决传统DDPG算法的局限性。


<details>
  <summary>Details</summary>
Motivation: 研究如何在MEC系统中实现高效的去中心化动态计算卸载策略，以解决传统方法的不足。

Method: 采用Twin Delayed DDPG算法，独立为每个用户学习计算卸载策略。

Result: 数值结果表明，用户能自主学习有效策略，且新方法性能优于传统DDPG策略。

Conclusion: 提出的新方法在MEC系统中表现更优，解决了传统算法的弱点。

Abstract: Mobile edge computing (MEC) allows appliances to offload workloads to
neighboring MEC servers that have the potential for computation-intensive tasks
with limited computational capabilities. This paper studied how deep
reinforcement learning (DRL) algorithms are used in an MEC system to find
feasible decentralized dynamic computation offloading strategies, which leads
to the construction of an extensible MEC system that operates effectively with
finite feedback. Even though the Deep Deterministic Policy Gradient (DDPG)
algorithm, subject to their knowledge of the MEC system, can be used to
allocate powers of both computation offloading and local execution, to learn a
computation offloading policy for each user independently, we realized that
this solution still has some inherent weaknesses. Hence, we introduced a new
approach for this problem based on the Twin Delayed DDPG algorithm, which
enables us to overcome this proneness and investigate cases where mobile users
are portable. Numerical results showed that individual users can autonomously
learn adequate policies through the proposed approach. Besides, the performance
of the suggested solution exceeded the conventional DDPG-based power control
strategy.

</details>


### [494] [Maximizing the Promptness of Metaverse Systems using Edge Computing by Deep Reinforcement Learning](https://arxiv.org/abs/2506.02657)
*Tam Ninh Thi-Thanh,Trinh Van Chien,Hung Tran,Nguyen Hoai Son,Van Nhan Vo*

Main category: cs.IT

Relevance: 40.0

TL;DR: 本文探讨了深度强化学习（DRL）在支持基于Metaverse的数字孪生（DT）系统中的应用，展示了其在动态环境中任务卸载的有效性。


<details>
  <summary>Details</summary>
Motivation: 研究旨在利用DRL优化Metaverse系统中的数字孪生，特别是在动态环境中处理数据卸载任务，以提高系统的实时性。

Method: 提出了一种DRL算法，用于在包含用户设备、虚拟接入点和边缘计算服务器的系统中动态卸载任务。

Result: 实验结果表明，所提出的DRL算法在动态环境中能有效确保数字孪生的实时性。

Conclusion: DRL在Metaverse数字孪生系统中具有潜力，特别是在动态任务卸载场景中。

Abstract: Metaverse and Digital Twin (DT) have attracted much academic and industrial
attraction to approach the future digital world. This paper introduces the
advantages of deep reinforcement learning (DRL) in assisting Metaverse
system-based Digital Twin. In this system, we assume that it includes several
Metaverse User devices collecting data from the real world to transfer it into
the virtual world, a Metaverse Virtual Access Point (MVAP) undertaking the
processing of data, and an edge computing server that receives the offloading
data from the MVAP. The proposed model works under a dynamic environment with
various parameters changing over time. The experiment results show that our
proposed DRL algorithm is suitable for offloading tasks to ensure the
promptness of DT in a dynamic environment.

</details>


### [495] [Computational Thresholds in Multi-Modal Learning via the Spiked Matrix-Tensor Model](https://arxiv.org/abs/2506.02664)
*Hugo Tabanelli,Pierre Mergny,Lenka Zdeborova,Florent Krzakala*

Main category: stat.ML

Relevance: 40.0

TL;DR: 论文研究了从两种噪声相关模态（尖峰矩阵和尖峰张量）中恢复高维信号的问题，揭示了模态间复杂交互及算法行为。通过贝叶斯近似消息传递实现高效恢复，而联合学习则表现不佳。提出了一种简单的顺序课程学习策略，实现了最优恢复。


<details>
  <summary>Details</summary>
Motivation: 探索高维信号在多模态相关噪声下的恢复问题，揭示模态间交互及算法行为，为多模态学习提供理论支持。

Method: 采用贝叶斯近似消息传递方法，并提出了顺序课程学习策略（先恢复矩阵，再引导张量恢复）。

Result: 顺序课程学习策略实现了最优弱恢复阈值，而联合学习表现不佳。

Conclusion: 结构相关性和学习顺序在多模态高维推理中至关重要。

Abstract: We study the recovery of multiple high-dimensional signals from two noisy,
correlated modalities: a spiked matrix and a spiked tensor sharing a common
low-rank structure. This setting generalizes classical spiked matrix and tensor
models, unveiling intricate interactions between inference channels and
surprising algorithmic behaviors. Notably, while the spiked tensor model is
typically intractable at low signal-to-noise ratios, its correlation with the
matrix enables efficient recovery via Bayesian Approximate Message Passing,
inducing staircase-like phase transitions reminiscent of neural network
phenomena. In contrast, empirical risk minimization for joint learning fails:
the tensor component obstructs effective matrix recovery, and joint
optimization significantly degrades performance, highlighting the limitations
of naive multi-modal learning. We show that a simple Sequential Curriculum
Learning strategy-first recovering the matrix, then leveraging it to guide
tensor recovery-resolves this bottleneck and achieves optimal weak recovery
thresholds. This strategy, implementable with spectral methods, emphasizes the
critical role of structural correlation and learning order in multi-modal
high-dimensional inference.

</details>


### [496] [Symmetry-Aware GFlowNets](https://arxiv.org/abs/2506.02685)
*Hohyun Kim,Seunggeun Lee,Min-hwan Oh*

Main category: stat.ML

Relevance: 40.0

TL;DR: SA-GFN通过奖励缩放引入对称性校正，解决了GFlowNets在图采样中的系统性偏差问题，无需显式计算状态转移概率。


<details>
  <summary>Details</summary>
Motivation: 现有GFlowNets方法因状态转移概率计算不准确而产生系统性偏差，影响图采样的多样性。

Method: 提出SA-GFN，将对称性校正直接融入奖励结构，避免显式计算状态转移概率。

Result: SA-GFN实现了无偏采样，提高了多样性，并能稳定生成高奖励图，与目标分布匹配。

Conclusion: SA-GFN通过奖励结构调整有效解决了GFlowNets的偏差问题，提升了采样质量。

Abstract: Generative Flow Networks (GFlowNets) offer a powerful framework for sampling
graphs in proportion to their rewards. However, existing approaches suffer from
systematic biases due to inaccuracies in state transition probability
computations. These biases, rooted in the inherent symmetries of graphs, impact
both atom-based and fragment-based generation schemes. To address this
challenge, we introduce Symmetry-Aware GFlowNets (SA-GFN), a method that
incorporates symmetry corrections into the learning process through reward
scaling. By integrating bias correction directly into the reward structure,
SA-GFN eliminates the need for explicit state transition computations.
Empirical results show that SA-GFN enables unbiased sampling while enhancing
diversity and consistently generating high-reward graphs that closely match the
target distribution.

</details>


### [497] [Safely Learning Controlled Stochastic Dynamics](https://arxiv.org/abs/2506.02754)
*Luc Brogat-Motte,Alessandro Rudi,Riccardo Bonalli*

Main category: stat.ML

Relevance: 40.0

TL;DR: 提出一种方法，通过基于核的置信边界迭代扩展初始安全控制集，确保在训练和部署期间系统轨迹保持在预定义安全区域内。


<details>
  <summary>Details</summary>
Motivation: 解决从离散时间轨迹观测中安全学习控制随机动力学的问题，确保系统轨迹在训练和部署期间始终处于安全区域，适用于自主机器人、金融和生物医学等安全关键应用。

Method: 使用基于核的置信边界迭代扩展初始安全控制集，仅需温和的平滑性假设和初始安全控制集。

Result: 实验证明该方法在安全性、估计准确性和计算效率方面具有实际有效性。

Conclusion: 该方法为复杂现实世界系统提供了广泛适用性，并提供了安全性和自适应学习率的理论保证。

Abstract: We address the problem of safely learning controlled stochastic dynamics from
discrete-time trajectory observations, ensuring system trajectories remain
within predefined safe regions during both training and deployment.
Safety-critical constraints of this kind are crucial in applications such as
autonomous robotics, finance, and biomedicine. We introduce a method that
ensures safe exploration and efficient estimation of system dynamics by
iteratively expanding an initial known safe control set using kernel-based
confidence bounds. After training, the learned model enables predictions of the
system's dynamics and permits safety verification of any given control. Our
approach requires only mild smoothness assumptions and access to an initial
safe control set, enabling broad applicability to complex real-world systems.
We provide theoretical guarantees for safety and derive adaptive learning rates
that improve with increasing Sobolev regularity of the true dynamics.
Experimental evaluations demonstrate the practical effectiveness of our method
in terms of safety, estimation accuracy, and computational efficiency.

</details>


### [498] [Doubly-Robust Estimation of Counterfactual Policy Mean Embeddings](https://arxiv.org/abs/2506.02793)
*Houssam Zenati,Bariscan Bozkurt,Arthur Gretton*

Main category: stat.ML

Relevance: 40.0

TL;DR: 论文提出了一种名为CPME的新框架，用于在RKHS中表示反事实结果分布，支持灵活的非参数分布外策略评估，并引入了双重稳健估计器和核测试统计量。


<details>
  <summary>Details</summary>
Motivation: 在推荐、广告和医疗等领域，估计反事实政策下的结果分布对决策至关重要。现有方法在灵活性和非参数评估方面存在局限。

Method: 提出了CPME框架，包括插件估计器和双重稳健估计器，后者通过校正结果嵌入和倾向模型的偏差提高了收敛速度。还开发了双重稳健核测试统计量。

Result: CPME在数值模拟中优于现有方法，支持从反事实分布中采样，并实现了渐近正态性，便于高效计算和置信区间构建。

Conclusion: CPME为反事实政策评估提供了灵活且高效的工具，适用于复杂决策场景。

Abstract: Estimating the distribution of outcomes under counterfactual policies is
critical for decision-making in domains such as recommendation, advertising,
and healthcare. We analyze a novel framework-Counterfactual Policy Mean
Embedding (CPME)-that represents the entire counterfactual outcome distribution
in a reproducing kernel Hilbert space (RKHS), enabling flexible and
nonparametric distributional off-policy evaluation. We introduce both a plug-in
estimator and a doubly robust estimator; the latter enjoys improved uniform
convergence rates by correcting for bias in both the outcome embedding and
propensity models. Building on this, we develop a doubly robust kernel test
statistic for hypothesis testing, which achieves asymptotic normality and thus
enables computationally efficient testing and straightforward construction of
confidence intervals. Our framework also supports sampling from the
counterfactual distribution. Numerical simulations illustrate the practical
benefits of CPME over existing methods.

</details>


### [499] [Ensemble-MIX: Enhancing Sample Efficiency in Multi-Agent RL Using Ensemble Methods](https://arxiv.org/abs/2506.02841)
*Tom Danino,Nahum Shimkin*

Main category: eess.SY

Relevance: 40.0

TL;DR: 提出了一种结合分解集中式评论家和分散式集成学习的多智能体强化学习算法，通过选择性探索和多样性正则化提升性能。


<details>
  <summary>Details</summary>
Motivation: 多智能体强化学习（MARL）算法通常需要比单智能体更多的环境交互才能收敛，且存在探索难度和高方差问题。

Method: 结合分解集中式评论家和分散式集成学习，利用集成峰度指导探索，采用截断TD(λ)算法提升样本效率，并混合策略和非策略损失训练执行器。

Result: 在标准MARL基准测试（如SMAC II地图）中优于现有最优方法。

Conclusion: 该方法通过选择性探索和混合训练策略，显著提升了MARL的性能和样本效率。

Abstract: Multi-agent reinforcement learning (MARL) methods have achieved
state-of-the-art results on a range of multi-agent tasks. Yet, MARL algorithms
typically require significantly more environment interactions than their
single-agent counterparts to converge, a problem exacerbated by the difficulty
in exploring over a large joint action space and the high variance intrinsic to
MARL environments. To tackle these issues, we propose a novel algorithm that
combines a decomposed centralized critic with decentralized ensemble learning,
incorporating several key contributions. The main component in our scheme is a
selective exploration method that leverages ensemble kurtosis. We extend the
global decomposed critic with a diversity-regularized ensemble of individual
critics and utilize its excess kurtosis to guide exploration toward
high-uncertainty states and actions. To improve sample efficiency, we train the
centralized critic with a novel truncated variation of the TD($\lambda$)
algorithm, enabling efficient off-policy learning with reduced variance. On the
actor side, our suggested algorithm adapts the mixed samples approach to MARL,
mixing on-policy and off-policy loss functions for training the actors. This
approach balances between stability and efficiency and outperforms purely
off-policy learning. The evaluation shows our method outperforms
state-of-the-art baselines on standard MARL benchmarks, including a variety of
SMAC II maps.

</details>


### [500] [Learned Controllers for Agile Quadrotors in Pursuit-Evasion Games](https://arxiv.org/abs/2506.02849)
*Alejandro Sanchez Roncero,Olov Andersson,Petter Ogren*

Main category: cs.RO

Relevance: 40.0

TL;DR: 本文提出了一种基于强化学习的框架，用于1v1四旋翼无人机追逃任务，通过异步多阶段种群算法（AMSPB）解决对抗性训练中的非平稳性和灾难性遗忘问题。


<details>
  <summary>Details</summary>
Motivation: 小型无人机在民用和军用领域的普及引发了安全和隐私问题，尤其是未经授权或恶意无人机进入限制区域的情况。本文旨在通过强化学习解决这一问题。

Method: 采用神经网络策略控制四旋翼无人机的角速度和推力，利用异步多阶段种群算法（AMSPB）进行对抗性训练，确保性能的单调提升和策略的保留。

Result: 实验表明，基于角速度的策略比基于速度的基线方法具有更高的捕获率和峰值速度，AMSPB算法在对抗基准对手时表现稳定且单调提升。

Conclusion: 本文提出的方法在四旋翼无人机追逃任务中表现出色，为无人机安全和对抗性训练提供了有效解决方案。

Abstract: The increasing proliferation of small UAVs in civilian and military airspace
has raised critical safety and security concerns, especially when unauthorized
or malicious drones enter restricted zones. In this work, we present a
reinforcement learning (RL) framework for agile 1v1 quadrotor pursuit-evasion.
We train neural network policies to command body rates and collective thrust,
enabling high-speed pursuit and evasive maneuvers that fully exploit the
quadrotor's nonlinear dynamics. To mitigate nonstationarity and catastrophic
forgetting during adversarial co-training, we introduce an Asynchronous
Multi-Stage Population-Based (AMSPB) algorithm where, at each stage, either the
pursuer or evader learns against a sampled opponent drawn from a growing
population of past and current policies. This continual learning setup ensures
monotonic performance improvement and retention of earlier strategies. Our
results show that (i) rate-based policies achieve significantly higher capture
rates and peak speeds than velocity-level baselines, and (ii) AMSPB yields
stable, monotonic gains against a suite of benchmark opponents.

</details>


### [501] [Simulation-Based Inference for Adaptive Experiments](https://arxiv.org/abs/2506.02881)
*Brian M Cho,Aurélien Bibaut,Nathan Kallus*

Main category: stat.ME

Relevance: 40.0

TL;DR: 论文提出了一种基于模拟的假设检验和置信区间构建方法，用于多臂老虎机实验设计，解决了现有方法的局限性。


<details>
  <summary>Details</summary>
Motivation: 多臂老虎机实验设计在提高参与者结果、快速识别最佳选项和增强参数估计精度方面具有潜力，但现有推断方法在实践中的效果有限。

Method: 采用基于模拟的方法，通过乐观模拟生成额外的实验轨迹，以表征非正态样本均值检验统计量的分布。

Result: 方法在保持所需覆盖率的同时，将置信区间宽度减少高达50%，尤其对设计未针对的臂有显著改进。

Conclusion: 提出的方法在多臂老虎机设计中具有广泛适用性，提供了更强的推断能力。

Abstract: Multi-arm bandit experimental designs are increasingly being adopted over
standard randomized trials due to their potential to improve outcomes for study
participants, enable faster identification of the best-performing options,
and/or enhance the precision of estimating key parameters. Current approaches
for inference after adaptive sampling either rely on asymptotic normality under
restricted experiment designs or underpowered martingale concentration
inequalities that lead to weak power in practice. To bypass these limitations,
we propose a simulation-based approach for conducting hypothesis tests and
constructing confidence intervals for arm specific means and their differences.
Our simulation-based approach uses positively biased nuisances to generate
additional trajectories of the experiment, which we call \textit{simulation
with optimism}. Using these simulations, we characterize the distribution
potentially non-normal sample mean test statistic to conduct inference. We
provide guarantees for (i) asymptotic type I error control, (ii) convergence of
our confidence intervals, and (iii) asymptotic strong consistency of our
estimator over a wide variety of common bandit designs. Our empirical results
show that our approach achieves the desired coverage while reducing confidence
interval widths by up to 50%, with drastic improvements for arms not targeted
by the design.

</details>


### [502] [Non-stationary Bandit Convex Optimization: A Comprehensive Study](https://arxiv.org/abs/2506.02980)
*Xiaoqi Liu,Dorian Baudry,Julian Zimmert,Patrick Rebeschini,Arya Akhavan*

Main category: stat.ML

Relevance: 40.0

TL;DR: 论文研究了非平稳环境下的Bandit凸优化问题，提出了两种算法TEWA-SE和cExO，分别在强凸和一般凸损失函数下实现最小化后悔。


<details>
  <summary>Details</summary>
Motivation: 解决非平稳环境下的Bandit凸优化问题，优化后悔性能。

Method: 提出了TEWA-SE算法（基于睡眠专家框架）和cExO算法（基于离散化动作空间的指数权重）。

Result: TEWA-SE在强凸损失下对已知S和Δ是最小最优的；cExO在一般凸损失下对已知S和Δ是最小最优的，且对P有改进。

Conclusion: 提出的算法在非平稳环境下显著优化了后悔性能。

Abstract: Bandit Convex Optimization is a fundamental class of sequential
decision-making problems, where the learner selects actions from a continuous
domain and observes a loss (but not its gradient) at only one point per round.
We study this problem in non-stationary environments, and aim to minimize the
regret under three standard measures of non-stationarity: the number of
switches $S$ in the comparator sequence, the total variation $\Delta$ of the
loss functions, and the path-length $P$ of the comparator sequence. We propose
a polynomial-time algorithm, Tilted Exponentially Weighted Average with
Sleeping Experts (TEWA-SE), which adapts the sleeping experts framework from
online convex optimization to the bandit setting. For strongly convex losses,
we prove that TEWA-SE is minimax-optimal with respect to known $S$ and $\Delta$
by establishing matching upper and lower bounds. By equipping TEWA-SE with the
Bandit-over-Bandit framework, we extend our analysis to environments with
unknown non-stationarity measures. For general convex losses, we introduce a
second algorithm, clipped Exploration by Optimization (cExO), based on
exponential weights over a discretized action space. While not polynomial-time
computable, this method achieves minimax-optimal regret with respect to known
$S$ and $\Delta$, and improves on the best existing bounds with respect to $P$.

</details>


### [503] [How do Pre-Trained Models Support Software Engineering? An Empirical Study in Hugging Face](https://arxiv.org/abs/2506.03013)
*Alexandra González,Xavier Franch,David Lo,Silverio Martínez-Fernández*

Main category: cs.SE

Relevance: 40.0

TL;DR: 论文提出了一种针对软件工程（SE）需求的开源预训练模型（PTMs）分类法，并在Hugging Face（HF）上应用该分类法，发现了2,205个SE相关的PTMs。


<details>
  <summary>Details</summary>
Motivation: 解决开源PTMs缺乏针对SE任务的分类问题，为SE领域提供更合适的资源支持。

Method: 通过系统化的HF API数据收集、模型卡描述和arXiv论文摘要分析，结合多步过滤（如异常值检测、近重复PTMs识别和Gemini 2.0 Flash验证）确定SE相关性。

Result: 发现2,205个SE PTMs，代码生成是最常见的SE任务，而需求工程和软件设计任务较少。文本生成在SE PTMs中占主导地位，且SE PTMs数量自2023年Q2起显著增加。

Conclusion: 提出的分类法为未来自动化SE场景（如PTMs的采样和选择）提供了基础。

Abstract: Open-Source Pre-Trained Models (PTMs) provide extensive resources for various
Machine Learning (ML) tasks, yet these resources lack a classification tailored
to Software Engineering (SE) needs. To address this gap, we derive a taxonomy
encompassing 147 SE tasks and apply an SE-oriented classification to PTMs in a
popular open-source ML repository, Hugging Face (HF). Our repository mining
study began with a systematically gathered database of PTMs from the HF API,
considering their model card descriptions and metadata, and the abstract of the
associated arXiv papers. We confirmed SE relevance through multiple filtering
steps: detecting outliers, identifying near-identical PTMs, and the use of
Gemini 2.0 Flash, which was validated with five pilot studies involving three
human annotators. This approach uncovered 2,205 SE PTMs. We find that code
generation is the most common SE task among PTMs, primarily focusing on
software implementation, while requirements engineering and software design
activities receive limited attention. In terms of ML tasks, text generation
dominates within SE PTMs. Notably, the number of SE PTMs has increased markedly
since 2023 Q2. Our classification provides a solid foundation for future
automated SE scenarios, such as the sampling and selection of suitable PTMs.

</details>


### [504] [On the Benefits of Accelerated Optimization in Robust and Private Estimation](https://arxiv.org/abs/2506.03044)
*Laurentiu Andrei Marchis,Po-Ling Loh*

Main category: math.ST

Relevance: 40.0

TL;DR: 论文研究了基于Frank-Wolfe方法和投影梯度下降的加速梯度方法在隐私和重尾鲁棒性方面的优势，通过优化学习率和梯度下界，减少了迭代复杂度，并提供了更强的统计保证。


<details>
  <summary>Details</summary>
Motivation: 研究动机在于探索加速梯度方法在隐私保护和重尾鲁棒性方面的潜力，以提升统计学习任务的效率和安全性。

Method: 方法包括：1) 针对Frank-Wolfe方法，设计定制学习率和梯度下界；2) 使用Nesterov动量加速投影梯度下降；3) 基于噪声梯度实现隐私和鲁棒性，包括高斯机制和几何中位数估计。

Result: 结果表明，这些加速方法在非随机数据、随机无模型数据和参数模型（如线性回归和广义线性模型）中均能减少迭代复杂度，并提供更强的统计保证。

Conclusion: 结论指出，该方法在特定场景下达到了最优收敛，并在隐私和鲁棒性方面表现优异。

Abstract: We study the advantages of accelerated gradient methods, specifically based
on the Frank-Wolfe method and projected gradient descent, for privacy and
heavy-tailed robustness. Our approaches are as follows: For the Frank-Wolfe
method, our technique is based on a tailored learning rate and a uniform lower
bound on the gradient of the $\ell_2$-norm over the constraint set. For
accelerating projected gradient descent, we use the popular variant based on
Nesterov's momentum, and we optimize our objective over $\mathbb{R}^p$. These
accelerations reduce iteration complexity, translating into stronger
statistical guarantees for empirical and population risk minimization. Our
analysis covers three settings: non-random data, random model-free data, and
parametric models (linear regression and generalized linear models).
Methodologically, we approach both privacy and robustness based on noisy
gradients. We ensure differential privacy via the Gaussian mechanism and
advanced composition, and we achieve heavy-tailed robustness using a geometric
median-of-means estimator, which also sharpens the dependency on the dimension
of the covariates. Finally, we compare our rates to existing bounds and
identify scenarios where our methods attain optimal convergence.

</details>


### [505] [GL-LowPopArt: A Nearly Instance-Wise Minimax Estimator for Generalized Low-Rank Trace Regression](https://arxiv.org/abs/2506.03074)
*Junghyun Lee,Kyoungseok Jang,Kwang-Sung Jun,Milan Vojnović,Se-Young Yun*

Main category: stat.ML

Relevance: 40.0

TL;DR: GL-LowPopArt是一种新型的Catoni风格估计器，用于广义低秩迹回归，采用两阶段方法（核范数正则化和矩阵Catoni估计），在估计误差界限上达到最优，并提出了新的实验设计目标GL(π)。


<details>
  <summary>Details</summary>
Motivation: 解决广义低秩迹回归中的非线性逆链接函数带来的偏差问题，提升估计精度。

Method: 两阶段方法：核范数正则化后接矩阵Catoni估计。

Result: 在广义线性矩阵补全和双线性决斗赌博机等应用中，GL-LowPopArt实现了最优的Frobenius误差保证和改进的Borda遗憾界限。

Conclusion: GL-LowPopArt在实例级别上达到最优性，并揭示了新的问题相关量。

Abstract: We present `GL-LowPopArt`, a novel Catoni-style estimator for generalized
low-rank trace regression. Building on `LowPopArt` (Jang et al., 2024), it
employs a two-stage approach: nuclear norm regularization followed by matrix
Catoni estimation. We establish state-of-the-art estimation error bounds,
surpassing existing guarantees (Fan et al., 2019; Kang et al., 2022), and
reveal a novel experimental design objective, $\mathrm{GL}(\pi)$. The key
technical challenge is controlling bias from the nonlinear inverse link
function, which we address by our two-stage approach. We prove a *local*
minimax lower bound, showing that our `GL-LowPopArt` enjoys instance-wise
optimality up to the condition number of the ground-truth Hessian. Applications
include generalized linear matrix completion, where `GL-LowPopArt` achieves a
state-of-the-art Frobenius error guarantee, and **bilinear dueling bandits**, a
novel setting inspired by general preference learning (Zhang et al., 2024). Our
analysis of a `GL-LowPopArt`-based explore-then-commit algorithm reveals a new,
potentially interesting problem-dependent quantity, along with improved Borda
regret bound than vectorization (Wu et al., 2024).

</details>


### [506] [Graph-Based Adversarial Domain Generalization with Anatomical Correlation Knowledge for Cross-User Human Activity Recognition](https://arxiv.org/abs/2506.01962)
*Xiaozhou Ye,Kevin I-Kai Wang*

Main category: cs.LG

Relevance: 30.0

TL;DR: 论文提出了一种名为GNN-ADG的新方法，结合图神经网络和对抗学习，解决了传感器基于人体活动识别中的跨用户泛化问题。


<details>
  <summary>Details</summary>
Motivation: 传统模型在跨用户泛化上表现不佳，主要由于用户行为、传感器位置和数据分布的差异。

Method: GNN-ADG通过构建三种解剖单元（互联单元、类比单元和侧向单元）的图结构，结合对抗学习，动态整合空间、功能和区域相关性。

Result: 该方法能够在不依赖目标用户数据的情况下，学习到对未见用户泛化良好的特征。

Conclusion: GNN-ADG为传感器基于人体活动识别提供了一种实用的跨用户泛化解决方案。

Abstract: Cross-user variability poses a significant challenge in sensor-based Human
Activity Recognition (HAR) systems, as traditional models struggle to
generalize across users due to differences in behavior, sensor placement, and
data distribution. To address this, we propose GNN-ADG (Graph Neural Network
with Adversarial Domain Generalization), a novel method that leverages both the
strength from both the Graph Neural Networks (GNNs) and adversarial learning to
achieve robust cross-user generalization. GNN-ADG models spatial relationships
between sensors on different anatomical body parts, extracting three types of
Anatomical Units: (1) Interconnected Units, capturing inter-relations between
neighboring sensors; (2) Analogous Units, grouping sensors on symmetrical or
functionally similar body parts; and (3) Lateral Units, connecting sensors
based on their position to capture region-specific coordination. These units
information are fused into an unified graph structure with a cyclic training
strategy, dynamically integrating spatial, functional, and lateral correlations
to facilitate a holistic, user-invariant representation. Information fusion
mechanism of GNN-ADG occurs by iteratively cycling through edge topologies
during training, allowing the model to refine its understanding of inter-sensor
relationships across diverse perspectives. By representing the spatial
configuration of sensors as an unified graph and incorporating adversarial
learning, Information Fusion GNN-ADG effectively learns features that
generalize well to unseen users without requiring target user data during
training, making it practical for real-world applications.

</details>


### [507] [Traffic and Mobility Optimization Using AI: Comparative Study between Dubai and Riyadh](https://arxiv.org/abs/2506.01974)
*Kanwal Aalijah*

Main category: cs.LG

Relevance: 30.0

TL;DR: 该论文探讨了如何利用AI结合实时交通数据和地理定位情感分析来解决现代城市交通拥堵问题，并提出了优化交通流的建议。


<details>
  <summary>Details</summary>
Motivation: 现代城市因快速城市化面临交通拥堵问题，影响经济增长、生活质量和环境可持续性。研究旨在利用AI分析交通和居民情感，为城市规划提供动态解决方案。

Method: 结合实时交通数据和地理定位情感分析，使用AI模型和探索性数据分析预测交通拥堵模式、分析通勤行为，并识别拥堵热点和不满意区域。

Result: 研究结果为优化交通流、提升通勤体验以及解决中东等地区特定交通挑战提供了可操作建议。

Conclusion: AI在交通规划和居民情感分析中的应用为现代城市交通问题提供了动态和全面的解决方案。

Abstract: Urban planning plays a very important role in development modern cities. It
effects the economic growth, quality of life, and environmental sustainability.
Modern cities face challenges in managing traffic congestion. These challenges
arise to due to rapid urbanization. In this study we will explore how AI can be
used to understand the traffic and mobility related issues and its effects on
the residents sentiment. The approach combines real-time traffic data with
geo-located sentiment analysis, offering a comprehensive and dynamic approach
to urban mobility planning. AI models and exploratory data analysis was used to
predict traffic congestion patterns, analyze commuter behaviors, and identify
congestion hotspots and dissatisfaction zones. The findings offer actionable
recommendations for optimizing traffic flow, enhancing commuter experiences,
and addressing city specific mobility challenges in the Middle East and beyond.

</details>


### [508] [Towards Unsupervised Training of Matching-based Graph Edit Distance Solver via Preference-aware GAN](https://arxiv.org/abs/2506.01977)
*Wei Huang,Hanchen Wang,Dong Wen,Shaozhen Ma,Wenjie Zhang,Xuemin Lin*

Main category: cs.LG

Relevance: 30.0

TL;DR: GEDRanker是一个基于GAN的无监督框架，用于计算图编辑距离（GED），无需依赖真实标签。


<details>
  <summary>Details</summary>
Motivation: 现有的GED计算方法依赖昂贵的真实标签，而GEDRanker旨在通过无监督学习解决这一问题。

Method: GEDRanker结合了基于匹配的GED求解器和偏好感知判别器，通过GAN框架生成高质量的节点匹配。

Result: 实验表明，GEDRanker能在无监督情况下实现接近最优的GED解质量。

Conclusion: GEDRanker为无监督GED计算提供了有效解决方案。

Abstract: Graph Edit Distance (GED) is a fundamental graph similarity metric widely
used in various applications. However, computing GED is an NP-hard problem.
Recent state-of-the-art hybrid GED solver has shown promising performance by
formulating GED as a bipartite graph matching problem, then leveraging a
generative diffusion model to predict node matching between two graphs, from
which both the GED and its corresponding edit path can be extracted using a
traditional algorithm. However, such methods typically rely heavily on
ground-truth supervision, where the ground-truth labels are often costly to
obtain in real-world scenarios. In this paper, we propose GEDRanker, a novel
unsupervised GAN-based framework for GED computation. Specifically, GEDRanker
consists of a matching-based GED solver and introduces an interpretable
preference-aware discriminator with an effective training strategy to guide the
matching-based GED solver toward generating high-quality node matching without
the need for ground-truth labels. Extensive experiments on benchmark datasets
demonstrate that our GEDRanker enables the matching-based GED solver to achieve
near-optimal solution quality without any ground-truth supervision.

</details>


### [509] [Surrogate Interpretable Graph for Random Decision Forests](https://arxiv.org/abs/2506.01988)
*Akshat Dubey,Aleksandar Anžel,Georges Hattab*

Main category: cs.LG

Relevance: 30.0

TL;DR: 论文提出了一种替代可解释性图方法，用于提升随机森林模型在健康信息学中的全局特征交互可解释性。


<details>
  <summary>Details</summary>
Motivation: 随机森林模型在健康信息学中应用广泛，但特征和估计器数量增加导致全局特征交互难以解释，影响信任和合规性。

Method: 使用图和混合整数线性规划分析并可视化特征交互，生成决策特征交互表和层次化决策特征交互图。

Result: 替代可解释性图显著提升了全局特征交互的可解释性。

Conclusion: 该方法为高风险的健康信息学领域提供了关键的可解释性工具。

Abstract: The field of health informatics has been profoundly influenced by the
development of random forest models, which have led to significant advances in
the interpretability of feature interactions. These models are characterized by
their robustness to overfitting and parallelization, making them particularly
useful in this domain. However, the increasing number of features and
estimators in random forests can prevent domain experts from accurately
interpreting global feature interactions, thereby compromising trust and
regulatory compliance. A method called the surrogate interpretability graph has
been developed to address this issue. It uses graphs and mixed-integer linear
programming to analyze and visualize feature interactions. This improves their
interpretability by visualizing the feature usage per
decision-feature-interaction table and the most dominant hierarchical decision
feature interactions for predictions. The implementation of a surrogate
interpretable graph enhances global interpretability, which is critical for
such a high-stakes domain.

</details>


### [510] [LibriBrain: Over 50 Hours of Within-Subject MEG to Improve Speech Decoding Methods at Scale](https://arxiv.org/abs/2506.02098)
*Miran Özdogan,Gilad Landau,Gereon Elvers,Dulhan Jayalath,Pratik Somaiya,Francesco Mantegna,Mark Woolrich,Oiwi Parker Jones*

Main category: cs.LG

Relevance: 30.0

TL;DR: LibriBrain是一个前所未有的单被试MEG数据集，用于语音解码，规模远超现有数据集，支持深度学习框架集成，并提供了基线实验结果。


<details>
  <summary>Details</summary>
Motivation: 通过提供大规模的单被试MEG数据，推动神经解码技术的发展，尤其是语音解码领域，并为临床脑机接口的开发提供支持。

Method: 数据集包含高质量MEG记录和详细注释，配套Python库支持深度学习框架集成，并提供了标准数据分割和基线实验结果。

Result: 实验表明增加训练数据显著提升解码性能，验证了大规模单被试数据集的价值。

Conclusion: LibriBrain的发布旨在推动语音解码方法的研究，加速安全有效的临床脑机接口的开发。

Abstract: LibriBrain represents the largest single-subject MEG dataset to date for
speech decoding, with over 50 hours of recordings -- 5$\times$ larger than the
next comparable dataset and 50$\times$ larger than most. This unprecedented
`depth' of within-subject data enables exploration of neural representations at
a scale previously unavailable with non-invasive methods. LibriBrain comprises
high-quality MEG recordings together with detailed annotations from a single
participant listening to naturalistic spoken English, covering nearly the full
Sherlock Holmes canon. Designed to support advances in neural decoding,
LibriBrain comes with a Python library for streamlined integration with deep
learning frameworks, standard data splits for reproducibility, and baseline
results for three foundational decoding tasks: speech detection, phoneme
classification, and word classification. Baseline experiments demonstrate that
increasing training data yields substantial improvements in decoding
performance, highlighting the value of scaling up deep, within-subject
datasets. By releasing this dataset, we aim to empower the research community
to advance speech decoding methodologies and accelerate the development of
safe, effective clinical brain-computer interfaces.

</details>


### [511] [Human Heterogeneity Invariant Stress Sensing](https://arxiv.org/abs/2506.02256)
*Yi Xiao,Harshit Sharma,Sawinder Kaur,Dessa Bergen-Cico,Asif Salekin*

Main category: cs.LG

Relevance: 30.0

TL;DR: HHISS提出了一种领域泛化方法，通过去除个体特异性差异来检测压力信号中的一致模式，提高了模型在新人群和环境中的准确性。


<details>
  <summary>Details</summary>
Motivation: 解决因个体差异和健康条件导致的生理信号变化问题，提升压力检测模型的泛化能力。

Method: 提出person-wise子网络修剪交集技术，专注于跨个体的共享特征，并利用连续标签防止过拟合。

Result: 在七个数据集上测试，HHISS始终优于现有基线方法，表现出实际应用的可行性。

Conclusion: HHISS在敏感现实应用中具有可行性和扩展性，特别适用于阿片类药物使用障碍患者的压力检测。

Abstract: Stress affects physical and mental health, and wearable devices have been
widely used to detect daily stress through physiological signals. However,
these signals vary due to factors such as individual differences and health
conditions, making generalizing machine learning models difficult. To address
these challenges, we present Human Heterogeneity Invariant Stress Sensing
(HHISS), a domain generalization approach designed to find consistent patterns
in stress signals by removing person-specific differences. This helps the model
perform more accurately across new people, environments, and stress types not
seen during training. Its novelty lies in proposing a novel technique called
person-wise sub-network pruning intersection to focus on shared features across
individuals, alongside preventing overfitting by leveraging continuous labels
while training. The study focuses especially on people with opioid use disorder
(OUD)-a group where stress responses can change dramatically depending on their
time of daily medication taking. Since stress often triggers cravings, a model
that can adapt well to these changes could support better OUD rehabilitation
and recovery. We tested HHISS on seven different stress datasets-four of which
we collected ourselves and three public ones. Four are from lab setups, one
from a controlled real-world setting, driving, and two are from real-world
in-the-wild field datasets without any constraints. This is the first study to
evaluate how well a stress detection model works across such a wide range of
data. Results show HHISS consistently outperformed state-of-the-art baseline
methods, proving both effective and practical for real-world use. Ablation
studies, empirical justifications, and runtime evaluations confirm HHISS's
feasibility and scalability for mobile stress sensing in sensitive real-world
applications.

</details>


### [512] [CACTI: Leveraging Copy Masking and Contextual Information to Improve Tabular Data Imputation](https://arxiv.org/abs/2506.02306)
*Aditya Gorla,Ryan Wang,Zhengtong Liu,Ulzee An,Sriram Sankararaman*

Main category: cs.LG

Relevance: 30.0

TL;DR: CACTI是一种基于掩码自编码的表格数据填补方法，利用缺失模式的结构和上下文信息，通过新的训练策略提升填补性能。


<details>
  <summary>Details</summary>
Motivation: 填补表格数据时，现有方法未能充分利用缺失模式的结构和上下文信息，CACTI旨在解决这一问题。

Method: 采用中位数截断复制掩码训练策略，结合列名和文本描述捕捉特征间语义关系。

Result: 在多种数据集和缺失条件下，CACTI平均R²提升7.8%，优于现有方法。

Conclusion: 利用数据集特定上下文信息和缺失模式能显著提升填补性能。

Abstract: We present CACTI, a masked autoencoding approach for imputing tabular data
that leverages the structure in missingness patterns and contextual
information. Our approach employs a novel median truncated copy masking
training strategy that encourages the model to learn from empirical patterns of
missingness while incorporating semantic relationships between features -
captured by column names and text descriptions - to better represent feature
dependence. These dual sources of inductive bias enable CACTI to outperform
state-of-the-art methods - an average $R^2$ gain of 7.8% over the next best
method (13.4%, 6.1%, and 5.3% under missing not at random, at random and
completely at random, respectively) - across a diverse range of datasets and
missingness conditions. Our results highlight the value of leveraging
dataset-specific contextual information and missingness patterns to enhance
imputation performance.

</details>


### [513] [Sensitivity-Aware Density Estimation in Multiple Dimensions](https://arxiv.org/abs/2506.02323)
*Aleix Boquet-Pujadas,Pol del Aguila Pla,Michael Unser*

Main category: cs.LG

Relevance: 30.0

TL;DR: 提出了一种基于样条和核范数正则化的多维概率密度估计方法，应用于PET重分箱。


<details>
  <summary>Details</summary>
Motivation: 解决多维问题中不均匀采样概率密度估计的挑战，利用样条的计算速度和灵活性。

Method: 使用网格上的样条，通过核范数正则化Hessian矩阵以促进稀疏性。

Result: 方法具有空间适应性，对正则化参数选择稳定，并在标准密度上测试有效。

Conclusion: 提供了一种有效的概率密度估计框架，并展示了在PET重分箱中的应用。

Abstract: We formulate an optimization problem to estimate probability densities in the
context of multidimensional problems that are sampled with uneven probability.
It considers detector sensitivity as an heterogeneous density and takes
advantage of the computational speed and flexible boundary conditions offered
by splines on a grid. We choose to regularize the Hessian of the spline via the
nuclear norm to promote sparsity. As a result, the method is spatially adaptive
and stable against the choice of the regularization parameter, which plays the
role of the bandwidth. We test our computational pipeline on standard densities
and provide software. We also present a new approach to PET rebinning as an
application of our framework.

</details>


### [514] [HIEGNet: A Heterogenous Graph Neural Network Including the Immune Environment in Glomeruli Classification](https://arxiv.org/abs/2506.02542)
*Niklas Kormann,Masoud Ramuz,Zeeshan Nisar,Nadine S. Schaadt,Hendrik Annuth,Benjamin Doerr,Friedrich Feuerhake,Thomas Lampert,Johannes F. Lutzeyer*

Main category: cs.LG

Relevance: 30.0

TL;DR: 该论文提出了一种名为HIEGNet的新型异构图神经网络，用于肾小球健康分类，结合了肾小球及其周围免疫细胞的信息，在肾移植患者的全切片图像数据集上表现优于基线模型。


<details>
  <summary>Details</summary>
Motivation: 肾小球健康分类是肾病学中的重要任务，但现有图神经网络（GNNs）在此任务中尚未充分探索，尤其是在节点、边和特征构建方面存在挑战。

Method: 提出了一种结合传统和机器学习技术的管道，用于构建异构图，并设计了HIEGNet模型，整合肾小球和免疫细胞信息进行分类。

Result: HIEGNet在肾移植患者的全切片图像数据集上表现优于基线模型，且在患者间泛化能力最佳。

Conclusion: HIEGNet为肾小球分类提供了一种有效方法，尤其在整合免疫环境信息方面表现出优势。

Abstract: Graph Neural Networks (GNNs) have recently been found to excel in
histopathology. However, an important histopathological task, where GNNs have
not been extensively explored, is the classification of glomeruli health as an
important indicator in nephropathology. This task presents unique difficulties,
particularly for the graph construction, i.e., the identification of nodes,
edges, and informative features. In this work, we propose a pipeline composed
of different traditional and machine learning-based computer vision techniques
to identify nodes, edges, and their corresponding features to form a
heterogeneous graph. We then proceed to propose a novel heterogeneous GNN
architecture for glomeruli classification, called HIEGNet, that integrates both
glomeruli and their surrounding immune cells. Hence, HIEGNet is able to
consider the immune environment of each glomerulus in its classification. Our
HIEGNet was trained and tested on a dataset of Whole Slide Images from kidney
transplant patients. Experimental results demonstrate that HIEGNet outperforms
several baseline models and generalises best between patients among all
baseline models. Our implementation is publicly available at
https://github.com/nklsKrmnn/HIEGNet.git.

</details>


### [515] [Assessing the Completeness of Traffic Scenario Categories for Automated Highway Driving Functions via Cluster-based Analysis](https://arxiv.org/abs/2506.02599)
*Niklas Roßberg,Marion Neumeier,Sinan Hasirlioglu,Mohamed Essayed Bouzouraa,Michael Botsch*

Main category: cs.LG

Relevance: 30.0

TL;DR: 论文提出了一种基于CVQ-VAE的高速公路交通场景聚类方法，分析了不同类别数量对场景完整性的影响，结果显示其聚类性能优于先前工作。


<details>
  <summary>Details</summary>
Motivation: 为了确保自动驾驶系统（ADS）在复杂交通场景中的安全运行，需要精确理解交通场景，因此开发了一种聚类和分析方法。

Method: 使用CVQ-VAE对高速公路交通场景进行聚类，生成不同类别数量的目录，并分析类别数量对场景完整性的影响。

Result: 聚类性能优于先前工作，同时讨论了聚类质量与数据量之间的权衡。

Conclusion: 该方法为ADS的安全发布提供了有效的场景聚类和分析工具。

Abstract: The ability to operate safely in increasingly complex traffic scenarios is a
fundamental requirement for Automated Driving Systems (ADS). Ensuring the safe
release of ADS functions necessitates a precise understanding of the occurring
traffic scenarios. To support this objective, this work introduces a pipeline
for traffic scenario clustering and the analysis of scenario category
completeness. The Clustering Vector Quantized - Variational Autoencoder
(CVQ-VAE) is employed for the clustering of highway traffic scenarios and
utilized to create various catalogs with differing numbers of traffic scenario
categories. Subsequently, the impact of the number of categories on the
completeness considerations of the traffic scenario categories is analyzed. The
results show an outperforming clustering performance compared to previous work.
The trade-off between cluster quality and the amount of required data to
maintain completeness is discussed based on the publicly available highD
dataset.

</details>


### [516] [Data Leakage and Deceptive Performance: A Critical Examination of Credit Card Fraud Detection Methodologies](https://arxiv.org/abs/2506.02703)
*Khizar Hayat,Baptiste Magnier*

Main category: cs.LG

Relevance: 30.0

TL;DR: 论文指出信用卡欺诈检测研究中存在方法论缺陷，即使简单模型在违反基本原则时也能取得虚假的高性能。


<details>
  <summary>Details</summary>
Motivation: 揭示当前欺诈检测研究中普遍存在的方法论问题，强调方法论严谨性比模型复杂性更重要。

Method: 通过实验展示数据泄露、方法报告模糊、时间验证不足和指标操纵等问题。

Result: 一个简单的神经网络在数据泄露情况下优于许多复杂方法，达到99.9%召回率。

Conclusion: 方法论严谨性应优先于模型复杂性，对机器学习研究实践有广泛启示。

Abstract: This study critically examines the methodological rigor in credit card fraud
detection research, revealing how fundamental evaluation flaws can overshadow
algorithmic sophistication. Through deliberate experimentation with improper
evaluation protocols, we demonstrate that even simple models can achieve
deceptively impressive results when basic methodological principles are
violated. Our analysis identifies four critical issues plaguing current
approaches: (1) pervasive data leakage from improper preprocessing sequences,
(2) intentional vagueness in methodological reporting, (3) inadequate temporal
validation for transaction data, and (4) metric manipulation through recall
optimization at precision's expense. We present a case study showing how a
minimal neural network architecture with data leakage outperforms many
sophisticated methods reported in literature, achieving 99.9\% recall despite
fundamental evaluation flaws. These findings underscore that proper evaluation
methodology matters more than model complexity in fraud detection research. The
study serves as a cautionary example of how methodological rigor must precede
architectural sophistication, with implications for improving research
practices across machine learning applications.

</details>


### [517] [Knowledge Graph Completion by Intermediate Variables Regularization](https://arxiv.org/abs/2506.02749)
*Changyi Xiao,Yixin Cao*

Main category: cs.LG

Relevance: 30.0

TL;DR: 本文总结了基于张量分解的知识图谱补全（KGC）模型，提出了一种新的正则化方法以减少过拟合，并通过理论和实验验证了其有效性。


<details>
  <summary>Details</summary>
Motivation: 现有的张量分解模型在KGC中表现优异，但容易过拟合，且现有正则化方法仅最小化嵌入范数，效果有限。

Method: 提出了一种新的正则化方法，最小化预测张量计算中涉及的中间变量的范数，适用于大多数张量分解模型。

Result: 实验验证了新正则化方法的有效性，理论分析表明其能降低预测张量的迹范数以减少过拟合。

Conclusion: 新正则化方法显著提升了张量分解模型的性能，为KGC任务提供了更可靠的解决方案。

Abstract: Knowledge graph completion (KGC) can be framed as a 3-order binary tensor
completion task. Tensor decomposition-based (TDB) models have demonstrated
strong performance in KGC. In this paper, we provide a summary of existing TDB
models and derive a general form for them, serving as a foundation for further
exploration of TDB models. Despite the expressiveness of TDB models, they are
prone to overfitting. Existing regularization methods merely minimize the norms
of embeddings to regularize the model, leading to suboptimal performance.
Therefore, we propose a novel regularization method for TDB models that
addresses this limitation. The regularization is applicable to most TDB models
and ensures tractable computation. Our method minimizes the norms of
intermediate variables involved in the different ways of computing the
predicted tensor. To support our regularization method, we provide a
theoretical analysis that proves its effect in promoting low trace norm of the
predicted tensor to reduce overfitting. Finally, we conduct experiments to
verify the effectiveness of our regularization technique as well as the
reliability of our theoretical analysis. The code is available at
https://github.com/changyi7231/IVR.

</details>


### [518] [Interaction Field Matching: Overcoming Limitations of Electrostatic Models](https://arxiv.org/abs/2506.02950)
*Stepan I. Manukhov,Alexander Kolesov,Vladimir V. Palyulin,Alexander Korotin*

Main category: cs.LG

Relevance: 30.0

TL;DR: 论文提出了一种名为Interaction Field Matching (IFM)的新方法，扩展了Electrostatic Field Matching (EFM)，通过引入更通用的交互场解决了EFM中建模静电场的复杂性。


<details>
  <summary>Details</summary>
Motivation: EFM作为一种基于物理启发的数据生成和传输方法，因需要建模复杂的静电场而存在挑战。论文旨在通过IFM解决这一问题。

Method: 提出IFM，允许使用超出静电场的通用交互场，并设计了一种基于强相互作用物理的特定实现。

Result: 在玩具和图像数据传输问题上展示了IFM的性能。

Conclusion: IFM成功解决了EFM中的建模问题，并展示了其有效性。

Abstract: Electrostatic field matching (EFM) has recently appeared as a novel
physics-inspired paradigm for data generation and transfer using the idea of an
electric capacitor. However, it requires modeling electrostatic fields using
neural networks, which is non-trivial because of the necessity to take into
account the complex field outside the capacitor plates. In this paper, we
propose Interaction Field Matching (IFM), a generalization of EFM which allows
using general interaction fields beyond the electrostatic one. Furthermore,
inspired by strong interactions between quarks and antiquarks in physics, we
design a particular interaction field realization which solves the problems
which arise when modeling electrostatic fields in EFM. We show the performance
on a series of toy and image data transfer problems.

</details>


### [519] [Computation- and Communication-Efficient Online FL for Resource-Constrained Aerial Vehicles](https://arxiv.org/abs/2506.02972)
*Md-Ferdous Pervej,Richeng Jin,Md Moin Uddin Chowdhury,Simran Singh,İsmail Güvenç,Huaiyu Dai*

Main category: cs.LG

Relevance: 30.0

TL;DR: 提出了一种计算和通信高效的在线空中联邦学习算法（2CEOAFL），用于处理空中连接车辆（ACV）的持续感知数据，同时优化资源利用。


<details>
  <summary>Details</summary>
Motivation: ACV的持续感知数据需要在线学习，但其资源受限，因此需要高效的机器学习解决方案。

Method: 提出2CEOAFL算法，包括模型剪枝、训练和概率量化梯度卸载。

Result: 算法性能与非剪枝和非量化的低效版本相当。

Conclusion: 2CEOAFL在保持性能的同时显著提升了计算和通信效率。

Abstract: Privacy-preserving distributed machine learning (ML) and aerial connected
vehicle (ACV)-assisted edge computing have drawn significant attention lately.
Since the onboard sensors of ACVs can capture new data as they move along their
trajectories, the continual arrival of such 'newly' sensed data leads to online
learning and demands carefully crafting the trajectories. Besides, as typical
ACVs are inherently resource-constrained, computation- and
communication-efficient ML solutions are needed. Therefore, we propose a
computation- and communication-efficient online aerial federated learning
(2CEOAFL) algorithm to take the benefits of continual sensed data and limited
onboard resources of the ACVs. In particular, considering independently owned
ACVs act as selfish data collectors, we first model their trajectories
according to their respective time-varying data distributions. We then propose
a 2CEOAFL algorithm that allows the flying ACVs to (a) prune the received dense
ML model to make it shallow, (b) train the pruned model, and (c)
probabilistically quantize and offload their trained accumulated gradients to
the central server (CS). Our extensive simulation results show that the
proposed 2CEOAFL algorithm delivers comparable performances to its non-pruned
and nonquantized, hence, computation- and communication-inefficient
counterparts.

</details>


### [520] [Protein Inverse Folding From Structure Feedback](https://arxiv.org/abs/2506.03028)
*Junde Xu,Zijun Gao,Xinyi Zhou,Jie Hu,Xingyi Cheng,Le Song,Guangyong Chen,Pheng-Ann Heng,Jiezhong Qiu*

Main category: cs.LG

Relevance: 30.0

TL;DR: 论文提出了一种基于直接偏好优化（DPO）的方法，通过蛋白质折叠模型的反馈来优化逆折叠模型，显著提升了序列恢复和结构相似性。


<details>
  <summary>Details</summary>
Motivation: 逆折叠问题在生物技术应用中至关重要，但现有模型在序列设计和结构相似性方面仍有改进空间。

Method: 使用DPO目标对逆折叠模型进行微调，通过蛋白质折叠模型生成的结构偏好标签优化模型。

Result: 在CATH 4.2测试集上，DPO微调将平均TM-Score从0.77提升至0.81，迭代应用后提升79.5%。

Conclusion: 该方法为通过偏好优化提升蛋白质序列设计能力提供了新方向。

Abstract: The inverse folding problem, aiming to design amino acid sequences that fold
into desired three-dimensional structures, is pivotal for various
biotechnological applications. Here, we introduce a novel approach leveraging
Direct Preference Optimization (DPO) to fine-tune an inverse folding model
using feedback from a protein folding model. Given a target protein structure,
we begin by sampling candidate sequences from the inverse-folding model, then
predict the three-dimensional structure of each sequence with the folding model
to generate pairwise structural-preference labels. These labels are used to
fine-tune the inverse-folding model under the DPO objective. Our results on the
CATH 4.2 test set demonstrate that DPO fine-tuning not only improves sequence
recovery of baseline models but also leads to a significant improvement in
average TM-Score from 0.77 to 0.81, indicating enhanced structure similarity.
Furthermore, iterative application of our DPO-based method on challenging
protein structures yields substantial gains, with an average TM-Score increase
of 79.5\% with regard to the baseline model. This work establishes a promising
direction for enhancing protein sequence design ability from structure feedback
by effectively utilizing preference optimization.

</details>


### [521] [Sample complexity of Schrödinger potential estimation](https://arxiv.org/abs/2506.03043)
*Nikita Puchkin,Iurii Pustovalov,Yuri Sapronov,Denis Suchkov,Alexey Naumov,Denis Belomestny*

Main category: cs.LG

Relevance: 30.0

TL;DR: 论文研究了Schrödinger势估计问题，提出了基于KL风险最小化的非渐近高概率上界，展示了样本量增加时KL风险的快速下降。


<details>
  <summary>Details</summary>
Motivation: Schrödinger势估计在生成建模和随机最优控制中至关重要，但现有方法在无界支持分布下的泛化能力尚未充分研究。

Method: 使用经验KL风险最小化方法，在可容许对数势类上拟合目标分布，并推导非渐近高概率上界。

Result: 在合理假设下，KL风险可以以O(log²n/n)的速度下降，即使分布支持无界。

Conclusion: 该方法在无界支持分布下仍能有效估计Schrödinger势，为生成建模提供了理论支持。

Abstract: We address the problem of Schr\"odinger potential estimation, which plays a
crucial role in modern generative modelling approaches based on Schr\"odinger
bridges and stochastic optimal control for SDEs. Given a simple prior diffusion
process, these methods search for a path between two given distributions
$\rho_0$ and $\rho_T^*$ requiring minimal efforts. The optimal drift in this
case can be expressed through a Schr\"odinger potential. In the present paper,
we study generalization ability of an empirical Kullback-Leibler (KL) risk
minimizer over a class of admissible log-potentials aimed at fitting the
marginal distribution at time $T$. Under reasonable assumptions on the target
distribution $\rho_T^*$ and the prior process, we derive a non-asymptotic
high-probability upper bound on the KL-divergence between $\rho_T^*$ and the
terminal density corresponding to the estimated log-potential. In particular,
we show that the excess KL-risk may decrease as fast as $O(\log^2 n / n)$ when
the sample size $n$ tends to infinity even if both $\rho_0$ and $\rho_T^*$ have
unbounded supports.

</details>


### [522] [Rectified Flows for Fast Multiscale Fluid Flow Modeling](https://arxiv.org/abs/2506.03111)
*Victor Armegioiu,Yannick Ramic,Siddhartha Mishra*

Main category: cs.LG

Relevance: 30.0

TL;DR: 提出了一种基于修正流框架的方法，通过更少的步骤实现高保真度的流体流动建模，显著提高了推理效率。


<details>
  <summary>Details</summary>
Motivation: 流体流动的多尺度动态和对初始条件的极端敏感性使其统计建模极具挑战性。现有方法（如扩散模型）需要大量推理步骤，效率低下。

Method: 引入修正流框架，学习时间依赖的速度场，将输入分布沿近乎直线的轨迹传输到输出分布。通过将采样转化为沿流场求解ODE，显著减少所需步骤。

Result: 实验表明，该方法在保持高保真度的同时，仅需8步即可完成推理，而标准扩散模型需要128步以上。

Conclusion: 修正流框架在高效性和保真度上优于现有方法，适用于多尺度流体流动建模。

Abstract: The statistical modeling of fluid flows is very challenging due to their
multiscale dynamics and extreme sensitivity to initial conditions. While
recently proposed conditional diffusion models achieve high fidelity, they
typically require hundreds of stochastic sampling steps at inference. We
introduce a rectified flow framework that learns a time-dependent velocity
field, transporting input to output distributions along nearly straight
trajectories. By casting sampling as solving an ordinary differential equation
(ODE) along this straighter flow field, our method makes each integration step
much more effective, using as few as eight steps versus (more than) 128 steps
in standard score-based diffusion, without sacrificing predictive fidelity.
Experiments on challenging multiscale flow benchmarks show that rectified flows
recover the same posterior distributions as diffusion models, preserve
fine-scale features that MSE-trained baselines miss, and deliver
high-resolution samples in a fraction of inference time.

</details>


### [523] [Machine Learning for Consistency Violation Faults Analysis](https://arxiv.org/abs/2506.02002)
*Kamal Giri,Amit Garu*

Main category: cs.DC

Relevance: 30.0

TL;DR: 该论文提出了一种基于机器学习的方法，用于分析一致性违规故障（CVFs）对分布式系统的影响，并以Dijkstra的令牌环问题为例。通过计算程序转换排名及其影响，量化了CVFs对系统行为的影响。


<details>
  <summary>Details</summary>
Motivation: 分布式系统中常见的一致性违规故障（CVFs）会影响系统性能和收敛性，但对其影响的量化分析仍具挑战性。本研究旨在填补这一空白。

Method: 采用两种神经网络模型（FNN和分布式神经网络）分析CVFs的影响，训练数据来自小规模图（3至10节点），预测关键参数。

Result: 实验结果显示模型性能良好，测试损失为4.39，平均绝对误差为1.5。分布式训练在CPU上未显著提速，但硬件加速器（如GPU/TPU）可能提升扩展性。

Conclusion: 该方法为量化CVFs影响提供了有效工具，未来可通过硬件优化进一步提升性能。

Abstract: Distributed systems frequently encounter consistency violation faults (cvfs),
where nodes operate on outdated or inaccurate data, adversely affecting
convergence and overall system performance. This study presents a machine
learning-based approach for analyzing the impact of CVFs, using Dijkstra's
Token Ring problem as a case study. By computing program transition ranks and
their corresponding effects, the proposed method quantifies the influence of
cvfs on system behavior. To address the state space explosion encountered in
larger graphs, two models are implemented: a Feedforward Neural Network (FNN)
and a distributed neural network leveraging TensorFlow's \texttt{tf.distribute}
API. These models are trained on datasets generated from smaller graphs (3 to
10 nodes) to predict parameters essential for determining rank effects.
Experimental results demonstrate promising performance, with a test loss of
4.39 and a mean absolute error of 1.5. Although distributed training on a CPU
did not yield significant speed improvements over a single-device setup, the
findings suggest that scalability could be enhanced through the use of advanced
hardware accelerators such as GPUs or TPUs.

</details>


### [524] [Blockchain Powered Edge Intelligence for U-Healthcare in Privacy Critical and Time Sensitive Environment](https://arxiv.org/abs/2506.02038)
*Anum Nawaz,Hafiz Humza Mahmood Ramzan,Xianjia Yu,Zhuo Zou,Tomi Westerlund*

Main category: cs.CR

Relevance: 30.0

TL;DR: 论文提出了一种结合区块链的边缘智能系统，用于隐私保护和实时健康监测，包括1D-CNN分类器和安全访问方案。


<details>
  <summary>Details</summary>
Motivation: 解决边缘智能系统中因数据交互和分布式存储带来的隐私和安全性问题，特别是在健康应用中。

Method: 提出自主计算模型、1D-CNN分类器及安全访问方案，支持实时监测和隐私保护。

Result: 验证了系统在安全性、性能和成本方面的效率与可靠性。

Conclusion: 系统适用于隐私敏感和时间敏感的健康应用，具有高效和可靠的访问控制。

Abstract: Edge Intelligence (EI) serves as a critical enabler for privacy-preserving
systems by providing AI-empowered computation and distributed caching services
at the edge, thereby minimizing latency and enhancing data privacy. The
integration of blockchain technology further augments EI frameworks by ensuring
transactional transparency, auditability, and system-wide reliability through a
decentralized network model. However, the operational architecture of such
systems introduces inherent vulnerabilities, particularly due to the extensive
data interactions between edge gateways (EGs) and the distributed nature of
information storage during service provisioning. To address these challenges,
we propose an autonomous computing model along with its interaction topologies
tailored for privacy-critical and time-sensitive health applications. The
system supports continuous monitoring, real-time alert notifications, disease
detection, and robust data processing and aggregation. It also includes a data
transaction handler and mechanisms for ensuring privacy at the EGs. Moreover, a
resource-efficient one-dimensional convolutional neural network (1D-CNN) is
proposed for the multiclass classification of arrhythmia, enabling accurate and
real-time analysis of constrained EGs. Furthermore, a secure access scheme is
defined to manage both off-chain and on-chain data sharing and storage. To
validate the proposed model, comprehensive security, performance, and cost
analyses are conducted, demonstrating the efficiency and reliability of the
fine-grained access control scheme.

</details>


### [525] [Second-order AAA algorithms for structured data-driven modeling](https://arxiv.org/abs/2506.02241)
*Michael S. Ackermann,Ion Victor Gosea,Serkan Gugercin,Steffen W. R. Werner*

Main category: math.NA

Relevance: 30.0

TL;DR: 本文提出了三种数据驱动建模方法，用于直接从频域数据构建具有二阶微分结构的动态系统，并扩展了经典的Adaptive Antoulas-Anderson算法。


<details>
  <summary>Details</summary>
Motivation: 动态系统的数据驱动建模常忽略物理现象的微分结构，导致模型难以物理解释。本文旨在解决这一问题。

Method: 基于二阶结构化重心形式，扩展Adaptive Antoulas-Anderson算法，提出两种变体以平衡计算速度和建模精度。

Result: 数值实验表明，新方法在建模精度和性能上优于传统非结构化方法。

Conclusion: 新方法为动态系统建模提供了更高效的解决方案，同时保留了物理意义。

Abstract: The data-driven modeling of dynamical systems has become an essential tool
for the construction of accurate computational models from real-world data. In
this process, the inherent differential structures underlying the considered
physical phenomena are often neglected making the reinterpretation of the
learned models in a physically meaningful sense very challenging. In this work,
we present three data-driven modeling approaches for the construction of
dynamical systems with second-order differential structure directly from
frequency domain data. Based on the second-order structured barycentric form,
we extend the well-known Adaptive Antoulas-Anderson algorithm to the case of
second-order systems. Depending on the available computational resources, we
propose variations of the proposed method that prioritize either higher
computation speed or greater modeling accuracy, and we present a theoretical
analysis for the expected accuracy and performance of the proposed methods.
Three numerical examples demonstrate the effectiveness of our new structured
approaches in comparison to classical unstructured data-driven modeling.

</details>


### [526] [Learning Optimal Posted Prices for a Unit-Demand Buyer](https://arxiv.org/abs/2506.02284)
*Yifeng Teng,Yifan Wang*

Main category: cs.GT

Relevance: 30.0

TL;DR: 研究了在单位需求买家独立物品价值下学习最优定价的问题，探讨了两种查询模型：样本访问模型和定价查询模型，并给出了问题的样本复杂性和定价查询复杂性的近乎紧的界限。


<details>
  <summary>Details</summary>
Motivation: 研究如何通过查询模型高效学习最优定价，以解决单位需求买家独立物品价值的定价问题。

Method: 采用两种查询模型：样本访问模型（获取物品价值样本）和定价查询模型（设置价格并获取二进制信号），分析其样本复杂性和定价查询复杂性。

Result: 得出了单位需求定价问题的样本复杂性和定价查询复杂性的近乎紧的界限。

Conclusion: 通过两种查询模型，能够高效学习最优定价，为相关领域提供了理论支持。

Abstract: We study the problem of learning the optimal item pricing for a unit-demand
buyer with independent item values, and the learner has query access to the
buyer's value distributions. We consider two common query models in the
literature: the sample access model where the learner can obtain a sample of
each item value, and the pricing query model where the learner can set a price
for an item and obtain a binary signal on whether the sampled value of the item
is greater than our proposed price. In this work, we give nearly tight sample
complexity and pricing query complexity of the unit-demand pricing problem.

</details>


### [527] [Tensor State Space-based Dynamic Multilayer Network Modeling](https://arxiv.org/abs/2506.02413)
*Tian Lan,Jie Guo,Chen Zhang*

Main category: stat.ML

Relevance: 30.0

TL;DR: 论文提出了一种新的张量状态空间模型（TSSDMN），用于动态多层网络分析，通过对称Tucker分解和变分推断方法捕捉网络的时空动态。


<details>
  <summary>Details</summary>
Motivation: 现有模型难以捕捉动态多层网络的时空动态，因此需要一种新方法来更好地理解和建模此类网络。

Method: 采用对称Tucker分解表示潜在节点特征及其交互模式，结合变分期望最大化算法进行高效推断。

Result: 数值模拟和案例研究表明TSSDMN能有效分析动态多层网络。

Conclusion: TSSDMN为动态多层网络的建模和分析提供了有效工具。

Abstract: Understanding the complex interactions within dynamic multilayer networks is
critical for advancements in various scientific domains. Existing models often
fail to capture such networks' temporal and cross-layer dynamics. This paper
introduces a novel Tensor State Space Model for Dynamic Multilayer Networks
(TSSDMN), utilizing a latent space model framework. TSSDMN employs a symmetric
Tucker decomposition to represent latent node features, their interaction
patterns, and layer transitions. Then by fixing the latent features and
allowing the interaction patterns to evolve over time, TSSDMN uniquely captures
both the temporal dynamics within layers and across different layers. The model
identifiability conditions are discussed. By treating latent features as
variables whose posterior distributions are approximated using a mean-field
variational inference approach, a variational Expectation Maximization
algorithm is developed for efficient model inference. Numerical simulations and
case studies demonstrate the efficacy of TSSDMN for understanding dynamic
multilayer networks.

</details>


### [528] [Online Bayesian system identification in multivariate autoregressive models via message passing](https://arxiv.org/abs/2506.02710)
*T. N. Nisslbeck,Wouter M. Kouw*

Main category: eess.SP

Relevance: 30.0

TL;DR: 提出了一种基于因子图中消息传递的递归贝叶斯估计方法，用于多变量自回归模型，能够提供完整后验分布。


<details>
  <summary>Details</summary>
Motivation: 传统递归最小二乘法无法提供完整的后验分布，而本方法能够量化估计和预测的不确定性。

Method: 使用因子图中的消息传递进行递归贝叶斯估计，计算自回归系数和噪声精度的后验分布。

Result: 在合成自回归系统和双质量-弹簧-阻尼系统上表现出收敛性和竞争力。

Conclusion: 该方法能够在线计算模型证据，并量化预测的不确定性。

Abstract: We propose a recursive Bayesian estimation procedure for multivariate
autoregressive models with exogenous inputs based on message passing in a
factor graph. Unlike recursive least-squares, our method produces full
posterior distributions for both the autoregressive coefficients and noise
precision. The uncertainties regarding these estimates propagate into the
uncertainties on predictions for future system outputs, and support online
model evidence calculations. We demonstrate convergence empirically on a
synthetic autoregressive system and competitive performance on a double
mass-spring-damper system.

</details>


### [529] [Diffusion Buffer: Online Diffusion-based Speech Enhancement with Sub-Second Latency](https://arxiv.org/abs/2506.02908)
*Bunlong Lay,Rostilav Makarov,Timo Gerkmann*

Main category: eess.AS

Relevance: 30.0

TL;DR: 本文提出了一种滑动窗口扩散框架，用于实时语音增强，解决了传统扩散模型计算成本高的问题。


<details>
  <summary>Details</summary>
Motivation: 传统扩散模型在语音增强中表现优异，但推理时计算成本高，无法实时处理流数据。本文旨在解决这一问题。

Method: 采用滑动窗口扩散框架，随时间逐步对语音信号加噪，缓冲区中靠近当前时间的帧加噪更多，实现性能与延迟的权衡。

Result: 实验表明，该方法优于标准扩散模型，GPU上运行时输入输出延迟为0.3至1秒，首次实现实用的在线语音增强扩散方案。

Conclusion: 滑动窗口扩散框架为在线语音增强提供了高效实用的解决方案。

Abstract: Diffusion models are a class of generative models that have been recently
used for speech enhancement with remarkable success but are computationally
expensive at inference time. Therefore, these models are impractical for
processing streaming data in real-time. In this work, we adapt a sliding window
diffusion framework to the speech enhancement task. Our approach progressively
corrupts speech signals through time, assigning more noise to frames close to
the present in a buffer. This approach outputs denoised frames with a delay
proportional to the chosen buffer size, enabling a trade-off between
performance and latency. Empirical results demonstrate that our method
outperforms standard diffusion models and runs efficiently on a GPU, achieving
an input-output latency in the order of 0.3 to 1 seconds. This marks the first
practical diffusion-based solution for online speech enhancement.

</details>


### [530] [Torsion in Persistent Homology and Neural Networks](https://arxiv.org/abs/2506.03049)
*Maria Walch*

Main category: math.AT

Relevance: 30.0

TL;DR: 论文探讨了在结合拓扑数据分析的混合深度学习模型中扭转的作用，重点关注自编码器。研究发现扭转信息在编码过程中可能丢失，且在标准解码器中难以重建，揭示了基于场的方法的局限性。


<details>
  <summary>Details</summary>
Motivation: 研究动机在于揭示整数同调中扭转特征的重要性，以及现有基于场系数的拓扑数据分析工具可能掩盖这些特征的问题。

Method: 方法包括使用合成和高维数据评估扭转对扰动的敏感性，并测试多种自编码器架构中扭转的可恢复性。

Result: 结果显示扭转信息在编码和解码过程中容易丢失或改变，表明基于场的方法存在局限性。

Conclusion: 结论强调了需要设计能够保留扭转信息的架构或损失函数，以实现更稳健的数据表示。

Abstract: We explore the role of torsion in hybrid deep learning models that
incorporate topological data analysis, focusing on autoencoders. While most TDA
tools use field coefficients, this conceals torsional features present in
integer homology. We show that torsion can be lost during encoding, altered in
the latent space, and in many cases, not reconstructed by standard decoders.
Using both synthetic and high-dimensional data, we evaluate torsion sensitivity
to perturbations and assess its recoverability across several autoencoder
architectures. Our findings reveal key limitations of field-based approaches
and underline the need for architectures or loss terms that preserve torsional
information for robust data representation.

</details>


### [531] [Causal Explainability of Machine Learning in Heart Failure Prediction from Electronic Health Records](https://arxiv.org/abs/2506.03068)
*Yina Hou,Shourav B. Rabbani,Liang Hong,Norou Diawara,Manar D. Samad*

Main category: stat.ML

Relevance: 30.0

TL;DR: 论文提出了一种新的计算框架，用于发现混合类型临床变量的因果结构，并评估其因果强度。结果表明，非线性因果关系的建模比线性更有效，且非线性分类器的特征重要性与因果强度高度相关。


<details>
  <summary>Details</summary>
Motivation: 探讨临床变量在疾病预后中的因果解释性，弥补统计和机器学习方法在因果关系分析上的不足。

Method: 提出一种新的计算框架，支持混合类型变量的因果结构发现（CSD）和因果强度评分，应用于心力衰竭分类。

Result: 非线性因果建模更有效；非线性分类器的特征重要性与因果强度强相关；相关变量可能是因果变量，但很少被识别为效应变量。

Conclusion: 该框架可为基于机器学习的预测模型提供变量的因果解释。

Abstract: The importance of clinical variables in the prognosis of the disease is
explained using statistical correlation or machine learning (ML). However, the
predictive importance of these variables may not represent their causal
relationships with diseases. This paper uses clinical variables from a heart
failure (HF) patient cohort to investigate the causal explainability of
important variables obtained in statistical and ML contexts. Due to inherent
regression modeling, popular causal discovery methods strictly assume that the
cause and effect variables are numerical and continuous. This paper proposes a
new computational framework to enable causal structure discovery (CSD) and
score the causal strength of mixed-type (categorical, numerical, binary)
clinical variables for binary disease outcomes. In HF classification, we
investigate the association between the importance rank order of three feature
types: correlated features, features important for ML predictions, and causal
features. Our results demonstrate that CSD modeling for nonlinear causal
relationships is more meaningful than its linear counterparts. Feature
importance obtained from nonlinear classifiers (e.g., gradient-boosting trees)
strongly correlates with the causal strength of variables without
differentiating cause and effect variables. Correlated variables can be causal
for HF, but they are rarely identified as effect variables. These results can
be used to add the causal explanation of variables important for ML-based
prediction modeling.

</details>


### [532] [Improvement of AMPs Identification with Generative Adversarial Network and Ensemble Classification](https://arxiv.org/abs/2506.01983)
*Reyhaneh Keshavarzpour,Eghbal Mansoori*

Main category: cs.LG

Relevance: 20.0

TL;DR: 论文提出了一种改进的深度神经网络方法，用于预测抗菌肽，通过结合最佳编码方法和处理不平衡数据集，显著提高了预测的准确性和效率。


<details>
  <summary>Details</summary>
Motivation: 抗菌肽在生物医学和药物设计中具有重要作用，但传统方法效率不高。本研究旨在通过人工智能算法改进抗菌肽的预测。

Method: 结合多种编码方法的最优部分，并使用深度神经网络处理不平衡数据集。

Result: 提出的方法在预测抗菌肽的准确性和效率上显著优于现有方法。

Conclusion: 该方法在医学和制药领域具有重要应用价值。

Abstract: Identification of antimicrobial peptides is an important and necessary issue
in today's era. Antimicrobial peptides are essential as an alternative to
antibiotics for biomedical applications and many other practical applications.
These oligopeptides are useful in drug design and cause innate immunity against
microorganisms. Artificial intelligence algorithms have played a significant
role in the ease of identifying these peptides.This research is improved by
improving proposed method in the field of antimicrobial peptides prediction.
Suggested method is improved by combining the best coding method from different
perspectives, In the following a deep neural network to balance the imbalanced
combined datasets. The results of this research show that the proposed method
have a significant improvement in the accuracy and efficiency of the prediction
of antimicrobial peptides and are able to provide the best results compared to
the existing methods. These development in the field of prediction and
classification of antimicrobial peptides, basically in the fields of medicine
and pharmaceutical industries, have high effectiveness and application.

</details>


### [533] [Learning Treatment Representations for Downstream Instrumental Variable Regression](https://arxiv.org/abs/2506.02200)
*Shiangyi Lin,Hui Lan,Vasilis Syrgkanis*

Main category: cs.LG

Relevance: 20.0

TL;DR: 论文提出了一种新方法，通过将工具变量信息融入表示学习过程，解决了高维内生变量处理中的遗漏变量偏差问题。


<details>
  <summary>Details</summary>
Motivation: 传统工具变量估计器在处理高维非结构化内生变量时面临限制，通常需要先进行无监督降维，但这种方法可能因隐式正则化导致遗漏变量偏差。

Method: 提出了一种新方法，在表示学习过程中显式结合工具变量信息，构建处理表示。

Result: 理论和实验表明，该方法能优化结果预测方向，优于传统的两阶段降维方法。

Conclusion: 该方法为高维内生变量处理提供了新框架，显著提升了工具变量回归的性能。

Abstract: Traditional instrumental variable (IV) estimators face a fundamental
constraint: they can only accommodate as many endogenous treatment variables as
available instruments. This limitation becomes particularly challenging in
settings where the treatment is presented in a high-dimensional and
unstructured manner (e.g. descriptions of patient treatment pathways in a
hospital). In such settings, researchers typically resort to applying
unsupervised dimension reduction techniques to learn a low-dimensional
treatment representation prior to implementing IV regression analysis. We show
that such methods can suffer from substantial omitted variable bias due to
implicit regularization in the representation learning step. We propose a novel
approach to construct treatment representations by explicitly incorporating
instrumental variables during the representation learning process. Our approach
provides a framework for handling high-dimensional endogenous variables with
limited instruments. We demonstrate both theoretically and empirically that
fitting IV models on these instrument-informed representations ensures
identification of directions that optimize outcome prediction. Our experiments
show that our proposed methodology improves upon the conventional two-stage
approaches that perform dimension reduction without incorporating instrument
information.

</details>


### [534] [Quantum Ensembling Methods for Healthcare and Life Science](https://arxiv.org/abs/2506.02213)
*Kahn Rhrissorrakrai,Kathleen E. Hamilton,Prerana Bangalore Parthsarathy,Aldo Guzman-Saenz,Tyler Alban,Filippo Utro,Laxmi Parida*

Main category: cs.LG

Relevance: 20.0

TL;DR: 该论文研究了量子集成模型在小数据问题（如医疗和生命科学）中的有效性，通过模拟和硬件实验验证了其性能。


<details>
  <summary>Details</summary>
Motivation: 解决小数据学习问题，尤其是在医疗和生命科学领域，量子计算可能提供新的解决方案。

Method: 构建了多种量子集成模型，使用最多26个量子比特（模拟）和56个量子比特（硬件），测试了合成数据集和基因表达数据。

Result: 展示了量子嵌入结构对性能的影响，并讨论了如何提取特征和构建有效模型。

Conclusion: 量子计算在小数据学习中具有潜力，为医疗和生命科学领域提供了新思路。

Abstract: Learning on small data is a challenge frequently encountered in many
real-world applications. In this work we study how effective quantum ensemble
models are when trained on small data problems in healthcare and life sciences.
We constructed multiple types of quantum ensembles for binary classification
using up to 26 qubits in simulation and 56 qubits on quantum hardware. Our
ensemble designs use minimal trainable parameters but require long-range
connections between qubits. We tested these quantum ensembles on synthetic
datasets and gene expression data from renal cell carcinoma patients with the
task of predicting patient response to immunotherapy. From the performance
observed in simulation and initial hardware experiments, we demonstrate how
quantum embedding structure affects performance and discuss how to extract
informative features and build models that can learn and generalize
effectively. We present these exploratory results in order to assist other
researchers in the design of effective learning on small data using ensembles.
Incorporating quantum computing in these data constrained problems offers hope
for a wide range of studies in healthcare and life sciences where biological
samples are relatively scarce given the feature space to be explored.

</details>


### [535] [Discovery of Probabilistic Dirichlet-to-Neumann Maps on Graphs](https://arxiv.org/abs/2506.02337)
*Adrienne M. Propp,Jonas A. Actor,Elise Walker,Houman Owhadi,Nathaniel Trask,Daniel M. Tartakovsky*

Main category: cs.LG

Relevance: 20.0

TL;DR: 论文提出了一种基于高斯过程的图学习Dirichlet-to-Neumann映射的方法，适用于受偏微分方程守恒约束的数据。


<details>
  <summary>Details</summary>
Motivation: 解决多物理场仿真中数据稀缺和不确定性量化的问题。

Method: 结合离散外微积分和非线性最优恢复，优化再生核希尔伯特空间范数，确保守恒定律。

Result: 在子表面裂缝网络和动脉血流应用中表现出高精度和良好校准的不确定性估计。

Conclusion: 该方法在数据稀缺的科学应用中具有潜力。

Abstract: Dirichlet-to-Neumann maps enable the coupling of multiphysics simulations
across computational subdomains by ensuring continuity of state variables and
fluxes at artificial interfaces. We present a novel method for learning
Dirichlet-to-Neumann maps on graphs using Gaussian processes, specifically for
problems where the data obey a conservation constraint from an underlying
partial differential equation. Our approach combines discrete exterior calculus
and nonlinear optimal recovery to infer relationships between vertex and edge
values. This framework yields data-driven predictions with uncertainty
quantification across the entire graph, even when observations are limited to a
subset of vertices and edges. By optimizing over the reproducing kernel Hilbert
space norm while applying a maximum likelihood estimation penalty on kernel
complexity, our method ensures that the resulting surrogate strictly enforces
conservation laws without overfitting. We demonstrate our method on two
representative applications: subsurface fracture networks and arterial blood
flow. Our results show that the method maintains high accuracy and
well-calibrated uncertainty estimates even under severe data scarcity,
highlighting its potential for scientific applications where limited data and
reliable uncertainty quantification are critical.

</details>


### [536] [Stop Chasing the C-index: This Is How We Should Evaluate Our Survival Models](https://arxiv.org/abs/2506.02075)
*Christian Marius Lillelund,Shi-ang Qi,Russell Greiner,Christian Fischer Pedersen*

Main category: stat.ME

Relevance: 20.0

TL;DR: 论文指出生存分析模型的评估方法存在问题，提出新的评估标准，并讨论其优缺点。


<details>
  <summary>Details</summary>
Motivation: 当前生存分析模型的评估主要依赖C-index，忽略了其他重要指标，如时间预测准确性和概率校准。

Method: 通过文献调查提出评估标准，并分析其适用性和局限性。

Result: 发现C-index不足以全面评估模型，需结合其他指标。

Conclusion: 模型和评估指标的假设需一致，提出了实践中的评估方法。

Abstract: We argue that many survival analysis and time-to-event models are incorrectly
evaluated. First, we survey many examples of evaluation approaches in the
literature and find that most rely on concordance (C-index). However, the
C-index only measures a model's discriminative ability and does not assess
other important aspects, such as the accuracy of the time-to-event predictions
or the calibration of the model's probabilistic estimates. Next, we present a
set of key desiderata for choosing the right evaluation metric and discuss
their pros and cons. These are tailored to the challenges in survival analysis,
such as sensitivity to miscalibration and various censoring assumptions. We
hypothesize that the current development of survival metrics conforms to a
double-helix ladder, and that model validity and metric validity must stand on
the same rung of the assumption ladder. Finally, we discuss the appropriate
methods for evaluating a survival model in practice and summarize various
viewpoints opposing our analysis.

</details>


### [537] [A Learned Cost Model-based Cross-engine Optimizer for SQL Workloads](https://arxiv.org/abs/2506.02802)
*András Strausz,Niels Pardon,Ioana Giurgiu*

Main category: cs.DB

Relevance: 20.0

TL;DR: 论文提出了一种跨引擎优化器，通过学习的成本模型自动选择适合不同SQL查询的执行引擎，减少了手动选择的需求。


<details>
  <summary>Details</summary>
Motivation: 解决Lakehouse系统中手动选择执行引擎的复杂性问题，尤其是随着新引擎和工作负载的出现，这一问题变得更加困难。

Method: 使用多任务学习框架，通过优化的查询计划和提示进行成本预测和路由选择，支持灵活添加新引擎。

Result: 实验表明，该方法在零样本和少样本设置下分别减少了25.2%和30.4%的总工作负载运行时间。

Conclusion: 跨引擎优化器显著提高了查询性能，同时降低了手动干预的需求。

Abstract: Lakehouse systems enable the same data to be queried with multiple execution
engines. However, selecting the engine best suited to run a SQL query still
requires a priori knowledge of the query computational requirements and an
engine capability, a complex and manual task that only becomes more difficult
with the emergence of new engines and workloads. In this paper, we address this
limitation by proposing a cross-engine optimizer that can automate engine
selection for diverse SQL queries through a learned cost model. Optimized with
hints, a query plan is used for query cost prediction and routing. Cost
prediction is formulated as a multi-task learning problem, and multiple
predictor heads, corresponding to different engines and provisionings, are used
in the model architecture. This eliminates the need to train engine-specific
models and allows the flexible addition of new engines at a minimal fine-tuning
cost. Results on various databases and engines show that using a query
optimized logical plan for cost estimation decreases the average Q-error by
even 12.6% over using unoptimized plans as input. Moreover, the proposed
cross-engine optimizer reduces the total workload runtime by up to 25.2% in a
zero-shot setting and 30.4% in a few-shot setting when compared to random
routing.

</details>


### [538] [Asymptotically perfect seeded graph matching without edge correlation (and applications to inference)](https://arxiv.org/abs/2506.02825)
*Tong Qi,Vera Andersson,Peter Viechnicki,Vince Lyzinski*

Main category: stat.ML

Relevance: 20.0

TL;DR: OmniMatch算法用于多图匹配，在RDPG模型下能高效对齐未标记顶点，提升测试能力。


<details>
  <summary>Details</summary>
Motivation: 解决多网络顶点对齐问题，尤其是在顶点错位导致测试能力下降的场景。

Method: 基于$d$-维RDPG模型，利用种子顶点对齐未标记顶点，适用于无边相关性的情况。

Result: 算法能高效对齐$O(s^{\alpha})$顶点（$\alpha<2\wedge d/4$），并在模拟和实际数据中验证有效性。

Conclusion: OmniMatch能显著提升顶点对齐和测试能力，适用于多种应用场景。

Abstract: We present the OmniMatch algorithm for seeded multiple graph matching. In the
setting of $d$-dimensional Random Dot Product Graphs (RDPG), we prove that
under mild assumptions, OmniMatch with $s$ seeds asymptotically and efficiently
perfectly aligns $O(s^{\alpha})$ unseeded vertices -- for $\alpha<2\wedge d/4$
-- across multiple networks even in the presence of no edge correlation. We
demonstrate the effectiveness of our algorithm across numerous simulations and
in the context of shuffled graph hypothesis testing. In the shuffled testing
setting, testing power is lost due to the misalignment/shuffling of vertices
across graphs, and we demonstrate the capacity of OmniMatch to correct for
misaligned vertices prior to testing and hence recover the lost testing power.
We further demonstrate the algorithm on a pair of data examples from
connectomics and machine translation.

</details>


### [539] [Validating remotely sensed biomass estimates with forest inventory data in the western US](https://arxiv.org/abs/2506.03120)
*Xiuyu Cao,Joseph O. Sexton,Panshi Wang,Dimitrios Gounaridis,Neil H. Carter,Kai Zhu*

Main category: stat.AP

Relevance: 20.0

TL;DR: 该论文提出了一种基于独立验证的方法，用于评估商业遥感数据（terraPulse）在估算地上生物量密度（AGBD）时的准确性，并与美国森林服务数据（FIA）进行了对比验证。


<details>
  <summary>Details</summary>
Motivation: 高分辨率监测地上生物量（AGB）及其密度（AGBD）对碳核算和生态系统管理至关重要，但目前商业遥感产品缺乏严格的独立验证。

Method: 使用FIA的独立参考数据对terraPulse的AGBD数据集进行区域验证，通过统计和空间分析评估其准确性。

Result: 在六边形和县级尺度上，terraPulse与FIA的估计值高度一致（R²=0.88-0.90，r=0.94-0.95），但在非森林区域和高生物量森林中存在偏差。

Conclusion: 研究为全球生物量监测提供了一个可扩展的验证框架，并展示了商业数据集在碳监测中的潜力。

Abstract: Monitoring aboveground biomass (AGB) and its density (AGBD) at high
resolution is essential for carbon accounting and ecosystem management. While
NASA's spaceborne Global Ecosystem Dynamics Investigation (GEDI) LiDAR mission
provides globally distributed reference measurements for AGBD estimation, the
majority of commercial remote sensing products based on GEDI remain without
rigorous or independent validation. Here, we present an independent regional
validation of an AGBD dataset offered by terraPulse, Inc., based on independent
reference data from the US Forest Service Forest Inventory and Analysis (FIA)
program. Aggregated to 64,000-hectare hexagons and US counties across the US
states of Utah, Nevada, and Washington, we found very strong agreement between
terraPulse and FIA estimates. At the hexagon scale, we report R2 = 0.88, RMSE =
26.68 Mg/ha, and a correlation coefficient (r) of 0.94. At the county scale,
agreement improves to R2 = 0.90, RMSE =32.62 Mg/ha, slope = 1.07, and r = 0.95.
Spatial and statistical analyses indicated that terraPulse AGBD values tended
to exceed FIA estimates in non-forest areas, likely due to FIA's limited
sampling of non-forest vegetation. The terraPulse AGBD estimates also exhibited
lower values in high-biomass forests, likely due to saturation effects in its
optical remote-sensing covariates. This study advances operational carbon
monitoring by delivering a scalable framework for comprehensive AGBD validation
using independent FIA data, as well as a benchmark validation of a new
commercial dataset for global biomass monitoring.

</details>


### [540] [A Data-Driven Approach to Enhancing Gravity Models for Trip Demand Prediction](https://arxiv.org/abs/2506.01964)
*Kamal Acharya,Mehul Lad,Liang Sun,Houbing Song*

Main category: cs.LG

Relevance: 10.0

TL;DR: 该论文提出了一种数据驱动的方法，通过整合地理、经济、社会和旅行数据，结合机器学习技术改进传统的重力模型，显著提升了交通规划的预测准确性。


<details>
  <summary>Details</summary>
Motivation: 传统的重力模型在表示现代旅行行为的复杂因素时表现不足，因此需要一种更先进的方法来提升预测能力。

Method: 整合多种数据集（地理、经济、社会、旅行数据），并应用机器学习技术扩展传统重力模型的能力。

Result: 实验结果显示，改进后的模型在R-squared、MAE和CPC指标上分别提升了51.48%、63.59%和44.32%。

Conclusion: 通过整合多样化的数据和先进算法，可以显著提升交通模型的预测能力，为城市规划者提供更可靠的工具。

Abstract: Accurate prediction of trips between zones is critical for transportation
planning, as it supports resource allocation and infrastructure development
across various modes of transport. Although the gravity model has been widely
used due to its simplicity, it often inadequately represents the complex
factors influencing modern travel behavior. This study introduces a data-driven
approach to enhance the gravity model by integrating geographical, economic,
social, and travel data from the counties in Tennessee and New York state.
Using machine learning techniques, we extend the capabilities of the
traditional model to handle more complex interactions between variables. Our
experiments demonstrate that machine learning-enhanced models significantly
outperform the traditional model. Our results show a 51.48% improvement in
R-squared, indicating a substantial enhancement in the model's explanatory
power. Also, a 63.59% reduction in Mean Absolute Error (MAE) reflects a
significant increase in prediction accuracy. Furthermore, a 44.32% increase in
Common Part of Commuters (CPC) demonstrates improved prediction reliability.
These findings highlight the substantial benefits of integrating diverse
datasets and advanced algorithms into transportation models. They provide urban
planners and policymakers with more reliable forecasting and decision-making
tools.

</details>


### [541] [Crack Path Prediction with Operator Learning using Discrete Particle System data Generation](https://arxiv.org/abs/2506.01976)
*Elham Kiyani,Venkatesh Ananchaperumal,Ahmad Peyvan,Mahendaran Uchimali,Gang Li,George Em Karniadakis*

Main category: cs.LG

Relevance: 10.0

TL;DR: 论文使用DeepONet模型预测裂纹扩展，Fusion DeepONet在非断裂情况下表现更优。


<details>
  <summary>Details</summary>
Motivation: 准确建模裂纹扩展对预测工程材料失效至关重要，尤其是裂纹与孔洞等不连续性的交互作用。

Method: 利用CPD模拟数据训练DeepONet模型，研究两种变体（vanilla和Fusion DeepONet）在不同几何和断裂条件下的表现。

Result: Fusion DeepONet在非断裂情况下预测更准确，但断裂驱动场景仍具挑战性。

Conclusion: Fusion DeepONet在复杂几何和时间依赖的裂纹扩展现象中具有泛化潜力。

Abstract: Accurately modeling crack propagation is critical for predicting failure in
engineering materials and structures, where small cracks can rapidly evolve and
cause catastrophic damage. The interaction of cracks with discontinuities, such
as holes, significantly affects crack deflection and arrest. Recent
developments in discrete particle systems with multibody interactions based on
constitutive behavior have demonstrated the ability to capture crack nucleation
and evolution without relying on continuum assumptions. In this work, we use
data from Constitutively Informed Particle Dynamics (CPD) simulations to train
operator learning models, specifically Deep Operator Networks (DeepONets),
which learn mappings between function spaces instead of finite-dimensional
vectors. We explore two DeepONet variants: vanilla and Fusion DeepONet, for
predicting time-evolving crack propagation in specimens with varying
geometries. Three representative cases are studied: (i) varying notch height
without active fracture; and (ii) and (iii) combinations of notch height and
hole radius where dynamic fracture occurs on irregular discrete meshes. The
models are trained on 32 to 45 samples, using geometric inputs in the branch
network and spatial-temporal coordinates in the trunk network. Results show
that Fusion DeepONet consistently outperforms the vanilla variant, with more
accurate predictions especially in non-fracturing cases. Fracture-driven
scenarios involving displacement and crack evolution remain more challenging.
These findings highlight the potential of Fusion DeepONet to generalize across
complex, geometry-varying, and time-dependent crack propagation phenomena.

</details>


### [542] [Predicting Blood Type: Assessing Model Performance with ROC Analysis](https://arxiv.org/abs/2506.02062)
*Malik A. Altayar,Muhyeeddin Alqaraleh,Mowafaq Salem Alzboon,Wesam T. Almagharbeh*

Main category: cs.LG

Relevance: 10.0

TL;DR: 研究探讨了指纹模式与ABO血型分类的关系，发现两者无显著相关性。


<details>
  <summary>Details</summary>
Motivation: 探索指纹模式与血型之间的潜在关联，以改进个人身份识别技术。

Method: 分析200名个体的指纹（分为环、涡、弓）和血型，使用卡方和皮尔逊相关检验。

Result: 指纹模式与血型无显著相关性（p > 0.05）。

Conclusion: 未来需更大样本和机器学习方法进一步研究。

Abstract: Introduction: Personal identification is a critical aspect of forensic
sciences, security, and healthcare. While conventional biometrics systems such
as DNA profiling and iris scanning offer high accuracy, they are time-consuming
and costly. Objectives: This study investigates the relationship between
fingerprint patterns and ABO blood group classification to explore potential
correlations between these two traits. Methods: The study analyzed 200
individuals, categorizing their fingerprints into three types: loops, whorls,
and arches. Blood group classification was also recorded. Statistical analysis,
including chi-square and Pearson correlation tests, was used to assess
associations between fingerprint patterns and blood groups. Results: Loops were
the most common fingerprint pattern, while blood group O+ was the most
prevalent among the participants. Statistical analysis revealed no significant
correlation between fingerprint patterns and blood groups (p > 0.05),
suggesting that these traits are independent. Conclusions: Although the study
showed limited correlation between fingerprint patterns and ABO blood groups,
it highlights the importance of future research using larger and more diverse
populations, incorporating machine learning approaches, and integrating
multiple biometric signals. This study contributes to forensic science by
emphasizing the need for rigorous protocols and comprehensive investigations in
personal identification.

</details>


### [543] [A meaningful prediction of functional decline in amyotrophic lateral sclerosis based on multi-event survival analysis](https://arxiv.org/abs/2506.02076)
*Christian Marius Lillelund,Sanjay Kalra,Russell Greiner*

Main category: q-bio.QM

Relevance: 10.0

TL;DR: 该研究提出了一种预测ALS患者功能衰退时间的新方法，通过多事件生存模型分析临床数据，优于传统方法，并支持个性化治疗规划。


<details>
  <summary>Details</summary>
Motivation: ALS的异质性使得确定最佳治疗时机困难，需要更精确的预测方法以改善患者生活质量。

Method: 使用多事件生存模型，基于PRO-ACT数据集训练五个协变量生存模型，预测患者功能衰退时间。

Result: 协变量模型优于Kaplan-Meier估计器，支持反事实预测，发现Riluzole对功能衰退影响有限，但发病部位影响显著。

Conclusion: 该方法可用于临床数据，评估功能衰退风险，实现个性化治疗规划。

Abstract: Amyotrophic lateral sclerosis (ALS) is a degenerative disorder of motor
neurons that causes progressive paralysis in patients. Current treatment
options aim to prolong survival and improve quality of life; however, due to
the heterogeneity of the disease, it is often difficult to determine the
optimal time for potential therapies or medical interventions. In this study,
we propose a novel method to predict the time until a patient with ALS
experiences significant functional impairment (ALSFRS-R<=2) with respect to
five common functions: speaking, swallowing, handwriting, walking and
breathing. We formulate this task as a multi-event survival problem and
validate our approach in the PRO-ACT dataset by training five covariate-based
survival models to estimate the probability of an event over a 500-day period
after a baseline visit. We then predict five event-specific individual survival
distributions (ISDs) for each patient, each providing an interpretable and
meaningful estimate of when that event will likely take place in the future.
The results show that covariate-based models are superior to the Kaplan-Meier
estimator at predicting time-to-event outcomes. Additionally, our method
enables practitioners to make individual counterfactual predictions, where
certain features (covariates) can be changed to see their effect on the
predicted outcome. In this regard, we find that Riluzole has little to no
impact on predicted functional decline. However, for patients with bulbar-onset
ALS, our method predicts considerably shorter counterfactual time-to-event
estimates for tasks related to speech and swallowing compared to limb-onset
ALS. The proposed method can be applied to current clinical examination data to
assess the risk of functional decline and thus allow more personalized
treatment planning.

</details>


### [544] [Comparison of spectrogram scaling in multi-label Music Genre Recognition](https://arxiv.org/abs/2506.02091)
*Bartosz Karpiński,Cyryl Leszczyński*

Main category: cs.SD

Relevance: 10.0

TL;DR: 论文探讨了音乐流派分类问题，比较了多种预处理方法和模型训练方法，使用了一个包含18000条手动标注的数据集。


<details>
  <summary>Details</summary>
Motivation: 随着数字音频工作站的普及，音乐数量激增，但流派界限模糊，需要更有效的分类方法。

Method: 采用了多种预处理方法和模型训练方法，使用了一个自定义的手动标注数据集。

Result: 实验结果表明，不同预处理和训练方法对音乐流派分类的效果有显著影响。

Conclusion: 论文为音乐流派分类提供了实用的方法和数据集，但未涉及LLM或基础模型架构。

Abstract: As the accessibility and ease-of-use of digital audio workstations increases,
so does the quantity of music available to the average listener; additionally,
differences between genres are not always well defined and can be abstract,
with widely varying combinations of genres across individual records. In this
article, multiple preprocessing methods and approaches to model training are
described and compared, accounting for the eclectic nature of today's albums. A
custom, manually labeled dataset of more than 18000 entries has been used to
perform the experiments.

</details>


### [545] [Olfactory Inertial Odometry: Methodology for Effective Robot Navigation by Scent](https://arxiv.org/abs/2506.02373)
*Kordel K. France,Ovidiu Daescu*

Main category: cs.RO

Relevance: 10.0

TL;DR: 论文提出了嗅觉惯性里程计（OIO）框架，结合惯性运动学和快速采样嗅觉传感器，实现类似视觉惯性里程计（VIO）的嗅觉导航。


<details>
  <summary>Details</summary>
Motivation: 嗅觉导航是生物探索的原始机制之一，但机器嗅觉导航的模拟和解决非常困难。研究旨在填补这一空白。

Method: 借鉴SLAM和VIO的原理，提出OIO框架，并在5自由度机器人手臂上测试了三种气味定位算法。

Result: 成功建立了OIO的基线框架，为嗅觉导航的后续研究奠定了基础，并指出了未来复杂任务的性能改进方向。

Conclusion: OIO为嗅觉导航提供了可行的解决方案，未来可进一步优化以适应更复杂的应用场景。

Abstract: Olfactory navigation is one of the most primitive mechanisms of exploration
used by organisms. Navigation by machine olfaction (artificial smell) is a very
difficult task to both simulate and solve. With this work, we define olfactory
inertial odometry (OIO), a framework for using inertial kinematics, and
fast-sampling olfaction sensors to enable navigation by scent analogous to
visual inertial odometry (VIO). We establish how principles from SLAM and VIO
can be extrapolated to olfaction to enable real-world robotic tasks. We
demonstrate OIO with three different odour localization algorithms on a real
5-DoF robot arm over an odour-tracking scenario that resembles real
applications in agriculture and food quality control. Our results indicate
success in establishing a baseline framework for OIO from which other research
in olfactory navigation can build, and we note performance enhancements that
can be made to address more complex tasks in the future.

</details>


### [546] [Agnostic Learning under Targeted Poisoning: Optimal Rates and the Role of Randomness](https://arxiv.org/abs/2506.03075)
*Bogdan Chornomaz,Yonatan Koren,Shay Moran,Tom Waknine*

Main category: cs.LG

Relevance: 1.0

TL;DR: Error


<details>
  <summary>Details</summary>
Motivation: Error

Method: Error

Result: Error

Conclusion: Error

Abstract: We study the problem of learning in the presence of an adversary that can
corrupt an $\eta$ fraction of the training examples with the goal of causing
failure on a specific test point. In the realizable setting, prior work
established that the optimal error under such instance-targeted poisoning
attacks scales as $\Theta(d\eta)$, where $d$ is the VC dimension of the
hypothesis class arXiv:2210.02713. In this work, we resolve the corresponding
question in the agnostic setting. We show that the optimal excess error is
$\tilde{\Theta}(\sqrt{d\eta})$, answering one of the main open problems left by
Hanneke et al. To achieve this rate, it is necessary to use randomized
learners: Hanneke et al. showed that deterministic learners can be forced to
suffer error close to 1, even under small amounts of poisoning. Perhaps
surprisingly, our upper bound remains valid even when the learner's random bits
are fully visible to the adversary . In the other direction, our lower bound is
stronger than standard PAC-style bounds: instead of tailoring a hard
distribution separately for each sample size, we exhibit a single fixed
distribution under which the adversary can enforce an excess error of
$\Omega(\sqrt{d\eta})$ infinitely often.

</details>


### [547] [Large Stepsizes Accelerate Gradient Descent for Regularized Logistic Regression](https://arxiv.org/abs/2506.02336)
*Jingfeng Wu,Pierre Marion,Peter Bartlett*

Main category: stat.ML

Relevance: 1.0

TL;DR: Error


<details>
  <summary>Details</summary>
Motivation: Error

Method: Error

Result: Error

Conclusion: Error

Abstract: We study gradient descent (GD) with a constant stepsize for
$\ell_2$-regularized logistic regression with linearly separable data.
Classical theory suggests small stepsizes to ensure monotonic reduction of the
optimization objective, achieving exponential convergence in
$\widetilde{\mathcal{O}}(\kappa)$ steps with $\kappa$ being the condition
number. Surprisingly, we show that this can be accelerated to
$\widetilde{\mathcal{O}}(\sqrt{\kappa})$ by simply using a large stepsize --
for which the objective evolves nonmonotonically. The acceleration brought by
large stepsizes extends to minimizing the population risk for separable
distributions, improving on the best-known upper bounds on the number of steps
to reach a near-optimum. Finally, we characterize the largest stepsize for the
local convergence of GD, which also determines the global convergence in
special scenarios. Our results extend the analysis of Wu et al. (2024) from
convex settings with minimizers at infinity to strongly convex cases with
finite minimizers.

</details>
