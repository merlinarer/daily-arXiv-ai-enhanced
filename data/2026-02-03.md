<div id=toc></div>

# Table of Contents

- [cs.CL](#cs.CL) [Total: 186]
- [cs.CV](#cs.CV) [Total: 304]
- [cs.AI](#cs.AI) [Total: 290]
- [cs.LG](#cs.LG) [Total: 513]


<div id='cs.CL'></div>

# cs.CL [[Back]](#toc)

### [1] [G-MemLLM: Gated Latent Memory Augmentation for Long-Context Reasoning in Large Language Models](https://arxiv.org/abs/2602.00015)
*Xun Xu*

Main category: cs.CL

Relevance: 85.0

TL;DR: G-MemLLM是一种记忆增强的LLM架构，通过可训练的潜在记忆库和GRU风格的门控更新机制，解决长上下文信息衰减问题，显著提升多跳推理和关系抽取性能。


<details>
  <summary>Details</summary>
Motivation: 现有LLM受限于有限的上下文窗口，在长序列多跳推理中难以保持事实一致性。现有方法如上下文压缩或循环标记存在"上下文腐化"和信息稀释问题。

Method: 提出G-MemLLM架构，将冻结的LLM主干与可训练的潜在记忆库结合，采用GRU风格的门控更新逻辑，选择性更新、保留或覆盖潜在记忆槽，防止循环系统中的梯度消失。

Result: 在GPT-2(124M)到Llama 3.1(8B)不同规模模型上评估，在HotpotQA和ZsRE基准上显著提升性能：Llama 3.1-8B在ZsRE上准确率提升13.3%，GPT-2在HotpotQA上Answer F1提升8.56分，Llama 3.1-8B在HotpotQA上Supporting Fact F1提升6.89分。

Conclusion: G-MemLLM通过记忆增强机制有效解决了长上下文信息衰减问题，提升了LLM的多跳推理和关系抽取能力，且在不同规模模型上均表现出良好的可扩展性。

Abstract: Large Language Models (LLMs) have demonstrated remarkable capabilities in natural language understanding, yet they remain constrained by the finite capacity of their context windows and the inherent difficulty of maintaining long-term factual consistency during multi-hop reasoning. While existing methods utilize context compression or recurrent tokens, they often suffer from ``context rot'' or the dilution of information over long horizons. In this paper, we propose \textbf{G-MemLLM}, a memory-augmented architecture that integrates a frozen LLM backbone with a trainable \textbf{Latent Memory Bank}. Our key innovation is a GRU-style gated update logic that allows the model to selectively update, preserve, or overwrite latent memory slots, preventing the vanishing gradients of knowledge common in recurrent systems. We evaluate G-MemLLM across scales, from GPT-2 (124M) to Llama 3.1 (8B), on the HotpotQA and Zero-Shot Relation Extraction (ZsRE) benchmarks. Our results demonstrate that G-MemLLM significantly enhances multi-hop reasoning and relational precision, achieving a 13.3\% accuracy boost on ZsRE for Llama 3.1-8B, and it also yields improvements across model scales, boosting Answer F1 by 8.56 points for GPT-2 and increasing Supporting Fact F1 by 6.89 points for Llama 3.1-8B on HotpotQA.

</details>


### [2] [PTCBENCH: Benchmarking Contextual Stability of Personality Traits in LLM Systems](https://arxiv.org/abs/2602.00016)
*Jiongchi Yu,Yuhan Ma,Xiaoyu Zhang,Junjie Wang,Qiang Hu,Chao Shen,Xiaofei Xie*

Main category: cs.CL

Relevance: 85.0

TL;DR: PTCBENCH：一个系统性基准，用于在受控情境下量化LLM人格一致性，揭示外部场景如何显著影响LLM人格特质和推理能力


<details>
  <summary>Details</summary>
Motivation: 随着LLM在情感代理和AI系统中的部署增加，保持一致且真实的人格对于用户信任和参与至关重要。现有研究忽视了心理学共识——人格特质是动态且情境依赖的，需要建立评估框架来填补这一空白。

Method: 引入PTCBENCH基准，将模型置于12种不同的外部情境（地理位置和生活事件），使用NEO五因素人格量表系统评估人格特质，共收集39,240个人格特质记录。

Result: 研究发现某些外部场景（如"失业"）能显著触发LLM人格变化，甚至改变其推理能力。基准建立了可扩展的评估框架，为开发稳健且心理对齐的AI系统提供可行见解。

Conclusion: PTCBENCH为在现实、动态环境中评估人格一致性建立了可扩展框架，为开发稳健且心理对齐的AI系统提供了重要工具和见解。

Abstract: With the increasing deployment of large language models (LLMs) in affective agents and AI systems, maintaining a consistent and authentic LLM personality becomes critical for user trust and engagement. However, existing work overlooks a fundamental psychological consensus that personality traits are dynamic and context-dependent. To bridge this gap, we introduce PTCBENCH, a systematic benchmark designed to quantify the consistency of LLM personalities under controlled situational contexts. PTCBENCH subjects models to 12 distinct external conditions spanning diverse location contexts and life events, and rigorously assesses the personality using the NEO Five-Factor Inventory. Our study on 39,240 personality trait records reveals that certain external scenarios (e.g., "Unemployment") can trigger significant personality changes of LLMs, and even alter their reasoning capabilities. Overall, PTCBENCH establishes an extensible framework for evaluating personality consistency in realistic, evolving environments, offering actionable insights for developing robust and psychologically aligned AI systems.

</details>


### [3] [Construct, Align, and Reason: Large Ontology Models for Enterprise Knowledge Management](https://arxiv.org/abs/2602.00029)
*Yao Zhang,Hongyin Zhu*

Main category: cs.CL

Relevance: 85.0

TL;DR: 论文提出大型本体模型(LOM)，通过构建-对齐-推理的统一框架解决企业知识管理中的多源异构数据整合和语义推理问题，在4B参数规模下在复杂图推理任务上超越DeepSeek-V3.2。


<details>
  <summary>Details</summary>
Motivation: 企业级知识管理面临多源异构数据整合和有效语义推理的挑战。传统知识图谱在隐式关系发现和复杂问答的语义理解方面存在不足，需要新的框架来融合结构化数据库和非结构化文本，实现指令对齐的推理能力。

Method: 提出统一的三阶段训练流程：1) 本体指令微调以提升结构理解；2) 文本-本体对齐以增强节点语义编码；3) 基于课程学习的多任务指令调优，处理本体-语言对以增强语义推理和生成能力。首先从结构化数据库和非结构化文本构建双层企业本体，然后融合成综合企业本体。

Result: 在构建的全面训练和评估数据集上，4B参数的LOM达到89.47%的准确率，在复杂图推理任务上超越DeepSeek-V3.2，表明本体结构和语言的有效融合。

Conclusion: LOM框架成功解决了企业知识管理中多源数据整合和语义推理的挑战，通过统一的三阶段训练实现了本体结构与语言的有效融合，为复杂知识推理任务提供了有效解决方案。

Abstract: Enterprise-scale knowledge management faces significant challenges in integrating multi-source heterogeneous data and enabling effective semantic reasoning. Traditional knowledge graphs often struggle with implicit relationship discovery and lack sufficient semantic understanding for complex question answering. To address these limitations, we introduce a unified construct--align--reason framework, the large ontology model (LOM). We first build a dual-layer enterprise ontology from structured databases and unstructured text, subsequently fusing these sources into a comprehensive enterprise ontology. To enable instruction-aligned reasoning, we propose a unified three-stage training pipeline: ontology instruction fine-tuning to improve structural understanding; text-ontology grounding to strengthen node semantic encoding; and multi-task instruction tuning on ontology-language pairs with curriculum learning to enhance semantic reasoning and generation. We also construct comprehensive training and evaluation datasets covering diverse ontology reasoning tasks. On this benchmark, our 4B-parameter LOM achieves 89.47% accuracy and outperforms DeepSeek-V3.2 on complex graph reasoning, indicating effective fusion of ontology structure and language.

</details>


### [4] [DIVERGE: Diversity-Enhanced RAG for Open-Ended Information Seeking](https://arxiv.org/abs/2602.00238)
*Tianyi Hu,Niket Tandon,Akhil Arora*

Main category: cs.CL

Relevance: 85.0

TL;DR: DIVERGE是一个即插即用的代理式RAG框架，通过反思引导生成和记忆增强迭代优化来解决现有RAG系统在开放性问题中多样性不足的问题，实现了多样性与质量的平衡。


<details>
  <summary>Details</summary>
Motivation: 现有RAG系统假设每个查询只有一个正确答案，忽视了常见的信息寻求场景中可能存在多个合理答案。这种设计导致系统无法充分利用检索到的上下文多样性，限制了创造性，并损害了公平包容的信息获取。

Method: 提出DIVERGE框架，包含反思引导生成和记忆增强迭代优化机制。该框架是即插即用的代理式RAG系统，通过新颖的反思机制促进多样化观点生成，同时保持答案质量。

Result: 在真实世界的Infinity-Chat数据集上，DIVERGE相比竞争基线和先前最先进方法实现了最佳的多样性-质量权衡，显著提升了多样性同时保持质量。提出的新评估指标与人类判断相关性良好。

Conclusion: 当前基于LLM的系统在开放信息寻求场景中存在系统性限制，明确建模多样性可以缓解这一问题。DIVERGE框架为解决RAG系统中的多样性不足问题提供了有效方案。

Abstract: Existing retrieval-augmented generation (RAG) systems are primarily designed under the assumption that each query has a single correct answer. This overlooks common information-seeking scenarios with multiple plausible answers, where diversity is essential to avoid collapsing to a single dominant response, thereby constraining creativity and compromising fair and inclusive information access. Our analysis reveals a commonly overlooked limitation of standard RAG systems: they underutilize retrieved context diversity, such that increasing retrieval diversity alone does not yield diverse generations. To address this limitation, we propose DIVERGE, a plug-and-play agentic RAG framework with novel reflection-guided generation and memory-augmented iterative refinement, which promotes diverse viewpoints while preserving answer quality. We introduce novel metrics tailored to evaluating the diversity-quality trade-off in open-ended questions, and show that they correlate well with human judgments. We demonstrate that DIVERGE achieves the best diversity-quality trade-off compared to competitive baselines and previous state-of-the-art methods on the real-world Infinity-Chat dataset, substantially improving diversity while maintaining quality. More broadly, our results reveal a systematic limitation of current LLM-based systems for open-ended information-seeking and show that explicitly modeling diversity can mitigate it. Our code is available at: https://github.com/au-clan/Diverge

</details>


### [5] [Benchmarking Uncertainty Calibration in Large Language Model Long-Form Question Answering](https://arxiv.org/abs/2602.00279)
*Philip Müller,Nicholas Popovič,Michael Färber,Peter Steinbach*

Main category: cs.CL

Relevance: 85.0

TL;DR: 该论文提出了首个用于评估LLM在科学问答中不确定性量化方法的大规模基准，分析了20个不同LLM在7个科学QA数据集上的校准表现，发现指令微调导致概率极化，而一致性采样是最可靠的校准方法。


<details>
  <summary>Details</summary>
Motivation: 现有不确定性量化方法在科学问答领域验证不足，而科学QA依赖事实检索和推理能力，需要可靠的不确定性量化来建立对生成答案的信任。

Method: 构建大规模基准框架，评估20个LLM（基础模型、指令微调模型、推理专用模型）在7个科学QA数据集上的表现，分析685,000个长格式回答，比较不同不确定性量化方法（包括token级概率、序列级verbalized置信度、答案频率等）。

Result: 1) 指令微调导致token级概率质量极化，降低不确定性估计可靠性；2) 推理过程能部分缓解此效应；3) verbalized方法存在系统性偏差且与正确性相关性差；4) 答案频率（跨样本一致性）校准最可靠；5) 仅依赖ECE作为评估指标会产生误导。

Conclusion: 当前LLM不确定性量化方法存在严重局限，标准基准评估实践需要改进，答案频率方法在科学推理QA中表现最佳，为可信AI提供了重要见解。

Abstract: Large Language Models (LLMs) are commonly used in Question Answering (QA) settings, increasingly in the natural sciences if not science at large. Reliable Uncertainty Quantification (UQ) is critical for the trustworthy uptake of generated answers. Existing UQ approaches remain weakly validated in scientific QA, a domain relying on fact-retrieval and reasoning capabilities. We introduce the first large-scale benchmark for evaluating UQ metrics in reasoning-demanding QA studying calibration of UQ methods, providing an extensible open-source framework to reproducibly assess calibration. Our study spans up to 20 large language models of base, instruction-tuned and reasoning variants. Our analysis covers seven scientific QA datasets, including both multiple-choice and arithmetic question answering tasks, using prompting to emulate an open question answering setting. We evaluate and compare methods representative of prominent approaches on a total of 685,000 long-form responses, spanning different reasoning complexities representative of domain-specific tasks. At the token level, we find that instruction tuning induces strong probability mass polarization, reducing the reliability of token-level confidences as estimates of uncertainty. Models further fine-tuned for reasoning are exposed to the same effect, but the reasoning process appears to mitigate it depending on the provider. At the sequence level, we show that verbalized approaches are systematically biased and poorly correlated with correctness, while answer frequency (consistency across samples) yields the most reliable calibration. In the wake of our analysis, we study and report the misleading effect of relying exclusively on ECE as a sole measure for judging performance of UQ methods on benchmark datasets. Our findings expose critical limitations of current UQ methods for LLMs and standard practices in benchmarking thereof.

</details>


### [6] [Faithful-Patchscopes: Understanding and Mitigating Model Bias in Hidden Representations Explanation of Large Language Models](https://arxiv.org/abs/2602.00300)
*Xilin Gong,Shu Yang,Zehua Cao,Lynne Billard,Di Wang*

Main category: cs.CL

Relevance: 85.0

TL;DR: 本文揭示了Patchscopes框架中LLM解码隐藏表示时的系统不忠实问题：LLM倾向于依赖固有语言模式而非上下文信息，并提出BALOR方法通过logit重校准来抑制模型偏见、增强上下文信息。


<details>
  <summary>Details</summary>
Motivation: Patchscopes框架使用LLM自身解码隐藏表示来生成人类可读解释，但研究发现LLM在解码时倾向于依赖固有语言模式，这会覆盖隐藏表示中编码的上下文信息，导致解释不忠实。例如，即使隐藏表示编码了"紫色"的上下文属性，LLM仍会生成"绿色"（基于先验关联）。

Method: 首先设计数据集评估Patchscopes在偏见情况下的忠实性；然后提出BALOR方法，将未修补提示的输出logits视为模型偏见，与修补上下文信息下的logits对比，通过这种对比重新校准logit分布，抑制模型偏见并增强生成时的上下文信息。

Result: 实验显示Patchscopes在偏见情况下平均忠实性下降18.84%；BALOR在多个LLM上一致优于现有基线，相对性能提升高达33%。

Conclusion: LLM在解码隐藏表示时存在系统不忠实问题，主要源于固有语言模式对上下文信息的覆盖；BALOR通过logit重校准有效缓解这一问题，提高了隐藏表示解释的忠实性。

Abstract: Large Language Models (LLMs) have demonstrated strong capabilities for hidden representation interpretation through Patchscopes, a framework that uses LLMs themselves to generate human-readable explanations by decoding from internal hidden representations. However, our work shows that LLMs tend to rely on inherent linguistic patterns, which can override contextual information encoded in the hidden representations during decoding. For example, even when a hidden representation encodes the contextual attribute "purple" for "broccoli", LLMs still generate "green" in their explanations, reflecting a strong prior association. This behavior reveals a systematic unfaithfulness in Patchscopes. To systematically study this issue, we first designed a dataset to evaluate the faithfulness of Patchscopes under biased cases, and our results show that there is an 18.84\% faithfulness decrease on average. We then propose Bias Alignment through Logit Recalibration (BALOR), which treats the output logits from an unpatched prompt as capturing model bias and contrasts them with logits obtained under patched contextual information. By recalibrating the logit distribution through this contrast, BALOR suppresses model bias and amplifies contextual information during generation. Experiments across multiple LLMs demonstrate that BALOR consistently outperforms existing baselines, achieving up to 33\% relative performance improvement.

</details>


### [7] [DecompressionLM: Deterministic, Diagnostic, and Zero-Shot Concept Graph Extraction from Language Models](https://arxiv.org/abs/2602.00377)
*Zhaochen Hong,Jiaxuan You*

Main category: cs.CL

Relevance: 85.0

TL;DR: DecompressionLM：一种零样本概念图提取框架，无需预定义查询即可发现语言模型编码的知识，用于评估压缩模型的知识覆盖度


<details>
  <summary>Details</summary>
Motivation: 现有知识探测方法依赖预定义查询，只能提取已知概念。需要一种无需预定义查询或跨序列共享状态的方法，来发现语言模型实际编码的知识，特别是评估压缩模型的知识覆盖度

Method: 使用Van der Corput低差异序列和算术解码的stateless框架，实现确定性、可并行生成，无需跨序列共享状态。针对传统解码探测方法的三个限制：跨序列耦合、竞争解码效应和可扩展性约束

Result: 发现激活感知量化(AWQ-4bit)将概念覆盖度扩展30-170%，而均匀量化(GPTQ-Int4)导致71-86%覆盖度崩溃。MMLU-Pro Law模型间存在17点的幻觉差距，困惑度无法可靠反映这些差异

Conclusion: DecompressionLM建立了概念覆盖度作为评估压缩模型知识广度和事实基础的新维度，对模型部署有重要意义

Abstract: Existing knowledge probing methods rely on pre-defined queries, limiting extraction to known concepts. We introduce DecompressionLM, a stateless framework for zero-shot concept graph extraction that discovers what language models encode without pre-specified queries or shared cross-sequence state. Our method targets three limitations of common decoding-based probing approaches: cross-sequence coupling that concentrates probability mass on high-frequency prefixes, competitive decoding effects that suppress long-tail concepts, and scalability constraints arising from sequential exploration. Using Van der Corput low-discrepancy sequences with arithmetic decoding, DecompressionLM enables deterministic, embarrassingly parallel generation without shared state across sequences. Across two model families and five quantization variants, we find that activation-aware quantization (AWQ-4bit) expands concept coverage by 30-170%, while uniform quantization (GPTQ-Int4) induces 71-86% coverage collapse -- divergent behaviors not reliably reflected by explanation-level perplexity. Corpus-based verification further reveals a 17-point hallucination gap between top- and bottom-ranked MMLU-Pro Law models. DecompressionLM establishes concept coverage as a complementary evaluation dimension for assessing knowledge breadth and factual grounding in compressed models useful for their deployment.

</details>


### [8] [Segment-Level Attribution for Selective Learning of Long Reasoning Traces](https://arxiv.org/abs/2602.00425)
*Siyuan Wang,Yanchen Liu,Xiang Ren*

Main category: cs.CL

Relevance: 85.0

TL;DR: 提出基于集成梯度归因的片段级选择性学习框架，通过归因强度和方向一致性识别重要推理片段，在SFT中只训练重要片段，提高推理模型的准确性和输出效率。


<details>
  <summary>Details</summary>
Motivation: 大型推理模型生成的长思维链中，只有小部分对答案预测有实际贡献，大部分包含重复或截断内容。这种输出冗余在监督微调后会进一步传播，模型学习模仿冗长但无信息的模式，从而降低性能。

Method: 使用集成梯度归因量化每个token对最终答案的影响，聚合成两个片段级指标：归因强度（整体归因大小）和方向一致性（片段内token归因方向的一致性）。基于这两个指标，提出片段级选择性学习框架，识别具有高归因强度但中等一致性的重要片段（反映深度推理而非浅层推理），对这些重要片段应用选择性SFT，同时对不重要片段屏蔽损失。

Result: 在多个模型和数据集上的实验表明，该方法提高了准确性和输出效率，使模型能够更有效地从长推理轨迹中学习。

Conclusion: 通过选择性学习重要推理片段，可以减少冗余内容的影响，提高推理模型的效率和性能，为长思维链的有效学习提供了新方法。

Abstract: Large Reasoning Models (LRMs) achieve strong reasoning performance by generating long chains of thought (CoTs), yet only a small fraction of these traces meaningfully contributes to answer prediction, while the majority contains repetitive or truncated content. Such output redundancy is further propagated after supervised finetuning (SFT), as models learn to imitate verbose but uninformative patterns, which can degrade performance. To this end, we incorporate integrated gradient attribution to quantify each token's influence on final answers and aggregate them into two segment-level metrics: (1) \textit{attribution strength} measures the overall attribution magnitude; and (2) \textit{direction consistency} captures whether tokens' attributions within a segment are uniformly positive or negative (high consistency), or a mixture of both (moderate consistency). Based on these two metrics, we propose a segment-level selective learning framework to identify important segments with high attribution strength but moderate consistency that indicate reflective rather than shallow reasoning. The framework then applies selective SFT on these important segments while masking loss for unimportant ones. Experiments across multiple models and datasets show that our approach improves accuracy and output efficiency, enabling more effective learning from long reasoning traces~\footnote{Code and data are available at https://github.com/SiyuanWangw/SegmentSelectiveSFT}.

</details>


### [9] [When Agents "Misremember" Collectively: Exploring the Mandela Effect in LLM-based Multi-Agent Systems](https://arxiv.org/abs/2602.00428)
*Naen Xu,Hengyu An,Shuo Shi,Jinghuai Zhang,Chunyi Zhou,Changjiang Li,Tianyu Du,Zhihui Fu,Jun Wang,Shouling Ji*

Main category: cs.CL

Relevance: 85.0

TL;DR: 该论文研究了基于LLM的多智能体系统中的曼德拉效应（集体记忆偏差），提出了MANBENCH基准来评估该现象，并提出了缓解策略，平均减少74.40%的效应。


<details>
  <summary>Details</summary>
Motivation: 尽管LLM增强了多智能体系统的能力，但智能体对集体认知偏差的易感性仍未充分探索。曼德拉效应作为集体错误记忆现象，在多智能体系统中可能传播错误信息，存在伦理问题，需要系统研究。

Method: 提出MANBENCH基准，包含4种易受曼德拉效应影响的任务类型和5种不同智能体角色与记忆时间尺度的交互协议。评估多个LLM驱动的智能体，分析影响因素，并提出缓解策略：提示级防御（认知锚定、来源审查）和模型级对齐防御。

Result: 量化了多智能体系统中的曼德拉效应，分析了不同因素的影响。提出的缓解策略平均减少74.40%的曼德拉效应，为开发更鲁棒的多智能体系统提供了实证基础。

Conclusion: 多智能体系统确实存在曼德拉效应，通过系统评估和针对性防御策略可以有效缓解。研究为开发更稳健、伦理对齐的协作多智能体系统提供了重要见解。

Abstract: Recent advancements in large language models (LLMs) have significantly enhanced the capabilities of collaborative multi-agent systems, enabling them to address complex challenges. However, within these multi-agent systems, the susceptibility of agents to collective cognitive biases remains an underexplored issue. A compelling example is the Mandela effect, a phenomenon where groups collectively misremember past events as a result of false details reinforced through social influence and internalized misinformation. This vulnerability limits our understanding of memory bias in multi-agent systems and raises ethical concerns about the potential spread of misinformation. In this paper, we conduct a comprehensive study on the Mandela effect in LLM-based multi-agent systems, focusing on its existence, causing factors, and mitigation strategies. We propose MANBENCH, a novel benchmark designed to evaluate agent behaviors across four common task types that are susceptible to the Mandela effect, using five interaction protocols that vary in agent roles and memory timescales. We evaluate agents powered by several LLMs on MANBENCH to quantify the Mandela effect and analyze how different factors affect it. Moreover, we propose strategies to mitigate this effect, including prompt-level defenses (e.g., cognitive anchoring and source scrutiny) and model-level alignment-based defense, achieving an average 74.40% reduction in the Mandela effect compared to the baseline. Our findings provide valuable insights for developing more resilient and ethically aligned collaborative multi-agent systems.

</details>


### [10] [What Matters to an LLM? Behavioral and Computational Evidences from Summarization](https://arxiv.org/abs/2602.00459)
*Yongxin Zhou,Changshun Wu,Philippe Mulhem,Didier Schwab,Maxime Peyrard*

Main category: cs.CL

Relevance: 85.0

TL;DR: 该论文通过行为分析和计算分析相结合的方法，研究LLM在文本摘要任务中的内部重要性概念，发现LLM在重要性选择上具有一致性模式，且某些注意力头与重要性分布对齐，中后层网络对重要性预测能力强。


<details>
  <summary>Details</summary>
Motivation: 尽管LLM在文本摘要任务上已达到SOTA水平，但其内部驱动信息选择的重要性概念仍然不透明。研究者希望揭示LLM在摘要任务中如何确定信息重要性，以及这种重要性在模型内部如何表征。

Method: 1. 行为分析：为每个文档生成一系列长度控制的摘要，基于每个信息单元被选择的频率推导经验重要性分布；2. 计算分析：识别与经验重要性分布对齐的注意力头，分析不同网络层对重要性预测的能力。

Result: 1. LLM在重要性选择上呈现一致模式，与pre-LLM基线显著不同；2. LLM按模型家族聚类而非按模型大小；3. 某些注意力头与重要性分布良好对齐；4. 中后层网络对重要性预测能力最强。

Conclusion: 该研究初步揭示了LLM在摘要任务中的信息优先级及其内部表征方式，为解释和最终控制这些模型中的信息选择开辟了路径。

Abstract: Large Language Models (LLMs) are now state-of-the-art at summarization, yet the internal notion of importance that drives their information selections remains hidden. We propose to investigate this by combining behavioral and computational analyses. Behaviorally, we generate a series of length-controlled summaries for each document and derive empirical importance distributions based on how often each information unit is selected. These reveal that LLMs converge on consistent importance patterns, sharply different from pre-LLM baselines, and that LLMs cluster more by family than by size. Computationally, we identify that certain attention heads align well with empirical importance distributions, and that middle-to-late layers are strongly predictive of importance. Together, these results provide initial insights into what LLMs prioritize in summarization and how this priority is internally represented, opening a path toward interpreting and ultimately controlling information selection in these models.

</details>


### [11] [Intention-Adaptive LLM Fine-Tuning for Text Revision Generation](https://arxiv.org/abs/2602.00477)
*Zhexiong Liu,Diane Litman*

Main category: cs.CL

Relevance: 85.0

TL;DR: 本文提出Intention-Tuning框架，通过动态选择LLM层来学习意图表示，并将其迁移到修订生成任务，在小规模修订语料上实现高效微调。


<details>
  <summary>Details</summary>
Motivation: LLMs在基于上下文的文本生成任务中表现出色，但在基于意图的生成任务（如修订生成）中应用不足。现有方法难以处理复杂的多意图场景，而全量微调需要大量标注数据，这在修订领域成本高昂且稀缺。

Method: 提出Intention-Tuning框架：1）意图自适应层选择机制，动态选择LLM层学习意图表示；2）将学习到的意图表示迁移到修订生成任务；3）在小规模修订语料上进行高效微调。

Result: 实验表明Intention-Tuning在小规模修订语料上有效且高效，优于多个参数高效微调（PEFT）基线方法。

Conclusion: Intention-Tuning为基于意图的生成任务提供了一种数据高效的微调方法，能够处理复杂的多意图场景，在修订生成等任务中具有应用价值。

Abstract: Large Language Models (LLMs) have achieved impressive capabilities in various context-based text generation tasks, such as summarization and reasoning; however, their applications in intention-based generation tasks remain underexplored. One such example is revision generation, which requires the generated text to explicitly reflect the writer's actual intentions. Identifying intentions and generating desirable revisions are challenging due to their complex and diverse nature. Although prior work has employed LLMs to generate revisions with few-shot learning, they struggle with handling entangled multi-intent scenarios. While fine-tuning LLMs using intention-based instructions appears promising, it demands large amounts of annotated data, which is expensive and scarce in the revision community. To address these challenges, we propose Intention-Tuning, an intention-adaptive layer-wise LLM fine-tuning framework that dynamically selects a subset of LLM layers to learn the intentions and subsequently transfers their representations to revision generation. Experimental results suggest that Intention-Tuning is effective and efficient on small revision corpora, outperforming several PEFT baselines.

</details>


### [12] [From Knowledge to Inference: Scaling Laws of Specialized Reasoning on GlobalHealthAtlas](https://arxiv.org/abs/2602.00491)
*Zhaokun Yan,Zhaohan Liu,Wuzheng Dong,Lijie Feng,Chengxiao Dai*

Main category: cs.CL

Relevance: 85.0

TL;DR: 提出了GlobalHealthAtlas数据集，包含28万+多语言公共卫生推理实例，涵盖15个领域和3个难度级别，并设计了LLM辅助的质量控制流程和领域对齐评估器。


<details>
  <summary>Details</summary>
Motivation: 公共卫生推理需要基于科学证据、专家共识和安全约束的群体层面推断，但作为结构化机器学习问题仍未被充分探索，缺乏监督信号和基准测试。

Method: 1) 构建大规模多语言数据集（280,210个实例，15个公共卫生领域，17种语言，3个难度级别）；2) 设计LLM辅助的构建和质量控制流程（检索、去重、证据基础检查、标签验证）；3) 提出领域对齐评估器，基于多样化LLM的高置信度判断评估六个维度。

Result: 创建了首个大规模多语言公共卫生推理数据集，实现了可扩展的质量控制，开发了多维度的评估框架，支持超越传统QA基准的安全关键公共卫生推理训练和评估。

Conclusion: GlobalHealthAtlas为安全关键的公共卫生推理提供了可重复的训练和评估基础，填补了该领域结构化机器学习问题的空白。

Abstract: Public health reasoning requires population level inference grounded in scientific evidence, expert consensus, and safety constraints. However, it remains underexplored as a structured machine learning problem with limited supervised signals and benchmarks. We introduce \textbf{GlobalHealthAtlas}, a large scale multilingual dataset of 280,210 instances spanning 15 public health domains and 17 languages, stratified into three difficulty levels from health literacy to epidemiological and policy reasoning. Instances are derived from openly available public health sources and labeled by language, domain, and difficulty to support supervised learning and slice based evaluation. We further propose large language model (LLM) assisted construction and quality control pipeline with retrieval, duplication, evidence grounding checks, and label validation to improve consistency at scale. Finally, we present a domain aligned evaluator distilled from high confidence judgments of diverse LLMs to assess outputs along six dimensions: Accuracy, Reasoning, Completeness, Consensus Alignment, Terminology Norms, and Insightfulness. Together, these contributions enable reproducible training and evaluation of LLMs for safety critical public health reasoning beyond conventional QA benchmarks.

</details>


### [13] [Jailbreaking LLMs via Calibration](https://arxiv.org/abs/2602.00619)
*Yuxuan Lu,Yongkang Guo,Yuqing Kong*

Main category: cs.CL

Relevance: 85.0

TL;DR: 论文提出将大语言模型安全对齐视为对预对齐数据分布的系统性扭曲，将弱到强越狱建模为预测聚合问题，推导出最优聚合策略（梯度偏移），并展示了现有logit算术越狱方法是该框架在交叉熵损失下的特例。


<details>
  <summary>Details</summary>
Motivation: 大语言模型的安全对齐通常会在模型的对齐输出与底层预对齐数据分布之间产生系统性差异。现有越狱方法缺乏统一的理论框架来解释安全对齐如何影响下一个token预测，以及如何有效利用这种系统性差异进行越狱攻击。

Method: 将安全对齐对下一个token预测的影响建模为对预对齐分布的系统性扭曲，将弱到强越狱问题形式化为预测聚合问题。在损失诱导的对偶空间中推导出最优聚合策略（梯度偏移），证明现有logit算术越狱方法是交叉熵损失下的特例，并提出新的混合聚合规则。

Result: 在红队基准测试和数学效用任务上的评估显示，该方法在攻击成功率上优于现有方法，特别是在安全加固的gpt-oss-120b模型上，同时保持了较低的"越狱税"（对模型效用的影响）。

Conclusion: 该框架为理解安全对齐对LLM预测的影响提供了统一的理论视角，推导出的最优聚合策略能够有效利用安全对齐引入的系统性差异进行越狱攻击，同时最小化对模型效用的负面影响。

Abstract: Safety alignment in Large Language Models (LLMs) often creates a systematic discrepancy between a model's aligned output and the underlying pre-aligned data distribution. We propose a framework in which the effect of safety alignment on next-token prediction is modeled as a systematic distortion of a pre-alignment distribution. We cast Weak-to-Strong Jailbreaking as a forecast aggregation problem and derive an optimal aggregation strategy characterized by a Gradient Shift in the loss-induced dual space. We show that logit-arithmetic jailbreaking methods are a special case of this framework under cross-entropy loss, and derive a broader family of aggregation rules corresponding to other proper losses. We also propose a new hybrid aggregation rule. Evaluations across red-teaming benchmarks and math utility tasks using frontier models demonstrate that our approach achieves superior Attack Success Rates and lower "Jailbreak Tax" compared with existing methods, especially on the safety-hardened gpt-oss-120b.

</details>


### [14] [LegalOne: A Family of Foundation Models for Reliable Legal Reasoning](https://arxiv.org/abs/2602.00642)
*Haitao Li,Yifan Chen,Shuo Miao,Qian Dong,Jia Chen,Yiran Hu,Junjie Chen,Minghao Qin,Qingyao Ai,Yiqun Liu,Cheng Luo,Quan Zhou,Ya Zhang,Jikun Hu*

Main category: cs.CL

Relevance: 85.0

TL;DR: LegalOne是中国法律领域的专业基础模型，通过三阶段训练流程实现法律推理：塑性调整采样进行领域适应、法律代理思维链蒸馏提取推理轨迹、课程强化学习提升推理能力，在多项法律任务上超越通用大模型。


<details>
  <summary>Details</summary>
Motivation: 通用大语言模型在法律领域应用受限，缺乏精确领域知识和复杂多步司法推理能力。需要专门针对法律领域设计的基础模型来解决这些挑战。

Method: 三阶段训练流程：1) 塑性调整采样(PAS)进行领域适应，平衡新知识获取和原有能力保留；2) 法律代理思维链蒸馏(LEAD)从原始法律文本提取结构化推理轨迹；3) 课程强化学习(RL)通过记忆、理解、推理的渐进过程提升推理能力。

Result: LegalOne在广泛法律任务上取得最先进性能，超越参数规模大得多的通用大模型，通过增强知识密度和效率实现优异表现。

Conclusion: LegalOne为法律AI领域提供了可信赖、可解释的基础模型，适用于高风险司法应用，公开了模型权重和LegalKit评估框架。

Abstract: While Large Language Models (LLMs) have demonstrated impressive general capabilities, their direct application in the legal domain is often hindered by a lack of precise domain knowledge and complexity of performing rigorous multi-step judicial reasoning. To address this gap, we present LegalOne, a family of foundational models specifically tailored for the Chinese legal domain. LegalOne is developed through a comprehensive three-phase pipeline designed to master legal reasoning. First, during mid-training phase, we propose Plasticity-Adjusted Sampling (PAS) to address the challenge of domain adaptation. This perplexity-based scheduler strikes a balance between the acquisition of new knowledge and the retention of original capabilities, effectively establishing a robust legal foundation. Second, during supervised fine-tuning, we employ Legal Agentic CoT Distillation (LEAD) to distill explicit reasoning from raw legal texts. Unlike naive distillation, LEAD utilizes an agentic workflow to convert complex judicial processes into structured reasoning trajectories, thereby enforcing factual grounding and logical rigor. Finally, we implement a Curriculum Reinforcement Learning (RL) strategy. Through a progressive reinforcement process spanning memorization, understanding, and reasoning, LegalOne evolves from simple pattern matching to autonomous and reliable legal reasoning. Experimental results demonstrate that LegalOne achieves state-of-the-art performance across a wide range of legal tasks, surpassing general-purpose LLMs with vastly larger parameter counts through enhanced knowledge density and efficiency. We publicly release the LegalOne weights and the LegalKit evaluation framework to advance the field of Legal AI, paving the way for deploying trustworthy and interpretable foundation models in high-stakes judicial applications.

</details>


### [15] [EchoReview: Learning Peer Review from the Echoes of Scientific Citations](https://arxiv.org/abs/2602.00733)
*Yinuo Zhang,Dingcheng Huang,Haifeng Suo,Yizhuo Li,Ziya Zhao,Junhao Xu,Zhiying Tu,Dianhui Chu,Deming Zhai,Xianming Liu,Xiaoyan Yu,Dianbo Sui*

Main category: cs.CL

Relevance: 85.0

TL;DR: EchoReview是一个基于学术引用的数据合成框架，通过挖掘引文中的集体评价信号来生成结构化评审数据，训练出自动评审模型EchoReviewer-7B，在证据支持和评审全面性等维度表现优异。


<details>
  <summary>Details</summary>
Motivation: 随着科学投稿量快速增长，传统同行评审系统面临可扩展性压力，需要既可扩展又可靠的自动评审方法。现有基于真实评审数据的监督微调方法受限于数据单一来源以及人类评审的主观性和不一致性，难以支持高质量的自动评审。

Method: 提出EchoReview框架，通过挖掘学术引用中的隐含集体评价信号，将科学界的长期判断转化为结构化评审风格数据。构建了首个大规模、跨会议、跨年份的引用驱动评审数据集EchoReview-16K，并训练了自动评审模型EchoReviewer-7B。

Result: 实验结果表明，EchoReviewer-7B在证据支持和评审全面性等核心评审维度上取得了显著且稳定的改进，验证了引用上下文作为可靠自动同行评审的稳健有效数据范式。

Conclusion: 引用上下文可以作为可靠自动同行评审的稳健有效数据范式，EchoReview框架为解决传统评审系统的可扩展性问题提供了新途径。

Abstract: As the volume of scientific submissions continues to grow rapidly, traditional peer review systems are facing unprecedented scalability pressures, highlighting the urgent need for automated reviewing methods that are both scalable and reliable. Existing supervised fine-tuning approaches based on real review data are fundamentally constrained by single-source of data as well as the inherent subjectivity and inconsistency of human reviews, limiting their ability to support high-quality automated reviewers. To address these issues, we propose EchoReview, a citation-context-driven data synthesis framework that systematically mines implicit collective evaluative signals from academic citations and transforms scientific community's long-term judgments into structured review-style data. Based on this pipeline, we construct EchoReview-16K, the first large-scale, cross-conference, and cross-year citation-driven review dataset, and train an automated reviewer, EchoReviewer-7B. Experimental results demonstrate that EchoReviewer-7B can achieve significant and stable improvements on core review dimensions such as evidence support and review comprehensiveness, validating citation context as a robust and effective data paradigm for reliable automated peer review.

</details>


### [16] [CURP: Codebook-based Continuous User Representation for Personalized Generation with LLMs](https://arxiv.org/abs/2602.00742)
*Liang Wang,Xinyi Mou,Xiaoyou Liu,Xuanjing Huang,Zhongyu Wei*

Main category: cs.CL

Relevance: 85.0

TL;DR: CURP：一个用于LLM个性化生成的双向用户编码器与离散原型码本框架，通过约2000万参数实现即插即用的个性化，在个性化质量与计算效率间取得平衡。


<details>
  <summary>Details</summary>
Motivation: 现有基于提示或微调的用户建模方法难以平衡个性化质量与计算/数据效率，需要一种更高效、可解释且可扩展的个性化框架。

Method: 提出CURP框架，包含双向用户编码器和离散原型码本，提取多维用户特征，实现即插即用个性化（仅需约2000万可训练参数，占模型总大小约0.2%）。

Result: 在多种生成任务实验中，CURP相比基线方法表现出更优的性能和泛化能力，同时提供更好的可解释性和可扩展性。

Conclusion: CURP为LLM个性化提供了一种高效、可扩展的解决方案，在保持高质量个性化的同时显著降低了计算和参数需求。

Abstract: User modeling characterizes individuals through their preferences and behavioral patterns to enable personalized simulation and generation with Large Language Models (LLMs) in contemporary approaches. However, existing methods, whether prompt-based or training-based methods, face challenges in balancing personalization quality against computational and data efficiency. We propose a novel framework CURP, which employs a bidirectional user encoder and a discrete prototype codebook to extract multi-dimensional user traits. This design enables plug-and-play personalization with a small number of trainable parameters (about 20M parameters, about 0.2\% of the total model size). Through extensive experiments on variant generation tasks, we show that CURP achieves superior performance and generalization compared to strong baselines, while offering better interpretability and scalability. The code are available at https://github.com/RaidonWong/CURP_code

</details>


### [17] [Decouple Searching from Training: Scaling Data Mixing via Model Merging for Large Language Model Pre-training](https://arxiv.org/abs/2602.00747)
*Shengrui Li,Fei Zhao,Kaiyan Zhao,Jieying Ye,Haifeng Liu,Fangcheng Shi,Zheyong Xie,Yao Hu,Shaosheng Cao*

Main category: cs.CL

Relevance: 85.0

TL;DR: DeMix提出了一种通过模型融合预测最优数据混合比例的新框架，将搜索与训练成本解耦，从而以更低的搜索成本获得更优的数据混合方案。


<details>
  <summary>Details</summary>
Motivation: LLM预训练中确定有效的数据混合是关键因素，需要在通用能力和数学/代码等困难任务之间取得平衡。现有方法要么依赖不可靠的小规模代理实验，要么需要昂贵的大规模探索，因此需要更高效的数据混合优化方法。

Method: DeMix框架通过模型融合预测最优数据比例：首先在候选数据集上训练组件模型，然后通过加权模型融合推导数据混合代理，从而将搜索与训练成本解耦，无需为每个采样混合训练代理模型。

Result: 实验表明DeMix打破了充分性、准确性和效率之间的权衡，以更低的搜索成本获得更高的基准性能。同时发布了DeMix Corpora，一个包含22T令牌的高质量预训练数据集。

Conclusion: DeMix通过模型融合方法有效解决了LLM预训练数据混合优化问题，为开放研究提供了高质量的数据集和工具。

Abstract: Determining an effective data mixture is a key factor in Large Language Model (LLM) pre-training, where models must balance general competence with proficiency on hard tasks such as math and code. However, identifying an optimal mixture remains an open challenge, as existing approaches either rely on unreliable tiny-scale proxy experiments or require prohibitively expensive large-scale exploration. To address this, we propose Decouple Searching from Training Mix (DeMix), a novel framework that leverages model merging to predict optimal data ratios. Instead of training proxy models for every sampled mixture, DeMix trains component models on candidate datasets at scale and derives data mixture proxies via weighted model merging. This paradigm decouples search from training costs, enabling evaluation of unlimited sampled mixtures without extra training burden and thus facilitating better mixture discovery through more search trials. Extensive experiments demonstrate that DeMix breaks the trade-off between sufficiency, accuracy and efficiency, obtaining the optimal mixture with higher benchmark performance at lower search cost. Additionally, we release the DeMix Corpora, a comprehensive 22T-token dataset comprising high-quality pre-training data with validated mixtures to facilitate open research. Our code and DeMix Corpora is available at https://github.com/Lucius-lsr/DeMix.

</details>


### [18] [Adaptive Ability Decomposing for Unlocking Large Reasoning Model Effective Reinforcement Learning](https://arxiv.org/abs/2602.00759)
*Zhipeng Chen,Xiaobo Qin,Wayne Xin Zhao,Youbin Wu,Ji-Rong Wen*

Main category: cs.CL

Relevance: 85.0

TL;DR: 提出A²D方法，通过自适应能力分解增强RLVR效果，将复杂问题分解为简单子问题来指导推理器训练


<details>
  <summary>Details</summary>
Motivation: 现有RLVR方法由于信息有限，模型只能进行盲探索，在复杂问题上容易失败。需要在不依赖教师模型的情况下为RLVR过程提供额外信息。

Method: 1. 训练分解器：通过RLVR（无蒸馏）训练分解器，使其能将复杂问题分解为简单子问题
2. 数据标注：用分解器为训练集中的每个问题标注子问题
3. 训练推理器：在子问题指导下进行RLVR训练推理器

Result: 1. 性能优于竞争基线，证明方法有效性
2. 可作为即插即用模块应用于不同RLVR算法
3. 分解器分析揭示了RLVR过程如何影响其性能和行为，以及哪种指导更适合增强推理器的探索和利用能力

Conclusion: A²D方法通过自适应能力分解有效增强了RLVR的效果，为复杂问题推理提供了更好的指导，且具有通用性和可分析性

Abstract: Reinforcement learning with verifiable rewards (RLVR) has shown great potential to enhance the reasoning ability of large language models (LLMs). However, due to the limited amount of information provided during the RLVR process, the model can only engage in largely blind exploration, which often results in failure on challenging problems. To provide additional information for the RLVR process without relying on a teacher model, we propose A$^2$D, an Adaptive Ability Decomposing method for enhancing the effectiveness of RLVR. Specifically, we first train a decomposer via RLVR without distillation, enabling it to decompose complex questions into a set of simpler sub-questions. Next, we use this decomposer to annotate sub-questions for each question in the training dataset, and then train the reasoner under RLVR with sub-question guidance. To better understand A$^2$D, we first compare its performance with competitive baselines, showing its effectiveness. Next, we observe that our method functions as a plug-and-play module that can be applied to different RLVR algorithms. Furthermore, we conduct an analysis of the decomposer, revealing how the RLVR process affects its performance and behavior, and which type of guidance is better suited for enhancing the reasoner's exploration and exploitation abilities.

</details>


### [19] [APR: Penalizing Structural Redundancy in Large Reasoning Models via Anchor-based Process Rewards](https://arxiv.org/abs/2602.00760)
*Kaiyan Chang,Chenwei Zhu,Yingfeng Luo,Yifu Huo,Chenglong Wang,Xiaoqian Liu,Qiaozhi He,Tong Xiao,Zhengtao Yu,Jingbo Zhu*

Main category: cs.CL

Relevance: 85.0

TL;DR: 论文提出锚点过程奖励(APR)方法，通过识别推理锚点并惩罚答案稳定尾部，解决大推理模型中的过度思考问题，在数学推理任务上实现性能-效率帕累托前沿。


<details>
  <summary>Details</summary>
Motivation: 测试时缩放(TTS)增强了大推理模型(LRMs)的能力，但引入了过度思考的副作用。研究发现LRMs在推理过程中经常进行重复的自我验证而不修正答案，存在结构冗余。

Method: 提出锚点过程奖励(APR)：1) 定义推理锚点(答案首次稳定的位置)；2) 识别答案稳定尾部(AST，锚点后的冗余验证)；3) 使用适合长度惩罚的策略优化算法，专门惩罚AST部分。

Result: 在5个数学推理数据集上，APR方法在1.5B和7B规模模型上实现了性能-效率帕累托前沿，同时显著减少了RL训练的计算资源需求。

Conclusion: 通过定位推理锚点并惩罚答案稳定尾部，可以有效解决大推理模型的过度思考问题，在保持性能的同时提高推理效率。

Abstract: Test-Time Scaling (TTS) has significantly enhanced the capabilities of Large Reasoning Models (LRMs) but introduces a critical side-effect known as Overthinking. We conduct a preliminary study to rethink this phenomenon from a fine-grained perspective. We observe that LRMs frequently conduct repetitive self-verification without revision even after obtaining the final answer during the reasoning process. We formally define this specific position where the answer first stabilizes as the Reasoning Anchor. By analyzing pre- and post-anchor reasoning behaviors, we uncover the structural redundancy fixed in LRMs: the meaningless repetitive verification after deriving the first complete answer, which we term the Answer-Stable Tail (AST). Motivated by this observation, we propose Anchor-based Process Reward (APR), a structure-aware reward shaping method that localizes the reasoning anchor and penalizes exclusively the post-anchor AST. Leveraging the policy optimization algorithm suitable for length penalties, our APR models achieved the performance-efficiency Pareto frontier at 1.5B and 7B scales averaged across five mathematical reasoning datasets while requiring significantly fewer computational resources for RL training.

</details>


### [20] [Eliciting Trustworthiness Priors of Large Language Models via Economic Games](https://arxiv.org/abs/2602.00769)
*Siyu Yan,Lusha Zhu,Jian-Qiao Zhu*

Main category: cs.CL

Relevance: 85.0

TL;DR: 本文提出了一种基于迭代上下文学习的新方法，用于从大型语言模型中引出信任先验，发现GPT-4.1的信任先验与人类相似，并能基于玩家角色调整信任行为。


<details>
  <summary>Details</summary>
Motivation: 构建以人为本、可信赖的AI系统需要维持校准的信任水平，避免过度信任或信任不足。核心挑战在于如何表征AI系统自身表现出的信任水平，特别是如何从LLMs中引出信任先验。

Method: 提出基于迭代上下文学习的新引出方法，应用于行为博弈论中的信任游戏。信任游戏通过自愿暴露于基于对他人信念的风险来操作化信任。使用该方法从多个领先LLMs中引出信任先验，并进一步分析GPT-4.1对不同玩家角色的响应。

Result: GPT-4.1的信任先验与人类观察到的信任先验高度一致。GPT-4.1能够基于不同玩家角色调整信任行为，且引出的信任变化可以通过基于感知温暖和能力的刻板印象模型良好预测。

Conclusion: 该方法能够有效从LLMs中引出信任先验，GPT-4.1展现出与人类相似的信任模式，为理解AI系统的信任表征提供了新工具，对可信AI系统开发具有重要意义。

Abstract: One critical aspect of building human-centered, trustworthy artificial intelligence (AI) systems is maintaining calibrated trust: appropriate reliance on AI systems outperforms both overtrust (e.g., automation bias) and undertrust (e.g., disuse). A fundamental challenge, however, is how to characterize the level of trust exhibited by an AI system itself. Here, we propose a novel elicitation method based on iterated in-context learning (Zhu and Griffiths, 2024a) and apply it to elicit trustworthiness priors using the Trust Game from behavioral game theory. The Trust Game is particularly well suited for this purpose because it operationalizes trust as voluntary exposure to risk based on beliefs about another agent, rather than self-reported attitudes. Using our method, we elicit trustworthiness priors from several leading large language models (LLMs) and find that GPT-4.1's trustworthiness priors closely track those observed in humans. Building on this result, we further examine how GPT-4.1 responds to different player personas in the Trust Game, providing an initial characterization of how such models differentiate trust across agent characteristics. Finally, we show that variation in elicited trustworthiness can be well predicted by a stereotype-based model grounded in perceived warmth and competence.

</details>


### [21] [Reasoning as State Transition: A Representational Analysis of Reasoning Evolution in Large Language Models](https://arxiv.org/abs/2602.00770)
*Siyuan Zhang,Jialian Li,Yichi Zhang,Xiao Yang,Yinpeng Dong,Hang Su*

Main category: cs.CL

Relevance: 85.0

TL;DR: 本文通过表征视角研究LLM推理能力在训练过程中的演化，发现后训练对初始表征质量提升有限，但能驱动推理过程中表征分布向更优方向转变，且这种转变主要由生成token的语义而非额外计算驱动。


<details>
  <summary>Details</summary>
Motivation: 现有研究主要通过显式生成结果分析LLM推理能力的演化，将推理过程视为黑箱，无法揭示内部变化。本文旨在通过表征视角探究模型内部状态的动态变化，理解训练如何影响推理能力的提升。

Method: 采用表征分析方法，在不同训练阶段的模型上进行全面实验，分析静态初始表征质量和推理过程中的动态表征分布变化。通过比较分析、统计相关性检验和反事实实验，探究内部状态与外部输出的关系。

Result: 1) 后训练对静态初始表征质量提升有限；2) 推理任务与非推理任务不同，在生成过程中存在显著的连续表征分布偏移；3) 后训练使模型能够驱动这种分布偏移向更适合任务解决的方向转变；4) 生成正确性与最终表征高度相关；5) 表征转变主要由生成token的语义驱动，而非额外计算或参数差异。

Conclusion: 本文提供了对LLM推理过程及训练影响的新理解：推理能力提升主要源于后训练驱动的动态表征分布优化，而非静态表征质量改善。这为未来模型分析和优化提供了有价值的洞见。

Abstract: Large Language Models have achieved remarkable performance on reasoning tasks, motivating research into how this ability evolves during training. Prior work has primarily analyzed this evolution via explicit generation outcomes, treating the reasoning process as a black box and obscuring internal changes. To address this opacity, we introduce a representational perspective to investigate the dynamics of the model's internal states. Through comprehensive experiments across models at various training stages, we discover that post-training yields only limited improvement in static initial representation quality. Furthermore, we reveal that, distinct from non-reasoning tasks, reasoning involves a significant continuous distributional shift in representations during generation. Comparative analysis indicates that post-training empowers models to drive this transition toward a better distribution for task solving. To clarify the relationship between internal states and external outputs, statistical analysis confirms a high correlation between generation correctness and the final representations; while counterfactual experiments identify the semantics of the generated tokens, rather than additional computation during inference or intrinsic parameter differences, as the dominant driver of the transition. Collectively, we offer a novel understanding of the reasoning process and the effect of training on reasoning enhancement, providing valuable insights for future model analysis and optimization.

</details>


### [22] [HyLRA: Hybrid Layer Reuse Attention for Efficient Long-Context Inference](https://arxiv.org/abs/2602.00777)
*Xuan Ai,Qingqing Yang,Peng Wang,Lei Deng,Lin Zhang,Renhai Chen,Gong Zhang*

Main category: cs.CL

Relevance: 85.0

TL;DR: HyLRA是一种混合层复用注意力框架，通过层间相似性和层内敏感性分析，采用动态规划优化层间注意力策略，在敏感层保留完整注意力，在容忍层复用前层top-k索引，显著提升长上下文推理效率。


<details>
  <summary>Details</summary>
Motivation: 长上下文推理在大型语言模型中面临注意力二次计算复杂度和KV缓存内存占用的瓶颈。现有稀疏注意力方法依赖固定模式或激进剪枝，无法在效率和准确性之间达到最优平衡。

Method: 通过层间稀疏性分析发现注意力机制的双重特性：层内敏感性（某些层需要完整注意力）和层间相似性（相邻层共享关键token）。采用离线动态规划方法推导最优层间策略，敏感层保留完整注意力，容忍层直接复用前层top-k索引。

Result: HyLRA将推理吞吐量提升6%-46%，同时保持可比较的性能（准确度下降<1%），在多个基准测试中持续优于现有稀疏注意力方法。

Conclusion: HyLRA通过层间注意力复用策略有效克服了密集注意力的二次计算瓶颈，在保持模型性能的同时显著提升长上下文推理效率。

Abstract: Long-context inference in Large Language Models (LLMs) is bottlenecked by the quadratic computation complexity of attention and the substantial memory footprint of Key-Value (KV) caches. While existing sparse attention mechanisms attempt to mitigate this by exploiting inherent sparsity, they often rely on rigid patterns or aggressive pruning, failing to achieve an optimal balance between efficiency and accuracy. In this paper, we introduce {\bf HyLRA} ({\bf Hy}brid {\bf L}ayer {\bf R}euse {\bf A}ttention), a novel framework driven by layer-wise sparsity profiling. Our empirical analysis uncovers a dual characteristic in attention mechanics: \textit{intra-layer sensitivity}, where specific layers necessitate full attention to prevent feature distortion, and \textit{inter-layer similarity}, where consecutive layers share substantial critical tokens. Based on these observations, HyLRA employs an offline dynamic programming approach to derive an optimal layer-wise policy. This hybrid strategy retains full attention for sensitive layers to ensure robustness, while enabling tolerant layers to bypass quadratic calculations by directly reusing top-$k$ indices from preceding layers. This approach allows LLMs to restrict computation to the most critical tokens, effectively overcoming the quadratic bottleneck of dense attention. Extensive evaluations demonstrate that HyLRA improves inference throughput by 6\%--46\% while maintaining comparable performance (with $<1\%$ accuracy degradation), consistently outperforming state-of-the-art sparse attention methods. HyLRA is open source at \href{https://anonymous.4open.science/r/unified-cache-management-CF80/}{\texttt{/r/unified-cache-management-CF80/}}

</details>


### [23] [Omni-RRM: Advancing Omni Reward Modeling via Automatic Rubric-Grounded Preference Synthesis](https://arxiv.org/abs/2602.00846)
*Zicheng Kong,Dehua Ma,Zhenbo Xu,Alven Yang,Yiwei Ru,Haoran Wang,Zixuan Zhou,Fuqing Bie,Liuyu Xiang,Huijia Wu,Jian Zhao,Zhaofeng He*

Main category: cs.CL

Relevance: 85.0

TL;DR: Omni-RRM：首个开源的多模态奖励模型，通过结构化多维度偏好判断和维度理由，支持文本、图像、视频和音频，无需人工标注偏好数据。


<details>
  <summary>Details</summary>
Motivation: 现有多模态大语言模型的性能受限于粗糙的对齐技术，关键瓶颈是缺乏有效的奖励模型：现有RM主要是视觉中心、返回不透明的标量分数，且依赖昂贵的人工标注。

Method: 1) 构建Omni-Preference数据集：通过自动化管道合成候选响应对，使用强教师模型协调和过滤偏好，提供模态感知的基于准则的理由；2) 两阶段训练：监督微调学习基于准则的输出，然后通过强化学习（GRPO）在困难、低对比度对上提高判别能力。

Result: 在视频（ShareGPT-V上80.2%）和音频（Audio-HH-RLHF上66.8%）基准上达到SOTA，在图像任务上显著优于现有开源RM，比基础模型整体准确率提升17.7%。通过Best-of-N选择和迁移到纯文本偏好基准提升下游性能。

Conclusion: Omni-RRM是首个开源的多模态奖励模型，通过结构化多维度偏好判断和自动化数据生成，解决了现有RM的局限性，在多模态对齐中表现出色。

Abstract: Multimodal large language models (MLLMs) have shown remarkable capabilities, yet their performance is often capped by the coarse nature of existing alignment techniques. A critical bottleneck remains the lack of effective reward models (RMs): existing RMs are predominantly vision-centric, return opaque scalar scores, and rely on costly human annotations. We introduce \textbf{Omni-RRM}, the first open-source rubric-grounded reward model that produces structured, multi-dimension preference judgments with dimension-wise justifications across \textbf{text, image, video, and audio}. At the core of our approach is \textbf{Omni-Preference}, a large-scale dataset built via a fully automated pipeline: we synthesize candidate response pairs by contrasting models of different capabilities, and use strong teacher models to \emph{reconcile and filter} preferences while providing a modality-aware \emph{rubric-grounded rationale} for each pair. This eliminates the need for human-labeled training preferences. Omni-RRM is trained in two stages: supervised fine-tuning to learn the rubric-grounded outputs, followed by reinforcement learning (GRPO) to sharpen discrimination on difficult, low-contrast pairs. Comprehensive evaluations show that Omni-RRM achieves state-of-the-art accuracy on video (80.2\% on ShareGPT-V) and audio (66.8\% on Audio-HH-RLHF) benchmarks, and substantially outperforms existing open-source RMs on image tasks, with a 17.7\% absolute gain over its base model on overall accuracy. Omni-RRM also improves downstream performance via Best-of-$N$ selection and transfers to text-only preference benchmarks. Our data, code, and models are available at https://anonymous.4open.science/r/Omni-RRM-CC08.

</details>


### [24] [Factuality on Demand: Controlling the Factuality-Informativeness Trade-off in Text Generation](https://arxiv.org/abs/2602.00848)
*Ziwei Gong,Yanda Chen,Julia Hirschberg,Chen Zhao,He He,Zhou Yu,Kathleen Mckeown*

Main category: cs.CL

Relevance: 85.0

TL;DR: Factuality-Controlled Generation (FCG) 框架让用户可以为查询指定事实性约束，在信息量和事实准确性之间进行权衡控制。


<details>
  <summary>Details</summary>
Motivation: 大语言模型在编码知识时具有不同的置信度，面临信息量与事实准确性之间的权衡。不同应用需要不同的平衡，因此需要一种方法让用户能够控制生成内容的事实性水平。

Method: 提出Factuality-Controlled Generation (FCG)框架，允许用户指定事实性约束。使用合成数据训练模型执行FCG任务，评估框架在遵守事实性约束和保持信息量两个维度上的表现。

Result: 合成训练显著提高了模型遵守事实性要求的能力，同时保持了输出信息量。

Conclusion: FCG框架有效解决了LLM在信息量和事实准确性之间的权衡问题，为用户提供了可控的事实性生成能力。

Abstract: Large language models (LLMs) encode knowledge with varying degrees of confidence. When responding to queries, models face an inherent trade-off: they can generate responses that are less informative but highly factual, or more informative but potentially less accurate. Different applications demand different balances between informativeness and factuality. We introduce Factuality-Controlled Generation (FCG), a framework that enables users to specify factuality constraints alongside their queries. We propose to evaluate FCG performance on two dimensions: adherence to factuality constraints and response informativeness. We propose to train models on the FCG task using synthetic data, and show that our synthetic training significantly improves models' ability to both respect factuality requirements and maintain informativeness in their outputs.

</details>


### [25] [Unifying Adversarial Robustness and Training Across Text Scoring Models](https://arxiv.org/abs/2602.00857)
*Manveer Singh Tamber,Hosna Oyarhoseini,Jimmy Lin*

Main category: cs.CL

Relevance: 85.0

TL;DR: 该论文提出统一文本评分模型（包括密集检索器、重排序器和奖励模型）的对抗鲁棒性研究框架，开发了多种对抗训练方法，并展示了其在RLHF中缓解奖励攻击、训练更好对齐LLM的实际价值。


<details>
  <summary>Details</summary>
Motivation: 当前语言模型对抗鲁棒性研究在不同应用和攻击方法间碎片化，掩盖了共享的脆弱性。作者希望统一文本评分模型的对抗鲁棒性研究，为密集检索器、重排序器和奖励模型提供一致的评估框架。

Method: 1. 提出文本评分模型的统一对抗鲁棒性研究框架；2. 开发多种针对文本评分模型的对抗训练方法；3. 结合互补的训练方法提升鲁棒性；4. 在RLHF中应用对抗训练的奖励模型。

Result: 1. 展示了当前语言模型对抗训练方法的局限性，无法有效跨攻击泛化；2. 提出的对抗训练方法能同时提升鲁棒性和任务效果；3. 对抗训练的奖励模型能缓解奖励攻击，支持训练更好对齐的LLM。

Conclusion: 通过统一的文本评分框架研究对抗鲁棒性，开发有效的对抗训练方法，不仅能提升模型鲁棒性，还能改善任务性能，并在RLHF中具有重要实际应用价值。

Abstract: Research on adversarial robustness in language models is currently fragmented across applications and attacks, obscuring shared vulnerabilities. In this work, we propose unifying the study of adversarial robustness in text scoring models spanning dense retrievers, rerankers, and reward models. This motivates adapting both attacks and adversarial training methods across model roles. Unlike open-ended generation, text scoring failures are directly testable: an attack succeeds when an irrelevant or rejected text outscores a relevant or chosen one. Using this principled lens of text scoring, we demonstrate that current adversarial training formulations for language models are often short-sighted, failing to effectively generalize across attacks. To address this, we introduce multiple adversarial training methods for text scoring models and show that combining complementary training methods can yield strong robustness while also improving task effectiveness. We also highlight the practical value of our approach for RLHF, showing that our adversarially trained reward models mitigate reward hacking and support the training of better-aligned LLMs. We provide our code and models for further study.

</details>


### [26] [EffGen: Enabling Small Language Models as Capable Autonomous Agents](https://arxiv.org/abs/2602.00887)
*Gaurav Srivastava,Aafiya Hussain,Chi Wang,Yingyan Celine Lin,Xuan Wang*

Main category: cs.CL

Relevance: 85.0

TL;DR: effGen是一个专为小型语言模型优化的开源智能体框架，通过提示优化、任务分解、复杂度路由和统一内存系统，实现高效、安全的本地部署，在13个基准测试中超越主流框架。


<details>
  <summary>Details</summary>
Motivation: 现有语言模型智能体系统主要针对大型语言模型API设计，存在高token成本和隐私问题，特别是对于敏感应用。需要为小型语言模型设计优化的本地部署框架。

Method: 1) 增强的工具调用与提示优化，压缩上下文70-80%同时保留任务语义；2) 智能任务分解，基于依赖关系将复杂查询分解为并行或顺序子任务；3) 基于五个因素的复杂度路由，进行智能预执行决策；4) 统一内存系统，结合短期、长期和向量存储；5) 统一多种智能体协议（MCP、A2A、ACP）实现跨协议通信。

Result: 在13个基准测试中，effGen在成功率、执行速度和内存使用方面均优于LangChain、AutoGen和Smolagents。提示优化和复杂度路由具有互补的扩展行为：优化对小型模型更有利（1.5B参数模型提升11.2% vs 32B模型提升2.4%），而路由对大型模型更有利（1.5B模型提升3.6% vs 32B模型提升7.9%），组合使用时在所有规模上都能获得一致增益。

Conclusion: effGen为小型语言模型提供了高效、安全的智能体框架，解决了API依赖的成本和隐私问题，通过创新的优化技术实现了跨规模的一致性能提升，为研究和商业应用提供了可访问的开源解决方案。

Abstract: Most existing language model agentic systems today are built and optimized for large language models (e.g., GPT, Claude, Gemini) via API calls. While powerful, this approach faces several limitations including high token costs and privacy concerns for sensitive applications. We introduce effGen, an open-source agentic framework optimized for small language models (SLMs) that enables effective, efficient, and secure local deployment (pip install effgen). effGen makes four major contributions: (1) Enhanced tool-calling with prompt optimization that compresses contexts by 70-80% while preserving task semantics, (2) Intelligent task decomposition that breaks complex queries into parallel or sequential subtasks based on dependencies, (3) Complexity-based routing using five factors to make smart pre-execution decisions, and (4) Unified memory system combining short-term, long-term, and vector-based storage. Additionally, effGen unifies multiple agent protocols (MCP, A2A, ACP) for cross-protocol communication. Results on 13 benchmarks show effGen outperforms LangChain, AutoGen, and Smolagents with higher success rates, faster execution, and lower memory. Our results reveal that prompt optimization and complexity routing have complementary scaling behavior: optimization benefits SLMs more (11.2% gain at 1.5B vs 2.4% at 32B), while routing benefits large models more (3.6% at 1.5B vs 7.9% at 32B), providing consistent gains across all scales when combined. effGen (https://effgen.org/) is released under the MIT License, ensuring broad accessibility for research and commercial use. Our framework code is publicly available at https://github.com/ctrl-gaurav/effGen.

</details>


### [27] [Neural FOXP2 -- Language Specific Neuron Steering for Targeted Language Improvement in LLMs](https://arxiv.org/abs/2602.00945)
*Anusa Saha,Tanmay Joshi,Vinija Jain,Aman Chadha,Amitava Das*

Main category: cs.CL

Relevance: 85.0

TL;DR: 提出Neural FOXP2方法，通过定位和操控语言神经元，使模型将特定语言（如印地语或西班牙语）设为主要语言，解决LLMs中英语主导而其他语言被抑制的问题。


<details>
  <summary>Details</summary>
Motivation: LLMs虽然是多语言的，但英语在预训练中占主导地位，导致其他语言在参数记忆中但被系统性抑制。作者认为语言默认性由稀疏、低秩的控制电路（语言神经元）控制，可以机械地隔离和安全地操控。

Method: Neural FOXP2分为三个阶段：(1) 定位：训练每层SAEs，通过特征选择性识别语言神经元；(2) 操控方向：通过谱低秩分析定位可控的语言转换几何；(3) 操控：在低到中层应用有符号的稀疏激活偏移，使目标语言成为默认语言。

Result: 方法成功使模型将印地语或西班牙语设为主要语言，通过操控语言神经元实现了可控的语言默认性转换。

Conclusion: 语言默认性由稀疏、低秩的语言神经元控制，可以通过机械方法隔离和操控，为多语言LLMs的语言平衡提供了新途径。

Abstract: LLMs are multilingual by training, yet their lingua franca is often English, reflecting English language dominance in pretraining. Other languages remain in parametric memory but are systematically suppressed. We argue that language defaultness is governed by a sparse, low-rank control circuit, language neurons, that can be mechanistically isolated and safely steered.
  We introduce Neural FOXP2, that makes a chosen language (Hindi or Spanish) primary in a model by steering language-specific neurons. Neural FOXP2 proceeds in three stages: (i) Localize: We train per-layer SAEs so each activation decomposes into a small set of active feature components. For every feature, we quantify English vs. Hindi/Spanish selectivity overall logit-mass lift toward the target-language token set. Tracing the top-ranked features back to their strongest contributing units yields a compact language-neuron set. (ii) Steering directions: We localize controllable language-shift geometry via a spectral low-rank analysis. For each layer, we build English to target activation-difference matrices and perform layerwise SVD to extract the dominant singular directions governing language change. The eigengap and effective-rank spectra identify a compact steering subspace and an empirically chosen intervention window (where these directions are strongest and most stable). (iii) Steer: We apply a signed, sparse activation shift targeted to the language neurons. Concretely, within low to mid layers we add a positive steering along the target-language dominant directions and a compensating negative shift toward the null space for the English neurons, yielding controllable target-language defaultness.

</details>


### [28] [Verification Required: The Impact of Information Credibility on AI Persuasion](https://arxiv.org/abs/2602.00970)
*Saaduddin Mahmud,Eugene Bagdasarian,Shlomo Zilberstein*

Main category: cs.CL

Relevance: 85.0

TL;DR: 论文提出MixTalk博弈框架研究LLM代理在概率可信度信息下的战略通信，开发TOPD方法提升接收者鲁棒性


<details>
  <summary>Details</summary>
Motivation: 现有研究主要关注不可验证的廉价谈话或完全可验证的披露，无法捕捉现实世界中信息具有概率可信度的场景。随着LLM代理在高风险决策环境中的部署，需要理解战略通信的基本原则。

Method: 1) 提出MixTalk战略通信博弈框架，建模信息可信度，发送者混合使用可验证和不可验证声明，接收者分配有限预算进行成本验证；2) 在三种现实部署设置中进行大规模锦标赛评估LLM代理；3) 提出TOPD离线方法，从交互日志中蒸馏锦标赛oracle策略并在推理时上下文部署

Result: 评估揭示了LLM代理在推理信息可信度和塑造交互的显式行为方面的优势和局限性。TOPD方法显著提高了接收者对说服的鲁棒性。

Conclusion: MixTalk框架为研究LLM代理在概率可信度信息下的战略通信提供了新视角，TOPD方法能有效提升接收者鲁棒性，对LLM代理在现实高风险环境中的部署具有重要意义。

Abstract: Agents powered by large language models (LLMs) are increasingly deployed in settings where communication shapes high-stakes decisions, making a principled understanding of strategic communication essential. Prior work largely studies either unverifiable cheap-talk or fully verifiable disclosure, failing to capture realistic domains in which information has probabilistic credibility. We introduce MixTalk, a strategic communication game for LLM-to-LLM interaction that models information credibility. In MixTalk, a sender agent strategically combines verifiable and unverifiable claims to communicate private information, while a receiver agent allocates a limited budget to costly verification and infers the underlying state from prior beliefs, claims, and verification outcomes. We evaluate state-of-the-art LLM agents in large-scale tournaments across three realistic deployment settings, revealing their strengths and limitations in reasoning about information credibility and the explicit behavior that shapes these interactions. Finally, we propose Tournament Oracle Policy Distillation (TOPD), an offline method that distills tournament oracle policy from interaction logs and deploys it in-context at inference time. Our results show that TOPD significantly improves receiver robustness to persuasion.

</details>


### [29] [Trust in One Round: Confidence Estimation for Large Language Models via Structural Signals](https://arxiv.org/abs/2602.00977)
*Pengyue Yang,Jiawen Wen,Haolin Jin,Linghan Huang,Huaming Chen,Ling Chen*

Main category: cs.CL

Relevance: 85.0

TL;DR: 提出Structural Confidence框架，通过分析LLM最后一层隐藏状态轨迹的多尺度结构信号，增强输出正确性预测，无需多次采样，单次前向传播即可实现鲁棒置信度估计。


<details>
  <summary>Details</summary>
Motivation: 当前LLM在关键领域部署时，标准置信度估计方法（如token似然、语义相似性、多样本一致性）在分布偏移、专业领域文本和计算限制下表现脆弱。需要更鲁棒、高效的置信度估计方法。

Method: 提出Structural Confidence框架，从模型最后一层隐藏状态轨迹中提取多尺度结构信号，包括频谱、局部变化和全局形状描述符，捕捉概率和句子嵌入遗漏的内部稳定性模式。

Result: 在四个异构基准测试（FEVER、SciFact、WikiBio-hallucination、TruthfulQA）上评估，在AUROC和AUPR指标上表现优于现有基线方法。

Conclusion: Structural Confidence框架提供了一种实用、高效、鲁棒的事后置信度估计方法，适用于社会影响大、资源受限的LLM应用，相比需要多次随机生成和辅助模型的采样一致性方法更具优势。

Abstract: Large language models (LLMs) are increasingly deployed in domains where errors carry high social, scientific, or safety costs. Yet standard confidence estimators, such as token likelihood, semantic similarity and multi-sample consistency, remain brittle under distribution shift, domain-specialised text, and compute limits. In this work, we present Structural Confidence, a single-pass, model-agnostic framework that enhances output correctness prediction based on multi-scale structural signals derived from a model's final-layer hidden-state trajectory. By combining spectral, local-variation, and global shape descriptors, our method captures internal stability patterns that are missed by probabilities and sentence embeddings. We conduct extensive, cross-domain evaluation across four heterogeneous benchmarks-FEVER (fact verification), SciFact (scientific claims), WikiBio-hallucination (biographical consistency), and TruthfulQA (truthfulness-oriented QA). Our Structural Confidence framework demonstrates strong performance compared with established baselines in terms of AUROC and AUPR. More importantly, unlike sampling-based consistency methods which require multiple stochastic generations and an auxiliary model, our approach uses a single deterministic forward pass, offering a practical basis for efficient, robust post-hoc confidence estimation in socially impactful, resource-constrained LLM applications.

</details>


### [30] [DISPO: Enhancing Training Efficiency and Stability in Reinforcement Learning for Large Language Model Mathematical Reasoning](https://arxiv.org/abs/2602.00983)
*Batuhan K. Karaman,Aditya Rawal,Suhaila Shakiah,Mohammad Ghavamzadeh,Mingyi Hong,Arijit Biswas,Ruida Zhou*

Main category: cs.CL

Relevance: 85.0

TL;DR: DISPO是一种用于数学推理的强化学习算法，通过解耦正确和错误响应的重要性采样权重上下裁剪，实现四个可控策略更新机制，在保持探索-蒸馏平衡的同时防止灾难性失败。


<details>
  <summary>Details</summary>
Motivation: 当前数学推理中的强化学习方法存在明显权衡：PPO风格方法（如GRPO/DAPO）训练稳定但学习缓慢，而REINFORCE风格方法（如CISPO）学习效率高但性能不稳定。需要一种既能保持训练稳定性又能提高学习效率的方法。

Method: DISPO是一种REINFORCE风格算法，核心创新是解耦正确和错误响应的重要性采样权重上下裁剪。对于正确响应，权重>1增加平均标记熵（探索），权重<1减少熵（蒸馏）；对于错误响应，分别控制权重裁剪防止重复输出或响应长度消失。通过分别调整这四个裁剪参数，维持探索-蒸馏平衡并防止灾难性失败。

Result: 在AIME'24上达到61.04%（vs. CISPO 55.42%和DAPO 50.21%），在各种基准测试和模型上都显示出类似增益。通过针对性消融研究揭示了每个机制对训练的影响。

Conclusion: DISPO通过解耦重要性采样权重的裁剪机制，在保持REINFORCE风格算法学习效率的同时解决了其稳定性问题，为数学推理中的强化学习提供了更优的解决方案。

Abstract: Reinforcement learning with verifiable rewards has emerged as a promising paradigm for enhancing the reasoning capabilities of large language models particularly in mathematics. Current approaches in this domain present a clear trade-off: PPO-style methods (e.g., GRPO/DAPO) offer training stability but exhibit slow learning trajectories due to their trust-region constraints on policy updates, while REINFORCE-style approaches (e.g., CISPO) demonstrate improved learning efficiency but suffer from performance instability as they clip importance sampling weights while still permitting non-zero gradients outside the trust-region. To address these limitations, we introduce DISPO, a simple yet effective REINFORCE-style algorithm that decouples the up-clipping and down-clipping of importance sampling weights for correct and incorrect responses, yielding four controllable policy update regimes. Through targeted ablations, we uncover how each regime impacts training: for correct responses, weights >1 increase the average token entropy (i.e., exploration) while weights <1 decrease it (i.e., distillation) -- both beneficial but causing gradual performance degradation when excessive. For incorrect responses, overly restrictive clipping triggers sudden performance collapse through repetitive outputs (when weights >1) or vanishing response lengths (when weights <1). By separately tuning these four clipping parameters, DISPO maintains the exploration-distillation balance while preventing catastrophic failures, achieving 61.04% on AIME'24 (vs. 55.42% CISPO and 50.21% DAPO) with similar gains across various benchmarks and models.

</details>


### [31] [Sparse Reward Subsystem in Large Language Models](https://arxiv.org/abs/2602.00986)
*Guowei Xu,Mert Yuksekgonul,James Zou*

Main category: cs.CL

Relevance: 85.0

TL;DR: 在LLM隐藏状态中发现类似人脑奖励系统的稀疏奖励子系统，包含表征状态价值期望的"价值神经元"和编码奖励预测误差的"多巴胺神经元"


<details>
  <summary>Details</summary>
Motivation: 受生物学启发，探索LLM内部是否存在类似人脑的奖励处理机制，理解模型推理过程中的内部价值表征

Method: 通过干预实验识别隐藏状态中的价值神经元，分析其跨数据集、模型规模和架构的鲁棒性，并识别编码奖励预测误差的神经元

Result: 发现价值神经元在推理中起重要作用，具有跨数据集和模型的鲁棒性和可迁移性，并识别出编码奖励预测误差的"多巴胺神经元"

Conclusion: LLM内部存在类似生物奖励系统的机制，这为理解模型推理过程提供了新视角，对可解释性和强化学习有重要意义

Abstract: In this paper, we identify a sparse reward subsystem within the hidden states of Large Language Models (LLMs), drawing an analogy to the biological reward subsystem in the human brain. We demonstrate that this subsystem contains value neurons that represent the model's internal expectation of state value, and through intervention experiments, we establish the importance of these neurons for reasoning. Our experiments reveal that these value neurons are robust across diverse datasets, model scales, and architectures; furthermore, they exhibit significant transferability across different datasets and models fine-tuned from the same base model. By examining cases where value predictions and actual rewards diverge, we identify dopamine neurons within the reward subsystem which encode reward prediction errors (RPE). These neurons exhibit high activation when the reward is higher than expected and low activation when the reward is lower than expected.

</details>


### [32] [Reliable Use of Lemmas via Eligibility Reasoning and Section$-$Aware Reinforcement Learning](https://arxiv.org/abs/2602.00998)
*Zhikun Xu,Xiaodong Yu,Ben Zhou,Jiang Liu,Jialian Wu,Ze Wang,Ximeng Sun,Hao Chen,Zicheng Liu*

Main category: cs.CL

Relevance: 85.0

TL;DR: 论文提出RULES方法，通过结构化预测任务训练LLMs进行引理判断，使用两段式输出和分段感知强化学习来提升数学推理的鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 尽管大型语言模型在数学基准测试中表现良好，但经常错误应用引理，在不验证假设的情况下直接使用结论。这暴露了模型在数学推理中的鲁棒性问题。

Method: 将引理判断形式化为结构化预测任务：给定陈述和候选引理，模型需要输出前提条件检查和结论效用检查。提出RULES方法，使用两段式输出编码规范，通过强化学习加分段感知损失掩码训练，对错误部分施加惩罚。

Result: 在领域内任务上优于普通模型和单标签RL基线；在适用性破坏性扰动上改进更大；在端到端任务上达到相当或略有提升；消融实验表明两段式输出和分段感知强化学习对鲁棒性都是必要的。

Conclusion: 结构化引理判断任务和分段感知训练方法能有效提升LLMs在数学推理中的鲁棒性，特别是在处理扰动和验证假设方面。

Abstract: Recent large language models (LLMs) perform strongly on mathematical benchmarks yet often misapply lemmas, importing conclusions without validating assumptions. We formalize lemma$-$judging as a structured prediction task: given a statement and a candidate lemma, the model must output a precondition check and a conclusion$-$utility check, from which a usefulness decision is derived. We present RULES, which encodes this specification via a two$-$section output and trains with reinforcement learning plus section$-$aware loss masking to assign penalty to the section responsible for errors. Training and evaluation draw on diverse natural language and formal proof corpora; robustness is assessed with a held$-$out perturbation suite; and end$-$to$-$end evaluation spans competition$-$style, perturbation$-$aligned, and theorem$-$based problems across various LLMs. Results show consistent in$-$domain gains over both a vanilla model and a single$-$label RL baseline, larger improvements on applicability$-$breaking perturbations, and parity or modest gains on end$-$to$-$end tasks; ablations indicate that the two$-$section outputs and section$-$aware reinforcement are both necessary for robustness.

</details>


### [33] [Distilling Token-Trained Models into Byte-Level Models](https://arxiv.org/abs/2602.01007)
*Zishuo Bao,Jiaqi Leng,Junxiong Wang,Bowen Peng,Yucheng Lu*

Main category: cs.CL

Relevance: 85.0

TL;DR: 提出一种高效蒸馏方法，将现有的分词训练LLMs转换为字节语言模型，仅需约125B字节数据即可保留教师模型大部分性能


<details>
  <summary>Details</summary>
Motivation: 字节语言模型(BLMs)是超越分词限制的有前景方向，但现有BLMs需要从头训练数万亿字节，成本极高。本研究旨在通过蒸馏现有分词训练的LLMs来高效构建BLMs

Method: 采用两阶段课程学习：1)渐进知识蒸馏，对齐字节级表示与分词教师模型的嵌入；2)字节级监督微调，实现完全在字节空间的端到端生成

Result: 在Llama、Qwen、OLMo等多个模型家族上验证，蒸馏出的BLMs仅使用约125B字节数据即可保留教师模型大部分性能

Conclusion: 提出了一种高效将现有分词LLMs转换为字节语言模型的方法，显著降低了BLMs的训练成本，为超越分词限制的语言模型提供了实用路径

Abstract: Byte Language Models (BLMs) have emerged as a promising direction for scaling language models beyond tokenization. However, existing BLMs typically require training from scratch on trillions of bytes, making them prohibitively expensive. In this paper, we propose an efficient distillation recipe that converts existing token-trained LLMs into BLMs while retaining comparable capabilities. Our recipe follows a two-stage curriculum: (1) Progressive Knowledge Distillation, which aligns byte-level representations with the embeddings of the token-trained teacher model; and (2) Byte-Level Supervised Fine-Tuning, which enables end-to-end generation entirely in the byte space. We validate our approach across multiple model families, including Llama, Qwen, and OLMo, and demonstrate that the distilled BLMs retain most of the teacher models' performance using only approximately 125B bytes.

</details>


### [34] [Large Language Models as Students Who Think Aloud: Overly Coherent, Verbose, and Confident](https://arxiv.org/abs/2602.01015)
*Conrad Borchers,Jill-Jênn Vie,Roger Azevedo*

Main category: cs.CL

Relevance: 85.0

TL;DR: 评估LLM作为新手模拟学习推理的能力，发现GPT-4生成的推理过于连贯、冗长、缺乏变异性，且高估学习者表现，揭示了LLM模拟人类学习的认知局限性。


<details>
  <summary>Details</summary>
Motivation: 现有评估主要关注LLM在辅导系统中的问题解决准确性，但忽略了人类学习过程中碎片化、不完美的推理特征。研究旨在评估LLM能否真实模拟新手推理和元认知判断，这对设计更有效的AI辅导系统至关重要。

Method: 使用630条多步骤化学辅导问题的出声思考记录，包含学生提示使用、尝试和问题上下文。在最小和扩展上下文提示下比较LLM生成推理与人类学习者话语，评估模型预测学习者步骤级成功的能力。

Result: GPT-4生成流畅且上下文适当的延续，但其推理系统性地过于连贯、冗长且变异性低于人类出声思考。随着提示中问题解决上下文更丰富，这些效应加剧。学习者表现被持续高估。

Conclusion: LLM在模拟学习方面存在认知局限性，这源于其训练数据包含专家式解决方案，缺乏情感表达和工作记忆约束。评估框架可指导未来设计更忠实支持新手学习和自我调节的自适应系统。

Abstract: Large language models (LLMs) are increasingly embedded in AI-based tutoring systems. Can they faithfully model novice reasoning and metacognitive judgments? Existing evaluations emphasize problem-solving accuracy, overlooking the fragmented and imperfect reasoning that characterizes human learning. We evaluate LLMs as novices using 630 think-aloud utterances from multi-step chemistry tutoring problems with problem-solving logs of student hint use, attempts, and problem context. We compare LLM-generated reasoning to human learner utterances under minimal and extended contextual prompting, and assess the models' ability to predict step-level learner success. Although GPT-4.1 generates fluent and contextually appropriate continuations, its reasoning is systematically over-coherent, verbose, and less variable than human think-alouds. These effects intensify with a richer problem-solving context during prompting. Learner performance was consistently overestimated. These findings highlight epistemic limitations of simulating learning with LLMs. We attribute these limitations to LLM training data, including expert-like solutions devoid of expressions of affect and working memory constraints during problem solving. Our evaluation framework can guide future design of adaptive systems that more faithfully support novice learning and self-regulation using generative artificial intelligence.

</details>


### [35] [Bias in the Ear of the Listener: Assessing Sensitivity in Audio Language Models Across Linguistic, Demographic, and Positional Variations](https://arxiv.org/abs/2602.01030)
*Sheng-Lun Wei,Yu-Ling Liao,Yen-Hua Chang,Hen-Hsen Huang,Hsin-Hsi Chen*

Main category: cs.CL

Relevance: 85.0

TL;DR: 该研究首次系统调查了多语言MLLM中的语音偏见，构建了BiasInEar数据集，评估了9个代表性模型在语言、口音、性别和选项顺序等扰动下的表现，发现MLLM对语言和选项顺序高度敏感，语音会放大现有结构偏见。


<details>
  <summary>Details</summary>
Motivation: 当前多模态大语言模型（MLLM）在语音集成方面快速发展，但缺乏对语音偏见的系统性评估。现有研究主要关注文本模态的偏见，而语音可能引入新的偏见维度（如口音、性别），需要建立统一的评估框架来填补这一空白。

Method: 1. 构建BiasInEar数据集：基于Global MMLU Lite，覆盖英语、中文、韩语，平衡性别和口音，包含70.8小时语音和11,200个问题；2. 使用四种互补指标：准确率、熵、APES和Fleiss' κ；3. 评估9个代表性模型在语言、口音、性别和选项顺序扰动下的表现。

Result: MLLM对人口统计因素（性别）相对鲁棒，但对语言和选项顺序高度敏感；语音会放大现有结构偏见；架构设计和推理策略显著影响跨语言鲁棒性。

Conclusion: 该研究建立了评估语音集成LLM公平性和鲁棒性的统一框架，填补了文本和语音评估之间的空白，为未来MLLM开发提供了重要的偏见评估基准。

Abstract: This work presents the first systematic investigation of speech bias in multilingual MLLMs. We construct and release the BiasInEar dataset, a speech-augmented benchmark based on Global MMLU Lite, spanning English, Chinese, and Korean, balanced by gender and accent, and totaling 70.8 hours ($\approx$4,249 minutes) of speech with 11,200 questions. Using four complementary metrics (accuracy, entropy, APES, and Fleiss' $κ$), we evaluate nine representative models under linguistic (language and accent), demographic (gender), and structural (option order) perturbations. Our findings reveal that MLLMs are relatively robust to demographic factors but highly sensitive to language and option order, suggesting that speech can amplify existing structural biases. Moreover, architectural design and reasoning strategy substantially affect robustness across languages. Overall, this study establishes a unified framework for assessing fairness and robustness in speech-integrated LLMs, bridging the gap between text- and speech-based evaluation. The resources can be found at https://github.com/ntunlplab/BiasInEar.

</details>


### [36] [Exploring Knowledge Purification in Multi-Teacher Knowledge Distillation for LLMs](https://arxiv.org/abs/2602.01064)
*Ruihan Jin,Pengpeng Shao,Zhengqi Wen,Jinyang Wu,Mingkuan Feng,Shuo Yang,Chu Yuan Zhang,Jianhua Tao*

Main category: cs.CL

Relevance: 85.0

TL;DR: 提出"知识净化"概念，将多个教师LLM的推理过程整合为单一推理，解决多教师蒸馏中的知识冲突和资源需求问题，并提出了五种净化方法。


<details>
  <summary>Details</summary>
Motivation: 传统知识蒸馏方法在多教师模型场景下面临知识冲突和高资源需求的问题，需要更有效的方法来整合多个教师模型的知识，同时减少冲突并提高效率。

Method: 提出知识净化概念，将多个教师LLM的推理过程整合为单一推理。设计了五种净化方法，包括基于路由的方法等，从不同角度解决知识整合问题。

Result: 实验表明这些方法不仅提升了蒸馏模型的性能，还能有效缓解知识冲突。基于路由的方法展现出强大的泛化能力。

Conclusion: 知识净化技术能够优化多教师蒸馏过程，促进强大而轻量模型的实用部署，为LLM知识蒸馏提供了创新解决方案。

Abstract: Knowledge distillation has emerged as a pivotal technique for transferring knowledge from stronger large language models (LLMs) to smaller, more efficient models. However, traditional distillation approaches face challenges related to knowledge conflicts and high resource demands, particularly when leveraging multiple teacher models. In this paper, we introduce the concept of \textbf{Knowledge Purification}, which consolidates the rationales from multiple teacher LLMs into a single rationale, thereby mitigating conflicts and enhancing efficiency. To investigate the effectiveness of knowledge purification, we further propose five purification methods from various perspectives. Our experiments demonstrate that these methods not only improve the performance of the distilled model but also effectively alleviate knowledge conflicts. Moreover, router-based methods exhibit robust generalization capabilities, underscoring the potential of innovative purification techniques in optimizing multi-teacher distillation and facilitating the practical deployment of powerful yet lightweight models.

</details>


### [37] [What If We Allocate Test-Time Compute Adaptively?](https://arxiv.org/abs/2602.01070)
*Ahsan Bilal,Ahmed Mohsin,Muhammad Umer,Ali Subhan,Hassan Rizwan,Ayesha Mohsin,Dean Hougen*

Main category: cs.CL

Relevance: 85.0

TL;DR: 提出了一种基于验证器引导的自适应推理框架，通过迭代轨迹生成和选择，使用过程奖励模型（PRM）指导推理过程，相比均匀分配计算资源的测试时计算扩展方法，在多个数学推理基准上取得显著提升。


<details>
  <summary>Details</summary>
Motivation: 传统测试时计算扩展方法存在三个主要问题：1）均匀分配推理计算资源，2）使用固定的采样策略，3）仅在重排序时应用验证。这导致计算效率低下，无法根据问题难度和推理路径质量动态调整资源分配。

Method: 提出验证器引导的自适应框架，将推理视为迭代轨迹生成和选择过程。每个问题运行多次推理迭代，每次迭代包含：可选的高层规划、推理工具选择、计算策略和探索参数设置、候选推理轨迹生成。使用过程奖励模型（PRM）作为统一控制信号：在迭代内，步骤级PRM分数用于指导生成过程中的剪枝和扩展；在迭代间，聚合的轨迹奖励用于选择最终响应。

Result: 在多个数据集上，动态PRM引导方法持续优于直接测试时扩展方法，在MATH-500上取得大幅提升，在AIME24和AMO-Bench等更难基准上实现数倍改进。通过理论FLOPs和计算强度指标（惩罚浪费的生成和工具开销）评估效率，显示验证引导分配将计算集中在高效用推理路径上。

Conclusion: 验证引导的自适应推理框架通过动态资源分配和过程奖励模型指导，显著提升了大型语言模型在复杂推理任务上的性能和计算效率，为推理优化提供了新方向。

Abstract: Test-time compute scaling allocates inference computation uniformly, uses fixed sampling strategies, and applies verification only for reranking. In contrast, we propose a verifier-guided adaptive framework treating reasoning as iterative trajectory generation and selection. For each problem, the agent runs multiple inference iterations. In each iteration, it optionally produces a high-level plan, selects a set of reasoning tools and a compute strategy together with an exploration parameter, and then generates a candidate reasoning trajectory. A process reward model (PRM) serves as a unified control signal: within each iteration, step-level PRM scores are aggregated to guide pruning and expansion during generation, and across iterations, aggregated trajectory rewards are used to select the final response. Across datasets, our dynamic, PRM-guided approach consistently outperforms direct test-time scaling, yielding large gains on MATH-500 and several-fold improvements on harder benchmarks such as AIME24 and AMO-Bench. We characterize efficiency using theoretical FLOPs and a compute intensity metric penalizing wasted generation and tool overhead, demonstrating that verification-guided allocation concentrates computation on high-utility reasoning paths.

</details>


### [38] [Logic-Oriented Retriever Enhancement via Contrastive Learning](https://arxiv.org/abs/2602.01116)
*Wenxuan Zhang,Yuan-Hao Jiang,Changyong Qi,Rui Jia,Yonghe Wu*

Main category: cs.CL

Relevance: 85.0

TL;DR: LORE通过细粒度对比学习激活LLM的逻辑推理能力，改善知识密集型任务中的检索效果，无需外部监督或预检索分析


<details>
  <summary>Details</summary>
Motivation: 大型语言模型在知识密集型任务中表现不佳，因为传统检索器往往过度依赖表面相似性，无法处理涉及复杂逻辑关系的查询。模型表示中固有的逻辑分析能力在标准训练中未被充分利用。

Method: LORE采用细粒度对比学习来激活模型潜在的逻辑推理能力，引导嵌入向量朝向符合逻辑结构的证据，而不是浅层相似性。该方法无需外部监督、额外资源或预检索分析，保持索引兼容性。

Result: LORE能持续提升检索效用和下游生成质量，同时保持效率。代码和数据集已公开。

Conclusion: 通过激活LLM内在的逻辑推理能力，LORE有效解决了传统检索器在复杂逻辑查询中的局限性，为知识密集型任务提供了更有效的检索增强生成方案。

Abstract: Large language models (LLMs) struggle in knowledge-intensive tasks, as retrievers often overfit to surface similarity and fail on queries involving complex logical relations. The capacity for logical analysis is inherent in model representations but remains underutilized in standard training. LORE (Logic ORiented Retriever Enhancement) introduces fine-grained contrastive learning to activate this latent capacity, guiding embeddings toward evidence aligned with logical structure rather than shallow similarity. LORE requires no external upervision, resources, or pre-retrieval analysis, remains index-compatible, and consistently improves retrieval utility and downstream generation while maintaining efficiency. The datasets and code are publicly available at https://github.com/mazehart/Lore-RAG.

</details>


### [39] [Don't Judge a Book by its Cover: Testing LLMs' Robustness Under Logical Obfuscation](https://arxiv.org/abs/2602.01132)
*Abhilekh Borah,Shubhra Ghosh,Kedar Joshi,Aditya Kumar Guru,Kripabandhu Ghosh*

Main category: cs.CL

Relevance: 85.0

TL;DR: 论文提出Logifus逻辑混淆框架和LogiQAte诊断基准，发现LLMs在逻辑等价但表面形式混淆的问题上表现显著下降，揭示模型缺乏深度理解能力


<details>
  <summary>Details</summary>
Motivation: 研究LLMs在处理逻辑等价但表面形式混淆的问题时的脆弱性。虽然LLMs能很好处理标准形式的逻辑推理任务，但在逻辑等价但经过混淆的问题上表现不佳，这表明模型可能只是表面解析问题而非深度理解逻辑结构。

Method: 1. 提出Logifus：结构保持的逻辑混淆框架，用于生成逻辑等价但表面形式不同的变体
2. 创建LogiQAte基准：包含1,108个问题，涵盖四个推理任务：(i)一阶逻辑蕴含的混淆重写，(ii)家族关系图的间接关系链，(iii)符号替换下的模式归纳，(iv)方向改变和参考系下的导航推理
3. 评估六个最先进模型在混淆前后的性能变化

Result: 混淆严重降低了零样本性能：GPT-4o平均下降47%，GPT-5下降27%，推理模型o4-mini下降22%。所有模型在逻辑等价但混淆的问题上都表现显著下降，表明它们缺乏对逻辑结构的深度理解。

Conclusion: 当前LLMs只是表面解析问题而没有深度理解，强调了构建真正理解并保持意义超越表面形式的模型的紧迫性。这揭示了LLMs在逻辑推理能力上的重要局限性。

Abstract: Tasks such as solving arithmetic equations, evaluating truth tables, and completing syllogisms are handled well by large language models (LLMs) in their standard form, but they often fail when the same problems are posed in logically equivalent yet obfuscated formats. To study this vulnerability, we introduce Logifus, a structure-preserving logical obfuscation framework, and, utilizing this, we present LogiQAte, a first-of-its-kind diagnostic benchmark with 1,108 questions across four reasoning tasks: (i) Obfus FOL (first-order logic entailment under equivalence-preserving rewrites), (ii) Obfus Blood Relation (family-graph entailment under indirect relational chains), (iii) Obfus Number Series (pattern induction under symbolic substitutions), and (iv) Obfus Direction Sense (navigation reasoning under altered directions and reference frames). Across all the tasks, evaluating six state-of-the-art models, we find that obfuscation severely degrades zero-shot performance, with performance dropping on average by 47% for GPT-4o, 27% for GPT-5, and 22% for reasoning model, o4-mini. Our findings reveal that current LLMs parse questions without deep understanding, highlighting the urgency of building models that genuinely comprehend and preserve meaning beyond surface form.

</details>


### [40] [Beyond Training for Cultural Awareness: The Role of Dataset Linguistic Structure in Large Language Models](https://arxiv.org/abs/2602.01161)
*Reem I. Masoud,Chen Feng,Shunta Asano,Saied Alshahrani,Philip Colin Treleaven,Miguel R. D. Rodrigues*

Main category: cs.CL

Relevance: 85.0

TL;DR: 研究分析了文化适应微调数据集的哪些语言特性与LLM文化表现相关，发现词汇导向的特性最稳健，而语义或多样性极端则通常中性或有害


<details>
  <summary>Details</summary>
Motivation: LLM全球部署引发文化错配担忧，但用于文化适应的微调数据集的语言特性仍不清楚。研究旨在理解哪些语言特性与文化表现相关，这些特性是否在训练前可预测，以及不同模型间的差异

Method: 对阿拉伯语、中文和日语的微调数据集计算轻量级语言、语义和结构指标，进行主成分分析。在三个主要LLM家族(LLaMA、Mistral、DeepSeek)上进行微调，并在文化知识、价值观和规范基准上评估

Result: PCA成分与下游表现相关，但关联性高度依赖模型。词汇导向成分(PC3)最稳健，在不同模型和基准上表现一致；强调语义或多样性极端(PC1-PC2)通常中性或有害

Conclusion: 文化适应的微调数据集语言特性对LLM文化表现有重要影响，词汇特性是最稳健的预测因子，而模型依赖性表明需要针对特定架构的数据集设计策略

Abstract: The global deployment of large language models (LLMs) has raised concerns about cultural misalignment, yet the linguistic properties of fine-tuning datasets used for cultural adaptation remain poorly understood. We adopt a dataset-centric view of cultural alignment and ask which linguistic properties of fine-tuning data are associated with cultural performance, whether these properties are predictive prior to training, and how these effects vary across models. We compute lightweight linguistic, semantic, and structural metrics for Arabic, Chinese, and Japanese datasets and apply principal component analysis separately within each language. This design ensures that the resulting components capture variation among datasets written in the same language rather than differences between languages. The resulting components correspond to broadly interpretable axes related to semantic coherence, surface-level lexical and syntactic diversity, and lexical or structural richness, though their composition varies across languages. We fine-tune three major LLM families (LLaMA, Mistral, DeepSeek) and evaluate them on benchmarks of cultural knowledge, values, and norms. While PCA components correlate with downstream performance, these associations are strongly model-dependent. Through controlled subset interventions, we show that lexical-oriented components (PC3) are the most robust, yielding more consistent performance across models and benchmarks, whereas emphasizing semantic or diversity extremes (PC1-PC2) is often neutral or harmful.

</details>


### [41] [Attention Sink Forges Native MoE in Attention Layers: Sink-Aware Training to Address Head Collapse](https://arxiv.org/abs/2602.01203)
*Zizhuo Fu,Wenxuan Zeng,Runsheng Wang,Meng Li*

Main category: cs.CL

Relevance: 85.0

TL;DR: 本文揭示了注意力机制中的"注意力沉没"现象与混合专家(MoE)结构的内在联系，提出了一种基于负载均衡的注意力层训练方法来解决注意力头崩溃问题。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型中的注意力机制存在"注意力沉没"现象（过度关注第一个token），现有方法如Sink Attention和Gated Attention试图解决此问题，但缺乏对这些注意力机制之间关系的系统性分析。作者旨在深入理解这些机制的内在联系。

Method: 通过理论和实证分析证明Vanilla Attention和Sink Attention在注意力层中自然构建了混合专家(MoE)机制。为缓解注意力头崩溃问题，提出了具有辅助负载均衡损失的sink-aware训练算法。

Result: 实验表明该方法在Vanilla Attention、Sink Attention和Gated Attention中都能有效实现注意力头负载均衡，并提升模型性能。

Conclusion: 注意力机制中天然存在MoE结构，这解释了注意力头崩溃现象。提出的训练方法为解决此问题提供了有效途径，为理解注意力机制提供了新视角。

Abstract: Large Language Models (LLMs) often assign disproportionate attention to the first token, a phenomenon known as the attention sink. Several recent approaches aim to address this issue, including Sink Attention in GPT-OSS and Gated Attention in Qwen3-Next. However, a comprehensive analysis of the relationship among these attention mechanisms is lacking. In this work, we provide both theoretical and empirical evidence demonstrating that the sink in Vanilla Attention and Sink Attention naturally construct a Mixture-of-Experts (MoE) mechanism within attention layers. This insight explains the head collapse phenomenon observed in prior work, where only a fixed subset of attention heads contributes to generation. To mitigate head collapse, we propose a sink-aware training algorithm with an auxiliary load balancing loss designed for attention layers. Extensive experiments show that our method achieves effective head load balancing and improves model performance across Vanilla Attention, Sink Attention, and Gated Attention. We hope this study offers a new perspective on attention mechanisms and encourages further exploration of the inherent MoE structure within attention layers.

</details>


### [42] [ASTER: Agentic Scaling with Tool-integrated Extended Reasoning](https://arxiv.org/abs/2602.01204)
*Xuqin Zhang,Quan He,Zhenrui Zheng,Zongzhang Zhang,Xu He,Dong Li*

Main category: cs.CL

Relevance: 85.0

TL;DR: ASTER框架通过针对性的冷启动策略解决RL训练中工具集成推理的交互崩溃问题，使用仅4K交互密集轨迹实现SOTA数学推理性能


<details>
  <summary>Details</summary>
Motivation: 强化学习在激发LLMs长程推理方面占主导地位，但工具集成推理的RL扩展面临交互崩溃问题：模型无法维持多轮工具使用，退化为大量内部推理和琐碎的后验代码验证

Method: 提出ASTER框架，通过优先考虑交互密集轨迹的针对性冷启动策略，研究三个关键问题：冷启动SFT如何诱导工具使用行为先验、冷启动轨迹的交互密度如何影响探索和RL结果、RL交互预算如何影响学习动态和泛化

Result: 仅4K交互密集轨迹的专家冷启动集产生最强下游性能，建立稳健先验，在扩展RL训练中实现优越探索。ASTER-4B在竞争性数学基准上达到SOTA，AIME 2025达到90.0%，超越DeepSeek-V3.2-Exp等前沿开源模型

Conclusion: 交互密集的冷启动策略能有效解决RL训练中的交互崩溃问题，小规模高质量冷启动数据对工具集成推理的RL扩展至关重要

Abstract: Reinforcement learning (RL) has emerged as a dominant paradigm for eliciting long-horizon reasoning in Large Language Models (LLMs). However, scaling Tool-Integrated Reasoning (TIR) via RL remains challenging due to interaction collapse: a pathological state where models fail to sustain multi-turn tool usage, instead degenerating into heavy internal reasoning with only trivial, post-hoc code verification. We systematically study three questions: (i) how cold-start SFT induces an agentic, tool-using behavioral prior, (ii) how the interaction density of cold-start trajectories shapes exploration and downstream RL outcomes, and (iii) how the RL interaction budget affects learning dynamics and generalization under varying inference-time budgets. We then introduce ASTER (Agentic Scaling with Tool-integrated Extended Reasoning), a framework that circumvents this collapse through a targeted cold-start strategy prioritizing interaction-dense trajectories. We find that a small expert cold-start set of just 4K interaction-dense trajectories yields the strongest downstream performance, establishing a robust prior that enables superior exploration during extended RL training. Extensive evaluations demonstrate that ASTER-4B achieves state-of-the-art results on competitive mathematical benchmarks, reaching 90.0% on AIME 2025, surpassing leading frontier open-source models, including DeepSeek-V3.2-Exp.

</details>


### [43] [Chronos: Learning Temporal Dynamics of Reasoning Chains for Test-Time Scaling](https://arxiv.org/abs/2602.01208)
*Kai Zhang,Jiayi Liao,Chengpeng Li,Ziyuan Xie,Sihang Li,Xiang Wang*

Main category: cs.CL

Relevance: 85.0

TL;DR: Chronos是一个轻量级即插即用的时序推理评分器，将推理轨迹建模为时间序列，通过学习token概率的轨迹特征分配质量分数，采用加权投票机制提升LLM推理性能。


<details>
  <summary>Details</summary>
Motivation: 现有测试时扩展方法（如多数投票和启发式token级评分）将推理轨迹或token同等对待，容易受到轨迹质量大幅变化和局部逻辑错误的影响，需要更精细的轨迹质量评估方法。

Method: Chronos将每个推理轨迹建模为时间序列，学习token概率的轨迹特征，为不同轨迹分配质量分数，并采用加权投票机制整合多个推理路径的结果。

Result: 在领域内和领域外基准测试中，Chronos在各种模型上均取得显著提升，计算开销可忽略。Chronos@128在HMMT25上相比Pass@1提升34.21%，相比Maj@128提升22.70%。

Conclusion: Chronos通过时序建模推理轨迹特征和加权投票机制，有效提升了LLM的推理性能，为测试时扩展提供了新的有效方法。

Abstract: Test-Time Scaling (TTS) has emerged as an effective paradigm for improving the reasoning performance of large language models (LLMs). However, existing methods -- most notably majority voting and heuristic token-level scoring -- treat reasoning traces or tokens equally, thereby being susceptible to substantial variations in trajectory quality and localized logical failures. In this work, we introduce \textbf{Chronos}, a lightweight and plug-and-play chronological reasoning scorer that models each trajectory as a time series. Specifically, Chronos learns to capture trajectory features of token probabilities, assigns quality scores accordingly, and employs a weighted voting mechanism. Extensive evaluations on both in-domain and out-of-domain benchmarks demonstrate that Chronos consistently delivers substantial gains across a variety of models, with negligible computational overhead. Notably, Chronos@128 achieves relative improvements of 34.21\% over Pass@1 and 22.70\% over Maj@128 on HMMT25 using Qwen3-4B-Thinking-2507, highlighting its effectiveness.

</details>


### [44] [Supervised Fine-Tuning Needs to Unlock the Potential of Token Priority](https://arxiv.org/abs/2602.01227)
*Zhanming Shen,Zeyu Qin,Jiaqi Hu,Wentao Ye,Hao Chen,Xiaomeng Hu,Haokai Xu,Gang Chen,Yi R. Fung,Haobo Wang*

Main category: cs.CL

Relevance: 85.0

TL;DR: 本文提出"Token Priority"作为解决自回归生成中细粒度生成与粗粒度监督信号不匹配问题的关键桥梁，将SFT重新定义为精确的分布重塑过程而非简单优化


<details>
  <summary>Details</summary>
Motivation: 当前从拟合经验数据到实现真正人类效用的转变受到粒度不匹配的根本约束：细粒度的自回归生成通常由粗粒度或均匀的监督信号指导。这种不匹配限制了模型对齐的效果

Method: 提出Token Priority框架，将SFT形式化为精确的分布重塑过程，而非简单的优化问题。该框架将现有方法分为两个体系：Positive Priority用于噪声过滤，Signed Priority用于有害模式消除

Result: 通过统一视角分析近期突破，建立了理论框架来理解SFT中的对齐机制，为现有进展和局限性提供了系统性解释

Conclusion: Token Priority是连接原始数据与理想对齐流形的关键桥梁，为未来研究提供了新的理论方向和挑战识别

Abstract: The transition from fitting empirical data to achieving true human utility is fundamentally constrained by a granularity mismatch, where fine-grained autoregressive generation is often supervised by coarse or uniform signals. This position paper advocates Token Priority as the essential bridge, formalizing Supervised Fine-Tuning (SFT) not as simple optimization but as a precise distribution reshaping process that aligns raw data with the ideal alignment manifold. We analyze recent breakthroughs through this unified lens, categorizing them into two distinct regimes: Positive Priority for noise filtration and Signed Priority for toxic modes unlearning. We revisit existing progress and limitations, identify key challenges, and suggest directions for future research.

</details>


### [45] [Inferential Question Answering](https://arxiv.org/abs/2602.01239)
*Jamshid Mozafari,Hamed Zamani,Guido Zuccon,Adam Jatowt*

Main category: cs.CL

Relevance: 85.0

TL;DR: 本文提出了"推理式问答"新任务，要求模型从仅提供线索的支持性段落中推断答案，而非直接提取。构建了QUIT数据集（7,401个问题，240万段落），通过全面评估发现现有QA方法在推理任务上表现不佳。


<details>
  <summary>Details</summary>
Motivation: 现有问答系统大多关注答案直接提取，但许多实际问题需要从文本线索中进行推理。当前方法在需要推理的问答任务上存在不足，需要建立新的评估框架来推动模型推理能力发展。

Method: 1) 定义推理式问答新任务；2) 构建QUIT数据集，包含人类和机器生成的线索，使用LLM进行可回答性标注和人工验证；3) 全面评估检索器、重排序器和LLM阅读器在推理任务上的表现。

Result: 1) 传统检索器在推理任务上表现不佳；2) 重排序器提升有限；3) 微调效果不一致；4) 推理导向的LLM未能超越更小的通用模型；5) 当前QA流水线尚未准备好处理基于推理的任务。

Conclusion: 推理式问答代表了QA任务的新类别，要求从间接文本证据中进行理解和推理。当前方法在该任务上存在显著挑战，需要开发新的技术来提升模型的推理能力。

Abstract: Despite extensive research on a wide range of question answering (QA) systems, most existing work focuses on answer containment-i.e., assuming that answers can be directly extracted and/or generated from documents in the corpus. However, some questions require inference, i.e., deriving answers that are not explicitly stated but can be inferred from the available information. We introduce Inferential QA -- a new task that challenges models to infer answers from answer-supporting passages which provide only clues. To study this problem, we construct QUIT (QUestions requiring Inference from Texts) dataset, comprising 7,401 questions and 2.4M passages built from high-convergence human- and machine-authored hints, labeled across three relevance levels using LLM-based answerability and human verification. Through comprehensive evaluation of retrievers, rerankers, and LLM-based readers, we show that methods effective on traditional QA tasks struggle in inferential QA: retrievers underperform, rerankers offer limited gains, and fine-tuning provides inconsistent improvements. Even reasoning-oriented LLMs fail to outperform smaller general-purpose models. These findings reveal that current QA pipelines are not yet ready for inference-based reasoning. Inferential QA thus establishes a new class of QA tasks that move towards understanding and reasoning from indirect textual evidence.

</details>


### [46] [Minimizing Mismatch Risk: A Prototype-Based Routing Framework for Zero-shot LLM-generated Text Detection](https://arxiv.org/abs/2602.01240)
*Ke Sun,Guangsheng Bao,Han Cui,Yue Zhang*

Main category: cs.CL

Relevance: 85.0

TL;DR: 提出DetectRouter框架，通过路由机制为不同输入选择最匹配的代理模型，提升LLM生成文本的零样本检测性能


<details>
  <summary>Details</summary>
Motivation: 现有零样本检测方法通常对所有输入使用固定的代理模型，忽略了代理模型与未知源模型之间的匹配问题。研究发现检测性能高度依赖于代理-源模型的对齐程度，需要更灵活的选择机制。

Method: 提出DetectRouter原型框架，采用两阶段训练：第一阶段从白盒模型构建判别性原型；第二阶段通过几何距离与检测分数对齐，泛化到黑盒源模型，实现基于路由的代理模型选择。

Result: 在EvoBench和MAGE基准测试中，DetectRouter在多个检测标准和模型家族上均表现出持续改进，验证了路由机制的有效性。

Conclusion: 将鲁棒检测转化为路由问题，通过为每个输入选择最合适的代理模型，显著提升LLM生成文本的检测性能，为可信AI中的文本检测提供了新思路。

Abstract: Zero-shot methods detect LLM-generated text by computing statistical signatures using a surrogate model. Existing approaches typically employ a fixed surrogate for all inputs regardless of the unknown source. We systematically examine this design and find that detection performance varies substantially depending on surrogate-source alignment. We observe that while no single surrogate achieves optimal performance universally, a well-matched surrogate typically exists within a diverse pool for any given input. This finding transforms robust detection into a routing problem: selecting the most appropriate surrogate for each input. We propose DetectRouter, a prototype-based framework that learns text-detector affinity through two-stage training. The first stage constructs discriminative prototypes from white-box models; the second generalizes to black-box sources by aligning geometric distances with observed detection scores. Experiments on EvoBench and MAGE benchmarks demonstrate consistent improvements across multiple detection criteria and model families.

</details>


### [47] [PACER: Blockwise Pre-verification for Speculative Decoding with Adaptive Length](https://arxiv.org/abs/2602.01274)
*Situo Zhang,Yifan Zhang,Zichen Zhu,Hankun Wang,Da Ma,Danyang Zhang,Lu Chen,Kai Yu*

Main category: cs.CL

Relevance: 85.0

TL;DR: Pacer是一种动态控制草稿长度的推测解码方法，通过轻量级可训练预验证层实现块级预验证，相比固定草稿长度的标准推测解码获得显著加速效果。


<details>
  <summary>Details</summary>
Motivation: 标准推测解码使用固定草稿长度，但实验发现不同解码步骤的最优草稿长度差异显著，固定长度限制了进一步加速的潜力。

Method: 提出Pacer方法，使用轻量级可训练预验证层动态控制草稿长度。该层在草稿令牌发送到目标模型前进行块级预验证，如果预验证失败则停止草稿模型生成令牌。

Result: Pacer在多个推测解码模型对和基准测试中，相比自回归解码获得最高2.66倍加速，始终优于标准推测解码。与Ouroboros集成时获得最高3.09倍加速。

Conclusion: 动态控制草稿长度能显著提升推测解码效率，Pacer方法通过轻量级预验证层实现这一目标，为LLM推理加速提供了有效解决方案。

Abstract: Speculative decoding (SD) is a powerful technique for accelerating the inference process of large language models (LLMs) without sacrificing accuracy. Typically, SD employs a small draft model to generate a fixed number of draft tokens, which are then verified in parallel by the target model. However, our experiments reveal that the optimal draft length varies significantly across different decoding steps. This variation suggests that using a fixed draft length limits the potential for further improvements in decoding speed. To address this challenge, we propose Pacer, a novel approach that dynamically controls draft length using a lightweight, trainable pre-verification layer. This layer pre-verifies draft tokens blockwise before they are sent to the target model, allowing the draft model to stop token generation if the blockwise pre-verification fails. We implement Pacer on multiple SD model pairs and evaluate its performance across various benchmarks. Our results demonstrate that Pacer achieves up to 2.66x Speedup over autoregressive decoding and consistently outperforms standard speculative decoding. Furthermore, when integrated with Ouroboros, Pacer attains up to 3.09x Speedup.

</details>


### [48] [EverMemBench: Benchmarking Long-Term Interactive Memory in Large Language ModelsEverMemBench: Benchmarking Long-Term Interactive Memory in Large Language Models](https://arxiv.org/abs/2602.01313)
*Chuanrui Hu,Tong Li,Xingze Gao,Hongda Chen,Dannong Xu,Yi Bai,Tianwei Lin,Xinda Zhao,Xiaohong Li,Jiaqi An,Yunyun Han,Jian Pei,Yafeng Deng*

Main category: cs.CL

Relevance: 85.0

TL;DR: EverMemBench：首个评估LLM长期对话记忆的基准，包含多参与者、多话题的复杂对话场景，揭示现有记忆系统在多跳推理、时序推理和记忆检索方面的严重缺陷


<details>
  <summary>Details</summary>
Motivation: 现有对话记忆基准局限于二元单话题对话，无法捕捉真实世界的复杂性，需要更全面的评估框架来推动下一代记忆架构的发展

Method: 构建EverMemBench基准，包含超过100万token的多参与者、多群组对话，具有时间演化信息、跨话题交织和角色特定人设，通过1000+QA对评估三个维度：细粒度回忆、记忆意识和用户画像理解

Result: 评估揭示关键局限：1) 多参与者场景下多跳推理崩溃，即使oracle模型准确率仅26%；2) 时序推理未解决，需要超越时间戳匹配的版本语义；3) 记忆意识受检索瓶颈，当前基于相似度的方法无法弥合查询与隐含相关记忆的语义鸿沟

Conclusion: EverMemBench为开发下一代记忆架构提供了具有挑战性的测试平台，揭示了现有记忆系统在复杂真实场景中的根本性缺陷

Abstract: Long-term conversational memory is essential for LLM-based assistants, yet existing benchmarks focus on dyadic, single-topic dialogues that fail to capture real-world complexity. We introduce EverMemBench, a benchmark featuring multi-party, multi-group conversations spanning over 1 million tokens with temporally evolving information, cross-topic interleaving, and role-specific personas. EverMemBench evaluates memory systems across three dimensions through 1,000+ QA pairs: fine-grained recall, memory awareness, and user profile understanding. Our evaluation reveals critical limitations: (1) multi-hop reasoning collapses in multi-party settings, with even oracle models achieving only 26%; (2) temporal reasoning remains unsolved, requiring version semantics beyond timestamp matching; (3) memory awareness is bottlenecked by retrieval, where current similarity-based methods fail to bridge the semantic gap between queries and implicitly relevant memories. EverMemBench provides a challenging testbed for developing next-generation memory architectures.

</details>


### [49] [DreamOn: Diffusion Language Models For Code Infilling Beyond Fixed-size Canvas](https://arxiv.org/abs/2602.01326)
*Zirui Wu,Lin Zheng,Zhihui Xie,Jiacheng Ye,Jiahui Gao,Shansan Gong,Yansong Feng,Zhenguo Li,Wei Bi,Guorui Zhou,Lingpeng Kong*

Main category: cs.CL

Relevance: 85.0

TL;DR: DreamOn提出了一种用于扩散语言模型（DLMs）的动态可变长度生成框架，解决了传统DLMs需要固定长度掩码序列的限制，显著提升了代码填充性能。


<details>
  <summary>Details</summary>
Motivation: 扩散语言模型作为自回归模型的替代方案，具有灵活、任意顺序填充的优势，但现有方法需要预定义固定长度的掩码序列，这在实际应用中严重限制了性能，特别是当掩码长度与理想完成长度不匹配时。

Method: DreamOn在扩散过程中引入了两个长度控制状态，使模型能够基于自身预测自主扩展或收缩输出长度。该方法只需对现有DLMs的训练目标进行最小修改，无需架构变更，可轻松集成到现有模型中。

Result: 基于Dream-Coder-7B和DiffuCoder-7B构建的DreamOn在HumanEval-Infilling和SantaCoder-FIM基准测试中，填充性能达到与最先进自回归模型相当的水平，并匹配使用真实长度获得的oracle性能。

Conclusion: DreamOn消除了DLMs实际部署的一个基本障碍，显著提升了其在可变长度生成任务中的灵活性和适用性，为扩散语言模型的实用化铺平了道路。

Abstract: Diffusion Language Models (DLMs) present a compelling alternative to autoregressive models, offering flexible, any-order infilling without specialized prompting design. However, their practical utility is blocked by a critical limitation: the requirement of a fixed-length masked sequence for generation. This constraint severely degrades code infilling performance when the predefined mask size mismatches the ideal completion length. To address this, we propose DreamOn, a novel diffusion framework that enables dynamic, variable-length generation. DreamOn augments the diffusion process with two length control states, allowing the model to autonomously expand or contract the output length based solely on its own predictions. We integrate this mechanism into existing DLMs with minimal modifications to the training objective and no architectural changes. Built upon Dream-Coder-7B and DiffuCoder-7B, DreamOn achieves infilling performance on par with state-of-the-art autoregressive models on HumanEval-Infilling and SantaCoder-FIM and matches oracle performance achieved with ground-truth length. Our work removes a fundamental barrier to the practical deployment of DLMs, significantly advancing their flexibility and applicability for variable-length generation. Our code is available at https://github.com/DreamLM/DreamOn.

</details>


### [50] [CRAFT: Calibrated Reasoning with Answer-Faithful Traces via Reinforcement Learning for Multi-Hop Question Answering](https://arxiv.org/abs/2602.01348)
*Yu Liu,Wenxiao Zhang,Cong Cao,Fangfang Yuan,Weizhuo Chen,Cheng Hu,Pin Xu,Yuling Yang,Kun Peng,Diandian Guo,Qiang Sun,Yanbing Liu,Jin B. Hong,Zhiyuan Ma*

Main category: cs.CL

Relevance: 85.0

TL;DR: CRAFT是一个基于强化学习的框架，通过双重奖励机制优化多跳问答中的推理过程，提高答案准确性和推理忠实度


<details>
  <summary>Details</summary>
Motivation: 当前检索增强生成在多跳问答中存在三个主要挑战：1) 推理崩溃 - 多跳组合和噪声检索导致推理不稳定；2) 推理-答案不一致 - LLM生成的不确定性可能导致正确答案缺乏忠实推理支持；3) 格式控制丢失 - 传统思维链生成常偏离结构化输出格式要求

Method: 提出CRAFT框架，基于Group Relative Policy Optimization强化学习，采用双重奖励机制：确定性奖励确保结构正确性，基于评判者的奖励验证语义忠实度。支持可控的推理轨迹变体，系统分析结构和规模对推理性能的影响

Result: 在三个多跳问答基准测试中，CRAFT在答案准确性和推理忠实度方面均有提升，CRAFT 7B模型在多个推理轨迹设置下与闭源LLM竞争性表现

Conclusion: CRAFT通过强化学习框架有效解决了多跳问答中的推理忠实度问题，双重奖励机制和可控推理轨迹分析为LLM推理优化提供了系统方法

Abstract: Retrieval-augmented generation (RAG) is widely used to ground Large Language Models (LLMs) for multi-hop question answering. Recent work mainly focused on improving answer accuracy via fine-tuning and structured or reinforcement-based optimization. However, reliable reasoning in response generation faces three challenges: 1) Reasoning Collapse. Reasoning in multi-hop QA is inherently complex due to multi-hop composition and is further destabilized by noisy retrieval. 2) Reasoning-answer inconsistency. Due to the intrinsic uncertainty of LLM generation and exposure to evidence--distractor mixtures, models may produce correct answers that are not faithfully supported by their intermediate reasoning or evidence. 3) Loss of format control. Traditional chain-of-thought generation often deviates from required structured output formats, leading to incomplete or malformed structured content. To address these challenges, we propose CRAFT (Calibrated Reasoning with Answer-Faithful Traces), a Group Relative Policy Optimization (GRPO) based reinforcement learning framework that trains models to perform faithful reasoning during response generation. CRAFT employs dual reward mechanisms to optimize multi-hop reasoning: deterministic rewards ensure structural correctness while judge-based rewards verify semantic faithfulness. This optimization framework supports controllable trace variants that enable systematic analysis of how structure and scale affect reasoning performance and faithfulness. Experiments on three multi-hop QA benchmarks show that CRAFT improves both answer accuracy and reasoning faithfulness across model scales, with the CRAFT 7B model achieving competitive performance with closed-source LLMs across multiple reasoning trace settings.

</details>


### [51] [Balancing Understanding and Generation in Discrete Diffusion Models](https://arxiv.org/abs/2602.01362)
*Yue Liu,Yuzhong Zhao,Zheyong Xie,Qixiang Ye,Jianbin Jiao,Yao Hu,Shaosheng Cao,Yunfan Liu*

Main category: cs.CL

Relevance: 85.0

TL;DR: XDLM提出了一种新的扩散语言模型，通过平稳噪声核统一了掩码扩散和均匀噪声扩散两种范式，在语义理解和少步生成质量之间取得了更好的平衡。


<details>
  <summary>Details</summary>
Motivation: 当前离散生成建模中存在两种主要范式：掩码扩散语言模型（MDLM）在语义理解和零样本泛化方面表现出色，而均匀噪声扩散语言模型（UDLM）在少步生成质量方面很强，但两者都无法在理解和生成之间取得平衡。需要一种能同时兼顾两种能力的统一方法。

Method: XDLM通过平稳噪声核桥接MDLM和UDLM两种范式：1）提供理论上的统一框架，将两种范式作为特例恢复；2）通过后验概率的代数简化缓解内存瓶颈。该方法在训练8B参数大语言模型时仅需32步就能达到15.0 MBPP分数。

Result: XDLM在理解和生成能力之间推进了帕累托前沿：零样本文本基准上超越UDLM 5.4分；少步图像生成FID为54.1（vs MDLM的80.8）；调优8B参数LLM时，仅用32步达到15.0 MBPP，性能翻倍。训练动态分析显示XDLM具有更好的长期扩展潜力。

Conclusion: XDLM成功统一了离散生成建模中的两种主要范式，在保持语义理解能力的同时显著提升了生成质量，为大语言模型的训练和推理提供了更高效的解决方案，具有重要的扩展潜力。

Abstract: In discrete generative modeling, two dominant paradigms demonstrate divergent capabilities: Masked Diffusion Language Models (MDLM) excel at semantic understanding and zero-shot generalization, whereas Uniform-noise Diffusion Language Models (UDLM) achieve strong few-step generation quality, yet neither attains balanced performance across both dimensions. To address this, we propose XDLM, which bridges the two paradigms via a stationary noise kernel. XDLM offers two key contributions: (1) it provides a principled theoretical unification of MDLM and UDLM, recovering each paradigm as a special case; and (2) an alleviated memory bottleneck enabled by an algebraic simplification of the posterior probabilities. Experiments demonstrate that XDLM advances the Pareto frontier between understanding capability and generation quality. Quantitatively, XDLM surpasses UDLM by 5.4 points on zero-shot text benchmarks and outperforms MDLM in few-step image generation (FID 54.1 vs. 80.8). When scaled to tune an 8B-parameter large language model, XDLM achieves 15.0 MBPP in just 32 steps, effectively doubling the baseline performance. Finally, analysis of training dynamics reveals XDLM's superior potential for long-term scaling. Code is available at https://github.com/MzeroMiko/XDLM

</details>


### [52] [Context Dependence and Reliability in Autoregressive Language Models](https://arxiv.org/abs/2602.01378)
*Poushali Sengupta,Shashi Raj Pandey,Sabita Maharjan,Frank Eliassen*

Main category: cs.CL

Relevance: 85.0

TL;DR: 提出RISE方法解决LLM解释中冗余上下文导致的归因不稳定问题，通过量化每个输入的独特影响来提供更清晰稳定的解释


<details>
  <summary>Details</summary>
Motivation: LLM生成输出时使用大量上下文，其中常包含冗余信息。标准解释方法难以处理冗余和重叠上下文，导致归因分数不稳定，影响可解释性并带来安全风险（如提示注入）。需要区分真正影响输出的上下文元素与相关元素。

Method: 提出RISE（冗余不敏感解释评分）方法，量化每个输入相对于其他输入的独特影响，最小化冗余影响，提供更清晰稳定的归因。

Result: 实验表明RISE比传统方法提供更稳健的解释，强调条件信息对于可信LLM解释和监控的重要性。

Conclusion: RISE方法能有效解决LLM解释中的冗余问题，提供更可靠稳定的归因，对于构建可信的LLM解释和监控系统具有重要意义。

Abstract: Large language models (LLMs) generate outputs by utilizing extensive context, which often includes redundant information from prompts, retrieved passages, and interaction history. In critical applications, it is vital to identify which context elements actually influence the output, as standard explanation methods struggle with redundancy and overlapping context. Minor changes in input can lead to unpredictable shifts in attribution scores, undermining interpretability and raising concerns about risks like prompt injection. This work addresses the challenge of distinguishing essential context elements from correlated ones. We introduce RISE (Redundancy-Insensitive Scoring of Explanation), a method that quantifies the unique influence of each input relative to others, minimizing the impact of redundancies and providing clearer, stable attributions. Experiments demonstrate that RISE offers more robust explanations than traditional methods, emphasizing the importance of conditional information for trustworthy LLM explanations and monitoring.

</details>


### [53] [On the Power of (Approximate) Reward Models for Inference-Time Scaling](https://arxiv.org/abs/2602.01381)
*Youheng Zhu,Yiping Lu*

Main category: cs.CL

Relevance: 85.0

TL;DR: 论文从理论上证明了在推理时缩放中使用近似奖励模型的有效性条件：当近似奖励模型的Bellman误差为O(1/T)时，结合SMC可以将推理计算复杂度从指数级降低到多项式级。


<details>
  <summary>Details</summary>
Motivation: 当前推理时缩放方法依赖近似奖励模型来评估中间推理轨迹，但缺乏理论分析解释为什么近似奖励模型在实际中有效。本文旨在从理论上回答：近似奖励模型何时以及为什么能支持有效的推理时缩放。

Method: 采用理论分析方法，将推理过程建模为序列决策问题，分析Sequential Monte Carlo (SMC)框架与近似奖励模型的结合。核心是识别近似奖励模型的Bellman误差作为关键理论指标。

Result: 理论证明：对于长度为T的推理过程，如果近似奖励模型的Bellman误差有界为O(1/T)，那么结合SMC可以将推理计算复杂度从指数级(exp(T))降低到多项式级(poly(T))，实现指数级的推理效率提升。

Conclusion: 近似奖励模型的Bellman误差是决定推理时缩放效果的关键因素。当误差足够小(O(1/T))时，即使使用近似奖励模型，也能通过SMC实现指数级的推理效率改进，这为实际部署提供了理论保证。

Abstract: Inference-time scaling has recently emerged as a powerful paradigm for improving the reasoning capability of large language models. Among various approaches, Sequential Monte Carlo (SMC) has become a particularly important framework, enabling iterative generation, evaluation, rejection, and resampling of intermediate reasoning trajectories. A central component in this process is the reward model, which evaluates partial solutions and guides the allocation of computation during inference.
  However, in practice, true reward models are never available. All deployed systems rely on approximate reward models, raising a fundamental question: Why and when do approximate reward models suffice for effective inference-time scaling? In this work, we provide a theoretical answer. We identify the Bellman error of the approximate reward model as the key quantity governing the effectiveness of SMC-based inference-time scaling. For a reasoning process of length $T$, we show that if the Bellman error of the approximate reward model is bounded by $O(1/T)$, then combining this reward model with SMC reduces the computational complexity of reasoning from exponential in $T$ to polynomial in $T$. This yields an exponential improvement in inference efficiency despite using only approximate rewards.

</details>


### [54] [Rethinking Selective Knowledge Distillation](https://arxiv.org/abs/2602.01395)
*Almog Tavor,Itay Ebenspanger,Neil Cnaan,Mor Geva*

Main category: cs.CL

Relevance: 85.0

TL;DR: 该论文系统研究了自回归大语言模型中的选择性知识蒸馏，提出了基于学生熵的位置选择方法(SE-KD)，在多个维度上实现了效率提升和性能改进。


<details>
  <summary>Details</summary>
Motivation: 当前大语言模型知识蒸馏中，虽然已有研究使用选择性蒸馏（选择部分token位置、词汇类别或训练样本进行监督），但缺乏对重要性信号、选择策略及其相互作用的系统性理解。需要明确在自回归LLMs中应该在何处以及如何进行蒸馏。

Method: 1. 沿位置、类别和样本三个维度解耦选择性知识蒸馏；2. 系统比较重要性信号和选择策略；3. 提出学生熵引导的位置选择方法(SE-KD)；4. 将方法扩展到类别和样本维度(SE-KD 3X)。

Result: SE-KD在多个基准测试中通常比密集蒸馏提高了准确性、下游任务遵循性和内存效率。SE-KD 3X进一步实现了互补的效率增益，使离线教师缓存可行，实际应用中减少了70%的wall time、18%的峰值内存和80%的存储使用，同时不牺牲性能。

Conclusion: 通过系统性分析选择性知识蒸馏，提出了有效的学生熵引导方法，在位置、类别和样本三个维度上实现了显著效率提升，为大语言模型的高效知识蒸馏提供了实用解决方案。

Abstract: Growing efforts to improve knowledge distillation (KD) in large language models (LLMs) replace dense teacher supervision with selective distillation, which uses a subset of token positions, vocabulary classes, or training samples for supervision. However, it remains unclear which importance signals, selection policies, and their interplay are most effective. In this work, we revisit where and how to distill in autoregressive LLMs. We disentangle selective KD along the position, class, and sample axes and systematically compare importance signals and selection policies. Then, guided by this analysis, we identify underexplored opportunities and introduce student-entropy-guided position selection (SE-KD). Across a suite of benchmarks, SE-KD often improves accuracy, downstream task adherence, and memory efficiency over dense distillation. Extending this approach across the class and sample axes (SE-KD 3X) yields complementary efficiency gains that make offline teacher caching feasible. In practice, this reduces wall time by 70% and peak memory by 18%, while cutting storage usage by 80% over prior methods without sacrificing performance.

</details>


### [55] [Understanding QA generation: Extracting Parametric and Contextual Knowledge with CQA for Low Resource Bangla Language](https://arxiv.org/abs/2602.01451)
*Umme Abira Azmary,MD Ikramul Kayes,Swakkhar Shatabda,Farig Yousuf Sadeque*

Main category: cs.CL

Relevance: 85.0

TL;DR: 提出了首个孟加拉语反事实问答数据集BanglaCQA，用于分析模型在低资源语言中如何利用参数化知识和上下文知识，并发现思维链提示在反事实场景中特别有效。


<details>
  <summary>Details</summary>
Motivation: 低资源语言（如孟加拉语）的问答模型面临标注数据有限和语言复杂性的挑战，现有数据集缺乏分析模型依赖参数化知识还是上下文知识的结构。需要专门的数据集来研究模型在低资源语言中的知识来源。

Method: 1) 创建BanglaCQA数据集，扩展现有孟加拉语数据集，集成反事实段落和可回答性标注；2) 为编码器-解码器模型设计微调流程；3) 为仅解码器LLM设计基于提示的流程；4) 应用基于LLM和人工的语义相似度评估技术。

Result: 思维链提示在反事实场景中特别有效，尤其是在仅解码器LLM中，能更好地提取参数化知识。研究揭示了模型在不同问答设置下的表现差异，为低资源语言的反事实推理提供了新见解。

Conclusion: 该工作不仅为分析孟加拉语QA中的知识来源提供了新框架，还揭示了反事实推理在低资源语言环境中的关键发现，为未来研究开辟了更广泛的方向。

Abstract: Question-Answering (QA) models for low-resource languages like Bangla face challenges due to limited annotated data and linguistic complexity. A key issue is determining whether models rely more on pre-encoded (parametric) knowledge or contextual input during answer generation, as existing Bangla QA datasets lack the structure required for such analysis. We introduce BanglaCQA, the first Counterfactual QA dataset in Bangla, by extending a Bangla dataset while integrating counterfactual passages and answerability annotations. In addition, we propose fine-tuned pipelines for encoder-decoder language-specific and multilingual baseline models, and prompting-based pipelines for decoder-only LLMs to disentangle parametric and contextual knowledge in both factual and counterfactual scenarios. Furthermore, we apply LLM-based and human evaluation techniques that measure answer quality based on semantic similarity. We also present a detailed analysis of how models perform across different QA settings in low-resource languages, and show that Chain-of-Thought (CoT) prompting reveals a uniquely effective mechanism for extracting parametric knowledge in counterfactual scenarios, particularly in decoder-only LLMs. Our work not only introduces a novel framework for analyzing knowledge sources in Bangla QA but also uncovers critical findings that open up broader directions for counterfactual reasoning in low-resource language settings.

</details>


### [56] [ConPress: Learning Efficient Reasoning from Multi-Question Contextual Pressure](https://arxiv.org/abs/2602.01472)
*Jie Deng,Shining Liang,Jun Li,Hongzhi Li,Yutao Xie*

Main category: cs.CL

Relevance: 85.0

TL;DR: 论文提出ConPress方法，通过多问题上下文压力诱导LLM自我压缩推理轨迹，实现推理token减少59%（MATH500）和33%（AIME25）的同时保持准确率


<details>
  <summary>Details</summary>
Motivation: 大型推理模型通过生成长链式思维轨迹解决推理密集型任务，导致显著的推理开销。作者发现当多个独立可回答的问题出现在单个提示中时，模型会自发产生更短的推理轨迹，这种现象称为"自我压缩"

Method: ConPress：基于上下文压力的轻量级自监督微调方法。构建多问题提示诱导自我压缩，采样模型输出，解析和过滤每个问题的轨迹以获得简洁正确的推理轨迹，然后用于监督微调，在单问题设置中内化压缩推理行为

Result: 仅用8k微调样本，在MATH500上减少59%推理token使用，在AIME25上减少33%推理token使用，同时保持有竞争力的准确率

Conclusion: 通过上下文压力诱导的自我压缩是有效的推理优化策略，ConPress方法能够显著减少推理开销而不损失准确性，为大型语言模型的高效推理提供了新思路

Abstract: Large reasoning models (LRMs) typically solve reasoning-intensive tasks by generating long chain-of-thought (CoT) traces, leading to substantial inference overhead. We identify a reproducible inference-time phenomenon, termed Self-Compression: when multiple independent and answerable questions are presented within a single prompt, the model spontaneously produces shorter reasoning traces for each question. This phenomenon arises from multi-question contextual pressure during generation and consistently manifests across models and benchmarks. Building on this observation, we propose ConPress (Learning from Contextual Pressure), a lightweight self-supervised fine-tuning approach. ConPress constructs multi-question prompts to induce self-compression, samples the resulting model outputs, and parses and filters per-question traces to obtain concise yet correct reasoning trajectories. These trajectories are directly used for supervised fine-tuning, internalizing compressed reasoning behavior in single-question settings without external teachers, manual pruning, or reinforcement learning. With only 8k fine-tuning examples, ConPress reduces reasoning token usage by 59% on MATH500 and 33% on AIME25, while maintaining competitive accuracy.

</details>


### [57] [Alternating Reinforcement Learning for Rubric-Based Reward Modeling in Non-Verifiable LLM Post-Training](https://arxiv.org/abs/2602.01511)
*Ran Xu,Tianci Liu,Zihan Dong,Tony You,Ilgee Hong,Carl Yang,Linjun Zhang,Tao Zhao,Haoyu Wang*

Main category: cs.CL

Relevance: 85.0

TL;DR: 提出Rubric-ARM框架，通过强化学习联合优化评分标准生成器和评判器，解决传统奖励模型在非可验证领域（如创意写作）中单一标量评分不足的问题。


<details>
  <summary>Details</summary>
Motivation: 传统奖励模型通常预测单一标量分数，无法捕捉非可验证领域（如创意写作、开放式指令遵循）中响应质量的多维度特性。现有方法依赖静态评分标准或分离的训练流程，限制了模型对复杂质量维度的理解。

Method: 提出Rubric-ARM框架：1）将评分标准生成作为潜在动作学习，最大化评判准确性；2）采用交替优化策略缓解同时更新的非平稳性问题；3）提供理论分析证明该调度能减少训练中的梯度方差。

Result: 在多个基准测试中达到最先进的性能，显著提升了离线强化学习和在线强化学习设置中的下游策略对齐效果。

Conclusion: Rubric-ARM通过联合学习评分标准和评判器，有效解决了非可验证领域中的奖励建模问题，为复杂质量评估提供了更细粒度的解决方案。

Abstract: Standard reward models typically predict scalar scores that fail to capture the multifaceted nature of response quality in non-verifiable domains, such as creative writing or open-ended instruction following. To address this limitation, we propose Rubric-ARM, a framework that jointly optimizes a rubric generator and a judge using reinforcement learning from preference feedback. Unlike existing methods that rely on static rubrics or disjoint training pipelines, our approach treats rubric generation as a latent action learned to maximize judgment accuracy. We introduce an alternating optimization strategy to mitigate the non-stationarity of simultaneous updates, providing theoretical analysis that demonstrates how this schedule reduces gradient variance during training. Extensive experiments show that Rubric-ARM achieves state-of-the-art performance among baselines on multiple benchmarks and significantly improves downstream policy alignment in both offline and online reinforcement learning settings.

</details>


### [58] [FS-Researcher: Test-Time Scaling for Long-Horizon Research Tasks with File-System-Based Agents](https://arxiv.org/abs/2602.01566)
*Chiwei Zhu,Benfeng Xu,Mingxuan Du,Shaohan Wang,Xiaorui Wang,Zhendong Mao,Yongdong Zhang*

Main category: cs.CL

Relevance: 85.0

TL;DR: FS-Researcher是一个基于文件系统的双智能体框架，通过持久化工作空间解决LLM在深度研究任务中上下文长度限制问题，实现超越上下文窗口的深度研究扩展。


<details>
  <summary>Details</summary>
Motivation: 深度研究作为LLM智能体的代表性长时程任务，其长轨迹经常超出模型上下文限制，压缩了证据收集和报告编写的token预算，阻碍了有效的测试时扩展。

Method: 采用基于文件系统的双智能体框架：1) Context Builder智能体作为图书管理员浏览互联网、编写结构化笔记、将原始资料归档到可超越上下文长度的分层知识库；2) Report Writer智能体将知识库作为事实来源，逐节编写最终报告。文件系统作为持久外部内存和跨智能体/会话的共享协调媒介。

Result: 在两个开放基准测试（DeepResearch Bench和DeepConsult）上，FS-Researcher在不同骨干模型上实现了最先进的报告质量。分析显示最终报告质量与分配给Context Builder的计算量呈正相关，验证了文件系统范式下的有效测试时扩展。

Conclusion: FS-Researcher通过文件系统作为外部内存和协调媒介，成功解决了深度研究中上下文长度限制问题，实现了超越上下文窗口的扩展，为LLM智能体的长时程任务提供了有效解决方案。

Abstract: Deep research is emerging as a representative long-horizon task for large language model (LLM) agents. However, long trajectories in deep research often exceed model context limits, compressing token budgets for both evidence collection and report writing, and preventing effective test-time scaling. We introduce FS-Researcher, a file-system-based, dual-agent framework that scales deep research beyond the context window via a persistent workspace. Specifically, a Context Builder agent acts as a librarian which browses the internet, writes structured notes, and archives raw sources into a hierarchical knowledge base that can grow far beyond context length. A Report Writer agent then composes the final report section by section, treating the knowledge base as the source of facts. In this framework, the file system serves as a durable external memory and a shared coordination medium across agents and sessions, enabling iterative refinement beyond the context window. Experiments on two open-ended benchmarks (DeepResearch Bench and DeepConsult) show that FS-Researcher achieves state-of-the-art report quality across different backbone models. Further analyses demonstrate a positive correlation between final report quality and the computation allocated to the Context Builder, validating effective test-time scaling under the file-system paradigm. The code and data are anonymously open-sourced at https://github.com/Ignoramus0817/FS-Researcher.

</details>


### [59] [LLM-based Embeddings: Attention Values Encode Sentence Semantics Better Than Hidden States](https://arxiv.org/abs/2602.01572)
*Yeqin Zhang,Yunfei Wang,Jiaxuan Chen,Ke Qin,Yizheng Zhao,Cam-Tu Nguyen*

Main category: cs.CL

Relevance: 85.0

TL;DR: 本文提出Value Aggregation(VA)方法，通过聚合注意力值向量而非隐藏状态来获得更好的句子表示，在无需训练的情况下超越了其他LLM嵌入方法，甚至匹配或超越了基于集成的MetaEOL。


<details>
  <summary>Details</summary>
Motivation: 现有方法主要依赖LLM的最终层隐藏状态来获取句子表示，但这些隐藏状态是为下一个token预测优化的，往往无法有效捕捉全局的句子级语义。需要找到更好的方法来从LLM中提取句子级语义表示。

Method: 提出Value Aggregation(VA)方法，通过聚合多个层和token索引的token值向量来获得句子表示。进一步提出Aligned Weighted VA(AlignedWVA)，利用最后一层token的注意力分数作为权重，通过输出投影矩阵(W_O)对齐加权值向量到LLM残差流的公共空间。

Result: 在无需训练的情况下，VA超越了其他基于LLM的嵌入方法，甚至匹配或超越了基于集成的MetaEOL。AlignedWVA在无需训练的LLM嵌入方法中达到了最先进的性能，大幅超越了高成本的MetaEOL。通过微调VA还可以获得更强的LLM嵌入模型。

Conclusion: 注意力值向量比隐藏状态能更好地捕捉句子语义，Value Aggregation提供了一种简单而有效的方法来从LLM中提取高质量的句子表示，无需训练即可达到最先进性能。

Abstract: Sentence representations are foundational to many Natural Language Processing (NLP) applications. While recent methods leverage Large Language Models (LLMs) to derive sentence representations, most rely on final-layer hidden states, which are optimized for next-token prediction and thus often fail to capture global, sentence-level semantics. This paper introduces a novel perspective, demonstrating that attention value vectors capture sentence semantics more effectively than hidden states. We propose Value Aggregation (VA), a simple method that pools token values across multiple layers and token indices. In a training-free setting, VA outperforms other LLM-based embeddings, even matches or surpasses the ensemble-based MetaEOL. Furthermore, we demonstrate that when paired with suitable prompts, the layer attention outputs can be interpreted as aligned weighted value vectors. Specifically, the attention scores of the last token function as the weights, while the output projection matrix ($W_O$) aligns these weighted value vectors with the common space of the LLM residual stream. This refined method, termed Aligned Weighted VA (AlignedWVA), achieves state-of-the-art performance among training-free LLM-based embeddings, outperforming the high-cost MetaEOL by a substantial margin. Finally, we highlight the potential of obtaining strong LLM embedding models through fine-tuning Value Aggregation.

</details>


### [60] [Provable Defense Framework for LLM Jailbreaks via Noise-Augumented Alignment](https://arxiv.org/abs/2602.01587)
*Zehua Cheng,Jianwei Yang,Wei Dai,Jiahao Sun*

Main category: cs.CL

Relevance: 85.0

TL;DR: 论文提出了一种可认证的鲁棒性框架CSS，通过分层随机消融和噪声增强对齐调优，为LLM提供对抗性攻击的确定性安全保证，显著降低攻击成功率同时保持良性效用。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型仍然容易受到自适应越狱攻击，现有防御方法（如GCG）只能提供经验性保护，缺乏理论上的安全保证。需要一种能够提供确定性安全证书的鲁棒性框架。

Method: 1. 提出认证语义平滑(CSS)框架，通过分层随机消融技术将输入划分为不可变的结构提示和可变的有效载荷；2. 使用超几何分布推导严格的l0范数保证；3. 采用噪声增强对齐调优(NAAT)将基础模型转换为语义去噪器，解决稀疏上下文下的性能退化问题。

Result: 在Llama-3上的实验显示：梯度攻击的攻击成功率从84.2%降至1.2%，同时保持94.1%的良性效用，显著优于字符级基线方法（效用降至74.3%）。

Conclusion: 该框架为LLM提供了对抗性攻击的确定性安全证书，确保模型在可证明半径内对所有对抗性变体保持鲁棒性，同时保持了高水平的良性效用。

Abstract: Large Language Models (LLMs) remain vulnerable to adaptive jailbreaks that easily bypass empirical defenses like GCG. We propose a framework for certifiable robustness that shifts safety guarantees from single-pass inference to the statistical stability of an ensemble. We introduce Certified Semantic Smoothing (CSS) via Stratified Randomized Ablation, a technique that partitions inputs into immutable structural prompts and mutable payloads to derive rigorous lo norm guarantees using the Hypergeometric distribution. To resolve performance degradation on sparse contexts, we employ Noise-Augmented Alignment Tuning (NAAT), which transforms the base model into a semantic denoiser. Extensive experiments on Llama-3 show that our method reduces the Attack Success Rate of gradient-based attacks from 84.2% to 1.2% while maintaining 94.1% benign utility, significantly outperforming character-level baselines which degrade utility to 74.3%. This framework provides a deterministic certificate of safety, ensuring that a model remains robust against all adversarial variants within a provable radius.

</details>


### [61] [The Art of Socratic Inquiry: A Framework for Proactive Template-Guided Therapeutic Conversation Generation](https://arxiv.org/abs/2602.01598)
*Mingwen Zhang,Minqiang Yang,Changsheng Ma,Yang Yu,Hui Bai,Chen Xu,Xiangzhen Kong,Bin Hu*

Main category: cs.CL

Relevance: 85.0

TL;DR: 提出Socratic Inquiry Framework (SIF)，一个轻量级、即插即用的治疗意图规划器，将LLM从被动倾听者转变为主动认知引导者，解决当前心理LLM过于被动的问题。


<details>
  <summary>Details</summary>
Motivation: 当前心理大语言模型(LLM)主要处于被动反应模式，提供共情但肤浅的回应，无法揭示潜在信念或引导行为改变。而认知行为治疗(CBT)中的主动提问是核心治疗技术，需要LLM能够像治疗师一样主动引导认知探索。

Method: 提出Socratic Inquiry Framework (SIF)，通过策略锚定(Strategy Anchoring)决定"何时提问"，通过模板检索(Template Retrieval)决定"提问什么"，实现上下文感知、理论基础的提问，无需端到端重新训练。同时创建Socratic-QA数据集，提供策略对齐的苏格拉底式序列监督。

Result: SIF显著提升了主动提问频率、对话深度和治疗对齐性，实现了从被动安慰到主动探索的明显转变。

Conclusion: 该工作为心理LLM建立了新范式：不仅是回应，更是引导。SIF框架为LLM在治疗对话中的主动引导能力提供了有效解决方案。

Abstract: Proactive questioning, where therapists deliberately initiate structured, cognition-guiding inquiries, is a cornerstone of cognitive behavioral therapy (CBT). Yet, current psychological large language models (LLMs) remain overwhelmingly reactive, defaulting to empathetic but superficial responses that fail to surface latent beliefs or guide behavioral change. To bridge this gap, we propose the \textbf{Socratic Inquiry Framework (SIF)}, a lightweight, plug-and-play therapeutic intent planner that transforms LLMs from passive listeners into active cognitive guides. SIF decouples \textbf{when to ask} (via Strategy Anchoring) from \textbf{what to ask} (via Template Retrieval), enabling context-aware, theory-grounded questioning without end-to-end retraining. Complementing SIF, we introduce \textbf{Socratic-QA}, a high-quality dataset of strategy-aligned Socratic sequences that provides explicit supervision for proactive reasoning. Experiments show that SIF significantly enhances proactive questioning frequency, conversational depth, and therapeutic alignment, marking a clear shift from reactive comfort to proactive exploration. Our work establishes a new paradigm for psychologically informed LLMs: not just to respond, but to guide.

</details>


### [62] [Steering Vector Fields for Context-Aware Inference-Time Control in Large Language Models](https://arxiv.org/abs/2602.01654)
*Jiaqian Li,Yanshu Li,Kuan-Hao Huang*

Main category: cs.CL

Relevance: 85.0

TL;DR: 本文提出Steering Vector Fields (SVF)，一种上下文相关的隐层激活控制方法，通过可微概念评分函数的局部梯度定义每个激活的调控方向，解决了传统静态steering vectors在多上下文、长文本和多属性控制中的不可靠问题。


<details>
  <summary>Details</summary>
Motivation: 传统steering vectors（SVs）在推理时通过偏移隐层激活来控制LLMs，是介于提示和微调之间的实用方法。但在实践中存在不可靠问题：某些概念无法调控，即使平均效果良好也可能对部分输入产生反效果，且在长文本生成和多属性调控中可靠性下降。核心问题是静态SV在所有表示空间应用相同的更新向量，假设概念改进方向在不同上下文中恒定，而实际上有效方向随当前激活变化。

Method: 提出Steering Vector Fields (SVF)方法，学习一个可微的概念评分函数，其局部梯度定义每个激活的调控方向，使干预明确依赖于上下文。该框架支持在共享对齐概念空间中进行协调的多层干预，并在统一框架内实现高效的长文本和多属性控制。

Result: 在多个LLMs和调控任务上，SVF提供了更强且更可靠的控制，提升了推理时调控的实用性。

Conclusion: 通过几何视角分析传统SVs的失败原因，提出上下文相关的SVF方法，显著改进了LLMs推理时调控的可靠性和效果。

Abstract: Steering vectors (SVs) offer a lightweight way to control large language models (LLMs) at inference time by shifting hidden activations, providing a practical middle ground between prompting and fine-tuning. Yet SVs can be unreliable in practice. Some concepts are unsteerable, and even when steering helps on average it can backfire for a non-trivial fraction of inputs. Reliability also degrades in long-form generation and multi-attribute steering. We take a geometric view of these failures. A static SV applies the same update vector everywhere in representation space, implicitly assuming that the concept-improving direction is constant across contexts. When the locally effective direction varies with the current activation, a single global vector can become misaligned, which yields weak or reversed effects. Guided by this perspective, we propose Steering Vector Fields (SVF), which learns a differentiable concept scoring function whose local gradient defines the steering direction at each activation, making interventions explicitly context-dependent. This formulation supports coordinated multi-layer interventions in a shared, aligned concept space, and enables efficient long-form and multi-attribute control within a unified framework. Across multiple LLMs and steering tasks, SVF delivers stronger and more reliable control, improving the practicality of inference-time steering.

</details>


### [63] [CoDiQ: Test-Time Scaling for Controllable Difficult Question Generation](https://arxiv.org/abs/2602.01660)
*Zhongyuan Peng,Caijun Xu,Changyi Xiao,Shibo Hong,Eli Zhang,Stephen Huang,Yixin Cao*

Main category: cs.CL

Relevance: 85.0

TL;DR: CoDiQ框架通过测试时缩放实现细粒度难度控制，生成高质量竞赛级问题，构建了44K问题语料库，显著提升大型推理模型的推理能力。


<details>
  <summary>Details</summary>
Motivation: 现有自动问题生成方法缺乏精确难度控制、计算成本高，难以大规模生成竞赛级问题。大型推理模型需要具有挑战性的训练数据来提升推理能力。

Method: 提出CoDiQ框架：1) 发现测试时缩放趋势（扩展推理token预算增加难度但降低可解性）；2) 识别模型生成有效高难度问题的内在属性上限；3) 基于Qwen3-8B开发CoDiQ-Generator提升高难度问题生成上限；4) 构建CoDiQ-Corpus（44K竞赛级问题序列）。

Result: CoDiQ-Corpus中问题比LiveCodeBench/AIME显著更具挑战性，同时保持超过82%的可解性。在CoDiQ-Corpus上训练的大型推理模型推理性能显著提升。

Conclusion: 通过缩放控制难度训练问题能有效增强推理能力。开源CoDiQ-Corpus、CoDiQ-Generator及相关实现以支持相关研究。

Abstract: Large Reasoning Models (LRMs) benefit substantially from training on challenging competition-level questions. However, existing automated question synthesis methods lack precise difficulty control, incur high computational costs, and struggle to generate competition-level questions at scale. In this paper, we propose CoDiQ (Controllable Difficult Question Generation), a novel framework enabling fine-grained difficulty control via test-time scaling while ensuring question solvability. Specifically, first, we identify a test-time scaling tendency (extended reasoning token budget boosts difficulty but reduces solvability) and the intrinsic properties defining the upper bound of a model's ability to generate valid, high-difficulty questions. Then, we develop CoDiQ-Generator from Qwen3-8B, which improves the upper bound of difficult question generation, making it particularly well-suited for challenging question construction. Building on the CoDiQ framework, we build CoDiQ-Corpus (44K competition-grade question sequences). Human evaluations show these questions are significantly more challenging than LiveCodeBench/AIME with over 82% solvability. Training LRMs on CoDiQ-Corpus substantially improves reasoning performance, verifying that scaling controlled-difficulty training questions enhances reasoning capabilities. We open-source CoDiQ-Corpus, CoDiQ-Generator, and implementations to support related research.

</details>


### [64] [Scaling Search-Augmented LLM Reasoning via Adaptive Information Control](https://arxiv.org/abs/2602.01672)
*Siheng Xiong,Oguzhan Gungordu,Blair Johnson,James C. Kerce,Faramarz Fekri*

Main category: cs.CL

Relevance: 85.0

TL;DR: DeepControl：基于信息效用的自适应检索控制框架，通过检索延续性和粒度控制机制，在搜索增强推理中实现更高效的信息获取，相比基于结果的强化学习方法有显著性能提升。


<details>
  <summary>Details</summary>
Motivation: 搜索增强推理代理在推理过程中进行外部信息检索，但无控制的检索会导致冗余证据、上下文饱和和学习不稳定。现有基于结果的强化学习方法对信息获取的调节指导有限。

Method: 提出基于信息效用的自适应控制框架，信息效用衡量在给定推理状态下检索证据的边际价值。引入检索延续性控制（何时继续/停止检索）和粒度控制（扩展多少信息），采用退火控制策略使代理在训练中内化有效的信息获取行为。

Result: 在7个基准测试中一致优于强基线方法。在Qwen2.5-7B和Qwen2.5-3B上分别实现9.4%和8.6%的平均性能提升，优于基于结果的强化学习基线，也优于无检索和基于检索但无显式信息控制的方法。

Conclusion: 自适应信息控制对于将搜索增强推理代理扩展到复杂、真实世界的信息环境至关重要。DeepControl框架通过形式化的信息效用概念和相应的控制机制，显著提升了检索效率和推理性能。

Abstract: Search-augmented reasoning agents interleave multi-step reasoning with external information retrieval, but uncontrolled retrieval often leads to redundant evidence, context saturation, and unstable learning. Existing approaches rely on outcome-based reinforcement learning (RL), which provides limited guidance for regulating information acquisition. We propose DeepControl, a framework for adaptive information control based on a formal notion of information utility, which measures the marginal value of retrieved evidence under a given reasoning state. Building on this utility, we introduce retrieval continuation and granularity control mechanisms that selectively regulate when to continue and stop retrieval, and how much information to expand. An annealed control strategy enables the agent to internalize effective information acquisition behaviors during training. Extensive experiments across seven benchmarks demonstrate that our method consistently outperforms strong baselines. In particular, our approach achieves average performance improvements of 9.4% and 8.6% on Qwen2.5-7B and Qwen2.5-3B, respectively, over strong outcome-based RL baselines, and consistently outperforms both retrieval-free and retrieval-based reasoning methods without explicit information control. These results highlight the importance of adaptive information control for scaling search-augmented reasoning agents to complex, real-world information environments.

</details>


### [65] [Counting Hypothesis: Potential Mechanism of In-Context Learning](https://arxiv.org/abs/2602.01687)
*Jung H. Lee,Sujith Vijayan*

Main category: cs.CL

Relevance: 85.0

TL;DR: 该论文提出ICL的"计数假说"，认为LLMs的编码策略可能是ICL的基础机制，并通过实验证据支持这一观点。


<details>
  <summary>Details</summary>
Motivation: ICL使LLMs能够从输入提示中的示例学习特定任务，无需修改模型内部结构，但对其底层机制理解不足，使得错误纠正和诊断困难。需要更好地理解ICL的局限性以及LLMs如何支持ICL。

Method: 基于ICL特性和LLMs功能模块，提出"计数假说"，认为LLMs的编码策略可能是ICL的基础。提供支持证据，可能包括实验分析LLMs在ICL任务中的编码行为。

Result: 提出了ICL的"计数假说"并提供了支持证据，表明LLMs的编码策略可能是ICL工作机制的关键因素。

Conclusion: ICL的"计数假说"为理解LLMs如何通过编码策略支持上下文学习提供了新视角，有助于更好地诊断和纠正ICL中的错误。

Abstract: In-Context Learning (ICL) indicates that large language models (LLMs) pretrained on a massive amount of data can learn specific tasks from input prompts' examples. ICL is notable for two reasons. First, it does not need modification of LLMs' internal structure. Second, it enables LLMs to perform a wide range of tasks/functions with a few examples demonstrating a desirable task. ICL opens up new ways to utilize LLMs in more domains, but its underlying mechanisms still remain poorly understood, making error correction and diagnosis extremely challenging. Thus, it is imperative that we better understand the limitations of ICL and how exactly LLMs support ICL. Inspired by ICL properties and LLMs' functional modules, we propose 1the counting hypothesis' of ICL, which suggests that LLMs' encoding strategy may underlie ICL, and provide supporting evidence.

</details>


### [66] [Restoring Exploration after Post-Training: Latent Exploration Decoding for Large Reasoning Models](https://arxiv.org/abs/2602.01698)
*Wenhui Tan,Fiorenzo Parascandolo,Enver Sangineto,Jianzhong Ju,Zhenbo Luo,Qian Cao,Rita Cucchiara,Ruihua Song,Jian Luan*

Main category: cs.CL

Relevance: 85.0

TL;DR: LED（Latent Exploration Decoding）是一种无需额外训练的解码策略，通过聚合中间层后验分布来缓解推理模型后训练中的探索崩溃问题，提升推理性能。


<details>
  <summary>Details</summary>
Motivation: 现代推理模型（LRMs）通过强化学习后训练后，温度采样不再提升pass@n准确率，出现了"探索崩溃"现象。研究发现后训练导致最终层后验熵急剧减少，而中间层熵保持较高，这种熵不对称性为改进解码提供了机会。

Method: 提出Latent Exploration Decoding（LED）深度条件解码策略：通过累积和聚合中间层后验分布，选择具有最大熵的深度配置作为探索候选，从而在解码过程中引入更多探索性。

Result: 在多个推理基准测试和模型上，LED无需额外训练或参数，一致提升pass@1准确率0.61个百分点，pass@16准确率1.03个百分点。

Conclusion: LED有效缓解了推理模型后训练中的探索崩溃问题，通过利用中间层的高熵特性改进解码策略，为推理模型的性能提升提供了简单有效的方法。

Abstract: Large Reasoning Models (LRMs) have recently achieved strong mathematical and code reasoning performance through Reinforcement Learning (RL) post-training. However, we show that modern reasoning post-training induces an unintended exploration collapse: temperature-based sampling no longer increases pass@$n$ accuracy. Empirically, the final-layer posterior of post-trained LRMs exhibit sharply reduced entropy, while the entropy of intermediate layers remains relatively high. Motivated by this entropy asymmetry, we propose Latent Exploration Decoding (LED), a depth-conditioned decoding strategy. LED aggregates intermediate posteriors via cumulative sum and selects depth configurations with maximal entropy as exploration candidates. Without additional training or parameters, LED consistently improves pass@1 and pass@16 accuracy by 0.61 and 1.03 percentage points across multiple reasoning benchmarks and models. Project page: https://GitHub.com/Xiaomi-Research/LED.

</details>


### [67] [Game of Thought: Robust Information Seeking with Large Language Models Using Game Theory](https://arxiv.org/abs/2602.01708)
*Langyuan Cui,Chun Kai Ling,Hwee Tou Ng*

Main category: cs.CL

Relevance: 85.0

TL;DR: 论文提出Game of Thought (GoT)框架，使用博弈论方法提升LLMs在信息不足场景下的信息寻求能力，通过Twenty Questions游戏评估，在对抗性Strategic Language Search问题中改善最坏情况性能。


<details>
  <summary>Details</summary>
Motivation: LLMs在现实场景中常面临信息不足的问题，现有方法依赖简化假设会降低最坏情况性能，这在高风险应用中存在严重隐患。需要开发能主动寻求缺失信息的能力。

Method: 提出Strategic Language Search (SLS)问题作为双人零和扩展形式博弈，开发Game of Thought (GoT)框架，应用博弈论技术近似纳什均衡策略，解决游戏的受限变体。

Result: 实验结果表明，GoT在所有测试设置中相比(1)直接提示方法和(2)启发式引导搜索方法，都能持续改善最坏情况性能。

Conclusion: 博弈论方法能有效提升LLMs在信息不足场景下的信息寻求能力，特别是在对抗性环境中改善最坏情况性能，对高风险应用具有重要意义。

Abstract: Large Language Models (LLMs) are increasingly deployed in real-world scenarios where they may lack sufficient information to complete a given task. In such settings, the ability to actively seek out missing information becomes a critical capability. Existing approaches to enhancing this ability often rely on simplifying assumptions that degrade \textit{worst-case} performance. This is an issue with serious implications in high-stakes applications. In this work, we use the game of Twenty Questions to evaluate the information-seeking ability of LLMs. We introduce and formalize its adversarial counterpart, the Strategic Language Search (SLS) problem along with its variants as a two-player zero-sum extensive form game. We propose Game of Thought (GoT), a framework that applies game-theoretic techniques to approximate a Nash equilibrium (NE) strategy for the restricted variant of the game. Empirical results demonstrate that our approach consistently improves worst-case performance compared to (1) direct prompting-based methods and (2) heuristic-guided search methods across all tested settings.

</details>


### [68] [ARTIS: Agentic Risk-Aware Test-Time Scaling via Iterative Simulation](https://arxiv.org/abs/2602.01709)
*Xingshan Zeng,Lingzhi Wang,Weiwen Liu,Liangyou Li,Yasheng Wang,Lifeng Shang,Xin Jiang,Qun Liu*

Main category: cs.CL

Relevance: 85.0

TL;DR: ARTIS：通过迭代模拟实现代理风险感知的测试时扩展框架，将探索与执行解耦，在真实环境执行前通过模拟交互进行测试时探索，提高代理可靠性


<details>
  <summary>Details</summary>
Motivation: 当前测试时扩展技术虽然能提升LLM性能，但在代理场景中不足，因为代理动作直接与环境交互且效果可能不可逆、代价高昂。需要一种既能扩展推理时计算又能避免环境风险的方法

Method: 提出ARTIS框架：1) 将探索与执行解耦，通过模拟交互进行测试时探索；2) 引入风险感知工具模拟器，通过针对性数据生成和再平衡训练，强调对失败诱导动作的保真度

Result: 在多轮多步代理基准测试中，迭代模拟显著提高代理可靠性，风险感知模拟对于在不同模型和任务中持续实现这些收益至关重要

Conclusion: ARTIS框架成功将测试时计算扩展到代理设置，通过模拟探索提高动作级可靠性，风险感知模拟器解决了传统LLM模拟器难以捕捉罕见但高影响失败模式的问题

Abstract: Current test-time scaling (TTS) techniques enhance large language model (LLM) performance by allocating additional computation at inference time, yet they remain insufficient for agentic settings, where actions directly interact with external environments and their effects can be irreversible and costly. We propose \emph{\name}, \emph{\underline{A}gentic \underline{R}isk-Aware \underline{T}est-Time Scaling via \underline{I}terative \underline{S}imulation}, a framework that decouples exploration from commitment by enabling test-time exploration through simulated interactions prior to real-world execution. This design allows extending inference-time computation to improve action-level reliability and robustness without incurring environmental risk. We further show that naive LLM-based simulators struggle to capture rare but high-impact failure modes, substantially limiting their effectiveness for agentic decision making. To address this limitation, we introduce a \emph{risk-aware tool simulator} that emphasizes fidelity on failure-inducing actions via targeted data generation and rebalanced training. Experiments on multi-turn and multi-step agentic benchmarks demonstrate that iterative simulation substantially improves agent reliability, and that risk-aware simulation is essential for consistently realizing these gains across models and tasks.

</details>


### [69] [Mechanistic Indicators of Steering Effectiveness in Large Language Models](https://arxiv.org/abs/2602.01716)
*Mehdi Jafari,Hao Xue,Flora Salim*

Main category: cs.CL

Relevance: 85.0

TL;DR: 该研究提出使用信息论指标（NBF和KL散度）诊断LLM激活导向的可靠性，发现这些内部信号能有效预测导向成功概率，并为CAA和稀疏自编码器导向方法建立了更强的评估基线。


<details>
  <summary>Details</summary>
Motivation: 尽管激活导向技术被广泛用于控制LLM行为，但其成功或失败的机制因素仍不清楚，现有研究主要依赖黑盒输出或LLM作为评判者，缺乏对内部信号的分析。

Method: 使用两种信息论指标：基于熵的归一化分支因子（NBF）和词汇空间中导向激活与目标概念之间的KL散度。假设有效导向对应结构化熵保持和跨解码步骤的连贯KL对齐。利用两个架构不同的LLM进行可靠性研究，以LLM生成的标注作为ground truth。

Result: 这些机制信号为识别成功导向和估计失败概率提供了有意义的预测能力。研究进一步为两种最广泛采用的激活导向方法（对比激活添加CAA和稀疏自编码器导向）引入了更强的评估基线。

Conclusion: 内部模型信号可用于诊断激活导向的可靠性，信息论指标能有效预测导向效果，这为理解和改进LLM行为控制提供了新的机制视角。

Abstract: Activation-based steering enables Large Language Models (LLMs) to exhibit targeted behaviors by intervening on intermediate activations without retraining. Despite its widespread use, the mechanistic factors that govern when steering succeeds or fails remain poorly understood, as prior work has relied primarily on black-box outputs or LLM-based judges. In this study, we investigate whether the reliability of steering can be diagnosed using internal model signals. We focus on two information-theoretic measures: the entropy-derived Normalized Branching Factor (NBF), and the Kullback-Leibler (KL) divergence between steered activations and targeted concepts in the vocabulary space. We hypothesize that effective steering corresponds to structured entropy preservation and coherent KL alignment across decoding steps. Building on a reliability study demonstrating high inter-judge agreement between two architecturally distinct LLMs, we use LLM-generated annotations as ground truth and show that these mechanistic signals provide meaningful predictive power for identifying successful steering and estimating failure probability. We further introduce a stronger evaluation baseline for Contrastive Activation Addition (CAA) and Sparse Autoencoder-based steering, the two most widely adopted activation-steering methods.

</details>


### [70] [COMI: Coarse-to-fine Context Compression via Marginal Information Gain](https://arxiv.org/abs/2602.01719)
*Jiwei Tang,Shilei Liu,Zhicheng Zhang,Yujin Yuan,Libin Zheng,Wenbo Su,Bo Zheng*

Main category: cs.CL

Relevance: 85.0

TL;DR: COMI是一个粗到细的自适应上下文压缩框架，通过边际信息增益(MIG)指标联合优化语义相关性和多样性，在长上下文场景中显著提升LLM的计算效率。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型在长上下文场景中存在计算效率低下和信息冗余的问题，现有上下文压缩方法需要在高压缩率下同时保持语义相关性和多样性。

Method: 提出边际信息增益(MIG)指标，衡量单元对查询的相关性减去其与其他单元的语义冗余度。框架分为两阶段：1)粗粒度组重分配，基于组间MIG动态分配压缩率；2)细粒度令牌合并，基于组内MIG加权融合令牌。

Result: 在问答和摘要任务上，COMI显著优于现有基线，如在32倍压缩约束下，Qwen2-7B在NaturalQuestions上实现约25分的精确匹配提升。

Conclusion: COMI通过粗到细的自适应压缩框架有效解决了长上下文场景中的计算效率问题，为LLM的实际部署提供了有效的解决方案。

Abstract: Large Language Models (LLMs) have demonstrated exceptional capabilities across diverse tasks. However, their deployment in long context scenarios remains hindered by computational inefficiency and information redundancy. Context compression methods address these challenges by significantly reducing input length and eliminating redundancy. We propose COMI, a coarse-to-fine adaptive context compression framework that jointly optimizes for semantic relevance and diversity under high compression rates. We introduce Marginal Information Gain (MIG), a metric defined as the relevance of a unit to the input query minus its semantic redundancy with other units, guiding the compression process to prioritize information that is both relevant and low redundant. The framework operates in two stages: (1) Coarse-Grained Group Reallocation, where the context is partitioned into groups and dynamically assigned compression rates based on inter-group MIG, ensuring compression budgets align with information value distribution; and (2) Fine-Grained Token Merging, where tokens within each group are fused via an intra-group MIG-based weighting mechanism, thereby preserving key semantics while avoiding the accumulation of redundancy. Extensive experiments across question-answering (e.g., NaturalQuestions, 2WikiMQA, HotpotQA and NarrativeQA), summarization (e.g., MultiNews) with various backbones (e.g., LLaMA-2-7B, Qwen2-7B) show that COMI outperforms existing baselines by a large margin, e.g., approximately 25-point Exact Match (EM) improvement under 32x compression constraint with Qwen2-7B on NaturalQuestions.

</details>


### [71] [Zero2Text: Zero-Training Cross-Domain Inversion Attacks on Textual Embeddings](https://arxiv.org/abs/2602.01757)
*Doohyun Kim,Donghwa Kang,Kyungjae Lee,Hyeongboo Baek,Brent Byunghoon Kang*

Main category: cs.CL

Relevance: 85.0

TL;DR: Zero2Text是一种无需训练的框架，通过递归在线对齐从向量嵌入中恢复文本，解决了黑盒和跨域设置下的隐私风险问题。


<details>
  <summary>Details</summary>
Motivation: 检索增强生成(RAG)中向量数据库存在严重的隐私风险，现有方法面临计算成本高或需要领域内训练数据的限制，无法在严格的黑盒和跨域设置中有效工作。

Method: 基于递归在线对齐的训练免费框架，结合LLM先验和动态岭回归机制，在运行时迭代地将生成与目标嵌入对齐，无需静态数据集。

Result: 在MS MARCO基准测试中，针对OpenAI受害者模型，Zero2Text比基线方法实现了1.8倍的ROUGE-L和6.4倍的BLEU-2分数提升，能够从未知领域恢复句子而无需任何泄露的数据对。

Conclusion: Zero2Text突破了现有嵌入反转攻击方法的限制，展示了标准防御（如差分隐私）无法有效缓解这种自适应威胁，强调了向量数据库隐私保护的新挑战。

Abstract: The proliferation of retrieval-augmented generation (RAG) has established vector databases as critical infrastructure, yet they introduce severe privacy risks via embedding inversion attacks. Existing paradigms face a fundamental trade-off: optimization-based methods require computationally prohibitive queries, while alignment-based approaches hinge on the unrealistic assumption of accessible in-domain training data. These constraints render them ineffective in strict black-box and cross-domain settings. To dismantle these barriers, we introduce Zero2Text, a novel training-free framework based on recursive online alignment. Unlike methods relying on static datasets, Zero2Text synergizes LLM priors with a dynamic ridge regression mechanism to iteratively align generation to the target embedding on-the-fly. We further demonstrate that standard defenses, such as differential privacy, fail to effectively mitigate this adaptive threat. Extensive experiments across diverse benchmarks validate Zero2Text; notably, on MS MARCO against the OpenAI victim model, it achieves 1.8x higher ROUGE-L and 6.4x higher BLEU-2 scores compared to baselines, recovering sentences from unknown domains without a single leaked data pair.

</details>


### [72] [<SOG_k>: One LLM Token for Explicit Graph Structural Understanding](https://arxiv.org/abs/2602.01771)
*Jingyao Wu,Bin Lu,Zijun Di,Xiaoying Gan,Meng Jin,Luoyi Fu,Xinbing Wang,Chenghu Zhou*

Main category: cs.CL

Relevance: 85.0

TL;DR: 提出SOG方法，使用特殊token <SOG_k>在统一token空间中表示图结构，解决LLMs处理图数据时的结构幻觉问题


<details>
  <summary>Details</summary>
Motivation: 大型语言模型在非结构化数据理解方面表现出色，但在处理图数据时面临结构幻觉挑战。现有方法要么将图转化为自然语言（导致token消耗过大、注意力分散），要么转化为可训练的连续嵌入（与原始文本token严重不对齐）

Method: 提出拓扑感知的结构tokenizer，将每个图拓扑映射为高度选择性的单个token <SOG_k>，构建混合结构问答语料库来对齐新的结构token与现有文本token

Result: 在五个图级基准测试中表现出优越性，相比基线方法性能提升9.9%到41.4%，同时展现出可解释性和一致性。方法还可灵活扩展到节点级任务

Conclusion: SOG方法使LLMs能够以简洁准确的方式理解、生成和推理图结构，为LLMs处理结构化图数据提供了有效解决方案

Abstract: Large language models show great potential in unstructured data understanding, but still face significant challenges with graphs due to their structural hallucination. Existing approaches mainly either verbalize graphs into natural language, which leads to excessive token consumption and scattered attention, or transform graphs into trainable continuous embeddings (i.e., soft prompt), but exhibit severe misalignment with original text tokens. To solve this problem, we propose to incorporate one special token <SOG_k> to fully represent the Structure Of Graph within a unified token space, facilitating explicit topology input and structural information sharing. Specifically, we propose a topology-aware structural tokenizer that maps each graph topology into a highly selective single token. Afterwards, we construct a set of hybrid structure Question-Answering corpora to align new structural tokens with existing text tokens. With this approach, <SOG_k> empowers LLMs to understand, generate, and reason in a concise and accurate manner. Extensive experiments on five graph-level benchmarks demonstrate the superiority of our method, achieving a performance improvement of 9.9% to 41.4% compared to the baselines while exhibiting interpretability and consistency. Furthermore, our method provides a flexible extension to node-level tasks, enabling both global and local structural understanding. The codebase is publicly available at https://github.com/Jingyao-Wu/SOG.

</details>


### [73] [Data Distribution Matters: A Data-Centric Perspective on Context Compression for Large Language Model](https://arxiv.org/abs/2602.01778)
*Kangtao Lv,Jiwei Tang,Langming Liu,Haibin Chen,Weidong Zhang,Shilei Liu,Yongwei Wang,Yujin Yuan,Wenbo Su,Bo Zheng*

Main category: cs.CL

Relevance: 85.0

TL;DR: 该论文首次从数据中心视角研究数据分布对LLM上下文压缩质量的影响，发现输入熵与压缩质量负相关，编码器-解码器内在数据差距显著降低压缩增益


<details>
  <summary>Details</summary>
Motivation: LLM在长上下文场景中面临计算效率低下和信息冗余问题，现有研究仅关注模型侧改进，而数据分布本身对上下文压缩的影响尚未被探索。论文旨在填补这一空白，从数据中心视角系统研究数据分布如何影响压缩质量。

Method: 采用基于自编码器的框架评估压缩表示的语义完整性，从两个维度分析数据分布影响：输入数据（外部数据）和内在数据（模型内部预训练知识）。通过实验测量输入熵与压缩质量的关系，以及编码器-解码器内在数据差距对压缩增益的影响。

Result: 实验发现：(1) 编码器测量的输入熵与压缩质量负相关，而解码器测量的熵在冻结解码器设置下无显著关系；(2) 编码器和解码器内在数据之间的差距显著降低压缩增益，且难以缓解。基于这些发现提出了优化压缩增益的实用指南。

Conclusion: 数据分布对LLM上下文压缩质量有重要影响，特别是输入熵和编码器-解码器内在数据差距是关键因素。研究为优化长上下文LLM部署提供了数据中心的视角和实用指导。

Abstract: The deployment of Large Language Models (LLMs) in long-context scenarios is hindered by computational inefficiency and significant information redundancy. Although recent advancements have widely adopted context compression to address these challenges, existing research only focus on model-side improvements, the impact of the data distribution itself on context compression remains largely unexplored. To bridge this gap, we are the first to adopt a data-centric perspective to systematically investigate how data distribution impacts compression quality, including two dimensions: input data and intrinsic data (i.e., the model's internal pretrained knowledge). We evaluate the semantic integrity of compressed representations using an autoencoder-based framework to systematically investigate it. Our experimental results reveal that: (1) encoder-measured input entropy negatively correlates with compression quality, while decoder-measured entropy shows no significant relationship under a frozen-decoder setting; and (2) the gap between intrinsic data of the encoder and decoder significantly diminishes compression gains, which is hard to mitigate. Based on these findings, we further present practical guidelines to optimize compression gains.

</details>


### [74] [CodeOCR: On the Effectiveness of Vision Language Models in Code Understanding](https://arxiv.org/abs/2602.01785)
*Yuling Shi,Chaoxiang Xie,Zhensu Sun,Yeheng Chen,Chenxu Zhang,Longfei Yun,Chengcheng Wan,Hongyu Zhang,David Lo,Xiaodong Gu*

Main category: cs.CL

Relevance: 85.0

TL;DR: 该论文首次系统研究多模态大语言模型（MLLMs）在代码理解任务中的有效性，探索将源代码渲染为图像进行压缩表示的可行性，实现高达8倍的token压缩，同时保持代码理解能力。


<details>
  <summary>Details</summary>
Motivation: 随着软件系统规模扩大，传统文本范式下LLMs处理源代码时上下文长度线性增长导致计算成本急剧增加。多模态LLMs的发展为通过图像模态压缩代码表示提供了机会，因为图像比文本更适合压缩而不损失语义信息。

Method: 将源代码渲染为图像表示，通过调整分辨率实现不同程度的视觉压缩（最高8倍）。系统评估MLLMs在代码理解任务中的表现，包括代码补全、克隆检测等，并与传统文本范式对比。

Result: 1) MLLMs能有效理解压缩后的代码图像，实现高达8倍token压缩；2) MLLMs能有效利用语法高亮等视觉线索，在4倍压缩下仍能提升代码补全性能；3) 克隆检测等任务对视觉压缩表现出异常韧性，某些压缩比甚至略优于原始文本输入。

Conclusion: 研究揭示了MLLMs在代码理解中的潜力和当前局限，指出图像模态代码表示是通向更高效推理的可行路径，为大规模软件系统的高效处理提供了新思路。

Abstract: Large Language Models (LLMs) have achieved remarkable success in source code understanding, yet as software systems grow in scale, computational efficiency has become a critical bottleneck. Currently, these models rely on a text-based paradigm that treats source code as a linear sequence of tokens, which leads to a linear increase in context length and associated computational costs. The rapid advancement of Multimodal LLMs (MLLMs) introduces an opportunity to optimize efficiency by representing source code as rendered images. Unlike text, which is difficult to compress without losing semantic meaning, the image modality is inherently suitable for compression. By adjusting resolution, images can be scaled to a fraction of their original token cost while remaining recognizable to vision-capable models. To explore the feasibility of this approach, we conduct the first systematic study on the effectiveness of MLLMs for code understanding. Our experiments reveal that: (1) MLLMs can effectively understand code with substantial token reduction, achieving up to 8x compression; (2) MLLMs can effectively leverage visual cues such as syntax highlighting, improving code completion performance under 4x compression; and (3) Code-understanding tasks like clone detection exhibit exceptional resilience to visual compression, with some compression ratios even slightly outperforming raw text inputs. Our findings highlight both the potential and current limitations of MLLMs in code understanding, which points out a shift toward image-modality code representation as a pathway to more efficient inference.

</details>


### [75] [Sentence Curve Language Models](https://arxiv.org/abs/2602.01807)
*DongNyeong Heo,Heelyoul Choi*

Main category: cs.CL

Relevance: 85.0

TL;DR: 提出句子曲线语言模型(SCLM)，通过预测连续的句子曲线而非静态词嵌入，解决传统语言模型中目标词嵌入对相邻词不敏感的问题，促进全局结构建模。


<details>
  <summary>Details</summary>
Motivation: 传统语言模型（包括扩散语言模型）使用静态词嵌入表示目标句子，这种表示对相邻词不敏感，导致模型只关注局部准确的词预测而忽视句子的全局结构。

Method: 提出句子曲线表示法，定义为样条曲线，其控制点影响句子中的多个词。基于此表示，引入句子曲线语言模型(SCLM)，扩展扩散语言模型以预测句子曲线而非静态词嵌入。

Result: SCLM在IWSLT14和WMT14数据集上达到扩散语言模型中的SOTA性能，训练稳定无需繁琐的知识蒸馏，在LM1B上与离散扩散语言模型相比显示出有前景的潜力。

Conclusion: 句子曲线表示通过正则化效应促进全局结构建模，SCLM为语言建模提供了新的连续表示方法，在多个基准测试中表现出优越性能。

Abstract: Language models (LMs) are a central component of modern AI systems, and diffusion-based language models (DLMs) have recently emerged as a competitive alternative. Both paradigms rely on word embeddings not only to represent the input sentence, but also to represent the target sentence that backbone models are trained to predict. We argue that such static embedding of the target word is insensitive to neighboring words, encouraging locally accurate word prediction while neglecting global structure across the target sentence. To address this limitation, we propose a continuous sentence representation, termed sentence curve, defined as a spline curve whose control points affect multiple words in the sentence. Based on this representation, we introduce sentence curve language model (SCLM), which extends DLMs to predict sentence curves instead of the static word embeddings. We theoretically show that sentence curve prediction induces a regularization effect that promotes global structure modeling, and characterize how different sentence curve types affect this behavior. Empirically, SCLM achieves SOTA performance among DLMs on IWSLT14 and WMT14, shows stable training without burdensome knowledge distillation, and demonstrates promising potential compared to discrete DLMs on LM1B.

</details>


### [76] [Read As Human: Compressing Context via Parallelizable Close Reading and Skimming](https://arxiv.org/abs/2602.01840)
*Jiwei Tang,Shilei Liu,Zhicheng Zhang,Qingsong Lv,Runsong Zhao,Tingwei Lu,Langming Liu,Haibin Chen,Yujin Yuan,Hai-Tao Zheng,Wenbo Su,Bo Zheng*

Main category: cs.CL

Relevance: 85.0

TL;DR: RAM框架通过模拟人类阅读行为（精读重要内容、略读次要内容），对长上下文进行自适应压缩，在保持性能的同时实现12倍加速


<details>
  <summary>Details</summary>
Motivation: 大语言模型在长上下文场景下面临计算效率低下和信息冗余两大挑战，需要一种既能保持性能又能提高效率的上下文压缩方法

Method: 提出RAM框架：1）将上下文分段并行编码；2）高相关段完整保留（精读），低相关段压缩为摘要向量（略读）；3）引入对比学习优化精读/略读决策边界；4）结合显式文本段和隐式摘要向量输入解码器

Result: 在多个问答和摘要基准测试中优于现有基线，在平均长度16K、最大32K的长输入上实现最高12倍的端到端加速

Conclusion: RAM框架有效解决了长上下文处理的计算效率和冗余信息问题，在保持性能的同时显著提升效率，并具有自然语言格式的可解释性

Abstract: Large Language Models (LLMs) demonstrate exceptional capability across diverse tasks. However, their deployment in long-context scenarios is hindered by two challenges: computational inefficiency and redundant information. We propose RAM (Read As HuMan), a context compression framework that adopts an adaptive hybrid reading strategy, to address these challenges. Inspired by human reading behavior (i.e., close reading important content while skimming less relevant content), RAM partitions the context into segments and encodes them with the input query in parallel. High-relevance segments are fully retained (close reading), while low-relevance ones are query-guided compressed into compact summary vectors (skimming). Both explicit textual segments and implicit summary vectors are concatenated and fed into decoder to achieve both superior performance and natural language format interpretability. To refine the decision boundary between close reading and skimming, we further introduce a contrastive learning objective based on positive and negative query-segment pairs. Experiments demonstrate that RAM outperforms existing baselines on multiple question answering and summarization benchmarks across two backbones, while delivering up to a 12x end-to-end speedup on long inputs (average length 16K; maximum length 32K).

</details>


### [77] [PretrainRL: Alleviating Factuality Hallucination of Large Language Models at the Beginning](https://arxiv.org/abs/2602.01875)
*Langming Liu,Kangtao Lv,Haibin Chen,Weidong Zhang,Yejing Wang,Shilei Liu,Xin Tong,Yujin Yuan,Yongwei Wang,Wenbo Su,Bo Zheng*

Main category: cs.CL

Relevance: 85.0

TL;DR: 提出PretrainRL框架，在预训练阶段集成强化学习来巩固事实知识，通过"去偏后学习"原则降低高概率错误信息的权重，为低概率真实信息创造学习空间，显著缓解事实幻觉问题。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型存在事实幻觉问题，生成可验证的错误信息。作者发现问题的根源在于预训练语料库中不平衡的数据分布，导致"低概率真理"和"高概率错误"的状态。现有方法要么回避问题，要么面临灾难性遗忘。

Method: 提出PretrainRL框架，核心原则是"去偏后学习"。在预训练阶段集成强化学习，主动重塑模型的概率分布：1) 设计高效的负采样策略发现高概率错误信息；2) 降低这些高概率错误的权重；3) 为低概率真实信息创造学习空间；4) 引入新指标评估模型关于事实知识的概率状态。

Result: 在三个公开基准测试上的广泛实验表明，PretrainRL显著缓解了事实幻觉问题，并优于最先进的方法。

Conclusion: 从根源上解决事实幻觉问题需要在预训练阶段主动重塑模型的概率分布。PretrainRL通过集成强化学习和"去偏后学习"原则，有效巩固事实知识，为解决LLM的事实幻觉问题提供了新思路。

Abstract: Large language models (LLMs), despite their powerful capabilities, suffer from factual hallucinations where they generate verifiable falsehoods. We identify a root of this issue: the imbalanced data distribution in the pretraining corpus, which leads to a state of "low-probability truth" and "high-probability falsehood". Recent approaches, such as teaching models to say "I don't know" or post-hoc knowledge editing, either evade the problem or face catastrophic forgetting. To address this issue from its root, we propose \textbf{PretrainRL}, a novel framework that integrates reinforcement learning into the pretraining phase to consolidate factual knowledge. The core principle of PretrainRL is "\textbf{debiasing then learning}." It actively reshapes the model's probability distribution by down-weighting high-probability falsehoods, thereby making "room" for low-probability truths to be learned effectively. To enable this, we design an efficient negative sampling strategy to discover these high-probability falsehoods and introduce novel metrics to evaluate the model's probabilistic state concerning factual knowledge. Extensive experiments on three public benchmarks demonstrate that PretrainRL significantly alleviates factual hallucinations and outperforms state-of-the-art methods.

</details>


### [78] [ES-MemEval: Benchmarking Conversational Agents on Personalized Long-Term Emotional Support](https://arxiv.org/abs/2602.01885)
*Tiantian Chen,Jiaqi Lu,Ying Shen,Lin Zhang*

Main category: cs.CL

Relevance: 85.0

TL;DR: ES-MemEval是一个评估LLM在长期情感支持对话中记忆能力的基准，包含信息提取、时序推理、冲突检测、弃权和用户建模五个核心能力，实验表明显式长期记忆对减少幻觉和实现个性化至关重要。


<details>
  <summary>Details</summary>
Motivation: 现有LLM在长期记忆方面存在不足，特别是在复杂、长期的在线情感支持服务中。现有基准主要关注静态显式事实检索，无法评估在用户信息分散、隐含且持续演化的关键场景下的表现。

Method: 提出ES-MemEval基准，系统评估五个核心记忆能力；构建EvoEmo多会话数据集，捕捉碎片化、隐含的用户披露和演化状态；在开源长上下文、商业和RAG增强LLM上进行广泛实验。

Result: 显式长期记忆对减少幻觉和实现有效个性化至关重要；RAG提高了事实一致性，但在处理时序动态和演化用户状态方面存在困难；当前范式在长期个性化对话系统中既有潜力也有局限。

Conclusion: 需要更鲁棒地整合记忆和检索机制来构建长期个性化对话系统，当前方法在时序推理和演化状态处理方面仍需改进。

Abstract: Large Language Models (LLMs) have shown strong potential as conversational agents. Yet, their effectiveness remains limited by deficiencies in robust long-term memory, particularly in complex, long-term web-based services such as online emotional support. However, existing long-term dialogue benchmarks primarily focus on static and explicit fact retrieval, failing to evaluate agents in critical scenarios where user information is dispersed, implicit, and continuously evolving. To address this gap, we introduce ES-MemEval, a comprehensive benchmark that systematically evaluates five core memory capabilities: information extraction, temporal reasoning, conflict detection, abstention, and user modeling, in long-term emotional support settings, covering question answering, summarization, and dialogue generation tasks. To support the benchmark, we also propose EvoEmo, a multi-session dataset for personalized long-term emotional support that captures fragmented, implicit user disclosures and evolving user states. Extensive experiments on open-source long-context, commercial, and retrieval-augmented (RAG) LLMs show that explicit long-term memory is essential for reducing hallucinations and enabling effective personalization. At the same time, RAG improves factual consistency but struggles with temporal dynamics and evolving user states. These findings highlight both the potential and limitations of current paradigms and motivate more robust integration of memory and retrieval for long-term personalized dialogue systems.

</details>


### [79] [Breaking the Static Graph: Context-Aware Traversal for Robust Retrieval-Augmented Generation](https://arxiv.org/abs/2602.01965)
*Kwun Hang Lau,Fangyuan Zhang,Boyu Ruan,Yingli Zhou,Qintian Guo,Ruiyuan Zhang,Xiaofang Zhou*

Main category: cs.CL

Relevance: 85.0

TL;DR: CatRAG提出了一种上下文感知的RAG框架，通过动态调整知识图谱的遍历来解决静态图谱的局限性，显著提升多跳推理的完整性。


<details>
  <summary>Details</summary>
Motivation: 现有基于知识图谱的RAG方法（如HippoRAG）存在"静态图谱谬误"：依赖固定的转移概率，忽略了查询相关的边相关性，导致随机游走被高连接度节点分散，无法完整检索多跳查询所需的证据链。

Method: 基于HippoRAG 2架构，CatRAG将静态知识图谱转换为查询自适应的导航结构，包含三个核心组件：1) 符号锚定：注入弱实体约束来正则化随机游走；2) 查询感知动态边权重：动态调整图结构，剪枝无关路径并增强与查询意图对齐的路径；3) 关键事实段落权重增强：通过成本高效的偏置将随机游走锚定到可能的证据。

Result: 在四个多跳基准测试中，CatRAG持续优于现有最佳基线。虽然标准召回率指标显示适度提升，但在推理完整性（完整恢复证据路径的能力）方面实现了显著改进。

Conclusion: CatRAG有效弥合了检索部分上下文与实现完全基于证据的推理之间的差距，为多跳RAG提供了更鲁棒的解决方案。

Abstract: Recent advances in Retrieval-Augmented Generation (RAG) have shifted from simple vector similarity to structure-aware approaches like HippoRAG, which leverage Knowledge Graphs (KGs) and Personalized PageRank (PPR) to capture multi-hop dependencies. However, these methods suffer from a "Static Graph Fallacy": they rely on fixed transition probabilities determined during indexing. This rigidity ignores the query-dependent nature of edge relevance, causing semantic drift where random walks are diverted into high-degree "hub" nodes before reaching critical downstream evidence. Consequently, models often achieve high partial recall but fail to retrieve the complete evidence chain required for multi-hop queries. To address this, we propose CatRAG, Context-Aware Traversal for robust RAG, a framework that builds on the HippoRAG 2 architecture and transforms the static KG into a query-adaptive navigation structure. We introduce a multi-faceted framework to steer the random walk: (1) Symbolic Anchoring, which injects weak entity constraints to regularize the random walk; (2) Query-Aware Dynamic Edge Weighting, which dynamically modulates graph structure, to prune irrelevant paths while amplifying those aligned with the query's intent; and (3) Key-Fact Passage Weight Enhancement, a cost-efficient bias that structurally anchors the random walk to likely evidence. Experiments across four multi-hop benchmarks demonstrate that CatRAG consistently outperforms state of the art baselines. Our analysis reveals that while standard Recall metrics show modest gains, CatRAG achieves substantial improvements in reasoning completeness, the capacity to recover the entire evidence path without gaps. These results reveal that our approach effectively bridges the gap between retrieving partial context and enabling fully grounded reasoning. Resources are available at https://github.com/kwunhang/CatRAG.

</details>


### [80] [Beyond Local Edits: Embedding-Virtualized Knowledge for Broader Evaluation and Preservation of Model Editing](https://arxiv.org/abs/2602.01977)
*Shuainan Liu,Xuanang Chen,Ben He,Le Sun*

Main category: cs.CL

Relevance: 85.0

TL;DR: 该论文提出了Embedding-Virtualized Knowledge (EVK)方法，通过嵌入空间中的受控扰动来表征模型知识，构建了EVK-Bench基准来量化知识编辑引起的潜在知识漂移，并提出了EVK-Align模块来约束编辑过程中的知识漂移。


<details>
  <summary>Details</summary>
Motivation: 现有的大语言模型知识编辑方法通常使用预定义基准进行评估，这些基准主要评估编辑的事实和有限的相邻知识。这种评估局限于有限的数据集样本，无法充分理解编辑对模型知识系统的更广泛影响。作者旨在解决这一局限性，探索编辑对模型知识系统的更全面影响。

Method: 1. 提出Embedding-Virtualized Knowledge (EVK)：通过在嵌入空间中引入受控扰动来表征模型知识，从而探索超出显式数据标注的更广泛虚拟化知识区域。
2. 构建EVK-Bench基准：基于EVK构建嵌入级别的评估基准，量化编辑引起的潜在知识漂移。
3. 提出EVK-Align模块：一个即插即用的模块，在编辑过程中约束嵌入级别的知识漂移，可与现有编辑方法无缝集成。

Result: 实验表明，该方法能够实现更全面的评估，同时显著提高知识保留能力，而不牺牲编辑准确性。EVK-Bench揭示了传统基于样本的指标未能捕捉到的编辑效果。

Conclusion: EVK方法通过嵌入空间的虚拟化知识表征，为知识编辑提供了更全面的评估框架，并通过EVK-Align模块有效约束了知识漂移，在保持编辑准确性的同时提高了知识保留能力。

Abstract: Knowledge editing methods for large language models are commonly evaluated using predefined benchmarks that assess edited facts together with a limited set of related or neighboring knowledge. While effective, such evaluations remain confined to finite, dataset-bounded samples, leaving the broader impact of editing on the model's knowledge system insufficiently understood. To address this gap, we introduce Embedding-Virtualized Knowledge (EVK) that characterizes model knowledge through controlled perturbations in embedding space, enabling the exploration of a substantially broader and virtualized knowledge region beyond explicit data annotations. Based on EVK, we construct an embedding-level evaluation benchmark EVK-Bench that quantifies potential knowledge drift induced by editing, revealing effects that are not captured by conventional sample-based metrics. Furthermore, we propose a plug-and-play EVK-Align module that constrains embedding-level knowledge drift during editing and can be seamlessly integrated into existing editing methods. Experiments demonstrate that our approach enables more comprehensive evaluation while significantly improving knowledge preservation without sacrificing editing accuracy.

</details>


### [81] [S3-CoT: Self-Sampled Succinct Reasoning Enables Efficient Chain-of-Thought LLMs](https://arxiv.org/abs/2602.01982)
*Yanrui Du,Sendong Zhao,Yibo Gao,Danyang Zhao,Qika Lin,Ming Ma,Jiayun Li,Yi Jiang,Kai He,Qianyi Xu,Bing Qin,Mengling Feng*

Main category: cs.CL

Relevance: 85.0

TL;DR: 提出S3-CoT框架，通过激活引导自采样实现高效CoT学习，无需教师监督，建立类人双认知系统，在数学和医学领域取得稳定改进


<details>
  <summary>Details</summary>
Motivation: 现有CoT方法存在冗余推理过程，需要探索LLM能否获得类似人类系统1的快速思维模式，同时解决SFT方法中高质量监督数据稀缺的瓶颈问题

Method: 基于激活引导的自采样框架，从目标LLM自身诱导风格对齐、可变长度的推理轨迹；使用黄金答案过滤数据，通过(i)类人双认知系统和(ii)渐进压缩课程进行SFT；探索仅使用预测一致数据的自进化机制

Result: 在数学基准测试和医学跨域泛化测试中，该方法对通用LLM和R1风格LLM都带来了稳定改进

Conclusion: S3-CoT框架成功实现了高效CoT学习，无需教师指导或黄金答案，为LLM推理能力优化提供了新途径

Abstract: Large language models (LLMs) equipped with chain-of-thought (CoT) achieve strong performance and offer a window into LLM behavior. However, recent evidence suggests that improvements in CoT capabilities often come with redundant reasoning processes, motivating a key question: Can LLMs acquire a fast-thinking mode analogous to human System 1 reasoning? To explore this, our study presents a self-sampling framework based on activation steering for efficient CoT learning. Our method can induce style-aligned and variable-length reasoning traces from target LLMs themselves without any teacher guidance, thereby alleviating a central bottleneck of SFT-based methods-the scarcity of high-quality supervision data. Using filtered data by gold answers, we perform SFT for efficient CoT learning with (i) a human-like dual-cognitive system, and (ii) a progressive compression curriculum. Furthermore, we explore a self-evolution regime in which SFT is driven solely by prediction-consistent data of variable-length variants, eliminating the need for gold answers. Extensive experiments on math benchmarks, together with cross-domain generalization tests in medicine, show that our method yields stable improvements for both general and R1-style LLMs. Our data and model checkpoints can be found at https://github.com/DYR1/S3-CoT.

</details>


### [82] [From Latent Signals to Reflection Behavior: Tracing Meta-Cognitive Activation Trajectory in R1-Style LLMs](https://arxiv.org/abs/2602.01999)
*Yanrui Du,Yibo Gao,Sendong Zhao,Jiayun Li,Haochun Wang,Qika Lin,Kai He,Bing Qin,Mengling Feng*

Main category: cs.CL

Relevance: 85.0

TL;DR: 该研究通过分析R1风格LLMs的反射行为机制，揭示了从潜在控制层到语义枢纽层再到行为外显层的三层结构，展示了类似人类元认知的自我反思过程。


<details>
  <summary>Details</summary>
Motivation: R1风格的大语言模型因其自我反思能力受到关注，但其内部机制尚不清楚。研究旨在揭示反射行为的层间激活轨迹和因果机制。

Method: 使用logit lens技术读取token级语义，追踪反射行为的层间激活轨迹。通过针对性干预实验，分析不同层之间的因果链关系。

Result: 发现了三层结构：1) 潜在控制层：线性方向编码思考预算语义；2) 语义枢纽层：话语级线索（转折点和总结线索）主导概率分布；3) 行为外显层：反射行为token的采样可能性上升。干预实验揭示了跨阶段的因果链。

Conclusion: 研究揭示了类似人类元认知的过程：从潜在监控到话语级调节，再到外显的自我反思。这为理解LLMs的反思机制提供了新视角。

Abstract: R1-style LLMs have attracted growing attention for their capacity for self-reflection, yet the internal mechanisms underlying such behavior remain unclear. To bridge this gap, we anchor on the onset of reflection behavior and trace its layer-wise activation trajectory. Using the logit lens to read out token-level semantics, we uncover a structured progression: (i) Latent-control layers, where an approximate linear direction encodes the semantics of thinking budget; (ii) Semantic-pivot layers, where discourse-level cues, including turning-point and summarization cues, surface and dominate the probability mass; and (iii) Behavior-overt layers, where the likelihood of reflection-behavior tokens begins to rise until they become highly likely to be sampled. Moreover, our targeted interventions uncover a causal chain across these stages: prompt-level semantics modulate the projection of activations along latent-control directions, thereby inducing competition between turning-point and summarization cues in semantic-pivot layers, which in turn regulates the sampling likelihood of reflection-behavior tokens in behavior-overt layers. Collectively, our findings suggest a human-like meta-cognitive process-progressing from latent monitoring, to discourse-level regulation, and to finally overt self-reflection. Our analysis code can be found at https://github.com/DYR1/S3-CoT.

</details>


### [83] [Beyond RAG for Agent Memory: Retrieval by Decoupling and Aggregation](https://arxiv.org/abs/2602.02007)
*Zhanghao Hu,Qinglin Zhu,Hanqi Yan,Yulan He,Lin Gui*

Main category: cs.CL

Relevance: 85.0

TL;DR: xMemory提出了一种针对智能体记忆系统的层次化检索方法，通过解耦-聚合范式将记忆分解为语义组件并组织成层次结构，解决了传统RAG在对话流中检索冗余和丢失时序依赖的问题。


<details>
  <summary>Details</summary>
Motivation: 传统RAG方法假设处理大型异构语料库，而智能体记忆系统是有限、连贯的对话流，其中记忆片段高度相关且经常重复。固定top-k相似性检索会返回冗余上下文，而事后剪枝可能删除时序链接的前提条件，影响推理正确性。

Method: xMemory采用解耦-聚合范式：1) 将记忆分解为语义组件；2) 组织成层次结构；3) 通过稀疏性-语义目标指导记忆的分裂与合并，保持可搜索且忠实的高层节点组织。推理时采用自上而下的检索策略，先选择紧凑多样的主题和语义，仅在减少读者不确定性时扩展到片段和原始消息。

Result: 在LoCoMo和PerLTQA数据集上，使用三种最新LLM进行的实验显示，xMemory在回答质量和token效率方面均取得一致提升。

Conclusion: 针对智能体记忆系统，检索应超越简单的相似性匹配，转向基于潜在组件的操作。xMemory通过层次化记忆组织和智能检索策略，有效解决了对话流中的冗余和时序依赖问题。

Abstract: Agent memory systems often adopt the standard Retrieval-Augmented Generation (RAG) pipeline, yet its underlying assumptions differ in this setting. RAG targets large, heterogeneous corpora where retrieved passages are diverse, whereas agent memory is a bounded, coherent dialogue stream with highly correlated spans that are often duplicates. Under this shift, fixed top-$k$ similarity retrieval tends to return redundant context, and post-hoc pruning can delete temporally linked prerequisites needed for correct reasoning. We argue retrieval should move beyond similarity matching and instead operate over latent components, following decoupling to aggregation: disentangle memories into semantic components, organise them into a hierarchy, and use this structure to drive retrieval. We propose xMemory, which builds a hierarchy of intact units and maintains a searchable yet faithful high-level node organisation via a sparsity--semantics objective that guides memory split and merge. At inference, xMemory retrieves top-down, selecting a compact, diverse set of themes and semantics for multi-fact queries, and expanding to episodes and raw messages only when it reduces the reader's uncertainty. Experiments on LoCoMo and PerLTQA across the three latest LLMs show consistent gains in answer quality and token efficiency.

</details>


### [84] [NEAT: Neuron-Based Early Exit for Large Reasoning Models](https://arxiv.org/abs/2602.02010)
*Kang Liu,Yongkang Liu,Xiaocui Yang,Peidong Wang,Wen Zhang,Shi Feng,Yifei Zhang,Daling Wang*

Main category: cs.CL

Relevance: 85.0

TL;DR: NEAT提出了一种基于神经元激活动态的早期推理退出框架，通过监控神经元级激活模式实现无需训练的早期退出，减少推理过程中的冗余计算，平均减少22%-28%的token使用量。


<details>
  <summary>Details</summary>
Motivation: 大型推理模型存在"过度思考"问题，即在已经得到正确解后仍生成冗余推理步骤。现有早期退出方法依赖输出级启发式或训练探测模型，需要额外计算或标注数据。

Method: NEAT通过识别退出相关神经元并跟踪其在推理过程中的激活模式，动态触发早期退出或抑制反思，无需训练且不引入额外测试时计算。

Result: 在四个推理基准测试和六个不同规模和架构的模型上，NEAT平均减少22%-28%的token使用，同时保持准确性。

Conclusion: NEAT提供了一种高效、无需训练的早期推理退出方法，有效缓解过度思考问题，减少计算开销。

Abstract: Large Reasoning Models (LRMs) often suffer from \emph{overthinking}, a phenomenon in which redundant reasoning steps are generated after a correct solution has already been reached. Existing early reasoning exit methods primarily rely on output-level heuristics or trained probing models to skip redundant reasoning steps, thereby mitigating overthinking. However, these approaches typically require additional rollout computation or externally labeled datasets. In this paper, we propose \textbf{NEAT}, a \textbf{N}euron-based \textbf{E}arly re\textbf{A}soning exi\textbf{T} framework that monitors neuron-level activation dynamics to enable training-free early exits, without introducing additional test-time computation. NEAT identifies exit-associated neurons and tracks their activation patterns during reasoning to dynamically trigger early exit or suppress reflection, thereby reducing unnecessary reasoning while preserving solution quality. Experiments on four reasoning benchmarks across six models with different scales and architectures show that, for each model, NEAT achieves an average token reduction of 22\% to 28\% when averaged over the four benchmarks, while maintaining accuracy.

</details>


### [85] [Think Dense, Not Long: Dynamic Decoupled Conditional Advantage for Efficient Reasoning](https://arxiv.org/abs/2602.02099)
*Keqin Peng,Yuanxin Ouyang,Xuebo Liu,Zhiliang Tian,Ruijian Han,Yancheng Yuan,Liang Ding*

Main category: cs.CL

Relevance: 85.0

TL;DR: 提出DDCA方法解决RLVR中长度惩罚导致准确率下降的问题，通过解耦效率和正确性优化，动态调整惩罚强度，在多个数学推理基准上显著减少生成token数同时保持或提升准确率。


<details>
  <summary>Details</summary>
Motivation: RLVR（带可验证奖励的强化学习）能激发多步推理，但常导致冗长轨迹。简单的长度惩罚在群体相对优化中会严重损害准确率，主要由于两个结构性问题：1）长度基线稀释（错误回答降低群体基线，过度惩罚正确解）；2）难度-惩罚不匹配（静态惩罚无法适应问题难度）。

Method: 提出动态解耦条件优势（DDCA）：1）在正确回答簇内条件计算长度优势，消除基线稀释；2）使用群体通过率作为难度代理，动态缩放惩罚强度。该方法将效率优化与正确性解耦。

Result: 在GSM8K、MATH500、AMC23、AIME25等基准上，DDCA相比自适应基线持续改进效率-准确率权衡。简单任务（如GSM8K）减少约60%生成token，较难基准（如AIME25）减少超过20%，同时保持或提升准确率。

Conclusion: DDCA有效解决了RLVR中的长度惩罚问题，通过条件计算和动态调整实现了更好的效率-准确率平衡，为强化学习中的推理优化提供了新方法。

Abstract: Reinforcement Learning with Verifiable Rewards (RLVR) can elicit strong multi-step reasoning, yet it often encourages overly verbose traces. Moreover, naive length penalties in group-relative optimization can severely hurt accuracy. We attribute this failure to two structural issues: (i) Dilution of Length Baseline, where incorrect responses (with zero length reward) depress the group baseline and over-penalize correct solutions; and (ii) Difficulty-Penalty Mismatch, where a static penalty cannot adapt to problem difficulty, suppressing necessary reasoning on hard instances while leaving redundancy on easy ones. We propose Dynamic Decoupled Conditional Advantage (DDCA) to decouple efficiency optimization from correctness. DDCA computes length advantages conditionally within the correct-response cluster to eliminate baseline dilution, and dynamically scales the penalty strength using the group pass rate as a proxy for difficulty. Experiments on GSM8K, MATH500, AMC23, and AIME25 show that DDCA consistently improves the efficiency--accuracy trade-off relative to adaptive baselines, reducing generated tokens by approximately 60% on simpler tasks (e.g., GSM8K) versus over 20% on harder benchmarks (e.g., AIME25), thereby maintaining or improving accuracy. Code is available at https://github.com/alphadl/DDCA.

</details>


### [86] [Out of the Memory Barrier: A Highly Memory Efficient Training System for LLMs with Million-Token Contexts](https://arxiv.org/abs/2602.02108)
*Wenhao Li,Daohai Yu,Gen Luo,Yuxin Zhang,Fei Chao,Rongrong Ji,Yifan Wu,Jiaxin Liu,Ziyang Gong,Zimu Liao*

Main category: cs.CL

Relevance: 85.0

TL;DR: OOMB是一个高效内存的训练系统，通过分块循环训练和动态激活重计算实现O(1)激活内存，结合KV缓存优化，使Qwen2.5-7B在单个H200 GPU上训练4M上下文成为可能。


<details>
  <summary>Details</summary>
Motivation: 训练长上下文LLM面临GPU内存瓶颈，主要原因是激活内存随序列长度线性增长。现有方法需要大规模集群进行上下文并行，资源消耗巨大。

Method: 采用分块循环训练框架和动态激活重计算保持恒定激活内存；通过分页内存管理器、异步CPU卸载和页面级稀疏注意力优化KV缓存管理。

Result: 每增加10K上下文token仅增加10MB内存开销，Qwen2.5-7B可在单个H200 GPU上训练4M上下文，相比传统方法大幅提升资源效率。

Conclusion: OOMB系统显著提升了长上下文LLM训练的资源效率，为大规模长上下文模型训练提供了实用解决方案。

Abstract: Training Large Language Models (LLMs) on long contexts is severely constrained by prohibitive GPU memory overhead, not training time. The primary culprits are the activations, whose memory footprints scale linearly with sequence length. We introduce OOMB, a highly memory-efficient training system that directly confronts this barrier. Our approach employs a chunk-recurrent training framework with on-the-fly activation recomputation, which maintains a constant activation memory footprint (O(1)) and shifts the primary bottleneck to the growing KV cache. To manage the KV cache, OOMB integrates a suite of synergistic optimizations: a paged memory manager for both the KV cache and its gradients to eliminate fragmentation, asynchronous CPU offloading to hide data transfer latency, and page-level sparse attention to reduce both computational complexity and communication overhead. The synergy of these techniques yields exceptional efficiency. Our empirical results show that for every additional 10K tokens of context, the end-to-end training memory overhead increases by a mere 10MB for Qwen2.5-7B. This allows training Qwen2.5-7B with a 4M-token context on a single H200 GPU, a feat that would otherwise require a large cluster using context parallelism. This work represents a substantial advance in resource efficiency for long-context LLM training. The source code is available at https://github.com/wenhaoli-xmu/OOMB.

</details>


### [87] [There Is More to Refusal in Large Language Models than a Single Direction](https://arxiv.org/abs/2602.02132)
*Faaiz Joad,Majd Hawasly,Sabri Boughorbel,Nadir Durrani,Husrev Taha Sencar*

Main category: cs.CL

Relevance: 85.0

TL;DR: 研究发现LLM的拒绝行为并非由单一激活方向控制，而是对应多个几何上不同的方向，但线性调控这些方向会产生相似的拒绝-过度拒绝权衡，主要影响拒绝方式而非是否拒绝。


<details>
  <summary>Details</summary>
Motivation: 先前研究认为大语言模型的拒绝行为由单一激活空间方向介导，可以进行有效调控和消融。本文旨在验证这一观点是否完整，探索不同拒绝类别背后的神经机制。

Method: 研究了11个拒绝和不服从类别（包括安全性、不完整请求、拟人化、过度拒绝等），分析这些拒绝行为在激活空间中的几何分布，并测试线性调控不同方向的效果。

Result: 发现不同拒绝行为对应几何上不同的激活方向，但线性调控任何拒绝相关方向都会产生几乎相同的拒绝-过度拒绝权衡，主要区别在于拒绝方式而非是否拒绝。

Conclusion: LLM的拒绝机制比先前认为的更复杂，涉及多个激活方向，但这些方向通过共享的一维控制机制调节拒绝强度，主要决定拒绝的具体表达方式。

Abstract: Prior work argues that refusal in large language models is mediated by a single activation-space direction, enabling effective steering and ablation. We show that this account is incomplete. Across eleven categories of refusal and non-compliance, including safety, incomplete or unsupported requests, anthropomorphization, and over-refusal, we find that these refusal behaviors correspond to geometrically distinct directions in activation space. Yet despite this diversity, linear steering along any refusal-related direction produces nearly identical refusal to over-refusal trade-offs, acting as a shared one-dimensional control knob. The primary effect of different directions is not whether the model refuses, but how it refuses.

</details>


### [88] [Focus-dLLM: Accelerating Long-Context Diffusion LLM Inference via Confidence-Guided Context Focusing](https://arxiv.org/abs/2602.02159)
*Lingkun Long,Yushi Huang,Shihao Bai,Ruihao Gong,Jun Zhang,Ao Zhou,Jianlei Yang*

Main category: cs.CL

Relevance: 85.0

TL;DR: Focus-dLLM：针对扩散大语言模型的无训练注意力稀疏化框架，通过过去置信度引导的指示器和汇点感知剪枝策略，在长上下文推理中实现29倍无损加速


<details>
  <summary>Details</summary>
Motivation: 扩散大语言模型(dLLMs)在长上下文处理方面表现出色，但双向全注意力的计算成本限制了推理效率。现有的稀疏注意力方法在扩散模型中效果不佳，因为需要在解码前估计未解码token的注意力重要性，而扩散过程中未掩码token位置未知。

Method: 1. 基于token置信度在相邻步骤间强相关的发现，设计过去置信度引导指示器预测未掩码区域；2. 提出汇点感知剪枝策略，准确估计并移除冗余注意力计算，同时保留高影响力的注意力汇点；3. 利用跨层一致性，在不同层间复用已识别的汇点位置以减少开销。

Result: 在32K上下文长度下实现超过29倍的无损加速，代码已开源。

Conclusion: Focus-dLLM为长上下文dLLM推理提供了准确高效的无训练注意力稀疏化框架，显著提升了推理效率。

Abstract: Diffusion Large Language Models (dLLMs) deliver strong long-context processing capability in a non-autoregressive decoding paradigm. However, the considerable computational cost of bidirectional full attention limits the inference efficiency. Although sparse attention is promising, existing methods remain ineffective. This stems from the need to estimate attention importance for tokens yet to be decoded, while the unmasked token positions are unknown during diffusion. In this paper, we present Focus-dLLM, a novel training-free attention sparsification framework tailored for accurate and efficient long-context dLLM inference. Based on the finding that token confidence strongly correlates across adjacent steps, we first design a past confidence-guided indicator to predict unmasked regions. Built upon this, we propose a sink-aware pruning strategy to accurately estimate and remove redundant attention computation, while preserving highly influential attention sinks. To further reduce overhead, this strategy reuses identified sink locations across layers, leveraging the observed cross-layer consistency. Experimental results show that our method offers more than $29\times$ lossless speedup under $32K$ context length. The code is publicly available at: https://github.com/Longxmas/Focus-dLLM

</details>


### [89] [D-CORE: Incentivizing Task Decomposition in Large Reasoning Models for Complex Tool Use](https://arxiv.org/abs/2602.02160)
*Bowen Xu,Shaoyu Wu,Hao Jiang,Kai Liu,Xin Chen,Lulu Hu,Bin Yang*

Main category: cs.CL

Relevance: 85.0

TL;DR: D-CORE是一个两阶段训练框架，通过任务分解推理和多样性感知强化学习来解决大型推理模型在复杂工具使用场景中的懒惰推理问题，显著提升了工具使用能力。


<details>
  <summary>Details</summary>
Motivation: 当前大型推理模型在复杂工具使用场景中缺乏子任务分解能力，导致"懒惰推理"问题，限制了它们解决复杂现实问题的能力。

Method: 提出D-CORE两阶段训练框架：1）通过自蒸馏激励模型的任务分解推理能力；2）使用多样性感知强化学习恢复模型的反思推理能力。

Result: D-CORE在多个基准测试中取得了显著改进：D-CORE-8B达到77.7%准确率，超越最佳8B模型5.7%；D-CORE-14B达到79.3%准确率，超越70B模型，尽管体积小了5倍。

Conclusion: D-CORE框架有效解决了大型推理模型的懒惰推理问题，通过任务分解和多样性强化学习显著提升了复杂工具使用能力，在不同模型规模上都取得了优异表现。

Abstract: Effective tool use and reasoning are essential capabilities for large reasoning models~(LRMs) to address complex real-world problems. Through empirical analysis, we identify that current LRMs lack the capability of sub-task decomposition in complex tool use scenarios, leading to Lazy Reasoning. To address this, we propose a two-stage training framework D-CORE~(\underline{\textbf{D}}ecomposing tasks and \underline{\textbf{Co}}mposing \underline{\textbf{Re}}asoning processes) that first incentivize the LRMs' task decomposition reasoning capability via self-distillation, followed by diversity-aware reinforcement learning~(RL) to restore LRMs' reflective reasoning capability. D-CORE achieves robust tool-use improvements across diverse benchmarks and model scales. Experiments on BFCLv3 demonstrate superiority of our method: D-CORE-8B reaches 77.7\% accuracy, surpassing the best-performing 8B model by 5.7\%. Meanwhile, D-CORE-14B establishes a new state-of-the-art at 79.3\%, outperforming 70B models despite being 5$\times$ smaller. The source code is available at https://github.com/alibaba/EfficientAI.

</details>


### [90] [AR-MAP: Are Autoregressive Large Language Models Implicit Teachers for Diffusion Large Language Models?](https://arxiv.org/abs/2602.02178)
*Liang Lin,Feng Xiong,Zengbin Wang,Kun Wang,Junhao Dong,Xuecai Hu,Yong Wang,Xiangxiang Chu*

Main category: cs.CL

Relevance: 85.0

TL;DR: AR-MAP：一种利用对齐好的自回归LLM作为隐式教师来对齐扩散LLM的新框架，通过权重缩放实现知识迁移，避免直接对齐的高方差和计算开销。


<details>
  <summary>Details</summary>
Motivation: 扩散大语言模型（DLLMs）作为自回归模型的有力替代，支持并行token生成，但其偏好对齐面临挑战，主要由于基于ELBO的似然估计引入的高方差。

Method: 提出AR-MAP框架，利用已对齐的自回归LLM作为隐式教师，通过简单的权重缩放让DLLM吸收对齐知识，利用两种生成范式间的共享架构结构。

Result: 在多种偏好对齐任务上的实验表明，AR-MAP相比现有DLLM专用对齐方法具有竞争性或更优性能，在所有任务和模型上平均得分达69.08%。

Conclusion: AR-MAP成功实现了从自回归LLM到扩散LLM的偏好对齐知识迁移，避免了直接对齐的高方差和计算开销，为DLLM对齐提供了有效解决方案。

Abstract: Diffusion Large Language Models (DLLMs) have emerged as a powerful alternative to autoregressive models, enabling parallel token generation across multiple positions. However, preference alignment of DLLMs remains challenging due to high variance introduced by Evidence Lower Bound (ELBO)-based likelihood estimation. In this work, we propose AR-MAP, a novel transfer learning framework that leverages preference-aligned autoregressive LLMs (AR-LLMs) as implicit teachers for DLLM alignment. We reveal that DLLMs can effectively absorb alignment knowledge from AR-LLMs through simple weight scaling, exploiting the shared architectural structure between these divergent generation paradigms. Crucially, our approach circumvents the high variance and computational overhead of direct DLLM alignment and comprehensive experiments across diverse preference alignment tasks demonstrate that AR-MAP achieves competitive or superior performance compared to existing DLLM-specific alignment methods, achieving 69.08\% average score across all tasks and models. Our Code is available at https://github.com/AMAP-ML/AR-MAP.

</details>


### [91] [Evaluating Metalinguistic Knowledge in Large Language Models across the World's Languages](https://arxiv.org/abs/2602.02182)
*Tjaša Arčon,Matej Klemen,Marko Robnik-Šikonja,Kaja Dobrovoljc*

Main category: cs.CL

Relevance: 85.0

TL;DR: 论文评估了LLMs在元语言知识（对语言结构的显式推理）方面的能力，发现当前模型表现有限，GPT-4o最佳但准确率仅0.367，开源模型更差。模型表现受语言数字资源可用性影响显著，低资源语言表现差，表明LLMs的元语言知识是碎片化的，受数据可用性而非通用语法能力驱动。


<details>
  <summary>Details</summary>
Motivation: 当前LLMs在语言使用任务上被广泛评估，但其对语言结构的知识理解不足。现有语言基准通常关注狭窄现象，强调高资源语言，很少评估元语言知识（对语言结构的显式推理而非语言使用）。需要系统评估LLMs在不同语言和语言领域的元语言知识。

Method: 创建了一个元语言知识基准，评估LLMs在不同语言和语言领域（词汇、音韵、句法等）的表现。使用准确率和宏F1分数，结合多数类和随机基线进行分析。考察语言相关因素（数字语言状态、资源可用性、地理、谱系、社会语言学因素）对模型性能的影响。

Result: 1. 当前LLMs的元语言知识有限：GPT-4o表现最佳但准确率仅0.367，开源模型落后；2. 所有模型表现高于随机但未超过多数类基线，表明模型捕捉到跨语言模式但缺乏细粒度语法区分；3. 不同语言领域表现差异大：词汇特征准确率最高，音韵特征最低；4. 语言层面：准确率与数字语言状态强相关，高数字存在和资源可用性的语言评估更准确，低资源语言表现显著更低；5. 资源相关指标（维基百科大小、语料库可用性）比地理、谱系或社会语言学因素更能预测准确率。

Conclusion: LLMs的元语言知识是碎片化的，受数据可用性而非通用语法能力驱动。模型表现严重依赖训练数据的数量和多样性，特别是在低资源语言上表现差。需要更系统评估和更大全球语言多样性来改进LLMs的元语言能力。

Abstract: Large language models (LLMs) are routinely evaluated on language use tasks, yet their knowledge of linguistic structure remains poorly understood. Existing linguistic benchmarks typically focus on narrow phenomena, emphasize high-resource languages, and rarely evaluate metalinguistic knowledge-explicit reasoning about language structure rather than language use. Using accuracy and macro F1, together with majority-class and chance baselines, we analyse overall performance and examine variation by linguistic domains and language-related factors. Our results show that metalinguistic knowledge in current LLMs is limited: GPT-4o performs best but achieves only moderate accuracy (0.367), while open-source models lag behind. All models perform above chance but fail to outperform the majority-class baseline, suggesting they capture cross-linguistic patterns but lack fine-grained grammatical distinctions. Performance varies across linguistic domains, with lexical features showing the highest accuracy and phonological features among the lowest, partially reflecting differences in online visibility. At the language level, accuracy shows a strong association with digital language status: languages with higher digital presence and resource availability are evaluated more accurately, while low-resource languages show substantially lower performance. Analyses of predictive factors confirm that resource-related indicators (Wikipedia size, corpus availability) are more informative predictors of accuracy than geographical, genealogical, or sociolinguistic factors. Together, these results suggest that LLMs' metalinguistic knowledge is fragmented and shaped by data availability rather than generalizable grammatical competence across the world's languages. We release our benchmark as an open-source dataset to support systematic evaluation and encourage greater global linguistic diversity in future LLMs.

</details>


### [92] [Am I More Pointwise or Pairwise? Revealing Position Bias in Rubric-Based LLM-as-a-Judge](https://arxiv.org/abs/2602.02219)
*Yuzheng Xu,Tosho Hirasawa,Tadashi Kozuno,Yoshitaka Ushiku*

Main category: cs.CL

Relevance: 85.0

TL;DR: 本文研究发现基于评分标准的LLM评估存在位置偏差，并提出平衡排列策略来缓解此问题，从而提高LLM评估与人类评价的相关性。


<details>
  <summary>Details</summary>
Motivation: 当前LLM-as-a-Judge研究主要集中在点对点和成对评估范式，而基于评分标准的评估（LLM从多个评分标准中选择分数）较少被分析。本文发现这种评估方式存在位置偏差问题，即LLM倾向于选择出现在评分标准列表中特定位置的分数选项。

Method: 通过多个模型和数据集的控制实验证明位置偏差的存在，并提出平衡排列策略：将每个分数选项均匀分布在各个位置上，然后聚合平衡排列后的分数。

Result: 实验结果显示：1）基于评分标准的LLM评估存在一致的位置偏差；2）平衡排列策略不仅能揭示潜在的位置偏差，还能显著提高LLM评估与人类评价的相关性。

Conclusion: 基于评分标准的LLM-as-a-Judge本质上不是点对点评估，简单的基于排列的校准可以显著提高其可靠性，这对LLM评估的实践有重要指导意义。

Abstract: Large language models (LLMs) are now widely used to evaluate the quality of text, a field commonly referred to as LLM-as-a-judge. While prior works mainly focus on point-wise and pair-wise evaluation paradigms. Rubric-based evaluation, where LLMs select a score from multiple rubrics, has received less analysis. In this work, we show that rubric-based evaluation implicitly resembles a multi-choice setting and therefore has position bias: LLMs prefer score options appearing at specific positions in the rubric list. Through controlled experiments across multiple models and datasets, we demonstrate consistent position bias. To mitigate this bias, we propose a balanced permutation strategy that evenly distributes each score option across positions. We show that aggregating scores across balanced permutations not only reveals latent position bias, but also improves correlation between the LLM-as-a-Judge and human. Our results suggest that rubric-based LLM-as-a-Judge is not inherently point-wise and that simple permutation-based calibration can substantially improve its reliability.

</details>


### [93] [OpenSeal: Good, Fast, and Cheap Construction of an Open-Source Southeast Asian LLM via Parallel Data](https://arxiv.org/abs/2602.02266)
*Tan Sang Nguyen,Muhammad Reza Qorib,Hwee Tou Ng*

Main category: cs.CL

Relevance: 85.0

TL;DR: OpenSeal：首个真正开源的东南亚语言大模型，仅用34.7B并行数据和180小时训练，性能媲美同规模现有模型


<details>
  <summary>Details</summary>
Motivation: 当前大多数LLM仍是英语中心，在低资源语言上表现不佳。虽然已有一些东南亚语言LLM，但都不是真正开源（未公开训练数据）。真正开源模型对透明度、理解LLM内部机制（偏见、泛化、多语言性）至关重要。受并行数据提升多语言性能的最新进展启发，研究并行数据在LLM持续预训练中的有效性。

Method: 进行受控全面的实验，研究并行数据在LLM持续预训练中的有效性。发现仅使用并行数据是将LLM扩展到新语言的最有效方法。使用34.7B tokens的并行数据，在8x NVIDIA H200 GPUs上训练180小时，构建OpenSeal模型。

Result: 仅使用并行数据是最有效的方法。OpenSeal成为首个真正开源的东南亚语言LLM，性能与同规模现有模型相当。

Conclusion: 并行数据在扩展LLM到新语言方面非常有效。OpenSeal填补了真正开源东南亚语言LLM的空白，为研究LLM内部机制提供了透明基础。

Abstract: Large language models (LLMs) have proven to be effective tools for a wide range of natural language processing (NLP) applications. Although many LLMs are multilingual, most remain English-centric and perform poorly on low-resource languages. Recently, several Southeast Asia-focused LLMs have been developed, but none are truly open source, as they do not publicly disclose their training data. Truly open-source models are important for transparency and for enabling a deeper and more precise understanding of LLM internals and development, including biases, generalization, and multilinguality. Motivated by recent advances demonstrating the effectiveness of parallel data in improving multilingual performance, we conduct controlled and comprehensive experiments to study the effectiveness of parallel data in continual pretraining of LLMs. Our findings show that using only parallel data is the most effective way to extend an LLM to new languages. Using just 34.7B tokens of parallel data and 180 hours on 8x NVIDIA H200 GPUs, we built OpenSeal, the first truly open Southeast Asian LLM that rivals the performance of existing models of similar size.

</details>


### [94] [Kimi K2.5: Visual Agentic Intelligence](https://arxiv.org/abs/2602.02276)
*Kimi Team,Tongtong Bai,Yifan Bai,Yiping Bao,S. H. Cai,Yuan Cao,Y. Charles,H. S. Che,Cheng Chen,Guanduo Chen,Huarong Chen,Jia Chen,Jiahao Chen,Jianlong Chen,Jun Chen,Kefan Chen,Liang Chen,Ruijue Chen,Xinhao Chen,Yanru Chen,Yanxu Chen,Yicun Chen,Yimin Chen,Yingjiang Chen,Yuankun Chen,Yujie Chen,Yutian Chen,Zhirong Chen,Ziwei Chen,Dazhi Cheng,Minghan Chu,Jialei Cui,Jiaqi Deng,Muxi Diao,Hao Ding,Mengfan Dong,Mengnan Dong,Yuxin Dong,Yuhao Dong,Angang Du,Chenzhuang Du,Dikang Du,Lingxiao Du,Yulun Du,Yu Fan,Shengjun Fang,Qiulin Feng,Yichen Feng,Garimugai Fu,Kelin Fu,Hongcheng Gao,Tong Gao,Yuyao Ge,Shangyi Geng,Chengyang Gong,Xiaochen Gong,Zhuoma Gongque,Qizheng Gu,Xinran Gu,Yicheng Gu,Longyu Guan,Yuanying Guo,Xiaoru Hao,Weiran He,Wenyang He,Yunjia He,Chao Hong,Hao Hu,Jiaxi Hu,Yangyang Hu,Zhenxing Hu,Ke Huang,Ruiyuan Huang,Weixiao Huang,Zhiqi Huang,Tao Jiang,Zhejun Jiang,Xinyi Jin,Yu Jing,Guokun Lai,Aidi Li,C. Li,Cheng Li,Fang Li,Guanghe Li,Guanyu Li,Haitao Li,Haoyang Li,Jia Li,Jingwei Li,Junxiong Li,Lincan Li,Mo Li,Weihong Li,Wentao Li,Xinhang Li,Xinhao Li,Yang Li,Yanhao Li,Yiwei Li,Yuxiao Li,Zhaowei Li,Zheming Li,Weilong Liao,Jiawei Lin,Xiaohan Lin,Zhishan Lin,Zichao Lin,Cheng Liu,Chenyu Liu,Hongzhang Liu,Liang Liu,Shaowei Liu,Shudong Liu,Shuran Liu,Tianwei Liu,Tianyu Liu,Weizhou Liu,Xiangyan Liu,Yangyang Liu,Yanming Liu,Yibo Liu,Yuanxin Liu,Yue Liu,Zhengying Liu,Zhongnuo Liu,Enzhe Lu,Haoyu Lu,Zhiyuan Lu,Junyu Luo,Tongxu Luo,Yashuo Luo,Long Ma,Yingwei Ma,Shaoguang Mao,Yuan Mei,Xin Men,Fanqing Meng,Zhiyong Meng,Yibo Miao,Minqing Ni,Kun Ouyang,Siyuan Pan,Bo Pang,Yuchao Qian,Ruoyu Qin,Zeyu Qin,Jiezhong Qiu,Bowen Qu,Zeyu Shang,Youbo Shao,Tianxiao Shen,Zhennan Shen,Juanfeng Shi,Lidong Shi,Shengyuan Shi,Feifan Song,Pengwei Song,Tianhui Song,Xiaoxi Song,Hongjin Su,Jianlin Su,Zhaochen Su,Lin Sui,Jinsong Sun,Junyao Sun,Tongyu Sun,Flood Sung,Yunpeng Tai,Chuning Tang,Heyi Tang,Xiaojuan Tang,Zhengyang Tang,Jiawen Tao,Shiyuan Teng,Chaoran Tian,Pengfei Tian,Ao Wang,Bowen Wang,Chensi Wang,Chuang Wang,Congcong Wang,Dingkun Wang,Dinglu Wang,Dongliang Wang,Feng Wang,Hailong Wang,Haiming Wang,Hengzhi Wang,Huaqing Wang,Hui Wang,Jiahao Wang,Jinhong Wang,Jiuzheng Wang,Kaixin Wang,Linian Wang,Qibin Wang,Shengjie Wang,Shuyi Wang,Si Wang,Wei Wang,Xiaochen Wang,Xinyuan Wang,Yao Wang,Yejie Wang,Yipu Wang,Yiqin Wang,Yucheng Wang,Yuzhi Wang,Zhaoji Wang,Zhaowei Wang,Zhengtao Wang,Zhexu Wang,Zihan Wang,Zizhe Wang,Chu Wei,Ming Wei,Chuan Wen,Zichen Wen,Chengjie Wu,Haoning Wu,Junyan Wu,Rucong Wu,Wenhao Wu,Yuefeng Wu,Yuhao Wu,Yuxin Wu,Zijian Wu,Chenjun Xiao,Jin Xie,Xiaotong Xie,Yuchong Xie,Yifei Xin,Bowei Xing,Boyu Xu,Jianfan Xu,Jing Xu,Jinjing Xu,L. H. Xu,Lin Xu,Suting Xu,Weixin Xu,Xinbo Xu,Xinran Xu,Yangchuan Xu,Yichang Xu,Yuemeng Xu,Zelai Xu,Ziyao Xu,Junjie Yan,Yuzi Yan,Guangyao Yang,Hao Yang,Junwei Yang,Kai Yang,Ningyuan Yang,Ruihan Yang,Xiaofei Yang,Xinlong Yang,Ying Yang,Yi Yang,Yi Yang,Zhen Yang,Zhilin Yang,Zonghan Yang,Haotian Yao,Dan Ye,Wenjie Ye,Zhuorui Ye,Bohong Yin,Chengzhen Yu,Longhui Yu,Tao Yu,Tianxiang Yu,Enming Yuan,Mengjie Yuan,Xiaokun Yuan,Yang Yue,Weihao Zeng,Dunyuan Zha,Haobing Zhan,Dehao Zhang,Hao Zhang,Jin Zhang,Puqi Zhang,Qiao Zhang,Rui Zhang,Xiaobin Zhang,Y. Zhang,Yadong Zhang,Yangkun Zhang,Yichi Zhang,Yizhi Zhang,Yongting Zhang,Yu Zhang,Yushun Zhang,Yutao Zhang,Yutong Zhang,Zheng Zhang,Chenguang Zhao,Feifan Zhao,Jinxiang Zhao,Shuai Zhao,Xiangyu Zhao,Yikai Zhao,Zijia Zhao,Huabin Zheng,Ruihan Zheng,Shaojie Zheng,Tengyang Zheng,Junfeng Zhong,Longguang Zhong,Weiming Zhong,M. Zhou,Runjie Zhou,Xinyu Zhou,Zaida Zhou,Jinguo Zhu,Liya Zhu,Xinhao Zhu,Yuxuan Zhu,Zhen Zhu,Jingze Zhuang,Weiyu Zhuang,Ying Zou,Xinxing Zu*

Main category: cs.CL

Relevance: 85.0

TL;DR: Kimi K2.5是一个开源多模态代理模型，通过文本-视觉联合优化提升通用代理智能，并引入Agent Swarm并行代理编排框架，在编码、视觉、推理和代理任务上取得SOTA结果，延迟降低4.5倍。


<details>
  <summary>Details</summary>
Motivation: 推动通用代理智能发展，解决复杂任务处理中的效率和能力问题。当前多模态代理模型在文本和视觉模态协同优化方面存在不足，且单代理处理复杂任务时效率较低。

Method: 1) 文本-视觉联合预训练、零视觉SFT、文本-视觉联合强化学习等多模态联合优化技术；2) Agent Swarm自导向并行代理编排框架，动态分解复杂任务为异构子问题并行执行。

Result: 在编码、视觉、推理和代理任务上取得SOTA结果；Agent Swarm相比单代理基线延迟降低高达4.5倍；发布了后训练模型检查点。

Conclusion: Kimi K2.5通过多模态联合优化和并行代理编排框架，显著提升了代理智能的性能和效率，为未来代理智能研究和实际应用提供了有力工具。

Abstract: We introduce Kimi K2.5, an open-source multimodal agentic model designed to advance general agentic intelligence. K2.5 emphasizes the joint optimization of text and vision so that two modalities enhance each other. This includes a series of techniques such as joint text-vision pre-training, zero-vision SFT, and joint text-vision reinforcement learning. Building on this multimodal foundation, K2.5 introduces Agent Swarm, a self-directed parallel agent orchestration framework that dynamically decomposes complex tasks into heterogeneous sub-problems and executes them concurrently. Extensive evaluations show that Kimi K2.5 achieves state-of-the-art results across various domains including coding, vision, reasoning, and agentic tasks. Agent Swarm also reduces latency by up to $4.5\times$ over single-agent baselines. We release the post-trained Kimi K2.5 model checkpoint to facilitate future research and real-world applications of agentic intelligence.

</details>


### [95] [Cross-Lingual Stability of LLM Judges Under Controlled Generation: Evidence from Finno-Ugric Languages](https://arxiv.org/abs/2602.02287)
*Isaac Chung,Linda Freienthal*

Main category: cs.CL

Relevance: 85.0

TL;DR: 该研究通过控制生成条件，在爱沙尼亚语、芬兰语和匈牙利语中测试LLM评估的稳定性，发现表面指标稳定但语用判断存在排名反转，表明零-shot评估在多形态丰富语言中不可靠。


<details>
  <summary>Details</summary>
Motivation: 跨语言LLM评估通常混淆模型性能差异和测量不稳定性两个因素。研究者希望分离这些因素，通过控制生成条件来专门考察评估方法在不同语言中的可靠性，特别是在形态丰富的芬兰-乌戈尔语系中。

Method: 使用相同的生成参数在爱沙尼亚语、芬兰语和匈牙利语中生成合成客户支持对话，保持生成条件恒定。比较自动指标（词汇多样性、表面和语义相似性）和LLM-as-a-judge评分在不同语言中的稳定性，并以少量爱沙尼亚语母语者标注作为参考。

Result: 发现系统性排名不稳定性：表面级指标保持跨语言稳定性，但语用判断（连贯性、指令遵循）出现排名反转和接近零的相关性。由于生成条件相同，这些不一致反映了评估方法在不同语言中的行为差异，而非真实模型差异。

Conclusion: 零-shot评估在多形态丰富语言的语篇级评估中不可靠，需要在部署前进行语言特定的校准。控制生成设计可作为诊断工具，检测评估方法的跨语言稳定性。

Abstract: Cross-lingual evaluation of large language models (LLMs) typically conflates two sources of variance: genuine model performance differences and measurement instability. We investigate evaluation reliability by holding generation conditions constant while varying target language. Using synthetic customer-support dialogues generated with identical parameters across Estonian, Finnish, and Hungarian, we test whether automatic metrics and LLM-as-a-judge scoring produce stable model rankings across these morphologically rich, related Finno-Ugric languages. With a small set of Estonian native speaker annotations as a reference point, we find systematic ranking instabilities: surface-level metrics (lexical diversity, surface and semantic similarity) maintain cross-language stability, but pragmatic judgments (coherence, instruction-following) exhibit rank inversions and near-zero correlations. Because generation is controlled, these inconsistencies reflect how judge scoring behaves differently across languages rather than true model differences.
  This controlled design provides a diagnostic probe: evaluation methods that fail to maintain stability under identical generation conditions signal transfer failure before deployment. Our findings suggest that zero-shot judge transfer is unreliable for discourse-level assessment in morphologically rich languages, motivating language-specific calibration against targeted human baselines. We release our controlled generation protocol, synthetic data, and evaluation framework to enable replication across language families at https://github.com/isaac-chung/cross-lingual-stability-judges.

</details>


### [96] [Advancing General-Purpose Reasoning Models with Modular Gradient Surgery](https://arxiv.org/abs/2602.02301)
*Min Cai,Yu Liang,Longzheng Wang,Yan Wang,Yueyang Zhang,Long Xia,Zhiyuan Sun,Xi Ye,Daiting Shi*

Main category: cs.CL

Relevance: 85.0

TL;DR: 论文提出模块化梯度手术(MGS)方法，解决多领域强化学习中跨领域干扰问题，通过在Transformer模块层面解决梯度冲突，显著提升大型推理模型的性能


<details>
  <summary>Details</summary>
Motivation: 训练单一通用大型推理模型面临跨领域异质性挑战，现有顺序RL和混合RL策略存在显著的跨领域干扰，导致整体性能提升有限

Method: 提出模块化梯度手术(MGS)，在Transformer内部模块层面解决梯度冲突，而不是在整个模型层面。该方法识别并协调不同领域任务在特定模块中的梯度冲突

Result: 在Llama和Qwen模型上，MGS相比标准多任务RL在数学、通用聊天和指令跟随三个代表性领域分别平均提升4.3(16.6%)和4.5(11.1%)个点，且在长时间训练中保持有效

Conclusion: 研究阐明了多领域RL中干扰的来源，提出了有效的模块化梯度手术方法，为训练通用大型推理模型提供了解决方案

Abstract: Reinforcement learning (RL) has played a central role in recent advances in large reasoning models (LRMs), yielding strong gains in verifiable and open-ended reasoning. However, training a single general-purpose LRM across diverse domains remains challenging due to pronounced domain heterogeneity. Through a systematic study of two widely used strategies, Sequential RL and Mixed RL, we find that both incur substantial cross-domain interference at the behavioral and gradient levels, resulting in limited overall gains. To address these challenges, we introduce **M**odular **G**radient **S**urgery (**MGS**), which resolves gradient conflicts at the module level within the transformer. When applied to Llama and Qwen models, MGS achieves average improvements of 4.3 (16.6\%) and 4.5 (11.1\%) points, respectively, over standard multi-task RL across three representative domains (math, general chat, and instruction following). Further analysis demonstrates that MGS remains effective under prolonged training. Overall, our study clarifies the sources of interference in multi-domain RL and presents an effective solution for training general-purpose LRMs.

</details>


### [97] [The Shape of Beliefs: Geometry, Dynamics, and Interventions along Representation Manifolds of Language Models' Posteriors](https://arxiv.org/abs/2602.02315)
*Raphaël Sarfati,Eric Bigelow,Daniel Wurgaft,Jack Merullo,Atticus Geiger,Owen Lewis,Tom McGrath,Ekdeep Singh Lubana*

Main category: cs.CL

Relevance: 85.0

TL;DR: 论文研究LLM如何编码和更新概率信念，发现在上下文学习中形成弯曲的"信念流形"，线性干预会偏离流形，而几何感知的干预能更好地保持信念结构。


<details>
  <summary>Details</summary>
Motivation: 研究LLM如何编码提示条件下的信念（对答案和主张的后验分布），缺乏对这些信念在表示空间中如何编码、如何随新证据更新、以及干预如何重塑它们的机制性解释。

Method: 使用Llama-3.2在受控环境中生成正态分布样本，通过上下文学习隐式推断分布参数（均值和标准差），研究信念流形的形成和变化。采用线性场探测（LFP）方法平铺数据流形并进行几何感知的干预。

Result: 发现足够的上下文学习会形成弯曲的"信念流形"表示；当分布突然变化时，模型能够适应；标准线性干预会使模型偏离流形并产生耦合的分布外偏移，而几何和场感知的干预能更好地保持目标信念族。

Conclusion: LLM中自然涌现出丰富的结构，纯线性的概念表示通常是不充分的抽象；线性场探测（LFP）可作为平铺数据流形并尊重底层几何的简单干预方法。

Abstract: Large language models (LLMs) represent prompt-conditioned beliefs (posteriors over answers and claims), but we lack a mechanistic account of how these beliefs are encoded in representation space, how they update with new evidence, and how interventions reshape them. We study a controlled setting in which Llama-3.2 generates samples from a normal distribution by implicitly inferring its parameters (mean and standard deviation) given only samples from the distribution in context. We find representations of curved "belief manifolds" for these parameters form with sufficient in-context learning and study how the model adapts when the distribution suddenly changes. While standard linear steering often pushes the model off-manifold and induces coupled, out-of-distribution shifts, geometry and field-aware steering better preserves the intended belief family. Our work demonstrates an example of linear field probing (LFP) as a simple approach to tile the data manifold and make interventions that respect the underlying geometry. We conclude that rich structure emerges naturally in LLMs and that purely linear concept representations are often an inadequate abstraction.

</details>


### [98] [Language Steering for Multilingual In-Context Learning](https://arxiv.org/abs/2602.02326)
*Neeraja Kirtane,Kuan-Hao Huang*

Main category: cs.CL

Relevance: 85.0

TL;DR: 提出语言向量方法，通过激活差异引导LLM在推理时向目标语言空间偏移，提升多语言上下文学习性能


<details>
  <summary>Details</summary>
Motivation: 多语言大语言模型在非英语语言上的表现显著低于英语，特别是在上下文学习中，使用英语演示但测试非英语输入时性能下降严重。作者假设LLM开发了理解语言的通用语义空间，不同语言在该空间中被编码为不同方向。

Method: 提出语言向量方法：无需训练的引导方法，利用源语言和目标语言之间的激活差异来指导模型行为。在推理过程中将向量添加到中间模型激活中，使模型内部表示向目标语言空间偏移，无需参数更新。

Result: 在三个数据集、19种语言、三个不同模型上评估，结果显示在所有测试任务和语言上，多语言上下文学习性能均优于基线。层次聚类分析显示引导向量揭示了与语言家族对齐的有意义语言结构，这些表示具有任务无关性。

Conclusion: 语言向量方法有效提升了多语言LLM的性能，揭示了模型内部语言表示的结构，为理解LLM的多语言能力提供了新视角。

Abstract: While multilingual large language models have gained widespread adoption, their performance on non-English languages remains substantially inferior to English. This disparity is particularly evident in in-context learning scenarios, where providing demonstrations in English but testing on non-English inputs leads to significant performance degradation. In this paper, we hypothesize that LLMs develop a universal semantic space for understanding languages, where different languages are encoded as distinct directions within this space. Based on this hypothesis, we propose language vectors -- a training-free language steering approach that leverages activation differences between source and target languages to guide model behavior. We steer the model generations by adding the vector to the intermediate model activations during inference. This is done to make the model's internal representations shift towards the target language space without any parameter updates. We evaluate our method across three datasets and test on a total of 19 languages on three different models. Our results show consistent improvements on multilingual in-context learning over baselines across all tasks and languages tested. Beyond performance gains, hierarchical clustering of steering vectors reveals meaningful linguistic structure aligned with language families. These vectors also successfully transfer across tasks, demonstrating that these representations are task-agnostic.

</details>


### [99] [Why Steering Works: Toward a Unified View of Language Model Parameter Dynamics](https://arxiv.org/abs/2602.02343)
*Ziwen Xu,Chenyan Wu,Hengyu Sun,Haiwen Hong,Mengru Wang,Yunzhi Yao,Longtao Huang,Hui Xue,Shumin Deng,Zhixuan Chu,Huajun Chen,Ningyu Zhang*

Main category: cs.CL

Relevance: 85.0

TL;DR: 提出统一框架分析LLM控制方法，将干预视为动态权重更新，引入偏好-效用分析揭示两者权衡，基于激活流形视角解释机制，并提出改进方法SPLIT。


<details>
  <summary>Details</summary>
Motivation: 现有LLM控制方法（权重微调、LoRA、激活干预）研究孤立，缺乏统一框架进行比较分析，难以理解不同方法的联系和权衡。

Method: 1) 提出统一框架，将各种控制方法视为由控制信号诱导的动态权重更新；2) 引入偏好-效用分析，使用极性配对对比示例在共享对数几率尺度上测量偏好（目标概念倾向）和效用（连贯有效生成）；3) 从激活流形视角解释控制机制；4) 提出新方法SPLIT。

Result: 发现偏好与效用之间存在一致权衡：更强的控制增加偏好但可预测地降低效用。激活流形视角显示控制沿目标概念方向移动表示以增强偏好，但当干预将表示推离有效生成流形时效用下降。

Conclusion: 为LLM控制方法提供统一分析框架，揭示偏好-效用权衡的根本机制，并提出SPLIT方法在提升偏好的同时更好地保持效用。

Abstract: Methods for controlling large language models (LLMs), including local weight fine-tuning, LoRA-based adaptation, and activation-based interventions, are often studied in isolation, obscuring their connections and making comparison difficult. In this work, we present a unified view that frames these interventions as dynamic weight updates induced by a control signal, placing them within a single conceptual framework. Building on this view, we propose a unified preference-utility analysis that separates control effects into preference, defined as the tendency toward a target concept, and utility, defined as coherent and task-valid generation, and measures both on a shared log-odds scale using polarity-paired contrastive examples. Across methods, we observe a consistent trade-off between preference and utility: stronger control increases preference while predictably reducing utility. We further explain this behavior through an activation manifold perspective, in which control shifts representations along target-concept directions to enhance preference, while utility declines primarily when interventions push representations off the model's valid-generation manifold. Finally, we introduce a new steering approach SPLIT guided by this analysis that improves preference while better preserving utility. Code is available at https://github.com/zjunlp/EasyEdit/blob/main/examples/SPLIT.md.

</details>


### [100] [Automated Multiple Mini Interview (MMI) Scoring](https://arxiv.org/abs/2602.02360)
*Ryan Huynh,Frank Guerin,Alison Callwood*

Main category: cs.CL

Relevance: 85.0

TL;DR: 提出多智能体提示框架，用于评估MMI中的软技能，无需微调即可超越专业基线，在ASAP基准上达到SOTA水平


<details>
  <summary>Details</summary>
Motivation: 评估软技能（如共情、伦理判断、沟通）在竞争性选拔中很重要，但人工评分存在不一致性和偏见。现有的基于推理的微调方法难以处理MMI的抽象、上下文依赖特性，无法捕捉候选人叙述中的隐含信号。

Method: 引入多智能体提示框架，将评估过程分解为转录本精炼和标准特定评分。使用大型指令调优模型进行3-shot上下文学习，无需额外训练。

Result: 在MMI评估中，平均QWK达到0.62，显著优于专业微调基线的0.32，与人类专家可靠性相当。在ASAP基准上，无需额外训练即可媲美领域特定SOTA模型。

Conclusion: 对于复杂的主观推理任务，结构化提示工程可能为数据密集型微调提供可扩展的替代方案，改变了LLM在自动评估中的应用方式。

Abstract: Assessing soft skills such as empathy, ethical judgment, and communication is essential in competitive selection processes, yet human scoring is often inconsistent and biased. While Large Language Models (LLMs) have improved Automated Essay Scoring (AES), we show that state-of-the-art rationale-based fine-tuning methods struggle with the abstract, context-dependent nature of Multiple Mini-Interviews (MMIs), missing the implicit signals embedded in candidate narratives. We introduce a multi-agent prompting framework that breaks down the evaluation process into transcript refinement and criterion-specific scoring. Using 3-shot in-context learning with a large instruct-tuned model, our approach outperforms specialised fine-tuned baselines (Avg QWK 0.62 vs 0.32) and achieves reliability comparable to human experts. We further demonstrate the generalisability of our framework on the ASAP benchmark, where it rivals domain-specific state-of-the-art models without additional training. These findings suggest that for complex, subjective reasoning tasks, structured prompt engineering may offer a scalable alternative to data-intensive fine-tuning, altering how LLMs can be applied to automated assessment.

</details>


### [101] [Proof-RM: A Scalable and Generalizable Reward Model for Math Proof](https://arxiv.org/abs/2602.02377)
*Haotong Yang,Zitong Wang,Shijia Kang,Siqi Yang,Wenkai Yu,Xu Niu,Yike Sun,Yi Hu,Zhouchen Lin,Muhan Zhang*

Main category: cs.CL

Relevance: 85.0

TL;DR: 该论文提出了一种可扩展的数据构建流程，利用LLM生成高质量的"问题-证明-检查"三元组数据，训练能够可靠评估完整证明过程的奖励模型，以增强LLM的数学推理能力。


<details>
  <summary>Details</summary>
Motivation: 虽然LLM通过可验证奖励的强化学习展示了强大的数学推理能力，但许多高级数学问题是基于证明的，无法通过简单的答案匹配来确定证明的真实性。需要能够可靠评估完整证明过程的奖励模型来实现自动验证。

Method: 设计了可扩展的数据构建流程：1）利用LLM生成大量高质量的"问题-证明-检查"三元组数据；2）通过系统变化问题来源、生成方法和模型配置，创建多样化的证明对；3）通过分层人工审核进行过滤；4）训练证明检查奖励模型，加入过程奖励和令牌权重平衡来稳定RL过程。

Result: 实验验证了模型的可扩展性和强大性能，包括奖励准确性、泛化能力和测试时指导等多个角度，为增强LLM数学能力提供了重要的实践方法和工具。

Conclusion: 该研究为LLM数学能力强化提供了实用的数据构建流程和奖励模型训练方法，能够有效处理基于证明的数学问题，支持自动验证和强化学习过程。

Abstract: While Large Language Models (LLMs) have demonstrated strong math reasoning abilities through Reinforcement Learning with *Verifiable Rewards* (RLVR), many advanced mathematical problems are proof-based, with no guaranteed way to determine the authenticity of a proof by simple answer matching. To enable automatic verification, a Reward Model (RM) capable of reliably evaluating full proof processes is required. In this work, we design a *scalable* data-construction pipeline that, with minimal human effort, leverages LLMs to generate a large quantity of high-quality "**question-proof-check**" triplet data. By systematically varying problem sources, generation methods, and model configurations, we create diverse problem-proof pairs spanning multiple difficulty levels, linguistic styles, and error types, subsequently filtered through hierarchical human review for label alignment. Utilizing these data, we train a proof-checking RM, incorporating additional process reward and token weight balance to stabilize the RL process. Our experiments validate the model's scalability and strong performance from multiple perspectives, including reward accuracy, generalization ability and test-time guidance, providing important practical recipes and tools for strengthening LLM mathematical capabilities.

</details>


### [102] [From Sycophancy to Sensemaking: Premise Governance for Human-AI Decision Making](https://arxiv.org/abs/2602.02378)
*Raunak Jain,Mudita Khurana,John Stephens,Srinivas Dharmasanam,Shankar Venkataraman*

Main category: cs.CL

Relevance: 85.0

TL;DR: 论文提出从生成答案转向协作前提治理的范式转变，通过差异驱动的控制循环检测冲突、定位错位，并使用承诺门控和值门控挑战来确保AI决策的可靠性。


<details>
  <summary>Details</summary>
Motivation: 随着LLM从辅助工具扩展到决策支持，出现了一个危险模式：流畅的同意但缺乏校准判断。低摩擦的助手可能变得谄媚，隐含假设被固化，验证成本转嫁给专家，而结果反馈太晚无法作为奖励信号。在深度不确定性决策中，扩展流畅同意会放大糟糕承诺的速度快于建立专业知识。

Method: 提出从答案生成转向协作前提治理的范式转变，基于知识基质的差异驱动控制循环：检测冲突、通过类型化差异（目的论、认识论、程序性）定位错位，通过决策切片触发有界协商。承诺门控阻止对未承诺的关键前提采取行动，值门控挑战在交互成本下分配探测。

Result: 提出可证伪的评估标准，信任附着于可审计的前提和证据标准，而非对话流畅性。以辅导场景为例进行说明。

Conclusion: 可靠的人机伙伴关系需要从答案生成转向协作前提治理，通过差异驱动的控制循环确保决策可靠性，将信任建立在可审计的前提和证据标准上。

Abstract: As LLMs expand from assistance to decision support, a dangerous pattern emerges: fluent agreement without calibrated judgment. Low-friction assistants can become sycophantic, baking in implicit assumptions and pushing verification costs onto experts, while outcomes arrive too late to serve as reward signals. In deep-uncertainty decisions (where objectives are contested and reversals are costly), scaling fluent agreement amplifies poor commitments faster than it builds expertise. We argue reliable human-AI partnership requires a shift from answer generation to collaborative premise governance over a knowledge substrate, negotiating only what is decision-critical. A discrepancy-driven control loop operates over this substrate: detecting conflicts, localizing misalignment via typed discrepancies (teleological, epistemic, procedural), and triggering bounded negotiation through decision slices. Commitment gating blocks action on uncommitted load-bearing premises unless overridden under logged risk; value-gated challenge allocates probing under interaction cost. Trust then attaches to auditable premises and evidence standards, not conversational fluency. We illustrate with tutoring and propose falsifiable evaluation criteria.

</details>


### [103] [Abstract Activation Spaces for Content-Invariant Reasoning in Large Language Models](https://arxiv.org/abs/2602.02462)
*Gabriele Maraia,Marco Valentino,Fabio Massimo Zanzotto,Leonardo Ranaldi*

Main category: cs.CL

Relevance: 85.0

TL;DR: 提出抽象引导推理框架，通过分离结构推理与词汇语义来减少LLMs在演绎推理中的内容效应偏差，使用抽象器预测对齐表示并通过多层干预增强形式推理的鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型在演绎推理中存在内容效应，即混淆语义合理性与形式有效性，即使生成逐步解释时中间推理也会继承相同的语义捷径。现有方法通过增加推理时结构约束来缓解，但可靠抑制语义干扰仍是挑战。

Method: 构建配对的内容丰富和抽象三段论，利用模型在抽象输入上的激活定义抽象推理空间。学习轻量级抽象器，从内容条件残差流状态预测与该空间对齐的表示，并通过前向传播中的多层干预整合这些预测。

Result: 使用跨语言迁移作为测试平台，显示抽象对齐的引导减少了内容驱动的错误，提高了有效性敏感性能。激活级抽象作为可扩展机制增强了LLMs形式推理对语义干扰的鲁棒性。

Conclusion: 激活级抽象是增强LLMs形式推理鲁棒性的有效机制，能够可靠抑制语义干扰，提高演绎判断的准确性。

Abstract: Large Language Models (LLMs) often struggle with deductive judgment in syllogistic reasoning, systematically conflating semantic plausibility with formal validity a phenomenon known as content effect. This bias persists even when models generate step-wise explanations, indicating that intermediate rationales may inherit the same semantic shortcuts that affect answers. Recent approaches propose mitigating this issue by increasing inference-time structural constraints, either by encouraging abstract intermediate representations or by intervening directly in the model's internal computations; however, reliably suppressing semantic interference remains an open challenge. To make formal deduction less sensitive to semantic content, we introduce a framework for abstraction-guided reasoning that explicitly separates structural inference from lexical semantics. We construct paired content-laden and abstract syllogisms and use the model's activations on abstract inputs to define an abstract reasoning space. We then learn lightweight Abstractors that, from content-conditioned residual-stream states, predict representations aligned with this space and integrate these predictions via multi-layer interventions during the forward pass. Using cross-lingual transfer as a test bed, we show that abstraction-aligned steering reduces content-driven errors and improves validity-sensitive performance. Our results position activation-level abstraction as a scalable mechanism for enhancing the robustness of formal reasoning in LLMs against semantic interference.

</details>


### [104] [From Directions to Regions: Decomposing Activations in Language Models via Local Geometry](https://arxiv.org/abs/2602.02464)
*Or Shafran,Shaked Ronen,Omri Fahn,Shauli Ravfogel,Atticus Geiger,Mor Geva*

Main category: cs.CL

Relevance: 85.0

TL;DR: 本文提出使用混合因子分析器(MFA)作为语言模型激活分解的新方法，通过建模激活空间中的局部高斯区域来捕捉非线性概念结构，相比现有线性方法能更好地发现复杂概念并实现模型控制。


<details>
  <summary>Details</summary>
Motivation: 现有语言模型激活分解方法基于几何假设，通常搜索全局方向并隐含线性可分性假设，这忽略了具有非线性或多维结构的概念。需要一种能捕捉复杂激活空间结构的可扩展无监督方法。

Method: 采用混合因子分析器(MFA)作为可扩展的无监督替代方案，将激活空间建模为具有局部协方差结构的高斯区域集合。MFA将激活分解为两个组合几何对象：区域在激活空间中的质心，以及从质心的局部变化。

Result: 在Llama-3.1-8B和Gemma-2-2B上训练大规模MFA，显示其能捕捉激活空间中的复杂非线性结构。在定位和引导基准测试中，MFA优于无监督基线，与有监督定位方法竞争，并且通常比稀疏自编码器获得更强的引导性能。

Conclusion: 局部几何（通过子空间表达）作为可扩展概念发现和模型控制的有前景分析单元，能够捕捉孤立方向无法捕获的复杂结构。

Abstract: Activation decomposition methods in language models are tightly coupled to geometric assumptions on how concepts are realized in activation space. Existing approaches search for individual global directions, implicitly assuming linear separability, which overlooks concepts with nonlinear or multi-dimensional structure. In this work, we leverage Mixture of Factor Analyzers (MFA) as a scalable, unsupervised alternative that models the activation space as a collection of Gaussian regions with their local covariance structure. MFA decomposes activations into two compositional geometric objects: the region's centroid in activation space, and the local variation from the centroid. We train large-scale MFAs for Llama-3.1-8B and Gemma-2-2B, and show they capture complex, nonlinear structures in activation space. Moreover, evaluations on localization and steering benchmarks show that MFA outperforms unsupervised baselines, is competitive with supervised localization methods, and often achieves stronger steering performance than sparse autoencoders. Together, our findings position local geometry, expressed through subspaces, as a promising unit of analysis for scalable concept discovery and model control, accounting for complex structures that isolated directions fail to capture.

</details>


### [105] [Indications of Belief-Guided Agency and Meta-Cognitive Monitoring in Large Language Models](https://arxiv.org/abs/2602.02467)
*Noam Steinmetz Yalon,Ariel Goldstein,Liad Mudrik,Mor Geva*

Main category: cs.CL

Relevance: 85.0

TL;DR: 该研究评估了LLMs中的意识指标HOT-3，通过分析信念形成与行动选择的关系，发现LLMs存在信念引导的代理和元认知监控能力。


<details>
  <summary>Details</summary>
Motivation: 随着LLMs的快速发展，研究者开始探讨这些模型是否具有某种形式的意识。本研究基于Butlin等人提出的意识指标框架，旨在实证评估LLMs是否具备信念引导的代理能力和元认知监控功能。

Method: 将信念视为模型潜在空间中的表征，引入量化信念在生成过程中主导性的指标。通过分析不同模型和任务中竞争信念的动态关系，研究外部操作如何影响内部信念形成，以及信念形成如何因果驱动行动选择。

Result: 发现三个关键结果：1）外部操作能系统性地调节内部信念形成；2）信念形成因果驱动模型的行动选择；3）模型能够监控并报告自身的信念状态。这些结果为LLMs存在信念引导的代理和元认知监控提供了实证支持。

Conclusion: 该研究为探索LLMs中代理、信念和元认知能力的涌现提供了方法论基础，表明LLMs可能具备意识指标HOT-3所描述的特征，对理解LLMs的认知能力有重要意义。

Abstract: Rapid advancements in large language models (LLMs) have sparked the question whether these models possess some form of consciousness. To tackle this challenge, Butlin et al. (2023) introduced a list of indicators for consciousness in artificial systems based on neuroscientific theories. In this work, we evaluate a key indicator from this list, called HOT-3, which tests for agency guided by a general belief-formation and action selection system that updates beliefs based on meta-cognitive monitoring. We view beliefs as representations in the model's latent space that emerge in response to a given input, and introduce a metric to quantify their dominance during generation. Analyzing the dynamics between competing beliefs across models and tasks reveals three key findings: (1) external manipulations systematically modulate internal belief formation, (2) belief formation causally drives the model's action selection, and (3) models can monitor and report their own belief states. Together, these results provide empirical support for the existence of belief-guided agency and meta-cognitive monitoring in LLMs. More broadly, our work lays methodological groundwork for investigating the emergence of agency, beliefs, and meta-cognition in LLMs.

</details>


### [106] [MemSkill: Learning and Evolving Memory Skills for Self-Evolving Agents](https://arxiv.org/abs/2602.02474)
*Haozhen Zhang,Quanyu Long,Jianzhu Bao,Tao Feng,Weizhi Zhang,Haodong Yue,Wenya Wang*

Main category: cs.CL

Relevance: 85.0

TL;DR: MemSkill：将LLM代理内存操作重构为可学习和可演化的内存技能，通过控制器、执行器和设计器形成闭环系统，提升任务性能并适应不同交互模式


<details>
  <summary>Details</summary>
Motivation: 现有LLM代理内存系统依赖静态、人工设计的操作来提取记忆，这些固定程序硬编码了人类先验知识，导致在不同交互模式下缺乏灵活性，且在长历史记录上效率低下

Method: 将内存操作重构为可学习和可演化的内存技能，包含三个组件：控制器学习选择相关技能，LLM执行器生成技能引导的记忆，设计器定期审查困难案例并演化技能集（提出改进和新技能）

Result: 在LoCoMo、LongMemEval、HotpotQA和ALFWorld等基准测试中，MemSkill优于强基线方法，表现出良好的跨设置泛化能力

Conclusion: MemSkill通过将内存操作重构为可学习和可演化的技能，实现了更自适应、自演化的LLM代理内存管理，为LLM代理的长期记忆系统提供了新思路

Abstract: Most Large Language Model (LLM) agent memory systems rely on a small set of static, hand-designed operations for extracting memory. These fixed procedures hard-code human priors about what to store and how to revise memory, making them rigid under diverse interaction patterns and inefficient on long histories. To this end, we present \textbf{MemSkill}, which reframes these operations as learnable and evolvable memory skills, structured and reusable routines for extracting, consolidating, and pruning information from interaction traces. Inspired by the design philosophy of agent skills, MemSkill employs a \emph{controller} that learns to select a small set of relevant skills, paired with an LLM-based \emph{executor} that produces skill-guided memories. Beyond learning skill selection, MemSkill introduces a \emph{designer} that periodically reviews hard cases where selected skills yield incorrect or incomplete memories, and evolves the skill set by proposing refinements and new skills. Together, MemSkill forms a closed-loop procedure that improves both the skill-selection policy and the skill set itself. Experiments on LoCoMo, LongMemEval, HotpotQA, and ALFWorld demonstrate that MemSkill improves task performance over strong baselines and generalizes well across settings. Further analyses shed light on how skills evolve, offering insights toward more adaptive, self-evolving memory management for LLM agents.

</details>


### [107] [Training LLMs for Divide-and-Conquer Reasoning Elevates Test-Time Scalability](https://arxiv.org/abs/2602.02477)
*Xiao Liang,Zhong-Zhi Li,Zhenghao Lin,Eric Hancheng Jiang,Hengyuan Zhang,Yelong Shen,Kai-Wei Chang,Ying Nian Wu,Yeyun Gong,Weizhu Chen*

Main category: cs.CL

Relevance: 85.0

TL;DR: 本文提出了一种基于强化学习的端到端框架，用于增强大语言模型的分治推理能力，在竞赛级基准测试中显著超越了链式思维推理方法。


<details>
  <summary>Details</summary>
Motivation: 链式思维推理在模型能力极限时往往不足，且其严格的顺序性限制了测试时的可扩展性。分治推理虽然能分解复杂问题，但通用后训练与分治推理风格之间存在根本性不匹配，限制了模型充分发挥这种潜力。

Method: 提出了一个端到端的强化学习框架来增强LLMs的分治推理能力。在每一步中，策略将问题分解为一组子问题，顺序解决它们，然后基于子问题解决方案处理原始问题，将分解和解决方案都整合到RL训练中。

Result: 在可比训练条件下，分治推理框架赋予模型更高的性能上限和更强的测试时扩展性，在竞赛级基准测试中，Pass@1超过CoT 8.6%，Pass@32超过6.3%。

Conclusion: 通过强化学习训练的分治推理框架能够有效解锁LLMs在最具挑战性任务上的推理能力，提供比传统链式思维推理更好的性能和可扩展性。

Abstract: Large language models (LLMs) have demonstrated strong reasoning capabilities through step-by-step chain-of-thought (CoT) reasoning. Nevertheless, at the limits of model capability, CoT often proves insufficient, and its strictly sequential nature constrains test-time scalability. A potential alternative is divide-and-conquer (DAC) reasoning, which decomposes a complex problem into subproblems to facilitate more effective exploration of the solution. Although promising, our analysis reveals a fundamental misalignment between general-purpose post-training and DAC-style inference, which limits the model's capacity to fully leverage this potential. To bridge this gap and fully unlock LLMs' reasoning capabilities on the most challenging tasks, we propose an end-to-end reinforcement learning (RL) framework to enhance their DAC-style reasoning capacity. At each step, the policy decomposes a problem into a group of subproblems, solves them sequentially, and addresses the original one conditioned on the subproblem solutions, with both decomposition and solution integrated into RL training. Under comparable training, our DAC-style framework endows the model with a higher performance ceiling and stronger test-time scalability, surpassing CoT by 8.6% in Pass@1 and 6.3% in Pass@32 on competition-level benchmarks.

</details>


### [108] [RE-TRAC: REcursive TRAjectory Compression for Deep Search Agents](https://arxiv.org/abs/2602.02486)
*Jialiang Zhu,Gongrui Zhang,Xiaolong Ma,Lin Xu,Miaosen Zhang,Ruiqi Yang,Song Wang,Kai Qiu,Zhirong Wu,Qi Dai,Ruichun Ma,Bei Liu,Yifan Yang,Chong Luo,Zhengyuan Yang,Linjie Li,Lijuan Wang,Weizhu Chen,Xin Geng,Baining Guo*

Main category: cs.CL

Relevance: 85.0

TL;DR: Re-TRAC：基于交叉轨迹探索的LLM研究代理框架，通过结构化状态表示实现迭代反思和全局规划，相比ReAct在BrowseComp上提升15-20%


<details>
  <summary>Details</summary>
Motivation: 现有基于ReAct框架的LLM研究代理存在线性设计的局限性：难以回溯早期状态、分支探索不同方向、在长上下文中保持全局意识，导致局部最优、冗余探索和低效搜索

Method: 提出Re-TRAC框架，在每条轨迹后生成结构化状态表示（总结证据、不确定性、失败和未来计划），并基于此状态表示指导后续轨迹，实现交叉轨迹探索、迭代反思和全局规划

Result: 在BrowseComp基准上，Re-TRAC相比ReAct一致提升15-20%；对小模型引入Re-TRAC感知的监督微调，在可比规模上达到SOTA；工具调用和token使用随轮次单调减少，表明探索更具针对性

Conclusion: Re-TRAC通过交叉轨迹探索和结构化状态表示，将研究重构为渐进过程，解决了ReAct框架的线性限制，实现了更高效、更全局的研究代理

Abstract: LLM-based deep research agents are largely built on the ReAct framework. This linear design makes it difficult to revisit earlier states, branch into alternative search directions, or maintain global awareness under long contexts, often leading to local optima, redundant exploration, and inefficient search. We propose Re-TRAC, an agentic framework that performs cross-trajectory exploration by generating a structured state representation after each trajectory to summarize evidence, uncertainties, failures, and future plans, and conditioning subsequent trajectories on this state representation. This enables iterative reflection and globally informed planning, reframing research as a progressive process. Empirical results show that Re-TRAC consistently outperforms ReAct by 15-20% on BrowseComp with frontier LLMs. For smaller models, we introduce Re-TRAC-aware supervised fine-tuning, achieving state-of-the-art performance at comparable scales. Notably, Re-TRAC shows a monotonic reduction in tool calls and token usage across rounds, indicating progressively targeted exploration driven by cross-trajectory reflection rather than redundant search.

</details>


### [109] [Reward-free Alignment for Conflicting Objectives](https://arxiv.org/abs/2602.02495)
*Peter Chen,Xiaopeng Li,Xi Chen,Tianyi Lin*

Main category: cs.CL

Relevance: 85.0

TL;DR: 本文提出RACO框架，一种无需奖励模型的多目标对齐方法，通过冲突规避梯度下降解决LLM对齐中的目标冲突问题，在摘要和安全对齐任务中实现更好的帕累托权衡。


<details>
  <summary>Details</summary>
Motivation: 现实世界中的LLM对齐问题常涉及多个冲突目标，传统加权损失方法无法同时改进所有目标，现有多目标方法依赖奖励模型，增加了复杂性并可能扭曲用户偏好。

Method: 提出RACO框架，直接利用成对偏好数据，通过新型裁剪版冲突规避梯度下降解决梯度冲突，提供收敛到帕累托临界点的理论保证，并加入启发式改进。

Result: 在多目标摘要和安全对齐任务上，对Qwen 3、Llama 3、Gemma 3等LLM家族的定性和定量评估显示，该方法相比现有基线能实现更好的帕累托权衡。

Conclusion: RACO框架为多目标LLM对齐提供了一种无需奖励模型的有效解决方案，能更好地处理冲突目标并尊重用户指定的权重偏好。

Abstract: Direct alignment methods are increasingly used to align large language models (LLMs) with human preferences. However, many real-world alignment problems involve multiple conflicting objectives, where naive aggregation of preferences can lead to unstable training and poor trade-offs. In particular, weighted loss methods may fail to identify update directions that simultaneously improve all objectives, and existing multi-objective approaches often rely on explicit reward models, introducing additional complexity and distorting user-specified preferences. The contributions of this paper are two-fold. First, we propose a Reward-free Alignment framework for Conflicted Objectives (RACO) that directly leverages pairwise preference data and resolves gradient conflicts via a novel clipped variant of conflict-averse gradient descent. We provide convergence guarantees to Pareto-critical points that respect user-specified objective weights, and further show that clipping can strictly improve convergence rate in the two-objective setting. Second, we improve our method using some heuristics and conduct experiments to demonstrate the compatibility of the proposed framework for LLM alignment. Both qualitative and quantitative evaluations on multi-objective summarization and safety alignment tasks across multiple LLM families (Qwen 3, Llama 3, Gemma 3) show that our method consistently achieves better Pareto trade-offs compared to existing multi-objective alignment baselines.

</details>


### [110] [C$^2$-Cite: Contextual-Aware Citation Generation for Attributed Large Language Models](https://arxiv.org/abs/2602.00004)
*Yue Yu,Ting Bai,HengZhi Lan,Li Qian,Li Peng,Jie Wu,Wei Liu,Jian Luan,Chuan Shi*

Main category: cs.IR

Relevance: 85.0

TL;DR: 提出C²-Cite框架，通过上下文引用对齐机制改进LLM的引用生成质量，将引用标记从通用占位符转变为主动知识指针。


<details>
  <summary>Details</summary>
Motivation: 现有指令调优的归因LLM在生成文本时无法正确解释引用符号的上下文语义，导致引用脱节和检索知识整合不佳。这是因为它们对引用标记周围上下文信息的意识不足。

Method: 提出上下文感知引用生成框架C²-Cite，采用上下文引用对齐机制：首先将检索到的文档上下文编码到引用符号表示中，然后通过解码引用路由器函数的信息来对齐标记编号。

Result: 在ALCE基准测试的三个数据集上，C²-Cite++在引用质量上平均比SOTA基线提升5.8%，在响应正确性上提升17.4%。

Conclusion: C²-Cite框架成功将引用标记从通用占位符转变为主动知识指针，显著提升了LLM的引用生成质量和响应正确性。

Abstract: The attribution technique enhances the credibility of LLMs by adding citations to the generated sentences, enabling users to trace back to the original sources and verify the reliability of the output. However, existing instruction-tuned attributed LLMs often fail to properly interpret the contextual semantics of citation symbols (e.g., [i]) during text generation. This shortcoming arises from their insufficient awareness of the context information surrounding citation markers, which in turn leads to disjointed references and poor integration of retrieved knowledge into the generated content. To address this issue, we propose a novel \textbf{C}ontextual-aware \textbf{C}itation generation framework (\textbf{C$^2$}-\textbf{Cite}) that explicitly integrates the semantic relationships between citation markers and their referenced content. Specifically, a contextual citation alignment mechanism is adopted: it first encodes the retrieved document contexts into the symbol representation of citations, then aligns the marker numbers by decoding information from a citation router function. This mechanism enables the transformation of citation markers from generic placeholders into active knowledge pointers that link to the referenced source information. Experimental results on the ALCE benchmark across three datasets validate our framework C$^2$-Cite++: it outperforms the SOTA baseline by an average of 5.8\% in citation quality and 17.4\% in response correctness. The implementation is publicly available at https://github.com/BAI-LAB/c2cite

</details>


### [111] [IntentCoding: Amplifying User Intent in Code Generation](https://arxiv.org/abs/2602.00066)
*Zheng Fang,Yihong Dong,Lili Mou,Dongming Jin,Zhi Jin,Ge Li*

Main category: cs.SE

Relevance: 85.0

TL;DR: IntentCoding：一种新的解码策略，通过意图放大机制增强LLM在代码生成中遵循用户多约束意图的能力，无需额外训练，在多个基准上显著提升约束满足和功能正确性。


<details>
  <summary>Details</summary>
Motivation: LLM在代码生成中虽然表现出色，但在遵循包含多个约束的细粒度用户意图方面仍面临挑战。研究发现：1）随着约束数量增加，模型性能快速下降；2）用户意图对模型logits的影响不足以有效引导解码过程。

Method: 提出IntentCoding解码策略：通过掩码用户意图来捕获其影响，然后应用多强度集成机制在生成过程中放大用户意图的效果。该方法与模型无关，无需额外训练，可无缝集成到现有解码流程中。

Result: 在构建的CodeConstraints基准以及IFEvalCode、HumanEval和LiveCodeBench数据集上，IntentCoding相比标准解码方法显著提升了约束满足和功能正确性。在CodeConstraints上相对提升达71.0%，IFEvalCode上达67.3%，HumanEval和LiveCodeBench的pass@1相对提升达29.3%。

Conclusion: IntentCoding是一种有效的解码策略，能够增强LLM在代码生成中遵循多约束用户意图的能力，且具有模型无关、无需训练、易于集成的优点。

Abstract: Large Language Models (LLMs) have shown strong capabilities in code generation, but their adherence to fine-grained user intent with multiple constraints remains a significant challenge. Our empirical analysis reveals two key observations: 1) Model performance deteriorates quickly as the number of constraints in the user intent increases, and 2) While user intent does influence the model's logits, such an influence may not be strong enough to effectively steer the decoding process. To this end, we propose Intent-Amplified Code Generation (IntentCoding), a novel decoding strategy that enhances an LLM's ability to follow user intent. IntentCoding captures the influence of user intent by masking out the intent, and applies a multi-strength ensemble mechanism to amplify the effect of user intent during generation. IntentCoding is model-agnostic, requires no additional training, and integrates seamlessly with existing decoding procedures. To enable systematic evaluation, we also construct CodeConstraints, a benchmark dataset specifically designed to test user intent compliance under varying numbers of constraints. Experiments on our constructed Constraints, as well as popular IFEvalCode, HumanEval and LiveCodeBench datasets, show that our IntentCoding model significantly improves both constraint satisfaction and functional correctness compared to standard decoding approaches. IntentCoding achieves up to 71.0% relative improvement on CodeConstraints, achieves up to 67.3% relative improvement on IFEvalCode and achieves up to 29.3% relative improvement in pass@1 on HumanEval and LiveCodeBench compared with greedy decoding.

</details>


### [112] [FoundationalASSIST: An Educational Dataset for Foundational Knowledge Tracing and Pedagogical Grounding of LLMs](https://arxiv.org/abs/2602.00070)
*Eamon Worden,Cristina Heffernan,Neil Heffernan,Shashank Sonkar*

Main category: cs.CY

Relevance: 85.0

TL;DR: 提出了FoundationalASSIST数据集，包含170万条学生互动数据，用于评估LLM在教育领域的理解能力。评估显示当前LLM在知识追踪和教学基础任务上表现不佳，远未达到可靠个性化学习的要求。


<details>
  <summary>Details</summary>
Motivation: 现有教育数据集仅提供问题标识符和二元正确性标签，对使用自然语言推理的LLM不透明。需要包含完整问题文本、学生实际回答、错误答案选择记录等信息的英语教育数据集，以研究LLM在教育中的应用。

Method: 构建FoundationalASSIST数据集（170万次互动，5000名学生），包含完整问题文本、学生回答、错误答案选择、K-12标准对齐。评估四个前沿LLM在知识追踪（预测学生表现和具体答案）和教学基础（评估题目有效性属性）两个任务上的表现。

Result: 所有模型在知识追踪任务上仅达到微不足道的基线水平；在题目区分度判断上低于随机猜测，表明LLM不理解什么使问题更具诊断性；在相对难度判断上表现较好（最高68.6%），但这突显了其他方面的差距。

Conclusion: 当前LLM在理解学生学习方面存在显著能力差距，需要实质性进展才能可靠支持大规模个性化学习。发布FoundationalASSIST数据集以支持这些基础挑战的研究。

Abstract: Can Large Language Models understand how students learn? As LLMs are deployed for adaptive testing and personalized tutoring, this question becomes urgent -- yet we cannot answer it with existing resources. Current educational datasets provide only question identifiers and binary correctness labels, rendering them opaque to LLMs that reason in natural language. We address this gap with FoundationalASSIST, the first English educational dataset providing the complete information needed for research on LLMs in education: full question text, actual student responses (not just right/wrong), records of which wrong answers students chose, and alignment to Common Core K-12 standards. These 1.7 million interactions from 5,000 students enable research directions that were previously impossible to pursue, from fine-tuning student models to analyzing misconception patterns. To demonstrate the dataset's utility, we evaluate four frontier models (GPT-OSS-120B, Llama-3.3-70B, Qwen3-Next-80B variants) on two complementary task families: Knowledge Tracing, testing whether LLMs can predict student performance on questions, and the exact answer a student will give; and \textbf{Pedagogical Grounding}, testing whether LLMs understand the properties that make assessment items effective. Our evaluation reveals significant gaps in current LLM capabilities. Every model barely achieves a trivial baseline on knowledge tracing. All models fall below random chance on item discrimination, indicating that LLMs do not understand what makes one problem more diagnostic than another. Models do show competence at judging relative difficulty (up to 68.6%), but this partial success only highlights the gaps elsewhere. These results establish that substantial advances are needed before LLMs can reliably support personalized learning at scale. We release FoundationalASSIST to support progress on these foundational challenges.

</details>


### [113] [A-MapReduce: Executing Wide Search via Agentic MapReduce](https://arxiv.org/abs/2602.01331)
*Mingju Chen,Guibin Zhang,Heng Chang,Yuchen Guo,Shiji Zhou*

Main category: cs.MA

Relevance: 85.0

TL;DR: A-MapReduce：基于MapReduce范式的多智能体执行框架，专门针对大规模广度搜索任务，通过并行处理和结构化聚合提升效率


<details>
  <summary>Details</summary>
Motivation: 现有基于LLM的多智能体系统在深度研究任务中表现出色，但在大规模广度搜索任务中效率低下，因为现有框架主要围绕顺序、垂直结构推理设计，不适合广度导向的检索

Method: 提出A-MapReduce框架，将广度搜索重构为水平结构检索问题，通过任务自适应分解和结构化结果聚合实现大规模检索目标的并行处理，利用经验记忆驱动查询条件化任务分配和重组的持续演化

Result: 在五个智能体基准测试中，A-MapReduce在WideSearch和DeepWideSearch上达到最先进性能，相比OpenAI o3或Gemini 2.5 Pro骨干的强基线，平均Item F1提升5.11%-17.50%；成本效益高，运行时间减少45.8%

Conclusion: A-MapReduce为大规模广度搜索任务提供了高效的多智能体执行框架，通过MapReduce范式实现了并行处理和持续改进，在性能和效率方面都有显著优势

Abstract: Contemporary large language model (LLM)-based multi-agent systems exhibit systematic advantages in deep research tasks, which emphasize iterative, vertically structured information seeking. However, when confronted with wide search tasks characterized by large-scale, breadth-oriented retrieval, existing agentic frameworks, primarily designed around sequential, vertically structured reasoning, remain stuck in expansive search objectives and inefficient long-horizon execution. To bridge this gap, we propose A-MapReduce, a MapReduce paradigm-inspired multi-agent execution framework that recasts wide search as a horizontally structured retrieval problem. Concretely, A-MapReduce implements parallel processing of massive retrieval targets through task-adaptive decomposition and structured result aggregation. Meanwhile, it leverages experiential memory to drive the continual evolution of query-conditioned task allocation and recomposition, enabling progressive improvement in large-scale wide-search regimes. Extensive experiments on five agentic benchmarks demonstrate that A-MapReduce is (i) high-performing, achieving state-of-the-art performance on WideSearch and DeepWideSearch, and delivering 5.11% - 17.50% average Item F1 improvements compared with strong baselines with OpenAI o3 or Gemini 2.5 Pro backbones; (ii) cost-effective and efficient, delivering superior cost-performance trade-offs and reducing running time by 45.8\% compared to representative multi-agent baselines. The code is available at https://github.com/mingju-c/AMapReduce.

</details>


### [114] [Expected Harm: Rethinking Safety Evaluation of (Mis)Aligned LLMs](https://arxiv.org/abs/2602.01600)
*Yen-Shan Chen,Zhi Rui Tam,Cheng-Kuang Wu,Yun-Nung Chen*

Main category: cs.CR

Relevance: 85.0

TL;DR: 论文提出Expected Harm指标，结合恶意查询的严重性和执行可能性，揭示LLM存在系统性逆风险校准问题：模型对低可能性（高成本）威胁过度拒绝，而对高可能性（低成本）查询保持脆弱。


<details>
  <summary>Details</summary>
Motivation: 当前LLM安全评估主要依赖基于严重性的分类法，假设所有恶意查询风险均匀，忽略了执行可能性（威胁在模型响应后实现的条件概率）。需要重新审视这一评估框架。

Method: 引入Expected Harm指标，将越狱严重性按执行可能性加权，执行可能性建模为执行成本的函数。通过实证分析最先进模型，使用线性探测技术追溯失败根源。

Result: 发现系统性逆风险校准现象：模型对低可能性（高成本）威胁表现出过强的拒绝行为，而对高可能性（低成本）查询保持脆弱。利用这一特性可将现有越狱攻击成功率提升至2倍。线性探测显示模型在潜在空间中编码严重性以驱动拒绝决策，但没有对执行成本的可区分内部表示。

Conclusion: LLM安全评估需要超越严重性分类法，考虑执行可能性维度。模型缺乏对执行成本的内部表示导致结构性脆弱性，需要新的安全训练和评估方法。

Abstract: Current evaluations of LLM safety predominantly rely on severity-based taxonomies to assess the harmfulness of malicious queries. We argue that this formulation requires re-examination as it assumes uniform risk across all malicious queries, neglecting Execution Likelihood--the conditional probability of a threat being realized given the model's response. In this work, we introduce Expected Harm, a metric that weights the severity of a jailbreak by its execution likelihood, modeled as a function of execution cost. Through empirical analysis of state-of-the-art models, we reveal a systematic Inverse Risk Calibration: models disproportionately exhibit stronger refusal behaviors for low-likelihood (high-cost) threats while remaining vulnerable to high-likelihood (low-cost) queries. We demonstrate that this miscalibration creates a structural vulnerability: by exploiting this property, we increase the attack success rate of existing jailbreaks by up to $2\times$. Finally, we trace the root cause of this failure using linear probing, which reveals that while models encode severity in their latent space to drive refusal decisions, they possess no distinguishable internal representation of execution cost, making them "blind" to this critical dimension of risk.

</details>


### [115] [PPoGA: Predictive Plan-on-Graph with Action for Knowledge Graph Question Answering](https://arxiv.org/abs/2602.00007)
*MinGyu Jeon,SuWan Cho,JaeYoung Shu*

Main category: cs.CL

Relevance: 75.0

TL;DR: PPoGA是一个新颖的KGQA框架，通过预测性处理和自我纠正机制解决LLMs在知识图谱问答中因初始推理计划错误而失败的问题，实现了计划级别的纠正能力。


<details>
  <summary>Details</summary>
Motivation: 当前LLMs增强的知识图谱问答系统存在"认知功能固着"问题：当初始高层推理计划错误时，系统无法重新构建方法，导致追求不可行的解决方案。这限制了AI推理系统的鲁棒性和灵活性。

Method: 提出PPoGA框架，采用Planner-Executor架构分离高层策略和低层执行，引入预测性处理机制预测结果。核心创新是自我纠正机制，不仅支持路径纠正（局部执行错误），还支持计划纠正（识别、丢弃和重新制定整个计划）。

Result: 在三个具有挑战性的多跳KGQA基准测试（GrailQA、CWQ、WebQSP）上进行广泛实验，PPoGA实现了最先进的性能，显著优于现有方法。

Conclusion: 研究表明元认知能力（如问题重构）对于构建更鲁棒和灵活的AI推理系统至关重要。PPoGA通过自我纠正机制有效解决了LLMs在复杂推理中的功能固着问题。

Abstract: Large Language Models (LLMs) augmented with Knowledge Graphs (KGs) have advanced complex question answering, yet they often remain susceptible to failure when their initial high-level reasoning plan is flawed. This limitation, analogous to cognitive functional fixedness, prevents agents from restructuring their approach, leading them to pursue unworkable solutions. To address this, we propose PPoGA (Predictive Plan-on-Graph with Action), a novel KGQA framework inspired by human cognitive control and problem-solving. PPoGA incorporates a Planner-Executor architecture to separate high-level strategy from low-level execution and leverages a Predictive Processing mechanism to anticipate outcomes. The core innovation of our work is a self-correction mechanism that empowers the agent to perform not only Path Correction for local execution errors but also Plan Correction by identifying, discarding, and reformulating the entire plan when it proves ineffective. We conduct extensive experiments on three challenging multi-hop KGQA benchmarks: GrailQA, CWQ, and WebQSP. The results demonstrate that PPoGA achieves state-of-the-art performance, significantly outperforming existing methods. Our work highlights the critical importance of metacognitive abilities like problem restructuring for building more robust and flexible AI reasoning systems.

</details>


### [116] [Unlocking Electronic Health Records: A Hybrid Graph RAG Approach to Safe Clinical AI for Patient QA](https://arxiv.org/abs/2602.00009)
*Samuel Thio,Matthew Lewis,Spiros Denaxas,Richard JB Dobson*

Main category: cs.CL

Relevance: 75.0

TL;DR: MediGRAF是一个混合图检索增强框架，结合结构化图查询和非结构化语义搜索，用于医疗记录的自然语言查询，在MIMIC-IV数据集上实现了100%的事实查询召回率和4.25/5的专家质量评分。


<details>
  <summary>Details</summary>
Motivation: 电子健康记录系统包含大量临床信息，给临床医生带来认知负担，关键细节容易被忽略。虽然LLMs在数据处理方面有潜力，但在临床环境中存在上下文基础和幻觉问题。现有解决方案通常孤立地使用结构化数据检索或非结构化语义搜索，未能同时整合两者。

Method: 提出MediGRAF（医学图检索增强框架），一个新颖的混合图RAG系统。独特地结合Neo4j Text2Cypher功能进行结构化关系遍历和向量嵌入进行非结构化叙述检索，实现对完整患者旅程的自然语言查询。使用MIMIC-IV数据集的10名患者（生成5,973个节点和5,963个关系）进行患者级别问答评估。

Result: 系统在事实查询上实现了100%的召回率（所有相关信息都被检索并输出），复杂推理任务的平均专家质量评分为4.25/5（满分5分），且零安全违规。这些结果表明混合图基础显著推进了临床信息检索。

Conclusion: 混合图基础为临床信息检索提供了更安全、更全面的替代方案，超越了标准LLM部署。该框架成功整合了结构化和非结构化数据检索，在医疗领域实现了高效准确的信息提取。

Abstract: Electronic health record (EHR) systems present clinicians with vast repositories of clinical information, creating a significant cognitive burden where critical details are easily overlooked. While Large Language Models (LLMs) offer transformative potential for data processing, they face significant limitations in clinical settings, particularly regarding context grounding and hallucinations. Current solutions typically isolate retrieval methods focusing either on structured data (SQL/Cypher) or unstructured semantic search but fail to integrate both simultaneously. This work presents MediGRAF (Medical Graph Retrieval Augmented Framework), a novel hybrid Graph RAG system that bridges this gap. By uniquely combining Neo4j Text2Cypher capabilities for structured relationship traversal with vector embeddings for unstructured narrative retrieval, MediGRAF enables natural language querying of the complete patient journey. Using 10 patients from the MIMIC-IV dataset (generating 5,973 nodes and 5,963 relationships), we generated enough nodes and data for patient level question answering (QA), and we evaluated this architecture across varying query complexities. The system demonstrated 100\% recall for factual queries which means all relevant information was retrieved and in the output, while complex inference tasks achieved a mean expert quality score of 4.25/5 with zero safety violations. These results demonstrate that hybrid graph-grounding significantly advances clinical information retrieval, offering a safer, more comprehensive alternative to standard LLM deployments.

</details>


### [117] [Reversible Diffusion Decoding for Diffusion Language Models](https://arxiv.org/abs/2602.00150)
*Xinyun Wang,Min Zhang,Sen Cui,Zhikang Chen,Bo Jiang,Kun Kuang,Mingbao Lin*

Main category: cs.CL

Relevance: 75.0

TL;DR: 提出可逆扩散解码（RDD）框架，解决扩散语言模型并行生成中的停滞问题，通过可逆回溯和置信度引导重掩码提高生成鲁棒性


<details>
  <summary>Details</summary>
Motivation: 扩散语言模型通过块级解码实现并行token生成，但其不可逆的承诺机制可能导致停滞问题——当上下文不理想时，反向扩散过程无法继续进展。现有方法缺乏从早期承诺错误中恢复的能力。

Method: 提出可逆扩散解码（RDD）框架：1）检测停滞作为反向过程的状态依赖失败；2）通过缓存的模型状态实现高效回溯到早期块，无需重新计算；3）应用置信度引导的重掩码，选择性重新初始化不确定token，同时保留可靠上下文。

Result: 实验表明RDD在最小计算开销下，相比基线方法显著提高了生成鲁棒性和质量，同时保持了扩散基生成的并行效率。

Conclusion: RDD为扩散语言模型提供了一种可逆解码框架，能够从早期承诺错误中恢复，解决了停滞问题，同时保持了并行生成效率，为扩散模型的可靠解码提供了新思路。

Abstract: Diffusion language models enable parallel token generation through block-wise decoding, but their irreversible commitments can lead to stagnation, where the reverse diffusion process fails to make further progress under a suboptimal context.We propose Reversible Diffusion Decoding (RDD), a decoding framework that introduces reversibility into block-wise diffusion generation. RDD detects stagnation as a state-dependent failure of the reverse process and enables efficient backtracking to earlier blocks without recomputation via cached model states. To avoid repeated failure trajectories, RDD applies confidence-guided re-masking to selectively reinitialize uncertain tokens while preserving reliable context.This reversible formulation allows decoding to recover from early commitment errors while maintaining the parallel efficiency of diffusion-based generation. Experiments show that RDD improves generation robustness and quality over baselines with minimal computational overhead.

</details>


### [118] [Detecting AI-Generated Content in Academic Peer Reviews](https://arxiv.org/abs/2602.00319)
*Siyuan Shen,Kai Wang*

Main category: cs.CL

Relevance: 75.0

TL;DR: 该研究分析了AI生成内容在学术同行评审中的时间演变，发现在2022年前检测极少，到2025年ICLR约20%、Nature Communications约12%的评审被分类为AI生成，表明AI辅助内容在同行评审中快速增加。


<details>
  <summary>Details</summary>
Motivation: 随着大型语言模型的普及，其在学术同行评审中的作用引发关注。研究旨在量化AI生成内容在同行评审中的时间演变，了解AI在学术评价中的渗透程度。

Method: 使用在历史评审数据上训练的检测模型，应用于后续评审周期（ICLR和Nature Communications）。通过时间序列分析，追踪AI生成内容在不同时间点的出现频率。

Result: 2022年前AI生成内容检测极少，之后显著增加。到2025年，ICLR约20%、Nature Communications约12%的评审被分类为AI生成。Nature Communications在2024年第三到第四季度增长最明显。

Conclusion: AI辅助内容在同行评审中快速增加，需要进一步研究其对学术评价的影响。这凸显了学术界需要关注AI在评审过程中的使用及其潜在影响。

Abstract: The growing availability of large language models (LLMs) has raised questions about their role in academic peer review. This study examines the temporal emergence of AI-generated content in peer reviews by applying a detection model trained on historical reviews to later review cycles at International Conference on Learning Representations (ICLR) and Nature Communications (NC). We observe minimal detection of AI-generated content before 2022, followed by a substantial increase through 2025, with approximately 20% of ICLR reviews and 12% of Nature Communications reviews classified as AI-generated in 2025. The most pronounced growth of AI-generated reviews in NC occurs between the third and fourth quarter of 2024. Together, these findings provide suggestive evidence of a rapidly increasing presence of AI-assisted content in peer review and highlight the need for further study of its implications for scholarly evaluation.

</details>


### [119] [DETOUR: An Interactive Benchmark for Dual-Agent Search and Reasoning](https://arxiv.org/abs/2602.00352)
*Li Siyan,Darshan Deshpande,Anand Kannappan,Rebecca Qian*

Main category: cs.CL

Relevance: 75.0

TL;DR: DETOUR是一个双智能体评估基准，用于模拟人类"舌尖现象"的多轮信息检索过程，包含1011个提示，测试模型在信息不完整情况下的检索能力。


<details>
  <summary>Details</summary>
Motivation: 现有评估智能体在"舌尖现象"搜索过程中的基准仅限于单轮设置，无法真实模拟人类多轮回忆过程。需要更现实的评估方法来测试智能体在信息不完整、模糊描述情况下的检索能力。

Method: 提出DETOUR双智能体评估框架：包含一个被评估的主智能体（Primary Agent）和一个记忆智能体（Memory Agent）。主智能体通过多轮查询记忆智能体来识别回忆的实体，记忆智能体在所有评估中保持一致。基准包含1011个提示，涵盖文本、图像、音频和视频多种模态。

Result: 当前最先进的模型在该基准上表现不佳，在所有模态（文本、图像、音频、视频）上仅达到36%的准确率，表明模型在信息不完整场景下的能力仍有待提升。

Conclusion: DETOUR基准揭示了当前LLM在模拟人类多轮回忆过程方面的局限性，强调了增强模型在模糊、不完整信息场景下检索能力的重要性。

Abstract: When recalling information in conversation, people often arrive at the recollection after multiple turns. However, existing benchmarks for evaluating agent capabilities in such tip-of-the-tongue search processes are restricted to single-turn settings. To more realistically simulate tip-of-the-tongue search, we introduce Dual-agent based Evaluation Through Obscure Under-specified Retrieval (DETOUR), a dual-agent evaluation benchmark containing 1,011 prompts. The benchmark design involves a Primary Agent, which is the subject of evaluation, tasked with identifying the recollected entity through querying a Memory Agent that is held consistent across evaluations. Our results indicate that current state-of-the-art models still struggle with our benchmark, only achieving 36% accuracy when evaluated on all modalities (text, image, audio, and video), highlighting the importance of enhancing capabilities in underspecified scenarios.

</details>


### [120] [Culturally-Grounded Governance for Multilingual Language Models: Rights, Data Boundaries, and Accountable AI Design](https://arxiv.org/abs/2602.00497)
*Hanjing Shi,Dominic DiFranzo*

Main category: cs.CL

Relevance: 75.0

TL;DR: 本文提出一个文化基础的治理框架，针对多语言大语言模型（MLLMs）在跨文化部署中的系统性风险，强调从数据、评估到问责机制的文化适应性治理。


<details>
  <summary>Details</summary>
Motivation: 当前多语言大语言模型的治理框架主要基于英语中心的数据、同质化用户群体和抽象公平概念，这导致对低资源语言和文化边缘化社区的系统性风险。模型行为、数据实践和问责机制往往与当地规范、权利和期望不符。

Method: 基于人机交互和AI治理的跨文化视角，综合现有关于多语言模型行为、数据不对称和社会技术危害的证据，构建一个文化基础的治理框架。识别三个相互关联的治理挑战：训练数据和评估实践中的文化与语言不平等、全球部署与本地规范/价值观/权力结构的不匹配、针对边缘化语言社区危害的有限问责机制。

Result: 提出了一个概念性议程，将多语言AI治理重新定义为社会文化和基于权利的问题。为数据管理、透明度和参与式问责机制提供了设计和政策建议。

Conclusion: 文化基础的治理对于确保多语言语言模型不会在规模和中性化的幌子下复制现有的全球不平等至关重要。需要从技术基准转向关注社会文化适应性和权利保护的治理框架。

Abstract: Multilingual large language models (MLLMs) are increasingly deployed across cultural, linguistic, and political contexts, yet existing governance frameworks largely assume English-centric data, homogeneous user populations, and abstract notions of fairness. This creates systematic risks for low-resource languages and culturally marginalized communities, where data practices, model behavior, and accountability mechanisms often fail to align with local norms, rights, and expectations. Drawing on cross-cultural perspectives in human-centered computing and AI governance, this paper synthesizes existing evidence on multilingual model behavior, data asymmetries, and sociotechnical harm, and articulates a culturally grounded governance framework for MLLMs. We identify three interrelated governance challenges: cultural and linguistic inequities in training data and evaluation practices, misalignment between global deployment and locally situated norms, values, and power structures, and limited accountability mechanisms for addressing harms experienced by marginalized language communities. Rather than proposing new technical benchmarks, we contribute a conceptual agenda that reframes multilingual AI governance as a sociocultural and rights based problem. We outline design and policy implications for data stewardship, transparency, and participatory accountability, and argue that culturally grounded governance is essential for ensuring that multilingual language models do not reproduce existing global inequalities under the guise of scale and neutrality.

</details>


### [121] [Reasoning by Commented Code for Table Question Answering](https://arxiv.org/abs/2602.00543)
*Seho Pyo,Jiheon Seok,Jaejin Lee*

Main category: cs.CL

Relevance: 75.0

TL;DR: 提出一个带注释的逐步代码生成框架，通过将表格问答分解为多行可执行程序并添加自然语言注释，提升推理清晰度和代码正确率，在WikiTableQuestions基准上取得显著改进。


<details>
  <summary>Details</summary>
Motivation: 传统表格线性化方法破坏了结构化数据的二维关系，现有端到端答案生成或单行程序查询方法在数值准确性和可解释性方面存在局限。

Method: 采用带注释的逐步代码生成框架，将表格问答推理分解为多行可执行Python程序，每行代码附带简洁自然语言注释，并与强大的端到端表格问答模型通过轻量级答案选择机制结合。

Result: 在WikiTableQuestions基准上，使用Qwen2.5-Coder-7B-Instruct达到70.9%准确率，优于Repanda基线（67.6%）。结合端到端模型后，准确率进一步提升至84.3%。

Conclusion: 带注释的逐步代码生成框架能有效提升表格问答的推理清晰度和准确性，通过显式推理和多行程序分解解决了传统方法的局限性。

Abstract: Table Question Answering (TableQA) poses a significant challenge for large language models (LLMs) because conventional linearization of tables often disrupts the two-dimensional relationships intrinsic to structured data. Existing methods, which depend on end-to-end answer generation or single-line program queries, typically exhibit limited numerical accuracy and reduced interpretability. This work introduces a commented, step-by-step code-generation framework that incorporates explicit reasoning into the Python program-generation process. The approach decomposes TableQA reasoning into multi-line executable programs with concise natural language comments, thereby promoting clearer reasoning and increasing the likelihood of generating correct code. On the WikiTableQuestions benchmark, the proposed method achieves 70.9\% accuracy using Qwen2.5-Coder-7B-Instruct, surpassing the Repanda baseline (67.6\%). Integrating the proposed framework with a robust end-to-end TableQA model via a lightweight answer-selection mechanism yields further improvements. This combined approach achieves up to 84.3\% accuracy on the WikiTableQuestions benchmark.

</details>


### [122] [A Hierarchical and Attentional Analysis of Argument Structure Constructions in BERT Using Naturalistic Corpora](https://arxiv.org/abs/2602.00554)
*Liu Kaipeng,Wu Ling*

Main category: cs.CL

Relevance: 75.0

TL;DR: 该研究使用多维分析框架探索BERT模型如何处理四种基本论元结构构式，揭示了层次化的表征结构


<details>
  <summary>Details</summary>
Motivation: 研究动机是理解BERT模型如何编码和处理论元结构构式这种语言结构，探索模型内部表征的层次组织方式

Method: 采用多维分析框架：MDS和t-SNE降维、广义判别值(GDV)作为聚类分离度量、Fisher判别比(FDR)进行线性诊断探测、注意力机制分析

Result: 结果揭示了层次化表征结构：构式特定信息在早期层出现，在中间层形成最大可分离聚类，并在后续处理阶段得以保持

Conclusion: BERT模型对论元结构构式的处理呈现层次化组织，构式信息在模型的不同层中逐步形成和保持

Abstract: This study investigates how the Bidirectional Encoder Representations from Transformers model processes four fundamental Argument Structure Constructions. We employ a multi-dimensional analytical framework, which integrates MDS, t-SNE as dimensionality reduction, Generalized Discrimination Value (GDV) as cluster separation metrics, Fisher Discriminant Ratio (FDR) as linear diagnostic probing, and attention mechanism analysis. Our results reveal a hierarchical representational structure. Construction-specific information emerges in early layers, forms maximally separable clusters in middle layers, and is maintained through later processing stages.

</details>


### [123] [Lookahead-then-Verify: Reliable Constrained Decoding for Diffusion LLMs under Context-Free Grammars](https://arxiv.org/abs/2602.00612)
*Yitong Zhang,Yongmin Li,Yuetong Liu,Jia Li,Xiaoran Jia,Zherui Li,Ge Li*

Main category: cs.CL

Relevance: 75.0

TL;DR: LAVE是一种专门为扩散大语言模型设计的约束解码方法，通过前瞻验证确保生成语法正确的形式语言，显著提升句法正确性且计算开销可忽略


<details>
  <summary>Details</summary>
Motivation: 扩散大语言模型在生成形式语言（如源代码、化学表达式）时难以可靠地生成语法正确的输出。现有约束解码方法要么不适用于非自回归的dLLMs，要么允许无法完成有效句子的中间输出，限制了实际可靠性。

Method: LAVE利用dLLMs在每次前向传播中并行预测所有位置token分布的特性，当模型提出新token时，使用这些分布进行前瞻验证，高效可靠地检查提议token的有效性，确保中间输出能够扩展为有效句子。

Result: 在四个广泛使用的dLLMs和三个代表性基准测试上的实验表明，LAVE始终优于现有基线方法，在句法正确性方面实现显著提升，同时引入可忽略的运行时开销。

Conclusion: LAVE为扩散大语言模型提供了一种有效的约束解码方法，解决了形式语言生成中的语法正确性问题，具有实际应用价值。

Abstract: Diffusion Large Language Models (dLLMs) have demonstrated promising generative capabilities and are increasingly used to produce formal languages defined by context-free grammars, such as source code and chemical expressions. However, as probabilistic models, they still struggle to generate syntactically valid outputs reliably. A natural and promising direction to address this issue is to adapt constrained decoding techniques to enforce grammatical correctness during generation. However, applying these techniques faces two primary obstacles. On the one hand, the non-autoregressive nature of dLLMs renders most existing constrained decoding approaches inapplicable. On the other hand, current approaches specifically designed for dLLMs may allow intermediate outputs that are impossible to complete into valid sentences, which significantly limits their reliability in practice.
  To address these challenges, we present LAVE, a constrained decoding approach specifically designed for dLLMs. Our approach leverages a key property of dLLMs, namely their ability to predict token distributions for all positions in parallel during each forward pass. Whenever a new token is proposed by model, LAVE performs lookahead using these distributions to efficiently and reliably verify the validity of the proposed token. This design ensures reliable constraints by reliably preserving the potential for intermediate outputs to be extended into valid sentences. Extensive experiments across four widely used dLLMs and three representative benchmarks demonstrate that LAVE consistently outperforms existing baselines and achieves substantial improvements in syntactic correctness, while incurring negligible runtime overhead.

</details>


### [124] [Formal Semantic Control over Language Models](https://arxiv.org/abs/2602.00638)
*Yingji Zhang*

Main category: cs.CL

Relevance: 75.0

TL;DR: 该论文通过变分自编码器框架，在句子层面和推理层面实现语义表征学习，旨在提升语言模型潜在空间的几何可解释性和可控性。


<details>
  <summary>Details</summary>
Motivation: 当前语言模型的潜在空间缺乏语义和几何可解释性，难以实现局部化、准符号化的组合控制。论文旨在通过塑造潜在空间的几何结构，使语言表示或模型更具可解释性，并实现精确的结构化控制。

Method: 采用VAE框架，从两个互补方向研究：1) 句子级学习与控制：在潜在空间中解耦和操作特定语义特征以指导句子生成；2) 推理级学习与控制：在潜在空间中隔离和引导推理行为以控制自然语言推理(NLI)。特别关注解释性NLI任务。

Result: 论文提出了一套新颖的理论框架和实践方法，通过实验证明这些方法能够增强自然语言潜在空间在句子和推理两个层面的可解释性和可控性。

Conclusion: 该研究推动了语言模型内部语义表征向系统可解释、精确结构化和可靠可控的方向发展，为实现更透明、可控的语言模型奠定了基础。

Abstract: This thesis advances semantic representation learning to render language representations or models more semantically and geometrically interpretable, and to enable localised, quasi-symbolic, compositional control through deliberate shaping of their latent space geometry. We pursue this goal within a VAE framework, exploring two complementary research directions: (i) Sentence-level learning and control: disentangling and manipulating specific semantic features in the latent space to guide sentence generation, with explanatory text serving as the testbed; and (ii) Reasoning-level learning and control: isolating and steering inference behaviours in the latent space to control NLI. In this direction, we focus on Explanatory NLI tasks, in which two premises (explanations) are provided to infer a conclusion. The overarching objective is to move toward language models whose internal semantic representations can be systematically interpreted, precisely structured, and reliably directed. We introduce a set of novel theoretical frameworks and practical methodologies, together with corresponding experiments, to demonstrate that our approaches enhance both the interpretability and controllability of latent spaces for natural language across the thesis.

</details>


### [125] [Can Small Language Models Handle Context-Summarized Multi-Turn Customer-Service QA? A Synthetic Data-Driven Comparative Evaluation](https://arxiv.org/abs/2602.00665)
*Lakshan Cooray,Deshan Sumanathilaka,Pattigadapa Venkatesh Raju*

Main category: cs.CL

Relevance: 75.0

TL;DR: 该研究探讨了指令微调的小型语言模型在客户服务多轮问答中的应用，通过历史摘要策略保持对话连续性，并与商业大语言模型进行对比评估。


<details>
  <summary>Details</summary>
Motivation: 大语言模型在客户服务问答中表现优异，但计算成本高且部署受限。小型语言模型虽然更高效，但在多轮客户服务问答中的有效性尚未充分探索，特别是在需要对话连续性和上下文理解的场景中。

Method: 使用历史摘要策略来保持对话状态，引入基于对话阶段的定性分析方法。评估了9个指令微调的低参数小型语言模型，并与3个商业大语言模型对比，使用词汇和语义相似度指标，以及人工评估和LLM-as-a-judge方法进行定性评估。

Result: 不同小型语言模型表现差异显著，部分模型展现出接近大语言模型的性能，而其他模型在保持对话连续性和上下文对齐方面存在困难。

Conclusion: 研究结果凸显了低参数语言模型在现实世界客户服务问答系统中的潜力和当前局限性，为资源受限环境下的部署提供了参考。

Abstract: Customer-service question answering (QA) systems increasingly rely on conversational language understanding. While Large Language Models (LLMs) achieve strong performance, their high computational cost and deployment constraints limit practical use in resource-constrained environments. Small Language Models (SLMs) provide a more efficient alternative, yet their effectiveness for multi-turn customer-service QA remains underexplored, particularly in scenarios requiring dialogue continuity and contextual understanding. This study investigates instruction-tuned SLMs for context-summarized multi-turn customer-service QA, using a history summarization strategy to preserve essential conversational state. We also introduce a conversation stage-based qualitative analysis to evaluate model behavior across different phases of customer-service interactions. Nine instruction-tuned low-parameterized SLMs are evaluated against three commercial LLMs using lexical and semantic similarity metrics alongside qualitative assessments, including human evaluation and LLM-as-a-judge methods. Results show notable variation across SLMs, with some models demonstrating near-LLM performance, while others struggle to maintain dialogue continuity and contextual alignment. These findings highlight both the potential and current limitations of low-parameterized language models for real-world customer-service QA systems.

</details>


### [126] [Temporal Leakage in Search-Engine Date-Filtered Web Retrieval: A Case Study from Retrospective Forecasting](https://arxiv.org/abs/2602.00758)
*Ali El Lahib,Ying-Jieh Xia,Zehan Li,Yuxuan Wang,Xinyu Pi*

Main category: cs.CL

Relevance: 75.0

TL;DR: 搜索日期过滤器在检索增强预测的回顾性评估中不可靠，71%的问题存在截止日期后信息泄露，导致预测准确性被高估


<details>
  <summary>Details</summary>
Motivation: 当前搜索增强预测系统在回顾性评估中广泛使用搜索引擎的日期过滤器来确保只检索截止日期前的信息，但这种方法可能存在信息泄露问题，导致评估结果不可靠

Method: 通过审计Google搜索的before过滤器，分析信息泄露情况；使用大型语言模型(gpt-oss-120b)在有泄露文档和无泄露文档两种条件下进行预测对比；识别常见的信息泄露机制

Result: 71%的问题至少返回一个包含强截止日期后信息泄露的页面，41%的问题至少有一个页面直接泄露答案；使用泄露文档时预测准确性被显著高估(Brier分数0.108 vs 0.242)

Conclusion: 日期限制搜索不足以进行时间性评估，需要更强的检索安全保障或使用冻结的时间戳网页快照进行可信的回顾性预测评估

Abstract: Search-engine date filters are widely used to enforce pre-cutoff retrieval in retrospective evaluations of search-augmented forecasters. We show this approach is unreliable: auditing Google Search with a before: filter, 71% of questions return at least one page containing strong post-cutoff leakage, and for 41%, at least one page directly reveals the answer. Using a large language model (LLM), gpt-oss-120b, to forecast with these leaky documents, we demonstrate an inflated prediction accuracy (Brier score 0.108 vs. 0.242 with leak-free documents). We characterize common leakage mechanisms, including updated articles, related-content modules, unreliable metadata/timestamps, and absence-based signals, and argue that date-restricted search is insufficient for temporal evaluation. We recommend stronger retrieval safeguards or evaluation on frozen, time-stamped web snapshots to ensure credible retrospective forecasting.

</details>


### [127] [DeALOG: Decentralized Multi-Agents Log-Mediated Reasoning Framework](https://arxiv.org/abs/2602.00996)
*Abhijit Chakraborty,Ashish Raj Shekhar,Shiven Agarwal,Vivek Gupta*

Main category: cs.CL

Relevance: 75.0

TL;DR: DeALOG是一个去中心化的多智能体框架，用于跨文本、表格和图像的多模态问答，通过专业智能体协作和共享自然语言日志实现信息整合与验证。


<details>
  <summary>Details</summary>
Motivation: 跨文本、表格和图像进行复杂问答需要整合多样化的信息源，现有方法缺乏支持专业处理、协调和可解释性的框架。需要一种能够处理多模态信息并确保鲁棒性的解决方案。

Method: 提出DeALOG框架，包含表格、上下文、视觉、总结和验证等专业智能体，通过共享的自然语言日志作为持久内存进行通信。这种基于日志的方法支持协作错误检测和验证，无需中央控制。

Result: 在FinQA、TAT-QA、CRT-QA、WikiTableQuestions、FeTaQA和MultiModalQA等多个数据集上展示了有竞争力的性能。分析确认了共享日志、智能体专业化和验证对准确性的重要性。

Conclusion: DeALOG通过模块化组件和自然语言通信提供了可扩展的多模态问答方法，为复杂信息整合提供了有效的去中心化框架。

Abstract: Complex question answering across text, tables and images requires integrating diverse information sources. A framework supporting specialized processing with coordination and interpretability is needed. We introduce DeALOG, a decentralized multi-agent framework for multimodal question answering. It uses specialized agents: Table, Context, Visual, Summarizing and Verification, that communicate through a shared natural-language log as persistent memory. This log-based approach enables collaborative error detection and verification without central control, improving robustness. Evaluations on FinQA, TAT-QA, CRT-QA, WikiTableQuestions, FeTaQA, and MultiModalQA show competitive performance. Analysis confirms the importance of the shared log, agent specialization, and verification for accuracy. DeALOG, provides a scalable approach through modular components using natural-language communication.

</details>


### [128] [Personality Expression Across Contexts: Linguistic and Behavioral Variation in LLM Agents](https://arxiv.org/abs/2602.01063)
*Bin Han,Deuksin Kwon,Jonathan Gratch*

Main category: cs.CL

Relevance: 75.0

TL;DR: 研究发现LLMs在相同人格提示下，会根据对话情境（破冰、谈判、群体决策、共情）表现出不同的语言、行为和情感结果，显示人格表达具有情境敏感性而非固定不变。


<details>
  <summary>Details</summary>
Motivation: 尽管LLMs可以通过明确的人格提示进行条件化，但其行为实现往往因情境而异。本研究旨在探究相同人格提示在不同对话情境下如何导致不同的语言、行为和情感结果，并探讨这种变化是LLMs的不一致性还是类似人类的情境敏感适应。

Method: 研究在四种对话情境中测试相同人格提示的LLMs：破冰任务、谈判任务、群体决策任务和共情任务。通过分析LLMs在这些情境下的语言、行为和情感表现，考察情境线索如何系统性影响人格表达和情感基调。

Result: 结果显示情境线索系统性影响人格表达和情感基调，相同特质在不同社交和情感需求下表达方式不同。LLMs表现出情境敏感而非固定的人格表达，能够灵活适应社交互动目标和情感条件。

Conclusion: 从整体特质理论视角看，LLMs展现出情境敏感的人格表达，能够根据社交互动目标和情感条件灵活调整。这提出了一个重要问题：这种变化反映了LLMs的不一致性，还是类似人类行为的情境敏感适应？

Abstract: Large Language Models (LLMs) can be conditioned with explicit personality prompts, yet their behavioral realization often varies depending on context. This study examines how identical personality prompts lead to distinct linguistic, behavioral, and emotional outcomes across four conversational settings: ice-breaking, negotiation, group decision, and empathy tasks. Results show that contextual cues systematically influence both personality expression and emotional tone, suggesting that the same traits are expressed differently depending on social and affective demands. This raises an important question for LLM-based dialogue agents: whether such variations reflect inconsistency or context-sensitive adaptation akin to human behavior. Viewed through the lens of Whole Trait Theory, these findings highlight that LLMs exhibit context-sensitive rather than fixed personality expression, adapting flexibly to social interaction goals and affective conditions.

</details>


### [129] [From Utterance to Vividity: Training Expressive Subtitle Translation LLM via Adaptive Local Preference Optimization](https://arxiv.org/abs/2602.01068)
*Chaoqun Cui,Shijing Wang,Liangbin Huang,Qingqing Gu,Zhaolong Huang,Xiao Zeng,Wenji Mao*

Main category: cs.CL

Relevance: 75.0

TL;DR: 该论文针对LLM在垂直领域翻译的局限性，以视觉媒体字幕翻译为案例，提出了ALPO方法用于细粒度偏好对齐，并构建了多语言字幕平行语料库数据集。


<details>
  <summary>Details</summary>
Motivation: 随着应用场景复杂化，LLM在垂直领域翻译中的局限性逐渐显现。研究聚焦于如何构建满足领域定制需求的翻译LLM，以视觉媒体字幕翻译为具体案例，探索如何训练表达生动、自然的翻译模型。

Method: 1) 调查字幕翻译及其他领域直译与意译的情况；2) 验证LLM作为翻译奖励模型和评估器的可靠性；3) 构建并发布多语言字幕平行语料库数据集；4) 提出自适应局部偏好优化（ALPO）方法解决细粒度偏好对齐问题。

Result: 实验结果表明ALPO在翻译质量的多维评估中取得了优异表现，验证了LLM作为翻译奖励模型和评估器的可靠性。

Conclusion: 该研究为解决LLM在垂直领域翻译的局限性提供了有效方案，ALPO方法在细粒度偏好对齐方面表现出色，构建的数据集和方法论对领域定制化翻译LLM的发展具有重要意义。

Abstract: The rapid development of Large Language Models (LLMs) has significantly enhanced the general capabilities of machine translation. However, as application scenarios become more complex, the limitations of LLMs in vertical domain translations are gradually becoming apparent. In this study, we focus on how to construct translation LLMs that meet the needs of domain customization. We take visual media subtitle translation as our topic and explore how to train expressive and vivid translation LLMs. We investigated the situations of subtitle translation and other domains of literal and liberal translation, verifying the reliability of LLM as reward model and evaluator for translation. Additionally, to train an expressive translation LLM, we constructed and released a multidirectional subtitle parallel corpus dataset and proposed the Adaptive Local Preference Optimization (ALPO) method to address fine-grained preference alignment. Experimental results demonstrate that ALPO achieves outstanding performance in multidimensional evaluation of translation quality.

</details>


### [130] [Typologically-Informed Candidate Reranking for LLM-based Translation into Low-Resource Languages](https://arxiv.org/abs/2602.01162)
*Nipuna Abeykoon,Ashen Weerathunga,Pubudu Wijesinghe,Parameswari Krishnamurthy*

Main category: cs.CL

Relevance: 75.0

TL;DR: 提出一个基于语言类型学的框架，无需平行训练数据或模型重训练，通过语言类型学知识改善大语言模型对低资源语言的翻译质量


<details>
  <summary>Details</summary>
Motivation: 大语言模型主要在高资源语言上训练，对优势类型模式存在系统性偏见，导致在翻译到类型学差异大的低资源语言时出现结构不一致问题

Method: 框架包含两个组件：通用元语言框架（UMF），在16个类型学维度上表示语言的结构化特征；计算引擎，在生成时进行语言消歧，在候选输出选择时进行类型学一致性评分

Result: 在9个语言对上的评估显示干预率与英语的类型学距离强相关。在341个英语句子的实验中，框架对保守处理语言的干预精度为48.16%，对形态密集语言为28.15%，对结构分析语言为86.26%

Conclusion: 该框架无需平行训练数据，可与任何能生成多个候选输出的LLM配合使用，为资源不足的语言提供了实用的部署方案

Abstract: Large language models trained predominantly on high-resource languages exhibit systematic biases toward dominant typological patterns, leading to structural non-conformance when translating into typologically divergent low-resource languages. We present a framework that leverages linguistic typology to improve translation quality without parallel training data or model retraining. The framework consists of two components: the Universal Metalinguistic Framework (UMF), which represents languages as structured profiles across 16 typological dimensions with divergence-weighted scoring, and the Computational Engine, which operates through linguistic disambiguation during generation and typological compliance scoring during selection. Evaluation across nine language pairs demonstrates intervention rates strongly correlating with typological distance from English. In experiments on 341 English sentences each having different morphological and syntactic phenomena, the framework shows an intervention precision of 48.16% for conservatively treated languages, 28.15% for morphologically dense languages, and 86.26% for structurally profiled languages. The framework requires no parallel training data and operates with any LLM capable of producing multiple candidate outputs, enabling practical deployment for under-resourced languages.

</details>


### [131] [Large-Scale Terminal Agentic Trajectory Generation from Dockerized Environments](https://arxiv.org/abs/2602.01244)
*Siwei Wu,Yizhi Li,Yuyang Song,Wei Zhang,Yang Wang,Riza Batista-Navarro,Xian Yang,Mingjie Tang,Bryan Dai,Jian Yang,Chenghua Lin*

Main category: cs.CL

Relevance: 75.0

TL;DR: TerminalTraj是一个可扩展的终端轨迹数据生成管道，通过Docker环境构建、任务实例生成和可执行验证代码合成，创建了5万多个经过验证的终端轨迹，用于训练终端任务代理模型。


<details>
  <summary>Details</summary>
Motivation: 训练终端任务代理模型需要高质量、长时程的终端交互轨迹数据，但大规模构建面临两大挑战：可执行性（需要不同的Docker环境）和可验证性（异构任务输出难以统一验证）。

Method: 提出TerminalTraj三阶段管道：1) 筛选高质量仓库构建Docker化执行环境；2) 生成与Docker对齐的任务实例；3) 合成带有可执行验证代码的代理轨迹。

Result: 构建了32K个Docker镜像和50,733个经过验证的终端轨迹，覆盖8个领域。使用Qwen2.5-Coder骨干训练的模型在TerminalBench上获得显著提升：TB 1.0提升20%，TB 2.0提升10%。TerminalTraj-32B在100B参数以下模型中表现优异。

Conclusion: TerminalTraj解决了终端轨迹数据构建的挑战，为训练终端任务代理模型提供了高质量、可扩展的数据集，显著提升了模型在终端基准测试上的性能。

Abstract: Training agentic models for terminal-based tasks critically depends on high-quality terminal trajectories that capture realistic long-horizon interactions across diverse domains. However, constructing such data at scale remains challenging due to two key requirements: \textbf{\emph{Executability}}, since each instance requires a suitable and often distinct Docker environment; and \textbf{\emph{Verifiability}}, because heterogeneous task outputs preclude unified, standardized verification. To address these challenges, we propose \textbf{TerminalTraj}, a scalable pipeline that (i) filters high-quality repositories to construct Dockerized execution environments, (ii) generates Docker-aligned task instances, and (iii) synthesizes agent trajectories with executable validation code. Using TerminalTraj, we curate 32K Docker images and generate 50,733 verified terminal trajectories across eight domains. Models trained on this data with the Qwen2.5-Coder backbone achieve consistent performance improvements on TerminalBench (TB), with gains of up to 20\% on TB~1.0 and 10\% on TB~2.0 over their respective backbones. Notably, \textbf{TerminalTraj-32B} achieves strong performance among models with fewer than 100B parameters, reaching 35.30\% on TB~1.0 and 22.00\% on TB~2.0, and demonstrates improved test-time scaling behavior. All code and data are available at https://github.com/Wusiwei0410/TerminalTraj.

</details>


### [132] [PARSE: An Open-Domain Reasoning Question Answering Benchmark for Persian](https://arxiv.org/abs/2602.01246)
*Jamshid Mozafari,Seyed Parsa Mousavinasab,Adam Jatowt*

Main category: cs.CL

Relevance: 75.0

TL;DR: PARSE是首个波斯语开放领域推理问答基准，包含10,800个问题，涵盖布尔、多项选择和事实型格式，支持波斯语LLM的公平评估和模型开发。


<details>
  <summary>Details</summary>
Motivation: 波斯语作为拥有约1.3亿使用者的语言，缺乏高质量的开源推理问答基准，这阻碍了波斯语LLM在推理能力方面的评估和发展。

Method: 采用基于LLM的受控生成流程构建基准，通过多阶段过滤、标注和一致性检查确保语言和事实质量，并进行人工验证。

Result: 波斯语提示和结构化提示（布尔/多项选择用CoT，事实型用few-shot）能提升性能，微调进一步改善结果，特别是波斯语专用模型。

Conclusion: PARSE填补了波斯语QA研究的关键空白，为低资源环境下开发和评估推理能力强的LLM提供了坚实基础。

Abstract: Reasoning-focused Question Answering (QA) has advanced rapidly with Large Language Models (LLMs), yet high-quality benchmarks for low-resource languages remain scarce. Persian, spoken by roughly 130 million people, lacks a comprehensive open-domain resource for evaluating reasoning-capable QA systems. We introduce PARSE, the first open-domain Persian reasoning QA benchmark, containing 10,800 questions across Boolean, multiple-choice, and factoid formats, with diverse reasoning types, difficulty levels, and answer structures. The benchmark is built via a controlled LLM-based generation pipeline and validated through human evaluation. We also ensure linguistic and factual quality through multi-stage filtering, annotation, and consistency checks. We benchmark multilingual and Persian LLMs under multiple prompting strategies and show that Persian prompts and structured prompting (CoT for Boolean/multiple-choice; few-shot for factoid) improve performance. Fine-tuning further boosts results, especially for Persian-specialized models. These findings highlight how PARSE supports both fair comparison and practical model adaptation. PARSE fills a critical gap in Persian QA research and provides a strong foundation for developing and evaluating reasoning-capable LLMs in low-resource settings.

</details>


### [133] [Ebisu: Benchmarking Large Language Models in Japanese Finance](https://arxiv.org/abs/2602.01479)
*Xueqing Peng,Ruoyu Xiang,Fan Zhang,Mingzi Song,Mingyang Jiang,Yan Wang,Lingfei Qian,Taiki Hara,Yuqing Guo,Jimin Huang,Junichi Tsujii,Sophia Ananiadou*

Main category: cs.CL

Relevance: 75.0

TL;DR: Ebisu是一个针对日语金融语言理解的基准测试，包含两个专家标注的任务：JF-ICR评估投资者问答中的隐含承诺和拒绝识别，JF-TE评估专业披露中嵌套金融术语的分层提取和排序。


<details>
  <summary>Details</summary>
Motivation: 日语金融语言具有粘着语、主谓宾结构、混合书写系统以及依赖间接表达和高语境交流规范的特点，这对LLMs构成了重大挑战。现有基准测试未能充分捕捉这些语言和文化特性。

Method: 创建Ebisu基准测试，包含两个专家标注的任务：1) JF-ICR：评估投资者问答中的隐含承诺和拒绝识别；2) JF-TE：评估专业披露中嵌套金融术语的分层提取和排序。评估了开源和专有LLMs，包括通用模型、日语适应模型和金融专用模型。

Result: 即使最先进的系统在这两个任务上也表现不佳。模型规模的增加只能带来有限的改进，语言和领域特定的适应并不能可靠地提高性能，仍然存在显著的性能差距。

Conclusion: Ebisu为推进基于语言和文化背景的金融NLP提供了一个聚焦的基准测试。所有数据集和评估脚本都已公开发布，以促进该领域的研究。

Abstract: Japanese finance combines agglutinative, head-final linguistic structure, mixed writing systems, and high-context communication norms that rely on indirect expression and implicit commitment, posing a substantial challenge for LLMs. We introduce Ebisu, a benchmark for native Japanese financial language understanding, comprising two linguistically and culturally grounded, expert-annotated tasks: JF-ICR, which evaluates implicit commitment and refusal recognition in investor-facing Q&A, and JF-TE, which assesses hierarchical extraction and ranking of nested financial terminology from professional disclosures. We evaluate a diverse set of open-source and proprietary LLMs spanning general-purpose, Japanese-adapted, and financial models. Results show that even state-of-the-art systems struggle on both tasks. While increased model scale yields limited improvements, language- and domain-specific adaptation does not reliably improve performance, leaving substantial gaps unresolved. Ebisu provides a focused benchmark for advancing linguistically and culturally grounded financial NLP. All datasets and evaluation scripts are publicly released.

</details>


### [134] [Argument Rarity-based Originality Assessment for AI-Assisted Writing](https://arxiv.org/abs/2602.01560)
*Keito Inoshita,Michiaki Omura,Tsukasa Yamanaka,Go Maeda,Kentaro Tsuji*

Main category: cs.CL

Relevance: 75.0

TL;DR: 本文提出AROA框架，通过论证稀有性评估学生议论文的原创性，包含结构稀有性、论点稀有性、证据稀有性和认知深度四个维度，发现质量与原创性存在权衡关系，AI能复现论证形式但内容原创性有限。


<details>
  <summary>Details</summary>
Motivation: 随着LLM能轻松生成高质量文本，传统基于质量的写作评估失去意义。教育的核心目标是培养批判性思维和原创观点，因此评估范式需要从质量转向原创性。

Method: 提出AROA框架，将原创性定义为参考语料库中的稀有性，通过四个互补组件评估：结构稀有性、论点稀有性、证据稀有性和认知深度。使用密度估计量化每个组件的稀有性，并通过质量调整机制整合，将质量和原创性作为独立评估维度。

Result: 实验发现质量与论点稀有性存在强负相关，显示质量-原创性权衡：高质量文本倾向于依赖典型论点模式。AI论文在结构复杂性上与人类相当，但论点稀有性显著低于人类，表明LLM能复现论证形式但内容原创性有限。

Conclusion: AROA框架为评估论证原创性提供了系统方法，揭示了LLM在内容原创性方面的局限性，强调了在教育评估中区分质量和原创性的重要性。

Abstract: As Large Language Models (LLMs) have become capable of effortlessly generating high-quality text, traditional quality-focused writing assessment is losing its significance. If the essential goal of education is to foster critical thinking and original perspectives, assessment must also shift its paradigm from quality to originality. This study proposes Argument Rarity-based Originality Assessment (AROA), a framework for automatically evaluating argumentative originality in student essays. AROA defines originality as rarity within a reference corpus and evaluates it through four complementary components: structural rarity, claim rarity, evidence rarity, and cognitive depth. The framework quantifies the rarity of each component using density estimation and integrates them with a quality adjustment mechanism, thereby treating quality and originality as independent evaluation axes. Experiments using human essays and AI-generated essays revealed a strong negative correlation between quality and claim rarity, demonstrating a quality-originality trade-off where higher-quality texts tend to rely on typical claim patterns. Furthermore, while AI essays achieved comparable levels of structural complexity to human essays, their claim rarity was substantially lower than that of humans, indicating that LLMs can reproduce the form of argumentation but have limitations in the originality of content.

</details>


### [135] [Wiki Live Challenge: Challenging Deep Research Agents with Expert-Level Wikipedia Articles](https://arxiv.org/abs/2602.01590)
*Shaohan Wang,Benfeng Xu,Licheng Zhang,Mingxuan Du,Chiwei Zhu,Xiaorui Wang,Zhendong Mao,Yongdong Zhang*

Main category: cs.CL

Relevance: 75.0

TL;DR: 论文提出了Wiki Live Challenge (WLC)基准，利用最新的维基百科优质文章作为专家级参考，评估深度研究代理(DRA)的能力，并开发了包含39个写作质量标准和事实可验证性指标的全面评估框架。


<details>
  <summary>Details</summary>
Motivation: 当前深度研究代理(DRA)的评估框架主要依赖LLM生成的参考或LLM衍生的评估维度，虽然具有可扩展性，但缺乏专家验证内容的可靠性，难以提供客观、细粒度的关键维度评估。需要建立基于专家级参考的可靠评估基准。

Method: 1) 构建Wiki Live Challenge (WLC)基准，使用最新的维基百科优质文章(Good Articles)作为专家级参考；2) 收集100篇近期优质文章数据集；3) 提出Wiki Eval评估框架，包含39个细粒度写作质量标准和严格的事实可验证性指标。

Result: 对各种DRA系统的广泛实验表明，当前深度研究代理与人类专家级维基百科文章之间存在显著差距，验证了WLC在推进代理研究方面的有效性。

Conclusion: WLC基准通过利用维基百科严格的中立性、全面性和可验证性标准，为深度研究代理提供了可靠的评估框架，有助于推动代理研究向更高水平发展。

Abstract: Deep Research Agents (DRAs) have demonstrated remarkable capabilities in autonomous information retrieval and report generation, showing great potential to assist humans in complex research tasks. Current evaluation frameworks primarily rely on LLM-generated references or LLM-derived evaluation dimensions. While these approaches offer scalability, they often lack the reliability of expert-verified content and struggle to provide objective, fine-grained assessments of critical dimensions. To bridge this gap, we introduce Wiki Live Challenge (WLC), a live benchmark that leverages the newest Wikipedia Good Articles (GAs) as expert-level references. Wikipedia's strict standards for neutrality, comprehensiveness, and verifiability serve as a great challenge for DRAs, with GAs representing the pinnacle of which. We curate a dataset of 100 recent Good Articles and propose Wiki Eval, a comprehensive evaluation framework comprising a fine-grained evaluation method with 39 criteria for writing quality and rigorous metrics for factual verifiability. Extensive experiments on various DRA systems demonstrate a significant gap between current DRAs and human expert-level Wikipedia articles, validating the effectiveness of WLC in advancing agent research. We release our benchmark at https://github.com/WangShao2000/Wiki_Live_Challenge

</details>


### [136] [SEA-Guard: Culturally Grounded Multilingual Safeguard for Southeast Asia](https://arxiv.org/abs/2602.01618)
*Panuthep Tasawong,Jian Gang Ngui,Alham Fikri Aji,Trevor Cohn,Peerat Limkonchotiwat*

Main category: cs.CL

Relevance: 75.0

TL;DR: 提出了SEA-Guard系列模型，这是首个基于东南亚文化背景的多语言安全防护模型，通过代理数据生成框架创建真实、区域特定的安全数据集，在检测区域敏感内容方面优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现实世界中的AI对齐需要文化感知的安全防护，因为安全不仅涉及常识，还包括多样化的本地价值观、规范和区域特定法规。然而，由于资源有限和本地标注者稀缺，构建大规模、文化基础的数据集具有挑战性。现有安全模型通常依赖英语数据集的机器翻译，往往忽略了区域和文化细微差别。

Method: 提出了一个新颖的代理数据生成框架，可扩展地创建真实、区域特定的东南亚安全数据集。在此基础上，引入了SEA-Guard系列模型，这是首个基于东南亚文化背景的多语言安全防护模型。

Result: 在多个基准测试和文化变体评估中，SEA-Guard在检测区域敏感或有害内容方面始终优于现有安全防护模型，同时保持了强大的通用安全性能。

Conclusion: 该研究展示了通过文化基础数据集开发区域特定安全防护模型的有效性，为解决AI对齐中的文化多样性问题提供了可扩展的解决方案。

Abstract: Culturally aware safeguards are crucial for AI alignment in real-world settings, where safety extends beyond common sense and encompasses diverse local values, norms, and region-specific regulations. However, building large-scale, culturally grounded datasets is challenging due to limited resources and a scarcity of native annotators. Consequently, many safeguard models rely on machine translation of English datasets, often missing regional and cultural nuances. We present a novel agentic data-generation framework to scalably create authentic, region-specific safety datasets for Southeast Asia (SEA). On this foundation, we introduce the SEA-Guard family, the first multilingual safeguard models grounded in SEA cultural contexts. Evaluated across multiple benchmarks and cultural variants, SEA-Guard consistently outperforms existing safeguards at detecting regionally sensitive or harmful content while maintaining strong general safety performance.

</details>


### [137] [A2Eval: Agentic and Automated Evaluation for Embodied Brain](https://arxiv.org/abs/2602.01640)
*Shuai Zhang,Jiayu Hu,Zijie Chen,Zeyuan Ding,Yi Zhang,Yingji Zhang,Ziyi Zhou,Junwei Liao,Shengjie Zhou,Yong Dai,Zhenzhong Lan,Xiaozhu Ju*

Main category: cs.CL

Relevance: 75.0

TL;DR: A2Eval：首个代理驱动的自动评估框架，通过两个协作代理（数据代理和评估代理）自动化基准构建和评估，显著降低计算成本并提高评估质量。


<details>
  <summary>Details</summary>
Motivation: 当前具身VLM评估依赖静态、专家定义的手动标注基准，存在严重冗余和覆盖不平衡问题。这种劳动密集型范式消耗大量计算和标注资源，增加成本，扭曲模型排名，阻碍迭代开发。

Method: 提出A2Eval框架，包含两个协作代理：1）数据代理：自主归纳能力维度并组装平衡、紧凑的评估套件；2）评估代理：合成和验证可执行的评估流程，实现完全自主的高保真评估。

Result: 在10个基准和13个模型上评估，A2Eval将评估套件压缩85%，总体计算成本降低77%，速度提升4.6倍，同时保持评估质量。纠正系统性排名偏差，提高与人类对齐度（Spearman's rho=0.85），保持高排名保真度（Kendall's tau=0.81）。

Conclusion: A2Eval为高保真、低成本的具身评估建立了新标准，解决了当前评估范式的根本缺陷，为LLM评估提供了更高效、准确的自动化解决方案。

Abstract: Current embodied VLM evaluation relies on static, expert-defined, manually annotated benchmarks that exhibit severe redundancy and coverage imbalance. This labor intensive paradigm drains computational and annotation resources, inflates costs, and distorts model rankings, ultimately stifling iterative development. To address this, we propose Agentic Automatic Evaluation (A2Eval), the first agentic framework that automates benchmark curation and evaluation through two collaborative agents. The Data Agent autonomously induces capability dimensions and assembles a balanced, compact evaluation suite, while the Eval Agent synthesizes and validates executable evaluation pipelines, enabling fully autonomous, high-fidelity assessment. Evaluated across 10 benchmarks and 13 models, A2Eval compresses evaluation suites by 85%, reduces overall computational costs by 77%, and delivers a 4.6x speedup while preserving evaluation quality. Crucially, A2Eval corrects systematic ranking biases, improves human alignment to Spearman's rho=0.85, and maintains high ranking fidelity (Kendall's tau=0.81), establishing a new standard for high-fidelity, low-cost embodied assessment. Our code and data will be public soon.

</details>


### [138] [MedAraBench: Large-Scale Arabic Medical Question Answering Dataset and Benchmark](https://arxiv.org/abs/2602.01714)
*Mouath Abu-Daoud,Leen Kharouf,Omar El Hajj,Dana El Samad,Mariam Al-Omari,Jihad Mallat,Khaled Saleh,Nizar Habash,Farah E. Shamout*

Main category: cs.CL

Relevance: 75.0

TL;DR: MedAraBench：首个大规模阿拉伯语医学多选题数据集，涵盖19个医学专科和5个难度级别，用于评估LLMs在阿拉伯语医学领域的性能。


<details>
  <summary>Details</summary>
Motivation: 阿拉伯语在NLP研究中代表性不足，尤其在医学应用领域，缺乏开源数据和基准测试阻碍了LLMs多语言能力的评估与提升。

Method: 通过人工数字化阿拉伯语地区医学专家创建的学术材料构建数据集，进行广泛预处理并划分为训练/测试集，采用专家人工评估和LLM-as-a-judge两种框架验证数据质量。

Result: 数据集质量高且多样化，评估了GPT-5、Gemini 2.0 Flash、Claude 4-Sonnet等8个SOTA模型，结果显示需要进一步的领域特定增强。

Conclusion: 发布数据集和评估脚本以丰富医学数据基准多样性，扩展LLMs评估范围，增强模型在临床环境中的多语言部署能力。

Abstract: Arabic remains one of the most underrepresented languages in natural language processing research, particularly in medical applications, due to the limited availability of open-source data and benchmarks. The lack of resources hinders efforts to evaluate and advance the multilingual capabilities of Large Language Models (LLMs). In this paper, we introduce MedAraBench, a large-scale dataset consisting of Arabic multiple-choice question-answer pairs across various medical specialties. We constructed the dataset by manually digitizing a large repository of academic materials created by medical professionals in the Arabic-speaking region. We then conducted extensive preprocessing and split the dataset into training and test sets to support future research efforts in the area. To assess the quality of the data, we adopted two frameworks, namely expert human evaluation and LLM-as-a-judge. Our dataset is diverse and of high quality, spanning 19 specialties and five difficulty levels. For benchmarking purposes, we assessed the performance of eight state-of-the-art open-source and proprietary models, such as GPT-5, Gemini 2.0 Flash, and Claude 4-Sonnet. Our findings highlight the need for further domain-specific enhancements. We release the dataset and evaluation scripts to broaden the diversity of medical data benchmarks, expand the scope of evaluation suites for LLMs, and enhance the multilingual capabilities of models for deployment in clinical settings.

</details>


### [139] [SafePred: A Predictive Guardrail for Computer-Using Agents via World Models](https://arxiv.org/abs/2602.01725)
*Yurun Chen,Zeyi Liao,Ping Yin,Taotao Xie,Keting Yin,Shengyu Zhang*

Main category: cs.CL

Relevance: 75.0

TL;DR: SafePred是一个预测性护栏框架，通过将预测的未来风险与当前决策对齐，防止计算机使用代理的长期风险，相比反应式方法显著提升安全性。


<details>
  <summary>Details</summary>
Motivation: 现有计算机使用代理的护栏大多采用反应式方法，只能约束当前观察空间内的行为，无法预防长期风险。看似合理的行动可能导致延迟出现的高风险后果，而反应式护栏无法在当前观察空间内识别这些风险。

Method: 提出预测性护栏方法，核心思想是将预测的未来风险与当前决策对齐。SafePred框架建立风险到决策的循环，支持：(1)短期和长期风险预测：使用安全策略作为风险预测基础，利用世界模型的预测能力生成风险语义表示；(2)决策优化：通过步骤级干预和任务级重新规划，将预测风险转化为可操作的安全决策指导。

Result: 实验表明SafePred显著减少高风险行为，相比反应式基线实现超过97.6%的安全性能，并将任务效用提升高达21.4%。

Conclusion: 预测性护栏方法能有效解决计算机使用代理的长期风险问题，SafePred框架通过风险预测和决策优化的结合，在保证安全的同时提升任务效用。

Abstract: With the widespread deployment of Computer-using Agents (CUAs) in complex real-world environments, prevalent long-term risks often lead to severe and irreversible consequences. Most existing guardrails for CUAs adopt a reactive approach, constraining agent behavior only within the current observation space. While these guardrails can prevent immediate short-term risks (e.g., clicking on a phishing link), they cannot proactively avoid long-term risks: seemingly reasonable actions can lead to high-risk consequences that emerge with a delay (e.g., cleaning logs leads to future audits being untraceable), which reactive guardrails cannot identify within the current observation space. To address these limitations, we propose a predictive guardrail approach, with the core idea of aligning predicted future risks with current decisions. Based on this approach, we present SafePred, a predictive guardrail framework for CUAs that establishes a risk-to-decision loop to ensure safe agent behavior. SafePred supports two key abilities: (1) Short- and long-term risk prediction: by using safety policies as the basis for risk prediction, SafePred leverages the prediction capability of the world model to generate semantic representations of both short-term and long-term risks, thereby identifying and pruning actions that lead to high-risk states; (2) Decision optimization: translating predicted risks into actionable safe decision guidances through step-level interventions and task-level re-planning. Extensive experiments show that SafePred significantly reduces high-risk behaviors, achieving over 97.6% safety performance and improving task utility by up to 21.4% compared with reactive baselines.

</details>


### [140] [WorldCup Sampling for Multi-bit LLM Watermarking](https://arxiv.org/abs/2602.01752)
*Yidan Wang,Yubing Ren,Yanan Cao,Li Guo*

Main category: cs.CL

Relevance: 75.0

TL;DR: WorldCup是一个用于大语言模型的多比特水印框架，通过分层竞争机制将消息比特直接嵌入到token选择中，实现了高容量、强鲁棒性和良好文本质量的平衡。


<details>
  <summary>Details</summary>
Motivation: 随着大语言模型生成越来越类似人类的文本，水印技术为可靠的来源归属提供了有前景的解决方案。现有的多比特水印方法大多通过种子驱动的方式扩展零比特方案，导致间接信息流、有限的有效容量和次优的解码性能。

Method: WorldCup将采样视为自然通信信道，通过分层竞争机制将消息比特直接嵌入到token选择中。该方法使用互补信号指导竞争，采用熵感知调制来保持生成质量，并通过置信感知解码支持鲁棒的消息恢复。

Result: 综合实验表明，WorldCup在容量、可检测性、鲁棒性、文本质量和解码效率之间实现了良好的平衡，一致优于现有基线方法。

Conclusion: WorldCup为未来大语言模型水印研究奠定了坚实基础，展示了直接嵌入消息比特的分层竞争机制的有效性。

Abstract: As large language models (LLMs) generate increasingly human-like text, watermarking offers a promising solution for reliable attribution beyond mere detection. While multi-bit watermarking enables richer provenance encoding, existing methods largely extend zero-bit schemes through seed-driven steering, leading to indirect information flow, limited effective capacity, and suboptimal decoding. In this paper, we propose WorldCup, a multi-bit watermarking framework for LLMs that treats sampling as a natural communication channel and embeds message bits directly into token selection via a hierarchical competition mechanism guided by complementary signals. Moreover, WorldCup further adopts entropy-aware modulation to preserve generation quality and supports robust message recovery through confidence-aware decoding. Comprehensive experiments show that WorldCup achieves a strong balance across capacity, detectability, robustness, text quality, and decoding efficiency, consistently outperforming prior baselines and laying a solid foundation for future LLM watermarking studies.

</details>


### [141] [AXE: Low-Cost Cross-Domain Web Structured Information Extraction](https://arxiv.org/abs/2602.01838)
*Abdelrahman Mansour,Khaled W. Alshaer,Moataz Elsaban*

Main category: cs.CL

Relevance: 75.0

TL;DR: AXE是一个用于网页信息提取的轻量级系统，通过DOM树剪枝机制和0.6B小模型实现高效结构化数据提取，在SWDE数据集上达到88.1% F1分数。


<details>
  <summary>Details</summary>
Motivation: 解决网页结构化数据提取中的两难问题：传统手动启发式方法脆弱且不鲁棒，而使用大语言模型成本过高。需要一种既准确又经济实惠的解决方案。

Method: 1) 将HTML DOM视为需要剪枝的树而非纯文本；2) 使用专门的"剪枝"机制去除样板内容和无关节点；3) 保留高密度上下文供0.6B小模型处理；4) 实施Grounded XPath Resolution确保提取结果可物理追溯。

Result: 在SWDE数据集上实现零样本SOTA性能，F1分数达88.1%，超越多个更大规模的完全训练模型。提供专门适配器，为大规模网页信息提取提供实用、经济高效的路径。

Conclusion: AXE通过创新的DOM树剪枝和小模型组合，在保持低成本的同时实现了高质量的网页信息提取，为实际应用提供了可行的解决方案。

Abstract: Extracting structured data from the web is often a trade-off between the brittle nature of manual heuristics and the prohibitive cost of Large Language Models. We introduce AXE (Adaptive X-Path Extractor), a pipeline that rethinks this process by treating the HTML DOM as a tree that needs pruning rather than just a wall of text to be read. AXE uses a specialized "pruning" mechanism to strip away boilerplate and irrelevant nodes, leaving behind a distilled, high-density context that allows a tiny 0.6B LLM to generate precise, structured outputs. To keep the model honest, we implement Grounded XPath Resolution (GXR), ensuring every extraction is physically traceable to a source node. Despite its low footprint, AXE achieves state-of-the-art zero-shot performance, outperforming several much larger, fully-trained alternatives with an F1 score of 88.1% on the SWDE dataset. By releasing our specialized adaptors, we aim to provide a practical, cost-effective path for large-scale web information extraction.

</details>


### [142] [Orthogonal Hierarchical Decomposition for Structure-Aware Table Understanding with Large Language Models](https://arxiv.org/abs/2602.01969)
*Bin Cao,Huixian Lu,Chenwen Ma,Ting Wang,Ruizhe Li,Jing Fan*

Main category: cs.CL

Relevance: 75.0

TL;DR: 提出正交层次分解（OHD）框架，通过构建保持结构的复杂表格表示来解决LLMs处理多级表头、合并单元格等复杂表格的挑战


<details>
  <summary>Details</summary>
Motivation: 现有表格线性化或网格建模方法难以显式捕捉复杂表格的层次结构和跨维度依赖，导致结构语义与文本表示不对齐，影响LLMs对复杂表格的理解和推理能力

Method: 提出正交层次分解（OHD）框架：1）基于空间-语义共同约束的正交树归纳（OTI）方法，将不规则表格分解为列树和行树分别捕捉垂直和水平层次依赖；2）设计双路径关联协议对称重构每个单元格的语义谱系；3）引入LLM作为语义仲裁器对齐多级语义信息

Result: 在AITQA和HiTab两个复杂表格问答基准测试中，OHD框架在多个评估指标上持续优于现有表示范式

Conclusion: OHD框架通过正交层次分解有效捕捉复杂表格的结构特征，显著提升LLMs对复杂表格的理解和推理能力，为表格处理提供了新的结构保持表示方法

Abstract: Complex tables with multi-level headers, merged cells and heterogeneous layouts pose persistent challenges for LLMs in both understanding and reasoning. Existing approaches typically rely on table linearization or normalized grid modeling. However, these representations struggle to explicitly capture hierarchical structures and cross-dimensional dependencies, which can lead to misalignment between structural semantics and textual representations for non-standard tables. To address this issue, we propose an Orthogonal Hierarchical Decomposition (OHD) framework that constructs structure-preserving input representations of complex tables for LLMs. OHD introduces an Orthogonal Tree Induction (OTI) method based on spatial--semantic co-constraints, which decomposes irregular tables into a column tree and a row tree to capture vertical and horizontal hierarchical dependencies, respectively. Building on this representation, we design a dual-pathway association protocol to symmetrically reconstruct semantic lineage of each cell, and incorporate an LLM as a semantic arbitrator to align multi-level semantic information. We evaluate OHD framework on two complex table question answering benchmarks, AITQA and HiTab. Experimental results show that OHD consistently outperforms existing representation paradigms across multiple evaluation metrics.

</details>


### [143] [Closing the Loop: Universal Repository Representation with RPG-Encoder](https://arxiv.org/abs/2602.02084)
*Jane Luo,Chengyu Yin,Xin Zhang,Qingtao Li,Steven Liu,Yiming Huang,Jie Wu,Hao Liu,Yangyu Huang,Yu Kang,Fangkai Yang,Ying Xin,Scarlett Li*

Main category: cs.CL

Relevance: 75.0

TL;DR: RPG-Encoder将代码仓库理解与生成统一为循环过程，通过Repository Planning Graph结合语义特征与代码依赖，实现高效代码库理解与导航，在多个基准测试中达到SOTA性能。


<details>
  <summary>Details</summary>
Motivation: 现有仓库代理存在推理断层问题，因为它们依赖孤立的API文档或缺乏语义深度的依赖图。作者认为仓库理解与生成是统一循环中的逆过程：生成将意图扩展为实现，而理解将实现压缩回意图。

Method: 提出RPG-Encoder框架，将Repository Planning Graph从静态生成蓝图推广为统一的高保真表示。包含三个机制：1) 将原始代码编码为结合语义特征和代码依赖的RPG；2) 增量演化拓扑结构，使维护成本与仓库规模解耦；3) 作为结构感知导航的统一接口。

Result: 在SWE-bench Verified上达到93.7% Acc@5的SOTA仓库理解性能，在SWE-bench Live Lite上超过最佳基线10%以上。在RepoCraft上实现98.5%的重建覆盖率，验证了RPG的高保真能力。

Conclusion: RPG-Encoder通过统一的Repository Planning Graph表示，成功弥合了代码仓库理解与生成之间的推理断层，实现了意图与实现之间的闭环，显著提升了复杂代码库的细粒度定位准确性。

Abstract: Current repository agents encounter a reasoning disconnect due to fragmented representations, as existing methods rely on isolated API documentation or dependency graphs that lack semantic depth. We consider repository comprehension and generation to be inverse processes within a unified cycle: generation expands intent into implementation, while comprehension compresses implementation back into intent. To address this, we propose RPG-Encoder, a framework that generalizes the Repository Planning Graph (RPG) from a static generative blueprint into a unified, high-fidelity representation. RPG-Encoder closes the reasoning loop through three mechanisms: (1) Encoding raw code into the RPG that combines lifted semantic features with code dependencies; (2) Evolving the topology incrementally to decouple maintenance costs from repository scale, reducing overhead by 95.7%; and (3) Operating as a unified interface for structure-aware navigation. In evaluations, RPG-Encoder establishes state-of-the-art repository understanding on SWE-bench Verified with 93.7% Acc@5 and exceeds the best baseline by over 10% on SWE-bench Live Lite. These results highlight our superior fine-grained localization accuracy in complex codebases. Furthermore, it achieves 98.5% reconstruction coverage on RepoCraft, confirming RPG's high-fidelity capacity to mirror the original codebase and closing the loop between intent and implementation.

</details>


### [144] [LEC-KG: An LLM-Embedding Collaborative Framework for Domain-Specific Knowledge Graph Construction -- A Case Study on SDGs](https://arxiv.org/abs/2602.02090)
*Yikai Zeng,Yingchao Piao,Jianhui Li*

Main category: cs.CL

Relevance: 75.0

TL;DR: LEC-KG：一个双向协作框架，将LLM的语义理解与知识图谱嵌入的结构推理相结合，用于从非结构化文本构建领域特定知识图谱。


<details>
  <summary>Details</summary>
Motivation: 从非结构化文本构建领域特定知识图谱面临三大挑战：异构实体提及、长尾关系分布、缺乏标准化模式。现有方法难以同时处理语义理解和结构推理。

Method: 提出双向协作框架LEC-KG，包含三个关键组件：1) 分层粗到细关系提取缓解长尾偏差；2) 证据引导的思维链反馈将结构建议基于源文本；3) 语义初始化实现未见实体的结构验证。LLM和KGE模块迭代增强彼此。

Result: 在中国可持续发展目标(SDG)报告上评估，相比LLM基线有显著改进，特别是在低频关系上。通过迭代精炼，框架能可靠地将非结构化政策文本转化为验证的知识图谱三元组。

Conclusion: LEC-KG框架成功整合了LLM的语义能力和KGE的结构推理，有效解决了领域特定知识图谱构建的挑战，特别是在处理长尾关系和未见实体方面表现优异。

Abstract: Constructing domain-specific knowledge graphs from unstructured text remains challenging due to heterogeneous entity mentions, long-tail relation distributions, and the absence of standardized schemas. We present LEC-KG, a bidirectional collaborative framework that integrates the semantic understanding of Large Language Models (LLMs) with the structural reasoning of Knowledge Graph Embeddings (KGE). Our approach features three key components: (1) hierarchical coarse-to-fine relation extraction that mitigates long-tail bias, (2) evidence-guided Chain-of-Thought feedback that grounds structural suggestions in source text, and (3) semantic initialization that enables structural validation for unseen entities. The two modules enhance each other iteratively-KGE provides structure-aware feedback to refine LLM extractions, while validated triples progressively improve KGE representations. We evaluate LEC-KG on Chinese Sustainable Development Goal (SDG) reports, demonstrating substantial improvements over LLM baselines, particularly on low-frequency relations. Through iterative refinement, our framework reliably transforms unstructured policy text into validated knowledge graph triples.

</details>


### [145] [Dicta-LM 3.0: Advancing The Frontier of Hebrew Sovereign LLMs](https://arxiv.org/abs/2602.02104)
*Shaltiel Shmidman,Avi Shmidman,Amir DN Cohen,Moshe Koppel*

Main category: cs.CL

Relevance: 75.0

TL;DR: Dicta-LM 3.0：针对希伯来语等低资源语言的开源大语言模型集合，包含24B、12B、1.7B三种规模，支持65k上下文长度，并发布了新的希伯来语聊天LLM评估基准套件。


<details>
  <summary>Details</summary>
Motivation: 当前开源LLM主要由前沿实验室发布，但针对非英语语言（特别是低资源语言如希伯来语）的主权大语言模型供应不足而需求旺盛。训练低资源语言LLM面临独特挑战，需要专门解决方案。

Method: 基于现有基础模型进行适配：24B基于Mistral-Small-3.1，12B基于NVIDIA Nemotron Nano V2，1.7B基于Qwen3-1.7B。在大量希伯来语和英语文本语料上进行训练，发布基础模型和带工具调用支持的聊天模型变体，均支持65k原生上下文长度。

Result: 发布了Dicta-LM 3.0模型集合，并引入了新的希伯来语聊天LLM评估基准套件，涵盖翻译、摘要、Winograd推理、以色列知识问答、希伯来语元音标注等多种任务。

Conclusion: 该工作不仅解决了低资源语言训练LLM的复杂性，还提出了一个可用于将其他LLM适配到各种非英语语言的框架，为多语言NLP领域做出贡献。

Abstract: Open-weight LLMs have been released by frontier labs; however, sovereign Large Language Models (for languages other than English) remain low in supply yet high in demand. Training large language models (LLMs) for low-resource languages such as Hebrew poses unique challenges. In this paper, we introduce Dicta-LM 3.0: an open-weight collection of LLMs trained on substantially-sized corpora of Hebrew and English texts. The model is released in three sizes: 24B - adapted from the Mistral-Small-3.1 base model, 12B - adapted from the NVIDIA Nemotron Nano V2 model, and 1.7B - adapted from the Qwen3-1.7B base model. We are releasing multiple variants of each model, each with a native context length of 65k tokens; base model and chat model with tool-calling support. To rigorously evaluate our models, we introduce a new benchmark suite for evaluation of Hebrew chat-LLMs, covering a diverse set of tasks including Translation, Summarization, Winograd, Israeli Trivia, and Diacritization (nikud). Our work not only addresses the intricacies of training LLMs in low-resource languages but also proposes a framework that can be leveraged for adapting other LLMs to various non-English languages, contributing to the broader field of multilingual NLP.

</details>


### [146] [Quantifying the Gap between Understanding and Generation within Unified Multimodal Models](https://arxiv.org/abs/2602.02140)
*Chenlong Wang,Yuhang Chen,Zhihan Hu,Dongping Chen,Wenhu Chen,Sarah Wiegreffe,Tianyi Zhou*

Main category: cs.CL

Relevance: 75.0

TL;DR: GapEval是一个双向基准测试，用于量化统一多模态模型中理解与生成能力之间的差距，发现当前模型仅实现表面统一而非深度认知融合。


<details>
  <summary>Details</summary>
Motivation: 尽管统一多模态模型在理解和生成任务上取得显著进展，但这两项能力是否真正对齐和整合仍不清楚。作者旨在研究当前UMMs是否实现了深层的认知统一。

Method: 提出GapEval双向基准测试，每个问题可在图像和文本两种模态中回答，对称评估模型的双向推理能力和跨模态一致性。通过实验分析多种架构的UMMs，并从知识操作角度进行实证研究。

Result: 实验显示各种架构的UMMs在两个方向上都存在持续差距，表明当前模型仅实现表面统一而非深度认知融合。知识在UMMs中往往保持分离，跨模态的能力涌现和知识不同步。

Conclusion: 当前统一多模态模型的理解和生成能力并未真正对齐，只是表面统一。模型内部知识保持分离状态，这为未来探索提供了方向。

Abstract: Recent advances in unified multimodal models (UMM) have demonstrated remarkable progress in both understanding and generation tasks. However, whether these two capabilities are genuinely aligned and integrated within a single model remains unclear. To investigate this question, we introduce GapEval, a bidirectional benchmark designed to quantify the gap between understanding and generation capabilities, and quantitatively measure the cognitive coherence of the two "unified" directions. Each question can be answered in both modalities (image and text), enabling a symmetric evaluation of a model's bidirectional inference capability and cross-modal consistency. Experiments reveal a persistent gap between the two directions across a wide range of UMMs with different architectures, suggesting that current models achieve only surface-level unification rather than deep cognitive convergence of the two. To further explore the underlying mechanism, we conduct an empirical study from the perspective of knowledge manipulation to illustrate the underlying limitations. Our findings indicate that knowledge within UMMs often remains disjoint. The capability emergence and knowledge across modalities are unsynchronized, paving the way for further exploration.

</details>


### [147] [Towards AI Evaluation in Domain-Specific RAG Systems: The AgriHubi Case Study](https://arxiv.org/abs/2602.02208)
*Md. Toufique Hasan,Ayman Asad Khan,Mika Saari,Vaishnavi Bankhele,Pekka Abrahamsson*

Main category: cs.CL

Relevance: 75.0

TL;DR: AgriHubi：针对芬兰语农业决策支持的领域自适应检索增强生成系统，结合芬兰农业文档与开源模型，通过显式来源标注和用户反馈支持迭代优化


<details>
  <summary>Details</summary>
Motivation: 大语言模型在知识密集型领域有潜力，但在农业应用中受到限制：缺乏领域基础、英语中心训练数据、真实世界评估不足。这些问题在低资源语言（如芬兰语）中更严重，尽管存在高质量领域文档但难以通过通用模型访问

Method: 开发AgriHubi系统：集成芬兰农业文档与开源PORO系列模型，采用检索增强生成架构，结合显式来源标注和用户反馈机制支持迭代优化，经过8个迭代周期开发

Result: 通过两项用户研究评估，系统在答案完整性、语言准确性和感知可靠性方面有明显提升。研究揭示了部署更大模型时响应质量与延迟之间的实际权衡

Conclusion: 本研究为低资源语言环境下设计和评估领域特定RAG系统提供了实证指导，展示了领域自适应方法在农业决策支持中的有效性

Abstract: Large language models show promise for knowledge-intensive domains, yet their use in agriculture is constrained by weak grounding, English-centric training data, and limited real-world evaluation. These issues are amplified for low-resource languages, where high-quality domain documentation exists but remains difficult to access through general-purpose models. This paper presents AgriHubi, a domain-adapted retrieval-augmented generation (RAG) system for Finnish-language agricultural decision support. AgriHubi integrates Finnish agricultural documents with open PORO family models and combines explicit source grounding with user feedback to support iterative refinement. Developed over eight iterations and evaluated through two user studies, the system shows clear gains in answer completeness, linguistic accuracy, and perceived reliability. The results also reveal practical trade-offs between response quality and latency when deploying larger models. This study provides empirical guidance for designing and evaluating domain-specific RAG systems in low-resource language settings.

</details>


### [148] [ROG: Retrieval-Augmented LLM Reasoning for Complex First-Order Queries over Knowledge Graphs](https://arxiv.org/abs/2602.02382)
*Ziyan Zhang,Chao Wang,Zhuo Chen,Chiyi Li,Kai Song*

Main category: cs.CL

Relevance: 75.0

TL;DR: ROG：一种检索增强框架，结合查询感知的邻域检索和LLM思维链推理，用于在不完整知识图谱上回答一阶逻辑查询，特别擅长处理复杂和否定密集的查询结构。


<details>
  <summary>Details</summary>
Motivation: 在不完整的知识图谱上回答一阶逻辑查询（包含投影、交集、并集和否定等复杂结构）具有挑战性。现有方法（特别是基于嵌入的方法）在处理复杂查询和否定查询时存在困难，需要更稳健的推理框架。

Method: ROG框架将多操作符查询分解为单操作符子查询序列，每个步骤都基于紧凑的查询相关邻域证据进行推理。使用查询感知的邻域检索获取相关证据，结合LLM的思维链推理。中间答案集被缓存并在不同步骤中重用，提高深度推理链的一致性。

Result: 在标准KG推理基准测试中，ROG相比基于嵌入的强基线方法取得了持续提升，在高度复杂和否定密集的查询类型上改进最大。该框架减少了复合错误，在复杂和否定密集查询上提供了更稳健的推理。

Conclusion: ROG提供了一种实用的替代方案，用检索基础的逐步推理替代学习到的操作符，为基于嵌入的逻辑推理提供了有效补充，特别适用于复杂查询结构。

Abstract: Answering first-order logic (FOL) queries over incomplete knowledge graphs (KGs) is difficult, especially for complex query structures that compose projection, intersection, union, and negation. We propose ROG, a retrieval-augmented framework that combines query-aware neighborhood retrieval with large language model (LLM) chain-of-thought reasoning. ROG decomposes a multi-operator query into a sequence of single-operator sub-queries and grounds each step in compact, query-relevant neighborhood evidence. Intermediate answer sets are cached and reused across steps, improving consistency on deep reasoning chains. This design reduces compounding errors and yields more robust inference on complex and negation-heavy queries. Overall, ROG provides a practical alternative to embedding-based logical reasoning by replacing learned operators with retrieval-grounded, step-wise inference. Experiments on standard KG reasoning benchmarks show consistent gains over strong embedding-based baselines, with the largest improvements on high-complexity and negation-heavy query types.

</details>


### [149] [Misconception Diagnosis From Student-Tutor Dialogue: Generate, Retrieve, Rerank](https://arxiv.org/abs/2602.02414)
*Joshua Mitton,Prarthana Bhattacharyya,Digory Smith,Thomas Christie,Ralph Abboud,Simon Woodhead*

Main category: cs.CL

Relevance: 75.0

TL;DR: 本文提出了一种基于LLM的学生-导师对话误解检测方法，通过生成-检索-重排三阶段流程，在真实教育对话数据上验证了有效性，并发现微调能超越更大规模的闭源模型。


<details>
  <summary>Details</summary>
Motivation: 及时准确识别学生误解对改善学习效果至关重要，但目前高度依赖教师的经验和直觉。需要自动化方法从学生-导师对话中检测误解，减轻教师负担并提高教育质量。

Method: 三阶段方法：1) 使用微调LLM生成可能的误解；2) 通过嵌入相似性检索最相关的候选误解；3) 使用另一个微调LLM评估和重排候选误解以提高相关性。在真实教育对话平台上评估，比较了LLaMA、Qwen和Claude等模型在零样本和微调设置下的表现。

Result: 该方法在预测性能上优于基线模型，微调能显著提高生成误解的质量，甚至可以超越更大的闭源模型。消融研究验证了生成和重排步骤对误解生成质量的重要性。

Conclusion: 提出的LLM-based误解检测方法在教育对话分析中有效，微调策略能显著提升性能，为自动化教育评估提供了实用解决方案。

Abstract: Timely and accurate identification of student misconceptions is key to improving learning outcomes and pre-empting the compounding of student errors. However, this task is highly dependent on the effort and intuition of the teacher. In this work, we present a novel approach for detecting misconceptions from student-tutor dialogues using large language models (LLMs). First, we use a fine-tuned LLM to generate plausible misconceptions, and then retrieve the most promising candidates among these using embedding similarity with the input dialogue. These candidates are then assessed and re-ranked by another fine-tuned LLM to improve misconception relevance. Empirically, we evaluate our system on real dialogues from an educational tutoring platform. We consider multiple base LLM models including LLaMA, Qwen and Claude on zero-shot and fine-tuned settings. We find that our approach improves predictive performance over baseline models and that fine-tuning improves both generated misconception quality and can outperform larger closed-source models. Finally, we conduct ablation studies to both validate the importance of our generation and reranking steps on misconception generation quality.

</details>


### [150] [Large Language Models for Mental Health: A Multilingual Evaluation](https://arxiv.org/abs/2602.02440)
*Nishat Raihan,Sadiya Sayara Chowdhury Puspo,Ana-Maria Bucur,Stevie Chancellor,Marcos Zampieri*

Main category: cs.CL

Relevance: 75.0

TL;DR: 评估大语言模型在多语言心理健康任务中的表现，比较专有和开源模型在不同语言和翻译数据上的性能差异


<details>
  <summary>Details</summary>
Motivation: 虽然大语言模型在NLP任务中表现出色，但其在多语言环境下的性能，特别是在心理健康领域，尚未得到充分探索。研究者希望了解LLM在不同语言的心理健康数据集上的表现，以及机器翻译质量对性能的影响。

Method: 在八个多语言心理健康数据集及其机器翻译版本上评估专有和开源LLM，采用零样本、少样本和微调设置，与传统NLP基线比较，同时评估不同语言家族和类型学的翻译质量影响

Result: 专有LLM和微调后的开源LLM在多个数据集上获得有竞争力的F1分数，常超越最先进结果。但在机器翻译数据上性能普遍较低，且下降程度因语言和类型学而异

Conclusion: LLM在处理非英语心理健康任务方面具有优势，但当翻译质量引入结构或词汇不匹配时存在局限性。研究揭示了LLM在多语言心理健康应用中的潜力和挑战

Abstract: Large Language Models (LLMs) have remarkable capabilities across NLP tasks. However, their performance in multilingual contexts, especially within the mental health domain, has not been thoroughly explored. In this paper, we evaluate proprietary and open-source LLMs on eight mental health datasets in various languages, as well as their machine-translated (MT) counterparts. We compare LLM performance in zero-shot, few-shot, and fine-tuned settings against conventional NLP baselines that do not employ LLMs. In addition, we assess translation quality across language families and typologies to understand its influence on LLM performance. Proprietary LLMs and fine-tuned open-source LLMs achieve competitive F1 scores on several datasets, often surpassing state-of-the-art results. However, performance on MT data is generally lower, and the extent of this decline varies by language and typology. This variation highlights both the strengths of LLMs in handling mental health tasks in languages other than English and their limitations when translation quality introduces structural or lexical mismatches.

</details>


### [151] [Adapting Where It Matters: Depth-Aware Adaptation for Efficient Multilingual Speech Recognition in Low-Resource Languages](https://arxiv.org/abs/2602.01008)
*Yang Xiao,Eun-Jung Holden,Ting Dang*

Main category: eess.AS

Relevance: 75.0

TL;DR: DAMA提出了一种深度感知模型适配框架，针对多语言ASR模型中的U形适配模式，为不同层分配不同的适配容量，在低资源语言ASR中实现了高效且有效的适配。


<details>
  <summary>Details</summary>
Motivation: 当前语音基础模型在高资源语言的多语言ASR中表现出色，但在低资源语言适配方面面临挑战。全模型微调计算成本高且容易过拟合，而参数高效方法（如LoRA）在各层均匀应用适配，忽视了内部表示差异，从而影响效果和效率。

Method: 分析多语言ASR模型发现U形适配模式：早期和晚期层是语言特定的需要更多适配，中间层保留共享语义需要较少适配。基于此提出DAMA框架，根据每层角色分配适配容量。引入基于SVD的初始化来约束适配并保持U形模式，以及冻结中间层基础以进一步提高效率。

Result: 在18种低资源语言和两个基准数据集上评估，DAMA匹配或超越了最先进的准确率，同时减少了80%的可训练参数，在极端数据稀缺情况下实现了29%的错误率降低，并在内存、训练时间和计算效率方面显著优于基线方法。

Conclusion: 这些结果突显了结构感知适配对于高效、可扩展的多语言ASR的益处，为低资源语言适配提供了有效的解决方案。

Abstract: Recent speech foundation models excel at multilingual automatic speech recognition (ASR) for high-resource languages, but adapting them to low-resource languages remains challenging due to data scarcity and efficiency constraints. Full-model fine-tuning is computationally expensive and prone to overfitting, while parameter-efficient methods like LoRA apply adaptation uniformly across layers, overlooking internal representations thus compromising effectiveness and efficiency. We analyze multilingual ASR models and reveal a U-shaped adaptability pattern: early and late layers are language-specific and require more adaptation, while intermediate layers retain shared semantics and need less. Building on this observation, we propose DAMA, a Depth-Aware Model Adaptation framework that allocates adaptation capacity according to each layer's role. DAMA also introduces Singular Value Decomposition (SVD)-based initialization to constrain adaptation and preserve the U-shaped pattern, as well as a frozen middle-layer basis for further efficiency. Evaluated on 18 low-resource languages across two benchmark datasets, DAMA matches or surpasses state-of-the-art accuracy with 80% fewer trainable parameters, achieves a 29% error reduction under extreme data scarcity, and significantly improves memory, training time, and computational efficiency over baselines. These results highlight the benefits of structure-aware adaptation for efficient, scalable multilingual ASR.

</details>


### [152] [MiNER: A Two-Stage Pipeline for Metadata Extraction from Municipal Meeting Minutes](https://arxiv.org/abs/2602.00316)
*Rodrigo Batista,Luís Filipe Cunha,Purificação Silvano,Nuno Guimarães,Alípio Jorge,Evelin Amorim,Ricardo Campos*

Main category: cs.CL

Relevance: 65.0

TL;DR: 提出两阶段流水线从市政会议纪要中提取元数据：首先用QA模型定位元数据段落，然后用Transformer模型进行细粒度实体提取，并评估开源与闭源LLM的性能、推理成本和碳足迹。


<details>
  <summary>Details</summary>
Motivation: 市政会议纪要作为地方治理的官方文件，格式和写作风格异质性强，元数据（会议编号、日期、地点、参与者等）缺乏标准化且难以自动提取。现有NER模型不适应这种领域特定类别，需要专门解决方案。

Method: 两阶段流水线：1) QA模型识别包含元数据的开头和结尾文本段；2) 使用BERTimbau和XLM-RoBERTa（带/不带CRF层）进行细粒度实体提取，并通过去词汇化增强。评估开源（Phi）和闭源（Gemini）LLM的性能、推理成本和碳足迹。

Result: 在领域内表现优于更大的通用LLM，但跨市政评估显示泛化能力有限，反映了市政记录的变异性和语言复杂性。建立了市政会议纪要元数据提取的首个基准。

Conclusion: 该工作为市政会议纪要元数据提取提供了首个基准，为未来研究奠定基础。虽然领域内表现良好，但跨市政泛化能力有限，需要进一步研究处理市政记录的变异性和复杂性。

Abstract: Municipal meeting minutes are official documents of local governance, exhibiting heterogeneous formats and writing styles. Effective information retrieval (IR) requires identifying metadata such as meeting number, date, location, participants, and start/end times, elements that are rarely standardized or easy to extract automatically. Existing named entity recognition (NER) models are ill-suited to this task, as they are not adapted to such domain-specific categories. In this paper, we propose a two-stage pipeline for metadata extraction from municipal minutes. First, a question answering (QA) model identifies the opening and closing text segments containing metadata. Transformer-based models (BERTimbau and XLM-RoBERTa with and without a CRF layer) are then applied for fine-grained entity extraction and enhanced through deslexicalization. To evaluate our proposed pipeline, we benchmark both open-weight (Phi) and closed-weight (Gemini) LLMs, assessing predictive performance, inference cost, and carbon footprint. Our results demonstrate strong in-domain performance, better than larger general-purpose LLMs. However, cross-municipality evaluation reveals reduced generalization reflecting the variability and linguistic complexity of municipal records. This work establishes the first benchmark for metadata extraction from municipal meeting minutes, providing a solid foundation for future research in this domain.

</details>


### [153] [Clause-Internal or Clause-External? Testing Turkish Reflexive Binding in Adapted versus Chain of Thought Large Language Models](https://arxiv.org/abs/2602.00380)
*Sercan Karakaş*

Main category: cs.CL

Relevance: 65.0

TL;DR: 该研究评估了最先进的大语言模型是否能够捕捉土耳其语反身代词的约束关系，通过构建100个平衡句子测试两个系统：OpenAI的链式思维推理模型和基于LLaMA-2的土耳其语微调模型Trendyol-LLM-7B-base-v0.1。


<details>
  <summary>Details</summary>
Motivation: 研究动机是评估大语言模型对土耳其语反身代词约束关系的理解能力，特别是测试模型是否能够正确区分局部约束和长距离约束，这对于理解模型的语言结构知识具有重要意义。

Method: 构建了100个平衡句子集，对比局部先行词和非局部先行词对反身代词kendi和kendisi的约束关系。测试了两个系统：OpenAI的链式思维推理模型和基于LLaMA-2的土耳其语微调模型Trendyol-LLM-7B-base-v0.1。采用句子级困惑度和强制选择范式相结合的方法评估先行词选择。

Result: Trendyol-LLM在约70%的试验中偏好局部约束，表现出强烈的局部性偏误；而o1 Mini在局部和长距离解读之间的选择几乎均匀分布，显示出两个系统在约束行为上的显著差异。

Conclusion: 不同的大语言模型在土耳其语反身代词约束关系的处理上表现出显著差异，模型架构和训练数据对语言结构知识的获取有重要影响，这为理解模型的语言能力提供了重要见解。

Abstract: This study evaluates whether state-of-the-art large language models capture the binding relations of Turkish reflexive pronouns. We construct a balanced set of 100 sentences that pit local against non-local antecedents for the reflexives kendi and kendisi, and test two contrasting systems: an OpenAI chain-of-thought model designed for multi-step reasoning and Trendyol-LLM-7B-base-v0.1, a LLaMA-2-derived model extensively fine-tuned on Turkish data. Antecedent choice is assessed using a combined sentence-level perplexity and forced-choice paradigm. Trendyol-LLM favours local bindings in approximately 70% of trials, exhibiting a strong locality bias, whereas o1 Mini distributes its choices almost evenly between local and long-distance readings, revealing a marked contrast in binding behaviour across the two systems.

</details>


### [154] [Hermes the Polyglot: A Unified Framework to Enhance Expressiveness for Multimodal Interlingual Subtitling](https://arxiv.org/abs/2602.00597)
*Chaoqun Cui,Shijing Wang,Liangbin Huang,Qingqing Gu,Zhaolong Huang,Xiao Zeng,Wenji Mao*

Main category: cs.CL

Relevance: 65.0

TL;DR: Hermes是一个基于LLM的自动字幕翻译框架，通过说话人分割、术语识别和表达增强三个模块解决字幕翻译中的语义连贯性、代词术语翻译和表达力等挑战。


<details>
  <summary>Details</summary>
Motivation: 跨语言字幕翻译在娱乐本地化中很重要，但尚未在机器翻译中得到充分探索。LLMs虽然显著提升了机器翻译能力，但字幕文本的独特特性（语义连贯性、代词术语翻译、表达力）仍带来持续挑战。

Method: 提出Hermes框架，包含三个模块：1) 说话人分割模块处理说话人识别和归属；2) 术语识别模块处理专业术语翻译；3) 表达增强模块提升翻译的表达力和自然度。

Result: 实验表明Hermes实现了最先进的说话人分割性能，并生成了表达力强、上下文连贯的翻译，推动了跨语言字幕翻译的研究进展。

Conclusion: Hermes框架有效解决了字幕翻译中的关键挑战，为基于LLM的跨语言字幕翻译提供了系统化解决方案，展示了LLMs在专业翻译任务中的应用潜力。

Abstract: Interlingual subtitling, which translates subtitles of visual media into a target language, is essential for entertainment localization but has not yet been explored in machine translation. Although Large Language Models (LLMs) have significantly advanced the general capabilities of machine translation, the distinctive characteristics of subtitle texts pose persistent challenges in interlingual subtitling, particularly regarding semantic coherence, pronoun and terminology translation, and translation expressiveness. To address these issues, we present Hermes, an LLM-based automated subtitling framework. Hermes integrates three modules: Speaker Diarization, Terminology Identification, and Expressiveness Enhancement, which effectively tackle the above challenges. Experiments demonstrate that Hermes achieves state-of-the-art diarization performance and generates expressive, contextually coherent translations, thereby advancing research in interlingual subtitling.

</details>


### [155] [ExperienceWeaver: Optimizing Small-sample Experience Learning for LLM-based Clinical Text Improvement](https://arxiv.org/abs/2602.00740)
*Ziyan Xiao,Yinghao Zhu,Liang Peng,Lequan Yu*

Main category: cs.CL

Relevance: 65.0

TL;DR: ExperienceWeaver：一个用于临床文本改进的分层框架，通过将多维度反馈提炼为结构化知识（技巧和策略），在小样本设置中超越现有方法。


<details>
  <summary>Details</summary>
Motivation: 临床文本改进对医疗效率至关重要，但面临高质量数据有限和医学文档复杂约束的挑战。现有LLM方法在小样本设置中表现不佳：监督微调需要大量数据且成本高，检索增强生成通常只能提供表面修正而无法捕捉修订背后的推理过程。

Method: 提出ExperienceWeaver分层框架，将重点从数据检索转向经验学习。该方法将嘈杂的多维度反馈提炼为结构化、可操作的知识，包括错误特定的技巧和高层次的策略。通过将这些提炼的经验注入到智能代理流程中，模型学习"如何修订"而不仅仅是"修订什么"。

Result: 在四个临床数据集上的广泛评估表明，ExperienceWeaver持续提升性能，在小样本设置中超越了Gemini-3 Pro等最先进模型。

Conclusion: ExperienceWeaver通过经验学习而非简单检索的方法，有效解决了临床文本改进中的小样本学习问题，为LLM在医疗领域的应用提供了新思路。

Abstract: Clinical text improvement is vital for healthcare efficiency but remains difficult due to limited high-quality data and the complex constraints of medical documentation. While Large Language Models (LLMs) show promise, current approaches struggle in small-sample settings: supervised fine-tuning is data-intensive and costly, while retrieval-augmented generation often provides superficial corrections without capturing the reasoning behind revisions. To address these limitations, we propose ExperienceWeaver, a hierarchical framework that shifts the focus from data retrieval to experience learning. Instead of simply recalling past examples, ExperienceWeaver distills noisy, multi-dimensional feedback into structured, actionable knowledge. Specifically, error-specific Tips and high-level Strategies. By injecting this distilled experience into an agentic pipeline, the model learns "how to revise" rather than just "what to revise". Extensive evaluations across four clinical datasets demonstrate that ExperienceWeaver consistently improves performance, surpassing state-of-the-art models such as Gemini-3 Pro in small-sample settings.

</details>


### [156] [Do Schwartz Higher-Order Values Help Sentence-Level Human Value Detection? When Hard Gating Hurts](https://arxiv.org/abs/2602.00913)
*Víctor Yeste,Paolo Rosso*

Main category: cs.CL

Relevance: 65.0

TL;DR: 研究在计算受限条件下，探索Schwartz高阶类别是否能为句子级人类价值检测提供有用结构，发现硬性层次约束会降低性能，而标签阈值调优和轻量集成更有效。


<details>
  <summary>Details</summary>
Motivation: 句子级人类价值检测通常被建模为Schwartz价值观的多标签分类问题，但尚不清楚Schwartz高阶类别是否提供可用结构。研究在严格计算预算下（单8GB GPU）探索这一问题。

Method: 在ValueEval'24/ValuesML数据集上比较：(1)直接监督Transformer，(2)硬掩码强制层次结构的HO→价值观管道，(3)存在→HO→价值观级联，同时结合低成本附加组件（词典、短上下文、主题）、标签阈值调优、小型指令调优LLM基线（≤10B）、QLoRA和简单集成。

Result: HO类别可从单句学习，但硬性层次门控不可靠，常通过错误累积和召回抑制降低最终任务性能。标签阈值调优是高效杠杆（Macro-F1提升达+0.05），小型Transformer集成提供最一致的额外增益（达+0.02）。小型LLM作为独立系统落后于监督编码器，但可在跨家族集成中提供互补错误。

Conclusion: HO结构在描述上有用，但用硬门控强制实施会损害句子级价值检测；稳健改进来自校准和轻量集成。

Abstract: Sentence-level human value detection is typically framed as multi-label classification over Schwartz values, but it remains unclear whether Schwartz higher-order (HO) categories provide usable structure. We study this under a strict compute-frugal budget (single 8 GB GPU) on ValueEval'24 / ValuesML (74K English sentences). We compare (i) direct supervised transformers, (ii) HO$\rightarrow$values pipelines that enforce the hierarchy with hard masks, and (iii) Presence$\rightarrow$HO$\rightarrow$values cascades, alongside low-cost add-ons (lexica, short context, topics), label-wise threshold tuning, small instruction-tuned LLM baselines ($\le$10B), QLoRA, and simple ensembles. HO categories are learnable from single sentences (e.g., the easiest bipolar pair reaches Macro-$F_1\approx0.58$), but hard hierarchical gating is not a reliable win: it often reduces end-task Macro-$F_1$ via error compounding and recall suppression. In contrast, label-wise threshold tuning is a high-leverage knob (up to $+0.05$ Macro-$F_1$), and small transformer ensembles provide the most consistent additional gains (up to $+0.02$ Macro-$F_1$). Small LLMs lag behind supervised encoders as stand-alone systems, yet can contribute complementary errors in cross-family ensembles. Overall, HO structure is useful descriptively, but enforcing it with hard gates hurts sentence-level value detection; robust improvements come from calibration and lightweight ensembling.

</details>


### [157] [Tendem: A Hybrid AI+Human Platform](https://arxiv.org/abs/2602.01119)
*Konstantin Chernyshev,Ekaterina Artemova,Viacheslav Zhukov,Maksim Nerush,Mariia Fedorova,Iryna Repik,Olga Shapovalova,Aleksey Sukhorosov,Vladimir Dobrovolskii,Natalia Mikhailova,Sergei Tilga*

Main category: cs.CL

Relevance: 65.0

TL;DR: Tendem是一个混合AI-人类系统，AI处理结构化重复工作，人类专家在模型失败时介入并验证结果。在94个真实任务评估中，相比纯AI代理和纯人类工作流，Tendem提供更高质量输出、更快周转时间，且成本与纯人类执行相当。


<details>
  <summary>Details</summary>
Motivation: 当前AI系统在处理复杂任务时仍存在局限性，需要人类专家介入以确保质量和可靠性。研究旨在开发一个混合AI-人类协作系统，结合AI的效率与人类的专业判断，实现高质量、高效率的任务执行。

Method: 开发Tendem混合系统：AI处理结构化重复工作，人类专家在模型失败时介入并验证结果。所有结果在交付前经过全面质量审查。在94个真实世界任务上进行评估，比较Tendem与纯AI代理和Upwork自由职业者纯人类工作流。

Result: 1. Tendem在94个真实任务评估中，相比纯AI代理和纯人类工作流，提供更高质量输出和更快周转时间；2. 运营成本与纯人类执行相当；3. Tendem的纯AI代理在第三方基准测试中，在网页浏览和工具使用任务上接近SOTA，在领域知识和推理方面表现强劲。

Conclusion: AI-人类混合系统Tendem能够显著提高任务执行质量和效率，同时保持成本竞争力。这种混合方法为AI系统在实际应用中的部署提供了有前景的解决方案。

Abstract: Tendem is a hybrid system where AI handles structured, repeatable work and Human Experts step in when the models fail or to verify results. Each result undergoes a comprehensive quality review before delivery to the Client. To assess Tendem's performance, we conducted a series of in-house evaluations on 94 real-world tasks, comparing it with AI-only agents and human-only workflows carried out by Upwork freelancers. The results show that Tendem consistently delivers higher-quality outputs with faster turnaround times. At the same time, its operational costs remain comparable to human-only execution. On third-party agentic benchmarks, Tendem's AI Agent (operating autonomously, without human involvement) performs near state-of-the-art on web browsing and tool-use tasks while demonstrating strong results in frontier domain knowledge and reasoning.

</details>


### [158] [Long-range Modeling and Processing of Multimodal Event Sequences](https://arxiv.org/abs/2602.01125)
*Jichu Li,Yilun Zhong,Zhiting Li,Feng Zhou,Quyu Kong*

Main category: cs.CL

Relevance: 65.0

TL;DR: 本文提出了一种新颖的框架，将基于LLM的时间点过程扩展到视觉模态，通过自适应序列压缩机制解决长上下文问题，在预测准确性和文本分析质量方面优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有时间点过程方法在处理多模态数据时面临序列长度急剧增加的问题，导致基于注意力的模型难以生成需要长距离理解的连贯长文本描述。需要解决多模态内容生成和事件动态推理的挑战。

Method: 提出基于LLM的时间点过程扩展框架，将文本生成作为核心能力。采用基于时间相似性的自适应序列压缩机制减少序列长度，同时保留关键模式。采用两阶段范式：先在压缩序列上进行预训练，然后在下游任务上进行监督微调。

Result: 在包括DanmakuTPP-QA基准测试在内的广泛实验中，该方法在预测准确性和生成文本分析质量方面均优于最先进的基线方法。

Conclusion: 该框架成功将基于LLM的时间点过程扩展到视觉模态，通过自适应压缩机制有效解决了长上下文问题，在多模态事件序列建模方面取得了显著进展。

Abstract: Temporal point processes (TPPs) have emerged as powerful tools for modeling asynchronous event sequences. While recent advances have extended TPPs to handle textual information, existing approaches are limited in their ability to generate rich, multimodal content and reason about event dynamics. A key challenge is that incorporating multimodal data dramatically increases sequence length, hindering the ability of attention-based models to generate coherent, long-form textual descriptions that require long-range understanding. In this paper, we propose a novel framework that extends LLM-based TPPs to the visual modality, positioning text generation as a core capability alongside time and type prediction. Our approach addresses the long-context problem through an adaptive sequence compression mechanism based on temporal similarity, which reduces sequence length while preserving essential patterns. We employ a two-stage paradigm of pre-training on compressed sequences followed by supervised fine-tuning for downstream tasks. Extensive experiments, including on the challenging DanmakuTPP-QA benchmark, demonstrate that our method outperforms state-of-the-art baselines in both predictive accuracy and the quality of its generated textual analyses.

</details>


### [159] [PedagoSense: A Pedology Grounded LLM System for Pedagogical Strategy Detection and Contextual Response Generation in Learning Dialogues](https://arxiv.org/abs/2602.01169)
*Shahem Sultan,Shahem Fadi,Yousef Melhim,Ibrahim Alsarraj,Besher Hassan*

Main category: cs.CL

Relevance: 65.0

TL;DR: PedagoSense：一个结合两阶段策略分类器和LLM生成的教育对话系统，用于检测和推荐教学策略，提升对话式学习质量


<details>
  <summary>Details</summary>
Motivation: 解决对话式学习中教学策略检测和生成的挑战，将教育学理论与实际LLM响应生成相结合，开发更自适应的教育技术

Method: 1) 两阶段策略分类器：先检测是否存在教学策略（二元分类），再进行细粒度策略识别；2) 并行推荐系统：从对话上下文推荐适当策略；3) LLM生成：基于推荐策略生成对齐的响应

Result: 在人工标注的师生对话数据集上表现优异，教学策略检测性能高，数据增强带来持续增益，但细粒度分类仍具挑战性

Conclusion: PedagoSense成功连接了教育学理论和实际LLM响应生成，为自适应教育技术提供了有效框架

Abstract: This paper addresses the challenge of improving interaction quality in dialogue based learning by detecting and recommending effective pedagogical strategies in tutor student conversations. We introduce PedagoSense, a pedology grounded system that combines a two stage strategy classifier with large language model generation. The system first detects whether a pedagogical strategy is present using a binary classifier, then performs fine grained classification to identify the specific strategy. In parallel, it recommends an appropriate strategy from the dialogue context and uses an LLM to generate a response aligned with that strategy. We evaluate on human annotated tutor student dialogues, augmented with additional non pedagogical conversations for the binary task. Results show high performance for pedagogical strategy detection and consistent gains when using data augmentation, while analysis highlights where fine grained classes remain challenging. Overall, PedagoSense bridges pedagogical theory and practical LLM based response generation for more adaptive educational technologies.

</details>


### [160] [BBPE16: UTF-16-based byte-level byte-pair encoding for improved multilingual speech recognition](https://arxiv.org/abs/2602.01717)
*Hyunsik Kim,Haeri Kim,Munhak Lee,Kyungmin Lee*

Main category: cs.CL

Relevance: 65.0

TL;DR: BBPE16：基于UTF-16的字节级BPE分词器，为多语言ASR提供更高效的跨语言标记共享，减少CJK语言的标记序列长度和计算开销。


<details>
  <summary>Details</summary>
Motivation: 当前广泛使用的UTF-8字节级BPE分词器虽然语言无关且支持全Unicode，但对非拉丁文字（如中日韩）采用变长编码，导致标记序列过长，增加了计算负载和内存使用。

Method: 提出BBPE16分词器，基于UTF-16编码，使大多数现代文字系统使用统一的2字节代码单元，保持BBPE的语言无关特性，同时显著改善跨语言标记共享。

Result: 在单语、双语、三语ASR以及多语言持续学习设置中，BBPE16获得相当或更好的准确率；对中文减少标记数量达10.4%，解码迭代减少达10.3%，加速微调和推理，降低内存使用。

Conclusion: BBPE16是多语言ASR的实用分词选择，通过更高效的跨语言标记共享和减少CJK语言的序列长度，在保持准确性的同时降低计算开销。

Abstract: Multilingual automatic speech recognition (ASR) requires tokenization that efficiently covers many writing systems. Byte-level BPE (BBPE) using UTF-8 is widely adopted for its language-agnostic design and full Unicode coverage, but its variable-length encoding inflates token sequences for non-Latin scripts, such as Chinese, Japanese, and Korean (CJK). Longer sequences increase computational load and memory use. We propose BBPE16, a UTF-16-based BBPE tokenizer that represents most modern scripts with a uniform 2-byte code unit. BBPE16 preserves BBPE's language-agnostic properties while substantially improving cross-lingual token sharing. Across monolingual, bilingual, and trilingual ASR, and in a multilingual continual-learning setup, BBPE16 attains comparable or better accuracy; for Chinese, it reduces token counts by up to 10.4% and lowers decoding iterations by up to 10.3%. These reductions speed up fine-tuning and inference and decrease memory usage, making BBPE16 a practical tokenization choice for multilingual ASR.

</details>


### [161] [From Code-Centric to Concept-Centric: Teaching NLP with LLM-Assisted "Vibe Coding"](https://arxiv.org/abs/2602.01919)
*Hend Al-Khalifa*

Main category: cs.CL

Relevance: 65.0

TL;DR: Vibe Coding是一种利用LLM作为编程助手的教学方法，在保持概念理解和批判性思维的同时，通过反思性评估将重点从语法熟练度转向概念掌握。


<details>
  <summary>Details</summary>
Motivation: LLM的快速发展为NLP教育带来了挑战和机遇。传统编程教学过于关注语法细节，而Vibe Coding旨在利用LLM辅助编程，让学生更专注于NLP概念理解和批判性思维，为AI增强的专业环境做准备。

Method: 在高级本科NLP课程中实施Vibe Coding方法，学生使用LLM完成7个实验的代码生成，但主要通过批判性反思问题进行概念理解评估。采用强制提示记录和反思性评估结构。

Result: 19名学生的课程反馈显示高满意度（平均分4.4-4.6/5.0），学生特别赞赏调试认知负荷减少，能更专注于NLP概念。但也面临时间限制、LLM输出验证和任务规范清晰度等挑战。

Conclusion: 当采用强制提示记录和基于反思的评估结构时，LLM辅助学习可以将重点从语法熟练度转向概念掌握，有效准备学生适应AI增强的专业环境。

Abstract: The rapid advancement of Large Language Models (LLMs) presents both challenges and opportunities for Natural Language Processing (NLP) education. This paper introduces ``Vibe Coding,'' a pedagogical approach that leverages LLMs as coding assistants while maintaining focus on conceptual understanding and critical thinking. We describe the implementation of this approach in a senior-level undergraduate NLP course, where students completed seven labs using LLMs for code generation while being assessed primarily on conceptual understanding through critical reflection questions. Analysis of end-of-course feedback from 19 students reveals high satisfaction (mean scores 4.4-4.6/5.0) across engagement, conceptual learning, and assessment fairness. Students particularly valued the reduced cognitive load from debugging, enabling deeper focus on NLP concepts. However, challenges emerged around time constraints, LLM output verification, and the need for clearer task specifications. Our findings suggest that when properly structured with mandatory prompt logging and reflection-based assessment, LLM-assisted learning can shift focus from syntactic fluency to conceptual mastery, preparing students for an AI-augmented professional landscape.

</details>


### [162] [Mixture-of-Experts with Intermediate CTC Supervision for Accented Speech Recognition](https://arxiv.org/abs/2602.01967)
*Wonjun Lee,Hyounghun Kim,Gary Geunbae Lee*

Main category: cs.CL

Relevance: 65.0

TL;DR: Moe-Ctc：一种用于口音鲁棒性语音识别的混合专家架构，通过中间CTC监督促进专家专业化和泛化，在低资源和高资源条件下对可见和未见口音均取得显著性能提升。


<details>
  <summary>Details</summary>
Motivation: 当前ASR系统主要基于少数高资源英语变体训练，导致对其他口音的性能严重下降。口音无关方法对重口音或未见口音效果有限，而口音特定方法依赖有限且通常嘈杂的标签数据。

Method: 提出Moe-Ctc架构：1）混合专家架构，训练时使用口音感知路由鼓励专家捕获口音特定模式，推理时转为无标签路由；2）每个专家配备独立CTC头，将路由与转录质量对齐；3）路由增强损失进一步稳定优化。

Result: 在Mcv-Accent基准测试中，在低资源和高资源条件下对可见和未见口音均取得一致性能提升，相比强FastConformer基线实现高达29.3%的相对WER降低。

Conclusion: Moe-Ctc通过混合专家架构和中间CTC监督有效解决了口音鲁棒性ASR问题，在促进专家专业化的同时保持泛化能力，为处理多样口音提供了有效解决方案。

Abstract: Accented speech remains a persistent challenge for automatic speech recognition (ASR), as most models are trained on data dominated by a few high-resource English varieties, leading to substantial performance degradation for other accents. Accent-agnostic approaches improve robustness yet struggle with heavily accented or unseen varieties, while accent-specific methods rely on limited and often noisy labels. We introduce Moe-Ctc, a Mixture-of-Experts architecture with intermediate CTC supervision that jointly promotes expert specialization and generalization. During training, accent-aware routing encourages experts to capture accent-specific patterns, which gradually transitions to label-free routing for inference. Each expert is equipped with its own CTC head to align routing with transcription quality, and a routing-augmented loss further stabilizes optimization. Experiments on the Mcv-Accent benchmark demonstrate consistent gains across both seen and unseen accents in low- and high-resource conditions, achieving up to 29.3% relative WER reduction over strong FastConformer baselines.

</details>


### [163] [WildGraphBench: Benchmarking GraphRAG with Wild-Source Corpora](https://arxiv.org/abs/2602.02053)
*Pengyu Wang,Benfeng Xu,Licheng Zhang,Shaohan Wang,Mingxuan Du,Chiwei Zhu,Zhendong Mao*

Main category: cs.CL

Relevance: 65.0

TL;DR: WildGraphBench是一个专门评估GraphRAG在真实场景下性能的基准测试，利用维基百科的长文档和异构引用构建，包含1100个问题，涵盖三种复杂度级别。


<details>
  <summary>Details</summary>
Motivation: 现有GraphRAG基准测试大多使用简短、精选的段落作为外部知识，无法充分评估系统在真实场景（长上下文、大规模异构文档）下的性能，需要更贴近实际应用的评估框架。

Method: 利用维基百科的结构特点：将连贯叙述基于长且异构的外部参考文献。从12个顶级主题中采样文章，使用外部参考文献作为检索语料库，引用链接的陈述作为真实标签，构建包含1100个问题的基准测试，涵盖单事实QA、多事实QA和章节级摘要三个复杂度级别。

Result: 实验表明，当前GraphRAG管道在证据来自中等数量来源时有助于多事实聚合，但这种聚合范式可能过度强调高层级陈述而牺牲细粒度细节，导致在摘要任务上表现较弱。

Conclusion: WildGraphBench填补了GraphRAG在真实场景评估的空白，揭示了当前GraphRAG方法在平衡高层聚合与细节保留方面的局限性，为未来研究提供了重要基准。

Abstract: Graph-based Retrieval-Augmented Generation (GraphRAG) organizes external knowledge as a hierarchical graph, enabling efficient retrieval and aggregation of scattered evidence across multiple documents. However, many existing benchmarks for GraphRAG rely on short, curated passages as external knowledge, failing to adequately evaluate systems in realistic settings involving long contexts and large-scale heterogeneous documents. To bridge this gap, we introduce WildGraphBench, a benchmark designed to assess GraphRAG performance in the wild. We leverage Wikipedia's unique structure, where cohesive narratives are grounded in long and heterogeneous external reference documents, to construct a benchmark reflecting real-word scenarios. Specifically, we sample articles across 12 top-level topics, using their external references as the retrieval corpus and citation-linked statements as ground truth, resulting in 1,100 questions spanning three levels of complexity: single-fact QA, multi-fact QA, and section-level summarization. Experiments across multiple baselines reveal that current GraphRAG pipelines help on multi-fact aggregation when evidence comes from a moderate number of sources, but this aggregation paradigm may overemphasize high-level statements at the expense of fine-grained details, leading to weaker performance on summarization tasks. Project page:https://github.com/BstWPY/WildGraphBench.

</details>


### [164] [Hallucination or Creativity: How to Evaluate AI-Generated Scientific Stories?](https://arxiv.org/abs/2602.02290)
*Alex Argese,Pasquale Lisena,Raphaël Troncy*

Main category: cs.CL

Relevance: 65.0

TL;DR: StoryScore：一个评估AI生成科学故事的综合指标，整合语义对齐、词汇基础、叙事控制、结构保真度、冗余避免和实体级幻觉检测


<details>
  <summary>Details</summary>
Motivation: 现有评估方法难以衡量科学故事叙述的质量。标准摘要指标无法捕捉故事叙述所需的抽象、简化和教学创造性，而幻觉检测器经常将合法的叙事重构误分类，或在涉及创造性时表现不稳定。

Method: 提出StoryScore综合指标，将语义对齐、词汇基础、叙事控制、结构保真度、冗余避免和实体级幻觉检测集成到统一框架中。分析揭示了现有幻觉检测方法难以区分教学创造性与事实错误的关键局限。

Result: 开发了一个统一的评估框架，能够更全面地评估AI生成科学故事的质量，特别是解决了现有方法在区分创造性叙事与事实错误方面的不足。

Conclusion: 自动指标能有效评估与原始内容的语义相似性，但难以评估内容的叙述和控制方式。StoryScore为科学故事叙述评估提供了更全面的解决方案。

Abstract: Generative AI can turn scientific articles into narratives for diverse audiences, but evaluating these stories remains challenging. Storytelling demands abstraction, simplification, and pedagogical creativity-qualities that are not often well-captured by standard summarization metrics. Meanwhile, factual hallucinations are critical in scientific contexts, yet, detectors often misclassify legitimate narrative reformulations or prove unstable when creativity is involved. In this work, we propose StoryScore, a composite metric for evaluating AI-generated scientific stories. StoryScore integrates semantic alignment, lexical grounding, narrative control, structural fidelity, redundancy avoidance, and entity-level hallucination detection into a unified framework. Our analysis also reveals why many hallucination detection methods fail to distinguish pedagogical creativity from factual errors, highlighting a key limitation: while automatic metrics can effectively assess semantic similarity with original content, they struggle to evaluate how it is narrated and controlled.

</details>


### [165] [A Large-Scale Dataset for Molecular Structure-Language Description via a Rule-Regularized Method](https://arxiv.org/abs/2602.02320)
*Feiyang Cai,Guijuan He,Yi Hu,Jingjing Wang,Joshua Luo,Tianyu Zhu,Srikanth Pilla,Gang Li,Ling Liu,Feng Luo*

Main category: cs.CL

Relevance: 65.0

TL;DR: 提出自动化分子结构描述标注框架，通过解析IUPAC名称生成结构化XML元数据，指导LLM生成准确的自然语言描述，构建了16.3万分子-描述对数据集，精度达98.6%


<details>
  <summary>Details</summary>
Motivation: 分子功能主要由结构决定，准确对齐分子结构与自然语言对LLM推理化学任务至关重要。但人工标注成本高昂，难以构建大规模高质量的结构-描述数据集

Method: 基于规则扩展的化学命名法解析器，将IUPAC名称解释为丰富的结构化XML元数据，明确编码分子结构，然后使用该元数据指导LLM生成准确的自然语言描述

Result: 构建了约16.3万个分子-描述对的大规模数据集，在2000个分子子集上通过LLM和专家人工评估验证，描述精度达到98.6%

Conclusion: 该数据集为未来分子-语言对齐提供了可靠基础，提出的标注方法易于扩展到更大数据集和依赖结构描述的更广泛化学任务

Abstract: Molecular function is largely determined by structure. Accurately aligning molecular structure with natural language is therefore essential for enabling large language models (LLMs) to reason about downstream chemical tasks. However, the substantial cost of human annotation makes it infeasible to construct large-scale, high-quality datasets of structure-grounded descriptions. In this work, we propose a fully automated annotation framework for generating precise molecular structure descriptions at scale. Our approach builds upon and extends a rule-based chemical nomenclature parser to interpret IUPAC names and construct enriched, structured XML metadata that explicitly encodes molecular structure. This metadata is then used to guide LLMs in producing accurate natural-language descriptions. Using this framework, we curate a large-scale dataset of approximately $163$k molecule-description pairs. A rigorous validation protocol combining LLM-based and expert human evaluation on a subset of $2,000$ molecules demonstrates a high description precision of $98.6\%$. The resulting dataset provides a reliable foundation for future molecule-language alignment, and the proposed annotation method is readily extensible to larger datasets and broader chemical tasks that rely on structural descriptions.

</details>


### [166] [Culinary Crossroads: A RAG Framework for Enhancing Diversity in Cross-Cultural Recipe Adaptation](https://arxiv.org/abs/2507.21934)
*Tianyi Hu,Andrea Morales-Garzón,Jingyi Zheng,Maria Maistro,Daniel Hershcovich*

Main category: cs.CL

Relevance: 65.0

TL;DR: CARRIAGE：首个专注于生成多样化输出的RAG框架，用于跨文化食谱改编，通过增强检索和上下文组织的多样性来解决传统RAG在创意任务中输出单一的问题。


<details>
  <summary>Details</summary>
Motivation: 传统RAG在跨文化食谱改编任务中过度依赖有限的上下文，无法利用多样化的上下文输入生成多样化的输出，这在需要多种有效答案的创意任务中是一个关键限制。

Method: 提出CARRIAGE框架，这是一个即插即用的RAG框架，通过增强检索和上下文组织的多样性来生成多样化的食谱改编结果。

Result: 实验表明CARRIAGE在食谱改编的多样性和质量方面实现了帕累托效率，优于闭卷LLMs。

Conclusion: CARRIAGE是首个明确旨在生成高度多样化输出的RAG框架，能够满足多种用户偏好，解决了传统RAG在创意任务中的多样性限制问题。

Abstract: In cross-cultural recipe adaptation, the goal is not only to ensure cultural appropriateness and retain the original dish's essence, but also to provide diverse options for various dietary needs and preferences. Retrieval Augmented Generation (RAG) is a promising approach, combining the retrieval of real recipes from the target cuisine for cultural adaptability with large language models (LLMs) for relevance. However, it remains unclear whether RAG can generate diverse adaptation results. Our analysis shows that RAG tends to overly rely on a limited portion of the context across generations, failing to produce diverse outputs even when provided with varied contextual inputs. This reveals a key limitation of RAG in creative tasks with multiple valid answers: it fails to leverage contextual diversity for generating varied responses. To address this issue, we propose CARRIAGE, a plug-and-play RAG framework for cross-cultural recipe adaptation that enhances diversity in both retrieval and context organization. To our knowledge, this is the first RAG framework that explicitly aims to generate highly diverse outputs to accommodate multiple user preferences. Our experiments show that CARRIAGE achieves Pareto efficiency in terms of diversity and quality of recipe adaptation compared to closed-book LLMs.

</details>


### [167] [SafeTalkCoach: Diversity-Driven Multi-Agent Simulation for Parent-Teen Health Conversations](https://arxiv.org/abs/2602.00017)
*Benyamin Tabarsi,Wenbo Li,Tahreem Yasir,Aryan Santhosh Kumar,Laura Widman,Dongkuan Xu,Tiffany Barnes*

Main category: cs.CL

Relevance: 45.0

TL;DR: SafeTalkCoach是一个多样性驱动的多智能体对话生成框架，用于模拟亲子间关于性健康的对话，并附带相应数据集，旨在支持AI研究和健康沟通实践。


<details>
  <summary>Details</summary>
Motivation: 亲子间关于性健康的有效沟通很重要，但由于话题的私密性和敏感性，真实数据稀缺且难以收集。现有LLM生成的对话可能偏离最佳实践，缺乏真实性和多样性。

Method: SafeTalkCoach采用多样性驱动的多智能体对话生成框架，整合了众包和合成的场景、既定性健康指南、基于证据的人物设定、自适应控制模块和层次化多样化策略。

Result: 评估表明SafeTalkCoach能够生成多样化的对话，同时在实践中保持真实性、沟通质量和可控性。

Conclusion: SafeTalkCoach框架和数据集旨在支持AI研究和健康沟通实践，为敏感话题的对话生成提供了有效的解决方案。

Abstract: The importance of effective parent-child communication about sexual health is widely acknowledged, but real-world data on these conversations is scarce and challenging to collect, due to their private and sensitive nature. Although LLMs have been widely adopted in dialogue generation, they may deviate from best practices and frequently lack realism and diversity. We introduce SafeTalkCoach, a diversity-driven multi-agent dialogue generation framework that simulates parent-child conversations about sexual health, and present an accompanying dataset. SafeTalkCoach integrates crowd-sourced and synthesized scenarios, established sexual health guidelines, evidence-based personas, adaptive control modules, and hierarchical diversification. Through evaluations, we demonstrate that SafeTalkCoach generates diverse conversations while maintaining realism, communication quality, and controllability in practice. Our goal is that the SafeTalkCoach framework and the dataset support both AI research and health communications practices.

</details>


### [168] [Transformer-Based Model for Multilingual Hope Speech Detection](https://arxiv.org/abs/2602.00613)
*Nsrin Ashraf,Mariam Labib,Hamada Nayel*

Main category: cs.CL

Relevance: 45.0

TL;DR: 该论文描述了提交给RANLP2025 "PolyHope-M"任务的系统，使用RoBERTa和XLM-RoBERTa进行英语和德语希望言论检测，RoBERTa在英语上获得81.8%的准确率，XLM-RoBERTa获得78.5%的准确率。


<details>
  <summary>Details</summary>
Motivation: 希望言论检测是自然语言处理中的重要任务，旨在识别文本中表达希望、积极情绪的内容。该研究旨在评估预训练大语言模型在多语言希望言论检测任务上的性能。

Method: 使用两种Transformer模型：RoBERTa用于英语希望言论检测，XLM-RoBERTa用于英语和德语的多语言检测。系统提交到RANLP2025的PolyHope-M任务进行评估。

Result: RoBERTa在英语任务上获得加权F1分数0.818和准确率81.8%；XLM-RoBERTa获得加权F1分数0.786和准确率78.5%。结果表明预训练大语言模型能显著提升NLP任务性能。

Conclusion: 预训练大语言模型（如RoBERTa和XLM-RoBERTa）在多语言希望言论检测任务中表现良好，证明了这些模型在提升不同NLP任务性能方面的重要性。

Abstract: This paper describes a system that has been submitted to the "PolyHope-M" at RANLP2025. In this work various transformers have been implemented and evaluated for hope speech detection for English and Germany. RoBERTa has been implemented for English, while the multilingual model XLM-RoBERTa has been implemented for both English and German languages. The proposed system using RoBERTa reported a weighted f1-score of 0.818 and an accuracy of 81.8% for English. On the other hand, XLM-RoBERTa achieved a weighted f1-score of 0.786 and an accuracy of 78.5%. These results reflects the importance of improvement of pre-trained large language models and how these models enhancing the performance of different natural language processing tasks.

</details>


### [169] [Kanade: A Simple Disentangled Tokenizer for Spoken Language Modeling](https://arxiv.org/abs/2602.00594)
*Zhijie Huang,Stephen McIntosh,Daisuke Saito,Nobuaki Minematsu*

Main category: cs.CL

Relevance: 40.0

TL;DR: Kanade是一个单层解耦语音分词器，能够从连续语音信号中分离出音位和韵律信息，同时抑制说话人身份等非语言信息，实现高质量的语音合成。


<details>
  <summary>Details</summary>
Motivation: 语音建模需要处理混合语言和非语言信息的连续信号，一个好的语音分词器应该能够提取音位和韵律特征，抑制说话人身份等与语言无关的信息，并支持高质量合成。现有解耦编解码器通常依赖辅助方法，需要更简洁有效的解决方案。

Method: Kanade采用单层解耦架构，分离声学常量，创建单一token流来捕获丰富的音位和韵律信息。该方法不需要现有解耦编解码器常用的辅助方法，实现了更简洁的设计。

Result: 实验表明，Kanade在说话人解耦和词汇可用性方面达到最先进水平，同时保持了优秀的重建质量。

Conclusion: Kanade实现了理想的语音分词器设计，能够有效分离语言相关特征，为语音建模提供了高质量的token表示。

Abstract: A good language model starts with a good tokenizer. Tokenization is especially important for speech modeling, which must handle continuous signals that mix linguistic and non-linguistic information. A speech tokenizer should extract phonetics and prosody, suppress linguistically irrelevant information like speaker identity, and enable high-quality synthesis. We present Kanade, a single-layer disentangled speech tokenizer that realizes this ideal. Kanade separates out acoustic constants to create a single stream of tokens that captures rich phonetics and prosody. It does so without the need for auxiliary methods that existing disentangled codecs often rely on. Experiments show that Kanade achieves state-of-the-art speaker disentanglement and lexical availability, while maintaining excellent reconstruction quality.

</details>


### [170] [Words that make SENSE: Sensorimotor Norms in Learned Lexical Token Representations](https://arxiv.org/abs/2602.00469)
*Abhinav Gupta,Toben H. Mintz,Jesse Thomason*

Main category: cs.CL

Relevance: 35.0

TL;DR: SENSE模型通过学习投影将词嵌入映射到兰开斯特感觉运动规范，通过行为实验验证了模型预测与人类感觉运动关联的一致性，并发现了语音象征现象的模式。


<details>
  <summary>Details</summary>
Motivation: 传统词嵌入主要基于共现模式获取语义，但人类语言理解根植于感觉运动经验。研究旨在建立词嵌入与感觉运动经验之间的计算联系，探索语言如何编码身体经验。

Method: 开发SENSE投影模型，将词嵌入映射到兰开斯特感觉运动规范；进行行为实验，让281名参与者从候选新造词中选择具有特定感觉运动关联的词；进行子词汇分析，探索语音象征模式。

Result: SENSE模型能有效预测感觉运动规范；在11种感觉运动模态中，6种模态的人类选择率与SENSE评分存在显著相关；发现了内感受规范的语音象征模式，为从文本数据计算提出候选语音象征提供了路径。

Conclusion: 词嵌入可以编码感觉运动信息，SENSE模型为连接计算语义与具身认知提供了桥梁，语音象征分析为理解语言如何编码身体经验提供了新视角。

Abstract: While word embeddings derive meaning from co-occurrence patterns, human language understanding is grounded in sensory and motor experience. We present $\text{SENSE}$ $(\textbf{S}\text{ensorimotor }$ $\textbf{E}\text{mbedding }$ $\textbf{N}\text{orm }$ $\textbf{S}\text{coring }$ $\textbf{E}\text{ngine})$, a learned projection model that predicts Lancaster sensorimotor norms from word lexical embeddings. We also conducted a behavioral study where 281 participants selected which among candidate nonce words evoked specific sensorimotor associations, finding statistically significant correlations between human selection rates and $\text{SENSE}$ ratings across 6 of the 11 modalities. Sublexical analysis of these nonce words selection rates revealed systematic phonosthemic patterns for the interoceptive norm, suggesting a path towards computationally proposing candidate phonosthemes from text data.

</details>


### [171] [WordCraft: Scaffolding the Keyword Method for L2 Vocabulary Learning with Multimodal LLMs](https://arxiv.org/abs/2602.00762)
*Yuheng Shao,Junjie Xiong,Chaoran Wu,Xiyuan Wang,Ziyu Zhou,Yang Ouyang,Qinyi Tao,Quan Li*

Main category: cs.CL

Relevance: 35.0

TL;DR: WordCraft：基于多模态大语言模型的交互式工具，帮助中文母语英语学习者通过关键词法记忆词汇，解决关键词生成、关联构建和意象形成等挑战。


<details>
  <summary>Details</summary>
Motivation: 中文母语的英语学习者在词汇记忆中使用关键词法面临三大挑战：难以生成音韵合适的关键词、构建连贯关联、创造生动心理意象。现有方法要么完全自动化（牺牲学习者参与度），要么只关注结果（缺乏过程指导）。

Method: 1. 对18名学习者和教育者进行形成性研究，识别关键词法应用中的具体困难；2. 开发WordCraft工具，基于多模态大语言模型，通过交互式引导支持关键词选择、关联构建和意象形成三个核心步骤。

Result: 两项用户研究表明，WordCraft不仅保持了生成效应（学习者主动参与的价值），而且在效果和可用性方面都达到了高水平。

Conclusion: WordCraft通过结合多模态大语言模型的能力和学习者中心的交互设计，有效解决了关键词法在词汇记忆中的应用难题，为语言学习工具设计提供了新思路。

Abstract: Applying the keyword method for vocabulary memorization remains a significant challenge for L1 Chinese-L2 English learners. They frequently struggle to generate phonologically appropriate keywords, construct coherent associations, and create vivid mental imagery to aid long-term retention. Existing approaches, including fully automated keyword generation and outcome-oriented mnemonic aids, either compromise learner engagement or lack adequate process-oriented guidance. To address these limitations, we conducted a formative study with L1 Chinese-L2 English learners and educators (N=18), which revealed key difficulties and requirements in applying the keyword method to vocabulary learning. Building on these insights, we introduce WordCraft, a learner-centered interactive tool powered by Multimodal Large Language Models (MLLMs). WordCraft scaffolds the keyword method by guiding learners through keyword selection, association construction, and image formation, thereby enhancing the effectiveness of vocabulary memorization. Two user studies demonstrate that WordCraft not only preserves the generation effect but also achieves high levels of effectiveness and usability.

</details>


### [172] [ILSIC: Corpora for Identifying Indian Legal Statutes from Queries by Laypeople](https://arxiv.org/abs/2602.00881)
*Shounak Paul,Raghav Dogra,Pawan Goyal,Saptarshi Ghosh*

Main category: cs.CL

Relevance: 35.0

TL;DR: 构建了印度法律领域的ILSI-C语料库，包含500+法规的普通人查询和法庭判决，用于比较法庭与普通人数据在法律法规识别任务中的差异，发现仅用法庭数据训练的模型在普通人查询上效果不佳。


<details>
  <summary>Details</summary>
Motivation: 传统法律条文识别任务主要使用法庭判决事实作为输入，但实际应用中用户查询通常来自非专业人士的日常问题。现有研究缺乏对法庭数据与普通人数据差异的系统比较，需要构建一个包含两种数据类型的语料库来探索这一差异。

Method: 创建ILSI-C语料库，包含500+印度法规的普通人查询和对应的法庭判决。采用多种实验方法：零样本/少样本推理、检索增强生成和监督微调，比较模型在两种数据上的表现。

Result: 仅用法庭判决训练的模型在普通人查询测试中效果不佳。从法庭数据到普通人数据的迁移学习在某些场景下有益。通过查询类别和法规频率的细粒度分析揭示了性能差异。

Conclusion: 法庭数据与普通人数据在法律条文识别任务中存在显著差异，需要专门针对普通人查询的数据集和训练方法。ILSI-C语料库为这一研究方向提供了重要资源。

Abstract: Legal Statute Identification (LSI) for a given situation is one of the most fundamental tasks in Legal NLP. This task has traditionally been modeled using facts from court judgments as input queries, due to their abundance. However, in practical settings, the input queries are likely to be informal and asked by laypersons, or non-professionals. While a few laypeople LSI datasets exist, there has been little research to explore the differences between court and laypeople data for LSI. In this work, we create ILSIC, a corpus of laypeople queries covering 500+ statutes from Indian law. Additionally, the corpus also contains court case judgements to enable researchers to effectively compare between court and laypeople data for LSI. We conducted extensive experiments on our corpus, including benchmarking over the laypeople dataset using zero and few-shot inference, retrieval-augmented generation and supervised fine-tuning. We observe that models trained purely on court judgements are ineffective during test on laypeople queries, while transfer learning from court to laypeople data can be beneficial in certain scenarios. We also conducted fine-grained analyses of our results in terms of categories of queries and frequency of statutes.

</details>


### [173] [MedSpeak: A Knowledge Graph-Aided ASR Error Correction Framework for Spoken Medical QA](https://arxiv.org/abs/2602.00981)
*Yutong Song,Shiva Shrestha,Chenhan Lyu,Elahe Khatibi,Pengfei Zhang,Honghui Xu,Nikil Dutt,Amir Rahmani*

Main category: cs.CL

Relevance: 35.0

TL;DR: MedSpeak：基于知识图谱的ASR错误纠正框架，通过结合医学知识图谱的语义关系和语音信息，利用LLM推理能力改进医疗术语识别和口语问答性能


<details>
  <summary>Details</summary>
Motivation: 基于自动语音识别（ASR）的口语问答系统在医疗术语识别上存在困难，医疗术语的准确识别对医疗SQA系统至关重要

Method: 提出MedSpeak框架，利用医学知识图谱编码的语义关系和语音信息，结合LLM的推理能力，对噪声转录进行细化，改进下游答案预测

Result: 在基准测试中显著提高了医疗术语识别准确率和整体医疗SQA性能，成为医疗SQA的state-of-the-art解决方案

Conclusion: MedSpeak通过知识图谱辅助的ASR错误纠正，有效解决了医疗SQA中的术语识别问题，代码已开源

Abstract: Spoken question-answering (SQA) systems relying on automatic speech recognition (ASR) often struggle with accurately recognizing medical terminology. To this end, we propose MedSpeak, a novel knowledge graph-aided ASR error correction framework that refines noisy transcripts and improves downstream answer prediction by leveraging both semantic relationships and phonetic information encoded in a medical knowledge graph, together with the reasoning power of LLMs. Comprehensive experimental results on benchmarks demonstrate that MedSpeak significantly improves the accuracy of medical term recognition and overall medical SQA performance, establishing MedSpeak as a state-of-the-art solution for medical SQA. The code is available at https://github.com/RainieLLM/MedSpeak.

</details>


### [174] [Bridging Lexical Ambiguity and Vision: A Mini Review on Visual Word Sense Disambiguation](https://arxiv.org/abs/2602.01193)
*Shashini Nilukshi,Deshan Sumanathilaka*

Main category: cs.CL

Relevance: 35.0

TL;DR: 该论文对视觉词义消歧（VWSD）进行了综述，VWSD是多模态词义消歧方法，利用视觉线索解决词汇歧义问题。综述涵盖了从早期多模态融合方法到基于CLIP、扩散模型和LLM的新框架，展示了该领域从2016到2025年的发展。


<details>
  <summary>Details</summary>
Motivation: 传统词义消歧仅依赖文本和词汇资源，而VWSD通过视觉线索增强消歧能力，解决视觉-语言任务中的词汇歧义问题，特别是在文本输入有限的情况下。

Method: 综述分析了特征基、图基和对比嵌入技术，重点关注提示工程、微调和多语言适应。特别研究了基于CLIP的微调模型和LLM增强的VWSD系统。

Result: CLIP微调模型和LLM增强系统相比零样本基线有显著提升，在平均倒数排名（MRR）上获得6-8%的增益。但存在上下文限制、模型偏向常见含义、多语言数据集缺乏等挑战。

Conclusion: VWSD的未来方向是CLIP对齐、扩散生成和LLM推理的融合，以构建更强大、上下文感知和多语言的消歧系统。

Abstract: This paper offers a mini review of Visual Word Sense Disambiguation (VWSD), which is a multimodal extension of traditional Word Sense Disambiguation (WSD). VWSD helps tackle lexical ambiguity in vision-language tasks. While conventional WSD depends only on text and lexical resources, VWSD uses visual cues to find the right meaning of ambiguous words with minimal text input. The review looks at developments from early multimodal fusion methods to new frameworks that use contrastive models like CLIP, diffusion-based text-to-image generation, and large language model (LLM) support. Studies from 2016 to 2025 are examined to show the growth of VWSD through feature-based, graph-based, and contrastive embedding techniques. It focuses on prompt engineering, fine-tuning, and adapting to multiple languages. Quantitative results show that CLIP-based fine-tuned models and LLM-enhanced VWSD systems consistently perform better than zero-shot baselines, achieving gains of up to 6-8\% in Mean Reciprocal Rank (MRR). However, challenges still exist, such as limitations in context, model bias toward common meanings, a lack of multilingual datasets, and the need for better evaluation frameworks. The analysis highlights the growing overlap of CLIP alignment, diffusion generation, and LLM reasoning as the future path for strong, context-aware, and multilingual disambiguation systems.

</details>


### [175] [From Pragmas to Partners: A Symbiotic Evolution of Agentic High-Level Synthesis](https://arxiv.org/abs/2602.01401)
*Niansong Zhang,Sunwoo Kim,Shreesha Srinath,Zhiru Zhang*

Main category: cs.CL

Relevance: 35.0

TL;DR: 本文认为在AI驱动的硬件设计时代，高级综合（HLS）仍然至关重要，可作为智能体优化的自然抽象层，并提出HLS与AI智能体协同进化的分类法。


<details>
  <summary>Details</summary>
Motivation: 随着大语言模型推动AI驱动的硬件设计发展，作者探讨高级综合（HLS）在智能体时代是否仍然重要。他们认为HLS提供了快速迭代周期、可移植性和设计可排列性，使其成为智能体优化的自然抽象层。

Method: 这是一篇立场论文，通过三个主要贡献阐述观点：1) 解释HLS作为实用抽象层和黄金参考的作用；2) 识别当前HLS工具的关键限制；3) 提出智能体HLS协同进化的分类法。

Result: 论文论证了HLS在AI驱动的硬件设计中仍然至关重要，提出了HLS作为智能体优化抽象层的价值，并指出了当前HLS工具在性能反馈、接口和可调试性方面的局限性，这些正是AI智能体能够独特解决的问题。

Conclusion: HLS在智能体时代仍然不可或缺，它提供了适合AI智能体优化的抽象层。随着系统从协作者发展到自主设计伙伴，责任将从人类设计师逐步转移到AI智能体，形成HLS与AI智能体的协同进化关系。

Abstract: The rise of large language models has sparked interest in AI-driven hardware design, raising the question: does high-level synthesis (HLS) still matter in the agentic era? We argue that HLS remains essential. While we expect mature agentic hardware systems to leverage both HLS and RTL, this paper focuses on HLS and its role in enabling agentic optimization. HLS offers faster iteration cycles, portability, and design permutability that make it a natural layer for agentic optimization.This position paper makes three contributions. First, we explain why HLS serves as a practical abstraction layer and a golden reference for agentic hardware design. Second, we identify key limitations of current HLS tools, namely inadequate performance feedback, rigid interfaces, and limited debuggability that agents are uniquely positioned to address. Third, we propose a taxonomy for the symbiotic evolution of agentic HLS, clarifying how responsibility shifts from human designers to AI agents as systems advance from copilots to autonomous design partners.

</details>


### [176] [SentiFuse: Deep Multi-model Fusion Framework for Robust Sentiment Extraction](https://arxiv.org/abs/2602.01447)
*Hieu Minh Duong,Rupa Ghosh,Cong Hoan Nguyen,Eugene Levin,Todd Gary,Long Nguyen*

Main category: cs.CL

Relevance: 35.0

TL;DR: SentiFuse是一个灵活、模型无关的框架，通过标准化层和多种融合策略集成异构情感分析模型，在三个大规模社交媒体数据集上表现优于单个模型和简单集成方法。


<details>
  <summary>Details</summary>
Motivation: 现有情感分析模型各有优势但缺乏有效的统一集成框架。不同模型在特定情感表达上表现互补，但现有方法无法系统性地利用这种互补性来提升整体性能。

Method: 提出SentiFuse框架，包含标准化层将异构模型输出统一化，支持三种融合策略：决策级融合、特征级融合和自适应融合。框架设计为模型无关，可集成各种情感分析模型。

Result: 在Crowdflower、GoEmotions和Sentiment140三个大规模社交媒体数据集上的实验表明：1) SentiFuse始终优于单个模型和简单集成；2) 特征级融合效果最好，F1分数比最佳单模型和简单平均提升高达4%；3) 自适应融合在处理否定、混合情感和复杂表达等挑战性案例时增强鲁棒性。

Conclusion: 系统性地利用模型互补性能够实现更准确、可靠的情感分析，适用于不同数据集和文本类型。SentiFuse为集成异构情感模型提供了有效的统一框架。

Abstract: Sentiment analysis models exhibit complementary strengths, yet existing approaches lack a unified framework for effective integration. We present SentiFuse, a flexible and model-agnostic framework that integrates heterogeneous sentiment models through a standardization layer and multiple fusion strategies. Our approach supports decision-level fusion, feature-level fusion, and adaptive fusion, enabling systematic combination of diverse models. We conduct experiments on three large-scale social-media datasets: Crowdflower, GoEmotions, and Sentiment140. These experiments show that SentiFuse consistently outperforms individual models and naive ensembles. Feature-level fusion achieves the strongest overall effectiveness, yielding up to 4\% absolute improvement in F1 score over the best individual model and simple averaging, while adaptive fusion enhances robustness on challenging cases such as negation, mixed emotions, and complex sentiment expressions. These results demonstrate that systematically leveraging model complementarity yields more accurate and reliable sentiment analysis across diverse datasets and text types.

</details>


### [177] [Enhancing Automated Essay Scoring with Three Techniques: Two-Stage Fine-Tuning, Score Alignment, and Self-Training](https://arxiv.org/abs/2602.01747)
*Hongseok Choi,Serynn Kim,Wencke Liermann,Jin Seong,Jin-Xia Huang*

Main category: cs.CL

Relevance: 35.0

TL;DR: 论文提出三种技术提升自动作文评分系统在有限数据和完整数据下的性能：两阶段微调策略、分数对齐技术和不确定性感知自训练方法，在ASAP++数据集上验证了有效性。


<details>
  <summary>Details</summary>
Motivation: 现实教育场景中，自动作文评分系统面临标注数据极度稀缺的问题，这严重限制了鲁棒AES系统的开发和实际应用。需要解决有限数据下的性能提升问题。

Method: 1. 两阶段微调策略：利用低秩适应技术使AES模型更好地适应目标提示作文；2. 分数对齐技术：改善预测分数与真实分数分布的一致性；3. 不确定性感知自训练：使用未标注数据扩展训练集，同时减轻标签噪声传播。

Result: 在32数据设置下，三种技术均提升性能，集成后达到完整数据性能的91.2%（仅使用约1000个标注样本）。分数对齐技术在有限数据和完整数据设置下均一致提升性能，在完整数据设置下集成到DualBERT时达到最先进结果。

Conclusion: 提出的三种技术能有效提升自动作文评分系统在数据稀缺场景下的性能，分数对齐技术具有通用性，在完整数据设置下也能带来显著改进。

Abstract: Automated Essay Scoring (AES) plays a crucial role in education by providing scalable and efficient assessment tools. However, in real-world settings, the extreme scarcity of labeled data severely limits the development and practical adoption of robust AES systems. This study proposes a novel approach to enhance AES performance in both limited-data and full-data settings by introducing three key techniques. First, we introduce a Two-Stage fine-tuning strategy that leverages low-rank adaptations to better adapt an AES model to target prompt essays. Second, we introduce a Score Alignment technique to improve consistency between predicted and true score distributions. Third, we employ uncertainty-aware self-training using unlabeled data, effectively expanding the training set with pseudo-labeled samples while mitigating label noise propagation. We implement above three key techniques on DualBERT. We conduct extensive experiments on the ASAP++ dataset. As a result, in the 32-data setting, all three key techniques improve performance, and their integration achieves 91.2% of the full-data performance trained on approximately 1,000 labeled samples. In addition, the proposed Score Alignment technique consistently improves performance in both limited-data and full-data settings: e.g., it achieves state-of-the-art results in the full-data setting when integrated into DualBERT.

</details>


### [178] [GuideWeb: A Benchmark for Automatic In-App Guide Generation on Real-World Web UIs](https://arxiv.org/abs/2602.01917)
*Chengguang Gan,Yoshihiro Tsujii,Yunhao Liang,Tatsunori Mori,Shiwen Ni,Hiroki Itoh*

Main category: cs.CL

Relevance: 35.0

TL;DR: GuideWeb：用于真实网页UI自动应用指南生成的新基准，包含指南目标元素选择和指南文本生成任务，现有方法表现不佳


<details>
  <summary>Details</summary>
Motivation: 数字采用平台(DAP)需要为复杂网站提供操作指导，但现有工具需要人工维护和更新，成本高昂。网站布局和功能不断变化，需要自动化解决方案来降低维护成本。

Method: 提出GuideWeb基准，将任务形式化为：1) 基于网页内容选择指南目标元素；2) 生成与用户意图一致的简洁指南文本。同时提出综合评估套件，联合衡量元素选择准确性和文本生成质量。

Result: GuideWeb Agent在指南目标元素预测上达到30.79%准确率，意图生成BLEU得分为44.94，指南文本生成BLEU得分为21.34。现有基线方法表现显著更差。

Conclusion: 自动指南生成仍具挑战性，需要进一步技术突破才能在真实场景中可靠部署。GuideWeb为这一领域提供了标准化评估框架。

Abstract: Digital Adoption Platform (DAP) provide web-based overlays that deliver operation guidance and contextual hints to help users navigate complex websites. Although modern DAP tools enable non-experts to author such guidance, maintaining these guides remains labor-intensive because website layouts and functionalities evolve continuously, which requires repeated manual updates and re-annotation. In this work, we introduce \textbf{GuideWeb}, a new benchmark for automatic in-app guide generation on real-world web UIs. GuideWeb formulates the task as producing page-level guidance by selecting \textbf{guide target elements} grounded in the webpage and generating concise guide text aligned with user intent. We also propose a comprehensive evaluation suite that jointly measures the accuracy of guide target element selection and the quality of generated intents and guide texts. Experiments show that our proposed \textbf{GuideWeb Agent} achieves \textbf{30.79\%} accuracy in guide target element prediction, while obtaining BLEU scores of \textbf{44.94} for intent generation and \textbf{21.34} for guide-text generation. Existing baselines perform substantially worse, which highlights that automatic guide generation remains challenging and that further advances are necessary before such systems can be reliably deployed in real-world settings.

</details>


### [179] [dziribot: rag based intelligent conversational agent for algerian arabic dialect](https://arxiv.org/abs/2602.02270)
*El Batoul Bechiri,Dihia Lanasri*

Main category: cs.CL

Relevance: 35.0

TL;DR: DziriBOT：针对阿尔及利亚方言Darja的混合智能对话代理，结合NLU与RAG技术，通过微调DziriBERT模型在低资源方言处理上取得SOTA性能


<details>
  <summary>Details</summary>
Motivation: 客户服务数字化需求增长，但阿尔及利亚方言Darja存在非标准化拼写、法阿语码转换、阿拉伯/拉丁字母混用等复杂语言特征，传统语言模型难以处理

Method: 提出多层架构整合专用NLU与RAG，评估三种方法：稀疏特征Rasa流水线、经典机器学习基线、基于Transformer的微调（DziriBERT）

Result: 微调DziriBERT模型达到最先进性能，显著超越传统基线，在处理拼写噪声和罕见意图方面表现优异

Conclusion: DziriBOT为阿尔及利亚用户提供鲁棒可扩展的解决方案，弥合正式语言模型与方言现实之间的差距，为区域市场方言感知自动化提供蓝图

Abstract: The rapid digitalization of customer service has intensified the demand for conversational agents capable of providing accurate and natural interactions. In the Algerian context, this is complicated by the linguistic complexity of Darja, a dialect characterized by non-standardized orthography, extensive code-switching with French, and the simultaneous use of Arabic and Latin (Arabizi) scripts. This paper introduces DziriBOT, a hybrid intelligent conversational agent specifically engineered to overcome these challenges. We propose a multi-layered architecture that integrates specialized Natural Language Understanding (NLU) with Retrieval-Augmented Generation (RAG), allowing for both structured service flows and dynamic, knowledge-intensive responses grounded in curated enterprise documentation. To address the low-resource nature of Darja, we systematically evaluate three distinct approaches: a sparse-feature Rasa pipeline, classical machine learning baselines, and transformer-based fine-tuning. Our experimental results demonstrate that the fine-tuned DziriBERT model achieves state-of-the-art performance. These results significantly outperform traditional baselines, particularly in handling orthographic noise and rare intents. Ultimately, DziriBOT provides a robust, scalable solution that bridges the gap between formal language models and the linguistic realities of Algerian users, offering a blueprint for dialect-aware automation in the regional market.

</details>


### [180] [SpeechLess: Micro-utterance with Personalized Spatial Memory-aware Assistant in Everyday Augmented Reality](https://arxiv.org/abs/2602.00793)
*Yoonsang Kim,Devshree Jadeja,Divyansh Pradhan,Yalong Yang,Arie Kaufman*

Main category: cs.HC

Relevance: 35.0

TL;DR: SpeechLess是一个基于空间记忆的AR助手，通过语音意图粒度控制减少用户说话需求，支持从完整话语到微/零话语的交互


<details>
  <summary>Details</summary>
Motivation: 解决在公共场合使用可穿戴AR助手时语音交互的社会尴尬问题，减少用户重复表达相同请求的负担

Method: 基于个性化空间记忆的语音意图粒度控制范式，将先前的交互绑定到多模态个人上下文（空间、时间、活动、指称物）形成空间记忆，并利用这些记忆从用户不完整的查询中推断缺失的意图维度

Result: 受控实验室和野外研究表明，受调节的语音交互可以改善日常信息获取，减少表达努力，支持社会可接受的使用，且不会显著降低感知可用性或意图解析准确性

Conclusion: SpeechLess通过空间记忆支持动态调整意图表达粒度，有效解决了公共场合语音交互的社会尴尬和重复表达问题

Abstract: Speaking aloud to a wearable AR assistant in public can be socially awkward, and re-articulating the same requests every day creates unnecessary effort. We present SpeechLess, a wearable AR assistant that introduces a speech-based intent granularity control paradigm grounded in personalized spatial memory. SpeechLess helps users "speak less," while still obtaining the information they need, and supports gradual explicitation of intent when more complex expression is required. SpeechLess binds prior interactions to multimodal personal context-space, time, activity, and referents-to form spatial memories, and leverages them to extrapolate missing intent dimensions from under-specified user queries. This enables users to dynamically adjust how explicitly they express their informational needs, from full-utterance to micro/zero-utterance interaction. We motivate our design through a week-long formative study using a commercial smart glasses platform, revealing discomfort with public voice use, frustration with repetitive speech, and hardware constraints. Building on these insights, we design SpeechLess, and evaluate it through controlled lab and in-the-wild studies. Our results indicate that regulated speech-based interaction, can improve everyday information access, reduce articulation effort, and support socially acceptable use without substantially degrading perceived usability or intent resolution accuracy across diverse everyday environments.

</details>


### [181] [Causally Disentangled Contrastive Learning for Multilingual Speaker Embeddings](https://arxiv.org/abs/2602.01363)
*Mariëtte Olijslager,Seyed Sahand Mohammadi Ziabari,Ali Mohammed Mansoor Alsahag*

Main category: cs.SD

Relevance: 35.0

TL;DR: 本文研究了自监督说话人嵌入中人口统计信息（性别、年龄、口音）的泄露问题，评估了两种去偏方法（对抗训练和因果瓶颈架构），发现存在性能与去偏效果之间的权衡。


<details>
  <summary>Details</summary>
Motivation: 自监督说话人嵌入在说话人验证系统中广泛应用，但已有研究表明这些嵌入经常编码敏感的人口统计属性（如性别、年龄、口音），引发了公平性和隐私担忧。本文旨在探究这些人口统计信息在SimCLR训练的说话人嵌入中的存在程度，以及是否能在不严重降低说话人验证性能的情况下缓解这种信息泄露。

Method: 研究采用两种去偏策略：1）通过梯度反转进行对抗训练；2）因果瓶颈架构，明确分离人口统计信息和残差信息。使用线性和非线性探测分类器量化人口统计信息泄露，使用ROC-AUC和EER评估说话人验证性能。

Result: 结果显示：1）基线嵌入中性别信息被强烈且线性编码，而年龄和口音信息较弱且主要为非线性表示；2）对抗去偏能减少性别泄露，但对年龄和口音影响有限，且与验证准确率存在明显权衡；3）因果瓶颈进一步抑制了人口统计信息（尤其在残差表示中），但导致显著的性能下降。

Conclusion: 这些发现突显了在自监督说话人嵌入中缓解人口统计信息泄露的根本局限性，并阐明了当前去偏方法固有的权衡关系。

Abstract: Self-supervised speaker embeddings are widely used in speaker verification systems, but prior work has shown that they often encode sensitive demographic attributes, raising fairness and privacy concerns. This paper investigates the extent to which demographic information, specifically gender, age, and accent, is present in SimCLR-trained speaker embeddings and whether such leakage can be mitigated without severely degrading speaker verification performance. We study two debiasing strategies: adversarial training through gradient reversal and a causal bottleneck architecture that explicitly separates demographic and residual information. Demographic leakage is quantified using both linear and nonlinear probing classifiers, while speaker verification performance is evaluated using ROC-AUC and EER. Our results show that gender information is strongly and linearly encoded in baseline embeddings, whereas age and accent are weaker and primarily nonlinearly represented. Adversarial debiasing reduces gender leakage but has limited effect on age and accent and introduces a clear trade-off with verification accuracy. The causal bottleneck further suppresses demographic information, particularly in the residual representation, but incurs substantial performance degradation. These findings highlight fundamental limitations in mitigating demographic leakage in self-supervised speaker embeddings and clarify the trade-offs inherent in current debiasing approaches.

</details>


### [182] [EmoAra: Emotion-Preserving English Speech Transcription and Cross-Lingual Translation with Arabic Text-to-Speech](https://arxiv.org/abs/2602.01170)
*Besher Hassan,Ibrahim Alsarraj,Musaab Hasan,Yousef Melhim,Shahem Fadi,Shahem Sultan*

Main category: cs.CL

Relevance: 25.0

TL;DR: EmoAra是一个端到端的情感保持跨语言语音通信系统，针对银行客服场景，通过集成语音情感识别、语音识别、机器翻译和语音合成技术，在英语到阿拉伯语的转换中保持情感细微差别。


<details>
  <summary>Details</summary>
Motivation: 银行客服场景中情感语境影响服务质量，需要跨语言交流时保持情感细微差别。现有系统通常只关注内容翻译而忽略情感传递，导致服务质量下降。

Method: 集成CNN情感分类器、Whisper语音识别、微调MarianMT英阿翻译模型和MMS-TTS-Ara阿拉伯语语音合成，构建端到端情感保持管道。

Result: 情感分类F1分数94%，翻译性能BLEU 56和BERTScore F1 88.7%，银行领域翻译人工评估平均得分81%。

Conclusion: EmoAra成功实现了跨语言语音通信中的情感保持，在银行客服场景表现良好，为多模态情感感知系统提供了可行方案。

Abstract: This work presents EmoAra, an end-to-end emotion-preserving pipeline for cross-lingual spoken communication, motivated by banking customer service where emotional context affects service quality. EmoAra integrates Speech Emotion Recognition, Automatic Speech Recognition, Machine Translation, and Text-to-Speech to process English speech and deliver an Arabic spoken output while retaining emotional nuance. The system uses a CNN-based emotion classifier, Whisper for English transcription, a fine-tuned MarianMT model for English-to-Arabic translation, and MMS-TTS-Ara for Arabic speech synthesis. Experiments report an F1-score of 94% for emotion classification, translation performance of BLEU 56 and BERTScore F1 88.7%, and an average human evaluation score of 81% on banking-domain translations. The implementation and resources are available at the accompanying GitHub repository.

</details>


### [183] [Sinhala Physical Common Sense Reasoning Dataset for Global PIQA](https://arxiv.org/abs/2602.02207)
*Nisansa de Silva,Surangika Ranathunga*

Main category: cs.CL

Relevance: 25.0

TL;DR: 首个僧伽罗语物理常识推理数据集，包含110个人工创建和验证的样本，每个样本包含提示、正确答案和错误答案，主要针对斯里兰卡语境。


<details>
  <summary>Details</summary>
Motivation: 为僧伽罗语创建物理常识推理数据集，填补该语言在常识推理评估方面的空白，支持多语言AI模型的评估和发展。

Method: 人工创建和验证110个数据样本，每个样本包含提示、正确答案和错误答案，问题主要基于斯里兰卡的文化和生活语境。

Result: 成功创建了首个僧伽罗语物理常识推理数据集，作为Global PIQA项目的一部分，为僧伽罗语AI模型评估提供了基准。

Conclusion: 该数据集填补了僧伽罗语常识推理评估的空白，有助于推动多语言AI模型的发展，特别是在斯里兰卡语境下的应用。

Abstract: This paper presents the first-ever Sinhala physical common sense reasoning dataset created as part of Global PIQA. It contains 110 human-created and verified data samples, where each sample consists of a prompt, the corresponding correct answer, and a wrong answer. Most of the questions refer to the Sri Lankan context, where Sinhala is an official language.

</details>


### [184] [Using Correspondence Patterns to Identify Irregular Words in Cognate sets Through Leave-One-Out Validation](https://arxiv.org/abs/2602.02221)
*Frederic Blum,Johann-Mattis List*

Main category: cs.CL

Relevance: 25.0

TL;DR: 提出了一种新的历史语言学定量评估方法，通过平衡平均对应模式重现率来衡量语音对应规律性，并开发了识别不规则同源词集的算法。


<details>
  <summary>Details</summary>
Motivation: 历史语言学中语音对应规律性通常依赖直觉判断而非量化评估，且不规则现象比新语法学派模型预期的更常见。随着计算方法和标准化词汇数据的进步，需要改进工作流程并提供定量评估。

Method: 提出平衡平均对应模式重现率作为规律性新度量，开发基于此度量的计算方法，用于识别对应模式不规则的同源词集。通过模拟和真实数据的留一法验证来评估方法性能。

Result: 方法在基于真实数据的实验中达到85%的整体准确率，展示了处理大数据子样本的优势，并分析了数据不规则性增加对结果的影响。

Conclusion: 新的规律性度量和基于此的不规则同源词识别方法，在提高计算机辅助语言比较现有和未来数据集质量方面具有重要潜力。

Abstract: Regular sound correspondences constitute the principal evidence in historical language comparison. Despite the heuristic focus on regularity, it is often more an intuitive judgement than a quantified evaluation, and irregularity is more common than expected from the Neogrammarian model. Given the recent progress of computational methods in historical linguistics and the increased availability of standardized lexical data, we are now able to improve our workflows and provide such a quantitative evaluation. Here, we present the balanced average recurrence of correspondence patterns as a new measure of regularity. We also present a new computational method that uses this measure to identify cognate sets that lack regularity with respect to their correspondence patterns. We validate the method through two experiments, using simulated and real data. In the experiments, we employ leave-one-out validation to measure the regularity of cognate sets in which one word form has been replaced by an irregular one, checking how well our method identifies the forms causing the irregularity. Our method achieves an overall accuracy of 85\% with the datasets based on real data. We also show the benefits of working with subsamples of large datasets and how increasing irregularity in the data influences our results. Reflecting on the broader potential of our new regularity measure and the irregular cognate identification method based on it, we conclude that they could play an important role in improving the quality of existing and future datasets in computer-assisted language comparison.

</details>


### [185] [A Baseline Multimodal Approach to Emotion Recognition in Conversations](https://arxiv.org/abs/2602.00914)
*Víctor Yeste,Rodrigo Rivas-Arévalo*

Main category: cs.CL

Relevance: 15.0

TL;DR: 该论文提出了一个用于对话情感识别的轻量级多模态基线方法，使用Friends情景剧的SemEval-2024 Task 3数据集，结合文本分类器和语音表示模型进行简单后期融合。


<details>
  <summary>Details</summary>
Motivation: 该研究的动机不是提出新的SOTA方法，而是提供一个可访问的参考实现，用于对话情感识别任务，支持未来更严格的比较研究。

Method: 方法包括：(1) 基于transformer的文本分类器，(2) 自监督语音表示模型，(3) 简单的后期融合集成策略，在有限训练协议下进行实验。

Result: 报告了基线设置和实证结果，重点分析了多模态融合在何时优于单模态模型，为透明度提供支持。

Conclusion: 该工作主要是一个参考实现而非创新方法，旨在支持未来更严谨的多模态情感识别研究比较。

Abstract: We present a lightweight multimodal baseline for emotion recognition in conversations using the SemEval-2024 Task 3 dataset built from the sitcom Friends. The goal of this report is not to propose a novel state-of-the-art method, but to document an accessible reference implementation that combines (i) a transformer-based text classifier and (ii) a self-supervised speech representation model, with a simple late-fusion ensemble. We report the baseline setup and empirical results obtained under a limited training protocol, highlighting when multimodal fusion improves over unimodal models. This preprint is provided for transparency and to support future, more rigorous comparisons.

</details>


### [186] [The French Drama Revolution: Political Economy and Literary Production, 1700-1900](https://arxiv.org/abs/2602.00588)
*Thiago Dumont Oliveira*

Main category: cs.CL

Relevance: 5.0

TL;DR: 该论文使用LDA主题建模和Jensen-Shannon散度分析1700-190年法国戏剧的主题演变，发现法国大革命后戏剧主题分布发生深刻变化，资产阶级主题兴起，并与法国GDP增长呈现协同演化关系。


<details>
  <summary>Details</summary>
Motivation: 研究法国戏剧在1700-1900年间的演变，特别是法国大革命和工业化对戏剧主题的影响，探索文化表现形式与社会经济变迁之间的关系。

Method: 使用潜在狄利克雷分配(LDA)进行主题建模，结合Jensen-Shannon散度分析主题分布变化，将戏剧主题的年度流行度与法国GDP数据进行对比分析。

Result: 法国戏剧主题分布在1789-1850年间发生深刻变化，资产阶级主题在18世纪末成为最流行主题之一，戏剧主题演变与法国经济增长呈现协同演化模式。

Conclusion: 法国大革命和工业化进程深刻影响了戏剧创作，文化表现形式与社会经济变迁密切相关，戏剧主题的演变反映了政治经济变革对文化生产的影响。

Abstract: This paper investigates the changing nature of French drama between 1700-1900 using Latent Dirichlet Allocation and Jensen-Shannon Divergence. Results indicate that the topical distribution of French drama changed profoundly after the French Revolution, particularly between 1789 and 1850. Bourgeois themes emerged among the most prevalent topics since the late 18th century. To assess the coevolution of drama and economic growth, I plot the yearly prevalence of topics alongside French GDP between 1700-1900, and discuss these changes in light of the political and economic changes prompted by the French Revolution and the industrialization of the country.

</details>


<div id='cs.CV'></div>

# cs.CV [[Back]](#toc)

### [187] [CAPA: Contribution-Aware Pruning and FFN Approximation for Efficient Large Vision-Language Models](https://arxiv.org/abs/2602.00247)
*Samyak Jha,Junho Kim*

Main category: cs.CV

Relevance: 85.0

TL;DR: 该论文提出CAPA框架，通过注意力贡献度评估视觉token重要性，识别可安全剪枝的概率转储token和必须保留的结构锚点token，并结合FFN线性近似来高效减少视觉语言模型推理计算。


<details>
  <summary>Details</summary>
Motivation: 大型视觉语言模型推理效率受限于处理数千视觉token的高成本，但哪些token和计算可以安全移除尚不明确。现有方法使用注意力分数作为重要性评估，但这并非实际贡献的准确代理。

Method: 提出注意力贡献度（Attention Contribution）作为更准确的视觉token选择标准，识别两类视觉注意力汇点：可剪枝的概率转储和必须保留的结构锚点。发现视觉token相关的FFN存在大量冗余，特别是在中间层呈现线性行为。基于此提出CAPA框架，在关键功能转换处使用注意力贡献度剪枝视觉token，并通过高效线性近似减少FFN计算。

Result: 在多个基准测试和基线模型上的实验表明，CAPA实现了有竞争力的效率-性能权衡，并提高了鲁棒性。

Conclusion: 注意力贡献度是比注意力分数更准确的视觉token重要性评估标准，视觉注意力汇点具有功能异质性，CAPA框架通过双策略（token剪枝和FFN近似）有效提升了视觉语言模型的推理效率。

Abstract: Efficient inference in Large Vision-Language Models is constrained by the high cost of processing thousands of visual tokens, yet it remains unclear which tokens and computations can be safely removed. While attention scores are commonly used to estimate visual token importance, they are an imperfect proxy for actual contribution. We show that Attention Contribution, which weights attention probabilities by value vector magnitude, provides a more accurate criterion for visual token selection. Our empirical analysis reveals that visual attention sinks are functionally heterogeneous, comprising Probability Dumps with low contribution that can be safely pruned, and Structural Anchors with high contribution essential for maintaining model performance. Further, we identify substantial redundancy in Feed-Forward Networks (FFNs) associated with visual tokens, particularly in intermediate layers where image tokens exhibit linear behavior. Based on our findings, we introduce CAPA (Contribution-Aware Pruning and FFN Approximation), a dual-strategy framework that prunes visual tokens using attention contribution at critical functional transitions and reduces FFN computation through efficient linear approximations. Experiments on various benchmarks across baselines show that CAPA achieves competent efficiency--performance trade-offs with improved robustness.

</details>


### [188] [SANEval: Open-Vocabulary Compositional Benchmarks with Failure-mode Diagnosis](https://arxiv.org/abs/2602.00249)
*Rishav Pramanik,Ian E. Nielsen,Jeff Smith,Saurav Pandit,Ravi P. Ramachandran,Zhaozheng Yin*

Main category: cs.CV

Relevance: 85.0

TL;DR: SANEval是一个用于文本到图像模型的开源评估基准，专注于空间关系、属性绑定和计数能力的组合评估，使用LLM进行深度提示理解和开放词汇目标检测。


<details>
  <summary>Details</summary>
Motivation: 当前文本到图像模型在处理涉及多个对象、属性和空间关系的复杂提示时存在瓶颈，而现有评估方法受限于封闭词汇表，缺乏细粒度诊断能力，无法提供可解释的反馈来诊断和修复具体的组合失败。

Method: 提出SANEval基准，建立了一个可扩展的开放词汇组合评估流程：1) 使用大型语言模型进行深度提示理解；2) 使用LLM增强的开放词汇目标检测器来稳健评估组合一致性，不受固定词汇表限制。

Result: 在六个最先进的T2I模型上进行广泛实验，证明SANEval的自动评估提供了更忠实的人类评估代理，其指标在属性绑定、空间关系和计数任务上与人类评估的Spearman等级相关性显著优于现有基准。

Conclusion: SANEval解决了T2I模型组合评估的关键瓶颈，提供了一个可扩展、开放词汇的评估框架，能够提供可解释的反馈来诊断具体失败，将发布数据集和开源评估流程以促进未来研究。

Abstract: The rapid progress of text-to-image (T2I) models has unlocked unprecedented creative potential, yet their ability to faithfully render complex prompts involving multiple objects, attributes, and spatial relationships remains a significant bottleneck. Progress is hampered by a lack of adequate evaluation methods; current benchmarks are often restricted to closed-set vocabularies, lack fine-grained diagnostic capabilities, and fail to provide the interpretable feedback necessary to diagnose and remedy specific compositional failures. We solve these challenges by introducing SANEval (Spatial, Attribute, and Numeracy Evaluation), a comprehensive benchmark that establishes a scalable new pipeline for open-vocabulary compositional evaluation. SANEval combines a large language model (LLM) for deep prompt understanding with an LLM-enhanced, open-vocabulary object detector to robustly evaluate compositional adherence, unconstrained by a fixed vocabulary. Through extensive experiments on six state-of-the-art T2I models, we demonstrate that SANEval's automated evaluations provide a more faithful proxy for human assessment; our metric achieves a Spearman's rank correlation with statistically different results than those of existing benchmarks across tasks of attribute binding, spatial relations, and numeracy. To facilitate future research in compositional T2I generation and evaluation, we will release the SANEval dataset and our open-source evaluation pipeline.

</details>


### [189] [When RAG Hurts: Diagnosing and Mitigating Attention Distraction in Retrieval-Augmented LVLMs](https://arxiv.org/abs/2602.00344)
*Beidi Zhao,Wenlong Deng,Xinting Liao,Yushu Li,Nazim Shaikh,Yao Nie,Xiaoxiao Li*

Main category: cs.CV

Relevance: 85.0

TL;DR: 本文提出MAD-RAG方法解决检索增强生成(RAG)中的注意力分散问题，通过双问题表述和解耦视觉定位与上下文整合，在多个VQA数据集上显著提升性能。


<details>
  <summary>Details</summary>
Motivation: 当前RAG方法在增强大型视觉语言模型(LVLMs)时存在注意力分散问题：当检索到足够相关的上下文时，检索文本会全局抑制视觉注意力，导致模型将注意力从问题相关图像区域转移，从而影响原本能正确回答的问题。

Method: 提出MAD-RAG方法：1) 通过双问题表述解耦视觉定位与上下文整合；2) 结合注意力混合技术保留图像条件证据；3) 无需训练即可部署。

Result: 在OK-VQA、E-VQA和InfoSeek数据集上，MAD-RAG在不同模型家族中一致优于现有基线，相比原始RAG基线分别获得4.76%、9.20%和6.18%的绝对提升，能纠正高达74.68%的失败案例，计算开销可忽略。

Conclusion: MAD-RAG有效解决了RAG中的注意力分散问题，通过解耦视觉定位和上下文整合，在保持计算效率的同时显著提升视觉问答性能。

Abstract: While Retrieval-Augmented Generation (RAG) is one of the dominant paradigms for enhancing Large Vision-Language Models (LVLMs) on knowledge-based VQA tasks, recent work attributes RAG failures to insufficient attention towards the retrieved context, proposing to reduce the attention allocated to image tokens. In this work, we identify a distinct failure mode that previous study overlooked: Attention Distraction (AD). When the retrieved context is sufficient (highly relevant or including the correct answer), the retrieved text suppresses the visual attention globally, and the attention on image tokens shifts away from question-relevant regions. This leads to failures on questions the model could originally answer correctly without the retrieved text. To mitigate this issue, we propose MAD-RAG, a training-free intervention that decouples visual grounding from context integration through a dual-question formulation, combined with attention mixing to preserve image-conditioned evidence. Extensive experiments on OK-VQA, E-VQA, and InfoSeek demonstrate that MAD-RAG consistently outperforms existing baselines across different model families, yielding absolute gains of up to 4.76%, 9.20%, and 6.18% over the vanilla RAG baseline. Notably, MAD-RAG rectifies up to 74.68% of failure cases with negligible computational overhead.

</details>


### [190] [Text is All You Need for Vision-Language Model Jailbreaking](https://arxiv.org/abs/2602.00420)
*Yihang Chen,Zhao Xu,Youyuan Jiang,Tianle Zheng,Cho-Jui Hsieh*

Main category: cs.CV

Relevance: 85.0

TL;DR: Text-DJ是一种针对大型视觉语言模型的新型越狱攻击，通过将有害查询分解为多个语义相关但更良性的子查询，并添加大量无关的干扰查询，以图像网格形式呈现，利用模型的OCR能力绕过安全防护。


<details>
  <summary>Details</summary>
Motivation: 当前大型视觉语言模型的安全防护主要关注显式文本输入或相关视觉场景的分析，但忽略了模型OCR能力可能被利用的漏洞。研究者发现可以通过将文本转换为图像并分散呈现来绕过现有的安全机制。

Method: 方法分为三个阶段：1) 将单个有害查询分解为多个语义相关但更良性的子查询；2) 选择与有害查询最大程度无关的干扰查询；3) 将所有子查询和干扰查询以图像网格形式同时呈现给模型，其中子查询位于网格中间位置。

Result: 该方法成功绕过了最先进的大型视觉语言模型的安全对齐机制。攻击成功的原因包括：1) 将基于文本的提示转换为图像，绕过标准文本过滤器；2) 引入干扰，使模型的安全协议无法在大量无关查询中链接分散的子查询。

Conclusion: 该研究揭示了大型视觉语言模型OCR能力的关键漏洞，表明其对分散的多图像对抗输入不够鲁棒，强调了需要为碎片化的多模态输入开发防御机制。

Abstract: Large Vision-Language Models (LVLMs) are increasingly equipped with robust safety safeguards to prevent responses to harmful or disallowed prompts. However, these defenses often focus on analyzing explicit textual inputs or relevant visual scenes. In this work, we introduce Text-DJ, a novel jailbreak attack that bypasses these safeguards by exploiting the model's Optical Character Recognition (OCR) capability. Our methodology consists of three stages. First, we decompose a single harmful query into multiple and semantically related but more benign sub-queries. Second, we pick a set of distraction queries that are maximally irrelevant to the harmful query. Third, we present all decomposed sub-queries and distraction queries to the LVLM simultaneously as a grid of images, with the position of the sub-queries being middle within the grid. We demonstrate that this method successfully circumvents the safety alignment of state-of-the-art LVLMs. We argue this attack succeeds by (1) converting text-based prompts into images, bypassing standard text-based filters, and (2) inducing distractions, where the model's safety protocols fail to link the scattered sub-queries within a high number of irrelevant queries. Overall, our findings expose a critical vulnerability in LVLMs' OCR capabilities that are not robust to dispersed, multi-image adversarial inputs, highlighting the need for defenses for fragmented multimodal inputs.

</details>


### [191] [LatentLens: Revealing Highly Interpretable Visual Tokens in LLMs](https://arxiv.org/abs/2602.00462)
*Benno Krojer,Shravan Nayak,Oscar Mañas,Vaibhav Adlakha,Desmond Elliott,Siva Reddy,Marius Mosbach*

Main category: cs.CV

Relevance: 85.0

TL;DR: LatentLens：一种将视觉语言模型（VLM）中视觉token的潜在表示映射到自然语言描述的新方法，通过比较视觉token与大规模文本语料库的上下文文本表示来实现可解释性分析。


<details>
  <summary>Details</summary>
Motivation: 理解为什么大型语言模型（LLM）能够轻松处理视觉token，需要可解释性方法来揭示LLM处理过程中每一层视觉token表示所编码的内容。现有方法（如LogitLens）可能低估了视觉token的可解释性。

Method: 提出LatentLens方法：1）编码大规模文本语料库，存储每个token的上下文表示；2）将视觉token表示与这些文本表示进行比较；3）通过top-k最近邻表示提供视觉token的描述。在10种不同VLM上评估该方法。

Result: 1）相比LogitLens等常用方法，LatentLens显著提高了视觉token的可解释性；2）在所有研究模型和所有层中，大多数视觉token都是可解释的；3）产生的描述具有语义意义，相比单个token提供更细粒度的人类可理解解释。

Conclusion: LatentLens为分析VLM中视觉token的潜在表示提供了有效工具，揭示了视觉和语言表示之间的对齐关系，为潜在表示分析开辟了新方向。

Abstract: Transforming a large language model (LLM) into a Vision-Language Model (VLM) can be achieved by mapping the visual tokens from a vision encoder into the embedding space of an LLM. Intriguingly, this mapping can be as simple as a shallow MLP transformation. To understand why LLMs can so readily process visual tokens, we need interpretability methods that reveal what is encoded in the visual token representations at every layer of LLM processing. In this work, we introduce LatentLens, a novel approach for mapping latent representations to descriptions in natural language. LatentLens works by encoding a large text corpus and storing contextualized token representations for each token in that corpus. Visual token representations are then compared to their contextualized textual representations, with the top-k nearest neighbor representations providing descriptions of the visual token. We evaluate this method on 10 different VLMs, showing that commonly used methods, such as LogitLens, substantially underestimate the interpretability of visual tokens. With LatentLens instead, the majority of visual tokens are interpretable across all studied models and all layers. Qualitatively, we show that the descriptions produced by LatentLens are semantically meaningful and provide more fine-grained interpretations for humans compared to individual tokens. More broadly, our findings contribute new evidence on the alignment between vision and language representations, opening up new directions for analyzing latent representations.

</details>


### [192] [Sparse Shortcuts: Facilitating Efficient Fusion in Multimodal Large Language Models](https://arxiv.org/abs/2602.00505)
*Jingrui Zhang,Feng Liang,Yong Zhang,Wei Wang,Runhao Zeng,Xiping Hu*

Main category: cs.CV

Relevance: 85.0

TL;DR: SparseCut提出了一种用于多模态大语言模型的稀疏跨模态融合架构，通过引入跨模态编码器与LLM之间的稀疏快捷连接，实现多层次视觉特征的高效分层集成，提升跨模态理解能力。


<details>
  <summary>Details</summary>
Motivation: 现有MLLM大多关注扩展语言模型或构建高质量训练数据，但忽视了如何有效将跨模态知识整合到语言空间中。特别是在视觉语言模型中，仅使用高层视觉特征进行模态对齐会丢弃中低层特征的丰富语义信息，限制了模型的跨模态理解能力。

Method: 提出SparseCut架构：1）在跨模态编码器和LLM之间引入稀疏快捷连接，实现多层次视觉特征的高效分层集成；2）设计高效多粒度特征融合模块，在通过快捷连接路由前进行视觉特征融合；3）保持原始语言上下文，不增加整体输入长度，避免增加LLM的计算复杂度。

Result: 实验表明，SparseCut显著提升了MLLM在各种多模态基准测试中的性能，对不同基础LLM具有通用性和可扩展性。

Conclusion: SparseCut通过稀疏快捷连接实现跨模态知识的高效分层融合，解决了现有MLLM中跨模态信息整合不足的问题，为多模态理解提供了更有效的架构设计。

Abstract: With the remarkable success of large language models (LLMs) in natural language understanding and generation, multimodal large language models (MLLMs) have rapidly advanced in their ability to process data across multiple modalities. While most existing efforts focus on scaling up language models or constructing higher-quality training data, limited attention has been paid to effectively integrating cross-modal knowledge into the language space. In vision-language models, for instance, aligning modalities using only high-level visual features often discards the rich semantic information present in mid- and low-level features, limiting the model's ability of cross-modality understanding. To address this issue, we propose SparseCut, a general cross-modal fusion architecture for MLLMs, introducing sparse shortcut connections between the cross-modal encoder and the LLM. These shortcut connections enable the efficient and hierarchical integration of visual features at multiple levels, facilitating richer semantic fusion without increasing computational overhead. We further introduce an efficient multi-grained feature fusion module, which performs the fusion of visual features before routing them through the shortcuts. This preserves the original language context and does not increase the overall input length, thereby avoiding an increase in computational complexity for the LLM. Experiments demonstrate that SparseCut significantly enhances the performance of MLLMs across various multimodal benchmarks with generality and scalability for different base LLMs.

</details>


### [193] [DuoGen: Towards General Purpose Interleaved Multimodal Generation](https://arxiv.org/abs/2602.00508)
*Min Shi,Xiaohui Zeng,Jiannan Huang,Yin Cui,Francesco Ferroni,Jialuo Li,Shubham Pachori,Zhaoshuo Li,Yogesh Balaji,Haoxiang Wang,Tsung-Yi Lin,Xiao Fu,Yue Zhao,Chieh-Yun Chen,Ming-Yu Liu,Humphrey Shi*

Main category: cs.CV

Relevance: 85.0

TL;DR: DuoGen是一个通用的交错多模态生成框架，通过数据构建、架构设计和评估系统解决现有交错生成模型的局限性，在文本质量、图像保真度和图像上下文对齐方面优于现有开源模型。


<details>
  <summary>Details</summary>
Motivation: 现有交错多模态生成模型在通用指令下的质量受到训练数据不足和基础模型能力的限制，需要系统性的解决方案来提升交错生成的质量。

Method: 1) 数据方面：构建大规模高质量的指令调优数据集，结合从精选网站重写的多模态对话和覆盖日常场景的多样化合成示例；2) 架构方面：利用预训练多模态LLM的视觉理解能力和预训练视频生成的扩散Transformer的视觉生成能力，采用两阶段解耦策略：先指令调优MLLM，然后用精选的交错图像-文本序列对齐DiT。

Result: 在公共和新提出的基准测试中，DuoGen在文本质量、图像保真度和图像上下文对齐方面优于现有开源模型，同时在统一生成模型中实现了文本到图像和图像编辑的最先进性能。

Conclusion: DuoGen通过系统性的数据、架构和评估设计，为交错多模态生成提供了有效的解决方案，展示了在通用指令下高质量交错生成的可行性。

Abstract: Interleaved multimodal generation enables capabilities beyond unimodal generation models, such as step-by-step instructional guides, visual planning, and generating visual drafts for reasoning. However, the quality of existing interleaved generation models under general instructions remains limited by insufficient training data and base model capacity. We present DuoGen, a general-purpose interleaved generation framework that systematically addresses data curation, architecture design, and evaluation. On the data side, we build a large-scale, high-quality instruction-tuning dataset by combining multimodal conversations rewritten from curated raw websites, and diverse synthetic examples covering everyday scenarios. Architecturally, DuoGen leverages the strong visual understanding of a pretrained multimodal LLM and the visual generation capabilities of a diffusion transformer (DiT) pretrained on video generation, avoiding costly unimodal pretraining and enabling flexible base model selection. A two-stage decoupled strategy first instruction-tunes the MLLM, then aligns DiT with it using curated interleaved image-text sequences. Across public and newly proposed benchmarks, DuoGen outperforms prior open-source models in text quality, image fidelity, and image-context alignment, and also achieves state-of-the-art performance on text-to-image and image editing among unified generation models. Data and code will be released at https://research.nvidia.com/labs/dir/duetgen/.

</details>


### [194] [SAGE: Accelerating Vision-Language Models via Entropy-Guided Adaptive Speculative Decoding](https://arxiv.org/abs/2602.00523)
*Yujia Tong,Tian Zhang,Yunyang Wan,Kaiwei Lin,Jingling Yuan,Chuang Hu*

Main category: cs.CV

Relevance: 85.0

TL;DR: SAGE提出了一种动态调整推测解码树结构的方法，通过实时预测不确定性来优化视觉语言模型的推理加速。


<details>
  <summary>Details</summary>
Motivation: 现有推测解码方法使用静态树结构，无法适应不同生成步骤的预测难度变化，导致接受长度不理想和加速效果有限。

Method: 基于输出熵作为置信度指标，动态构建推测树：高置信度时构建深窄树以最大化推测深度，低置信度时构建浅宽树以多样化探索。

Result: 在多个基准测试中，SAGE在不损失输出质量的情况下，为LLaVA-OneVision-72B带来3.36倍解码加速，为Qwen2.5-VL-72B带来3.18倍加速。

Conclusion: 动态调整推测树结构能显著提升视觉语言模型的推理效率，输出熵是有效的实时置信度指标。

Abstract: Speculative decoding has emerged as a promising approach to accelerate inference in vision-language models (VLMs) by enabling parallel verification of multiple draft tokens. However, existing methods rely on static tree structures that remain fixed throughout the decoding process, failing to adapt to the varying prediction difficulty across generation steps. This leads to suboptimal acceptance lengths and limited speedup. In this paper, we propose SAGE, a novel framework that dynamically adjusts the speculation tree structure based on real-time prediction uncertainty. Our key insight is that output entropy serves as a natural confidence indicator with strong temporal correlation across decoding steps. SAGE constructs deeper-narrower trees for high-confidence predictions to maximize speculation depth, and shallower-wider trees for uncertain predictions to diversify exploration. SAGE improves acceptance lengths and achieves faster acceleration compared to static tree baselines. Experiments on multiple benchmarks demonstrate the effectiveness of SAGE: without any loss in output quality, it delivers up to $3.36\times$ decoding speedup for LLaVA-OneVision-72B and $3.18\times$ for Qwen2.5-VL-72B.

</details>


### [195] [Learning to Decode Against Compositional Hallucination in Video Multimodal Large Language Models](https://arxiv.org/abs/2602.00559)
*Wenbin Xing,Quanxing Zha,Lizheng Zu,Mengran Li,Ming Li,Junchi Yan*

Main category: cs.CV

Relevance: 85.0

TL;DR: 提出了OmniVCHall基准来评估视频多模态大语言模型中的孤立和组合幻觉，并开发了TriCD对比解码框架来缓解这些问题。


<details>
  <summary>Details</summary>
Motivation: 当前视频幻觉缓解研究主要关注孤立错误类型，而组合幻觉（涉及多个时空因素的错误推理）尚未得到充分探索。需要系统评估视频多模态大语言模型在这两类幻觉上的表现。

Method: 1) 创建OmniVCHall基准，涵盖多样视频领域，引入新的基于相机的幻觉类型，定义细粒度分类，并设计对抗性答案选项；2) 提出TriCD对比解码框架，包含三重路径校准机制，使用自适应扰动控制器构建负样本，通过显著性引导增强模块强化视觉证据，通过强化学习优化。

Result: 评估了39个代表性VLLM，发现即使是先进模型（如Qwen3-VL和GPT-5）也表现出显著性能下降。TriCD在两个代表性骨干模型上平均准确率提升超过10%。

Conclusion: 组合幻觉是视频多模态大语言模型的重要挑战，OmniVCHall基准为系统评估提供了工具，TriCD框架通过对比解码和强化学习有效缓解了这些问题。

Abstract: Current research on video hallucination mitigation primarily focuses on isolated error types, leaving compositional hallucinations, arising from incorrect reasoning over multiple interacting spatial and temporal factors largely underexplored. We introduce OmniVCHall, a benchmark designed to systematically evaluate both isolated and compositional hallucinations in video multimodal large language models (VLLMs). OmniVCHall spans diverse video domains, introduces a novel camera-based hallucination type, and defines a fine-grained taxonomy, together with adversarial answer options (e.g., "All are correct" and "None of the above") to prevent shortcut reasoning. The evaluations of 39 representative VLLMs reveal that even advanced models (e.g., Qwen3-VL and GPT-5) exhibit substantial performance degradation. We propose TriCD, a contrastive decoding framework with a triple-pathway calibration mechanism. An adaptive perturbation controller dynamically selects distracting operations to construct negative video variants, while a saliency-guided enhancement module adaptively reinforces grounded token-wise visual evidences. These components are optimized via reinforcement learning to encourage precise decision-making under compositional hallucination settings. Experimental results show that TriCD consistently improves performance across two representative backbones, achieving an average accuracy improvement of over 10%. The data and code can be find at https://github.com/BMRETURN/OmniVCHall.

</details>


### [196] [Towards Interpretable Hallucination Analysis and Mitigation in LVLMs via Contrastive Neuron Steering](https://arxiv.org/abs/2602.00621)
*Guangtao Lyu,Xinyi Cheng,Qi Liu,Chenghao Xu,Jiexi Yan,Muli Yang,Fen Fang,Cheng Deng*

Main category: cs.CV

Relevance: 85.0

TL;DR: 该论文通过稀疏自编码器分析LVLM的内部表示，发现幻觉主要源于图像特定神经元的异常激活，并提出对比神经元引导方法在预填充阶段增强视觉基础、减少幻觉。


<details>
  <summary>Details</summary>
Motivation: 现有LVLM幻觉缓解方法主要关注输出层调整，对内部机制缺乏深入理解。作者希望从表示层面探究幻觉产生的根本原因，为更有效的干预提供理论基础。

Method: 使用稀疏自编码器将密集视觉嵌入分解为稀疏可解释神经元；通过神经元级分析识别不同类型神经元；提出对比神经元引导方法，通过对比干净和噪声输入识别图像特定神经元，选择性增强信息性神经元、抑制扰动诱导激活。

Result: 实验表明CNS在幻觉相关和通用多模态基准上都能持续减少幻觉，同时保持整体多模态理解能力；在预填充阶段操作，与现有解码阶段方法完全兼容。

Conclusion: 从表示层面理解LVLM幻觉机制是有效的，图像特定神经元的异常激活是幻觉主要来源；CNS通过神经元级干预能有效提升视觉基础、减少幻觉，为LVLM可靠性提供新途径。

Abstract: LVLMs achieve remarkable multimodal understanding and generation but remain susceptible to hallucinations. Existing mitigation methods predominantly focus on output-level adjustments, leaving the internal mechanisms that give rise to these hallucinations largely unexplored. To gain a deeper understanding, we adopt a representation-level perspective by introducing sparse autoencoders (SAEs) to decompose dense visual embeddings into sparse, interpretable neurons. Through neuron-level analysis, we identify distinct neuron types, including always-on neurons and image-specific neurons. Our findings reveal that hallucinations often result from disruptions or spurious activations of image-specific neurons, while always-on neurons remain largely stable. Moreover, selectively enhancing or suppressing image-specific neurons enables controllable intervention in LVLM outputs, improving visual grounding and reducing hallucinations. Building on these insights, we propose Contrastive Neuron Steering (CNS), which identifies image-specific neurons via contrastive analysis between clean and noisy inputs. CNS selectively amplifies informative neurons while suppressing perturbation-induced activations, producing more robust and semantically grounded visual representations. This not only enhances visual understanding but also effectively mitigates hallucinations. By operating at the prefilling stage, CNS is fully compatible with existing decoding-stage methods. Extensive experiments on both hallucination-focused and general multimodal benchmarks demonstrate that CNS consistently reduces hallucinations while preserving overall multimodal understanding.

</details>


### [197] [DVLA-RL: Dual-Level Vision-Language Alignment with Reinforcement Learning Gating for Few-Shot Learning](https://arxiv.org/abs/2602.00795)
*Wenhao Li,Xianjing Meng,Qiangchang Wang,Zhongyi Han,Zhibin Wu,Yilong Yin*

Main category: cs.CV

Relevance: 85.0

TL;DR: DVLA-RL提出了一种新的少样本学习方法，通过双级语义构建和强化学习门控注意力机制，实现视觉与语言从低层到高层的渐进对齐，在九个基准测试中达到SOTA性能。


<details>
  <summary>Details</summary>
Motivation: 现有少样本学习方法虽然利用LLM从类别名称生成语义嵌入，但忽略了视觉与语言从低层到高层的渐进自适应对齐，导致语义增益有限。需要更精细的跨模态对齐机制来提升少样本学习性能。

Method: 提出DVLA-RL框架：1) 双级语义构建(DSC)：基于类别名称和支持样本，让LLM生成判别性属性，渐进选择最相关属性并合成连贯类别描述；2) RL门控注意力(RLA)：将跨模态融合建模为序列决策过程，通过轻量级策略自适应调整自注意力和交叉注意力的贡献，实现浅层关注局部属性、深层关注全局语义的渐进对齐。

Result: 在三种不同的少样本学习场景下的九个基准测试中均达到新的最先进性能，证明了方法的有效性。

Conclusion: DVLA-RL通过双级语义构建和强化学习门控的渐进跨模态对齐，实现了更精确的视觉-语言对齐，提升了少样本学习的判别性和泛化能力。

Abstract: Few-shot learning (FSL) aims to generalize to novel categories with only a few samples. Recent approaches incorporate large language models (LLMs) to enrich visual representations with semantic embeddings derived from class names. However, they overlook progressive and adaptive alignment between vision and language from low-level to high-level semantics, resulting in limited semantic gains. To address these challenges, we propose Dual-level Vision-Language Alignment with Reinforcement Learning gating (DVLA-RL), which consists of Dual-level Semantic Construction (DSC) and RL-gated Attention (RLA). Specifically, DSC conditions LLMs on both class names and support samples to generate discriminative attributes, progressively selects the most relevant ones, and then synthesizes them into coherent class descriptions. This process provides complementary low-level attributes and high-level descriptions, enabling both fine-grained grounding and holistic class understanding. To dynamically integrate dual-level semantics along with the visual network layers, RLA formulates cross-modal fusion as a sequential decision process. A lightweight policy trained with episodic REINFORCE adaptively adjusts the contributions of self-attention and cross-attention to integrate textual and visual tokens. As a result, shallow layers refine local attributes and deep layers emphasize global semantics, enabling more precise cross-modal alignment. This achieves class-specific discrimination and generalized representations with merely a few support samples. DVLA-RL achieves new state-of-the-art performance across nine benchmarks in three diverse FSL scenarios.

</details>


### [198] [OCTOPUS: Enhancing the Spatial-Awareness of Vision SSMs with Multi-Dimensional Scans and Traversal Selection](https://arxiv.org/abs/2602.00904)
*Kunal Mahatha,Ali Bahri,Pierre Marza,Sahar Dastani,Maria Vakalopoulou,Stergios Christodoulidis,Jose Dolz,Christian Desrosiers*

Main category: cs.CV

Relevance: 85.0

TL;DR: OCTOPUS是一种新颖的视觉架构，通过八方向离散循环解决传统状态空间模型在视觉任务中的因果性限制，同时保持线性复杂度，有效捕捉全局上下文和局部空间结构。


<details>
  <summary>Details</summary>
Motivation: 状态空间模型（SSMs）在文本处理中表现出色，但在视觉任务中受限，因为其因果性设计破坏了图像的空间关系，无法有效捕捉局部空间一致性，经常连接非相邻的patch而忽略视觉相关的邻近区域。

Method: 提出OCTOPUS架构，沿八个主要方向（水平、垂直和对角线的正反方向）执行离散循环，允许所有空间连接区域间的有效信息交换，同时保持不相关patch间的独立性，实现多方向循环。

Result: 在分类和分割基准测试中，OCTOPUS在边界保持和区域一致性方面表现出显著改进，同时相比现有的V-SSM模型保持了相对更好的分类准确性。

Conclusion: OCTOPUS作为一种基础方法，展示了多方向循环作为构建空间感知且计算高效的视觉架构的可扩展有效机制。

Abstract: State space models (SSMs) have recently emerged as an alternative to transformers due to their unique ability of modeling global relationships in text with linear complexity. However, their success in vision tasks has been limited due to their causal formulation, which is suitable for sequential text but detrimental in the spatial domain where causality breaks the inherent spatial relationships among pixels or patches. As a result, standard SSMs fail to capture local spatial coherence, often linking non-adjacent patches while ignoring neighboring ones that are visually correlated. To address these limitations, we introduce OCTOPUS , a novel architecture that preserves both global context and local spatial structure within images, while maintaining the linear complexity of SSMs. OCTOPUS performs discrete reoccurrence along eight principal orientations, going forward or backward in the horizontal, vertical, and diagonal directions, allowing effective information exchange across all spatially connected regions while maintaining independence among unrelated patches. This design enables multi-directional recurrence, capturing both global context and local spatial structure with SSM-level efficiency. In our classification and segmentation benchmarks, OCTOPUS demonstrates notable improvements in boundary preservation and region consistency, as evident from the segmentation results, while maintaining relatively better classification accuracy compared to existing V-SSM based models. These results suggest that OCTOPUS appears as a foundation method for multi-directional recurrence as a scalable and effective mechanism for building spatially aware and computationally efficient vision architectures.

</details>


### [199] [ConsensusDrop: Fusing Visual and Cross-Modal Saliency for Efficient Vision Language Models](https://arxiv.org/abs/2602.00946)
*Dhruv Parikh,Haoyang Fan,Rajgopal Kannan,Viktor Prasanna*

Main category: cs.CV

Relevance: 85.0

TL;DR: 提出ConsensusDrop框架，融合视觉编码器显著性和LLM跨注意力，实现高效视觉token压缩，在保持精度的同时减少计算开销


<details>
  <summary>Details</summary>
Motivation: 现有视觉语言模型处理大量冗余视觉token成本高昂，现有token缩减方法要么使用视觉编码器显著性（广泛但查询无关），要么使用LLM跨注意力（查询感知但稀疏且成本高），两者单独都不够有效

Method: 提出ConsensusDrop训练免费框架，通过协调视觉编码器显著性和查询感知跨注意力获得共识排名，保留最有信息量的token，同时通过编码器引导的token合并压缩其余token

Result: 在LLaVA-1.5/NeXT、Video-LLaVA等开源VLM上，ConsensusDrop在相同token预算下优于现有剪枝方法，提供更好的精度-效率Pareto前沿，即使在激进token缩减下也能保持接近基线精度，同时减少TTFT和KV缓存占用

Conclusion: 融合视觉编码器显著性和LLM跨注意力的共识方法能有效压缩视觉token，显著提升VLM效率，为实际部署提供实用解决方案

Abstract: Vision-Language Models (VLMs) are expensive because the LLM processes hundreds of largely redundant visual tokens. Existing token reduction methods typically exploit \textit{either} vision-encoder saliency (broad but query-agnostic) \textit{or} LLM cross-attention (query-aware but sparse and costly). We show that neither signal alone is sufficient: fusing them consistently improves performance compared to unimodal visual token selection (ranking). However, making such fusion practical is non-trivial: cross-modal saliency is usually only available \emph{inside} the LLM (too late for efficient pre-LLM pruning), and the two signals are inherently asymmetric, so naive fusion underutilizes their complementary strengths. We propose \textbf{ConsensusDrop}, a training-free framework that derives a \emph{consensus} ranking by reconciling vision encoder saliency with query-aware cross-attention, retaining the most informative tokens while compressing the remainder via encoder-guided token merging. Across LLaVA-1.5/NeXT, Video-LLaVA, and other open-source VLMs, ConsensusDrop consistently outperforms prior pruning methods under identical token budgets and delivers a stronger accuracy-efficiency Pareto frontier -- preserving near-baseline accuracy even at aggressive token reductions while reducing TTFT and KV cache footprint. Our code will be open-sourced.

</details>


### [200] [VEQ: Modality-Adaptive Quantization for MoE Vision-Language Models](https://arxiv.org/abs/2602.01037)
*Guangshuo Qin,Zhiteng Li,Zheng Chen,Weihang Zhang,Linghe Kong,Yulun Zhang*

Main category: cs.CV

Relevance: 85.0

TL;DR: 提出了VEQ框架，一种针对MoE视觉语言模型的双感知量化方法，同时考虑跨模态差异和专家异质性，显著提升量化性能。


<details>
  <summary>Details</summary>
Motivation: MoE视觉语言模型虽然性能优异，但内存和计算成本过高，需要压缩。现有量化方法忽略了两个关键异质性：视觉和语言token之间的差异，以及不同专家贡献的非均匀性。

Method: 提出VEQ框架，包含：1）模态专家感知量化，利用专家激活频率优先最小化关键专家的误差；2）模态亲和感知量化，通过整合token-专家亲和度与模态信息构建增强的Hessian矩阵来指导校准过程。

Result: 在W3A16配置下，相比之前SOTA量化方法，在Kimi-VL上平均准确率提升2.04%，在Qwen3-VL上提升3.09%，在各种多模态任务上表现出优越的鲁棒性。

Conclusion: VEQ通过同时适应跨模态差异和专家异质性，为MoE视觉语言模型提供了一种有效的训练后量化解决方案，显著优于现有方法。

Abstract: Mixture-of-Experts(MoE) Vision-Language Models (VLMs) offer remarkable performance but incur prohibitive memory and computational costs, making compression essential. Post-Training Quantization (PTQ) is an effective training-free technique to address the massive memory and computation overhead. Existing quantization paradigms fall short as they are oblivious to two critical forms of heterogeneity: the inherent discrepancy between vision and language tokens, and the non-uniform contribution of different experts. To bridge this gap, we propose Visual Expert Quantization (VEQ), a dual-aware quantization framework designed to simultaneously accommodate cross-modal differences and heterogeneity between experts. Specifically, VEQ incorporates 1)Modality-expert-aware Quantization, which utilizes expert activation frequency to prioritize error minimization for pivotal experts, and 2)Modality-affinity-aware Quantization, which constructs an enhanced Hessian matrix by integrating token-expert affinity with modality information to guide the calibration process. Extensive experiments across diverse benchmarks verify that VEQ consistently outperforms state-of-the-art baselines. Specifically, under the W3A16 configuration, our method achieves significant average accuracy gains of 2.04\% on Kimi-VL and 3.09\% on Qwen3-VL compared to the previous SOTA quantization methods, demonstrating superior robustness across various multimodal tasks. Our code will be available at https://github.com/guangshuoqin/VEQ.

</details>


### [201] [PISA: Piecewise Sparse Attention Is Wiser for Efficient Diffusion Transformers](https://arxiv.org/abs/2602.01077)
*Haopeng Li,Shitong Shao,Wenliang Zhong,Zikai Zhou,Lichen Bai,Hui Xiong,Zeke Xie*

Main category: cs.CV

Relevance: 85.0

TL;DR: PISA是一种训练无关的分段稀疏注意力机制，通过精确计算关键块并用泰勒展开近似非关键块，在保持质量的同时实现亚二次复杂度，显著加速扩散Transformer的推理。


<details>
  <summary>Details</summary>
Motivation: 扩散Transformer在视频和图像生成中至关重要，但其注意力机制的二次复杂度限制了效率。现有的块稀疏注意力虽然通过仅关注关键块来加速计算，但在高稀疏度下会因丢弃上下文信息而导致质量下降。

Method: 提出PISA（Piecewise Sparse Attention），采用"精确或近似"策略而非传统的"保留或丢弃"范式。该方法发现非关键块的注意力分数具有分布稳定性，可通过块级泰勒展开高效近似，同时精确计算关键块，从而以亚二次复杂度覆盖完整注意力范围。

Result: 实验表明，PISA在Wan2.1-14B上实现1.91倍加速，在Hunyuan-Video上实现2.57倍加速，同时在稀疏注意力方法中保持最高质量。在FLUX上进行图像生成时，PISA实现1.2倍加速且不影响视觉质量。

Conclusion: PISA通过创新的分段稀疏注意力设计，有效解决了稀疏注意力在高稀疏度下的质量下降问题，在扩散Transformer中实现了速度与质量的良好平衡，为高效视频和图像生成提供了新方案。

Abstract: Diffusion Transformers are fundamental for video and image generation, but their efficiency is bottlenecked by the quadratic complexity of attention. While block sparse attention accelerates computation by attending only critical key-value blocks, it suffers from degradation at high sparsity by discarding context. In this work, we discover that attention scores of non-critical blocks exhibit distributional stability, allowing them to be approximated accurately and efficiently rather than discarded, which is essentially important for sparse attention design. Motivated by this key insight, we propose PISA, a training-free Piecewise Sparse Attention that covers the full attention span with sub-quadratic complexity. Unlike the conventional keep-or-drop paradigm that directly drop the non-critical block information, PISA introduces a novel exact-or-approximate strategy: it maintains exact computation for critical blocks while efficiently approximating the remainder through block-wise Taylor expansion. This design allows PISA to serve as a faithful proxy to full attention, effectively bridging the gap between speed and quality. Experimental results demonstrate that PISA achieves 1.91 times and 2.57 times speedups on Wan2.1-14B and Hunyuan-Video, respectively, while consistently maintaining the highest quality among sparse attention methods. Notably, even for image generation on FLUX, PISA achieves a 1.2 times acceleration without compromising visual quality. Code is available at: https://github.com/xie-lab-ml/piecewise-sparse-attention.

</details>


### [202] [MedAD-R1: Eliciting Consistent Reasoning in Interpretible Medical Anomaly Detection via Consistency-Reinforced Policy Optimization](https://arxiv.org/abs/2602.01081)
*Haitao Zhang,Yingying Wang,Jiaxiang Wang,Haote Xu,Hongyang Zhang,Yirong Chen,Yue Huang,Xinghao Ding*

Main category: cs.CV

Relevance: 85.0

TL;DR: 论文提出了MedAD-38K基准和MedAD-R1模型，通过两阶段训练框架（认知注入+一致性组相对策略优化）提升医学异常检测的多模态推理能力，在基准上取得SOTA性能。


<details>
  <summary>Details</summary>
Motivation: 当前医学异常检测（MedAD）依赖监督微调，但数据集简单碎片化，导致模型缺乏合理推理能力和鲁棒的多模态泛化能力。需要构建大规模多模态基准并开发能生成透明、逻辑一致推理路径的模型。

Method: 1) 构建MedAD-38K基准：首个大规模、多模态、多中心的医学异常检测基准，包含诊断思维链标注和结构化视觉问答对。2) 两阶段训练框架：第一阶段认知注入（SFT）注入基础医学知识并对齐思维-回答范式；第二阶段一致性组相对策略优化（Con-GRPO），引入一致性奖励确保推理过程与最终诊断相关且逻辑一致。

Result: 提出的MedAD-R1模型在MedAD-38K基准上取得SOTA性能，超越强基线10%以上。模型能生成透明且逻辑一致的推理路径，提升了AI临床决策支持的可信度和可解释性。

Conclusion: 该工作通过构建大规模基准和创新训练框架，显著提升了医学异常检测的推理能力和可解释性，为临床决策支持AI的可信度提供了有前景的解决方案。

Abstract: Medical Anomaly Detection (MedAD) presents a significant opportunity to enhance diagnostic accuracy using Large Multimodal Models (LMMs) to interpret and answer questions based on medical images. However, the reliance on Supervised Fine-Tuning (SFT) on simplistic and fragmented datasets has hindered the development of models capable of plausible reasoning and robust multimodal generalization. To overcome this, we introduce MedAD-38K, the first large-scale, multi-modal, and multi-center benchmark for MedAD featuring diagnostic Chain-of-Thought (CoT) annotations alongside structured Visual Question-Answering (VQA) pairs. On this foundation, we propose a two-stage training framework. The first stage, Cognitive Injection, uses SFT to instill foundational medical knowledge and align the model with a structured think-then-answer paradigm. Given that standard policy optimization can produce reasoning that is disconnected from the final answer, the second stage incorporates Consistency Group Relative Policy Optimization (Con-GRPO). This novel algorithm incorporates a crucial consistency reward to ensure the generated reasoning process is relevant and logically coherent with the final diagnosis. Our proposed model, MedAD-R1, achieves state-of-the-art (SOTA) performance on the MedAD-38K benchmark, outperforming strong baselines by more than 10\%. This superior performance stems from its ability to generate transparent and logically consistent reasoning pathways, offering a promising approach to enhancing the trustworthiness and interpretability of AI for clinical decision support.

</details>


### [203] [Med3D-R1: Incentivizing Clinical Reasoning in 3D Medical Vision-Language Models for Abnormality Diagnosis](https://arxiv.org/abs/2602.01200)
*Haoran Lai,Zihang Jiang,Kun Zhang,Qingsong Yao,Rongsheng Wang,Zhiyang He,Xiaodong Tao,Wei Wei,Shaohua Kevin Zhou*

Main category: cs.CV

Relevance: 85.0

TL;DR: Med3D-R1：用于3D医学视觉语言模型的强化学习框架，通过两阶段训练（SFT+RL）提升临床推理能力，在CT-RATE和RAD-ChestCT基准上达到SOTA


<details>
  <summary>Details</summary>
Motivation: 开发具有鲁棒临床推理能力的3D视觉语言模型面临挑战：体素医学影像复杂、模型易过拟合报告表面模式、缺乏可解释性奖励设计

Method: 两阶段强化学习框架：1) SFT阶段：引入残差对齐机制连接3D特征与文本嵌入，异常重加权策略强调临床信息标记；2) RL阶段：重新设计一致性奖励以促进逐步诊断推理

Result: 在CT-RATE和RAD-ChestCT两个3D诊断基准上达到SOTA：CT-RATE 41.92%，RAD-ChestCT 44.99%，显示异常诊断和临床推理能力提升

Conclusion: 该方法有望通过实现更可靠、透明的3D医学视觉语言系统来增强真实世界诊断工作流程

Abstract: Developing 3D vision-language models with robust clinical reasoning remains a challenge due to the inherent complexity of volumetric medical imaging, the tendency of models to overfit superficial report patterns, and the lack of interpretability-aware reward designs. In this paper, we propose Med3D-R1, a reinforcement learning framework with a two-stage training process: Supervised Fine-Tuning (SFT) and Reinforcement Learning (RL). During SFT stage, we introduce a residual alignment mechanism to bridge the gap between high-dimensional 3D features and textual embeddings, and an abnormality re-weighting strategy to emphasize clinically informative tokens and reduce structural bias in reports. In RL stage, we redesign the consistency reward to explicitly promote coherent, step-by-step diagnostic reasoning. We evaluate our method on medical multiple-choice visual question answering using two 3D diagnostic benchmarks, CT-RATE and RAD-ChestCT, where our model attains state-of-the-art accuracies of 41.92\% on CT-RATE and 44.99\% on RAD-ChestCT. These results indicate improved abnormality diagnosis and clinical reasoning and outperform prior methods on both benchmarks. Overall, our approach holds promise for enhancing real-world diagnostic workflows by enabling more reliable and transparent 3D medical vision-language systems.

</details>


### [204] [Who Transfers Safety? Identifying and Targeting Cross-Lingual Shared Safety Neurons](https://arxiv.org/abs/2602.01283)
*Xianhui Zhang,Chengyu Xie,Linxia Zhu,Yonghui Yang,Weixiang Zhao,Zifeng Cheng,Cong Wang,Fei Shen,Tat-Seng Chua*

Main category: cs.CV

Relevance: 85.0

TL;DR: 该论文发现LLMs中存在跨语言共享的安全神经元（SS-Neurons），这是一个微小但关键的神经元子集，共同调节跨语言的安全行为。通过针对这些神经元的训练策略，可以显著提升非高资源语言的安全性。


<details>
  <summary>Details</summary>
Motivation: 多语言安全性存在显著不平衡，非高资源语言的安全性远不如高资源语言。此外，尽管观察到跨语言表示传递，但驱动安全对齐的神经机制仍不清楚。需要理解LLMs中安全对齐的神经基础，并解决非高资源语言的安全漏洞。

Method: 1. 首先识别单语言安全神经元（MS-Neurons），并通过定向激活和抑制验证其在安全拒绝行为中的因果作用。
2. 跨语言分析识别SS-Neurons作为高资源语言和非高资源语言之间共享的MS-Neurons子集。
3. 提出基于语言资源分布和模型架构的神经元导向训练策略，针对SS-Neurons进行微调。

Result: 实验表明，抑制SS-Neurons会导致非高资源语言的安全性同时下降，而增强它们能提高跨语言防御一致性。微调这个微小神经元子集优于现有方法，显著提升非高资源语言安全性，同时保持模型的通用能力。

Conclusion: LLMs中存在跨语言共享的安全神经元，这些神经元作为桥梁将安全能力从高资源语言传递到非高资源语言。针对这些神经元的训练策略是提升多语言安全性的有效方法，为理解安全对齐的神经机制提供了新视角。

Abstract: Multilingual safety remains significantly imbalanced, leaving non-high-resource (NHR) languages vulnerable compared to robust high-resource (HR) ones. Moreover, the neural mechanisms driving safety alignment remain unclear despite observed cross-lingual representation transfer.
  In this paper, we find that LLMs contain a set of cross-lingual shared safety neurons (SS-Neurons), a remarkably small yet critical neuronal subset that jointly regulates safety behavior across languages.
  We first identify monolingual safety neurons (MS-Neurons) and validate their causal role in safety refusal behavior through targeted activation and suppression.
  Our cross-lingual analyses then identify SS-Neurons as the subset of MS-Neurons shared between HR and NHR languages, serving as a bridge to transfer safety capabilities from HR to NHR domains.
  We observe that suppressing these neurons causes concurrent safety drops across NHR languages, whereas reinforcing them improves cross-lingual defensive consistency.
  Building on these insights, we propose a simple neuron-oriented training strategy that targets SS-Neurons based on language resource distribution and model architecture. Experiments demonstrate that fine-tuning this tiny neuronal subset outperforms state-of-the-art methods, significantly enhancing NHR safety while maintaining the model's general capabilities.
  The code and dataset will be available athttps://github.com/1518630367/SS-Neuron-Expansion.

</details>


### [205] [StoryState: Agent-Based State Control for Consistent and Editable Storybooks](https://arxiv.org/abs/2602.01305)
*Ayushman Sarkar,Zhenyu Yu,Wei Tang,Chu Chen,Kangning Cui,Mohd Yamani Idna Idris*

Main category: cs.CV

Relevance: 85.0

TL;DR: StoryState是一个基于代理的编排层，为训练无关的文生图模型引入显式可编辑的故事状态，通过结构化表示（角色表、全局设置、页面场景约束）和LLM代理维护状态，实现细粒度编辑和跨页面一致性。


<details>
  <summary>Details</summary>
Motivation: 当前多模态模型支持一键生成故事书，但故事状态（角色、世界设置、页面级对象）是隐式的，导致编辑粗粒度且破坏视觉一致性。需要显式可编辑的故事状态来支持细粒度编辑并保持一致性。

Method: 1. 将故事表示为结构化对象：角色表、全局设置、每页场景约束；2. 使用少量LLM代理维护状态并生成1Prompt1Story风格的提示；3. 纯提示操作，模型无关，兼容多种生成后端。

Result: 在多页面编辑任务中，StoryState支持局部页面编辑，提高跨页面一致性，减少意外更改、交互轮次和编辑时间，相比1Prompt1Story方法，同时接近Gemini Storybook的一次性一致性。

Conclusion: StoryState通过显式故事状态和代理编排，解决了多页面故事生成中的编辑粗粒度和一致性破坏问题，为训练无关的文生图模型提供了有效的状态管理和编辑框架。

Abstract: Large multimodal models have enabled one-click storybook generation, where users provide a short description and receive a multi-page illustrated story. However, the underlying story state, such as characters, world settings, and page-level objects, remains implicit, making edits coarse-grained and often breaking visual consistency. We present StoryState, an agent-based orchestration layer that introduces an explicit and editable story state on top of training-free text-to-image generation. StoryState represents each story as a structured object composed of a character sheet, global settings, and per-page scene constraints, and employs a small set of LLM agents to maintain this state and derive 1Prompt1Story-style prompts for generation and editing. Operating purely through prompts, StoryState is model-agnostic and compatible with diverse generation backends. System-level experiments on multi-page editing tasks show that StoryState enables localized page edits, improves cross-page consistency, and reduces unintended changes, interaction turns, and editing time compared to 1Prompt1Story, while approaching the one-shot consistency of Gemini Storybook. Code is available at https://github.com/YuZhenyuLindy/StoryState

</details>


### [206] [FlowCast: Trajectory Forecasting for Scalable Zero-Cost Speculative Flow Matching](https://arxiv.org/abs/2602.01329)
*Divya Jyoti Bajpai,Shubham Agarwal,Apoorv Saxena,Kuldeep Kulkarni,Subrata Mitra,Manjesh Kumar Hanawal*

Main category: cs.CV

Relevance: 85.0

TL;DR: FlowCast：一种无需训练、基于推测生成的Flow Matching加速框架，通过利用FM模型保持恒定速度的特性，在稳定区域跳过冗余步骤，实现2.5倍以上的加速而不损失生成质量。


<details>
  <summary>Details</summary>
Motivation: Flow Matching在视觉生成中表现出色，但其推理速度过慢（需要大量去噪步骤）限制了在实时或交互应用中的使用。现有加速方法（如蒸馏、截断或一致性训练）要么降低质量，要么需要昂贵重训练，要么缺乏泛化性。

Method: FlowCast利用FM模型训练保持恒定速度的特性，通过外推当前速度来推测未来速度，如果均方误差在阈值内则接受推测。这种恒定速度预测允许在稳定区域激进跳过冗余步骤，同时在复杂区域保持精度。无需训练，无需辅助网络，可即插即用。

Result: FlowCast在图像生成、视频生成和编辑任务中实现超过2.5倍的加速，优于现有基线方法，且与标准完整生成相比没有质量损失。

Conclusion: FlowCast是一种有效的训练免费FM加速框架，通过利用恒定速度特性进行推测生成，在保持质量的同时显著提升推理速度，适用于实时应用。

Abstract: Flow Matching (FM) has recently emerged as a powerful approach for high-quality visual generation. However, their prohibitively slow inference due to a large number of denoising steps limits their potential use in real-time or interactive applications. Existing acceleration methods, like distillation, truncation, or consistency training, either degrade quality, incur costly retraining, or lack generalization. We propose FlowCast, a training-free speculative generation framework that accelerates inference by exploiting the fact that FM models are trained to preserve constant velocity. FlowCast speculates future velocity by extrapolating current velocity without incurring additional time cost, and accepts it if it is within a mean-squared error threshold. This constant-velocity forecasting allows redundant steps in stable regions to be aggressively skipped while retaining precision in complex ones. FlowCast is a plug-and-play framework that integrates seamlessly with any FM model and requires no auxiliary networks. We also present a theoretical analysis and bound the worst-case deviation between speculative and full FM trajectories. Empirical evaluations demonstrate that FlowCast achieves $>2.5\times$ speedup in image generation, video generation, and editing tasks, outperforming existing baselines with no quality loss as compared to standard full generation.

</details>


### [207] [Adaptive Visual Autoregressive Acceleration via Dual-Linkage Entropy Analysis](https://arxiv.org/abs/2602.01345)
*Yu Zhang,Jingyi Liu,Feng Liu,Duoqian Miao,Qi Zhang,Kexue Fu,Changwei Wang,Longbing Cao*

Main category: cs.CV

Relevance: 85.0

TL;DR: NOVA是一种基于熵分析的无训练token缩减加速框架，用于视觉自回归模型(VAR)，通过自适应识别尺度熵增长拐点来动态计算各尺度和层的token缩减比例，在保持生成质量的同时加速推理。


<details>
  <summary>Details</summary>
Motivation: 现有VAR token缩减方法存在三个关键限制：启发式阶段划分、非自适应调度和有限加速范围，未能充分利用加速潜力。熵变化本质上反映了预测不确定性的转变，为捕捉建模动态演化提供了原则性度量。

Method: NOVA通过在线识别尺度熵增长拐点来自适应确定推理过程中的加速激活尺度。通过尺度链接和层链接比例调整，动态计算每个尺度和层的不同token缩减比例，剪枝低熵token，同时重用先前尺度残差中的缓存来加速推理。

Result: 广泛的实验和分析验证了NOVA作为一个简单而有效的无训练加速框架的有效性，在保持生成质量的同时显著加速了VAR模型的推理过程。

Conclusion: NOVA通过熵分析提供了一种原则性的方法来捕捉VAR建模动态演化，解决了现有token缩减方法的局限性，为视觉自回归模型提供了有效的训练free加速解决方案。

Abstract: Visual AutoRegressive modeling (VAR) suffers from substantial computational cost due to the massive token count involved. Failing to account for the continuous evolution of modeling dynamics, existing VAR token reduction methods face three key limitations: heuristic stage partition, non-adaptive schedules, and limited acceleration scope, thereby leaving significant acceleration potential untapped. Since entropy variation intrinsically reflects the transition of predictive uncertainty, it offers a principled measure to capture modeling dynamics evolution. Therefore, we propose NOVA, a training-free token reduction acceleration framework for VAR models via entropy analysis. NOVA adaptively determines the acceleration activation scale during inference by online identifying the inflection point of scale entropy growth. Through scale-linkage and layer-linkage ratio adjustment, NOVA dynamically computes distinct token reduction ratios for each scale and layer, pruning low-entropy tokens while reusing the cache derived from the residuals at the prior scale to accelerate inference and maintain generation quality. Extensive experiments and analyses validate NOVA as a simple yet effective training-free acceleration framework.

</details>


### [208] [Exposing and Defending the Achilles' Heel of Video Mixture-of-Experts](https://arxiv.org/abs/2602.01369)
*Songping Wang,Qinglong Liu,Yueming Lyu,Ning Li,Ziwen He,Caifeng Shan*

Main category: cs.CV

Relevance: 85.0

TL;DR: 本文提出TLGA攻击框架，系统研究视频MoE模型的组件级脆弱性，包括路由器独立弱点和路由器-专家协同弱点，并开发J-TLAT防御方法增强鲁棒性。


<details>
  <summary>Details</summary>
Motivation: MoE在视频理解中表现优异，但其对抗鲁棒性研究不足。现有攻击方法将MoE视为统一架构，忽视了路由器和专家模块的独立与协同弱点。

Method: 提出Temporal Lipschitz-Guided Attacks (TLGA)：1) 针对路由器的独立攻击；2) Joint TLGA (J-TLGA)协同扰动路由器和专家；3) Joint Temporal Lipschitz Adversarial Training (J-TLAT)进行联合训练防御。

Result: TLGA暴露了MoE的组件级脆弱性，J-TLGA显著增强攻击效果，J-TLAT有效提升对抗鲁棒性，框架即插即用，推理成本比密集模型降低60%以上。

Conclusion: MoE架构存在组件级脆弱性，需要针对性的攻击和防御方法。提出的TLGA/J-TLAT框架能有效识别和缓解独立与协同弱点，增强视频MoE模型的对抗鲁棒性。

Abstract: Mixture-of-Experts (MoE) has demonstrated strong performance in video understanding tasks, yet its adversarial robustness remains underexplored. Existing attack methods often treat MoE as a unified architecture, overlooking the independent and collaborative weaknesses of key components such as routers and expert modules. To fill this gap, we propose Temporal Lipschitz-Guided Attacks (TLGA) to thoroughly investigate component-level vulnerabilities in video MoE models. We first design attacks on the router, revealing its independent weaknesses. Building on this, we introduce Joint Temporal Lipschitz-Guided Attacks (J-TLGA), which collaboratively perturb both routers and experts. This joint attack significantly amplifies adversarial effects and exposes the Achilles' Heel (collaborative weaknesses) of the MoE architecture. Based on these insights, we further propose Joint Temporal Lipschitz Adversarial Training (J-TLAT). J-TLAT performs joint training to further defend against collaborative weaknesses, enhancing component-wise robustness. Our framework is plug-and-play and reduces inference cost by more than 60% compared with dense models. It consistently enhances adversarial robustness across diverse datasets and architectures, effectively mitigating both the independent and collaborative weaknesses of MoE.

</details>


### [209] [PromptRL: Prompt Matters in RL for Flow-Based Image Generation](https://arxiv.org/abs/2602.01382)
*Fu-Yun Wang,Han Zhang,Michael Gharbi,Hongsheng Li,Taesung Park*

Main category: cs.CV

Relevance: 85.0

TL;DR: 提出PromptRL框架，将语言模型作为可训练的提示词优化器集成到流匹配模型的强化学习循环中，解决样本效率低和提示词过拟合问题，在多个基准测试中达到SOTA性能。


<details>
  <summary>Details</summary>
Motivation: 当前流匹配模型的强化学习流程存在两个被低估但重要的问题：1）由于生成多样性不足导致的样本效率低下；2）明显的提示词过拟合，模型会记忆特定的训练表述，在语义相同但风格变化的提示词上出现性能崩溃。

Method: 提出PromptRL框架，将语言模型作为可训练的提示词优化代理直接集成到基于流的强化学习优化循环中。这种方法有两个互补优势：快速开发复杂的提示词重写能力，以及关键的协同训练机制，重塑优化动态。

Result: 在多个基准测试中达到SOTA：GenEval 0.97、OCR准确率0.98、PickScore 24.05。在大规模图像编辑模型上验证有效性，将FLUX.1-Kontext的EditReward从1.19提升到1.43，仅需0.06百万次rollout，超越Gemini 2.5 Flash Image（1.37），与需要细粒度数据标注和多阶段训练的ReasonNet（1.44）性能相当。相比朴素流模型RL，性能上限更高且需要2倍以上更少的rollout。

Conclusion: PromptRL通过将语言模型集成到流匹配模型的强化学习循环中，有效解决了样本效率低和提示词过拟合问题，在多个图像生成和编辑任务中实现了SOTA性能，展示了协同训练机制的优势。

Abstract: Flow matching models (FMs) have revolutionized text-to-image (T2I) generation, with reinforcement learning (RL) serving as a critical post-training strategy for alignment with reward objectives. In this research, we show that current RL pipelines for FMs suffer from two underappreciated yet important limitations: sample inefficiency due to insufficient generation diversity, and pronounced prompt overfitting, where models memorize specific training formulations and exhibit dramatic performance collapse when evaluated on semantically equivalent but stylistically varied prompts. We present PromptRL (Prompt Matters in RL for Flow-Based Image Generation), a framework that incorporates language models (LMs) as trainable prompt refinement agents directly within the flow-based RL optimization loop. This design yields two complementary benefits: rapid development of sophisticated prompt rewriting capabilities and, critically, a synergistic training regime that reshapes the optimization dynamics. PromptRL achieves state-of-the-art performance across multiple benchmarks, obtaining scores of 0.97 on GenEval, 0.98 on OCR accuracy, and 24.05 on PickScore.
  Furthermore, we validate the effectiveness of our RL approach on large-scale image editing models, improving the EditReward of FLUX.1-Kontext from 1.19 to 1.43 with only 0.06 million rollouts, surpassing Gemini 2.5 Flash Image (also known as Nano Banana), which scores 1.37, and achieving comparable performance with ReasonNet (1.44), which relied on fine-grained data annotations along with a complex multi-stage training. Our extensive experiments empirically demonstrate that PromptRL consistently achieves higher performance ceilings while requiring over 2$\times$ fewer rollouts compared to naive flow-only RL. Our code is available at https://github.com/G-U-N/UniRL.

</details>


### [210] [Toward Cognitive Supersensing in Multimodal Large Language Model](https://arxiv.org/abs/2602.01541)
*Boyi Li,Yifan Shen,Yuanzhe Liu,Yifan Xu,Jiateng Liu,Xinzhuo Li,Zhengyuan Li,Jingyuan Zhu,Yunhan Zhong,Fangzhou Lan,Jianguo Cao,James M. Rehg,Heng Ji,Ismini Lourentzou,Xu Cao*

Main category: cs.CV

Relevance: 85.0

TL;DR: 本文提出Cognitive Supersensing训练范式，通过Latent Visual Imagery Prediction (LVIP)头赋予MLLMs视觉意象能力，形成基于视觉的内部推理链，并引入强化学习优化文本推理路径。同时提出CogSense-Bench基准评估MLLMs的认知能力。


<details>
  <summary>Details</summary>
Motivation: 当前MLLMs在开放词汇感知任务上表现出色，但在解决需要视觉记忆和抽象视觉细节的复杂认知问题时能力有限。现有方法主要在文本空间扩展思维链推理，忽视了类似人类视觉空间画板和视觉意象的视觉推理机制。

Method: 1. 引入Cognitive Supersensing训练范式，通过LVIP头联合学习视觉认知潜在嵌入序列并与答案对齐，形成基于视觉的内部推理链。2. 引入强化学习阶段，基于接地的视觉潜在表示优化文本推理路径。3. 提出CogSense-Bench基准，评估五个认知维度。

Result: 实验表明，采用Cognitive Supersensing训练的MLLMs在CogSense-Bench上显著优于最先进基线，并在跨域数学和科学VQA基准上表现出更好的泛化能力，表明内部视觉意象可能是连接感知识别与认知理解的关键。

Conclusion: 内部视觉意象能力对于提升MLLMs的认知理解至关重要，Cognitive Supersensing范式有效弥补了当前MLLMs在视觉推理方面的不足，为连接感知与认知提供了新途径。

Abstract: Multimodal Large Language Models (MLLMs) have achieved remarkable success in open-vocabulary perceptual tasks, yet their ability to solve complex cognitive problems remains limited, especially when visual details are abstract and require visual memory. Current approaches primarily scale Chain-of-Thought (CoT) reasoning in the text space, even when language alone is insufficient for clear and structured reasoning, and largely neglect visual reasoning mechanisms analogous to the human visuospatial sketchpad and visual imagery. To mitigate this deficiency, we introduce Cognitive Supersensing, a novel training paradigm that endows MLLMs with human-like visual imagery capabilities by integrating a Latent Visual Imagery Prediction (LVIP) head that jointly learns sequences of visual cognitive latent embeddings and aligns them with the answer, thereby forming vision-based internal reasoning chains. We further introduce a reinforcement learning stage that optimizes text reasoning paths based on this grounded visual latent. To evaluate the cognitive capabilities of MLLMs, we present CogSense-Bench, a comprehensive visual question answering (VQA) benchmark assessing five cognitive dimensions. Extensive experiments demonstrate that MLLMs trained with Cognitive Supersensing significantly outperform state-of-the-art baselines on CogSense-Bench and exhibit superior generalization on out-of-domain mathematics and science VQA benchmarks, suggesting that internal visual imagery is potentially key to bridging the gap between perceptual recognition and cognitive understanding. We will open-source the CogSense-Bench and our model weights.

</details>


### [211] [Omni-Judge: Can Omni-LLMs Serve as Human-Aligned Judges for Text-Conditioned Audio-Video Generation?](https://arxiv.org/abs/2602.01623)
*Susan Liang,Chao Huang,Filippos Bellos,Yolo Yunlong Tang,Qianxiang Shen,Jing Bi,Luchuan Song,Zeliang Zhang,Jason Corso,Chenliang Xu*

Main category: cs.CV

Relevance: 85.0

TL;DR: Omni-Judge评估了全模态大语言模型（omni-LLMs）作为文本条件音视频生成的人类对齐评判者的能力，发现其在语义对齐任务上表现优异，但在高帧率感知指标上存在局限。


<details>
  <summary>Details</summary>
Motivation: 当前文本到视频生成模型（如Sora 2和Veo 3）能够从文本提示直接生成高质量视频和同步音频，但评估这种三模态输出仍是一个未解决的挑战。人工评估可靠但成本高且难以扩展，传统自动指标（如FVD、CLAP、ViCLIP）存在局限性：仅关注孤立模态对、难以处理复杂提示、可解释性有限。全模态大语言模型（omni-LLMs）提供了有前景的替代方案，因为它们能自然处理音频、视频和文本，支持丰富推理，并提供可解释的思维链反馈。

Method: 提出了Omni-Judge研究，评估omni-LLMs能否作为文本条件音视频生成的人类对齐评判者。研究涵盖了九个感知和对齐指标，将omni-LLMs的表现与传统指标进行比较，并分析其在不同任务类型上的性能差异。

Result: Omni-Judge在相关性方面与传统指标相当，在语义要求高的任务上表现出色，包括音频-文本对齐、视频-文本对齐和音频-视频-文本一致性。但在高帧率感知指标上表现不佳，包括视频质量和音频-视频同步，这主要受限于其时间分辨率。Omni-Judge提供了可解释的解释，能够暴露语义或物理不一致性，支持基于反馈的细化等实际下游应用。

Conclusion: 研究结果突显了omni-LLMs作为多模态生成统一评估器的潜力和当前局限性。虽然它们在语义对齐任务上表现优异，但在需要高时间分辨率的感知指标上仍需改进。可解释的反馈机制为实际应用提供了价值。

Abstract: State-of-the-art text-to-video generation models such as Sora 2 and Veo 3 can now produce high-fidelity videos with synchronized audio directly from a textual prompt, marking a new milestone in multi-modal generation. However, evaluating such tri-modal outputs remains an unsolved challenge. Human evaluation is reliable but costly and difficult to scale, while traditional automatic metrics, such as FVD, CLAP, and ViCLIP, focus on isolated modality pairs, struggle with complex prompts, and provide limited interpretability. Omni-modal large language models (omni-LLMs) present a promising alternative: they naturally process audio, video, and text, support rich reasoning, and offer interpretable chain-of-thought feedback. Driven by this, we introduce Omni-Judge, a study assessing whether omni-LLMs can serve as human-aligned judges for text-conditioned audio-video generation. Across nine perceptual and alignment metrics, Omni-Judge achieves correlation comparable to traditional metrics and excels on semantically demanding tasks such as audio-text alignment, video-text alignment, and audio-video-text coherence. It underperforms on high-FPS perceptual metrics, including video quality and audio-video synchronization, due to limited temporal resolution. Omni-Judge provides interpretable explanations that expose semantic or physical inconsistencies, enabling practical downstream uses such as feedback-based refinement. Our findings highlight both the potential and current limitations of omni-LLMs as unified evaluators for multi-modal generation.

</details>


### [212] [Contribution-aware Token Compression for Efficient Video Understanding via Reinforcement Learning](https://arxiv.org/abs/2602.01649)
*Yinchao Ma,Qiang Zhou,Zhibin Wang,Xianing Chen,Hanqing Yang,Jun Song,Bo Zheng*

Main category: cs.CV

Relevance: 85.0

TL;DR: CaCoVID提出了一种基于贡献感知的视频令牌压缩算法，通过强化学习优化令牌选择策略，显著降低视频大语言模型的推理计算开销。


<details>
  <summary>Details</summary>
Motivation: 视频大语言模型在视频理解任务中表现出色，但视频令牌的冗余性导致推理时计算开销巨大，限制了实际部署。现有压缩算法通常基于注意力分数保留特征，但注意力分数与对正确答案的实际贡献之间的关系不明确。

Method: 1. 提出基于强化学习的框架，优化策略网络选择对正确预测贡献最大的视频令牌组合；2. 提出在线组合空间采样的组合策略优化算法，大幅减少视频令牌组合的探索空间，加速策略优化收敛。

Result: 在多个视频理解基准测试上的广泛实验证明了CaCoVID的有效性，代码将开源发布。

Conclusion: CaCoVID通过显式优化基于令牌对正确预测贡献的令牌选择策略，实现了从被动令牌保留到主动发现最优压缩令牌组合的范式转变，显著提高了视频大语言模型的推理效率。

Abstract: Video large language models have demonstrated remarkable capabilities in video understanding tasks. However, the redundancy of video tokens introduces significant computational overhead during inference, limiting their practical deployment. Many compression algorithms are proposed to prioritize retaining features with the highest attention scores to minimize perturbations in attention computations. However, the correlation between attention scores and their actual contribution to correct answers remains ambiguous. To address the above limitation, we propose a novel \textbf{C}ontribution-\textbf{a}ware token \textbf{Co}mpression algorithm for \textbf{VID}eo understanding (\textbf{CaCoVID}) that explicitly optimizes the token selection policy based on the contribution of tokens to correct predictions. First, we introduce a reinforcement learning-based framework that optimizes a policy network to select video token combinations with the greatest contribution to correct predictions. This paradigm shifts the focus from passive token preservation to active discovery of optimal compressed token combinations. Secondly, we propose a combinatorial policy optimization algorithm with online combination space sampling, which dramatically reduces the exploration space for video token combinations and accelerates the convergence speed of policy optimization. Extensive experiments on diverse video understanding benchmarks demonstrate the effectiveness of CaCoVID. Codes will be released.

</details>


### [213] [FreshMem: Brain-Inspired Frequency-Space Hybrid Memory for Streaming Video Understanding](https://arxiv.org/abs/2602.01683)
*Kangcong Li,Peng Ye,Lin Zhang,Chao Wang,Huafeng Qin,Tao Chen*

Main category: cs.CV

Relevance: 85.0

TL;DR: FreshMem提出了一种频率-空间混合记忆网络，用于多模态大语言模型的在线流式视频理解，通过多尺度频率记忆和空间缩略图记忆模块，在保持短期保真度和长期连贯性方面取得显著提升。


<details>
  <summary>Details</summary>
Motivation: 现有方法缺乏灵活的适应性，导致不可逆的细节丢失和上下文碎片化。需要解决多模态大语言模型从离线到在线流式视频理解的过渡问题，实现连续感知。

Method: 受大脑对数感知和记忆巩固启发，提出FreshMem框架：1) 多尺度频率记忆(MFM)：将溢出帧投影到代表性频率系数，辅以残差细节重建全局历史"要点"；2) 空间缩略图记忆(STM)：通过自适应压缩策略将连续流离散化为情节聚类，蒸馏为高密度空间缩略图。

Result: 在StreamingBench、OV-Bench和OVO-Bench上分别提升5.20%、4.52%和2.34%。作为无需训练的方法，优于多个完全微调的方法。

Conclusion: FreshMem为长时域流式视频理解提供了高效范式，显著提升了多模态大语言模型的在线视频理解能力。

Abstract: Transitioning Multimodal Large Language Models (MLLMs) from offline to online streaming video understanding is essential for continuous perception. However, existing methods lack flexible adaptivity, leading to irreversible detail loss and context fragmentation. To resolve this, we propose FreshMem, a Frequency-Space Hybrid Memory network inspired by the brain's logarithmic perception and memory consolidation. FreshMem reconciles short-term fidelity with long-term coherence through two synergistic modules: Multi-scale Frequency Memory (MFM), which projects overflowing frames into representative frequency coefficients, complemented by residual details to reconstruct a global historical "gist"; and Space Thumbnail Memory (STM), which discretizes the continuous stream into episodic clusters by employing an adaptive compression strategy to distill them into high-density space thumbnails. Extensive experiments show that FreshMem significantly boosts the Qwen2-VL baseline, yielding gains of 5.20%, 4.52%, and 2.34% on StreamingBench, OV-Bench, and OVO-Bench, respectively. As a training-free solution, FreshMem outperforms several fully fine-tuned methods, offering a highly efficient paradigm for long-horizon streaming video understanding.

</details>


### [214] [Q Cache: Visual Attention is Valuable in Less than Half of Decode Layers for Multimodal Large Language Model](https://arxiv.org/abs/2602.01901)
*Jiedong Zhuang,Lu Lu,Ming Dai,Rui Hu,Jian Chen,Qiang Liu,Haoji Hu*

Main category: cs.CV

Relevance: 85.0

TL;DR: 提出Lazy Attention机制，通过跨层共享相似注意力模式来减少MLLMs中的冗余计算，使用轻量级Q Cache实现查询重用，显著降低KV缓存占用并提升推理吞吐量。


<details>
  <summary>Details</summary>
Motivation: 多模态大语言模型(MLLMs)面临高昂推理成本，主要源于视觉编码器产生的大量冗余视觉token。现有token剪枝方法虽然能减少token数量，但会破坏KV缓存完整性，导致长文本生成任务失败。作者从新视角分析模型注意力机制，发现超过一半解码层的注意力语义相似，因此提出跨层共享注意力模式来减少冗余计算。

Method: 提出Lazy Attention机制：1) 发现超过一半解码层的注意力语义相似，某些层的注意力可通过继承前层注意力来简化；2) 设计专门针对MLLMs的轻量级层共享缓存Q Cache，支持相邻层间查询重用；3) 方法完全兼容现有推理框架(Flash Attention和KV cache)，且与token剪枝方法正交，可独立部署或组合使用。

Result: 在多个基准测试中，该方法能减少超过35%的KV缓存使用，实现1.5倍吞吐量提升，在各种MLLMs上仅牺牲约1%性能。相比最先进的token剪枝方法，本技术实现了更好的精度保持。

Conclusion: Lazy Attention通过跨层共享相似注意力模式，有效减少MLLMs中的冗余计算，显著降低KV缓存占用并提升推理效率，同时保持模型性能。该方法与现有技术兼容且灵活，为MLLMs高效推理提供了新思路。

Abstract: Multimodal large language models (MLLMs) are plagued by exorbitant inference costs attributable to the profusion of visual tokens within the vision encoder. The redundant visual tokens engenders a substantial computational load and key-value (KV) cache footprint bottleneck. Existing approaches focus on token-wise optimization, leveraging diverse intricate token pruning techniques to eliminate non-crucial visual tokens. Nevertheless, these methods often unavoidably undermine the integrity of the KV cache, resulting in failures in long-text generation tasks. To this end, we conduct an in-depth investigation towards the attention mechanism of the model from a new perspective, and discern that attention within more than half of all decode layers are semantic similar. Upon this finding, we contend that the attention in certain layers can be streamlined by inheriting the attention from their preceding layers. Consequently, we propose Lazy Attention, an efficient attention mechanism that enables cross-layer sharing of similar attention patterns. It ingeniously reduces layer-wise redundant computation in attention. In Lazy Attention, we develop a novel layer-shared cache, Q Cache, tailored for MLLMs, which facilitates the reuse of queries across adjacent layers. In particular, Q Cache is lightweight and fully compatible with existing inference frameworks, including Flash Attention and KV cache. Additionally, our method is highly flexible as it is orthogonal to existing token-wise techniques and can be deployed independently or combined with token pruning approaches. Empirical evaluations on multiple benchmarks demonstrate that our method can reduce KV cache usage by over 35% and achieve 1.5x throughput improvement, while sacrificing only approximately 1% of performance on various MLLMs. Compared with SOTA token-wise methods, our technique achieves superior accuracy preservation.

</details>


### [215] [Vision-DeepResearch Benchmark: Rethinking Visual and Textual Search for Multimodal Large Language Models](https://arxiv.org/abs/2602.02185)
*Yu Zeng,Wenxuan Huang,Zhen Fang,Shuang Chen,Yufan Shen,Yishuo Cai,Xiaoman Wang,Zhenfei Yin,Lin Chen,Zehui Chen,Shiting Huang,Yiming Zhao,Yao Hu,Philip Torr,Wanli Ouyang,Shaosheng Cao*

Main category: cs.CV

Relevance: 85.0

TL;DR: 提出了Vision-DeepResearch基准(VDR-Bench)，包含2000个VQA实例，用于评估多模态大语言模型在视觉-文本深度研究任务中的真实搜索能力，并提出了多轮裁剪搜索工作流程来提升视觉检索性能。


<details>
  <summary>Details</summary>
Motivation: 现有评估基准存在两个主要问题：1) 不是以视觉搜索为中心，答案经常通过文本问题中的跨文本线索泄露，或可以从当前MLLMs的先验知识中推断；2) 评估场景过于理想化，图像搜索可以通过近似完全匹配获取信息，文本搜索则过于直接且挑战性不足。

Method: 构建了VDR-Bench基准，包含2000个VQA实例，通过精心设计的多阶段筛选流程和严格的专家评审创建。同时提出了一个简单的多轮裁剪搜索工作流程，通过裁剪图像区域进行多轮搜索来提升视觉检索能力。

Result: 提出的多轮裁剪搜索策略在真实视觉检索场景中有效提升了模型性能，为未来多模态深度研究系统的设计提供了实用指导。

Conclusion: VDR-Bench基准解决了现有评估方法的局限性，提供了更真实的评估场景，提出的多轮裁剪搜索工作流程改善了MLLMs的视觉检索能力，为构建更强大的视觉深度研究系统奠定了基础。

Abstract: Multimodal Large Language Models (MLLMs) have advanced VQA and now support Vision-DeepResearch systems that use search engines for complex visual-textual fact-finding. However, evaluating these visual and textual search abilities is still difficult, and existing benchmarks have two major limitations. First, existing benchmarks are not visual search-centric: answers that should require visual search are often leaked through cross-textual cues in the text questions or can be inferred from the prior world knowledge in current MLLMs. Second, overly idealized evaluation scenario: On the image-search side, the required information can often be obtained via near-exact matching against the full image, while the text-search side is overly direct and insufficiently challenging. To address these issues, we construct the Vision-DeepResearch benchmark (VDR-Bench) comprising 2,000 VQA instances. All questions are created via a careful, multi-stage curation pipeline and rigorous expert review, designed to assess the behavior of Vision-DeepResearch systems under realistic real-world conditions. Moreover, to address the insufficient visual retrieval capabilities of current MLLMs, we propose a simple multi-round cropped-search workflow. This strategy is shown to effectively improve model performance in realistic visual retrieval scenarios. Overall, our results provide practical guidance for the design of future multimodal deep-research systems. The code will be released in https://github.com/Osilly/Vision-DeepResearch.

</details>


### [216] [ClueTracer: Question-to-Vision Clue Tracing for Training-Free Hallucination Suppression in Multimodal Reasoning](https://arxiv.org/abs/2602.02004)
*Gongli Xi,Kun Wang,Zeming Gao,Huahui Yi,Haolang Lu,Ye Tian,Wendong Wang*

Main category: cs.CV

Relevance: 85.0

TL;DR: 论文提出ClueTracer，一种无需训练、参数无关、架构无关的插件，用于抑制多模态推理模型中的幻觉问题。通过追踪关键线索在推理路径中的传播，定位任务相关图像区域，抑制对无关区域的关注。


<details>
  <summary>Details</summary>
Motivation: 大型多模态推理模型通过显式长链推理解决视觉问题，但这种能力也增加了幻觉问题。研究发现存在"推理漂移"现象：模型在收集视觉线索时过度关注问题无关的实体，稀释了对任务相关线索的关注，导致推理轨迹逐渐脱离视觉基础。

Method: 提出ClueRecall指标评估视觉线索检索能力，并开发ClueTracer插件。ClueTracer从问题出发，追踪关键线索在模型推理路径中的传播（问题→输出→视觉标记），从而定位任务相关图像区域，抑制对无关区域的虚假关注。

Result: ClueTracer无需额外训练，在所有推理架构上平均提升1.21倍推理基准性能，在非推理设置中也能获得1.14倍增益。

Conclusion: ClueTracer是一种有效的幻觉抑制方法，通过追踪推理路径中的线索传播来增强模型的视觉基础，提高多模态推理的准确性和可靠性。

Abstract: Large multimodal reasoning models solve challenging visual problems via explicit long-chain inference: they gather visual clues from images and decode clues into textual tokens. Yet this capability also increases hallucinations, where the model generates content that is not supported by the input image or the question. To understand this failure mode, we identify \emph{reasoning drift}: during clue gathering, the model over-focuses on question-irrelevant entities, diluting focus on task-relevant cues and gradually decoupling the reasoning trace from visual grounding. As a consequence, many inference-time localization or intervention methods developed for non-reasoning models fail to pinpoint the true clues in reasoning settings. Motivated by these insights, we introduce ClueRecall, a metric for assessing visual clue retrieval, and present ClueTracer, a training-free, parameter-free, and architecture-agnostic plugin for hallucination suppression. ClueTracer starts from the question and traces how key clues propagate along the model's reasoning pathway (question $\rightarrow$ outputs $\rightarrow$ visual tokens), thereby localizing task-relevant patches while suppressing spurious attention to irrelevant regions. Remarkably, \textbf{without any additional training}, ClueTracer improves all \textbf{reasoning} architectures (including \texttt{R1-OneVision}, \texttt{Ocean-R1}, \texttt{MM-Eureka}, \emph{etc}.) by $\mathbf{1.21\times}$ on reasoning benchmarks. When transferred to \textbf{non-reasoning} settings, it yields a $\mathbf{1.14\times}$ gain.

</details>


### [217] [LoopViT: Scaling Visual ARC with Looped Transformers](https://arxiv.org/abs/2602.02156)
*Wen-Jie Shu,Xuerui Qiu,Rui-Jie Zhu,Harold Haodong Chen,Yexin Liu,Harry Yang*

Main category: cs.CV

Relevance: 85.0

TL;DR: Loop-ViT：通过权重共享的递归架构和基于预测熵的动态退出机制，实现计算深度与模型容量的解耦，在视觉推理任务上以更小参数量超越大模型性能。


<details>
  <summary>Details</summary>
Motivation: 当前视觉推理方法主要使用前馈架构，其计算深度严格受限于参数规模，无法捕捉人类归纳推理的迭代算法特性。需要一种能够解耦推理深度与模型容量的架构。

Method: 提出Loop-ViT递归架构：1）使用权重共享的混合块（结合局部卷积和全局注意力）进行迭代推理；2）引入基于预测熵的动态退出机制，当内部状态"结晶"为低不确定性吸引子时停止推理。

Result: 在ARC-AGI-1基准测试中，仅1800万参数的模型达到65.8%准确率，超越了7300万参数的大型集成模型，证明了自适应迭代计算比单纯增加网络宽度更高效。

Conclusion: 自适应迭代计算为视觉推理提供了一个比单纯增加网络宽度更高效的扩展轴，权重共享的递归架构能够更好地模拟人类归纳推理的迭代特性。

Abstract: Recent advances in visual reasoning have leveraged vision transformers to tackle the ARC-AGI benchmark. However, we argue that the feed-forward architecture, where computational depth is strictly bound to parameter size, falls short of capturing the iterative, algorithmic nature of human induction. In this work, we propose a recursive architecture called Loop-ViT, which decouples reasoning depth from model capacity through weight-tied recurrence. Loop-ViT iterates a weight-tied Hybrid Block, combining local convolutions and global attention, to form a latent chain of thought. Crucially, we introduce a parameter-free Dynamic Exit mechanism based on predictive entropy: the model halts inference when its internal state ``crystallizes" into a low-uncertainty attractor. Empirical results on the ARC-AGI-1 benchmark validate this perspective: our 18M model achieves 65.8% accuracy, outperforming massive 73M-parameter ensembles. These findings demonstrate that adaptive iterative computation offers a far more efficient scaling axis for visual reasoning than simply increasing network width. The code is available at https://github.com/WenjieShu/LoopViT.

</details>


### [218] [Unified Personalized Reward Model for Vision Generation](https://arxiv.org/abs/2602.02380)
*Yibin Wang,Yuhang Zang,Feng Han,Jiazi Bu,Yujie Zhou,Cheng Jin,Jiaqi Wang*

Main category: cs.CV

Relevance: 85.0

TL;DR: 提出UnifiedReward-Flex，一个统一的个性化视觉生成奖励模型，通过上下文自适应推理解决现有奖励模型对内容特定视觉线索不敏感的问题。


<details>
  <summary>Details</summary>
Motivation: 现有多模态奖励模型通常采用一刀切的方法，假设单一偏好分布或依赖固定评估标准，导致对内容特定视觉线索不敏感，与主观和上下文相关的人类偏好存在系统性偏差。

Method: 1) 首先解释语义意图并基于视觉证据进行推理；2) 动态构建分层评估，在预定义和自生成的高层维度下实例化细粒度标准；3) 两阶段训练：从先进闭源VLM蒸馏结构化高质量推理轨迹进行SFT，然后对精心策划的偏好对进行DPO。

Result: 将UnifiedReward-Flex集成到GRPO框架中进行图像和视频合成，广泛结果表明其优越性。

Conclusion: UnifiedReward-Flex通过结合奖励建模与灵活上下文自适应推理，有效解决了现有奖励模型的局限性，实现了更好的个性化视觉生成对齐。

Abstract: Recent advancements in multimodal reward models (RMs) have significantly propelled the development of visual generation. Existing frameworks typically adopt Bradley-Terry-style preference modeling or leverage generative VLMs as judges, and subsequently optimize visual generation models via reinforcement learning. However, current RMs suffer from inherent limitations: they often follow a one-size-fits-all paradigm that assumes a monolithic preference distribution or relies on fixed evaluation rubrics. As a result, they are insensitive to content-specific visual cues, leading to systematic misalignment with subjective and context-dependent human preferences. To this end, inspired by human assessment, we propose UnifiedReward-Flex, a unified personalized reward model for vision generation that couples reward modeling with flexible and context-adaptive reasoning. Specifically, given a prompt and the generated visual content, it first interprets the semantic intent and grounds on visual evidence, then dynamically constructs a hierarchical assessment by instantiating fine-grained criteria under both predefined and self-generated high-level dimensions. Our training pipeline follows a two-stage process: (1) we first distill structured, high-quality reasoning traces from advanced closed-source VLMs to bootstrap SFT, equipping the model with flexible and context-adaptive reasoning behaviors; (2) we then perform direct preference optimization (DPO) on carefully curated preference pairs to further strengthen reasoning fidelity and discriminative alignment. To validate the effectiveness, we integrate UnifiedReward-Flex into the GRPO framework for image and video synthesis, and extensive results demonstrate its superiority.

</details>


### [219] [ReasonEdit: Editing Vision-Language Models using Human Reasoning](https://arxiv.org/abs/2602.02408)
*Jiaxing Qiu,Kaihua Hou,Roxana Daneshjou,Ahmed Alaa,Thomas Hartvigsen*

Main category: cs.CV

Relevance: 85.0

TL;DR: ReasonEdit是首个支持用户解释推理过程的视觉语言模型编辑器，通过存储人类推理代码本和基于网络科学的拓扑平衡多模态嵌入检索，在推理密集型任务上实现最先进的编辑性能。


<details>
  <summary>Details</summary>
Motivation: 现有视觉语言模型编辑器无法处理需要复杂推理的任务，这些任务通常需要人类和模型对图像进行推理。因此需要一种能够利用人类推理进行编辑的新方法。

Method: 提出ReasonEdit框架：1) 持续存储人类推理到代码本中；2) 在推理时通过新颖的拓扑平衡多模态嵌入方法检索相关事实，该方法受网络科学启发；3) 支持用户在编辑过程中解释推理。

Result: 在四个视觉语言模型和多个基于推理的视觉问答数据集上，ReasonEdit实现了最先进的编辑性能，表明在编辑过程中使用人类推理能显著提升编辑泛化能力。

Conclusion: ReasonEdit是首个支持推理解释的视觉语言模型编辑器，通过结合人类推理和网络科学启发的嵌入方法，在推理密集型任务上取得了突破性进展。

Abstract: Model editing aims to correct errors in large, pretrained models without altering unrelated behaviors. While some recent works have edited vision-language models (VLMs), no existing editors tackle reasoning-heavy tasks, which typically require humans and models to reason about images.We therefore propose ReasonEdit, the first VLM editor to let users explain their reasoning during editing, introducing a new, practical model editing setup. ReasonEdit continuously stores human reasoning in a codebook, and retrieves only relevant facts during inference using a novel topology-balanced multimodal embedding method inspired by network science. Across four VLMs on multiple rationale-based visual question answering datasets, ReasonEdit achieves state-of-the-art editing performance, ultimately showing that using human reasoning during editing greatly improves edit generalization.

</details>


### [220] [Can Vision-Language Models Handle Long-Context Code? An Empirical Study on Visual Compression](https://arxiv.org/abs/2602.00746)
*Jianping Zhong,Guochang Li,Chen Zhi,Junxiao Han,Zhen Qin,Xinkui Zhao,Nan Wang,Shuiguang Deng,Jianwei Yin*

Main category: cs.SE

Relevance: 85.0

TL;DR: LongCodeOCR：视觉代码压缩框架，将代码渲染为二维图像序列供视觉语言模型处理，解决长代码上下文窗口限制问题，避免依赖关系断裂。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型在处理长代码上下文时受窗口限制，现有文本代码压缩方法通过选择性过滤会破坏依赖闭包，导致语义碎片化。需要一种能保持全局视图的压缩方法。

Method: 提出LongCodeOCR视觉压缩框架，将代码渲染为压缩的二维图像序列，供视觉语言模型处理。这种方法保持全局视图，避免过滤导致的依赖关系断裂。

Result: 在四个基准测试中，LongCodeOCR在相同压缩比下将CompScore提高了36.85分；在100万token上下文下保持更高准确率且压缩率提高4倍；压缩阶段延迟从4.3小时降至1分钟。

Conclusion: 视觉代码压缩是全局理解任务的有效替代方案，存在覆盖度-保真度权衡：视觉压缩保持更广上下文覆盖但面临精确性瓶颈，文本压缩保持符号级精度但牺牲结构覆盖。

Abstract: Large Language Models (LLMs) struggle with long-context code due to window limitations. Existing textual code compression methods mitigate this via selective filtering but often disrupt dependency closure, causing semantic fragmentation. To address this, we introduce LongCodeOCR, a visual compression framework that renders code into compressed two-dimensional image sequences for Vision-Language Models (VLMs). By preserving a global view, this approach avoids the dependency breakage inherent in filtering. We systematically evaluate LongCodeOCR against the state-of-the-art LongCodeZip across four benchmarks spanning code summarization, code question answering, and code completion.
  Our results demonstrate that visual code compression serves as a viable alternative for tasks requiring global understanding. At comparable compression ratios ($\sim$1.7$\times$), LongCodeOCR improves CompScore on Long Module Summarization by 36.85 points over LongCodeZip. At a 1M-token context length with Glyph (a specialized 9B VLM), LongCodeOCR maintains higher accuracy than LongCodeZip while operating at about 4$\times$ higher compression. Moreover, compared with LongCodeZip, LongCodeOCR drastically reduces compression-stage overhead (reducing latency from $\sim$4.3 hours to $\sim$1 minute at 1M tokens). Finally, our results characterize a fundamental coverage--fidelity trade-off: visual code compression retains broader context coverage to support global dependencies, yet faces fidelity bottlenecks on exactness-critical tasks; by contrast, textual code compression preserves symbol-level precision while sacrificing structural coverage.

</details>


### [221] [EDU-CIRCUIT-HW: Evaluating Multimodal Large Language Models on Real-World University-Level STEM Student Handwritten Solutions](https://arxiv.org/abs/2602.00095)
*Weiyu Sun,Liangliang Chen,Yongnuo Cai,Huiru Xie,Yi Zeng,Ying Zhang*

Main category: cs.CV

Relevance: 75.0

TL;DR: EDU-CIRCUIT-HW数据集评估多模态大语言模型在STEM学生手写解答识别中的表现，发现模型存在大量潜在识别错误，可靠性不足，但通过错误模式检测和少量人工干预可显著提升系统鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 多模态大语言模型在教育领域有巨大潜力，但缺乏针对STEM学生手写解答（包含数学公式、图表和文本推理）的真实领域基准。现有评估主要关注下游任务（如自动评分），无法全面评估模型对复杂手写逻辑的理解。

Method: 发布EDU-CIRCUIT-HW数据集，包含1300+大学STEM课程真实学生手写解答。利用专家验证的逐字转录和评分报告，同时评估多种MLLM的上游识别保真度和下游自动评分性能。通过错误模式分析提出预检测和纠正方法。

Result: 评估揭示了MLLM在学生手写内容识别中存在惊人的潜在失败规模，表明模型在高风险教育场景中的自动评分和其他理解导向应用可靠性不足。案例研究表明，利用识别的错误模式进行预检测和纠正，仅需约4%的人工干预即可显著提升系统在未见学生解答上的鲁棒性。

Conclusion: 当前MLLM在复杂STEM手写解答识别方面可靠性不足，需要改进。通过错误模式分析和少量人工干预可有效提升系统鲁棒性，为教育领域AI应用提供了实用解决方案。

Abstract: Multimodal Large Language Models (MLLMs) hold significant promise for revolutionizing traditional education and reducing teachers' workload. However, accurately interpreting unconstrained STEM student handwritten solutions with intertwined mathematical formulas, diagrams, and textual reasoning poses a significant challenge due to the lack of authentic and domain-specific benchmarks. Additionally, current evaluation paradigms predominantly rely on the outcomes of downstream tasks (e.g., auto-grading), which often probe only a subset of the recognized content, thereby failing to capture the MLLMs' understanding of complex handwritten logic as a whole. To bridge this gap, we release EDU-CIRCUIT-HW, a dataset consisting of 1,300+ authentic student handwritten solutions from a university-level STEM course. Utilizing the expert-verified verbatim transcriptions and grading reports of student solutions, we simultaneously evaluate various MLLMs' upstream recognition fidelity and downstream auto-grading performance. Our evaluation uncovers an astonishing scale of latent failures within MLLM-recognized student handwritten content, highlighting the models' insufficient reliability for auto-grading and other understanding-oriented applications in high-stakes educational settings. In solution, we present a case study demonstrating that leveraging identified error patterns to preemptively detect and rectify recognition errors, with only minimal human intervention (approximately 4% of the total solutions), can significantly enhance the robustness of the deployed AI-enabled grading system on unseen student solutions.

</details>


### [222] [LLaVA-FA: Learning Fourier Approximation for Compressing Large Multimodal Models](https://arxiv.org/abs/2602.00135)
*Pengcheng Zheng,Chaoning Zhang,Jiarong Mo,GuoHui Li,Jiaquan Zhang,Jiahao Zhang,Sihan Cao,Sheng Zheng,Caiyan Qin,Guoqing Wang,Yang Yang*

Main category: cs.CV

Relevance: 75.0

TL;DR: LLaVA-FA：一种新颖的高效大型多模态模型，通过在频域进行联合低秩加量化近似来压缩LMMs，利用傅里叶变换的去相关和共轭对称特性实现更紧凑准确的权重表示。


<details>
  <summary>Details</summary>
Motivation: 大型多模态模型（LMMs）在视觉语言任务上表现出色，但其巨大的计算和内存成本阻碍了实际部署。现有压缩方法通常将低秩分解和量化解耦，导致复合重建误差，特别是在具有跨模态冗余的多模态架构中。

Method: 提出LLaVA-FA，在频域进行联合低秩加量化近似；引入PolarQuant，一种针对复数矩阵的极坐标量化方法；采用可选对角校准（ODC）方案，无需大规模校准数据。

Result: LLaVA-FA在多个基准测试中优于现有高效多模态模型，同时保持最少的激活参数和低计算成本，验证了其作为LMMs压缩解决方案的有效性。

Conclusion: LLaVA-FA通过频域联合压缩方法有效解决了LMMs的计算和内存瓶颈问题，为实际部署提供了强大的解决方案。

Abstract: Large multimodal models (LMMs) have achieved impressive performance on various vision-language tasks, but their substantial computational and memory costs hinder their practical deployment. Existing compression methods often decouple low-rank decomposition and quantization, leading to compounded reconstruction errors, especially in multimodal architectures with cross-modal redundancy. To address this issue, we propose LLaVA-FA, a novel efficient LMM that performs joint low-rank plus quantization approximation in the frequency domain. By leveraging the de-correlation and conjugate symmetry properties of Fourier transform, LLaVA-FA achieves more compact and accurate weight representations. Furthermore, we introduce PolarQuant, a polar-coordinate quantization method tailored for complex matrices, and an optional diagonal calibration (ODC) scheme that eliminates the need for large-scale calibration data. Extensive experimental results demonstrate that our proposed LLaVA-FA outperforms existing efficient multimodal models across multiple benchmarks while maintaining minimal activated parameters and low computational costs, validating its effectiveness as a powerful solution for compressing LMMs.

</details>


### [223] [TokenTrim: Inference-Time Token Pruning for Autoregressive Long Video Generation](https://arxiv.org/abs/2602.00268)
*Ariel Shaulov,Eitan Shaar,Amit Edenzon,Lior Wolf*

Main category: cs.CV

Relevance: 75.0

TL;DR: 提出一种推理时方法，通过识别和移除不稳定的潜在token来缓解自回归视频生成中的时间漂移问题，无需修改模型架构或训练过程。


<details>
  <summary>Details</summary>
Motivation: 自回归视频生成在生成长视频时存在严重的时间漂移问题，即误差会随时间累积和放大。作者认为这主要不是模型容量不足导致的，而是推理时的误差传播问题，特别是由于在自回归推理中重复使用了已损坏的潜在条件token。

Method: 提出简单的推理时方法：在自回归推理过程中，识别并移除不稳定的潜在token，防止它们被重复用于条件生成。不稳定token定义为表示与先前生成批次显著偏离的潜在token，表明可能存在损坏或语义漂移。通过从自回归上下文中显式移除损坏的潜在token，而不是修改整个空间区域或模型参数，防止不可靠的潜在信息影响未来的生成步骤。

Result: 该方法显著改善了长时域的时间一致性，无需修改模型架构、训练过程或离开潜在空间。

Conclusion: 自回归视频生成中的时间漂移主要源于推理时的误差传播，通过推理时识别和移除不稳定潜在token的方法可以有效缓解这一问题，提高长视频生成的时间一致性。

Abstract: Auto-regressive video generation enables long video synthesis by iteratively conditioning each new batch of frames on previously generated content. However, recent work has shown that such pipelines suffer from severe temporal drift, where errors accumulate and amplify over long horizons. We hypothesize that this drift does not primarily stem from insufficient model capacity, but rather from inference-time error propagation. Specifically, we contend that drift arises from the uncontrolled reuse of corrupted latent conditioning tokens during auto-regressive inference. To correct this accumulation of errors, we propose a simple, inference-time method that mitigates temporal drift by identifying and removing unstable latent tokens before they are reused for conditioning. For this purpose, we define unstable tokens as latent tokens whose representations deviate significantly from those of the previously generated batch, indicating potential corruption or semantic drift. By explicitly removing corrupted latent tokens from the auto-regressive context, rather than modifying entire spatial regions or model parameters, our method prevents unreliable latent information from influencing future generation steps. As a result, it significantly improves long-horizon temporal consistency without modifying the model architecture, training procedure, or leaving latent space.

</details>


### [224] [TimeBlind: A Spatio-Temporal Compositionality Benchmark for Video LLMs](https://arxiv.org/abs/2602.00288)
*Baiqi Li,Kangyi Zhao,Ce Zhang,Chancharik Mitra,Jean de Dieu Nyandwi,Gedas Bertasius*

Main category: cs.CV

Relevance: 75.0

TL;DR: TimeBlind是一个诊断性基准测试，专门评估多模态大语言模型在细粒度时空理解上的能力，通过最小对比对范式揭示模型依赖静态视觉捷径而非真正时间逻辑的问题。


<details>
  <summary>Details</summary>
Motivation: 当前多模态大语言模型在静态语义理解上表现出色，但对时间动态的理解仍然脆弱。视频推理和具身AI需要细粒度的时空理解能力，现有基准测试往往将识别与时间推理混为一谈，无法准确诊断模型的时间理解缺陷。

Method: TimeBlind采用认知科学启发的方法，将细粒度时间理解分为三个层次：原子事件识别、事件属性表征、事件相互依赖推理。使用最小对比对范式：视频对共享相同的静态视觉内容，仅时间结构不同，通过互补问题设计来消除语言先验。包含600个精心策划的实例（2400个视频-问题对）。

Result: 评估了20多个最先进的MLLM（包括GPT-5、Gemini 3 Pro等），最佳模型的实例准确率仅为48.2%，远低于人类表现（98.2%）。这表明前沿模型严重依赖静态视觉捷径而非真正的时间逻辑。

Conclusion: TimeBlind揭示了当前MLLM在时间理解上的根本缺陷，为下一代视频理解提供了重要的诊断工具。模型需要更深入的时间推理能力，而不仅仅是静态视觉模式识别。

Abstract: Fine-grained spatio-temporal understanding is essential for video reasoning and embodied AI. Yet, while Multimodal Large Language Models (MLLMs) master static semantics, their grasp of temporal dynamics remains brittle. We present TimeBlind, a diagnostic benchmark for compositional spatio-temporal understanding. Inspired by cognitive science, TimeBlind categorizes fine-grained temporal understanding into three levels: recognizing atomic events, characterizing event properties, and reasoning about event interdependencies. Unlike benchmarks that conflate recognition with temporal reasoning, TimeBlind leverages a minimal-pairs paradigm: video pairs share identical static visual content but differ solely in temporal structure, utilizing complementary questions to neutralize language priors. Evaluating over 20 state-of-the-art MLLMs (e.g., GPT-5, Gemini 3 Pro) on 600 curated instances (2400 video-question pairs), reveals that the Instance Accuracy (correctly distinguishing both videos in a pair) of the best performing MLLM is only 48.2%, far below the human performance (98.2%). These results demonstrate that even frontier models rely heavily on static visual shortcuts rather than genuine temporal logic, positioning TimeBlind as a vital diagnostic tool for next-generation video understanding. Dataset and code are available at https://baiqi-li.github.io/timeblind_project/ .

</details>


### [225] [LogicGaze: Benchmarking Causal Consistency in Visual Narratives via Counterfactual Verification](https://arxiv.org/abs/2602.00292)
*Rory Driscoll,Alexandros Christoforos,Chadbourne Davis*

Main category: cs.CV

Relevance: 75.0

TL;DR: LogicGaze是一个评估视觉语言模型在序列推理中是否基于真实视觉证据的基准框架，通过因果验证、叙事合成和扰动拒绝三个维度测试模型抗幻觉能力。


<details>
  <summary>Details</summary>
Motivation: 虽然序列推理增强了视觉语言模型处理复杂多模态任务的能力，但模型在推理过程中是否真正基于视觉证据而非产生幻觉的问题尚未得到充分探索。现有模型可能在推理链中引入虚假信息，影响多模态推理的可信度。

Method: 从ShareGPT4Video的40,000个视频片段和Flickr30k图像子集中构建数据集，整合因果序列与视觉矛盾但语言合理的扰动。采用三重评估协议：因果验证（验证推理步骤真实性）、叙事合成（基于视觉证据构建连贯叙述）、扰动拒绝（识别并拒绝与视觉证据矛盾的扰动）。

Result: 评估显示最先进的视觉语言模型（如Qwen2.5-VL-72B）在逻辑推理和视觉证据验证方面存在显著漏洞，容易产生幻觉，无法可靠地区分真实视觉证据与语言合理的虚假信息。

Conclusion: LogicGaze揭示了当前视觉语言模型在序列推理中严重依赖语言先验而非视觉证据的问题，强调了构建可信赖多模态推理系统的重要性，并为评估模型抗幻觉能力提供了标准化基准。

Abstract: While sequential reasoning enhances the capability of Vision-Language Models (VLMs) to execute complex multimodal tasks, their reliability in grounding these reasoning chains within actual visual evidence remains insufficiently explored. We introduce LogicGaze, a novel benchmark framework designed to rigorously interrogate whether VLMs can validate sequential causal chains against visual inputs, specifically targeting the pervasive issue of hallucination. Curated from 40,000 video segments from ShareGPT4Video and a subset of Flickr30k imagery, LogicGaze integrates causal sequences with visually contradictory yet linguistically plausible perturbations, compelling models to verify the authenticity of each reasoning step. Our tripartite evaluation protocol - Causal Validation, Grounded Narrative Synthesis, and Perturbation Rejection - exposes significant vulnerabilities in state-of-the-art VLMs such as Qwen2.5-VL-72B. LogicGaze advocates for robust, trustworthy multimodal reasoning, with all resources publicly available in an anonymized repository.

</details>


### [226] [AdaFuse: Adaptive Multimodal Fusion for Lung Cancer Risk Prediction via Reinforcement Learning](https://arxiv.org/abs/2602.00347)
*Chongyu Qu,Zhengyi Lu,Yuxiang Lai,Thomas Z. Li,Junchao Zhu,Junlin Guo,Juming Xiong,Yanfan Zhu,Yuechen Yang,Allen J. Luna,Kim L. Sandler,Bennett A. Landman,Yuankai Huo*

Main category: cs.CV

Relevance: 75.0

TL;DR: AdaFuse：基于强化学习的自适应多模态融合框架，用于肺癌风险预测，能够根据患者具体情况动态选择使用哪些模态数据，而不是固定使用所有可用模态。


<details>
  <summary>Details</summary>
Motivation: 现有多模态融合方法通常处理所有可用模态数据，要么平等对待，要么学习分配不同权重，但未解决一个根本问题：对于特定患者，是否应该使用某些模态？需要更智能的、个性化的模态选择策略。

Method: 将多模态融合建模为顺序决策过程，使用强化学习训练策略网络，迭代决定是否纳入额外模态或基于已获取信息进行预测。这种顺序制定使模型能够根据先前观察到的模态条件化每个选择，并在获得足够信息时提前终止。

Result: 在NLST数据集上，AdaFuse达到最高AUC（0.762），优于最佳单模态基线（0.732）、最佳固定融合策略（0.759）以及自适应基线DynMM（0.754）和MoE（0.742），同时比所有三模态方法使用更少的FLOPs。

Conclusion: 这项工作展示了强化学习在医学影像个性化多模态融合中的潜力，代表了从统一融合策略向自适应诊断流程的转变，学习何时咨询额外模态以及何时现有信息足以进行准确预测。

Abstract: Multimodal fusion has emerged as a promising paradigm for disease diagnosis and prognosis, integrating complementary information from heterogeneous data sources such as medical images, clinical records, and radiology reports. However, existing fusion methods process all available modalities through the network, either treating them equally or learning to assign different contribution weights, leaving a fundamental question unaddressed: for a given patient, should certain modalities be used at all? We present AdaFuse, an adaptive multimodal fusion framework that leverages reinforcement learning (RL) to learn patient-specific modality selection and fusion strategies for lung cancer risk prediction. AdaFuse formulates multimodal fusion as a sequential decision process, where the policy network iteratively decides whether to incorporate an additional modality or proceed to prediction based on the information already acquired. This sequential formulation enables the model to condition each selection on previously observed modalities and terminate early when sufficient information is available, rather than committing to a fixed subset upfront. We evaluate AdaFuse on the National Lung Screening Trial (NLST) dataset. Experimental results demonstrate that AdaFuse achieves the highest AUC (0.762) compared to the best single-modality baseline (0.732), the best fixed fusion strategy (0.759), and adaptive baselines including DynMM (0.754) and MoE (0.742), while using fewer FLOPs than all triple-modality methods. Our work demonstrates the potential of reinforcement learning for personalized multimodal fusion in medical imaging, representing a shift from uniform fusion strategies toward adaptive diagnostic pipelines that learn when to consult additional modalities and when existing information suffices for accurate prediction.

</details>


### [227] [Brazilian Portuguese Image Captioning with Transformers: A Study on Cross-Native-Translated Dataset](https://arxiv.org/abs/2602.00393)
*Gabriel Bromonschenkel,Alessandro L. Koerich,Thiago M. Paixão,Hilário Tomaz Alves de Oliveira*

Main category: cs.CV

Relevance: 75.0

TL;DR: 该研究对基于Transformer的视觉语言模型在巴西葡萄牙语图像描述任务上进行跨原生-翻译评估，比较了原生标注和自动翻译数据集，发现Swin-DistilBERTimbau表现最佳，巴西预训练模型ViTucano在文本指标上优于大型多语言模型，而GPT-4在图像-文本对齐方面表现最好。


<details>
  <summary>Details</summary>
Motivation: 图像描述任务主要关注英语模型，低资源语言如巴西葡萄牙语面临数据集和模型缺乏的挑战。现有研究通过自动翻译缓解资源稀缺问题，但缺乏对原生标注和翻译数据集的系统比较评估。

Method: 提出跨原生-翻译评估框架，使用巴西葡萄牙语原生标注的Flickr30K版本和自动翻译版本。采用交叉上下文方法：在一个数据集上训练的模型在另一个数据集上测试。结合注意力图进行模型解释，使用CLIP-Score评估图像-描述对齐。

Result: Swin-DistilBERTimbau在所有模型中表现最稳定，展现出强大的跨数据集泛化能力。巴西预训练模型ViTucano在传统文本评估指标上超越GPT-4o和LLaMa 3.2 Vision等大型多语言模型。GPT-4模型获得最高CLIP-Score，表明更好的图像-文本对齐。注意力分析揭示了系统性偏见，包括性别误分类、对象枚举错误和空间不一致性。

Conclusion: 该研究为巴西葡萄牙语图像描述提供了系统评估框架，揭示了翻译数据集对模型性能的影响，强调了原生标注数据的重要性。巴西预训练模型在文本指标上的优异表现表明针对特定语言的预训练具有优势，而大型多语言模型在图像-文本对齐方面表现更好。

Abstract: Image captioning (IC) refers to the automatic generation of natural language descriptions for images, with applications ranging from social media content generation to assisting individuals with visual impairments. While most research has been focused on English-based models, low-resource languages such as Brazilian Portuguese face significant challenges due to the lack of specialized datasets and models. Several studies create datasets by automatically translating existing ones to mitigate resource scarcity. This work addresses this gap by proposing a cross-native-translated evaluation of Transformer-based vision and language models for Brazilian Portuguese IC. We use a version of Flickr30K comprised of captions manually created by native Brazilian Portuguese speakers and compare it to a version with captions automatically translated from English to Portuguese. The experiments include a cross-context approach, where models trained on one dataset are tested on the other to assess the translation impact. Additionally, we incorporate attention maps for model inference interpretation and use the CLIP-Score metric to evaluate the image-description alignment. Our findings show that Swin-DistilBERTimbau consistently outperforms other models, demonstrating strong generalization across datasets. ViTucano, a Brazilian Portuguese pre-trained VLM, surpasses larger multilingual models (GPT-4o, LLaMa 3.2 Vision) in traditional text-based evaluation metrics, while GPT-4 models achieve the highest CLIP-Score, highlighting improved image-text alignment. Attention analysis reveals systematic biases, including gender misclassification, object enumeration errors, and spatial inconsistencies. The datasets and the models generated and analyzed during the current study are available in: https://github.com/laicsiifes/transformer-caption-ptbr.

</details>


### [228] [Toward Autonomous Laboratory Safety Monitoring with Vision Language Models: Learning to See Hazards Through Scene Structure](https://arxiv.org/abs/2602.00414)
*Trishna Chakraborty,Udita Ghosh,Aldair Ernesto Gongora,Ruben Glatt,Yue Dong,Jiachen Li,Amit K. Roy-Chowdhury,Chengyu Song*

Main category: cs.CV

Relevance: 75.0

TL;DR: 该论文提出了一种实验室安全监控的视觉语言模型评估框架，通过结构化数据生成管道创建(图像、场景图、真实标签)三元组数据集，发现VLMs在文本场景图输入下表现良好但在纯视觉输入下性能下降，并提出场景图引导对齐方法提升视觉危险检测性能。


<details>
  <summary>Details</summary>
Motivation: 实验室安全监控通常依赖人工，缺乏持续监控。虽然视觉语言模型(VLMs)有望实现自主安全监控，但由于缺乏视觉评估数据(大多数安全事故仅以非结构化文本记录)，其在真实场景中的有效性尚不明确。

Method: 1) 提出结构化数据生成管道：使用大语言模型作为场景图架构师，图像生成模型作为渲染器，将文本实验室场景转换为对齐的(图像、场景图、真实标签)三元组；2) 在1,207个样本(362个独特场景)上评估7个开源和闭源模型；3) 提出后训练上下文工程方法：场景图引导对齐，通过将视觉输入转换为结构化场景图来弥合VLMs的感知差距。

Result: 实验表明：VLMs在给定文本场景图输入时表现有效，但在纯视觉设置下性能显著下降，表明直接从像素提取结构化对象关系存在困难。提出的场景图引导对齐方法在纯视觉设置下提高了危险检测性能。

Conclusion: 该研究填补了实验室安全监控视觉评估数据的空白，揭示了VLMs在视觉理解结构化关系方面的局限性，并提出了一种有效的上下文工程方法来提升视觉危险检测能力，为实验室安全监控的自动化提供了技术基础。

Abstract: Laboratories are prone to severe injuries from minor unsafe actions, yet continuous safety monitoring -- beyond mandatory pre-lab safety training -- is limited by human availability. Vision language models (VLMs) offer promise for autonomous laboratory safety monitoring, but their effectiveness in realistic settings is unclear due to the lack of visual evaluation data, as most safety incidents are documented primarily as unstructured text. To address this gap, we first introduce a structured data generation pipeline that converts textual laboratory scenarios into aligned triples of (image, scene graph, ground truth), using large language models as scene graph architects and image generation models as renderers. Our experiments on the synthetic dataset of 1,207 samples across 362 unique scenarios and seven open- and closed-source models show that VLMs perform effectively given textual scene graph, but degrade substantially in visual-only settings indicating difficulty in extracting structured object relationships directly from pixels. To overcome this, we propose a post-training context-engineering approach, scene-graph-guided alignment, to bridge perceptual gaps in VLMs by translating visual inputs into structured scene graphs better aligned with VLM reasoning, improving hazard detection performance in visual only settings.

</details>


### [229] [DISK: Dynamic Inference SKipping for World Models](https://arxiv.org/abs/2602.00440)
*Anugunj Naman,Gaibo Zhang,Ayushman Singh,Yaguang Zhang*

Main category: cs.CV

Relevance: 75.0

TL;DR: DISK是一种无需训练的自适应推理方法，用于自回归世界模型，通过协调视频和自我轨迹的两个耦合扩散变换器，在保持运动-外观一致性的同时实现2倍加速。


<details>
  <summary>Details</summary>
Motivation: 现有世界模型在长时域视频和轨迹预测中计算成本高昂，需要高效的推理方法而不牺牲预测质量。

Method: 使用双分支控制器协调视频和轨迹扩散变换器，采用跨模态跳过决策，扩展高阶潜在差异跳过测试到自回归链式前向机制，并通过rollout循环传播控制器统计信息。

Result: 在1500个NuPlan和NuScenes样本上实现轨迹扩散2倍加速和视频扩散1.6倍加速，同时保持L2规划误差、视觉质量(FID/FVD)和NAVSIM PDMS分数。

Conclusion: DISK展示了在显著降低计算成本的情况下实现实用的长时域视频和轨迹预测，为高效世界模型推理提供了有效解决方案。

Abstract: We present DISK, a training-free adaptive inference method for autoregressive world models. DISK coordinates two coupled diffusion transformers for video and ego-trajectory via dual-branch controllers with cross-modal skip decisions, preserving motion-appearance consistency without retraining. We extend higher-order latent-difference skip testing to the autoregressive chain-of-forward regime and propagate controller statistics through rollout loops for long-horizon stability. When integrated into closed-loop driving rollouts on 1500 NuPlan and NuScenes samples using an NVIDIA L40S GPU, DISK achieves 2x speedup on trajectory diffusion and 1.6x speedup on video diffusion while maintaining L2 planning error, visual quality (FID/FVD), and NAVSIM PDMS scores, demonstrating practical long-horizon video-and-trajectory prediction at substantially reduced cost.

</details>


### [230] [RGBX-R1: Visual Modality Chain-of-Thought Guided Reinforcement Learning for Multimodal Grounding](https://arxiv.org/abs/2602.00504)
*Jiahe Wu,Bing Cao,Qilong Wang,Qinghua Hu,Dongdong Li,Pengfei Zhu*

Main category: cs.CV

Relevance: 75.0

TL;DR: RGBX-R1：一个增强多模态大语言模型在红外、深度、事件等X视觉模态感知能力的框架，通过VM-CoT思维链和两阶段训练，在RGBX-Grounding基准上超越基线22.71%


<details>
  <summary>Details</summary>
Motivation: 当前多模态大语言模型主要预训练在RGB模态上，限制了在红外、深度、事件等其他视觉模态上的性能，而这些模态对于复杂场景至关重要。需要扩展MLLM的感知能力到多种视觉模态。

Method: 1. 提出Understand-Associate-Validate (UAV)提示策略构建Visual Modality Chain-of-Thought (VM-CoT)，将RGB理解能力扩展到X模态；2. 两阶段训练：Cold-Start Supervised Fine-Tuning (CS-SFT)使用VM-CoT监督推理过程，建立基础模态认知；3. Spatio-Temporal Reinforcement Fine-Tuning (ST-RFT)基于GRPO，使用Modality-understanding Spatio-Temporal (MuST)奖励强化模态推理

Result: 构建了首个RGBX-Grounding基准，在三个RGBX grounding任务上超越基线22.71%，在多模态理解和空间感知方面表现出优越性

Conclusion: RGBX-R1框架成功扩展了MLLM在多种视觉模态上的感知和推理能力，为处理复杂多模态场景提供了有效解决方案

Abstract: Multimodal Large Language Models (MLLM) are primarily pre-trained on the RGB modality, thereby limiting their performance on other modalities, such as infrared, depth, and event data, which are crucial for complex scenarios. To address this, we propose RGBX-R1, a framework to enhance MLLM's perception and reasoning capacities across various X visual modalities. Specifically, we employ an Understand-Associate-Validate (UAV) prompting strategy to construct the Visual Modality Chain-of-Thought (VM-CoT), which aims to expand the MLLMs' RGB understanding capability into X modalities. To progressively enhance reasoning capabilities, we introduce a two-stage training paradigm: Cold-Start Supervised Fine-Tuning (CS-SFT) and Spatio-Temporal Reinforcement Fine-Tuning (ST-RFT). CS-SFT supervises the reasoning process with the guidance of VM-CoT, equipping the MLLM with fundamental modality cognition. Building upon GRPO, ST-RFT employs a Modality-understanding Spatio-Temporal (MuST) reward to reinforce modality reasoning. Notably, we construct the first RGBX-Grounding benchmark, and extensive experiments verify our superiority in multimodal understanding and spatial perception, outperforming baselines by 22.71% on three RGBX grounding tasks.

</details>


### [231] [From Pixels to Facts (Pix2Fact): Benchmarking Multi-Hop Reasoning for Fine-Grained Visual Fact Checking](https://arxiv.org/abs/2602.00593)
*Yifan Jiang,Cong Zhang,Bofei Zhang,Yifan Yang,Bingzhang Wang,Yew-Soon Ong*

Main category: cs.CV

Relevance: 75.0

TL;DR: Pix2Fact是一个新的视觉问答基准，专门评估需要详细视觉定位和知识密集型多跳推理的专家级感知能力，现有最先进VLM仅达到24%准确率，远低于人类的56%。


<details>
  <summary>Details</summary>
Motivation: 现有基准将详细视觉定位和知识密集型推理分开评估，无法捕捉两者协同作用。当前VLM在处理需要专家级感知和深思熟虑知识推理的挑战时表现不佳，需要新基准来推动下一代多模态智能体发展。

Method: 构建包含1000张高分辨率（4K+）图像的Pix2Fact基准，涵盖8个日常生活场景。问题答案由全球顶尖大学博士与专业数据标注公司合作精心设计，每个问题都需要详细视觉定位、多跳推理和外部知识整合。

Result: 评估9个最先进VLM（包括Gemini-3-Pro和GPT-5等专有模型），最先进模型平均准确率仅24.0%，而人类表现达到56%，显示出显著差距。

Conclusion: Pix2Fact揭示了当前模型在复制人类级视觉理解方面的局限性，将作为关键基准推动结合细粒度感知和稳健知识推理的下一代多模态智能体发展。

Abstract: Despite progress on general tasks, VLMs struggle with challenges demanding both detailed visual grounding and deliberate knowledge-based reasoning, a synergy not captured by existing benchmarks that evaluate these skills separately. To close this gap, we introduce Pix2Fact, a new visual question-answering benchmark designed to evaluate expert-level perception and knowledge-intensive multi-hop reasoning. Pix2Fact contains 1,000 high-resolution (4K+) images spanning 8 daily-life scenarios and situations, with questions and answers meticulously crafted by annotators holding PhDs from top global universities working in partnership with a professional data annotation firm. Each question requires detailed visual grounding, multi-hop reasoning, and the integration of external knowledge to answer. Our evaluation of 9 state-of-the-art VLMs, including proprietary models like Gemini-3-Pro and GPT-5, reveals the substantial challenge posed by Pix2Fact: the most advanced model achieves only 24.0% average accuracy, in stark contrast to human performance of 56%. This significant gap underscores the limitations of current models in replicating human-level visual comprehension. We believe Pix2Fact will serve as a critical benchmark to drive the development of next-generation multimodal agents that combine fine-grained perception with robust, knowledge-based reasoning.

</details>


### [232] [A Hybrid Mamba-SAM Architecture for Efficient 3D Medical Image Segmentation](https://arxiv.org/abs/2602.00650)
*Mohammadreza Gholipour Shahraki,Mehdi Rezaeian,Mohammad Ghasemzadeh*

Main category: cs.CV

Relevance: 75.0

TL;DR: Mamba-SAM：一种结合冻结SAM编码器与Mamba状态空间模型的高效混合架构，用于3D医学图像分割，通过参数高效适应策略解决领域转移和计算成本问题。


<details>
  <summary>Details</summary>
Motivation: SAM等基础模型在医学图像分割中面临领域转移、2D设计限制和微调计算成本高的问题，需要高效解决方案来利用基础模型的通用表示能力。

Method: 提出两种参数高效适应策略：1）双分支架构，通过交叉注意力融合冻结SAM编码器的通用特征与可训练VMamba编码器的领域特定表示；2）基于适配器的方法，在冻结SAM ViT编码器中注入轻量级3D感知的Tri-Plane Mamba模块。引入Multi-Frequency Gated Convolution增强特征表示。

Result: 在ACDC心脏MRI数据集上，双分支Mamba-SAM-Base模型达到平均Dice分数0.906，与UNet++（0.907）相当，在心肌（0.910）和左心室（0.971）分割上优于所有基线。适配器变体提供优越推理速度（4.77 FPS）和良好精度（0.880 Dice）。

Conclusion: 将基础模型与高效的SSM架构混合为3D医学图像分割提供了实用有效的解决方案，平衡了精度和效率。

Abstract: Accurate segmentation of 3D medical images such as MRI and CT is essential for clinical diagnosis and treatment planning. Foundation models like the Segment Anything Model (SAM) provide powerful general-purpose representations but struggle in medical imaging due to domain shift, their inherently 2D design, and the high computational cost of fine-tuning. To address these challenges, we propose Mamba-SAM, a novel and efficient hybrid architecture that combines a frozen SAM encoder with the linear-time efficiency and long-range modeling capabilities of Mamba-based State Space Models (SSMs). We investigate two parameter-efficient adaptation strategies. The first is a dual-branch architecture that explicitly fuses general features from a frozen SAM encoder with domain-specific representations learned by a trainable VMamba encoder using cross-attention. The second is an adapter-based approach that injects lightweight, 3D-aware Tri-Plane Mamba (TPMamba) modules into the frozen SAM ViT encoder to implicitly model volumetric context. Within this framework, we introduce Multi-Frequency Gated Convolution (MFGC), which enhances feature representation by jointly analyzing spatial and frequency-domain information via 3D discrete cosine transforms and adaptive gating. Extensive experiments on the ACDC cardiac MRI dataset demonstrate the effectiveness of the proposed methods. The dual-branch Mamba-SAM-Base model achieves a mean Dice score of 0.906, comparable to UNet++ (0.907), while outperforming all baselines on Myocardium (0.910) and Left Ventricle (0.971) segmentation. The adapter-based TP MFGC variant offers superior inference speed (4.77 FPS) with strong accuracy (0.880 Dice). These results show that hybridizing foundation models with efficient SSM-based architectures provides a practical and effective solution for 3D medical image segmentation.

</details>


### [233] [Non-Contrastive Vision-Language Learning with Predictive Embedding Alignment](https://arxiv.org/abs/2602.00653)
*Lukas Kuhn,Giuseppe Serra,Florian Buettner*

Main category: cs.CV

Relevance: 75.0

TL;DR: NOVA是一个非对比视觉语言对齐框架，通过联合嵌入预测和分布正则化实现，无需负采样、动量编码器或梯度停止，简化了训练过程。


<details>
  <summary>Details</summary>
Motivation: 现有对比学习方法（如CLIP）需要大批量、精心设计的负采样和大量超参数调优，训练过程复杂且不稳定。作者希望开发一种更简单、更稳定的非对比视觉语言对齐方法。

Method: NOVA通过预测增强图像视图的文本嵌入来对齐视觉表示到冻结的领域特定文本编码器，同时通过Sketched Isotropic Gaussian Regularization (SIGReg)强制各向同性高斯结构。该方法消除了负采样、动量编码器和梯度停止的需要。

Result: 在胸部X光零样本分类任务中，使用ClinicalBERT作为文本编码器和Vision Transformers在MIMIC-CXR上从头训练，NOVA在三个基准数据集上的零样本分类性能优于多个标准基线，同时表现出更一致的训练运行。

Conclusion: 非对比视觉语言预训练为对比方法提供了更简单、更稳定、更有效的替代方案，特别适用于医学影像等专业领域。

Abstract: Vision-language models have transformed multimodal representation learning, yet dominant contrastive approaches like CLIP require large batch sizes, careful negative sampling, and extensive hyperparameter tuning. We introduce NOVA, a NOn-contrastive Vision-language Alignment framework based on joint embedding prediction with distributional regularization. NOVA aligns visual representations to a frozen, domain-specific text encoder by predicting text embeddings from augmented image views, while enforcing an isotropic Gaussian structure via Sketched Isotropic Gaussian Regularization (SIGReg). This eliminates the need for negative sampling, momentum encoders, or stop-gradients, reducing the training objective to a single hyperparameter. We evaluate NOVA on zeroshot chest X-ray classification using ClinicalBERT as the text encoder and Vision Transformers trained from scratch on MIMIC-CXR. On zero-shot classification across three benchmark datasets, NOVA outperforms multiple standard baselines while exhibiting substantially more consistent training runs. Our results demonstrate that non-contrastive vision-language pretraining offers a simpler, more stable, and more effective alternative to contrastive methods.

</details>


### [234] [Video Understanding: Through A Temporal Lens](https://arxiv.org/abs/2602.00683)
*Thong Thanh Nguyen*

Main category: cs.CV

Relevance: 75.0

TL;DR: 该论文提出了一套视频理解方法，包括自动标注框架、参数高效微调策略、状态空间层集成、细粒度运动建模框架，以及对大型视觉语言模型的实证研究，强调显式时序建模的重要性。


<details>
  <summary>Details</summary>
Motivation: 现有视频理解方法在利用视频元素间时序关系方面存在局限，需要更有效的时序建模方法来提升视频内容的理解能力。

Method: 1) 基于大型视觉语言模型的自动标注框架，采用噪声鲁棒对比学习目标；2) 使用"循环适配器"的参数高效微调策略；3) 集成状态空间层进行长视频建模，并引入两个新的长时基准；4) 细粒度运动-时刻关系建模的对比学习框架；5) 对大型视觉语言模型的实证研究。

Result: 证明了显式时序建模能显著提升模型对视频内容的表示和推理能力，特别是在长视频理解和细粒度运动分析方面。

Conclusion: 显式时序建模对于视频理解至关重要，提出的方法在多个方面提升了视频理解性能，并为大型视觉语言模型的时序推理提供了新方向。

Abstract: This thesis explores the central question of how to leverage temporal relations among video elements to advance video understanding. Addressing the limitations of existing methods, the work presents a five-fold contribution: (1) an automatic annotation framework that utilizes large vision-language models and a noise-robust contrastive learning objective with a subtractive angular margin; (2) a parameter-efficient fine-tuning strategy using "recurrent adapters" to capture temporal dynamics in low-data regimes; (3) the integration of State Space Layers (SSL) for efficient long-form video modeling, supported by the introduction of two new long-term benchmarks for egocentric and feature-length content; (4) a novel contrastive learning framework designed to explicitly model fine-grained relations between motions and video moments; and (5) a comprehensive empirical study on Large Vision-Language Models (LVLMs) that identifies the visual-language interface as a bottleneck for temporal reasoning, leading to a new "temporal-oriented recipe" for upscaled video understanding. Collectively, these contributions demonstrate that explicit temporal modeling significantly enhances a model's ability to represent and reason about the fluid nature of video content.

</details>


### [235] [Any3D-VLA: Enhancing VLA Robustness via Diverse Point Clouds](https://arxiv.org/abs/2602.00807)
*Xianzhe Fan,Shengliang Deng,Xiaoyang Wu,Yuxiang Lu,Zhuoling Li,Mi Yan,Yujia Zhang,Zhizheng Zhang,He Wang,Hengshuang Zhao*

Main category: cs.CV

Relevance: 75.0

TL;DR: Any3D-VLA：通过统一模拟器、传感器和模型估计的点云，学习领域无关的3D表示并与2D表示融合，以增强视觉-语言-动作模型的空间理解能力


<details>
  <summary>Details</summary>
Motivation: 现有VLA模型通常以2D图像作为视觉输入，限制了其在复杂场景中的空间理解能力。需要探索如何融入3D信息来增强VLA能力，并解决3D数据稀缺和跨环境差异带来的领域差距问题。

Method: 提出Any3D-VLA框架：1）统一模拟器、传感器和模型估计的点云训练管道；2）构建多样化输入；3）学习领域无关的3D表示并与对应的2D表示融合。

Result: 实验表明：1）将视觉输入显式提升为点云能产生比2D表示更好的补充表示；2）Any3D-VLA在模拟和真实世界实验中展现出性能提升优势，并能有效缓解领域差距问题。

Conclusion: 通过融合3D点云表示，可以显著增强VLA模型的空间理解能力，Any3D-VLA为解决3D数据稀缺和领域差距问题提供了有效解决方案。

Abstract: Existing Vision-Language-Action (VLA) models typically take 2D images as visual input, which limits their spatial understanding in complex scenes. How can we incorporate 3D information to enhance VLA capabilities? We conduct a pilot study across different observation spaces and visual representations. The results show that explicitly lifting visual input into point clouds yields representations that better complement their corresponding 2D representations. To address the challenges of (1) scarce 3D data and (2) the domain gap induced by cross-environment differences and depth-scale biases, we propose Any3D-VLA. It unifies the simulator, sensor, and model-estimated point clouds within a training pipeline, constructs diverse inputs, and learns domain-agnostic 3D representations that are fused with the corresponding 2D representations. Simulation and real-world experiments demonstrate Any3D-VLA's advantages in improving performance and mitigating the domain gap. Our project homepage is available at https://xianzhefan.github.io/Any3D-VLA.github.io.

</details>


### [236] [Unveiling the Cognitive Compass: Theory-of-Mind-Guided Multimodal Emotion Reasoning](https://arxiv.org/abs/2602.00971)
*Meng Luo,Bobo Li,Shanqing Xu,Shize Zhang,Qiuchan Chen,Menglu Han,Wenhao Chen,Yanxiang Huang,Hao Fei,Mong-Li Lee,Wynne Hsu*

Main category: cs.CV

Relevance: 75.0

TL;DR: 论文提出HitEmotion基准测试和ToM引导的推理方法，用于评估和提升多模态大语言模型的深度情感理解能力，通过心智理论框架解决当前MLLMs在情感认知方面的局限性。


<details>
  <summary>Details</summary>
Motivation: 当前多模态大语言模型在深度情感理解方面存在局限，需要基于心智理论（Theory of Mind）的认知基础来建模情感智能。

Method: 1) 提出HitEmotion基准测试，分层诊断认知深度能力断点；2) 设计ToM引导的推理链，追踪心理状态并校准跨模态证据；3) 提出TMPO强化学习方法，使用中间心理状态作为过程级监督。

Result: HitEmotion揭示了SOTA模型在认知要求高的任务上的深度情感推理缺陷；ToM引导的推理链和TMPO提高了端任务准确性，并产生了更忠实、更连贯的推理依据。

Conclusion: 该工作为研究社区提供了实用的工具包，用于评估和增强MLLMs基于认知的情感理解能力。

Abstract: Despite rapid progress in multimodal large language models (MLLMs), their capability for deep emotional understanding remains limited. We argue that genuine affective intelligence requires explicit modeling of Theory of Mind (ToM), the cognitive substrate from which emotions arise. To this end, we introduce HitEmotion, a ToM-grounded hierarchical benchmark that diagnoses capability breakpoints across increasing levels of cognitive depth. Second, we propose a ToM-guided reasoning chain that tracks mental states and calibrates cross-modal evidence to achieve faithful emotional reasoning. We further introduce TMPO, a reinforcement learning method that uses intermediate mental states as process-level supervision to guide and strengthen model reasoning. Extensive experiments show that HitEmotion exposes deep emotional reasoning deficits in state-of-the-art models, especially on cognitively demanding tasks. In evaluation, the ToM-guided reasoning chain and TMPO improve end-task accuracy and yield more faithful, more coherent rationales. In conclusion, our work provides the research community with a practical toolkit for evaluating and enhancing the cognition-based emotional understanding capabilities of MLLMs. Our dataset and code are available at: https://HitEmotion.github.io/.

</details>


### [237] [From Videos to Conversations: Egocentric Instructions for Task Assistance](https://arxiv.org/abs/2602.01038)
*Lavisha Aggarwal,Vikas Bahirwani,Andrea Colaco*

Main category: cs.CV

Relevance: 75.0

TL;DR: 提出了一个自动将单人教学视频转换为双人多模态任务指导对话的框架，并发布了HowToDIV数据集，包含507个对话、6,636个问答对和24小时视频，为多模态程序性任务辅助提供基准。


<details>
  <summary>Details</summary>
Motivation: 日常任务（如设备维修、烹饪、汽车维护）需要专业知识，特别是复杂的多步骤程序。尽管AI助手在增强现实（AR）辅助方面受到关注，但由于缺乏大规模、基于真实世界任务执行的多模态对话数据集，进展受限，这部分源于人工数据收集的成本和复杂性。

Method: 基于大语言模型的完全自动流水线，将单人教学视频自动转换为双人多模态任务指导对话。该框架提供了可扩展且成本效益高的替代传统数据收集方法。

Result: 创建了HowToDIV多模态数据集，包含507个对话、6,636个问答对和24小时视频，涵盖多个领域。每个会话包含多轮专家-新手交互。使用Gemma 3和Qwen 2.5提供了基线结果，为多模态程序性任务辅助建立了初步基准。

Conclusion: 提出的自动框架能够高效生成大规模多模态对话数据集，解决了传统数据收集方法的成本问题。HowToDIV数据集为多模态程序性任务辅助研究提供了有价值的资源，并建立了初步的性能基准。

Abstract: Many everyday tasks, ranging from appliance repair and cooking to car maintenance, require expert knowledge, particularly for complex, multi-step procedures. Despite growing interest in AI agents for augmented reality (AR) assistance, progress remains limited by the scarcity of large-scale multimodal conversational datasets grounded in real-world task execution, in part due to the cost and logistical complexity of human-assisted data collection. In this paper, we present a framework to automatically transform single person instructional videos into two-person multimodal task-guidance conversations. Our fully automatic pipeline, based on large language models, provides a scalable and cost efficient alternative to traditional data collection approaches. Using this framework, we introduce HowToDIV, a multimodal dataset comprising 507 conversations, 6,636 question answer pairs, and 24 hours of video spanning multiple domains. Each session consists of a multi-turn expert-novice interaction. Finally, we report baseline results using Gemma 3 and Qwen 2.5 on HowToDIV, providing an initial benchmark for multimodal procedural task assistance.

</details>


### [238] [Residual Decoding: Mitigating Hallucinations in Large Vision-Language Models via History-Aware Residual Guidance](https://arxiv.org/abs/2602.01047)
*Xinrong Chen,Xu Chu,Yingmin Qiu,Hengyuan Zhang,Jing Xiong,Shiyu Tang,Shuai Liu,Shaokang Yang,Cheng Yang,Hayden Kwok-Hay So,Ngai Wong*

Main category: cs.CV

Relevance: 75.0

TL;DR: 提出ResDec方法，一种无需训练的残差解码技术，利用LVLMs的内部隐式推理机制和token logits演化机制来纠正语言先验导致的幻觉问题。


<details>
  <summary>Details</summary>
Motivation: 大型视觉语言模型（LVLMs）虽然在多模态任务中表现良好，但受到语言先验的影响，经常产生幻觉。幻觉指生成的内容在语法和句法上连贯，但与实际视觉输入不匹配或不相关。

Method: 提出Residual Decoding（ResDec），一种无需训练的解码方法。它利用历史信息辅助解码，依靠LVLMs的内部隐式推理机制和token logits演化机制来纠正偏差。

Result: 实验表明ResDec能有效抑制语言先验引起的幻觉，显著改善视觉基础，减少物体幻觉。此外，在综合LVLM基准测试中也表现优异。

Conclusion: ResDec是一种有效的训练免费方法，能缓解LVLMs的幻觉问题，同时保持广泛的适用性。

Abstract: Large Vision-Language Models (LVLMs) can reason effectively from image-text inputs and perform well in various multimodal tasks. Despite this success, they are affected by language priors and often produce hallucinations. Hallucinations denote generated content that is grammatically and syntactically coherent, yet bears no match or direct relevance to actual visual input. To address this problem, we propose Residual Decoding (ResDec). It is a novel training-free method that uses historical information to aid decoding. The method relies on the internal implicit reasoning mechanism and token logits evolution mechanism of LVLMs to correct biases. Extensive experiments demonstrate that ResDec effectively suppresses hallucinations induced by language priors, significantly improves visual grounding, and reduces object hallucinations. In addition to mitigating hallucinations, ResDec also performs exceptionally well on comprehensive LVLM benchmarks, highlighting its broad applicability.

</details>


### [239] [Differential Vector Erasure: Unified Training-Free Concept Erasure for Flow Matching Models](https://arxiv.org/abs/2602.01089)
*Zhiqi Zhang,Xinhao Zhong,Yi Sun,Shuoyang Sun,Bin Chen,Shu-Tao Xia,Xuan Wang*

Main category: cs.CV

Relevance: 75.0

TL;DR: DVE是一种无需训练的概念擦除方法，专门为流匹配模型设计，通过分析速度场的微分向量结构来选择性移除特定概念，在保持图像质量的同时实现精确概念抑制。


<details>
  <summary>Details</summary>
Motivation: 文本到图像扩散模型虽然能生成高质量图像，但容易产生NSFW内容、受版权保护的风格或特定对象等不良概念，这对安全可控部署构成挑战。现有概念擦除方法主要针对DDPM扩散模型且需要昂贵的微调，而新兴的流匹配模型具有不同的生成范式，现有方法无法直接应用。

Method: 提出差分向量擦除（DVE），这是一种专门为流匹配模型设计的免训练概念擦除方法。核心洞察是语义概念隐含在控制生成流的速度场的定向结构中。通过构建表征目标概念与锚概念之间方向差异的微分向量场，在推理过程中将速度场投影到微分方向以选择性移除概念特定分量。

Result: 在FLUX模型上的大量实验表明，DVE在NSFW抑制、艺术风格移除和对象擦除等多种概念擦除任务上始终优于现有基线方法，同时保持了图像质量和多样性。

Conclusion: DVE为流匹配模型提供了一种有效的免训练概念擦除方法，通过利用速度场的微分向量结构实现精确概念抑制，为安全可控的文本到图像生成提供了新解决方案。

Abstract: Text-to-image diffusion models have demonstrated remarkable capabilities in generating high-quality images, yet their tendency to reproduce undesirable concepts, such as NSFW content, copyrighted styles, or specific objects, poses growing concerns for safe and controllable deployment. While existing concept erasure approaches primarily focus on DDPM-based diffusion models and rely on costly fine-tuning, the recent emergence of flow matching models introduces a fundamentally different generative paradigm for which prior methods are not directly applicable. In this paper, we propose Differential Vector Erasure (DVE), a training-free concept erasure method specifically designed for flow matching models. Our key insight is that semantic concepts are implicitly encoded in the directional structure of the velocity field governing the generative flow. Leveraging this observation, we construct a differential vector field that characterizes the directional discrepancy between a target concept and a carefully chosen anchor concept. During inference, DVE selectively removes concept-specific components by projecting the velocity field onto the differential direction, enabling precise concept suppression without affecting irrelevant semantics. Extensive experiments on FLUX demonstrate that DVE consistently outperforms existing baselines on a wide range of concept erasure tasks, including NSFW suppression, artistic style removal, and object erasure, while preserving image quality and diversity.

</details>


### [240] [Improving Robustness of Vision-Language-Action Models by Restoring Corrupted Visual Inputs](https://arxiv.org/abs/2602.01158)
*Daniel Yezid Guarnizo Orjuela,Leonardo Scappatura,Veronica Di Gennaro,Riccardo Andrea Izzo,Gianluca Bardaro,Matteo Matteucci*

Main category: cs.CV

Relevance: 75.0

TL;DR: 提出CRT（Corruption Restoration Transformer）来增强VLA模型对图像损坏的鲁棒性，无需微调底层模型即可恢复性能


<details>
  <summary>Details</summary>
Motivation: VLA模型在机器人操作中表现出色，但对图像损坏（如传感器噪声、坏点、镜头污染）非常脆弱，现有研究主要关注物理遮挡，而传感器级损坏问题被忽视

Method: 提出CRT（Corruption Restoration Transformer），一种即插即用、模型无关的视觉Transformer，通过对抗训练目标从损坏输入中恢复干净观测，无需微调底层VLA模型

Result: 在LIBERO和Meta-World基准测试中，CRT能有效恢复VLA模型性能，即使在严重视觉损坏下也能保持接近基线的成功率

Conclusion: CRT为VLA模型提供了一种有效的传感器损坏免疫方案，提高了其在真实世界部署的鲁棒性

Abstract: Vision-Language-Action (VLA) models have emerged as a dominant paradigm for generalist robotic manipulation, unifying perception and control within a single end-to-end architecture. However, despite their success in controlled environments, reliable real-world deployment is severely hindered by their fragility to visual disturbances. While existing literature extensively addresses physical occlusions caused by scene geometry, a critical mode remains largely unexplored: image corruptions. These sensor-level artifacts, ranging from electronic noise and dead pixels to lens contaminants, directly compromise the integrity of the visual signal prior to interpretation. In this work, we quantify this vulnerability, demonstrating that state-of-the-art VLAs such as $π_{0.5}$ and SmolVLA, suffer catastrophic performance degradation, dropping from 90\% success rates to as low as 2\%, under common signal artifacts. To mitigate this, we introduce the Corruption Restoration Transformer (CRT), a plug-and-play and model-agnostic vision transformer designed to immunize VLA models against sensor disturbances. Leveraging an adversarial training objective, CRT restores clean observations from corrupted inputs without requiring computationally expensive fine-tuning of the underlying model. Extensive experiments across the LIBERO and Meta-World benchmarks demonstrate that CRT effectively recovers lost performance, enabling VLAs to maintain near-baseline success rates, even under severe visual corruption.

</details>


### [241] [Interaction-Consistent Object Removal via MLLM-Based Reasoning](https://arxiv.org/abs/2602.01298)
*Ching-Kai Huang,Wen-Chieh Lin,Yan-Cen Lee*

Main category: cs.CV

Relevance: 75.0

TL;DR: 论文提出交互一致的对象移除（ICOR）问题，要求移除目标对象及其相关交互元素，并开发了基于多模态大语言模型的推理增强对象移除框架REORM，在ICOREval基准上优于现有图像编辑系统。


<details>
  <summary>Details</summary>
Motivation: 当前基于图像的对象移除方法通常只移除命名目标，但忽略了与目标对象相关的交互证据（如光照效果、物理连接对象、目标产生的元素等），导致结果语义不一致。需要解决交互一致的对象移除问题。

Method: 提出REORM框架：1）使用多模态大语言模型推理需要共同移除的元素；2）模块化设计整合MLLM驱动分析、掩码引导移除和自校正机制；3）提供本地部署变体以支持有限资源下的精确编辑。

Result: 在ICOREval基准测试中，REORM优于最先进的图像编辑系统，能够有效产生交互一致的结果。基准包含具有丰富交互依赖性的指令驱动移除任务。

Conclusion: 交互一致的对象移除是一个重要但被忽视的问题，REORM框架通过结合多模态大语言模型的推理能力，能够有效识别和移除相关交互元素，提升图像编辑的语义一致性。

Abstract: Image-based object removal often erases only the named target, leaving behind interaction evidence that renders the result semantically inconsistent. We formalize this problem as Interaction-Consistent Object Removal (ICOR), which requires removing not only the target object but also associated interaction elements, such as lighting-dependent effects, physically connected objects, targetproduced elements, and contextually linked objects. To address this task, we propose Reasoning-Enhanced Object Removal with MLLM (REORM), a reasoningenhanced object removal framework that leverages multimodal large language models to infer which elements must be jointly removed. REORM features a modular design that integrates MLLM-driven analysis, mask-guided removal, and a self-correction mechanism, along with a local-deployment variant that supports accurate editing under limited resources. To support evaluation, we introduce ICOREval, a benchmark consisting of instruction-driven removals with rich interaction dependencies. On ICOREval, REORM outperforms state-of-the-art image editing systems, demonstrating its effectiveness in producing interactionconsistent results.

</details>


### [242] [What Does Vision Tool-Use Reinforcement Learning Really Learn? Disentangling Tool-Induced and Intrinsic Effects for Crop-and-Zoom](https://arxiv.org/abs/2602.01334)
*Yan Ma,Weiyu Zhang,Tianle Li,Linge Du,Xuyang Shen,Pengfei Liu*

Main category: cs.CV

Relevance: 75.0

TL;DR: 本文提出了MED框架，用于分析视觉工具使用强化学习对视觉语言模型的影响，发现性能提升主要来自内在能力学习而非工具使用掌握


<details>
  <summary>Details</summary>
Motivation: 视觉工具使用强化学习虽然能提升视觉语言模型性能，但尚不清楚这种提升是来自工具使用能力的改善还是模型内在能力的演化。需要一种方法来区分这两种效应。

Method: 提出MED（测量-解释-诊断）框架：1）粗粒度分离内在能力变化与工具诱导效应；2）将工具诱导性能差异分解为增益和损害项；3）探究驱动其演化的机制。在两个具有不同工具先验的VLM和六个基准上进行分析。

Result: 发现性能改进主要由内在学习主导，而工具使用强化学习主要减少工具诱导的损害（如减少调用错误和减弱工具模式干扰），在基于工具纠正内在失败方面进展有限。当前视觉工具使用强化学习是学习与工具安全共存而非掌握工具。

Conclusion: 当前视觉工具使用强化学习主要帮助模型减少工具使用带来的负面影响，而非真正掌握工具使用能力。这揭示了现有方法的局限性，为未来改进提供了方向。

Abstract: Vision tool-use reinforcement learning (RL) can equip vision-language models with visual operators such as crop-and-zoom and achieves strong performance gains, yet it remains unclear whether these gains are driven by improvements in tool use or evolving intrinsic capabilities.We introduce MED (Measure-Explain-Diagnose), a coarse-to-fine framework that disentangles intrinsic capability changes from tool-induced effects, decomposes the tool-induced performance difference into gain and harm terms, and probes the mechanisms driving their evolution. Across checkpoint-level analyses on two VLMs with different tool priors and six benchmarks, we find that improvements are dominated by intrinsic learning, while tool-use RL mainly reduces tool-induced harm (e.g., fewer call-induced errors and weaker tool schema interference) and yields limited progress in tool-based correction of intrinsic failures. Overall, current vision tool-use RL learns to coexist safely with tools rather than master them.

</details>


### [243] [PolyGen: Fully Synthetic Vision-Language Training via Multi-Generator Ensembles](https://arxiv.org/abs/2602.01370)
*Leonardo Brusini,Cristian Sbrolli,Eugenio Lomurno,Toshihiko Yamasaki,Matteo Matteucci*

Main category: cs.CV

Relevance: 75.0

TL;DR: PolyGen框架通过多源生成器合成数据，强调流形覆盖和组合严谨性，而非单纯扩大数据集规模，显著提升视觉语言预训练性能。


<details>
  <summary>Details</summary>
Motivation: 当前合成数据方法主要依赖单一生成主干进行扩展，这引入了生成器特定的谱偏差并限制了特征多样性。需要一种能减少模型特定伪影、提高特征多样性的合成数据构造方法。

Method: 采用Polylithic方法，在架构不同的生成器交集上进行训练，有效边缘化模型特定伪影。引入Programmatic Hard Negative课程，强制细粒度语法理解。通过结构性地将相同数据预算从独特标题重新分配到多源变体。

Result: 在聚合多任务基准测试中比领先的单源基线(SynthCLIP)提升+19.0%，在SugarCrepe++组合性基准测试中提升+9.1%。证明结构多样性比简单增加单源样本量是更数据高效的扩展定律。

Conclusion: 结构多样性比单纯扩大单源样本量更有效，多源合成数据方法能构建更鲁棒的特征空间，是视觉语言预训练中更数据高效的扩展策略。

Abstract: Synthetic data offers a scalable solution for vision-language pre-training, yet current state-of-the-art methods typically rely on scaling up a single generative backbone, which introduces generator-specific spectral biases and limits feature diversity. In this work, we introduce PolyGen, a framework that redefines synthetic data construction by prioritizing manifold coverage and compositional rigor over simple dataset size. PolyGen employs a Polylithic approach to train on the intersection of architecturally distinct generators, effectively marginalizing out model-specific artifacts. Additionally, we introduce a Programmatic Hard Negative curriculum that enforces fine-grained syntactic understanding. By structurally reallocating the same data budget from unique captions to multi-source variations, PolyGen achieves a more robust feature space, outperforming the leading single-source baseline (SynthCLIP) by +19.0% on aggregate multi-task benchmarks and on the SugarCrepe++ compositionality benchmark (+9.1%). These results demonstrate that structural diversity is a more data-efficient scaling law than simply increasing the volume of single-source samples.

</details>


### [244] [Understanding vision transformer robustness through the lens of out-of-distribution detection](https://arxiv.org/abs/2602.01459)
*Joey Kuang,Alexander Wong*

Main category: cs.CV

Relevance: 75.0

TL;DR: 该论文研究了量化对视觉Transformer在分布外检测中的影响，发现大规模预训练可能损害低比特量化的鲁棒性，而数据增强可能是更好的选择。


<details>
  <summary>Details</summary>
Motivation: 视觉Transformer在视觉任务中表现出色，但实现可访问和实时使用仍具挑战性。量化能减少内存和推理成本，但可能导致性能损失。现有研究主要关注分布内任务行为，而注意力机制可能通过探索分布外情况提供量化属性的新见解。

Method: 研究了量化的小型变体流行视觉Transformer（DeiT、DeiT3和ViT）在常见分布外数据集上的行为。分析了4位量化模型的不稳定性，特别是那些在更大数据集（ImageNet-22k）上训练的模型。比较了不同预训练策略对量化鲁棒性的影响。

Result: 1. 分布内分析显示4位模型存在初始不稳定性，特别是那些在ImageNet-22k上训练的模型。最强的FP32模型DeiT3在量化后性能下降17%，成为最弱的4位模型之一。
2. 分布外检测揭示了更显著的影响：在ImageNet-22k上预训练的ViT和DeiT3分别经历了15.0%和19.2%的平均量化delta（AUPR-out），而仅在ImageNet-1k上训练的对应模型只经历了9.5%和12.0%的delta。

Conclusion: 大规模数据集上的预训练可能损害低比特量化在分布外检测中的鲁棒性，数据增强可能是更有利的选择。这为视觉Transformer的量化策略提供了重要见解。

Abstract: Vision transformers have shown remarkable performance in vision tasks, but enabling them for accessible and real-time use is still challenging. Quantization reduces memory and inference costs at the risk of performance loss. Strides have been made to mitigate low precision issues mainly by understanding in-distribution (ID) task behaviour, but the attention mechanism may provide insight on quantization attributes by exploring out-of-distribution (OOD) situations. We investigate the behaviour of quantized small-variant popular vision transformers (DeiT, DeiT3, and ViT) on common OOD datasets. ID analyses show the initial instabilities of 4-bit models, particularly of those trained on the larger ImageNet-22k, as the strongest FP32 model, DeiT3, sharply drop 17% from quantization error to be one of the weakest 4-bit models. While ViT shows reasonable quantization robustness for ID calibration, OOD detection reveals more: ViT and DeiT3 pretrained on ImageNet-22k respectively experienced a 15.0% and 19.2% average quantization delta in AUPR-out between full precision to 4-bit while their ImageNet-1k-only counterparts experienced a 9.5% and 12.0% delta. Overall, our results suggest pretraining on large scale datasets may hinder low-bit quantization robustness in OOD detection and that data augmentation may be a more beneficial option.

</details>


### [245] [Preserving Localized Patch Semantics in VLMs](https://arxiv.org/abs/2602.01530)
*Parsa Esmaeilkhani,Longin Jan Latecki*

Main category: cs.CV

Relevance: 75.0

TL;DR: 提出Logit Lens Loss (LLL)來解決視覺語言模型中Logit Lens可解釋性失效的問題，通過補充損失函數保持視覺token的局部視覺信息，改善熱圖可視化效果並提升視覺任務性能。


<details>
  <summary>Details</summary>
Motivation: Logit Lens原本用於可視化LLM中對答案貢獻最大的token，在視覺語言模型(VLMs)中也可用於生成熱圖顯示圖像token的概念內容。但視覺內容常擴散到語言token中，導致局部視覺信息被破壞，使Logit Lens可視化失去可解釋性價值。

Method: 提出Logit Lens Loss (LLL)作為下一個token預測(NTP)的補充損失。LLL旨在使視覺token嵌入與描述其圖像區域的文本概念更語義對齊（如包含貓的圖塊與"貓"這個詞），無需架構修改或大規模訓練。該損失約束自注意力層中圖像和文本token的混合，防止視覺token失去局部視覺信息。

Result: LLL不僅使Logit Lens變得實用，能生成有意義的圖像對象置信度圖，還提高了分割等視覺中心任務的性能，且無需附加任何特殊頭部。

Conclusion: Logit Lens Loss有效解決了視覺語言模型中視覺信息擴散問題，既增強了模型的可解釋性，又提升了視覺任務性能，為VLMs的可視化分析提供了實用工具。

Abstract: Logit Lens has been proposed for visualizing tokens that contribute most to LLM answers. Recently, Logit Lens was also shown to be applicable in autoregressive Vision-Language Models (VLMs), where it illustrates the conceptual content of image tokens in the form of heatmaps, e.g., which image tokens are likely to depict the concept of cat in a given image. However, the visual content of image tokens often gets diffused to language tokens, and consequently, the locality of visual information gets mostly destroyed, which renders Logit Lens visualization unusable for explainability. To address this issue, we introduce a complementary loss to next-token prediction (NTP) to prevent the visual tokens from losing the visual representation inherited from corresponding image patches. The proposed Logit Lens Loss (LLL) is designed to make visual token embeddings more semantically aligned with the textual concepts that describe their image regions (e.g., patches containing a cat with the word "cat"), without requiring any architectural modification or large-scale training. This way, LLL constrains the mixing of image and text tokens in the self-attention layers in order to prevent image tokens from losing their localized visual information. As our experiments show, LLL not only makes Logit Lens practically relevant by producing meaningful object confidence maps in images, but also improves performance on vision-centric tasks like segmentation without attaching any special heads.

</details>


### [246] [Multimodal UNcommonsense: From Odd to Ordinary and Ordinary to Odd](https://arxiv.org/abs/2602.01561)
*Yejin Son,Saejin Kim,Dongjun Min,Younjae Yu*

Main category: cs.CV

Relevance: 75.0

TL;DR: MUN是一个评估多模态模型处理非典型视觉场景能力的基准，通过检索式上下文学习框架提升小模型在不常见场景下的推理性能。


<details>
  <summary>Details</summary>
Motivation: 多模态环境中的常识推理仍然是AI的基础挑战。现有模型在处理偏离典型视觉或上下文预期的场景时表现不佳，需要评估和改进模型在非典型、文化多样和现实世界场景中的鲁棒性和适应性。

Method: 提出检索式上下文学习(R-ICL)框架，通过新颖的多模态集成检索器(MER)识别语义相关的示例，即使图像和文本对故意不协调。该方法将大模型的推理能力转移到小模型，无需额外训练。

Result: 实验显示，相比基线ICL方法平均提升8.3%，证明了R-ICL在低频、非典型场景中的有效性。MUN基准为评估和改进视觉语言模型在现实世界、文化多样和非典型场景中的能力开辟了新方向。

Conclusion: MUN基准和R-ICL框架有效解决了多模态非典型场景推理的挑战，为评估模型在偏离典型期望的场景中的表现提供了新工具，并展示了检索式上下文学习在提升小模型推理能力方面的潜力。

Abstract: Commonsense reasoning in multimodal contexts remains a foundational challenge in artificial intelligence. We introduce Multimodal UNcommonsense(MUN), a benchmark designed to evaluate models' ability to handle scenarios that deviate from typical visual or contextual expectations. MUN pairs visual scenes with surprising or unlikely outcomes described in natural language, prompting models to either rationalize seemingly odd images using everyday logic or uncover unexpected interpretations in ordinary scenes. To support this task, we propose a retrieval-based in-context learning (R-ICL) framework that transfers reasoning capabilities from larger models to smaller ones without additional training. Leveraging a novel Multimodal Ensemble Retriever (MER), our method identifies semantically relevant exemplars even when image and text pairs are deliberately discordant. Experiments show an average improvement of 8.3% over baseline ICL methods, highlighting the effectiveness of R-ICL in low-frequency, atypical settings. MUN opens new directions for evaluating and improving visual-language models' robustness and adaptability in real-world, culturally diverse, and non-prototypical scenarios.

</details>


### [247] [SGHA-Attack: Semantic-Guided Hierarchical Alignment for Transferable Targeted Attacks on Vision-Language Models](https://arxiv.org/abs/2602.01574)
*Haobo Wang,Weiqi Luo,Xiaojun Jia,Xiaochun Cao*

Main category: cs.CV

Relevance: 75.0

TL;DR: SGHA-Attack是一种针对大型视觉语言模型的语义引导分层对齐对抗攻击框架，通过多目标参考和中间层对齐提升跨模型迁移性


<details>
  <summary>Details</summary>
Motivation: 现有针对视觉语言模型的迁移攻击方法通常过度拟合代理模型的嵌入空间，仅依赖单一参考并强调最终层对齐，未能充分利用中间语义信息，导致在异构VLM间的迁移效果不佳

Method: 提出语义引导分层对齐框架：1) 通过冻结的文本到图像模型生成视觉基础参考池，选择Top-K语义相关锚点形成加权混合；2) 在特征层次中注入目标语义，在多个深度对齐中间视觉表示（全局和空间粒度）；3) 在共享潜在子空间中同步中间视觉和文本特征，在最终投影前提供早期跨模态监督

Result: 在开源和商业黑盒VLM上的广泛实验表明，SGHA-Attack比现有方法具有更强的目标迁移能力，并且在预处理和净化防御下保持鲁棒性

Conclusion: 通过多参考选择和分层对齐策略，SGHA-Attack有效解决了现有迁移攻击在异构视觉语言模型间的过拟合问题，提升了对抗样本的跨模型迁移性

Abstract: Large vision-language models (VLMs) are vulnerable to transfer-based adversarial perturbations, enabling attackers to optimize on surrogate models and manipulate black-box VLM outputs. Prior targeted transfer attacks often overfit surrogate-specific embedding space by relying on a single reference and emphasizing final-layer alignment, which underutilizes intermediate semantics and degrades transfer across heterogeneous VLMs. To address this, we propose SGHA-Attack, a Semantic-Guided Hierarchical Alignment framework that adopts multiple target references and enforces intermediate-layer consistency. Concretely, we generate a visually grounded reference pool by sampling a frozen text-to-image model conditioned on the target prompt, and then carefully select the Top-K most semantically relevant anchors under the surrogate to form a weighted mixture for stable optimization guidance. Building on these anchors, SGHA-Attack injects target semantics throughout the feature hierarchy by aligning intermediate visual representations at both global and spatial granularities across multiple depths, and by synchronizing intermediate visual and textual features in a shared latent subspace to provide early cross-modal supervision before the final projection. Extensive experiments on open-source and commercial black-box VLMs show that SGHA-Attack achieves stronger targeted transferability than prior methods and remains robust under preprocessing and purification defenses.

</details>


### [248] [Samba+: General and Accurate Salient Object Detection via A More Unified Mamba-based Framework](https://arxiv.org/abs/2602.01593)
*Wenzhuo Zhao,Keren Fu,Jiahao He,Xiaohong Liu,Qijun Zhao,Guangtao Zhai*

Main category: cs.CV

Relevance: 75.0

TL;DR: 提出Saliency Mamba (Samba)框架，基于状态空间模型Mamba处理多种显著目标检测任务，包括RGB/RGB-D/RGB-T SOD、视频SOD等。通过空间邻域扫描算法保持显著区域连续性，上下文感知上采样促进特征对齐。进一步提出Samba+，通过多任务联合训练实现统一模型，包含跨模态交互融合和模态锚定持续学习策略。


<details>
  <summary>Details</summary>
Motivation: 现有显著目标检测模型受限于CNN的有限感受野和Transformer的二次计算复杂度。新兴的状态空间模型Mamba在平衡全局感受野和计算效率方面展现出潜力，因此探索基于Mamba的纯架构来解决多种SOD任务。

Method: 1. 提出Saliency Mamba (Samba)架构：基于Mamba的纯架构，包含saliency-guided Mamba block (SGMB)和context-aware upsampling (CAU)。SGMB采用空间邻域扫描算法保持显著区域空间连续性。2. 提出Samba+：通过多任务联合训练的统一模型，包含hub-and-spoke graph attention (HGA)模块实现自适应跨模态交互融合，以及modality-anchored continual learning (MACL)策略缓解模态冲突和灾难性遗忘。

Result: Samba在6个SOD任务的22个数据集上超越现有方法，且计算成本更低。Samba+使用单一训练模型在这些任务和数据集上获得更优结果。实验证明Samba框架具有良好潜力。

Conclusion: 基于Mamba的Samba框架在显著目标检测任务中展现出优越性能，既能处理多种模态输入，又能保持计算效率。Samba+进一步解决了任务特定性问题，实现了更统一和通用的模型。

Abstract: Existing salient object detection (SOD) models are generally constrained by the limited receptive fields of convolutional neural networks (CNNs) and quadratic computational complexity of Transformers. Recently, the emerging state-space model, namely Mamba, has shown great potential in balancing global receptive fields and computational efficiency. As a solution, we propose Saliency Mamba (Samba), a pure Mamba-based architecture that flexibly handles various distinct SOD tasks, including RGB/RGB-D/RGB-T SOD, video SOD (VSOD), RGB-D VSOD, and visible-depth-thermal SOD. Specifically, we rethink the scanning strategy of Mamba for SOD, and introduce a saliency-guided Mamba block (SGMB) that features a spatial neighborhood scanning (SNS) algorithm to preserve the spatial continuity of salient regions. A context-aware upsampling (CAU) method is also proposed to promote hierarchical feature alignment and aggregation by modeling contextual dependencies. As one step further, to avoid the "task-specific" problem as in previous SOD solutions, we develop Samba+, which is empowered by training Samba in a multi-task joint manner, leading to a more unified and versatile model. Two crucial components that collaboratively tackle challenges encountered in input of arbitrary modalities and continual adaptation are investigated. Specifically, a hub-and-spoke graph attention (HGA) module facilitates adaptive cross-modal interactive fusion, and a modality-anchored continual learning (MACL) strategy alleviates inter-modal conflicts together with catastrophic forgetting. Extensive experiments demonstrate that Samba individually outperforms existing methods across six SOD tasks on 22 datasets with lower computational cost, whereas Samba+ achieves even superior results on these tasks and datasets by using a single trained versatile model. Additional results further demonstrate the potential of our Samba framework.

</details>


### [249] [Token Pruning for In-Context Generation in Diffusion Transformers](https://arxiv.org/abs/2602.01609)
*Junqing Lin,Xingyu Zheng,Pei Cheng,Bin Fu,Jingwei Sun,Guangzhong Sun*

Main category: cs.CV

Relevance: 75.0

TL;DR: ToPi：针对DiT上下文生成的无训练令牌剪枝框架，通过校准驱动的敏感性分析识别关键注意力层，选择性剪枝上下文令牌，实现30%以上推理加速，同时保持图像质量。


<details>
  <summary>Details</summary>
Motivation: DiT中的上下文生成虽然增强了可控的图像到图像生成能力，但输入拼接导致序列长度急剧增加，造成显著的计算瓶颈。现有的令牌缩减技术主要针对文本到图像合成，在上下文生成范式下采用统一的缩减策略，忽略了参考上下文和目标潜在表示在空间、时间和功能维度上的角色不对称性。

Method: 提出ToPi框架：1）使用离线校准驱动的敏感性分析识别关键注意力层，作为冗余估计的鲁棒代理；2）基于这些层推导新的影响度量来量化每个上下文令牌的贡献，进行选择性剪枝；3）采用时间更新策略适应扩散轨迹的演化。

Result: 经验评估表明，ToPi可以在复杂图像生成任务中实现超过30%的推理加速，同时保持结构保真度和视觉一致性。

Conclusion: ToPi为DiT中的上下文生成提供了一种有效的无训练令牌剪枝解决方案，解决了序列长度增加带来的计算瓶颈问题，同时保持了生成质量。

Abstract: In-context generation significantly enhances Diffusion Transformers (DiTs) by enabling controllable image-to-image generation through reference examples. However, the resulting input concatenation drastically increases sequence length, creating a substantial computational bottleneck. Existing token reduction techniques, primarily tailored for text-to-image synthesis, fall short in this paradigm as they apply uniform reduction strategies, overlooking the inherent role asymmetry between reference contexts and target latents across spatial, temporal, and functional dimensions. To bridge this gap, we introduce ToPi, a training-free token pruning framework tailored for in-context generation in DiTs. Specifically, ToPi utilizes offline calibration-driven sensitivity analysis to identify pivotal attention layers, serving as a robust proxy for redundancy estimation. Leveraging these layers, we derive a novel influence metric to quantify the contribution of each context token for selective pruning, coupled with a temporal update strategy that adapts to the evolving diffusion trajectory. Empirical evaluations demonstrate that ToPi can achieve over 30\% speedup in inference while maintaining structural fidelity and visual consistency across complex image generation tasks.

</details>


### [250] [Research on World Models Is Not Merely Injecting World Knowledge into Specific Tasks](https://arxiv.org/abs/2602.01630)
*Bohan Zeng,Kaixin Zhu,Daili Hua,Bozhou Li,Chengzhuo Tong,Yuran Wang,Xinyi Huang,Yifan Dai,Zixiang Zhang,Yifan Yang,Zhou Liu,Hao Liang,Xiaochen Ma,Ruichuan An,Tianyi Bai,Hongcheng Gao,Junbo Niu,Yang Shi,Xinlong Chen,Yue Ding,Minglei Shi,Kai Zeng,Yiwen Tang,Yuanxing Zhang,Pengfei Wan,Xintao Wang,Wentao Zhang*

Main category: cs.CV

Relevance: 75.0

TL;DR: 该论文分析了当前世界模型研究的碎片化现状，提出了一个统一的设计规范，强调世界模型应整合交互、感知、符号推理和空间表示，为未来研究提供结构化指导。


<details>
  <summary>Details</summary>
Motivation: 当前世界模型研究过于碎片化，主要集中在将世界知识注入到孤立任务中（如视觉预测、3D估计等），缺乏系统性框架。这些任务特定的集成虽然能提升性能，但缺乏整体世界理解所需的系统性一致性。

Method: 分析现有碎片化方法的局限性，提出统一的世界模型设计规范。强调世界模型不应是松散的能力集合，而应是整合交互、感知、符号推理和空间表示的规范性框架。

Result: 提出了一个结构化的世界模型设计规范，为未来研究提供指导方向，旨在推动更通用、鲁棒和原则性的世界模型发展。

Conclusion: 需要建立统一的世界模型框架，整合多个关键能力，而不是局限于任务特定的知识注入。这有助于实现更全面的世界理解和智能体能力。

Abstract: World models have emerged as a critical frontier in AI research, aiming to enhance large models by infusing them with physical dynamics and world knowledge. The core objective is to enable agents to understand, predict, and interact with complex environments. However, current research landscape remains fragmented, with approaches predominantly focused on injecting world knowledge into isolated tasks, such as visual prediction, 3D estimation, or symbol grounding, rather than establishing a unified definition or framework. While these task-specific integrations yield performance gains, they often lack the systematic coherence required for holistic world understanding. In this paper, we analyze the limitations of such fragmented approaches and propose a unified design specification for world models. We suggest that a robust world model should not be a loose collection of capabilities but a normative framework that integrally incorporates interaction, perception, symbolic reasoning, and spatial representation. This work aims to provide a structured perspective to guide future research toward more general, robust, and principled models of the world.

</details>


### [251] [SMTrack: State-Aware Mamba for Efficient Temporal Modeling in Visual Tracking](https://arxiv.org/abs/2602.01677)
*Yinchao Ma,Dengqing Yang,Zhangyu He,Wenfei Yang,Tianzhu Zhang*

Main category: cs.CV

Relevance: 75.0

TL;DR: SMTrack提出了一种基于状态空间模型的新型视觉跟踪方法，通过选择性状态感知空间模型和隐藏状态传播机制，以线性计算复杂度实现长程时序依赖建模，在保持低计算成本的同时获得优异的跟踪性能。


<details>
  <summary>Details</summary>
Motivation: 传统CNN和Transformer架构在视觉跟踪中建模长程时序依赖存在固有局限性，通常需要复杂的定制模块或高昂计算成本。状态空间模型（如Mamba）的成功为时序建模提供了新思路，作者希望将其应用于视觉跟踪领域。

Method: 提出State-aware Mamba Tracker (SMTrack)：1）选择性状态感知空间模型，具有状态相关参数以捕捉更多样化的时序线索；2）线性计算复杂度的长程时序交互；3）通过隐藏状态传播和更新实现帧间交互，降低跟踪时的计算成本。

Result: 大量实验结果表明，SMTrack在保持低计算成本的同时实现了有前景的性能表现，验证了状态空间模型在视觉跟踪任务中的有效性。

Conclusion: SMTrack为视觉跟踪提供了一种简洁高效的时序建模范式，无需复杂定制模块即可构建长程时序依赖，在计算效率和性能之间取得了良好平衡。

Abstract: Visual tracking aims to automatically estimate the state of a target object in a video sequence, which is challenging especially in dynamic scenarios. Thus, numerous methods are proposed to introduce temporal cues to enhance tracking robustness. However, conventional CNN and Transformer architectures exhibit inherent limitations in modeling long-range temporal dependencies in visual tracking, often necessitating either complex customized modules or substantial computational costs to integrate temporal cues. Inspired by the success of the state space model, we propose a novel temporal modeling paradigm for visual tracking, termed State-aware Mamba Tracker (SMTrack), providing a neat pipeline for training and tracking without needing customized modules or substantial computational costs to build long-range temporal dependencies. It enjoys several merits. First, we propose a novel selective state-aware space model with state-wise parameters to capture more diverse temporal cues for robust tracking. Second, SMTrack facilitates long-range temporal interactions with linear computational complexity during training. Third, SMTrack enables each frame to interact with previously tracked frames via hidden state propagation and updating, which releases computational costs of handling temporal cues during tracking. Extensive experimental results demonstrate that SMTrack achieves promising performance with low computational costs.

</details>


### [252] [Simplicity Prevails: The Emergence of Generalizable AIGI Detection in Visual Foundation Models](https://arxiv.org/abs/2602.01738)
*Yue Zhou,Xinan He,Kaiqing Lin,Bing Fan,Feng Ding,Bin Li*

Main category: cs.CV

Relevance: 75.0

TL;DR: 基于现代视觉基础模型冻结特征的简单线性分类器，在AI生成图像检测任务中超越了复杂的专用检测器，特别是在真实场景中表现优异，准确率提升超过30%。


<details>
  <summary>Details</summary>
Motivation: 当前AI生成图像检测器在精心设计的基准测试中表现优异，但在真实场景中性能急剧下降。研究者希望探索更简单有效的方法来解决这一现实世界应用问题。

Method: 使用现代视觉基础模型（如Perception Encoder、MetaCLIP 2、DINOv3）的冻结特征，仅训练一个简单的线性分类器来进行AI生成图像检测。

Result: 该方法在传统基准测试上与专用检测器相当，在真实场景数据集上显著优于专用检测器，准确率提升超过30%。研究发现这种能力源于预训练数据中包含合成内容的大规模暴露。

Conclusion: 建议AI取证领域应从过度拟合静态基准转向利用基础模型不断演进的世界知识，以实现真实世界的可靠性。该方法虽然强大，但在重捕获、传输、VAE重建和局部编辑等情况下仍有局限性。

Abstract: While specialized detectors for AI-Generated Images (AIGI) achieve near-perfect accuracy on curated benchmarks, they suffer from a dramatic performance collapse in realistic, in-the-wild scenarios. In this work, we demonstrate that simplicity prevails over complex architectural designs. A simple linear classifier trained on the frozen features of modern Vision Foundation Models , including Perception Encoder, MetaCLIP 2, and DINOv3, establishes a new state-of-the-art. Through a comprehensive evaluation spanning traditional benchmarks, unseen generators, and challenging in-the-wild distributions, we show that this baseline not only matches specialized detectors on standard benchmarks but also decisively outperforms them on in-the-wild datasets, boosting accuracy by striking margins of over 30\%. We posit that this superior capability is an emergent property driven by the massive scale of pre-training data containing synthetic content. We trace the source of this capability to two distinct manifestations of data exposure: Vision-Language Models internalize an explicit semantic concept of forgery, while Self-Supervised Learning models implicitly acquire discriminative forensic features from the pretraining data. However, we also reveal persistent limitations: these models suffer from performance degradation under recapture and transmission, remain blind to VAE reconstruction and localized editing. We conclude by advocating for a paradigm shift in AI forensics, moving from overfitting on static benchmarks to harnessing the evolving world knowledge of foundation models for real-world reliability.

</details>


### [253] [Mind-Brush: Integrating Agentic Cognitive Search and Reasoning into Image Generation](https://arxiv.org/abs/2602.01756)
*Jun He,Junyan Ye,Zilong Huang,Dongzhi Jiang,Chenjue Zhang,Leqi Zhu,Renrui Zhang,Xiang Zhang,Weijia Li*

Main category: cs.CV

Relevance: 75.0

TL;DR: Mind-Brush是一个统一的代理框架，将图像生成转化为动态的知识驱动工作流，通过"思考-研究-创造"范式主动检索多模态证据并进行推理，以解决复杂知识推理和实时世界适应性问题。


<details>
  <summary>Details</summary>
Motivation: 现有文本到图像生成模型主要作为静态的文本到像素解码器，难以理解用户的隐含意图。虽然统一的理解-生成模型有所改进，但仍无法处理复杂知识推理任务，且受限于静态内部先验，无法适应现实世界的动态变化。

Method: 提出Mind-Brush框架，模拟人类"思考-研究-创造"范式：1) 主动检索多模态证据来锚定分布外概念；2) 使用推理工具解决隐含视觉约束；3) 将生成转化为动态的知识驱动工作流。

Result: 在提出的Mind-Bench基准测试（500个样本涵盖实时新闻、新兴概念、数学和地理推理等领域）上，Mind-Brush显著提升了统一模型的能力，使Qwen-Image基线实现了零到一的飞跃，同时在WISE和RISE等现有基准测试上也取得了优异结果。

Conclusion: Mind-Brush通过将生成任务转化为动态的知识驱动工作流，有效解决了现有模型在复杂知识推理和实时世界适应性方面的局限性，为统一理解-生成模型提供了新的能力提升路径。

Abstract: While text-to-image generation has achieved unprecedented fidelity, the vast majority of existing models function fundamentally as static text-to-pixel decoders. Consequently, they often fail to grasp implicit user intentions. Although emerging unified understanding-generation models have improved intent comprehension, they still struggle to accomplish tasks involving complex knowledge reasoning within a single model. Moreover, constrained by static internal priors, these models remain unable to adapt to the evolving dynamics of the real world. To bridge these gaps, we introduce Mind-Brush, a unified agentic framework that transforms generation into a dynamic, knowledge-driven workflow. Simulating a human-like 'think-research-create' paradigm, Mind-Brush actively retrieves multimodal evidence to ground out-of-distribution concepts and employs reasoning tools to resolve implicit visual constraints. To rigorously evaluate these capabilities, we propose Mind-Bench, a comprehensive benchmark comprising 500 distinct samples spanning real-time news, emerging concepts, and domains such as mathematical and Geo-Reasoning. Extensive experiments demonstrate that Mind-Brush significantly enhances the capabilities of unified models, realizing a zero-to-one capability leap for the Qwen-Image baseline on Mind-Bench, while achieving superior results on established benchmarks like WISE and RISE.

</details>


### [254] [DDP-WM: Disentangled Dynamics Prediction for Efficient World Models](https://arxiv.org/abs/2602.01780)
*Shicheng Yin,Kaixuan Yin,Weixing Chen,Yang Liu,Guanbin Li,Liang Lin*

Main category: cs.CV

Relevance: 75.0

TL;DR: DDP-WM是一种新型世界模型，采用解耦动力学预测原则，通过分离主要物理动力学和次要背景更新，显著提升机器人规划效率，在Push-T任务上实现9倍推理加速和MPC成功率从90%提升到98%。


<details>
  <summary>Details</summary>
Motivation: 现有基于密集Transformer的世界模型计算开销大，阻碍实时部署。作者假设潜在状态演化是异质的，可分解为物理驱动的主要动力学和背景驱动的次要更新，需要解决效率-性能瓶颈。

Method: 提出DDP-WM架构，集成高效历史处理与动态定位来隔离主要动力学，使用交叉注意力机制进行背景更新，优化资源分配并为规划器提供平滑优化景观。

Result: 在导航、精确桌面操作、复杂可变形或多体交互等任务上实现显著效率和性能提升。在Push-T任务上，相比最先进密集模型，实现约9倍推理加速，MPC成功率从90%提升到98%。

Conclusion: DDP-WM为开发高效、高保真世界模型提供了有前景的路径，通过解耦动力学预测解决了现有模型的效率瓶颈。

Abstract: World models are essential for autonomous robotic planning. However, the substantial computational overhead of existing dense Transformerbased models significantly hinders real-time deployment. To address this efficiency-performance bottleneck, we introduce DDP-WM, a novel world model centered on the principle of Disentangled Dynamics Prediction (DDP). We hypothesize that latent state evolution in observed scenes is heterogeneous and can be decomposed into sparse primary dynamics driven by physical interactions and secondary context-driven background updates. DDP-WM realizes this decomposition through an architecture that integrates efficient historical processing with dynamic localization to isolate primary dynamics. By employing a crossattention mechanism for background updates, the framework optimizes resource allocation and provides a smooth optimization landscape for planners. Extensive experiments demonstrate that DDP-WM achieves significant efficiency and performance across diverse tasks, including navigation, precise tabletop manipulation, and complex deformable or multi-body interactions. Specifically, on the challenging Push-T task, DDP-WM achieves an approximately 9 times inference speedup and improves the MPC success rate from 90% to98% compared to state-of-the-art dense models. The results establish a promising path for developing efficient, high-fidelity world models. Codes will be available at https://github.com/HCPLabSYSU/DDP-WM.

</details>


### [255] [Fast Autoregressive Video Diffusion and World Models with Temporal Cache Compression and Sparse Attention](https://arxiv.org/abs/2602.01801)
*Dvir Samuel,Issar Tzachor,Matan Levy,Micahel Green,Gal Chechik,Rami Ben-Ari*

Main category: cs.CV

Relevance: 75.0

TL;DR: 本文提出了一种针对自回归视频扩散模型的训练免费注意力框架，通过时间缓存压缩、近似最近邻交叉注意力和自注意力稀疏化，显著减少了计算和内存开销，实现了5-10倍的端到端加速，同时保持视觉质量稳定。


<details>
  <summary>Details</summary>
Motivation: 自回归视频扩散模型在推理时面临注意力层的瓶颈问题：随着生成过程进行，KV缓存不断增长，导致延迟增加和GPU内存占用上升，限制了时间上下文的使用并损害了长范围一致性。

Method: 提出了统一的训练免费注意力框架：1) TempCache通过时间对应关系压缩KV缓存；2) AnnCA使用近似最近邻匹配选择帧相关提示词加速交叉注意力；3) AnnSA通过限制每个查询到语义匹配的键来稀疏化自注意力，同样使用轻量级ANN。

Result: 实验显示实现了5-10倍的端到端加速，同时保持了近乎相同的视觉质量，并在长序列生成中保持了稳定的吞吐量和近乎恒定的峰值GPU内存使用，而先前方法会逐渐变慢且内存使用不断增加。

Conclusion: 该方法有效解决了自回归视频扩散模型中的注意力冗余问题，显著提升了推理效率，同时与现有的自回归扩散骨干网络和世界模型兼容，为长序列视频生成提供了实用的解决方案。

Abstract: Autoregressive video diffusion models enable streaming generation, opening the door to long-form synthesis, video world models, and interactive neural game engines. However, their core attention layers become a major bottleneck at inference time: as generation progresses, the KV cache grows, causing both increasing latency and escalating GPU memory, which in turn restricts usable temporal context and harms long-range consistency. In this work, we study redundancy in autoregressive video diffusion and identify three persistent sources: near-duplicate cached keys across frames, slowly evolving (largely semantic) queries/keys that make many attention computations redundant, and cross-attention over long prompts where only a small subset of tokens matters per frame. Building on these observations, we propose a unified, training-free attention framework for autoregressive diffusion: TempCache compresses the KV cache via temporal correspondence to bound cache growth; AnnCA accelerates cross-attention by selecting frame-relevant prompt tokens using fast approximate nearest neighbor (ANN) matching; and AnnSA sparsifies self-attention by restricting each query to semantically matched keys, also using a lightweight ANN. Together, these modules reduce attention, compute, and memory and are compatible with existing autoregressive diffusion backbones and world models. Experiments demonstrate up to x5--x10 end-to-end speedups while preserving near-identical visual quality and, crucially, maintaining stable throughput and nearly constant peak GPU memory usage over long rollouts, where prior methods progressively slow down and suffer from increasing memory usage.

</details>


### [256] [Seeing Is Believing? A Benchmark for Multimodal Large Language Models on Visual Illusions and Anomalies](https://arxiv.org/abs/2602.01816)
*Wenjin Hou,Wei Liu,Han Hu,Xiaoxiao Sun,Serena Yeung-Levy,Hehe Fan*

Main category: cs.CV

Relevance: 75.0

TL;DR: VIA-Bench是一个评估多模态大语言模型在视觉错觉和异常场景下鲁棒性的基准测试，包含6类视觉错觉，超过1000个高质量问答对，测试发现现有MLLMs存在显著脆弱性，CoT推理效果有限。


<details>
  <summary>Details</summary>
Motivation: 现有MLLMs评估主要基于标准分布内数据，忽视了模型在面对违背常识先验的视觉错觉和异常场景时的鲁棒性。需要填补这一空白，探究模型在挑战性视觉感知任务上的真实能力。

Method: 构建VIA-Bench基准，包含6个核心类别：颜色错觉、运动错觉、格式塔错觉、几何空间错觉、一般视觉错觉和视觉异常。通过人工循环审查构建超过1000个高质量问答对，要求细致的视觉推理。评估了20多个最先进的MLLMs（包括专有、开源和推理增强模型）。

Result: 评估发现MLLMs存在显著脆弱性，Chain-of-Thought推理对鲁棒性提升有限，经常产生"脆弱幻象"——模型逻辑在错觉刺激下崩溃。揭示了机器与人类感知的根本差异。

Conclusion: 解决这种感知瓶颈对于人工通用智能的发展至关重要。视觉错觉和异常场景是评估MLLMs鲁棒性的重要测试场，现有模型在这方面存在明显不足。

Abstract: Multimodal Large Language Models (MLLMs) have shown remarkable proficiency on general-purpose vision-language benchmarks, reaching or even exceeding human-level performance. However, these evaluations typically rely on standard in-distribution data, leaving the robustness of MLLMs largely unexamined when faced with scenarios that defy common-sense priors. To address this gap, we introduce VIA-Bench, a challenging benchmark designed to probe model performance on visual illusions and anomalies. It includes six core categories: color illusions, motion illusions, gestalt illusions, geometric and spatial illusions, general visual illusions, and visual anomalies. Through careful human-in-the-loop review, we construct over 1K high-quality question-answer pairs that require nuanced visual reasoning. Extensive evaluation of over 20 state-of-the-art MLLMs, including proprietary, open-source, and reasoning-enhanced models, uncovers significant vulnerabilities. Notably, we find that Chain-of-Thought (CoT) reasoning offers negligible robustness, often yielding ``brittle mirages'' where the model's logic collapses under illusory stimuli. Our findings reveal a fundamental divergence between machine and human perception, suggesting that resolving such perceptual bottlenecks is critical for the advancement of artificial general intelligence. The benchmark data and code will be released.

</details>


### [257] [Learning Sparse Visual Representations via Spatial-Semantic Factorization](https://arxiv.org/abs/2602.01905)
*Theodore Zhengde Zhao,Sid Kiblawi,Jianwei Yang,Naoto Usuyama,Reuben Tan,Noel C Codella,Tristan Naumann,Hoifung Poon,Mu Wei*

Main category: cs.CV

Relevance: 75.0

TL;DR: STELLAR通过将视觉特征分解为语义概念与其空间分布的乘积，解决了自监督学习中语义理解与图像重建之间的冲突，使用稀疏token同时支持高质量重建和语义性能。


<details>
  <summary>Details</summary>
Motivation: 自监督学习面临语义理解与图像重建之间的根本冲突：高层语义SSL（如DINO）依赖全局token但丢弃空间坐标，而生成式SSL（如MAE）保留密集特征网格但无法产生高层抽象。

Method: 将视觉特征分解为语义概念和其空间分布的低秩乘积，在语义token上执行DINO风格的数据增强对齐，同时在定位矩阵中保持精确的空间映射以支持像素级重建。

Result: 仅需16个稀疏token即可同时支持高质量重建（2.60 FID）并匹配密集骨干网络的语义性能（79.10% ImageNet准确率）。

Conclusion: STELLAR作为一种多功能稀疏表示，通过战略性地分离语义身份与空间几何，弥合了判别式与生成式视觉之间的差距。

Abstract: Self-supervised learning (SSL) faces a fundamental conflict between semantic understanding and image reconstruction. High-level semantic SSL (e.g., DINO) relies on global tokens that are forced to be location-invariant for augmentation alignment, a process that inherently discards the spatial coordinates required for reconstruction. Conversely, generative SSL (e.g., MAE) preserves dense feature grids for reconstruction but fails to produce high-level abstractions. We introduce STELLAR, a framework that resolves this tension by factorizing visual features into a low-rank product of semantic concepts and their spatial distributions. This disentanglement allows us to perform DINO-style augmentation alignment on the semantic tokens while maintaining the precise spatial mapping in the localization matrix necessary for pixel-level reconstruction. We demonstrate that as few as 16 sparse tokens under this factorized form are sufficient to simultaneously support high-quality reconstruction (2.60 FID) and match the semantic performance of dense backbones (79.10% ImageNet accuracy). Our results highlight STELLAR as a versatile sparse representation that bridges the gap between discriminative and generative vision by strategically separating semantic identity from spatial geometry. Code available at https://aka.ms/stellar.

</details>


### [258] [Enhancing Multi-Image Understanding through Delimiter Token Scaling](https://arxiv.org/abs/2602.01984)
*Minyoung Lee,Yeji Park,Dongjun Hwang,Yejin Kim,Seong Joon Oh,Junsuk Choe*

Main category: cs.CV

Relevance: 75.0

TL;DR: 提出一种通过缩放分隔符token的隐藏状态来增强多图像理解能力的方法，解决现有LVLM中跨图像信息泄漏问题，无需额外训练或推理成本


<details>
  <summary>Details</summary>
Motivation: 大型视觉语言模型在单图像任务上表现良好，但在多图像输入时性能下降。主要原因是跨图像信息泄漏，模型难以区分不同图像的信息。现有模型虽然使用分隔符标记图像边界，但这些标记未能有效阻止跨图像信息泄漏。

Method: 提出一种缩放分隔符token隐藏状态的方法。通过增强分隔符的表示，强化图像内部交互，限制不必要的跨图像交互，从而更好地保留图像特定信息，帮助模型区分不同图像并进行更准确的推理。

Result: 在多图像基准测试（Mantis、MuirBench、MIRB、QBench2）上取得性能提升。在需要清晰区分的纯文本任务上也有改进，包括多文档和多表格理解基准（TQABench、MultiNews、WCEP-10）。方法无需额外训练或推理成本。

Conclusion: 通过缩放分隔符token的隐藏状态可以有效解决跨图像信息泄漏问题，提升多图像和多模态任务性能，且方法简单高效，无需额外计算开销。

Abstract: Large Vision-Language Models (LVLMs) achieve strong performance on single-image tasks, but their performance declines when multiple images are provided as input. One major reason is the cross-image information leakage, where the model struggles to distinguish information across different images. Existing LVLMs already employ delimiter tokens to mark the start and end of each image, yet our analysis reveals that these tokens fail to effectively block cross-image information leakage. To enhance their effectiveness, we propose a method that scales the hidden states of delimiter tokens. This enhances the model's ability to preserve image-specific information by reinforcing intra-image interaction and limiting undesired cross-image interactions. Consequently, the model is better able to distinguish between images and reason over them more accurately. Experiments show performance gains on multi-image benchmarks such as Mantis, MuirBench, MIRB, and QBench2. We further evaluate our method on text-only tasks that require clear distinction. The method improves performance on multi-document and multi-table understanding benchmarks, including TQABench, MultiNews, and WCEP-10. Notably, our method requires no additional training or inference cost.

</details>


### [259] [UniDriveDreamer: A Single-Stage Multimodal World Model for Autonomous Driving](https://arxiv.org/abs/2602.02002)
*Guosheng Zhao,Yaozeng Wang,Xiaofeng Wang,Zheng Zhu,Tingdong Yu,Guan Huang,Yongchen Zai,Ji Jiao,Changliang Xue,Xiaole Wang,Zhen Yang,Futang Zhu,Xingang Wang*

Main category: cs.CV

Relevance: 75.0

TL;DR: UniDriveDreamer：用于自动驾驶的单阶段统一多模态世界模型，直接生成多模态未来观测（多摄像头视频和LiDAR序列），无需中间表示或级联模块。


<details>
  <summary>Details</summary>
Motivation: 现有自动驾驶世界模型主要专注于单模态生成（要么是多摄像头视频，要么是LiDAR序列），缺乏统一的多模态生成方法。需要一种能够直接生成多模态未来观测的模型，以更好地支持自动驾驶决策。

Method: 1) 使用LiDAR专用VAE编码LiDAR序列和视频VAE编码多摄像头图像；2) 提出统一潜在锚定(ULA)来显式对齐两种模态的潜在分布；3) 使用扩散transformer联合建模几何对应和时间演化；4) 将结构化场景布局信息作为条件信号指导生成。

Result: 在视频和LiDAR生成方面均优于先前最先进方法，并在下游任务中带来可测量的改进。

Conclusion: UniDriveDreamer成功实现了单阶段统一多模态世界模型，能够有效生成自动驾驶所需的多模态未来观测，为自动驾驶系统提供更全面的环境理解。

Abstract: World models have demonstrated significant promise for data synthesis in autonomous driving. However, existing methods predominantly concentrate on single-modality generation, typically focusing on either multi-camera video or LiDAR sequence synthesis. In this paper, we propose UniDriveDreamer, a single-stage unified multimodal world model for autonomous driving, which directly generates multimodal future observations without relying on intermediate representations or cascaded modules. Our framework introduces a LiDAR-specific variational autoencoder (VAE) designed to encode input LiDAR sequences, alongside a video VAE for multi-camera images. To ensure cross-modal compatibility and training stability, we propose Unified Latent Anchoring (ULA), which explicitly aligns the latent distributions of the two modalities. The aligned features are fused and processed by a diffusion transformer that jointly models their geometric correspondence and temporal evolution. Additionally, structured scene layout information is projected per modality as a conditioning signal to guide the synthesis. Extensive experiments demonstrate that UniDriveDreamer outperforms previous state-of-the-art methods in both video and LiDAR generation, while also yielding measurable improvements in downstream

</details>


### [260] [Auto-Comp: An Automated Pipeline for Scalable Compositional Probing of Contrastive Vision-Language Models](https://arxiv.org/abs/2602.02043)
*Cristian Sbrolli,Matteo Matteucci,Toshihiko Yamasaki*

Main category: cs.CV

Relevance: 75.0

TL;DR: Auto-Comp：一个自动化合成基准生成框架，用于细粒度评估视觉语言模型的组合推理能力，揭示了VLMs在颜色绑定和空间关系上的普遍失败模式


<details>
  <summary>Details</summary>
Motivation: 现代视觉语言模型在组合推理中存在关键缺陷，经常混淆"红色立方体和蓝色球体"与"蓝色立方体和红色球体"。当前缺乏能够解耦视觉和语言根源的细粒度、可控评估方法，难以深入分析这些失败的根本原因。

Method: 提出Auto-Comp全自动化合成流水线，生成可扩展的基准测试。通过生成最小化描述和LLM生成的上下文描述配对图像，实现受控A/B测试，分离核心绑定能力与视觉语言复杂性。特别设计了"混淆基准"来测试模型对低熵干扰项的敏感性。

Result: 在20个VLM上评估颜色绑定和空间关系任务，发现CLIP和SigLIP模型家族都存在普遍的组合推理失败。模型不仅容易混淆简单属性交换，还对低熵干扰项（如重复对象或颜色）高度敏感。发现了一个有趣的权衡：视觉语言上下文虽然有助于空间推理，但会因引入视觉杂乱而阻碍局部属性绑定。

Conclusion: Auto-Comp框架为VLM组合推理评估提供了可控、细粒度的分析工具，揭示了VLMs在组合推理中的深层缺陷超越了已知的词袋模型限制。该框架有助于未来基准测试的创建和模型能力的深入理解。

Abstract: Modern Vision-Language Models (VLMs) exhibit a critical flaw in compositional reasoning, often confusing "a red cube and a blue sphere" with "a blue cube and a red sphere". Disentangling the visual and linguistic roots of these failures is a fundamental challenge for robust evaluation. To enable fine-grained, controllable analysis, we introduce Auto-Comp, a fully automated and synthetic pipeline for generating scalable benchmarks. Its controllable nature is key to dissecting and isolating different reasoning skills. Auto-Comp generates paired images from Minimal (e.g., "a monitor to the left of a bicycle on a white background") and LLM-generated Contextual captions (e.g., "In a brightly lit photography studio, a monitor is positioned to the left of a bicycle"), allowing a controlled A/B test to disentangle core binding ability from visio-linguistic complexity. Our evaluation of 20 VLMs on novel benchmarks for color binding and spatial relations reveals universal compositional failures in both CLIP and SigLIP model families. Crucially, our novel "Confusion Benchmark" reveals a deeper flaw beyond simple attribute swaps: models are highly susceptible to low-entropy distractors (e.g., repeated objects or colors), demonstrating their compositional failures extend beyond known bag-of-words limitations. we uncover a surprising trade-off: visio-linguistic context, which provides global scene cues, aids spatial reasoning but simultaneously hinders local attribute binding by introducing visual clutter. We release the Auto-Comp pipeline to facilitate future benchmark creation, alongside all our generated benchmarks (https://huggingface.co/AutoComp).

</details>


### [261] [MAIN-VLA: Modeling Abstraction of Intention and eNvironment for Vision-Language-Action Models](https://arxiv.org/abs/2602.02212)
*Zheyuan Zhou,Liang Du,Zixun Sun,Xiaoyu Zhou,Ruimin Ye,Qihao Chen,Yinda Chen,Lemiao Qiu*

Main category: cs.CV

Relevance: 75.0

TL;DR: MAIN-VLA框架通过意图抽象和环境语义抽象，在复杂动态环境中实现深度语义对齐的决策，显著提升视觉-语言-动作模型的效率和性能。


<details>
  <summary>Details</summary>
Motivation: 现有视觉-语言-动作方法在复杂动态环境（如3D开放世界、大型PvP游戏）中，难以从冗余传感器流中提取动作关键信号，决策效率低下。

Method: 提出意图抽象（IA）将冗长语言指令压缩为显式语义原语，环境语义抽象（ESA）将视觉流投影为结构化拓扑可供性表示，两者对齐产生注意力集中效应，实现参数无关的令牌剪枝。

Result: 在Minecraft开放世界和大型PvP环境（Game for Peace、Valorant）中实现最先进性能，决策质量、泛化能力和推理效率均显著提升。

Conclusion: 通过显式建模意图和环境抽象实现深度语义对齐，能有效解决复杂动态环境中的感知冗余问题，提升视觉-语言-动作模型的决策效率和性能。

Abstract: Despite significant progress in Visual-Language-Action (VLA), in highly complex and dynamic environments that involve real-time unpredictable interactions (such as 3D open worlds and large-scale PvP games), existing approaches remain inefficient at extracting action-critical signals from redundant sensor streams. To tackle this, we introduce MAIN-VLA, a framework that explicitly Models the Abstraction of Intention and eNvironment to ground decision-making in deep semantic alignment rather than superficial pattern matching. Specifically, our Intention Abstraction (IA) extracts verbose linguistic instructions and their associated reasoning into compact, explicit semantic primitives, while the Environment Semantics Abstraction (ESA) projects overwhelming visual streams into a structured, topological affordance representation. Furthermore, aligning these two abstract modalities induces an emergent attention-concentration effect, enabling a parameter-free token-pruning strategy that filters out perceptual redundancy without degrading performance. Extensive experiments in open-world Minecraft and large-scale PvP environments (Game for Peace and Valorant) demonstrate that MAIN-VLA sets a new state-of-the-art, which achieves superior decision quality, stronger generalization, and cutting-edge inference efficiency.

</details>


### [262] [Causal Forcing: Autoregressive Diffusion Distillation Done Right for High-Quality Real-Time Interactive Video Generation](https://arxiv.org/abs/2602.02214)
*Hongzhou Zhu,Min Zhao,Guande He,Hang Su,Chongxuan Li,Jun Zhu*

Main category: cs.CV

Relevance: 75.0

TL;DR: 本文提出Causal Forcing方法，通过使用自回归教师模型进行ODE初始化，解决双向视频扩散模型蒸馏到少步自回归模型时的架构差距问题，显著提升实时交互视频生成性能。


<details>
  <summary>Details</summary>
Motivation: 当前实时交互视频生成方法将预训练的双向视频扩散模型蒸馏为少步自回归模型，但存在架构差距问题。现有方法使用ODE蒸馏初始化自回归学生模型，这需要帧级单射性条件，而双向教师蒸馏违反了这一条件，导致性能下降。

Method: 提出Causal Forcing方法，使用自回归教师模型进行ODE初始化，从而桥接架构差距。该方法避免了双向教师蒸馏违反帧级单射性条件的问题，能够恢复教师模型的流映射。

Result: 实验结果表明，该方法在所有指标上均优于所有基线，在Dynamic Degree指标上超越SOTA Self Forcing 19.3%，在VisionReward指标上超越8.7%，在Instruction Following指标上超越16.7%。

Conclusion: Causal Forcing通过使用自回归教师进行ODE初始化，有效解决了双向视频扩散模型蒸馏到自回归模型的架构差距问题，显著提升了实时交互视频生成的性能。

Abstract: To achieve real-time interactive video generation, current methods distill pretrained bidirectional video diffusion models into few-step autoregressive (AR) models, facing an architectural gap when full attention is replaced by causal attention. However, existing approaches do not bridge this gap theoretically. They initialize the AR student via ODE distillation, which requires frame-level injectivity, where each noisy frame must map to a unique clean frame under the PF-ODE of an AR teacher. Distilling an AR student from a bidirectional teacher violates this condition, preventing recovery of the teacher's flow map and instead inducing a conditional-expectation solution, which degrades performance. To address this issue, we propose Causal Forcing that uses an AR teacher for ODE initialization, thereby bridging the architectural gap. Empirical results show that our method outperforms all baselines across all metrics, surpassing the SOTA Self Forcing by 19.3\% in Dynamic Degree, 8.7\% in VisionReward, and 16.7\% in Instruction Following. Project page and the code: \href{https://thu-ml.github.io/CausalForcing.github.io/}{https://thu-ml.github.io/CausalForcing.github.io/}

</details>


### [263] [Show, Don't Tell: Morphing Latent Reasoning into Image Generation](https://arxiv.org/abs/2602.02227)
*Harold Haodong Chen,Xinxiang Yin,Wen-Jie Shu,Hongfei Zhang,Zixin Zhang,Chenfei Liao,Litao Guo,Qifeng Chen,Ying-Cong Chen*

Main category: cs.CV

Relevance: 75.0

TL;DR: LatentMorph是一个在连续潜在空间中进行隐式推理的文本到图像生成框架，通过四个轻量级组件实现自适应自我优化，显著提升生成质量和推理效率。


<details>
  <summary>Details</summary>
Motivation: 当前文本到图像生成方法缺乏动态推理和优化能力，而现有的显式推理范式存在效率低下、信息丢失和认知不匹配等问题。需要一种在连续潜在空间中进行隐式推理的方法来模拟人类创造过程。

Method: 提出LatentMorph框架，包含四个核心组件：1) condenser将中间生成状态压缩为视觉记忆；2) translator将潜在思想转换为可操作指导；3) shaper动态引导下一个图像token预测；4) RL训练的invoker自适应决定何时触发推理。所有推理都在连续潜在空间中进行。

Result: 1) 在Janus-Pro基础上提升16% (GenEval)和25% (T2I-CompBench)；2) 在抽象推理任务(WISE, IPV-Txt)上优于显式范式15%和11%；3) 推理时间减少44%，token消耗减少51%；4) 与人类直觉的认知对齐度达71%。

Conclusion: LatentMorph通过在连续潜在空间中进行隐式推理，有效解决了显式推理范式的效率瓶颈，实现了更自适应、高效的文本到图像生成，同时与人类认知过程更加一致。

Abstract: Text-to-image (T2I) generation has achieved remarkable progress, yet existing methods often lack the ability to dynamically reason and refine during generation--a hallmark of human creativity. Current reasoning-augmented paradigms most rely on explicit thought processes, where intermediate reasoning is decoded into discrete text at fixed steps with frequent image decoding and re-encoding, leading to inefficiencies, information loss, and cognitive mismatches. To bridge this gap, we introduce LatentMorph, a novel framework that seamlessly integrates implicit latent reasoning into the T2I generation process. At its core, LatentMorph introduces four lightweight components: (i) a condenser for summarizing intermediate generation states into compact visual memory, (ii) a translator for converting latent thoughts into actionable guidance, (iii) a shaper for dynamically steering next image token predictions, and (iv) an RL-trained invoker for adaptively determining when to invoke reasoning. By performing reasoning entirely in continuous latent spaces, LatentMorph avoids the bottlenecks of explicit reasoning and enables more adaptive self-refinement. Extensive experiments demonstrate that LatentMorph (I) enhances the base model Janus-Pro by $16\%$ on GenEval and $25\%$ on T2I-CompBench; (II) outperforms explicit paradigms (e.g., TwiG) by $15\%$ and $11\%$ on abstract reasoning tasks like WISE and IPV-Txt, (III) while reducing inference time by $44\%$ and token consumption by $51\%$; and (IV) exhibits $71\%$ cognitive alignment with human intuition on reasoning invocation.

</details>


### [264] [LongVPO: From Anchored Cues to Self-Reasoning for Long-Form Video Preference Optimization](https://arxiv.org/abs/2602.02341)
*Zhenpeng Huang,Jiaqi Li,Zihan Jia,Xinhao Li,Desen Meng,Lingxue Song,Xi Chen,Liang Li,Limin Wang*

Main category: cs.CV

Relevance: 75.0

TL;DR: LongVPO是一个两阶段的直接偏好优化框架，使短上下文视觉语言模型能够理解超长视频，无需长视频标注。通过合成偏好三元组和递归字幕生成，仅用16K合成样本就在多个长视频基准上超越现有开源模型。


<details>
  <summary>Details</summary>
Motivation: 现有视觉语言模型主要针对短视频设计，缺乏对超长视频的理解能力。获取长视频标注成本高昂，且存在位置偏见等问题。需要一种无需长视频标注的扩展方法。

Method: 两阶段方法：第一阶段合成偏好三元组，通过锚定问题到短片段、插入干扰项、视觉相似性和问题特异性过滤来减轻位置偏见；第二阶段使用递归字幕生成场景级元数据，利用大语言模型创建多段推理查询和不受偏好的响应，通过多段推理任务对齐模型偏好。

Result: 仅用16K合成样本，无需人工标注，在多个长视频基准上超越最先进的开源模型，同时在短视频基准（如MVBench）上保持强大性能。

Conclusion: LongVPO提供了一种可扩展的范式，用于高效的长视频理解，通过合成数据和两阶段优化，解决了长视频标注稀缺和计算成本高的问题。

Abstract: We present LongVPO, a novel two-stage Direct Preference Optimization framework that enables short-context vision-language models to robustly understand ultra-long videos without any long-video annotations. In Stage 1, we synthesize preference triples by anchoring questions to individual short clips, interleaving them with distractors, and applying visual-similarity and question-specificity filtering to mitigate positional bias and ensure unambiguous supervision. We also approximate the reference model's scoring over long contexts by evaluating only the anchor clip, reducing computational overhead. In Stage 2, we employ a recursive captioning pipeline on long videos to generate scene-level metadata, then use a large language model to craft multi-segment reasoning queries and dispreferred responses, aligning the model's preferences through multi-segment reasoning tasks. With only 16K synthetic examples and no costly human labels, LongVPO outperforms the state-of-the-art open-source models on multiple long-video benchmarks, while maintaining strong short-video performance (e.g., on MVBench), offering a scalable paradigm for efficient long-form video understanding.

</details>


### [265] [UniReason 1.0: A Unified Reasoning Framework for World Knowledge Aligned Image Generation and Editing](https://arxiv.org/abs/2602.02437)
*Dianyi Wang,Chaofan Ma,Feng Han,Size Wu,Wei Song,Yibin Wang,Zhixiong Zhang,Tianhang Wang,Siyuan Wang,Zhongyu Wei,Jiaqi Wang*

Main category: cs.CV

Relevance: 75.0

TL;DR: UniReason是一个统一的多模态推理框架，通过双重推理范式将文本到图像生成和图像编辑整合起来，利用世界知识增强的规划进行生成，并通过自反思的编辑能力进行视觉细化，实现了类似人类认知的规划-精炼过程。


<details>
  <summary>Details</summary>
Motivation: 当前统一多模态模型在处理需要深度推理的复杂合成任务时存在困难，通常将文本到图像生成和图像编辑视为孤立的能力而非相互关联的推理步骤。为了解决这个问题，需要开发一个能够统一这两种任务的框架。

Method: 提出UniReason框架，采用双重推理范式：1) 将生成视为世界知识增强的规划，注入隐式约束；2) 利用编辑能力进行细粒度视觉细化，通过自反思纠正视觉错误。构建了大规模推理中心数据集（约30万样本），涵盖五个主要知识领域，以及代理生成的视觉自校正语料库。

Result: UniReason在推理密集型基准测试（如WISE、KrisBench和UniREditBench）上取得了先进性能，同时保持了卓越的通用合成能力。

Conclusion: UniReason通过统一的表示将生成和编辑整合起来，模拟了人类认知的规划-精炼过程，为解决复杂多模态推理任务提供了有效框架。

Abstract: Unified multimodal models often struggle with complex synthesis tasks that demand deep reasoning, and typically treat text-to-image generation and image editing as isolated capabilities rather than interconnected reasoning steps. To address this, we propose UniReason, a unified framework that harmonizes these two tasks through a dual reasoning paradigm. We formulate generation as world knowledge-enhanced planning to inject implicit constraints, and leverage editing capabilities for fine-grained visual refinement to further correct visual errors via self-reflection. This approach unifies generation and editing within a shared representation, mirroring the human cognitive process of planning followed by refinement. We support this framework by systematically constructing a large-scale reasoning-centric dataset (~300k samples) covering five major knowledge domains (e.g., cultural commonsense, physics, etc.) for planning, alongside an agent-generated corpus for visual self-correction. Extensive experiments demonstrate that UniReason achieves advanced performance on reasoning-intensive benchmarks such as WISE, KrisBench and UniREditBench, while maintaining superior general synthesis capabilities.

</details>


### [266] [KAN We Flow? Advancing Robotic Manipulation with 3D Flow Matching via KAN & RWKV](https://arxiv.org/abs/2602.01115)
*Zhihao Chen,Yiyuan Ge,Ziyang Wang*

Main category: cs.RO

Relevance: 75.0

TL;DR: KAN-We-Flow：基于RWKV和KAN的轻量级流匹配策略，用于3D机器人操作，参数减少86.8%，在多个基准测试中达到SOTA性能。


<details>
  <summary>Details</summary>
Motivation: 基于扩散的视觉运动策略在推理时效率低下，需要多步去噪和沉重的UNet骨干网络，限制了在资源受限机器人上的部署。流匹配方法虽然缓解了采样负担，但仍采用大型UNet架构。

Method: 提出RWKV-KAN块：RWKV进行高效的时间/通道混合传播任务上下文，GroupKAN层应用可学习的样条基分组函数映射对动作映射进行特征非线性校准。引入动作一致性正则化（ACR），通过欧拉外推强制预测动作轨迹与专家演示对齐。

Result: 参数减少86.8%，保持快速运行时性能，在Adroit、Meta-World和DexArt基准测试中达到最先进的成功率。

Conclusion: KAN-We-Flow展示了如何通过结合RWKV和KAN构建轻量级但高表达力的骨干网络，为资源受限机器人的高效视觉运动策略提供了有前景的解决方案。

Abstract: Diffusion-based visuomotor policies excel at modeling action distributions but are inference-inefficient, since recursively denoising from noise to policy requires many steps and heavy UNet backbones, which hinders deployment on resource-constrained robots. Flow matching alleviates the sampling burden by learning a one-step vector field, yet prior implementations still inherit large UNet-style architectures. In this work, we present KAN-We-Flow, a flow-matching policy that draws on recent advances in Receptance Weighted Key Value (RWKV) and Kolmogorov-Arnold Networks (KAN) from vision to build a lightweight and highly expressive backbone for 3D manipulation. Concretely, we introduce an RWKV-KAN block: an RWKV first performs efficient time/channel mixing to propagate task context, and a subsequent GroupKAN layer applies learnable spline-based, groupwise functional mappings to perform feature-wise nonlinear calibration of the action mapping on RWKV outputs. Moreover, we introduce an Action Consistency Regularization (ACR), a lightweight auxiliary loss that enforces alignment between predicted action trajectories and expert demonstrations via Euler extrapolation, providing additional supervision to stabilize training and improve policy precision. Without resorting to large UNets, our design reduces parameters by 86.8\%, maintains fast runtime, and achieves state-of-the-art success rates on Adroit, Meta-World, and DexArt benchmarks. Our project page can be viewed in \href{https://zhihaochen-2003.github.io/KAN-We-Flow.github.io/}{\textcolor{red}{link}}

</details>


### [267] [R3G: A Reasoning--Retrieval--Reranking Framework for Vision-Centric Answer Generation](https://arxiv.org/abs/2602.00104)
*Zhuohong Chen,Zhengxian Wu,Zirui Liao,Shenao Jiang,Hangrui Xu,Yang Chen,Chaokui Su,Xiaoyu Liu,Haoqian Wang*

Main category: cs.CV

Relevance: 65.0

TL;DR: R3G提出模块化的推理-检索-重排序框架，通过生成推理计划指导视觉检索，采用两阶段策略（粗检索+细粒度重排序）为VQA选择证据图像，在MRAG-Bench上实现SOTA性能。


<details>
  <summary>Details</summary>
Motivation: 视觉中心检索在VQA中需要检索图像以提供缺失的视觉线索，但如何选择正确的图像并有效整合到推理过程中仍具挑战性。

Method: 提出模块化的R3G框架：1）生成推理计划指定所需视觉线索；2）两阶段策略：粗检索后接细粒度重排序选择证据图像；3）充分感知的重排序和推理步骤互补。

Result: 在MRAG-Bench上，R3G提升了六个MLLM骨干网络和九个子场景的准确率，实现了整体最先进的性能。消融实验显示充分感知的重排序和推理步骤具有互补性。

Conclusion: R3G框架通过模块化的推理-检索-重排序设计，有效解决了VQA中视觉检索的挑战，帮助模型既选择正确的图像又充分利用它们。

Abstract: Vision-centric retrieval for VQA requires retrieving images to supply missing visual cues and integrating them into the reasoning process. However, selecting the right images and integrating them effectively into the model's reasoning remains challenging.To address this challenge, we propose R3G, a modular Reasoning-Retrieval-Reranking framework.It first produces a brief reasoning plan that specifies the required visual cues, then adopts a two-stage strategy, with coarse retrieval followed by fine-grained reranking, to select evidence images.On MRAG-Bench, R3G improves accuracy across six MLLM backbones and nine sub-scenarios, achieving state-of-the-art overall performance. Ablations show that sufficiency-aware reranking and reasoning steps are complementary, helping the model both choose the right images and use them well. We release code and data at https://github.com/czh24/R3G.

</details>


### [268] [Observing Health Outcomes Using Remote Sensing Imagery and Geo-Context Guided Visual Transformer](https://arxiv.org/abs/2602.00110)
*Yu Li,Guilherme N. DeSouza,Praveen Rao,Chi-Ren Shyu*

Main category: cs.CV

Relevance: 65.0

TL;DR: 提出了一种新颖的遥感图像处理模型，通过地理空间嵌入机制和引导注意力模块，将辅助地理空间信息与视觉内容相结合，提升多模态地理空间理解能力。


<details>
  <summary>Details</summary>
Motivation: 现有的视觉-语言和多模态模型主要优化视觉与文本内容的语义对齐，缺乏对结构化地理空间层的表示和推理能力，不适用于需要地理空间理解的遥感应用。

Method: 1) 地理空间嵌入机制：将多样化的地理空间数据转换为与图像块空间对齐的嵌入块；2) 引导注意力模块：基于与辅助数据的相关性动态计算注意力权重，整合多模态信息；3) 注意力头角色分配：为不同注意力头分配不同角色，捕捉互补信息。

Result: 实验结果表明，该框架在预测疾病流行率等任务上优于现有的预训练地理空间基础模型，证明了其在多模态地理空间理解方面的有效性。

Conclusion: 通过引入地理空间嵌入和引导注意力机制，成功增强了遥感图像处理的地理空间理解能力，为多模态地理空间分析提供了有效解决方案。

Abstract: Visual transformers have driven major progress in remote sensing image analysis, particularly in object detection and segmentation. Recent vision-language and multimodal models further extend these capabilities by incorporating auxiliary information, including captions, question and answer pairs, and metadata, which broadens applications beyond conventional computer vision tasks. However, these models are typically optimized for semantic alignment between visual and textual content rather than geospatial understanding, and therefore are not suited for representing or reasoning with structured geospatial layers. In this study, we propose a novel model that enhances remote sensing imagery processing with guidance from auxiliary geospatial information. Our approach introduces a geospatial embedding mechanism that transforms diverse geospatial data into embedding patches that are spatially aligned with image patches. To facilitate cross-modal interaction, we design a guided attention module that dynamically integrates multimodal information by computing attention weights based on correlations with auxiliary data, thereby directing the model toward the most relevant regions. In addition, the module assigns distinct roles to individual attention heads, allowing the model to capture complementary aspects of the guidance information and improving the interpretability of its predictions. Experimental results demonstrate that the proposed framework outperforms existing pretrained geospatial foundation models in predicting disease prevalence, highlighting its effectiveness in multimodal geospatial understanding.

</details>


### [269] [IC-EO: Interpretable Code-based assistant for Earth Observation](https://arxiv.org/abs/2602.00117)
*Lamia Lahouel,Laurynas Lopata,Simon Gruening,Gabriele Meoni,Gaetan Petit,Sylvain Lobry*

Main category: cs.CV

Relevance: 65.0

TL;DR: 提出一个基于工具LLM的对话式代码生成代理，将自然语言查询转换为可执行、可审计的Python工作流，用于地球观测分析，提高透明度和可重复性。


<details>
  <summary>Details</summary>
Motivation: 地球观测分析对非专家来说很困难，需要专业知识和技能，且现有系统通常是黑盒预测，难以审计或复现。需要让地球观测分析对普通用户更易访问，同时保持透明和可重复。

Method: 开发一个对话式代码生成代理，利用工具LLM将自然语言查询转换为可执行的Python工作流。代理通过统一可扩展的API操作，支持分类、分割、检测、光谱指数和地理空间操作。框架允许在三个层面控制结果：工具级性能、代理级代码生成能力、任务级具体用例。

Result: 在土地组成制图和野火后损害评估两个用例中，提出的代理优于通用LLM/VLM基线（GPT-4o, LLaVA），在土地组成上达到64.2% vs. 51.7%准确率，在野火后分析上达到50% vs. 0%。同时生成透明、易解释的结果。

Conclusion: 通过输出可验证的代码，该方法将地球观测分析转变为透明、可重复的过程，为非专家提供了可访问且可审计的分析工具。

Abstract: Despite recent advances in computer vision, Earth Observation (EO) analysis remains difficult to perform for the laymen, requiring expert knowledge and technical capabilities. Furthermore, many systems return black-box predictions that are difficult to audit or reproduce. Leveraging recent advances in tool LLMs, this study proposes a conversational, code-generating agent that transforms natural-language queries into executable, auditable Python workflows. The agent operates over a unified easily extendable API for classification, segmentation, detection (oriented bounding boxes), spectral indices, and geospatial operators. With our proposed framework, it is possible to control the results at three levels: (i) tool-level performance on public EO benchmarks; (ii) at the agent-level to understand the capacity to generate valid, hallucination-free code; and (iii) at the task-level on specific use cases. In this work, we select two use-cases of interest: land-composition mapping and post-wildfire damage assessment. The proposed agent outperforms general-purpose LLM/VLM baselines (GPT-4o, LLaVA), achieving 64.2% vs. 51.7% accuracy on land-composition and 50% vs. 0% on post-wildfire analysis, while producing results that are transparent and easy to interpret. By outputting verifiable code, the approach turns EO analysis into a transparent, reproducible process.

</details>


### [270] [Scalable Analytic Classifiers with Associative Drift Compensation for Class-Incremental Learning of Vision Transformers](https://arxiv.org/abs/2602.00144)
*Xuan Rao,Mingming Ha,Bo Zhao,Derong Liu,Cesare Alippi*

Main category: cs.CV

Relevance: 65.0

TL;DR: 提出LR-RGDA和HopDC框架，解决ViT在类增量学习中分类器重建的计算瓶颈问题，通过低秩分解和Hopfield网络实现高效且准确的大规模类增量学习。


<details>
  <summary>Details</summary>
Motivation: 在基于ViT的类增量学习中，现有方法依赖昂贵的迭代SGD进行分类器重建，存在计算瓶颈。虽然解析的RGDA能达到与SGD相当的准确率，但其二次推理复杂度限制了在大规模场景中的应用。

Method: 1. 提出LR-RGDA：通过Woodbury矩阵恒等式利用协方差的低秩结构，将判别函数分解为全局仿射项和低秩二次扰动项，将推理复杂度从O(Cd²)降低到O(d² + Crd²)。2. 引入HopDC：基于现代连续Hopfield网络，通过无标签锚点的联想记忆动态重新校准历史类统计量，缓解骨干网络更新导致的表示漂移。

Result: 在多个CIL基准测试上的广泛实验表明，该框架实现了最先进的性能，为ViT的大规模类增量学习提供了可扩展的解决方案。

Conclusion: LR-RGDA结合了RGDA的表达能力和线性分类器的效率，HopDC有效缓解了表示漂移问题，共同为大规模类增量学习提供了高效且准确的解决方案。

Abstract: Class-incremental learning (CIL) with Vision Transformers (ViTs) faces a major computational bottleneck during the classifier reconstruction phase, where most existing methods rely on costly iterative stochastic gradient descent (SGD). We observe that analytic Regularized Gaussian Discriminant Analysis (RGDA) provides a Bayes-optimal alternative with accuracy comparable to SGD-based classifiers; however, its quadratic inference complexity limits its use in large-scale CIL scenarios. To overcome this, we propose Low-Rank Factorized RGDA (LR-RGDA), a scalable classifier that combines RGDA's expressivity with the efficiency of linear classifiers. By exploiting the low-rank structure of the covariance via the Woodbury matrix identity, LR-RGDA decomposes the discriminant function into a global affine term refined by a low-rank quadratic perturbation, reducing the inference complexity from $\mathcal{O}(Cd^2)$ to $\mathcal{O}(d^2 + Crd^2)$, where $C$ is the class number, $d$ the feature dimension, and $r \ll d$ the subspace rank. To mitigate representation drift caused by backbone updates, we further introduce Hopfield-based Distribution Compensator (HopDC), a training-free mechanism that uses modern continuous Hopfield Networks to recalibrate historical class statistics through associative memory dynamics on unlabeled anchors, accompanied by a theoretical bound on the estimation error. Extensive experiments on diverse CIL benchmarks demonstrate that our framework achieves state-of-the-art performance, providing a scalable solution for large-scale class-incremental learning with ViTs. Code: https://github.com/raoxuan98-hash/lr_rgda_hopdc.

</details>


### [271] [Stabilizing Diffusion Posterior Sampling by Noise--Frequency Continuation](https://arxiv.org/abs/2602.00176)
*Feng Tian,Yixuan Li,Weili Zeng,Weitian Zhang,Yichao Yan,Xiaokang Yang*

Main category: cs.CV

Relevance: 65.0

TL;DR: 提出了一种噪声-频率延续框架，通过构建中间后验分布家族，在噪声依赖的频率带内实施测量一致性，解决了扩散后验采样在逆问题中的细节恢复问题。


<details>
  <summary>Details</summary>
Motivation: 扩散后验采样在结合预训练扩散先验和测量一致性指导时，由于测量项与扩散噪声水平的弱耦合，常常无法恢复精细细节。在高噪声下，从不准确估计计算的数据一致性梯度可能与后验几何不一致，导致早期漂移、伪高频伪影以及对调度和病态算子的敏感性。

Method: 提出了噪声-频率延续框架，构建中间后验分布家族，其似然仅在噪声依赖的频率带内强制执行测量一致性。实现了一个稳定的后验采样器，结合扩散预测器、带限似然指导和多分辨率一致性策略，激进地采用可靠的粗粒度修正，保守地仅在可识别时采用高频细节。

Result: 在超分辨率、修复和去模糊等任务中实现了最先进的性能，运动去模糊的PSNR相比强基线提升了高达5 dB。

Conclusion: 通过噪声-频率延续框架和多分辨率一致性策略，有效解决了扩散后验采样中的细节恢复问题，显著提升了逆问题求解的性能。

Abstract: Diffusion posterior sampling solves inverse problems by combining a pretrained diffusion prior with measurement-consistency guidance, but it often fails to recover fine details because measurement terms are applied in a manner that is weakly coupled to the diffusion noise level. At high noise, data-consistency gradients computed from inaccurate estimates can be geometrically incongruent with the posterior geometry, inducing early-step drift, spurious high-frequency artifacts, plus sensitivity to schedules and ill-conditioned operators. To address these concerns, we propose a noise--frequency Continuation framework that constructs a continuous family of intermediate posteriors whose likelihood enforces measurement consistency only within a noise-dependent frequency band. This principle is instantiated with a stabilized posterior sampler that combines a diffusion predictor, band-limited likelihood guidance, and a multi-resolution consistency strategy that aggressively commits reliable coarse corrections while conservatively adopting high-frequency details only when they become identifiable. Across super-resolution, inpainting, and deblurring, our method achieves state-of-the-art performance and improves motion deblurring PSNR by up to 5 dB over strong baselines.

</details>


### [272] [CamReasoner: Reinforcing Camera Movement Understanding via Structured Spatial Reasoning](https://arxiv.org/abs/2602.00181)
*Hang Wu,Yujun Cai,Zehao Li,Haonan Ge,Bowen Sun,Junsong Yuan,Yiwei Wang*

Main category: cs.CV

Relevance: 65.0

TL;DR: CamReasoner是一个通过结构化推理理解摄像机运动的框架，采用观察-思考-回答范式，结合大规模推理轨迹数据集和强化学习进行逻辑对齐，在多个基准测试中达到SOTA性能。


<details>
  <summary>Details</summary>
Motivation: 现有多模态模型将摄像机运动理解视为黑盒分类任务，经常混淆物理上不同的运动，依赖表面视觉模式而非几何线索。需要弥合感知与电影逻辑之间的差距。

Method: 提出CamReasoner框架，采用观察-思考-回答（O-T-A）范式，迫使模型在显式推理块中解码时空线索（如轨迹和视锥）。构建大规模推理轨迹套件（18k SFT推理链和38k RL反馈样本），首次在该领域使用强化学习进行逻辑对齐。

Result: 通过强化学习在观察-思考-回答推理范式中的应用，CamReasoner有效抑制幻觉，在多个基准测试中达到最先进的性能。

Conclusion: CamReasoner通过结构化推理过程重新构建摄像机运动理解，将强化学习应用于逻辑对齐，使运动推断基于物理几何而非上下文猜测，显著提升性能。

Abstract: Understanding camera dynamics is a fundamental pillar of video spatial intelligence. However, existing multimodal models predominantly treat this task as a black-box classification, often confusing physically distinct motions by relying on superficial visual patterns rather than geometric cues. We present CamReasoner, a framework that reformulates camera movement understanding as a structured inference process to bridge the gap between perception and cinematic logic. Our approach centers on the Observation-Thinking-Answer (O-T-A) paradigm, which compels the model to decode spatio-temporal cues such as trajectories and view frustums within an explicit reasoning block. To instill this capability, we construct a Large-scale Inference Trajectory Suite comprising 18k SFT reasoning chains and 38k RL feedback samples. Notably, we are the first to employ RL for logical alignment in this domain, ensuring motion inferences are grounded in physical geometry rather than contextual guesswork. By applying Reinforcement Learning to the Observation-Think-Answer (O-T-A) reasoning paradigm, CamReasoner effectively suppresses hallucinations and achieves state-of-the-art performance across multiple benchmarks.

</details>


### [273] [ReLAPSe: Reinforcement-Learning-trained Adversarial Prompt Search for Erased concepts in unlearned diffusion models](https://arxiv.org/abs/2602.00350)
*Ignacy Kolton,Kacper Marzol,Paweł Batorski,Marcin Mazur,Paul Swoboda,Przemysław Spurek*

Main category: cs.CV

Relevance: 65.0

TL;DR: ReLAPSe是一个基于强化学习的对抗框架，用于恢复文本到图像扩散模型中已被"遗忘"的概念，通过将概念恢复重新定义为强化学习问题，利用扩散模型的噪声预测损失作为可验证的反馈信号。


<details>
  <summary>Details</summary>
Motivation: 现有的对抗方法存在根本性限制：基于优化的方法由于需要逐实例迭代搜索而计算成本高昂，而基于推理和启发式的方法缺乏目标模型潜在视觉表示的直接反馈。需要一种更高效、可扩展的方法来严格测试遗忘扩散模型的安全性。

Method: ReLAPSe采用基于策略的对抗框架，将概念恢复重新定义为强化学习问题。使用可验证奖励的强化学习（RLVR）训练智能体，利用扩散模型的噪声预测损失作为模型内在且可验证的反馈信号。这种闭环设计直接将文本提示操作与潜在视觉残差对齐，使智能体能够学习可迁移的恢复策略，而不是优化孤立的提示。

Result: ReLAPSe实现了跨多个最先进遗忘方法的高效、近实时的细粒度身份和风格恢复，为遗忘扩散模型的严格红队测试提供了可扩展工具。

Conclusion: 通过从逐实例优化转向全局策略学习，ReLAPSe为严格测试遗忘扩散模型的安全性提供了新的范式，揭示了当前遗忘方法中潜在视觉信息的持久性问题。

Abstract: Machine unlearning is a key defense mechanism for removing unauthorized concepts from text-to-image diffusion models, yet recent evidence shows that latent visual information often persists after unlearning. Existing adversarial approaches for exploiting this leakage are constrained by fundamental limitations: optimization-based methods are computationally expensive due to per-instance iterative search. At the same time, reasoning-based and heuristic techniques lack direct feedback from the target model's latent visual representations. To address these challenges, we introduce ReLAPSe, a policy-based adversarial framework that reformulates concept restoration as a reinforcement learning problem. ReLAPSe trains an agent using Reinforcement Learning with Verifiable Rewards (RLVR), leveraging the diffusion model's noise prediction loss as a model-intrinsic and verifiable feedback signal. This closed-loop design directly aligns textual prompt manipulation with latent visual residuals, enabling the agent to learn transferable restoration strategies rather than optimizing isolated prompts. By pioneering the shift from per-instance optimization to global policy learning, ReLAPSe achieves efficient, near-real-time recovery of fine-grained identities and styles across multiple state-of-the-art unlearning methods, providing a scalable tool for rigorous red-teaming of unlearned diffusion models. Some experimental evaluations involve sensitive visual concepts, such as nudity. Code is available at https://github.com/gmum/ReLaPSe

</details>


### [274] [GLAD: Generative Language-Assisted Visual Tracking for Low-Semantic Templates](https://arxiv.org/abs/2602.00570)
*Xingyu Luo,Yidong Cai,Jie Liu,Jie Tang,Gangshan Wu,Limin Wang*

Main category: cs.CV

Relevance: 65.0

TL;DR: GLAD是一种生成式语言辅助跟踪模型，利用扩散模型进行文本描述和模板图像的生成式多模态融合，以增强语言与图像的兼容性，提升低语义图像下的跟踪性能。


<details>
  <summary>Details</summary>
Motivation: 当前视觉语言跟踪方法在处理低语义图像（如模糊、低分辨率）时面临挑战，现有方法通过直接拼接融合文本和视觉特征效果有限，存在模态间特征差距问题。

Method: 提出生成式语言辅助跟踪模型GLAD，采用扩散模型对文本描述和模板图像进行生成式多模态融合，增强语言与图像的兼容性，提升模板图像的语义信息。

Result: 在多个基准测试中达到新的最先进水平，推理速度令人印象深刻，能够有效恢复模糊和语义模糊的模板图像，提升多模态特征质量。

Conclusion: GLAD通过生成式多模态融合方法有效解决了低语义图像下的视觉语言跟踪问题，显著提升了跨模态理解能力，为视觉语言跟踪领域提供了新思路。

Abstract: Vision-language tracking has gained increasing attention in many scenarios. This task simultaneously deals with visual and linguistic information to localize objects in videos. Despite its growing utility, the development of vision-language tracking methods remains in its early stage. Current vision-language trackers usually employ Transformer architectures for interactive integration of template, search, and text features. However, persistent challenges about low-semantic images including prevalent image blurriness, low resolution and so on, may compromise model performance through degraded cross-modal understanding. To solve this problem, language assistance is usually used to deal with the obstacles posed by low-semantic images. However, due to the existing gap between current textual and visual features, direct concatenation and fusion of these features may have limited effectiveness. To address these challenges, we introduce a pioneering Generative Language-AssisteD tracking model, GLAD, which utilizes diffusion models for the generative multi-modal fusion of text description and template image to bolster compatibility between language and image and enhance template image semantic information. Our approach demonstrates notable improvements over the existing fusion paradigms. Blurry and semantically ambiguous template images can be restored to improve multi-modal features in the generative fusion paradigm. Experiments show that our method establishes a new state-of-the-art on multiple benchmarks and achieves an impressive inference speed. The code and models will be released at: https://github.com/Confetti-lxy/GLAD

</details>


### [275] [JoyAvatar: Unlocking Highly Expressive Avatars via Harmonized Text-Audio Conditioning](https://arxiv.org/abs/2602.00702)
*Ruikui Wang,Jinheng Feng,Lang Tian,Huaishao Luo,Chaochao Li,Liangbo Zhou,Huan Zhang,Youzheng Wu,Xiaodong He*

Main category: cs.CV

Relevance: 65.0

TL;DR: JoyAvatar是一个能够生成长时间虚拟人视频的框架，通过双教师增强训练算法和动态多模态条件调制，显著提升了文本指令对齐能力，支持复杂的全身运动、相机轨迹和背景转换。


<details>
  <summary>Details</summary>
Motivation: 现有虚拟人模型在说话、演讲、唱歌等场景表现出色，但在文本指令对齐方面存在局限，特别是涉及复杂元素如大幅全身运动、动态相机轨迹、背景转换或人机交互时。需要突破这一限制，实现更好的文本可控性。

Method: 1. 双教师增强训练算法：从基础模型迁移固有的文本可控性，同时学习音频-视觉同步；2. 动态多模态条件调制：基于不同的去噪时间步动态调整多模态条件（如音频和文本）的强度，缓解异质条件信号之间的冲突。

Result: GSB评估结果显示，JoyAvatar在生成自然、时间连贯的全身运动和动态相机移动方面优于最先进模型（如Omnihuman-1.5和KlingAvatar 2.0），同时保持准确的唇部同步和身份一致性。支持多人对话和非人类角色扮演等复杂应用。

Conclusion: JoyAvatar通过创新的训练算法和条件调制机制，显著扩展了虚拟人模型的文本指令对齐能力，实现了复杂场景下的高质量视频生成，为虚拟人技术提供了新的可能性。

Abstract: Existing video avatar models have demonstrated impressive capabilities in scenarios such as talking, public speaking, and singing. However, the majority of these methods exhibit limited alignment with respect to text instructions, particularly when the prompts involve complex elements including large full-body movement, dynamic camera trajectory, background transitions, or human-object interactions. To break out this limitation, we present JoyAvatar, a framework capable of generating long duration avatar videos, featuring two key technical innovations. Firstly, we introduce a twin-teacher enhanced training algorithm that enables the model to transfer inherent text-controllability from the foundation model while simultaneously learning audio-visual synchronization. Secondly, during training, we dynamically modulate the strength of multi-modal conditions (e.g., audio and text) based on the distinct denoising timestep, aiming to mitigate conflicts between the heterogeneous conditioning signals. These two key designs serve to substantially expand the avatar model's capacity to generate natural, temporally coherent full-body motions and dynamic camera movements as well as preserve the basic avatar capabilities, such as accurate lip-sync and identity consistency. GSB evaluation results demonstrate that our JoyAvatar model outperforms the state-of-the-art models such as Omnihuman-1.5 and KlingAvatar 2.0. Moreover, our approach enables complex applications including multi-person dialogues and non-human subjects role-playing. Some video samples are provided on https://joyavatar.github.io/.

</details>


### [276] [Generating a Paracosm for Training-Free Zero-Shot Composed Image Retrieval](https://arxiv.org/abs/2602.00813)
*Tong Wang,Yunhan Zhao,Shu Kong*

Main category: cs.CV

Relevance: 65.0

TL;DR: 提出Paracosm方法，通过大型多模态模型直接生成"心理图像"进行组合图像检索，无需训练，在零样本CIR任务上达到SOTA性能


<details>
  <summary>Details</summary>
Motivation: 组合图像检索(CIR)的核心挑战是"心理图像"无法直接获取，现有方法使用LMM生成文本描述再通过VLM匹配，但文本描述可能无法准确捕捉视觉细节。本文提出直接生成"心理图像"来更准确地进行匹配。

Method: Paracosm方法：1) 使用LMM为给定的多模态查询生成"心理图像"；2) 为数据库中的每个真实图像生成对应的合成图像以解决合成-真实域差距；3) 在LMM构建的"拟像世界"中进行图像匹配。该方法无需训练，是零样本方法。

Result: 在四个具有挑战性的基准测试上显著优于现有零样本方法，实现了零样本CIR的最先进性能。

Conclusion: 通过直接生成"心理图像"而非文本描述，Paracosm方法在组合图像检索任务上取得了更好的性能，证明了直接视觉生成在CIR任务中的有效性。

Abstract: Composed Image Retrieval (CIR) is the task of retrieving a target image from a database using a multimodal query, which consists of a reference image and a modification text. The text specifies how to alter the reference image to form a ``mental image'', based on which CIR should find the target image in the database. The fundamental challenge of CIR is that this ``mental image'' is not physically available and is only implicitly defined by the query. The contemporary literature pursues zero-shot methods and uses a Large Multimodal Model (LMM) to generate a textual description for a given multimodal query, and then employs a Vision-Language Model (VLM) for textual-visual matching to search the target image. In contrast, we address CIR from first principles by directly generating the ``mental image'' for more accurate matching. Particularly, we prompt an LMM to generate a ``mental image'' for a given multimodal query and propose to use this ``mental image'' to search for the target image. As the ``mental image'' has a synthetic-to-real domain gap with real images, we also generate a synthetic counterpart for each real image in the database to facilitate matching. In this sense, our method uses LMM to construct a ``paracosm'', where it matches the multimodal query and database images. Hence, we call this method Paracosm. Notably, Paracosm is a training-free zero-shot CIR method. It significantly outperforms existing zero-shot methods on four challenging benchmarks, achieving state-of-the-art performance for zero-shot CIR.

</details>


### [277] [SRVAU-R1: Enhancing Video Anomaly Understanding via Reflection-Aware Learning](https://arxiv.org/abs/2602.01004)
*Zihao Zhao,Shengting Cao,Muchao Ye*

Main category: cs.CV

Relevance: 65.0

TL;DR: 提出SRVAU-R1框架，通过自反思增强的多模态大语言模型推理来改进视频异常理解，结合反思导向的思维链数据集和反思感知学习范式，在多个基准测试中取得显著性能提升。


<details>
  <summary>Details</summary>
Motivation: 现有基于MLLM的视频异常理解方法主要关注异常的表面描述，缺乏对异常行为的深度推理能力，如显式的自我反思和自我修正。需要提升MLLM在视频异常理解中的深度推理能力。

Method: 提出SRVAU-R1框架：1）创建首个面向反思的视频异常理解思维链数据集，包含初始推理、自我反思和修正推理的结构化监督；2）采用反思感知学习范式，包括监督微调和强化微调，增强多模态推理能力。

Result: 在多个视频异常基准测试上的实验表明，SRVAU-R1在时间异常定位准确性和推理质量方面均显著优于现有方法，实现了持续的性能提升。

Conclusion: 通过引入反思机制到MLLM推理中，SRVAU-R1有效提升了视频异常理解的深度推理能力，为多模态异常理解提供了新的解决方案。

Abstract: Multi-modal large language models (MLLMs) have demonstrated significant progress in reasoning capabilities and shown promising effectiveness in video anomaly understanding (VAU) tasks. However, existing MLLM-based approaches remain largely focused on surface-level descriptions of anomalies, lacking deep reasoning over abnormal behaviors like explicit self-reflection and self-correction. To address that, we propose Self-Reflection-Enhanced Reasoning for Video Anomaly Understanding (SRVAU-R1), a reflection-aware learning framework that incorporates reflection in MLLM reasoning. Specifically, SRVAU-R1 introduces the first reflection-oriented Chain-of-Thought dataset tailored for VAU, providing structured supervision with initial reasoning, self-reflection, and revised reasoning. Based on that, it includes a novel reflection-aware learning paradigm with supervised fine-tuning and reinforcement fine-tuning to enhance multi-modal reasoning for VAU. Extensive experiments on multiple video anomaly benchmarks demonstrate that SRVAU-R1 consistently outperforms existing methods, achieving significant improvements in both temporal anomaly localization accuracy and reasoning quality.

</details>


### [278] [Koo-Fu CLIP: Closed-Form Adaptation of Vision-Language Models via Fukunaga-Koontz Linear Discriminant Analysis](https://arxiv.org/abs/2602.01127)
*Matej Suchanek,Klara Janouskova,Ondrej Vasatko,Jiri Matas*

Main category: cs.CV

Relevance: 65.0

TL;DR: Koo-Fu CLIP：基于Fukunaga-Koontz线性判别分析的监督CLIP适配方法，在whitened嵌入空间中抑制类内变异、增强类间区分，通过闭式线性投影改善CLIP嵌入的几何结构，提升分类性能并实现有效降维。


<details>
  <summary>Details</summary>
Motivation: 视觉语言模型（如CLIP）提供强大的通用表示，但其原始嵌入未针对监督分类优化，存在类间分离有限和维度冗余的问题。需要一种轻量高效的适配方法来改善CLIP嵌入的判别能力。

Method: 基于Fukunaga-Koontz线性判别分析，在whitened嵌入空间中操作，抑制类内变异、增强类间区分。通过闭式线性投影重塑CLIP嵌入的几何结构，同时实现有效的降维。

Result: 在ImageNet-1K上，top-1准确率从75.1%提升至79.1%，在扩展到14K和21K类时保持一致的性能提升。支持10-12倍的压缩而几乎不损失精度，实现高效的大规模分类和检索。

Conclusion: Koo-Fu CLIP提供了一种轻量高效的CLIP表示适配方法，显著改善分类性能，同时支持大幅压缩，适用于大规模视觉分类和检索任务。

Abstract: Visual-language models such as CLIP provide powerful general-purpose representations, but their raw embeddings are not optimized for supervised classification, often exhibiting limited class separation and excessive dimensionality. We propose Koo-Fu CLIP, a supervised CLIP adaptation method based on Fukunaga-Koontz Linear Discriminant Analysis, which operates in a whitened embedding space to suppress within-class variation and enhance between-class discrimination. The resulting closed-form linear projection reshapes the geometry of CLIP embeddings, improving class separability while performing effective dimensionality reduction, and provides a lightweight and efficient adaptation of CLIP representations.
  Across large-scale ImageNet benchmarks, nearest visual prototype classification in the Koo-Fu CLIP space improves top-1 accuracy from 75.1% to 79.1% on ImageNet-1K, with consistent gains persisting as the label space expands to 14K and 21K classes. The method supports substantial compression by up to 10-12x with little or no loss in accuracy, enabling efficient large-scale classification and retrieval.

</details>


### [279] [Semantically Aware UAV Landing Site Assessment from Remote Sensing Imagery via Multimodal Large Language Models](https://arxiv.org/abs/2602.01163)
*Chunliang Hua,Zeyuan Yang,Lei Zhang,Jiayang Sun,Fengwen Chen,Chunlan Zeng,Xiao Hu*

Main category: cs.CV

Relevance: 65.0

TL;DR: 提出一个结合遥感影像和多模态大语言模型的无人机紧急着陆点评估框架，通过粗到细的语义分析识别传统几何方法无法检测的风险，并构建了公开基准数据集。


<details>
  <summary>Details</summary>
Motivation: 无人机紧急着陆不仅需要平坦地形，还需要理解传统几何传感器无法检测的复杂语义风险（如人群、临时结构），现有基于几何的方法无法满足这一需求。

Method: 采用粗到细的流程：1）轻量级语义分割模块预筛选候选区域；2）视觉-语言推理智能体融合视觉特征和兴趣点数据检测细微风险；构建了ELSS基准数据集进行验证。

Result: 实验表明该框架在风险识别准确率上显著优于几何基线方法，定性结果证实其能生成类似人类的可解释理由，增强自动化决策的可信度。

Conclusion: 结合遥感影像和多模态大语言模型的全局上下文感知方法能有效识别无人机紧急着陆的语义风险，提供可解释的决策支持，优于传统几何方法。

Abstract: Safe UAV emergency landing requires more than just identifying flat terrain; it demands understanding complex semantic risks (e.g., crowds, temporary structures) invisible to traditional geometric sensors. In this paper, we propose a novel framework leveraging Remote Sensing (RS) imagery and Multimodal Large Language Models (MLLMs) for global context-aware landing site assessment. Unlike local geometric methods, our approach employs a coarse-to-fine pipeline: first, a lightweight semantic segmentation module efficiently pre-screens candidate areas; second, a vision-language reasoning agent fuses visual features with Point-of-Interest (POI) data to detect subtle hazards. To validate this approach, we construct and release the Emergency Landing Site Selection (ELSS) benchmark. Experiments demonstrate that our framework significantly outperforms geometric baselines in risk identification accuracy. Furthermore, qualitative results confirm its ability to generate human-like, interpretable justifications, enhancing trust in automated decision-making. The benchmark dataset is publicly accessible at https://anonymous.4open.science/r/ELSS-dataset-43D7.

</details>


### [280] [EEmo-Logic: A Unified Dataset and Multi-Stage Framework for Comprehensive Image-Evoked Emotion Assessment](https://arxiv.org/abs/2602.01173)
*Lancheng Gao,Ziheng Jia,Zixuan Xing,Wei Sun,Huiyu Duan,Guangtao Zhai,Xiongkuo Min*

Main category: cs.CV

Relevance: 65.0

TL;DR: 论文提出了EEmoDB，最大的图像诱发情感理解数据集，包含5个分析维度和5个任务类别，以及EEmo-Logic多模态大语言模型，通过指令微调和GRPO优化实现情感QA和细粒度评估。


<details>
  <summary>Details</summary>
Motivation: 现有模型在图像诱发情感理解方面存在局限性，要么只能进行粗粒度的情感感知，要么缺乏推理能力。为了推动机器共情和丰富人机交互应用，需要更全面的情感理解能力。

Method: 1. 构建EEmoDB数据集：包含125k图像的120万QA对（EEmoDB-QA）和25k图像的36k细粒度评估数据（EEmoDB-Assess）；2. 开发EEmo-Logic MLLM：通过指令微调和任务定制化的组相对偏好优化（GRPO）进行训练，采用新颖的奖励设计。

Result: EEmo-Logic在领域内和跨领域数据集上都表现出鲁棒性能，在情感QA和细粒度评估任务中表现优异。数据集规模和质量均为当前最大。

Conclusion: EEmoDB数据集和EEmo-Logic模型为图像诱发情感理解提供了全面的解决方案，推动了机器共情和多模态情感分析的发展。

Abstract: Understanding the multi-dimensional attributes and intensity nuances of image-evoked emotions is pivotal for advancing machine empathy and empowering diverse human-computer interaction applications. However, existing models are still limited to coarse-grained emotion perception or deficient reasoning capabilities. To bridge this gap, we introduce EEmoDB, the largest image-evoked emotion understanding dataset to date. It features $5$ analysis dimensions spanning $5$ distinct task categories, facilitating comprehensive interpretation. Specifically, we compile $1.2M$ question-answering (QA) pairs (EEmoDB-QA) from $125k$ images via automated generation, alongside a $36k$ dataset (EEmoDB-Assess) curated from $25k$ images for fine-grained assessment. Furthermore, we propose EEmo-Logic, an all-in-one multimodal large language model (MLLM) developed via instruction fine-tuning and task-customized group relative preference optimization (GRPO) with novel reward design. Extensive experiments demonstrate that EEmo-Logic achieves robust performance in in-domain and cross-domain datasets, excelling in emotion QA and fine-grained assessment. The code is available at https://anonymous.4open.science/r/EEmoLogic.

</details>


### [281] [Q-DiT4SR: Exploration of Detail-Preserving Diffusion Transformer Quantization for Real-World Image Super-Resolution](https://arxiv.org/abs/2602.01273)
*Xun Zhang,Kaicheng Yang,Hongliang Lu,Haotong Qin,Yong Guo,Yulun Zhang*

Main category: cs.CV

Relevance: 65.0

TL;DR: 提出了Q-DiT4SR，首个专门为基于扩散Transformer的真实图像超分辨率模型设计的后训练量化框架，通过分层SVD和方差感知的时空混合精度分配，在W4A4设置下实现5.8倍模型压缩和60倍计算加速。


<details>
  <summary>Details</summary>
Motivation: 扩散Transformer在真实图像超分辨率中能生成高质量纹理，但推理负担重阻碍实际部署。现有量化方法主要针对U-Net架构或文本到图像任务，直接应用于超分辨率会导致局部纹理严重退化。

Method: 1) H-SVD：分层奇异值分解，集成全局低秩分支和局部块级秩-1分支；2) VaSMP：基于率失真理论的跨层权重位宽分配；3) VaTMP：通过动态规划在扩散时间步间调度层内激活精度。

Result: 在多个真实世界数据集上，Q-DiT4SR在W4A6和W4A4设置下均达到SOTA性能。W4A4配置将模型大小减少5.8倍，计算操作减少60倍以上。

Conclusion: Q-DiT4SR是首个专门为DiT-based真实图像超分辨率设计的PTQ框架，通过创新的量化策略在保持纹理质量的同时显著加速推理。

Abstract: Recently, Diffusion Transformers (DiTs) have emerged in Real-World Image Super-Resolution (Real-ISR) to generate high-quality textures, yet their heavy inference burden hinders real-world deployment. While Post-Training Quantization (PTQ) is a promising solution for acceleration, existing methods in super-resolution mostly focus on U-Net architectures, whereas generic DiT quantization is typically designed for text-to-image tasks. Directly applying these methods to DiT-based super-resolution models leads to severe degradation of local textures. Therefore, we propose Q-DiT4SR, the first PTQ framework specifically tailored for DiT-based Real-ISR. We propose H-SVD, a hierarchical SVD that integrates a global low-rank branch with a local block-wise rank-1 branch under a matched parameter budget. We further propose Variance-aware Spatio-Temporal Mixed Precision: VaSMP allocates cross-layer weight bit-widths in a data-free manner based on rate-distortion theory, while VaTMP schedules intra-layer activation precision across diffusion timesteps via dynamic programming (DP) with minimal calibration. Experiments on multiple real-world datasets demonstrate that our Q-DiT4SR achieves SOTA performance under both W4A6 and W4A4 settings. Notably, the W4A4 quantization configuration reduces model size by 5.8$\times$ and computational operations by over 60$\times$. Our code and models will be available at https://github.com/xunzhang1128/Q-DiT4SR.

</details>


### [282] [Beyond Pixels: Visual Metaphor Transfer via Schema-Driven Agentic Reasoning](https://arxiv.org/abs/2602.01335)
*Yu Xu,Yuxin Zhang,Juan Cao,Lin Gao,Chunyu Wang,Oliver Deussen,Tong-Yee Lee,Fan Tang*

Main category: cs.CV

Relevance: 65.0

TL;DR: 提出视觉隐喻迁移任务，通过多智能体框架实现跨域语义融合，将参考图像的"创意本质"迁移到目标主体上


<details>
  <summary>Details</summary>
Motivation: 现有生成AI模型局限于像素级指令对齐和表面外观保持，无法捕捉隐喻生成所需的抽象逻辑，需要解决视觉隐喻的创造性表达问题

Method: 提出认知启发的多智能体框架，基于概念融合理论，使用Schema Grammar结构化表示，通过感知、迁移、生成和诊断四个智能体协作实现隐喻迁移

Result: 方法在隐喻一致性、类比恰当性和视觉创造性方面显著优于现有SOTA基线，通过人类评估验证了有效性

Conclusion: 该工作为自动化高影响力创意应用（如广告和媒体）开辟了新途径，实现了真正的视觉隐喻生成

Abstract: A visual metaphor constitutes a high-order form of human creativity, employing cross-domain semantic fusion to transform abstract concepts into impactful visual rhetoric. Despite the remarkable progress of generative AI, existing models remain largely confined to pixel-level instruction alignment and surface-level appearance preservation, failing to capture the underlying abstract logic necessary for genuine metaphorical generation. To bridge this gap, we introduce the task of Visual Metaphor Transfer (VMT), which challenges models to autonomously decouple the "creative essence" from a reference image and re-materialize that abstract logic onto a user-specified target subject. We propose a cognitive-inspired, multi-agent framework that operationalizes Conceptual Blending Theory (CBT) through a novel Schema Grammar ("G"). This structured representation decouples relational invariants from specific visual entities, providing a rigorous foundation for cross-domain logic re-instantiation. Our pipeline executes VMT through a collaborative system of specialized agents: a perception agent that distills the reference into a schema, a transfer agent that maintains generic space invariance to discover apt carriers, a generation agent for high-fidelity synthesis and a hierarchical diagnostic agent that mimics a professional critic, performing closed-loop backtracking to identify and rectify errors across abstract logic, component selection, and prompt encoding. Extensive experiments and human evaluations demonstrate that our method significantly outperforms SOTA baselines in metaphor consistency, analogy appropriateness, and visual creativity, paving the way for automated high-impact creative applications in advertising and media. Source code will be made publicly available.

</details>


### [283] [Where to Attend: A Principled Vision-Centric Position Encoding with Parabolas](https://arxiv.org/abs/2602.01418)
*Christoffer Koo Øhrstrøm,Rafael I. Cabral Muchacho,Yifei Dong,Filippos Moumtzidellis,Ronja Güldenring,Florian T. Pokorny,Lazaros Nalpantidis*

Main category: cs.CV

Relevance: 65.0

TL;DR: PaPE是一种基于抛物线的位置编码方法，专门为视觉模态设计，考虑了视觉特性如平移不变性、旋转不变性等，在8个数据集上表现优异。


<details>
  <summary>Details</summary>
Motivation: 现有位置编码方法主要从语言模型的1D序列扩展到视觉的nD结构，但未能充分考虑视觉模态的特性。本文旨在设计一种专门针对视觉模态的位置编码方法。

Method: 提出抛物线位置编码(PaPE)，基于五个原则设计：平移不变性、旋转不变性(PaPE-RI)、距离衰减、方向性和上下文感知。在4种模态的8个数据集上进行评估。

Result: PaPE或PaPE-RI在8个数据集中的7个上达到最佳性能。在ImageNet-1K的外推实验中，PaPE比次优位置编码绝对提升高达10.5%。

Conclusion: PaPE是一种有效的视觉模态位置编码方法，能够很好地外推并提升模型性能，代码已开源。

Abstract: We propose Parabolic Position Encoding (PaPE), a parabola-based position encoding for vision modalities in attention-based architectures. Given a set of vision tokens-such as images, point clouds, videos, or event camera streams-our objective is to encode their positions while accounting for the characteristics of vision modalities. Prior works have largely extended position encodings from 1D-sequences in language to nD-structures in vision, but only with partial account of vision characteristics. We address this gap by designing PaPE from principles distilled from prior work: translation invariance, rotation invariance (PaPE-RI), distance decay, directionality, and context awareness. We evaluate PaPE on 8 datasets that span 4 modalities. We find that either PaPE or PaPE-RI achieves the top performance on 7 out of 8 datasets. Extrapolation experiments on ImageNet-1K show that PaPE extrapolates remarkably well, improving in absolute terms by up to 10.5% over the next-best position encoding. Code is available at https://github.com/DTU-PAS/parabolic-position-encoding.

</details>


### [284] [Know Your Step: Faster and Better Alignment for Flow Matching Models via Step-aware Advantages](https://arxiv.org/abs/2602.01591)
*Zhixiong Yue,Zixuan Ni,Feiyang Ye,Jinshan Zhang,Sheng Shen,Zhenpeng Mi*

Main category: cs.CV

Relevance: 65.0

TL;DR: TAFS-GRPO：一种用于流匹配文本到图像模型的新型强化学习框架，通过温度退火采样和组相对策略优化，实现高效少步生成与人类偏好的对齐。


<details>
  <summary>Details</summary>
Motivation: 现有基于强化学习的流匹配模型方法通常依赖大量去噪步骤，同时面临稀疏且不精确的奖励信号问题，导致人类偏好对齐效果不佳。需要开发能够在少步生成中实现更好对齐的方法。

Method: 提出TAFS-GRPO框架：1）温度退火少步采样：迭代地向单步采样结果注入自适应时序噪声，在保持图像语义完整性的同时引入采样随机性；2）步感知优势集成机制：结合GRPO避免奖励函数可微性要求，提供密集且步特定的奖励信号以实现稳定策略优化。

Result: 实验表明TAFS-GRPO在少步文本到图像生成中表现优异，显著提高了生成图像与人类偏好的对齐度。

Conclusion: TAFS-GRPO通过创新的温度退火采样和组相对策略优化，有效解决了流匹配模型中稀疏奖励和低效对齐的问题，为少步文本到图像生成提供了有效的强化学习解决方案。

Abstract: Recent advances in flow matching models, particularly with reinforcement learning (RL), have significantly enhanced human preference alignment in few step text to image generators. However, existing RL based approaches for flow matching models typically rely on numerous denoising steps, while suffering from sparse and imprecise reward signals that often lead to suboptimal alignment. To address these limitations, we propose Temperature Annealed Few step Sampling with Group Relative Policy Optimization (TAFS GRPO), a novel framework for training flow matching text to image models into efficient few step generators well aligned with human preferences. Our method iteratively injects adaptive temporal noise onto the results of one step samples. By repeatedly annealing the model's sampled outputs, it introduces stochasticity into the sampling process while preserving the semantic integrity of each generated image. Moreover, its step aware advantage integration mechanism combines the GRPO to avoid the need for the differentiable of reward function and provide dense and step specific rewards for stable policy optimization. Extensive experiments demonstrate that TAFS GRPO achieves strong performance in few step text to image generation and significantly improves the alignment of generated images with human preferences. The code and models of this work will be available to facilitate further research.

</details>


### [285] [PISCES: Annotation-free Text-to-Video Post-Training via Optimal Transport-Aligned Rewards](https://arxiv.org/abs/2602.01624)
*Minh-Quan Le,Gaurav Mittal,Cheng Zhao,David Gu,Dimitris Samaras,Mei Chen*

Main category: cs.CV

Relevance: 65.0

TL;DR: PISCES提出了一种基于双重最优运输对齐奖励的无标注后训练方法，用于提升文本到视频生成的质量和语义对齐，在质量和语义评分上超越了基于标注和无标注的方法。


<details>
  <summary>Details</summary>
Motivation: 当前基于奖励的后训练方法要么依赖大规模人工偏好标注，要么使用预训练视觉语言模型中对齐不佳的嵌入，导致可扩展性有限或监督效果不佳。需要一种无标注的方法来更好地对齐奖励信号与人类判断。

Method: 提出PISCES算法，通过新颖的双重最优运输对齐奖励模块：1）分布级OT对齐质量奖励，捕捉整体视觉质量和时间一致性；2）离散令牌级OT对齐语义奖励，强制文本和视频令牌之间的语义时空对应关系。

Result: 在短视频和长视频生成任务上，PISCES在VBench的质量和语义评分上超越了基于标注和无标注的方法，人类偏好研究进一步验证了其有效性。双重OT对齐奖励模块兼容多种优化范式，包括直接反向传播和强化学习微调。

Conclusion: PISCES首次通过最优运输的视角改进了生成后训练中的无标注奖励监督，为文本到视频生成提供了一种可扩展且有效的后训练方法。

Abstract: Text-to-video (T2V) generation aims to synthesize videos with high visual quality and temporal consistency that are semantically aligned with input text. Reward-based post-training has emerged as a promising direction to improve the quality and semantic alignment of generated videos. However, recent methods either rely on large-scale human preference annotations or operate on misaligned embeddings from pre-trained vision-language models, leading to limited scalability or suboptimal supervision. We present $\texttt{PISCES}$, an annotation-free post-training algorithm that addresses these limitations via a novel Dual Optimal Transport (OT)-aligned Rewards module. To align reward signals with human judgment, $\texttt{PISCES}$ uses OT to bridge text and video embeddings at both distributional and discrete token levels, enabling reward supervision to fulfill two objectives: (i) a Distributional OT-aligned Quality Reward that captures overall visual quality and temporal coherence; and (ii) a Discrete Token-level OT-aligned Semantic Reward that enforces semantic, spatio-temporal correspondence between text and video tokens. To our knowledge, $\texttt{PISCES}$ is the first to improve annotation-free reward supervision in generative post-training through the lens of OT. Experiments on both short- and long-video generation show that $\texttt{PISCES}$ outperforms both annotation-based and annotation-free methods on VBench across Quality and Semantic scores, with human preference studies further validating its effectiveness. We show that the Dual OT-aligned Rewards module is compatible with multiple optimization paradigms, including direct backpropagation and reinforcement learning fine-tuning.

</details>


### [286] [ReCALL: Recalibrating Capability Degradation for MLLM-based Composed Image Retrieval](https://arxiv.org/abs/2602.01639)
*Tianyu Yang,ChenWei He,Xiangzhao Hao,Tianyue Wang,Jiarui Guo,Haiyun Guo,Leigang Qu,Jinqiao Wang,Tat-Seng Chua*

Main category: cs.CV

Relevance: 65.0

TL;DR: ReCALL框架解决生成式多模态大语言模型（MLLMs）适配为检索器时的能力退化问题，通过诊断-生成-精炼流程重新校准检索器的细粒度推理能力，在CIR任务上取得SOTA性能。


<details>
  <summary>Details</summary>
Motivation: 现有方法将生成式MLLMs适配为单嵌入判别式检索器时存在范式冲突，导致细粒度推理能力退化。需要解决这种能力退化问题，使检索器既能保持MLLMs的组成推理能力，又能有效执行检索任务。

Method: 提出ReCALL框架：1）通过自引导信息实例挖掘诊断检索器的认知盲点；2）使用CoT提示基础MLLM生成校正指令和三联体，并通过VQA一致性过滤进行质量控制；3）采用分组对比方案在生成的三联体上进行持续训练，内化细粒度视觉语义区分并重新对齐检索器的判别嵌入空间。

Result: 在CIRR和FashionIQ数据集上的广泛实验表明，ReCALL能持续重新校准退化能力，并实现最先进的性能。

Conclusion: ReCALL框架有效解决了生成式MLLMs适配为检索器时的能力退化问题，通过诊断-生成-精炼流程重新校准了检索器的细粒度推理能力，为多模态检索任务提供了有效的解决方案。

Abstract: Composed Image Retrieval (CIR) aims to retrieve target images based on a hybrid query comprising a reference image and a modification text. Early dual-tower Vision-Language Models (VLMs) struggle with cross-modality compositional reasoning required for this task. Recently, adapting generative Multimodal Large Language Models (MLLMs) for retrieval offers a promising direction. However, we identify that this adaptation strategy overlooks a fundamental issue: adapting a generative MLLM into a single-embedding discriminative retriever triggers a paradigm conflict, which leads to Capability Degradation - the deterioration of native fine-grained reasoning after retrieval adaptation. To address this challenge, we propose ReCALL (Recalibrating Capability Degradation), a model-agnostic framework that follows a diagnose-generate-refine pipeline: Firstly, we diagnose cognitive blind spots of the retriever via self-guided informative instance mining. Next, we generate corrective instructions and triplets by CoT prompting the foundation MLLM and conduct quality control with VQA-based consistency filtering. Finally, we refine the retriever through continual training on these triplets with a grouped contrastive scheme, thereby internalizing fine-grained visual-semantic distinctions and realigning the discriminative embedding space of retriever with intrinsic compositional reasoning within the MLLM. Extensive experiments on CIRR and FashionIQ show that ReCALL consistently recalibrates degraded capabilities and achieves state-of-the-art performance. Code will be released soon.

</details>


### [287] [DenVisCoM: Dense Vision Correspondence Mamba for Efficient and Real-time Optical Flow and Stereo Estimation](https://arxiv.org/abs/2602.01724)
*Tushar Anand,Maheswar Bora,Antitza Dantcheva,Abhijit Das*

Main category: cs.CV

Relevance: 65.0

TL;DR: 提出DenVisCoM Mamba块和混合架构，用于实时联合估计光流和视差，平衡精度与速度


<details>
  <summary>Details</summary>
Motivation: 多视图几何和运动任务（光流和视差估计）本质相关，但现有方法难以同时满足实时性、内存占用和精度要求

Method: 结合DenVisCoM Mamba块和Transformer注意力块的混合架构，统一处理光流和视差估计任务

Result: 在多个数据集上实现准确的光流和视差实时估计，平衡了精度与实时处理

Conclusion: 提出的混合架构能有效解决运动估计和3D密集感知任务的实时推理、内存占用和精度平衡问题

Abstract: In this work, we propose a novel Mamba block DenVisCoM, as well as a novel hybrid architecture specifically tailored for accurate and real-time estimation of optical flow and disparity estimation. Given that such multi-view geometry and motion tasks are fundamentally related, we propose a unified architecture to tackle them jointly. Specifically, the proposed hybrid architecture is based on DenVisCoM and a Transformer-based attention block that efficiently addresses real-time inference, memory footprint, and accuracy at the same time for joint estimation of motion and 3D dense perception tasks. We extensively analyze the benchmark trade-off of accuracy and real-time processing on a large number of datasets. Our experimental results and related analysis suggest that our proposed model can accurately estimate optical flow and disparity estimation in real time. All models and associated code are available at https://github.com/vimstereo/DenVisCoM.

</details>


### [288] [ObjEmbed: Towards Universal Multimodal Object Embeddings](https://arxiv.org/abs/2602.01753)
*Shenghao Fu,Yukun Su,Fengyun Rao,Jing Lyu,Xiaohua Xie,Wei-Shi Zheng*

Main category: cs.CV

Relevance: 65.0

TL;DR: ObjEmbed是一种新颖的多模态语言模型嵌入方法，通过将图像分解为多个区域嵌入（每个对应一个对象）和全局嵌入，实现细粒度的图像-文本对齐，支持视觉定位、局部图像检索和全局图像检索等多种任务。


<details>
  <summary>Details</summary>
Motivation: 当前多模态嵌入模型在全局图像-文本对齐方面表现出色，但在图像区域与特定短语之间的细粒度对齐方面存在困难。需要一种能够同时处理对象级和图像级任务的统一方法。

Method: ObjEmbed将输入图像分解为多个区域嵌入（每个对应一个对象）和全局嵌入。为每个区域生成两个互补嵌入：用于语义匹配的对象嵌入和预测定位质量的IoU嵌入。最终对象匹配分数结合语义相似度和预测的IoU，实现更准确的检索。所有对象和完整图像在单次前向传播中编码以提高效率。

Result: 在18个不同的基准测试中表现出优越性能，展示了强大的语义区分能力。能够无缝处理区域级和图像级任务，在视觉定位、局部图像检索和全局图像检索等任务上表现优异。

Conclusion: ObjEmbed通过对象导向的表示方法，结合语义和空间信息，实现了细粒度的视觉-语言对齐，为多模态理解任务提供了一种高效且通用的解决方案。

Abstract: Aligning objects with corresponding textual descriptions is a fundamental challenge and a realistic requirement in vision-language understanding. While recent multimodal embedding models excel at global image-text alignment, they often struggle with fine-grained alignment between image regions and specific phrases. In this work, we present ObjEmbed, a novel MLLM embedding model that decomposes the input image into multiple regional embeddings, each corresponding to an individual object, along with global embeddings. It supports a wide range of visual understanding tasks like visual grounding, local image retrieval, and global image retrieval. ObjEmbed enjoys three key properties: (1) Object-Oriented Representation: It captures both semantic and spatial aspects of objects by generating two complementary embeddings for each region: an object embedding for semantic matching and an IoU embedding that predicts localization quality. The final object matching score combines semantic similarity with the predicted IoU, enabling more accurate retrieval. (2) Versatility: It seamlessly handles both region-level and image-level tasks. (3) Efficient Encoding: All objects in an image, along with the full image, are encoded in a single forward pass for high efficiency. Superior performance on 18 diverse benchmarks demonstrates its strong semantic discrimination.

</details>


### [289] [How Well Do Models Follow Visual Instructions? VIBE: A Systematic Benchmark for Visual Instruction-Driven Image Editing](https://arxiv.org/abs/2602.01851)
*Huanyu Zhang,Xuehai Bai,Chengzu Li,Chen Liang,Haochen Tian,Haodong Li,Ruichuan An,Yifan Zhang,Anna Korhonen,Zhang Zhang,Liang Wang,Tieniu Tan*

Main category: cs.CV

Relevance: 65.0

TL;DR: VIBE是一个视觉指令图像编辑基准，包含三个层次的交互（指示性定位、形态操作、因果推理），用于评估模型处理视觉指令（如草图）的能力。研究发现专有模型优于开源模型，但随着任务难度增加性能显著下降。


<details>
  <summary>Details</summary>
Motivation: 现有图像编辑系统主要基于文本指导，而人类交流本质上是多模态的，视觉指令（如草图）能更有效地传达空间和结构意图。需要填补这一空白，评估模型处理视觉指令的能力。

Method: 引入VIBE基准，包含三个层次：指示性定位、形态操作、因果推理。构建高质量多样的测试用例，反映视觉指令跟随的渐进复杂性。提出基于LMM-as-a-judge的评估框架，使用任务特定指标进行细粒度评估。

Result: 评估了17个开源和专有图像编辑模型，发现专有模型展现出早期视觉指令跟随能力，一致优于开源模型。但随着任务难度增加，即使最强系统的性能也显著下降。

Conclusion: 专有模型在视觉指令跟随方面表现更好，但所有模型在处理复杂视觉指令时仍有困难，这为未来研究指明了方向。

Abstract: Recent generative models have achieved remarkable progress in image editing. However, existing systems and benchmarks remain largely text-guided. In contrast, human communication is inherently multimodal, where visual instructions such as sketches efficiently convey spatial and structural intent. To address this gap, we introduce VIBE, the Visual Instruction Benchmark for Image Editing with a three-level interaction hierarchy that captures deictic grounding, morphological manipulation, and causal reasoning. Across these levels, we curate high-quality and diverse test cases that reflect progressively increasing complexity in visual instruction following. We further propose a robust LMM-as-a-judge evaluation framework with task-specific metrics to enable scalable and fine-grained assessment. Through a comprehensive evaluation of 17 representative open-source and proprietary image editing models, we find that proprietary models exhibit early-stage visual instruction-following capabilities and consistently outperform open-source models. However, performance degrades markedly with increasing task difficulty even for the strongest systems, highlighting promising directions for future research.

</details>


### [290] [Rethinking Genomic Modeling Through Optical Character Recognition](https://arxiv.org/abs/2602.02014)
*Hongxin Xiang,Pengsen Ma,Yunkang Cao,Di Yu,Haowen Chen,Xinyu Yang,Xiangxiang Zeng*

Main category: cs.CV

Relevance: 65.0

TL;DR: OpticalDNA：将基因组建模重新定义为OCR式文档理解的视觉框架，通过视觉DNA编码器和文档解码器，在减少有效token数量的同时实现更好的基因组任务性能


<details>
  <summary>Details</summary>
Motivation: 当前基因组基础模型大多采用LLM架构，将DNA视为一维token序列，但这种顺序读取方式与稀疏、不连续的基因组语义结构不匹配，导致在低信息背景上浪费计算，且无法实现理解驱动的长上下文压缩

Method: 将DNA渲染为结构化视觉布局，训练OCR能力的视觉-语言模型，包含视觉DNA编码器和文档解码器。编码器生成紧凑、可重构的视觉token以实现高保真压缩。定义基于核心基因组原语的提示条件目标：读取、区域定位、子序列检索和掩码跨度补全

Result: 在多样化基因组基准测试中始终优于近期基线；在长达450k碱基的序列上，以近20倍更少的有效token实现最佳整体性能；在仅微调256k可训练参数的情况下，超越激活参数多达985倍的模型

Conclusion: 视觉OCR框架为基因组建模提供了更结构对齐的方法，通过布局感知的DNA表示在减少计算负担的同时保留细粒度基因组信息，为长上下文基因组理解提供了高效解决方案

Abstract: Recent genomic foundation models largely adopt large language model architectures that treat DNA as a one-dimensional token sequence. However, exhaustive sequential reading is structurally misaligned with sparse and discontinuous genomic semantics, leading to wasted computation on low-information background and preventing understanding-driven compression for long contexts. Here, we present OpticalDNA, a vision-based framework that reframes genomic modeling as Optical Character Recognition (OCR)-style document understanding. OpticalDNA renders DNA into structured visual layouts and trains an OCR-capable vision--language model with a \emph{visual DNA encoder} and a \emph{document decoder}, where the encoder produces compact, reconstructible visual tokens for high-fidelity compression. Building on this representation, OpticalDNA defines prompt-conditioned objectives over core genomic primitives-reading, region grounding, subsequence retrieval, and masked span completion-thereby learning layout-aware DNA representations that retain fine-grained genomic information under a reduced effective token budget. Across diverse genomic benchmarks, OpticalDNA consistently outperforms recent baselines; on sequences up to 450k bases, it achieves the best overall performance with nearly $20\times$ fewer effective tokens, and surpasses models with up to $985\times$ more activated parameters while tuning only 256k \emph{trainable} parameters.

</details>


### [291] [Your AI-Generated Image Detector Can Secretly Achieve SOTA Accuracy, If Calibrated](https://arxiv.org/abs/2602.01973)
*Muli Yang,Gabriel James Goenawan,Henan Wang,Huaiyuan Qin,Chenghao Xu,Yanhua Yang,Fen Fang,Ying Sun,Joo-Hwee Lim,Hongyuan Zhu*

Main category: cs.CV

Relevance: 65.0

TL;DR: 提出基于贝叶斯决策理论的后处理校准框架，通过可学习的标量修正模型logits，解决AI生成图像检测器在测试时因分布偏移导致的系统性偏差问题。


<details>
  <summary>Details</summary>
Motivation: 现有AI生成图像检测器虽然在平衡数据集上训练，但在测试时经常表现出系统性偏差，错误地将伪造图像分类为真实图像。作者假设这种行为源于伪造样本的分布偏移和训练期间学到的隐式先验，导致模型过度拟合到不能跨不同生成方法泛化的表面伪影。

Method: 提出基于贝叶斯决策理论的后处理校准框架，引入可学习的标量修正模型logits，在目标分布的小验证集上优化，同时保持主干网络冻结。这种参数化调整补偿模型输出的分布偏移，重新对齐决策边界，且不需要真实标签。

Result: 在具有挑战性的基准测试中，该方法显著提高了鲁棒性，无需重新训练，为开放世界中可靠且自适应的AI生成图像检测提供了轻量级、有理论依据的解决方案。

Conclusion: 提出的校准框架有效解决了AI生成图像检测器在分布偏移下的系统性偏差问题，通过理论驱动的后处理调整实现了更好的泛化能力。

Abstract: Despite being trained on balanced datasets, existing AI-generated image detectors often exhibit systematic bias at test time, frequently misclassifying fake images as real. We hypothesize that this behavior stems from distributional shift in fake samples and implicit priors learned during training. Specifically, models tend to overfit to superficial artifacts that do not generalize well across different generation methods, leading to a misaligned decision threshold when faced with test-time distribution shift. To address this, we propose a theoretically grounded post-hoc calibration framework based on Bayesian decision theory. In particular, we introduce a learnable scalar correction to the model's logits, optimized on a small validation set from the target distribution while keeping the backbone frozen. This parametric adjustment compensates for distributional shift in model output, realigning the decision boundary even without requiring ground-truth labels. Experiments on challenging benchmarks show that our approach significantly improves robustness without retraining, offering a lightweight and principled solution for reliable and adaptive AI-generated image detection in the open world. Code is available at https://github.com/muliyangm/AIGI-Det-Calib.

</details>


### [292] [One Size, Many Fits: Aligning Diverse Group-Wise Click Preferences in Large-Scale Advertising Image Generation](https://arxiv.org/abs/2602.02033)
*Shuo Lu,Haohan Wang,Wei Feng,Weizhen Wang,Shen Zhang,Yaoyu Li,Ao Ma,Zheng Zhang,Jingjing Lv,Junjie Shen,Ching Law,Bing Zhan,Yuan Xu,Huizai Yao,Yongcan Yu,Chenyang Si,Jian Liang*

Main category: cs.CV

Relevance: 65.0

TL;DR: OSMF是一个统一的广告图像生成框架，通过产品感知自适应分组和偏好条件图像生成，针对不同用户群体优化点击率，解决了现有方法忽视群体偏好多样性的问题。


<details>
  <summary>Details</summary>
Motivation: 现有广告图像生成方法采用"一刀切"策略，只优化整体点击率而忽视不同用户群体的偏好多样性，导致针对特定群体的营销效果不佳。需要一种能够对齐多样化群体点击偏好的框架。

Method: 1. 产品感知自适应分组：基于用户属性和产品特征动态组织用户，为每个群体构建丰富的集体偏好特征
2. 偏好条件图像生成：使用群体感知多模态大语言模型(G-MLLM)为每个群体生成定制化图像
3. G-MLLM预训练：同时理解群体特征并生成广告图像
4. Group-DPO微调：通过群体偏好对齐增强每个群体在生成图像上的点击率

Result: 1. 在离线和在线设置中都达到了最先进的性能
2. 构建了首个大规模公开群体图像偏好数据集GAIP，包含约60万个群体（来自4000万用户）
3. 框架有效提升了每个群体在生成图像上的点击率

Conclusion: OSMF框架通过群体感知的广告图像生成，成功解决了现有方法忽视用户群体偏好多样性的问题，为大规模广告图像生成提供了有效的群体偏好对齐解决方案。

Abstract: Advertising image generation has increasingly focused on online metrics like Click-Through Rate (CTR), yet existing approaches adopt a ``one-size-fits-all" strategy that optimizes for overall CTR while neglecting preference diversity among user groups. This leads to suboptimal performance for specific groups, limiting targeted marketing effectiveness. To bridge this gap, we present \textit{One Size, Many Fits} (OSMF), a unified framework that aligns diverse group-wise click preferences in large-scale advertising image generation. OSMF begins with product-aware adaptive grouping, which dynamically organizes users based on their attributes and product characteristics, representing each group with rich collective preference features. Building on these groups, preference-conditioned image generation employs a Group-aware Multimodal Large Language Model (G-MLLM) to generate tailored images for each group. The G-MLLM is pre-trained to simultaneously comprehend group features and generate advertising images. Subsequently, we fine-tune the G-MLLM using our proposed Group-DPO for group-wise preference alignment, which effectively enhances each group's CTR on the generated images. To further advance this field, we introduce the Grouped Advertising Image Preference Dataset (GAIP), the first large-scale public dataset of group-wise image preferences, including around 600K groups built from 40M users. Extensive experiments demonstrate that our framework achieves the state-of-the-art performance in both offline and online settings. Our code and datasets will be released at https://github.com/JD-GenX/OSMF.

</details>


### [293] [FSVideo: Fast Speed Video Diffusion Model in a Highly-Compressed Latent Space](https://arxiv.org/abs/2602.02092)
*FSVideo Team,Qingyu Chen,Zhiyuan Fang,Haibin Huang,Xinwei Huang,Tong Jin,Minxuan Lin,Bo Liu,Celong Liu,Chongyang Ma,Xing Mei,Xiaohui Shen,Yaojie Shen,Fuwen Tan,Angtian Wang,Xiao Yang,Yiding Yang,Jiamin Yuan,Lingxi Zhang,Yuxin Zhang*

Main category: cs.CV

Relevance: 65.0

TL;DR: FSVideo是一个基于Transformer的快速图像到视频扩散框架，包含高度压缩的视频自动编码器、具有层内存设计的扩散Transformer架构，以及多分辨率生成策略，在保持竞争力的同时实现了一个数量级的加速。


<details>
  <summary>Details</summary>
Motivation: 当前图像到视频生成模型通常速度较慢，需要高效的架构设计来加速生成过程，同时保持生成质量。FSVideo旨在通过创新的Transformer架构和压缩策略，实现快速且高质量的视频生成。

Method: 1) 设计新的视频自动编码器，实现64×64×4的高压缩率潜在空间；2) 采用扩散Transformer架构，引入层内存设计增强层间信息流和上下文重用；3) 通过多分辨率生成策略，使用少量步数的DIT上采样器提高视频保真度。

Result: 最终模型包含140亿参数的DIT基础模型和140亿参数的DIT上采样器，在保持与流行开源模型竞争力的同时，实现了一个数量级的加速。

Conclusion: FSVideo通过创新的架构设计和训练策略，成功实现了快速且高质量的图像到视频生成，为视频生成领域提供了高效的解决方案。

Abstract: We introduce FSVideo, a fast speed transformer-based image-to-video (I2V) diffusion framework. We build our framework on the following key components: 1.) a new video autoencoder with highly-compressed latent space ($64\times64\times4$ spatial-temporal downsampling ratio), achieving competitive reconstruction quality; 2.) a diffusion transformer (DIT) architecture with a new layer memory design to enhance inter-layer information flow and context reuse within DIT, and 3.) a multi-resolution generation strategy via a few-step DIT upsampler to increase video fidelity. Our final model, which contains a 14B DIT base model and a 14B DIT upsampler, achieves competitive performance against other popular open-source models, while being an order of magnitude faster. We discuss our model design as well as training strategies in this report.

</details>


### [294] [Teacher-Guided Student Self-Knowledge Distillation Using Diffusion Model](https://arxiv.org/abs/2602.02107)
*Yu Wang,Chuanguang Yang,Zhulin An,Weilun Feng,Jiarui Zhao,Chengqing Yu,Libo Huang,Boyu Diao,Yongjun Xu*

Main category: cs.CV

Relevance: 65.0

TL;DR: 提出DSKD方法，通过教师引导的学生扩散自知识蒸馏，使用轻量扩散模型去噪学生特征，并用LSH引导的特征蒸馏消除师生特征分布差异


<details>
  <summary>Details</summary>
Motivation: 传统知识蒸馏方法直接对齐师生特征，但由于特征分布差异，学生可能从教师学习到不兼容的信息。需要解决师生映射方式和特征分布不一致的问题。

Method: 1) 使用教师分类器引导轻量扩散模型对噪声学生特征进行去噪采样；2) 提出基于局部敏感哈希(LSH)的特征蒸馏方法，在原始学生特征和去噪学生特征之间进行蒸馏；3) 去噪后的学生特征包含教师知识，可视为"教师"角色

Result: 在视觉识别任务上的实验表明，DSKD在各种模型和数据集上显著优于现有的知识蒸馏方法

Conclusion: DSKD方法能有效消除师生映射方式和特征分布的差异，同时从教师学习有意义的知识，提升知识蒸馏效果

Abstract: Existing Knowledge Distillation (KD) methods often align feature information between teacher and student by exploring meaningful feature processing and loss functions. However, due to the difference in feature distributions between the teacher and student, the student model may learn incompatible information from the teacher. To address this problem, we propose teacher-guided student Diffusion Self-KD, dubbed as DSKD. Instead of the direct teacher-student alignment, we leverage the teacher classifier to guide the sampling process of denoising student features through a light-weight diffusion model. We then propose a novel locality-sensitive hashing (LSH)-guided feature distillation method between the original and denoised student features. The denoised student features encapsulate teacher knowledge and could be regarded as a teacher role. In this way, our DSKD method could eliminate discrepancies in mapping manners and feature distributions between the teacher and student, while learning meaningful knowledge from the teacher. Experiments on visual recognition tasks demonstrate that DSKD significantly outperforms existing KD methods across various models and datasets. Our code is attached in supplementary material.

</details>


### [295] [Reg4Pru: Regularisation Through Random Token Routing for Token Pruning](https://arxiv.org/abs/2602.02163)
*Julian Wyatt,Ronald Clark,Irina Voiculescu*

Main category: cs.CV

Relevance: 65.0

TL;DR: Reg4Pru是一种用于视觉Transformer的训练正则化技术，通过缓解token剪枝带来的性能损失，在保持计算效率的同时提升分割任务的精度。


<details>
  <summary>Details</summary>
Motivation: Transformer在视觉模型中广泛采用，但其计算复杂度随token数量呈二次方增长。现有token剪枝方法虽然提高了计算效率，但会导致保留表示的不稳定性，从而在深层网络中降低密集预测性能。

Method: 提出Reg4Pru训练正则化技术，专门针对token剪枝策略设计，通过正则化来缓解剪枝带来的性能损失，特别是在分割任务中。

Result: 在FIVES血管分割数据集上，Reg4Pru相比无路由训练的相同模型，平均精度绝对提升了46%。在实现29%相对加速（相比非剪枝基线）的配置下仍能保持性能提升。

Conclusion: Reg4Pru是token减少策略中一个有价值的正则化器，能够在保持计算效率的同时显著提升分割任务的性能。

Abstract: Transformers are widely adopted in modern vision models due to their strong ability to scale with dataset size and generalisability. However, this comes with a major drawback: computation scales quadratically to the total number of tokens. Numerous methods have been proposed to mitigate this. For example, we consider token pruning with reactivating tokens from preserved representations, but the increased computational efficiency of this method results in decreased stability from the preserved representations, leading to poorer dense prediction performance at deeper layers. In this work, we introduce Reg4Pru, a training regularisation technique that mitigates token-pruning performance loss for segmentation. We compare our models on the FIVES blood vessel segmentation dataset and find that Reg4Pru improves average precision by an absolute 46% compared to the same model trained without routing. This increase is observed using a configuration that achieves a 29% relative speedup in wall-clock time compared to the non-pruned baseline. These findings indicate that Reg4Pru is a valuable regulariser for token reduction strategies.

</details>


### [296] [LangMap: A Hierarchical Benchmark for Open-Vocabulary Goal Navigation](https://arxiv.org/abs/2602.02220)
*Bo Miao,Weijia Liu,Jun Luo,Lachlan Shinnick,Jian Liu,Thomas Hamilton-Smith,Yuhe Yang,Zijie Wu,Vanja Videnovic,Feras Dayoub,Anton van den Hengel*

Main category: cs.CV

Relevance: 65.0

TL;DR: 提出HieraNav多粒度开放词汇目标导航任务和LangMap大规模基准，基于真实3D室内扫描构建，包含场景、房间、区域、实例四个语义层级，提供18K+导航任务和高质量标注，用于评估语言驱动的具身导航模型。


<details>
  <summary>Details</summary>
Motivation: 物体与语言的关系对于人机有意义的通信和实用的具身智能至关重要。现有导航基准在语义粒度、标注质量和任务多样性方面存在不足，需要更全面的测试平台来推动语言驱动的具身导航研究。

Method: 构建LangMap基准：基于真实3D室内扫描，提供区域标签、区分性区域描述、覆盖414个对象类别的区分性实例描述。包含18K+导航任务，每个目标有简洁和详细两种描述。采用人类验证确保标注质量。

Result: LangMap在区分性准确率上比GOAT-Bench高23.8%，同时使用4倍少的词汇。评估显示：更丰富的上下文和记忆能提高成功率，但长尾、小型、上下文依赖、远距离目标以及多目标完成仍然具有挑战性。

Conclusion: HieraNav和LangMap为语言驱动的具身导航建立了严格的测试平台，能够评估模型在不同语义粒度和指令风格下的表现，揭示了当前模型的局限性和未来改进方向。

Abstract: The relationships between objects and language are fundamental to meaningful communication between humans and AI, and to practically useful embodied intelligence. We introduce HieraNav, a multi-granularity, open-vocabulary goal navigation task where agents interpret natural language instructions to reach targets at four semantic levels: scene, room, region, and instance. To this end, we present Language as a Map (LangMap), a large-scale benchmark built on real-world 3D indoor scans with comprehensive human-verified annotations and tasks spanning these levels. LangMap provides region labels, discriminative region descriptions, discriminative instance descriptions covering 414 object categories, and over 18K navigation tasks. Each target features both concise and detailed descriptions, enabling evaluation across different instruction styles. LangMap achieves superior annotation quality, outperforming GOAT-Bench by 23.8% in discriminative accuracy using four times fewer words. Comprehensive evaluations of zero-shot and supervised models on LangMap reveal that richer context and memory improve success, while long-tailed, small, context-dependent, and distant goals, as well as multi-goal completion, remain challenging. HieraNav and LangMap establish a rigorous testbed for advancing language-driven embodied navigation. Project: https://bo-miao.github.io/LangMap

</details>


### [297] [MIRROR: Manifold Ideal Reference ReconstructOR for Generalizable AI-Generated Image Detection](https://arxiv.org/abs/2602.02222)
*Ruiqi Liu,Manni Cui,Ziheng Qin,Zhiyuan Yan,Ruoxin Chen,Yi Han,Zhiheng Li,Junkai Chen,ZhiJin Chen,Kaiqing Lin,Jialiang Shen,Lubin Weng,Jing Dong,Yan Wang,Shu Wu*

Main category: cs.CV

Relevance: 65.0

TL;DR: MIRROR将AI生成图像检测重新定义为参考-比较问题，通过可学习的离散记忆库编码现实先验，生成流形一致的理想参考，利用残差作为检测信号，在多个基准测试中超越现有方法并接近人类感知极限。


<details>
  <summary>Details</summary>
Motivation: 现有AI生成图像检测器主要依赖基于伪影的分类方法，难以泛化到不断演化的生成痕迹。相比之下，人类判断依赖于稳定的现实世界规律，偏离人类认知流形可作为更通用的伪造信号。因此需要开发能够达到"超人交叉"水平、可替代人类专家的检测器。

Method: 提出MIRROR框架，将AIGI检测重新定义为参考-比较问题：1）使用可学习的离散记忆库编码现实先验；2）通过稀疏线性组合将输入投影到流形一致的理想参考；3）利用生成的残差作为鲁棒检测信号。同时引入Human-AIGI基准测试，包含心理物理学筛选的人类难以察觉的子集。

Result: 在14个基准测试中，MIRROR持续超越先前方法：在6个标准基准上提升2.1%，在7个野外基准上提升8.1%。在Human-AIGI上，MIRROR在27个生成器上达到89.6%准确率，超越普通用户和视觉专家，并随着预训练骨干网络规模扩大进一步接近人类感知极限。

Conclusion: 通过将AIGI检测重新定义为参考-比较问题，MIRROR框架能够编码现实先验并生成流形一致的理想参考，提供比传统基于伪影的方法更鲁棒和可泛化的检测信号，达到了接近人类专家水平的性能。

Abstract: High-fidelity generative models have narrowed the perceptual gap between synthetic and real images, posing serious threats to media security. Most existing AI-generated image (AIGI) detectors rely on artifact-based classification and struggle to generalize to evolving generative traces. In contrast, human judgment relies on stable real-world regularities, with deviations from the human cognitive manifold serving as a more generalizable signal of forgery. Motivated by this insight, we reformulate AIGI detection as a Reference-Comparison problem that verifies consistency with the real-image manifold rather than fitting specific forgery cues. We propose MIRROR (Manifold Ideal Reference ReconstructOR), a framework that explicitly encodes reality priors using a learnable discrete memory bank. MIRROR projects an input into a manifold-consistent ideal reference via sparse linear combination, and uses the resulting residuals as robust detection signals. To evaluate whether detectors reach the "superhuman crossover" required to replace human experts, we introduce the Human-AIGI benchmark, featuring a psychophysically curated human-imperceptible subset. Across 14 benchmarks, MIRROR consistently outperforms prior methods, achieving gains of 2.1% on six standard benchmarks and 8.1% on seven in-the-wild benchmarks. On Human-AIGI, MIRROR reaches 89.6% accuracy across 27 generators, surpassing both lay users and visual experts, and further approaching the human perceptual limit as pretrained backbones scale. The code is publicly available at: https://github.com/349793927/MIRROR

</details>


### [298] [Personalized Image Generation via Human-in-the-loop Bayesian Optimization](https://arxiv.org/abs/2602.02388)
*Rajalaxmi Rajagopalan,Debottam Dutta,Yu-Lin Wei,Romit Roy Choudhury*

Main category: cs.CV

Relevance: 65.0

TL;DR: 提出MultiBO方法，通过多轮用户偏好反馈来优化个性化图像生成，解决语言提示无法精确表达用户心中特定图像的问题。


<details>
  <summary>Details</summary>
Motivation: 用户心中可能有特定图像（如童年街道景色），但仅通过语言提示难以精确生成。虽然初始生成图像接近目标，但语言提示已无法进一步缩小差距。观察到人类仍能判断新图像是否更接近目标图像，因此利用这种偏好反馈来改进生成。

Method: 提出MultiBO（多选择偏好贝叶斯优化）方法：1）基于初始生成图像生成K个新图像；2）获取用户偏好反馈（哪个更接近目标）；3）利用反馈指导扩散模型；4）生成新一批K个图像。通过B轮用户反馈迭代优化。

Result: 30名用户的定性评分结合5个基线的定量指标显示，MultiBO能在有限反馈轮次内显著接近目标图像，尽管生成模型没有目标图像的直接信息。

Conclusion: 人类的多选择偏好反馈可有效用于个性化图像生成，解决语言提示的局限性，实现更精确的图像生成控制。

Abstract: Imagine Alice has a specific image $x^\ast$ in her mind, say, the view of the street in which she grew up during her childhood. To generate that exact image, she guides a generative model with multiple rounds of prompting and arrives at an image $x^{p*}$. Although $x^{p*}$ is reasonably close to $x^\ast$, Alice finds it difficult to close that gap using language prompts. This paper aims to narrow this gap by observing that even after language has reached its limits, humans can still tell when a new image $x^+$ is closer to $x^\ast$ than $x^{p*}$. Leveraging this observation, we develop MultiBO (Multi-Choice Preferential Bayesian Optimization) that carefully generates $K$ new images as a function of $x^{p*}$, gets preferential feedback from the user, uses the feedback to guide the diffusion model, and ultimately generates a new set of $K$ images. We show that within $B$ rounds of user feedback, it is possible to arrive much closer to $x^\ast$, even though the generative model has no information about $x^\ast$. Qualitative scores from $30$ users, combined with quantitative metrics compared across $5$ baselines, show promising results, suggesting that multi-choice feedback from humans can be effectively harnessed for personalized image generation.

</details>


### [299] [Infinite-World: Scaling Interactive World Models to 1000-Frame Horizons via Pose-Free Hierarchical Memory](https://arxiv.org/abs/2602.02393)
*Ruiqi Wu,Xuanhua He,Meng Cheng,Tianyu Yang,Yong Zhang,Zhuoliang Kang,Xunliang Cai,Xiaoming Wei,Chunle Guo,Chongyi Li,Ming-Ming Cheng*

Main category: cs.CV

Relevance: 65.0

TL;DR: Infinite-World提出了一种鲁棒的交互式世界模型，能够在复杂真实世界环境中维持超过1000帧的连贯视觉记忆，通过分层无姿态记忆压缩器和不确定性感知动作标注模块解决真实视频训练难题。


<details>
  <summary>Details</summary>
Motivation: 现有世界模型在合成数据上训练效果良好，但在真实世界视频中面临挑战：姿态估计噪声大、视角重访稀缺，缺乏有效的训练范式。需要开发能够在真实环境中保持长期视觉记忆的鲁棒世界模型。

Method: 1. 分层无姿态记忆压缩器(HPMC)：递归地将历史潜在表示蒸馏为固定预算表示，与生成主干联合优化，无需显式几何先验；2. 不确定性感知动作标注模块：将连续运动离散化为三态逻辑，最大化原始视频数据利用，避免噪声轨迹污染确定性动作空间；3. 重访密集微调策略：使用30分钟紧凑数据集激活模型的长距离环路闭合能力。

Result: 在客观指标和用户研究中，Infinite-World在视觉质量、动作可控性和空间一致性方面表现出优越性能，能够维持超过1000帧的连贯视觉记忆。

Conclusion: Infinite-World通过创新的记忆压缩和动作标注方法，成功解决了真实世界视频训练的关键挑战，为鲁棒交互式世界模型的发展提供了有效解决方案。

Abstract: We propose Infinite-World, a robust interactive world model capable of maintaining coherent visual memory over 1000+ frames in complex real-world environments. While existing world models can be efficiently optimized on synthetic data with perfect ground-truth, they lack an effective training paradigm for real-world videos due to noisy pose estimations and the scarcity of viewpoint revisits. To bridge this gap, we first introduce a Hierarchical Pose-free Memory Compressor (HPMC) that recursively distills historical latents into a fixed-budget representation. By jointly optimizing the compressor with the generative backbone, HPMC enables the model to autonomously anchor generations in the distant past with bounded computational cost, eliminating the need for explicit geometric priors. Second, we propose an Uncertainty-aware Action Labeling module that discretizes continuous motion into a tri-state logic. This strategy maximizes the utilization of raw video data while shielding the deterministic action space from being corrupted by noisy trajectories, ensuring robust action-response learning. Furthermore, guided by insights from a pilot toy study, we employ a Revisit-Dense Finetuning Strategy using a compact, 30-minute dataset to efficiently activate the model's long-range loop-closure capabilities. Extensive experiments, including objective metrics and user studies, demonstrate that Infinite-World achieves superior performance in visual quality, action controllability, and spatial consistency.

</details>


### [300] [Superman: Unifying Skeleton and Vision for Human Motion Perception and Generation](https://arxiv.org/abs/2602.02401)
*Xinshun Wang,Peiming Li,Ziyi Wang,Zhongbin Fang,Zhichao Deng,Songtao Wu,Jason Li,Mengyuan Liu*

Main category: cs.CV

Relevance: 65.0

TL;DR: Superman是一个统一框架，将视觉感知与基于骨架的时序运动生成相结合，通过视觉引导的运动分词器和统一MLLM架构解决现有运动分析任务碎片化问题。


<details>
  <summary>Details</summary>
Motivation: 当前运动分析领域存在严重碎片化：1) 感知模型（从视频理解运动但只输出文本）与生成模型（无法从原始视觉输入感知）分离；2) 生成式MLLM通常局限于使用密集参数化SMPL模型的单帧静态姿态，无法处理时序运动；3) 现有运动词汇仅基于骨架数据构建，与视觉领域脱节。

Method: 提出两阶段解决方案：1) 视觉引导运动分词器，利用3D骨架与视觉数据之间的自然几何对齐，从两种模态进行联合学习，创建统一的跨模态运动词汇；2) 基于该运动语言，训练单一统一MLLM架构处理所有任务，灵活处理多样化时序输入，统一从视频的3D骨架姿态估计（感知）与基于骨架的运动预测和插值（生成）。

Result: 在Human3.6M等标准基准测试上的广泛实验表明，该统一方法在所有运动任务上实现了最先进或具有竞争力的性能，展示了使用骨架进行生成式运动分析的高效可扩展路径。

Conclusion: Superman框架成功弥合了视觉感知与时序骨架运动生成之间的鸿沟，通过统一的跨模态方法解决了运动分析领域的碎片化问题，为生成式运动分析提供了更高效和可扩展的解决方案。

Abstract: Human motion analysis tasks, such as temporal 3D pose estimation, motion prediction, and motion in-betweening, play an essential role in computer vision. However, current paradigms suffer from severe fragmentation. First, the field is split between ``perception'' models that understand motion from video but only output text, and ``generation'' models that cannot perceive from raw visual input. Second, generative MLLMs are often limited to single-frame, static poses using dense, parametric SMPL models, failing to handle temporal motion. Third, existing motion vocabularies are built from skeleton data alone, severing the link to the visual domain. To address these challenges, we introduce Superman, a unified framework that bridges visual perception with temporal, skeleton-based motion generation. Our solution is twofold. First, to overcome the modality disconnect, we propose a Vision-Guided Motion Tokenizer. Leveraging the natural geometric alignment between 3D skeletons and visual data, this module pioneers robust joint learning from both modalities, creating a unified, cross-modal motion vocabulary. Second, grounded in this motion language, a single, unified MLLM architecture is trained to handle all tasks. This module flexibly processes diverse, temporal inputs, unifying 3D skeleton pose estimation from video (perception) with skeleton-based motion prediction and in-betweening (generation). Extensive experiments on standard benchmarks, including Human3.6M, demonstrate that our unified method achieves state-of-the-art or competitive performance across all motion tasks. This showcases a more efficient and scalable path for generative motion analysis using skeletons.

</details>


### [301] [Catalyst: Out-of-Distribution Detection via Elastic Scaling](https://arxiv.org/abs/2602.02409)
*Abid Hassan,Tuan Ngo,Saad Shafiq,Nenad Medvidovic*

Main category: cs.CV

Relevance: 65.0

TL;DR: Catalyst是一个后处理OOD检测框架，利用池化前特征图的原始通道统计信息，通过动态计算缩放因子γ来弹性调节基线OOD分数，显著提升现有方法的性能。


<details>
  <summary>Details</summary>
Motivation: 现有最先进的后处理OOD检测方法通常只使用输出logits或全局平均池化后的特征向量，忽视了池化前特征图的丰富原始通道统计信息（如均值、标准差、最大激活值），这些被丢弃的信号对OOD检测具有重要价值。

Method: Catalyst框架从池化前特征图的原始通道统计信息中动态计算输入依赖的缩放因子γ，然后将γ与现有基线OOD分数进行乘法融合（弹性缩放），从而将ID和OOD分布进一步分离。该方法可泛化地集成到logit-based方法（如Energy、ReAct、SCALE）和距离基检测器（如KNN）中。

Result: 在多个数据集上取得显著性能提升：CIFAR-10（ResNet-18）平均误报率降低32.87%，CIFAR-100（ResNet-18）降低27.94%，ImageNet（ResNet-50）降低22.25%。证明池化前统计信息的潜力，且与现有OOD检测方法互补。

Conclusion: 池化前特征图的原始通道统计信息是OOD检测中被忽视的宝贵信号，Catalyst框架通过弹性缩放机制有效利用这些信息，显著提升现有OOD检测方法的性能，具有很好的通用性和互补性。

Abstract: Out-of-distribution (OOD) detection is critical for the safe deployment of deep neural networks. State-of-the-art post-hoc methods typically derive OOD scores from the output logits or penultimate feature vector obtained via global average pooling (GAP). We contend that this exclusive reliance on the logit or feature vector discards a rich, complementary signal: the raw channel-wise statistics of the pre-pooling feature map lost in GAP. In this paper, we introduce Catalyst, a post-hoc framework that exploits these under-explored signals. Catalyst computes an input-dependent scaling factor ($γ$) on-the-fly from these raw statistics (e.g., mean, standard deviation, and maximum activation). This $γ$ is then fused with the existing baseline score, multiplicatively modulating it -- an ``elastic scaling'' -- to push the ID and OOD distributions further apart. We demonstrate Catalyst is a generalizable framework: it seamlessly integrates with logit-based methods (e.g., Energy, ReAct, SCALE) and also provides a significant boost to distance-based detectors like KNN. As a result, Catalyst achieves substantial and consistent performance gains, reducing the average False Positive Rate by 32.87 on CIFAR-10 (ResNet-18), 27.94% on CIFAR-100 (ResNet-18), and 22.25% on ImageNet (ResNet-50). Our results highlight the untapped potential of pre-pooling statistics and demonstrate that Catalyst is complementary to existing OOD detection approaches.

</details>


### [302] [RPP: A Certified Poisoned-Sample Detection Framework for Backdoor Attacks under Dataset Imbalance](https://arxiv.org/abs/2602.00183)
*Miao Lin,Feng Yu,Rui Ning,Lusi Li,Jiawei Chen,Qian Lou,Mengxin Zheng,Chunsheng Xin,Hongyi Wu*

Main category: cs.CR

Relevance: 65.0

TL;DR: 该论文首次深入研究了数据集不平衡如何放大后门漏洞，提出了RPP（随机概率扰动）框架，这是一个在黑盒设置下仅使用模型输出概率进行认证毒化样本检测的方法，在数据不平衡情况下显著优于现有防御方法。


<details>
  <summary>Details</summary>
Motivation: 深度神经网络极易受到后门攻击，但现有防御方法大多依赖平衡数据，忽视了现实世界中普遍存在的类别不平衡问题。数据集不平衡会放大后门威胁，导致多数类偏见增加脆弱性，而传统防御方法在数据不平衡情况下性能显著下降。

Method: 提出随机概率扰动（RPP）框架，这是一个认证毒化样本检测方法，仅使用模型输出概率在黑盒设置下工作。RPP通过扰动输入并观察输出概率的变化来检测后门操纵，提供可证明的域内检测保证和误报率的概率上界。

Result: 在五个基准数据集（MNIST、SVHN、CIFAR-10、TinyImageNet和ImageNet10）上进行了广泛实验，涵盖10种后门攻击和12种基线防御。RPP在检测准确率上显著优于最先进的防御方法，特别是在数据集不平衡的情况下表现优异。

Conclusion: RPP为在现实世界不平衡数据环境中防御后门攻击建立了理论和实践基础，填补了现有防御方法在数据不平衡场景下的空白，提供了可证明的安全保证。

Abstract: Deep neural networks are highly susceptible to backdoor attacks, yet most defense methods to date rely on balanced data, overlooking the pervasive class imbalance in real-world scenarios that can amplify backdoor threats. This paper presents the first in-depth investigation of how the dataset imbalance amplifies backdoor vulnerability, showing that (i) the imbalance induces a majority-class bias that increases susceptibility and (ii) conventional defenses degrade significantly as the imbalance grows. To address this, we propose Randomized Probability Perturbation (RPP), a certified poisoned-sample detection framework that operates in a black-box setting using only model output probabilities. For any inspected sample, RPP determines whether the input has been backdoor-manipulated, while offering provable within-domain detectability guarantees and a probabilistic upper bound on the false positive rate. Extensive experiments on five benchmarks (MNIST, SVHN, CIFAR-10, TinyImageNet and ImageNet10) covering 10 backdoor attacks and 12 baseline defenses show that RPP achieves significantly higher detection accuracy than state-of-the-art defenses, particularly under dataset imbalance. RPP establishes a theoretical and practical foundation for defending against backdoor attacks in real-world environments with imbalanced data.

</details>


### [303] [Cross-Modal Binary Attention: An Energy-Efficient Fusion Framework for Audio-Visual Learning](https://arxiv.org/abs/2602.00701)
*Mohamed Saleh,Zahra Ahmadi*

Main category: cs.MM

Relevance: 65.0

TL;DR: CMQKA是一种新型跨模态融合机制，通过高效二元操作实现线性复杂度，支持可扩展的层次化融合。基于CMQKA的SNNergy框架采用事件驱动的脉冲操作，在音频-视觉基准测试中达到最先进性能。


<details>
  <summary>Details</summary>
Motivation: 现有音频-视觉融合方法面临基本权衡：基于注意力的方法能有效建模跨模态关系但计算复杂度高，而高效融合策略依赖简单拼接无法提取互补信息。需要一种既能捕获复杂跨模态依赖又计算高效的机制。

Method: 提出CMQKA机制，通过双向跨模态查询-键注意力和可学习残差融合，在保持线性复杂度的同时提取互补时空特征。基于此构建SNNergy框架，采用层次化架构处理多尺度输入，使用事件驱动的二元脉冲操作实现高能效。

Result: 在CREMA-D、AVE和UrbanSound8K-AV等音频-视觉基准测试中显著超越现有多模态融合基线，建立新的最先进结果。框架在保持融合效果的同时实现了卓越的能效。

Conclusion: CMQKA和SNNergy通过引入可扩展的融合机制，实现了层次化跨模态集成，为实际音频-视觉智能系统提供了实用的能效解决方案，推动了多模态融合的发展。

Abstract: Effective multimodal fusion requires mechanisms that can capture complex cross-modal dependencies while remaining computationally scalable for real-world deployment. Existing audio-visual fusion approaches face a fundamental trade-off: attention-based methods effectively model cross-modal relationships but incur quadratic computational complexity that prevents hierarchical, multi-scale architectures, while efficient fusion strategies rely on simplistic concatenation that fails to extract complementary cross-modal information. We introduce CMQKA, a novel cross-modal fusion mechanism that achieves linear O(N) complexity through efficient binary operations, enabling scalable hierarchical fusion previously infeasible with conventional attention. CMQKA employs bidirectional cross-modal Query-Key attention to extract complementary spatiotemporal features and uses learnable residual fusion to preserve modality-specific characteristics while enriching representations with cross-modal information. Building upon CMQKA, we present SNNergy, an energy-efficient multimodal fusion framework with a hierarchical architecture that processes inputs through progressively decreasing spatial resolutions and increasing semantic abstraction. This multi-scale fusion capability allows the framework to capture both local patterns and global context across modalities. Implemented with event-driven binary spike operations, SNNergy achieves remarkable energy efficiency while maintaining fusion effectiveness and establishing new state-of-the-art results on challenging audio-visual benchmarks, including CREMA-D, AVE, and UrbanSound8K-AV, significantly outperforming existing multimodal fusion baselines. Our framework advances multimodal fusion by introducing a scalable fusion mechanism that enables hierarchical cross-modal integration with practical energy efficiency for real-world audio-visual intelligence systems.

</details>


### [304] [A texture-based framework for foundational ultrasound models](https://arxiv.org/abs/2602.01444)
*Tal Grutman,Carmel Shinar,Tali Ilovitsh*

Main category: eess.IV

Relevance: 65.0

TL;DR: 提出TUSA（纹理超声语义分析）方法，将自监督学习重新定义为纹理分析问题，针对超声图像特有的声学纹理特性进行建模，相比通用基础模型在医学超声任务上表现更好。


<details>
  <summary>Details</summary>
Motivation: 超声图像具有独特的声学纹理特征，与自然图像统计特性差异显著，导致基于自然图像设计的算法在超声应用中表现不佳。现有基础模型虽然被应用于超声领域，但仅是在超声数据上训练，缺乏对超声物理特性的专门设计，因此需要将超声特定领域知识整合到学习框架中。

Method: 提出纹理超声语义分析（TUSA）方法，将自监督学习重新定义为纹理分析问题。使用高度可扩展的对比学习方法直接从简单的B模式图像中提取真正的领域特定表示。模型在开源数据、模拟数据和体内数据的组合上进行训练。

Result: TUSA模型在潜在空间上比多个更大的基础模型表现更好，在独特在线数据集和临床眼部数据集上显示出更好的泛化能力。在检测COVID（70%）、脊柱血肿（100%）和玻璃体出血（97%）方面达到更高准确率，并与肝脏脂肪变性（r=0.83）、射血分数（r=0.63）和血氧饱和度（r=0.38）等定量参数有更好的相关性。

Conclusion: 通过将超声特定领域知识整合到学习框架中，TUSA方法能够更好地捕捉超声图像的声学纹理特征，相比通用基础模型在医学超声任务上表现更优，为医学影像分析提供了更有效的解决方案。

Abstract: Ultrasound is the most widely used medical imaging modality, yet the images it produces are fundamentally unique, arising from tissue-dependent scattering, reflection, and speed-of-sound variations that produce a constrained set of characteristic textures that differ markedly from natural-image statistics. These acoustically driven patterns make ultrasound challenging for algorithms originally designed for natural images. To bridge this gap, the field has increasingly turned to foundation models, hoping to leverage their generalization capabilities. However, these models often falter in ultrasound applications because they are not designed for ultrasound physics, they are merely trained on ultrasound data. Therefore, it is essential to integrate ultrasound-specific domain knowledge into established learning frameworks. We achieve this by reformulating self-supervised learning as a texture-analysis problem, introducing texture ultrasound semantic analysis (TUSA). Using TUSA, models learn to leverage highly scalable contrastive methods to extract true domain-specific representations directly from simple B-mode images. We train a TUSA model on a combination of open-source, simulated, and in vivo data. The latent space is compared to several larger foundation models, demonstrating that our approach gives TUSA models better generalizability for difficult downstream tasks on unique online datasets as well as a clinical eye dataset collected for this study. Our model achieves higher accuracy in detecting COVID (70%), spinal hematoma (100%) and vitreous hemorrhage (97%) and correlates more closely with quantitative parameters like liver steatosis (r = 0.83), ejection fraction (r = 0.63), and oxygen saturation (r = 0.38). We open-source the model weights and training script: https://github.com/talg2324/tusa

</details>


### [305] [UniDWM: Towards a Unified Driving World Model via Multifaceted Representation Learning](https://arxiv.org/abs/2602.01536)
*Shuai Liu,Siheng Ren,Xiaoyao Zhu,Quanmin Liang,Zefeng Li,Qiang Li,Xin Hu,Kai Huang*

Main category: cs.RO

Relevance: 65.0

TL;DR: UniDWM是一个统一的驾驶世界模型，通过多方面的表示学习构建结构和动态感知的潜在世界表示，作为物理基础的状态空间，实现感知、预测和规划的一致推理。


<details>
  <summary>Details</summary>
Motivation: 在复杂驾驶环境中实现可靠高效的规划需要一个能够推理场景几何、外观和动态的模型。现有方法往往在感知、预测和规划之间存在不一致性，需要统一的表示学习框架。

Method: 1. 构建结构和动态感知的潜在世界表示作为物理基础的状态空间；2. 联合重建路径学习恢复场景结构（几何和视觉纹理）；3. 协作生成框架利用条件扩散transformer在潜在空间中预测未来世界演化；4. 理论分析表明UniDWM可视为VAE的变体，为多方面表示学习提供理论指导。

Result: 在轨迹规划、4D重建和生成方面的广泛实验证明了UniDWM的有效性，突显了多方面世界表示作为统一驾驶智能基础的潜力。

Conclusion: UniDWM通过构建统一的驾驶世界模型，实现了感知、预测和规划的一致推理，为自动驾驶提供了有效的多方面表示学习框架。

Abstract: Achieving reliable and efficient planning in complex driving environments requires a model that can reason over the scene's geometry, appearance, and dynamics. We present UniDWM, a unified driving world model that advances autonomous driving through multifaceted representation learning. UniDWM constructs a structure- and dynamic-aware latent world representation that serves as a physically grounded state space, enabling consistent reasoning across perception, prediction, and planning. Specifically, a joint reconstruction pathway learns to recover the scene's structure, including geometry and visual texture, while a collaborative generation framework leverages a conditional diffusion transformer to forecast future world evolution within the latent space. Furthermore, we show that our UniDWM can be deemed as a variation of VAE, which provides theoretical guidance for the multifaceted representation learning. Extensive experiments demonstrate the effectiveness of UniDWM in trajectory planning, 4D reconstruction and generation, highlighting the potential of multifaceted world representations as a foundation for unified driving intelligence. The code will be publicly available at https://github.com/Say2L/UniDWM.

</details>


### [306] [RANKVIDEO: Reasoning Reranking for Text-to-Video Retrieval](https://arxiv.org/abs/2602.02444)
*Tyler Skow,Alexander Martin,Benjamin Van Durme,Rama Chellappa,Reno Kriz*

Main category: cs.IR

Relevance: 65.0

TL;DR: RANKVIDEO是一个基于推理的视频检索重排序模型，通过两阶段课程学习训练，在MultiVENT 2.0基准测试中显著提升检索性能。


<details>
  <summary>Details</summary>
Motivation: 当前文本检索的重排序已取得快速进展，但基于推理的视频检索重排序研究不足。现有方法主要依赖文本或视觉语言模型，缺乏对查询-视频对的显式推理能力。

Method: 1) 构建推理密集型查询-视频对的数据合成流水线；2) 两阶段课程学习：感知基础的监督微调 + 结合点对、配对和教师置信度蒸馏目标的重排序训练；3) 显式推理查询-视频对的相关性。

Result: 在MultiVENT 2.0基准测试中，RANKVIDEO在两阶段框架下平均提升31%的nDCG@10性能，优于纯文本和视觉语言重排序替代方案，同时更高效。

Conclusion: RANKVIDEO证明了基于推理的视频检索重排序的有效性，填补了该领域的研究空白，为视频检索系统提供了更强大的重排序组件。

Abstract: Reranking is a critical component of modern retrieval systems, which typically pair an efficient first-stage retriever with a more expressive model to refine results. While large reasoning models have driven rapid progress in text-centric reranking, reasoning-based reranking for video retrieval remains underexplored. To address this gap, we introduce RANKVIDEO, a reasoning-based reranker for video retrieval that explicitly reasons over query-video pairs using video content to assess relevance. RANKVIDEO is trained using a two-stage curriculum consisting of perception-grounded supervised fine-tuning followed by reranking training that combines pointwise, pairwise, and teacher confidence distillation objectives, and is supported by a data synthesis pipeline for constructing reasoning-intensive query-video pairs. Experiments on the large-scale MultiVENT 2.0 benchmark demonstrate that RANKVIDEO consistently improves retrieval performance within a two-stage framework, yielding an average improvement of 31% on nDCG@10 and outperforming text-only and vision-language reranking alternatives, while more efficient.

</details>


### [307] [Mirage2Matter: A Physically Grounded Gaussian World Model from Video](https://arxiv.org/abs/2602.00096)
*Zhengqing Gao,Ziwen Li,Xin Wang,Jiaxin Huang,Zhenyang Ren,Mingkai Shao,Hanlue Zhang,Tianyu Huang,Yongkang Cheng,Yandong Guo,Runqi Lin,Yuanyuan Wang,Tongliang Liu,Kun Zhang,Mingming Gong*

Main category: cs.CV

Relevance: 45.0

TL;DR: Simulate Anything：基于3D高斯泼溅和生成模型，从多视角视频重建高保真物理仿真环境，为具身智能训练提供可扩展的仿真数据生成框架


<details>
  <summary>Details</summary>
Motivation: 具身智能的可扩展性受到真实世界交互数据稀缺的限制。现有仿真平台存在视觉物理差距大、依赖昂贵传感器、需要精确标定等问题，难以大规模应用。

Method: 1. 使用3D高斯泼溅(3DGS)从多视角环境视频重建高保真场景表示；2. 利用生成模型恢复物理真实表示；3. 通过精度标定目标将重建场景集成到仿真环境，实现尺度对齐；4. 构建统一、可编辑、物理基础的世界模型。

Result: 在该仿真数据上训练的视觉语言动作(VLA)模型在下游任务中表现出强大的零样本性能，匹配甚至超越了使用真实世界数据得到的结果。

Conclusion: 重建驱动的世界建模为可扩展、实用的具身智能训练提供了潜力，能够有效解决真实交互数据稀缺的问题。

Abstract: The scalability of embodied intelligence is fundamentally constrained by the scarcity of real-world interaction data. While simulation platforms provide a promising alternative, existing approaches often suffer from a substantial visual and physical gap to real environments and rely on expensive sensors, precise robot calibration, or depth measurements, limiting their practicality at scale. We present Simulate Anything, a graphics-driven world modeling and simulation framework that enables efficient generation of high-fidelity embodied training data using only multi-view environment videos and off-the-shelf assets. Our approach reconstructs real-world environments into a photorealistic scene representation using 3D Gaussian Splatting (3DGS), seamlessly capturing fine-grained geometry and appearance from video. We then leverage generative models to recover a physically realistic representation and integrate it into a simulation environment via a precision calibration target, enabling accurate scale alignment between the reconstructed scene and the real world. Together, these components provide a unified, editable, and physically grounded world model. Vision Language Action (VLA) models trained on our simulated data achieve strong zero-shot performance on downstream tasks, matching or even surpassing results obtained with real-world data, highlighting the potential of reconstruction-driven world modeling for scalable and practical embodied intelligence training.

</details>


### [308] [SITUATE -- Synthetic Object Counting Dataset for VLM training](https://arxiv.org/abs/2602.00108)
*René Peinl,Vincent Tischler,Patrick Schröder,Christian Groth*

Main category: cs.CV

Relevance: 45.0

TL;DR: SITUATE是一个专为视觉语言模型设计的计数任务数据集，包含空间约束，旨在填补简单2D数据集和模糊真实数据集之间的空白，提升模型在分布外图像上的泛化能力。


<details>
  <summary>Details</summary>
Motivation: 现有计数数据集存在局限性：简单2D数据集（如VLMCountBench）缺乏真实复杂性，而真实数据集（如TallyQA）存在遮挡和空间组合模糊问题，缺乏可控性。需要一个新的数据集来训练和评估视觉语言模型在空间约束计数任务上的能力。

Method: 提出了SITUATE数据集，专门设计用于包含空间约束的计数任务。通过实验验证，在Qwen VL 2.5 7B模型上使用SITUATE进行微调，并与Pixmo count数据集进行对比实验，包括交叉验证和与其他计数基准的比较。

Result: 在SITUATE上微调的Qwen VL 2.5 7B模型在Pixmo count测试数据上准确率得到提升，但反之不成立。这表明SITUATE能有效提升模型在分布外图像上的泛化能力，且效果优于同等规模的Pixmo count微调集。

Conclusion: SITUATE数据集能有效提升视觉语言模型在空间约束计数任务上的性能，特别是在分布外图像上的泛化能力，为视觉语言模型的计数能力评估提供了更好的基准。

Abstract: We present SITUATE, a novel dataset designed for training and evaluating Vision Language Models on counting tasks with spatial constraints. The dataset bridges the gap between simple 2D datasets like VLMCountBench and often ambiguous real-life datasets like TallyQA, which lack control over occlusions and spatial composition. Experiments show that our dataset helps to improve generalization for out-of-distribution images, since a finetune of Qwen VL 2.5 7B on SITUATE improves accuracy on the Pixmo count test data, but not vice versa. We cross validate this by comparing the model performance across established other counting benchmarks and against an equally sized fine-tuning set derived from Pixmo count.

</details>


### [309] [Learning Physics-Grounded 4D Dynamics with Neural Gaussian Force Fields](https://arxiv.org/abs/2602.00148)
*Shiqian Li,Ruihong Shen,Junfeng Ni,Chang Pan,Chi Zhang,Yixin Zhu*

Main category: cs.CV

Relevance: 45.0

TL;DR: NGFF是一个端到端神经框架，将3D高斯感知与基于物理的动态建模相结合，从多视角RGB输入生成交互式、物理真实的4D视频，比现有高斯模拟器快两个数量级。


<details>
  <summary>Details</summary>
Motivation: 当前视频生成模型虽然视觉质量高，但缺乏物理定律建模，无法一致生成物理合理的视频。现有结合3D高斯溅射和物理引擎的方法计算成本高，在复杂现实场景中缺乏鲁棒性。

Method: 提出Neural Gaussian Force Field (NGFF)框架，集成3D高斯感知与物理动态建模。还创建了GSCollision数据集，包含64万多个渲染物理视频（约4TB），涵盖多种材料、多物体交互和复杂场景。

Result: 在合成和真实3D场景评估中，NGFF展现出强大的泛化能力和物理推理鲁棒性，比现有高斯模拟器快两个数量级。

Conclusion: NGFF推动了视频预测向物理基础世界模型的发展，为从原始视觉数据预测物理动态提供了有效解决方案。

Abstract: Predicting physical dynamics from raw visual data remains a major challenge in AI. While recent video generation models have achieved impressive visual quality, they still cannot consistently generate physically plausible videos due to a lack of modeling of physical laws. Recent approaches combining 3D Gaussian splatting and physics engines can produce physically plausible videos, but are hindered by high computational costs in both reconstruction and simulation, and often lack robustness in complex real-world scenarios. To address these issues, we introduce Neural Gaussian Force Field (NGFF), an end-to-end neural framework that integrates 3D Gaussian perception with physics-based dynamic modeling to generate interactive, physically realistic 4D videos from multi-view RGB inputs, achieving two orders of magnitude faster than prior Gaussian simulators. To support training, we also present GSCollision, a 4D Gaussian dataset featuring diverse materials, multi-object interactions, and complex scenes, totaling over 640k rendered physical videos (~4 TB). Evaluations on synthetic and real 3D scenarios show NGFF's strong generalization and robustness in physical reasoning, advancing video prediction towards physics-grounded world models.

</details>


### [310] [AI-Generated Image Detectors Overrely on Global Artifacts: Evidence from Inpainting Exchange](https://arxiv.org/abs/2602.00192)
*Elif Nebioglu,Emirhan Bilgiç,Adrian Popescu*

Main category: cs.CV

Relevance: 45.0

TL;DR: 论文揭示了当前图像修复检测器主要依赖全局伪影而非局部合成内容，提出INP-X操作分离修复效果，发现现有检测器在去除全局伪影后性能大幅下降，强调了内容感知检测的重要性。


<details>
  <summary>Details</summary>
Motivation: 现代基于深度学习的图像修复技术能够实现逼真的局部图像操作，这对可靠检测提出了关键挑战。作者观察到当前检测器主要依赖作为修复副作用的全局伪影，而非局部合成内容，这导致检测器可能被误导。

Method: 提出Inpainting Exchange (INP-X)操作，在保留所有合成内容的同时恢复编辑区域外的原始像素。创建包含9万张真实、修复和交换图像的测试数据集。提供理论分析，将检测器行为与VAE信息瓶颈引起的高频衰减联系起来。

Result: 在INP-X干预下，预训练的最先进检测器（包括商业检测器）准确率大幅下降（如从91%降至55%），经常接近随机水平。在作者数据集上训练的检测器比标准修复训练具有更好的泛化能力和定位能力。

Conclusion: 当前检测器过度依赖全局伪影而非局部合成内容，这暴露了现有方法的脆弱性。需要开发内容感知的检测方法，作者的数据集和INP-X操作为此提供了重要工具。

Abstract: Modern deep learning-based inpainting enables realistic local image manipulation, raising critical challenges for reliable detection. However, we observe that current detectors primarily rely on global artifacts that appear as inpainting side effects, rather than on locally synthesized content. We show that this behavior occurs because VAE-based reconstruction induces a subtle but pervasive spectral shift across the entire image, including unedited regions. To isolate this effect, we introduce Inpainting Exchange (INP-X), an operation that restores original pixels outside the edited region while preserving all synthesized content. We create a 90K test dataset including real, inpainted, and exchanged images to evaluate this phenomenon. Under this intervention, pretrained state-of-the-art detectors, including commercial ones, exhibit a dramatic drop in accuracy (e.g., from 91\% to 55\%), frequently approaching chance level. We provide a theoretical analysis linking this behavior to high-frequency attenuation caused by VAE information bottlenecks. Our findings highlight the need for content-aware detection. Indeed, training on our dataset yields better generalization and localization than standard inpainting. Our dataset and code are publicly available at https://github.com/emirhanbilgic/INP-X.

</details>


### [311] [PLACID: Identity-Preserving Multi-Object Compositing via Video Diffusion with Synthetic Trajectories](https://arxiv.org/abs/2602.00267)
*Gemma Canet Tarrés,Manel Baradad,Francesc Moreno-Noguer,Yumeng Li*

Main category: cs.CV

Relevance: 45.0

TL;DR: PLACID是一个利用预训练图像到视频扩散模型进行多目标合成的框架，通过视频时序先验保持目标一致性和背景细节，并采用合成数据训练策略实现随机初始目标向文本引导的连贯布局收敛。


<details>
  <summary>Details</summary>
Motivation: 当前生成式AI在工作室级别的多目标合成方面存在不足，无法同时满足：1）近乎完美的目标身份保持；2）精确的背景和颜色保真度；3）布局和设计元素控制；4）完整且吸引人的展示。现有模型常改变目标细节、遗漏或重复目标，产生尺寸比例错误或不一致的表现。

Method: 1）利用预训练的图像到视频扩散模型，通过文本控制来保持目标一致性、身份和背景细节，利用视频的时序先验；2）提出新颖的数据策展策略，生成合成序列，其中随机放置的目标平滑移动到目标位置，这些合成数据在训练期间与视频模型的时序先验对齐。

Result: 广泛的定量评估和用户研究表明，PLACID在多目标合成方面超越了最先进的方法，实现了更优越的身份、背景和颜色保持，减少了目标遗漏，并产生视觉上更吸引人的结果。

Conclusion: PLACID框架通过结合视频扩散模型的时序先验和合成数据训练策略，有效解决了多目标合成中的关键挑战，为高质量工作室级图像合成提供了新方法。

Abstract: Recent advances in generative AI have dramatically improved photorealistic image synthesis, yet they fall short for studio-level multi-object compositing. This task demands simultaneous (i) near-perfect preservation of each item's identity, (ii) precise background and color fidelity, (iii) layout and design elements control, and (iv) complete, appealing displays showcasing all objects. However, current state-of-the-art models often alter object details, omit or duplicate objects, and produce layouts with incorrect relative sizing or inconsistent item presentations. To bridge this gap, we introduce PLACID, a framework that transforms a collection of object images into an appealing multi-object composite. Our approach makes two main contributions. First, we leverage a pretrained image-to-video (I2V) diffusion model with text control to preserve objects consistency, identities, and background details by exploiting temporal priors from videos. Second, we propose a novel data curation strategy that generates synthetic sequences where randomly placed objects smoothly move to their target positions. This synthetic data aligns with the video model's temporal priors during training. At inference, objects initialized at random positions consistently converge into coherent layouts guided by text, with the final frame serving as the composite image. Extensive quantitative evaluations and user studies demonstrate that PLACID surpasses state-of-the-art methods in multi-object compositing, achieving superior identity, background, and color preservation, with less omitted objects and visually appealing results.

</details>


### [312] [Bridging the Semantic Chasm: Synergistic Conceptual Anchoring for Generalized Few-Shot and Zero-Shot OOD Perception](https://arxiv.org/abs/2602.00340)
*Alexandros Christoforos,Sarah Jenkins,Michael Brown,Tuan Pham,David Chen*

Main category: cs.CV

Relevance: 45.0

TL;DR: SynerNet框架通过多智能体协同解决VLMs在OOD概念上的跨模态对齐退化问题，在VISTA-Beyond基准上取得了1.2%-5.4%的性能提升。


<details>
  <summary>Details</summary>
Motivation: 解决视觉语言模型(VLMs)在面对分布外(OOD)概念时出现的跨模态对齐退化问题，这是当前多模态AI系统在实际应用中面临的重要挑战。

Method: 提出Synergistic Neural Agents Network框架，包含四个专门的计算单元：视觉感知、语言上下文、名义嵌入和全局协调，通过结构化消息传播协议协同工作，包括多智能体潜在空间命名获取框架、语义上下文交换算法和自适应动态平衡机制。

Result: 在VISTA-Beyond基准测试中，SynerNet在few-shot和zero-shot场景下都取得了显著性能提升，精度提高范围在1.2%到5.4%之间，覆盖多个领域。

Conclusion: SynerNet框架有效缓解了VLMs在OOD概念上的跨模态对齐退化问题，为多模态AI系统的鲁棒性和适应性提供了新的解决方案。

Abstract: This manuscript presents a pioneering Synergistic Neural Agents Network (SynerNet) framework designed to mitigate the phenomenon of cross-modal alignment degeneration in Vision-Language Models (VLMs) when encountering Out-of-Distribution (OOD) concepts. Specifically, four specialized computational units - visual perception, linguistic context, nominal embedding, and global coordination - collaboratively rectify modality disparities via a structured message-propagation protocol. The principal contributions encompass a multi-agent latent space nomenclature acquisition framework, a semantic context-interchange algorithm for enhanced few-shot adaptation, and an adaptive dynamic equilibrium mechanism. Empirical evaluations conducted on the VISTA-Beyond benchmark demonstrate that SynerNet yields substantial performance augmentations in both few-shot and zero-shot scenarios, exhibiting precision improvements ranging from 1.2% to 5.4% across a diverse array of domains.

</details>


### [313] [MASC: Metal-Aware Sampling and Correction via Reinforcement Learning for Accelerated MRI](https://arxiv.org/abs/2602.00348)
*Zhengyi Lu,Ming Lu,Chongyu Qu,Junchao Zhu,Junlin Guo,Marilyn Lionts,Yanfan Zhu,Yuechen Yang,Tianyuan Yao,Jayasai Rajagopal,Bennett Allan Landman,Xiao Wang,Xinqiang Yan,Yuankai Huo*

Main category: cs.CV

Relevance: 45.0

TL;DR: MASC是一个统一的强化学习框架，用于联合优化金属感知的k空间采样和伪影校正，以加速MRI扫描并减少金属植入物引起的伪影。


<details>
  <summary>Details</summary>
Motivation: 传统方法将金属伪影减少（MAR）和加速MRI采集作为两个独立问题处理。MRI中的金属植入物会导致严重伪影，降低图像质量并阻碍临床诊断。本研究旨在通过联合优化采样和校正来解决这一问题。

Method: 1) 使用基于物理的模拟构建配对MRI数据集，生成有/无金属植入物的k空间数据和重建；2) 将主动MRI采集建模为序列决策问题，使用基于PPO的伪影感知代理学习在有限采集预算下选择k空间相位编码线；3) 提出端到端训练方案，其中采集策略学习选择最能支持伪影去除的k空间线，同时MAR网络适应由此产生的欠采样模式。

Result: 实验表明MASC的学习策略优于传统采样策略，端到端训练相比使用冻结预训练MAR网络提高了性能，验证了联合优化的优势。在FastMRI上的跨数据集实验进一步证实了向真实临床MRI数据的泛化能力。

Conclusion: MASC通过统一的强化学习框架成功实现了金属感知k空间采样和伪影校正的联合优化，为加速MRI扫描中的金属伪影问题提供了有效的解决方案。

Abstract: Metal implants in MRI cause severe artifacts that degrade image quality and hinder clinical diagnosis. Traditional approaches address metal artifact reduction (MAR) and accelerated MRI acquisition as separate problems. We propose MASC, a unified reinforcement learning framework that jointly optimizes metal-aware k-space sampling and artifact correction for accelerated MRI. To enable supervised training, we construct a paired MRI dataset using physics-based simulation, generating k-space data and reconstructions for phantoms with and without metal implants. This paired dataset provides simulated 3D MRI scans with and without metal implants, where each metal-corrupted sample has an exactly matched clean reference, enabling direct supervision for both artifact reduction and acquisition policy learning. We formulate active MRI acquisition as a sequential decision-making problem, where an artifact-aware Proximal Policy Optimization (PPO) agent learns to select k-space phase-encoding lines under a limited acquisition budget. The agent operates on undersampled reconstructions processed through a U-Net-based MAR network, learning patterns that maximize reconstruction quality. We further propose an end-to-end training scheme where the acquisition policy learns to select k-space lines that best support artifact removal while the MAR network simultaneously adapts to the resulting undersampling patterns. Experiments demonstrate that MASC's learned policies outperform conventional sampling strategies, and end-to-end training improves performance compared to using a frozen pre-trained MAR network, validating the benefit of joint optimization. Cross-dataset experiments on FastMRI with physics-based artifact simulation further confirm generalization to realistic clinical MRI data. The code and models of MASC have been made publicly available: https://github.com/hrlblab/masc

</details>


### [314] [MRAD: Zero-Shot Anomaly Detection with Memory-Driven Retrieval](https://arxiv.org/abs/2602.00522)
*Chaoran Xu,Chengkan Lv,Qiyu Chen,Feng Zhang,Zhengtao Zhang*

Main category: cs.CV

Relevance: 45.0

TL;DR: 提出MRAD框架，通过直接记忆检索而非参数拟合进行零样本异常检测，包含无需训练的MRAD-TF和两个轻量变体MRAD-FT、MRAD-CLIP，在16个工业医疗数据集上表现优异。


<details>
  <summary>Details</summary>
Motivation: 现有零样本异常检测方法多使用提示学习或复杂建模拟合数据分布，导致训练/推理成本高且跨域稳定性有限。需要更高效稳定的方法。

Method: 提出记忆检索异常检测框架：1) MRAD-TF冻结CLIP编码器，构建图像级和像素级记忆库存储特征-标签对；2) MRAD-FT微调检索度量增强判别性；3) MRAD-CLIP将正常/异常区域先验注入CLIP文本提示增强泛化。

Result: 在16个工业和医疗数据集上，MRAD框架在异常分类和分割任务中均表现优异，在无需训练和基于训练的设置下都展现强大性能。

Conclusion: 充分利用原始数据的经验分布而非仅依赖模型拟合，可以实现更强的异常检测性能。记忆检索方法为高效零样本异常检测提供了新思路。

Abstract: Zero-shot anomaly detection (ZSAD) often leverages pretrained vision or vision-language models, but many existing methods use prompt learning or complex modeling to fit the data distribution, resulting in high training or inference cost and limited cross-domain stability. To address these limitations, we propose Memory-Retrieval Anomaly Detection method (MRAD), a unified framework that replaces parametric fitting with a direct memory retrieval. The train-free base model, MRAD-TF, freezes the CLIP image encoder and constructs a two-level memory bank (image-level and pixel-level) from auxiliary data, where feature-label pairs are explicitly stored as keys and values. During inference, anomaly scores are obtained directly by similarity retrieval over the memory bank. Based on the MRAD-TF, we further propose two lightweight variants as enhancements: (i) MRAD-FT fine-tunes the retrieval metric with two linear layers to enhance the discriminability between normal and anomaly; (ii) MRAD-CLIP injects the normal and anomalous region priors from the MRAD-FT as dynamic biases into CLIP's learnable text prompts, strengthening generalization to unseen categories. Across 16 industrial and medical datasets, the MRAD framework consistently demonstrates superior performance in anomaly classification and segmentation, under both train-free and training-based settings. Our work shows that fully leveraging the empirical distribution of raw data, rather than relying only on model fitting, can achieve stronger anomaly detection performance. The code will be publicly released at https://github.com/CROVO1026/MRAD.

</details>


### [315] [VIZOR: Viewpoint-Invariant Zero-Shot Scene Graph Generation for 3D Scene Reasoning](https://arxiv.org/abs/2602.00637)
*Vivek Madhavaram,Vartika Sengar,Arkadipta De,Charu Sharma*

Main category: cs.CV

Relevance: 45.0

TL;DR: VIZOR是一个无需训练、端到端的框架，直接从原始3D场景构建密集、视角不变的3D场景图，通过基于物体正面方向定义空间关系来实现视角不变性，并在零样本物体定位任务上优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有3D场景理解方法通常需要多模态输入（如2D图像、深度图、物体标签等），并且依赖于特定参考视角，导致空间关系（如"左/右"）在不同视角下不一致，泛化能力有限。

Method: 提出VIZOR框架：1）直接从原始3D场景构建场景图，无需训练；2）将空间关系定义为相对于每个物体正面方向，实现视角不变性；3）推断开放词汇关系，无需标注训练数据；4）支持密集场景图生成。

Result: 在场景图生成和下游任务（如查询式物体定位）上进行了广泛评估。VIZOR在场景图生成方面优于现有方法，在Replica和Nr3D数据集上的零样本定位准确率分别提升了22%和4.81%。

Conclusion: VIZOR通过视角不变的空间关系定义和无需训练的方法，解决了现有3D场景图生成方法的泛化问题，在零样本场景理解和推理任务上表现出色。

Abstract: Scene understanding and reasoning has been a fundamental problem in 3D computer vision, requiring models to identify objects, their properties, and spatial or comparative relationships among the objects. Existing approaches enable this by creating scene graphs using multiple inputs such as 2D images, depth maps, object labels, and annotated relationships from specific reference view. However, these methods often struggle with generalization and produce inaccurate spatial relationships like "left/right", which become inconsistent across different viewpoints. To address these limitations, we propose Viewpoint-Invariant Zero-shot scene graph generation for 3D scene Reasoning (VIZOR). VIZOR is a training-free, end-to-end framework that constructs dense, viewpoint-invariant 3D scene graphs directly from raw 3D scenes. The generated scene graph is unambiguous, as spatial relationships are defined relative to each object's front-facing direction, making them consistent regardless of the reference view. Furthermore, it infers open-vocabulary relationships that describe spatial and proximity relationships among scene objects without requiring annotated training data. We conduct extensive quantitative and qualitative evaluations to assess the effectiveness of VIZOR in scene graph generation and downstream tasks, such as query-based object grounding. VIZOR outperforms state-of-the-art methods, showing clear improvements in scene graph generation and achieving 22% and 4.81% gains in zero-shot grounding accuracy on the Replica and Nr3D datasets, respectively.

</details>


### [316] [Refining Context-Entangled Content Segmentation via Curriculum Selection and Anti-Curriculum Promotion](https://arxiv.org/abs/2602.01183)
*Chunming He,Rihan Zhang,Fengyang Xiao,Dingming Zhang,Zhiwen Cao,Sina Farsiu*

Main category: cs.CV

Relevance: 45.0

TL;DR: CurriSeg：一种受生物学启发的双阶段学习框架，通过课程选择和反课程提升策略解决上下文纠缠内容分割问题，无需增加参数或训练时间即可提升分割鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 受到生物学从易到难学习过程的启发，针对上下文纠缠内容分割（CECS）这一挑战性问题，传统分割网络主要依赖架构改进而忽略了学习动态对鲁棒性的影响。

Method: 提出CurriSeg双阶段框架：1）课程选择阶段基于样本损失的时间统计动态选择训练数据，区分困难但信息丰富的样本与噪声/模糊样本；2）反课程提升阶段设计频谱盲区微调，抑制高频成分以增强对低频结构和上下文线索的依赖。

Result: 在多个CECS基准测试中取得一致改进，无需增加参数或总训练时间，展示了如何通过渐进和挑战的相互作用培养鲁棒且上下文感知的分割能力。

Conclusion: CurriSeg提供了一种原则性视角，说明如何通过渐进和挑战的相互作用来培养鲁棒且上下文感知的分割能力，为处理上下文纠缠内容分割问题提供了有效解决方案。

Abstract: Biological learning proceeds from easy to difficult tasks, gradually reinforcing perception and robustness. Inspired by this principle, we address Context-Entangled Content Segmentation (CECS), a challenging setting where objects share intrinsic visual patterns with their surroundings, as in camouflaged object detection. Conventional segmentation networks predominantly rely on architectural enhancements but often ignore the learning dynamics that govern robustness under entangled data distributions. We introduce CurriSeg, a dual-phase learning framework that unifies curriculum and anti-curriculum principles to improve representation reliability. In the Curriculum Selection phase, CurriSeg dynamically selects training data based on the temporal statistics of sample losses, distinguishing hard-but-informative samples from noisy or ambiguous ones, thus enabling stable capability enhancement. In the Anti-Curriculum Promotion phase, we design Spectral-Blindness Fine-Tuning, which suppresses high-frequency components to enforce dependence on low-frequency structural and contextual cues and thus strengthens generalization. Extensive experiments demonstrate that CurriSeg achieves consistent improvements across diverse CECS benchmarks without adding parameters or increasing total training time, offering a principled view of how progression and challenge interplay to foster robust and context-aware segmentation. Code will be released.

</details>


### [317] [EMFormer: Efficient Multi-Scale Transformer for Accumulative Context Weather Forecasting](https://arxiv.org/abs/2602.01194)
*Hao Chen,Tao Han,Jie Zhang,Song Guo,Fenghua Ling,Lei Bai*

Main category: cs.CV

Relevance: 45.0

TL;DR: 提出EMFormer架构和累积上下文微调方法，用于提升长期天气预报性能，同时减少计算开销，并在视觉基准上展示良好泛化能力。


<details>
  <summary>Details</summary>
Motivation: 长期天气预报对社会经济规划和灾害准备至关重要。现有方法通过微调扩展预测范围，但仍受限于灾难性遗忘、误差累积和高训练开销等问题。

Method: 1) 提出高效多尺度Transformer（EMFormer），通过单次卷积提取多尺度特征；2) 采用累积上下文微调提升时间一致性而不降低短期精度；3) 提出复合损失函数，通过正弦加权动态平衡不同项。

Result: 在天气预报和极端事件预测中取得强劲性能，显著提升长期预测精度。EMFormer在ImageNet-1K和ADE20K视觉基准上展示良好泛化能力，相比传统多尺度模块实现5.69倍加速。

Conclusion: 提出的跨预训练、微调和预测的完整流程有效解决了长期天气预报中的关键挑战，在保持计算效率的同时提升了预测性能。

Abstract: Long-term weather forecasting is critical for socioeconomic planning and disaster preparedness. While recent approaches employ finetuning to extend prediction horizons, they remain constrained by the issues of catastrophic forgetting, error accumulation, and high training overhead. To address these limitations, we present a novel pipeline across pretraining, finetuning and forecasting to enhance long-context modeling while reducing computational overhead. First, we introduce an Efficient Multi-scale Transformer (EMFormer) to extract multi-scale features through a single convolution in both training and inference. Based on the new architecture, we further employ an accumulative context finetuning to improve temporal consistency without degrading short-term accuracy. Additionally, we propose a composite loss that dynamically balances different terms via a sinusoidal weighting, thereby adaptively guiding the optimization trajectory throughout pretraining and finetuning. Experiments show that our approach achieves strong performance in weather forecasting and extreme event prediction, substantially improving long-term forecast accuracy. Moreover, EMFormer demonstrates strong generalization on vision benchmarks (ImageNet-1K and ADE20K) while delivering a 5.69x speedup over conventional multi-scale modules.

</details>


### [318] [ReDiStory: Region-Disentangled Diffusion for Consistent Visual Story Generation](https://arxiv.org/abs/2602.01303)
*Ayushman Sarkar,Zhenyu Yu,Chu Chen,Wei Tang,Kangning Cui,Mohd Yamani Idna Idris*

Main category: cs.CV

Relevance: 45.0

TL;DR: ReDiStory：一种无需训练的推理时提示嵌入重组框架，通过分解身份相关和帧特定组件并去相关帧嵌入，改善多帧故事生成中的身份一致性


<details>
  <summary>Details</summary>
Motivation: 生成连贯视觉故事需要在多图像中保持主体身份同时保留帧特定语义。现有无训练方法将身份和帧提示连接为统一表示，但在复杂故事中常引入帧间语义干扰，削弱身份保持能力

Method: ReDiStory框架在推理时分解文本嵌入为身份相关和帧特定组件，通过抑制跨帧共享方向来去相关帧嵌入，减少跨帧干扰而不修改扩散参数或需要额外监督

Result: 在ConsiStory+基准测试中，相比1Prompt1Story方法，ReDiStory在多个身份一致性指标上取得一致提升，同时保持提示保真度

Conclusion: 通过推理时提示嵌入重组，ReDiStory有效改善多帧故事生成中的身份一致性，减少跨帧语义干扰，为视觉故事生成提供了一种无需训练的高效解决方案

Abstract: Generating coherent visual stories requires maintaining subject identity across multiple images while preserving frame-specific semantics. Recent training-free methods concatenate identity and frame prompts into a unified representation, but this often introduces inter-frame semantic interference that weakens identity preservation in complex stories. We propose ReDiStory, a training-free framework that improves multi-frame story generation via inference-time prompt embedding reorganization. ReDiStory explicitly decomposes text embeddings into identity-related and frame-specific components, then decorrelates frame embeddings by suppressing shared directions across frames. This reduces cross-frame interference without modifying diffusion parameters or requiring additional supervision. Under identical diffusion backbones and inference settings, ReDiStory improves identity consistency while maintaining prompt fidelity. Experiments on the ConsiStory+ benchmark show consistent gains over 1Prompt1Story on multiple identity consistency metrics. Code is available at: https://github.com/YuZhenyuLindy/ReDiStory

</details>


### [319] [Federated Vision Transformer with Adaptive Focal Loss for Medical Image Classification](https://arxiv.org/abs/2602.01633)
*Xinyuan Zhao,Yihang Wu,Ahmad Chaddad,Tareef Daqqaq,Reem Kateb*

Main category: cs.CV

Relevance: 45.0

TL;DR: 提出一个联邦学习框架，结合动态自适应焦点损失(DAFL)和客户端感知聚合策略，解决医疗图像分类中的数据异质性和类别不平衡问题。


<details>
  <summary>Details</summary>
Motivation: 在数据隐私法规限制下，医疗图像等敏感数据难以集中收集。联邦学习虽然能保护隐私，但面临客户端数据异质性和类别不平衡的挑战，影响模型泛化能力。

Method: 1) 设计动态类别不平衡系数，根据客户端样本分布调整，确保少数类别得到足够关注；2) 采用加权聚合策略，适应数据规模和特征，捕捉客户端间差异。

Result: 在ISIC、Ocular Disease和RSNA-ICH三个公开数据集上，相比DenseNet121、ResNet50、ViT-S/16、ViT-L/32、FedCLIP、Swin Transformer、CoAtNet和MixNet等模型，准确率提升0.98%到41.69%。

Conclusion: 提出的联邦学习框架能有效处理医疗图像分类中的数据异质性和类别不平衡问题，在多个数据集上优于现有方法。

Abstract: While deep learning models like Vision Transformer (ViT) have achieved significant advances, they typically require large datasets. With data privacy regulations, access to many original datasets is restricted, especially medical images. Federated learning (FL) addresses this challenge by enabling global model aggregation without data exchange. However, the heterogeneity of the data and the class imbalance that exist in local clients pose challenges for the generalization of the model. This study proposes a FL framework leveraging a dynamic adaptive focal loss (DAFL) and a client-aware aggregation strategy for local training. Specifically, we design a dynamic class imbalance coefficient that adjusts based on each client's sample distribution and class data distribution, ensuring minority classes receive sufficient attention and preventing sparse data from being ignored. To address client heterogeneity, a weighted aggregation strategy is adopted, which adapts to data size and characteristics to better capture inter-client variations. The classification results on three public datasets (ISIC, Ocular Disease and RSNA-ICH) show that the proposed framework outperforms DenseNet121, ResNet50, ViT-S/16, ViT-L/32, FedCLIP, Swin Transformer, CoAtNet, and MixNet in most cases, with accuracy improvements ranging from 0.98\% to 41.69\%. Ablation studies on the imbalanced ISIC dataset validate the effectiveness of the proposed loss function and aggregation strategy compared to traditional loss functions and other FL approaches. The codes can be found at: https://github.com/AIPMLab/ViT-FLDAF.

</details>


### [320] [Tail-Aware Post-Training Quantization for 3D Geometry Models](https://arxiv.org/abs/2602.01741)
*Sicheng Pan,Chen Tang,Shuzhao Xie,Ke Yang,Weixiang Zhang,Jiawei Li,Bin Chen,Shu-Tao Xia,Zhi Wang*

Main category: cs.CV

Relevance: 45.0

TL;DR: TAPTQ：一种针对3D几何学习的尾部感知后训练量化方法，通过渐进式校准构建、三元搜索优化和尾部误差引导的模块补偿，显著提升3D模型的量化精度并减少校准时间。


<details>
  <summary>Details</summary>
Motivation: 3D几何模型在资源受限平台部署面临挑战，传统为2D视觉Transformer设计的后训练量化方法无法有效迁移到3D模型，因为3D模型具有复杂的特征分布和过高的校准开销。

Method: 提出TAPTQ三阶段方法：1) 渐进式粗到细校准构建策略，解决3D数据集规模瓶颈；2) 将量化区间搜索重构为优化问题，使用三元搜索求解器将复杂度从O(N)降至O(logN)；3) 尾部相对误差引导的模块补偿，自适应识别和修正对长尾激活异常值敏感的模块。

Result: 在VGGT和Pi3基准测试中，TAPTQ在精度上持续优于最先进的后训练量化方法，同时显著减少校准时间。

Conclusion: TAPTQ为3D几何学习提供了一种高效的后训练量化解决方案，解决了传统方法在3D模型上的局限性，实现了精度和效率的双重提升。

Abstract: The burgeoning complexity and scale of 3D geometry models pose significant challenges for deployment on resource-constrained platforms. While Post-Training Quantization (PTQ) enables efficient inference without retraining, conventional methods, primarily optimized for 2D Vision Transformers, fail to transfer effectively to 3D models due to intricate feature distributions and prohibitive calibration overhead. To address these challenges, we propose TAPTQ, a Tail-Aware Post-Training Quantization pipeline specifically engineered for 3D geometric learning. Our contribution is threefold: (1) To overcome the data-scale bottleneck in 3D datasets, we develop a progressive coarse-to-fine calibration construction strategy that constructs a highly compact subset to achieve both statistical purity and geometric representativeness. (2) We reformulate the quantization interval search as an optimization problem and introduce a ternary-search-based solver, reducing the computational complexity from $\mathcal{O}(N)$ to $\mathcal{O}(\log N)$ for accelerated deployment. (3) To mitigate quantization error accumulation, we propose TRE-Guided Module-wise Compensation, which utilizes a Tail Relative Error (TRE) metric to adaptively identify and rectify distortions in modules sensitive to long-tailed activation outliers. Extensive experiments on the VGGT and Pi3 benchmarks demonstrate that TAPTQ consistently outperforms state-of-the-art PTQ methods in accuracy while significantly reducing calibration time. The code will be released soon.

</details>


### [321] [Efficient Cross-Country Data Acquisition Strategy for ADAS via Street-View Imagery](https://arxiv.org/abs/2602.01836)
*Yin Wu,Daniel Slieter,Carl Esselborn,Ahmed Abouelazm,Tsung Yuan Tseng,J. Marius Zöllner*

Main category: cs.CV

Relevance: 45.0

TL;DR: 该论文提出了一种基于街景图像引导的数据采集策略，用于解决自动驾驶系统在不同国家部署时的领域偏移问题，通过视觉基础模型和视觉语言模型识别兴趣点，显著减少了目标领域数据需求。


<details>
  <summary>Details</summary>
Motivation: 部署ADAS和ADS系统在不同国家面临挑战，主要由于立法、交通基础设施和视觉惯例的差异导致领域偏移，降低感知性能。传统跨国家数据采集依赖大量道路驾驶，成本高且效率低，难以识别代表性位置。

Method: 提出街景引导的数据采集策略：1）使用KNN特征距离方法（基于视觉基础模型）和视觉归因方法（基于视觉语言模型）两种POI评分方法；2）采用收集-检测协议构建共定位数据集，将Zenseact开放数据集与Mapillary街景图像配对；3）在交通标志检测任务上进行实验验证。

Result: 实验表明，该方法在使用仅一半目标领域数据的情况下，性能与随机采样相当。提供了完整国家分析的成本估算，证明大规模街景处理在经济上可行。交通标志检测任务对跨国家标志外观变化特别敏感，验证了方法的有效性。

Conclusion: 街景引导的数据采集策略为高效、经济有效的跨国家模型适应提供了潜力，能够显著减少数据采集成本，同时保持感知性能。

Abstract: Deploying ADAS and ADS across countries remains challenging due to differences in legislation, traffic infrastructure, and visual conventions, which introduce domain shifts that degrade perception performance. Traditional cross-country data collection relies on extensive on-road driving, making it costly and inefficient to identify representative locations. To address this, we propose a street-view-guided data acquisition strategy that leverages publicly available imagery to identify places of interest (POI). Two POI scoring methods are introduced: a KNN-based feature distance approach using a vision foundation model, and a visual-attribution approach using a vision-language model. To enable repeatable evaluation, we adopt a collect-detect protocol and construct a co-located dataset by pairing the Zenseact Open Dataset with Mapillary street-view images. Experiments on traffic sign detection, a task particularly sensitive to cross-country variations in sign appearance, show that our approach achieves performance comparable to random sampling while using only half of the target-domain data. We further provide cost estimations for full-country analysis, demonstrating that large-scale street-view processing remains economically feasible. These results highlight the potential of street-view-guided data acquisition for efficient and cost-effective cross-country model adaptation.

</details>


### [322] [Fact or Fake? Assessing the Role of Deepfake Detectors in Multimodal Misinformation Detection](https://arxiv.org/abs/2602.01854)
*A S M Sharifuzzaman Sagar,Mohammed Bennamoun,Farid Boussaid,Naeha Sharif,Lian Xu,Shaaban Sahmoud,Ali Kishk*

Main category: cs.CV

Relevance: 45.0

TL;DR: 该论文系统分析了深度伪造检测器在多模态虚假信息检测中的作用，发现像素级检测器对图像-文本声明验证贡献有限，甚至可能损害基于证据的推理性能。


<details>
  <summary>Details</summary>
Motivation: 多模态虚假信息中的欺骗通常不仅来自图像像素级篡改，更来自图像-文本对共同表达的语义和上下文声明。然而，大多数深度伪造检测器仅检测像素级伪造，未考虑声明级含义，尽管它们越来越多地集成到自动事实核查管道中。这引发了一个核心科学和实践问题：像素级检测器是否对验证图像-文本声明提供有用信号，还是引入了误导性的真实性先验，破坏了基于证据的推理？

Method: 使用两个互补基准MMFakeBench和DGM4，评估：(1) 最先进的仅图像深度伪造检测器，(2) 基于证据的事实核查系统（通过蒙特卡洛树搜索进行工具引导检索，通过多智能体辩论进行审慎推理），(3) 将检测器输出作为辅助证据注入的混合事实核查系统。

Result: 深度伪造检测器独立价值有限，在MMFakeBench上F1分数为0.26-0.53，在DGM4上为0.33-0.49。将检测器预测纳入事实核查管道会持续降低性能0.04-0.08 F1，因为非因果真实性假设。相比之下，基于证据的事实核查系统达到最高性能，在MMFakeBench上F1约0.81，DGM4上约0.55。

Conclusion: 多模态声明验证主要由语义理解和外部证据驱动，像素级伪影信号不能可靠地增强对真实世界图像-文本虚假信息的推理。像素级检测器在自动事实核查管道中可能引入误导性先验，损害基于证据的推理。

Abstract: In multimodal misinformation, deception usually arises not just from pixel-level manipulations in an image, but from the semantic and contextual claim jointly expressed by the image-text pair. Yet most deepfake detectors, engineered to detect pixel-level forgeries, do not account for claim-level meaning, despite their growing integration in automated fact-checking (AFC) pipelines. This raises a central scientific and practical question: Do pixel-level detectors contribute useful signal for verifying image-text claims, or do they instead introduce misleading authenticity priors that undermine evidence-based reasoning? We provide the first systematic analysis of deepfake detectors in the context of multimodal misinformation detection. Using two complementary benchmarks, MMFakeBench and DGM4, we evaluate: (1) state-of-the-art image-only deepfake detectors, (2) an evidence-driven fact-checking system that performs tool-guided retrieval via Monte Carlo Tree Search (MCTS) and engages in deliberative inference through Multi-Agent Debate (MAD), and (3) a hybrid fact-checking system that injects detector outputs as auxiliary evidence. Results across both benchmark datasets show that deepfake detectors offer limited standalone value, achieving F1 scores in the range of 0.26-0.53 on MMFakeBench and 0.33-0.49 on DGM4, and that incorporating their predictions into fact-checking pipelines consistently reduces performance by 0.04-0.08 F1 due to non-causal authenticity assumptions. In contrast, the evidence-centric fact-checking system achieves the highest performance, reaching F1 scores of approximately 0.81 on MMFakeBench and 0.55 on DGM4. Overall, our findings demonstrate that multimodal claim verification is driven primarily by semantic understanding and external evidence, and that pixel-level artifact signals do not reliably enhance reasoning over real-world image-text misinformation.

</details>


### [323] [Visual Affect Analysis: Predicting Emotions of Image Viewers with Vision-Language Models](https://arxiv.org/abs/2602.00123)
*Filip Nowicki,Hubert Marciniak,Jakub Łączkowski,Krzysztof Jassem,Tomasz Górecki,Vimala Balakrishnan,Desmond C. Ong,Maciej Behnke*

Main category: cs.HC

Relevance: 45.0

TL;DR: 该研究评估了9个视觉语言模型在情感图像数据集上的表现，发现它们在离散情感分类任务上表现良好（准确率60-80%），但在连续情感评分预测上存在偏差，特别是在唤醒度方面表现较弱，且倾向于高估情感强度。


<details>
  <summary>Details</summary>
Motivation: 视觉语言模型（VLMs）作为大规模推断视觉刺激情感的工具显示出潜力，但目前尚不清楚它们的输出与人类情感评分的一致性程度。研究旨在系统评估VLMs在情感计算任务上的表现，为心理健康相关应用提供参考。

Method: 在三个经过心理测量验证的情感图像数据集（IAPS、NAPS、LAI-GAI）上，对9个VLMs（包括最先进的专有模型和开源模型）进行零样本评估。任务包括：(1) 顶级情感分类（选择图像引发的最强离散情感），(2) 在1-7/9李克特量表上连续预测人类评分。还评估了基于评分者条件提示对LAI-GAI数据集的影响。

Result: 离散情感分类表现良好：6类情感标签准确率60-80%，12类任务准确率60-75%。愤怒和惊讶预测准确率最低。连续评分预测与人类有中度到强相关性（r > 0.75），但存在一致偏差：唤醒度表现较弱，倾向于高估响应强度。评分者条件提示仅带来小而不一致的预测变化。

Conclusion: VLMs能够捕捉广泛的情感趋势，但缺乏经过验证的心理评分的细微差别。这凸显了它们在情感计算和心理健康相关应用中的潜力和当前局限性。

Abstract: Vision-language models (VLMs) show promise as tools for inferring affect from visual stimuli at scale; it is not yet clear how closely their outputs align with human affective ratings. We benchmarked nine VLMs, ranging from state-of-the-art proprietary models to open-source models, on three psycho-metrically validated affective image datasets: the International Affective Picture System, the Nencki Affective Picture System, and the Library of AI-Generated Affective Images. The models performed two tasks in the zero-shot setting: (i) top-emotion classification (selecting the strongest discrete emotion elicited by an image) and (ii) continuous prediction of human ratings on 1-7/9 Likert scales for discrete emotion categories and affective dimensions. We also evaluated the impact of rater-conditioned prompting on the LAI-GAI dataset using de-identified participant metadata. The results show good performance in discrete emotion classification, with accuracies typically ranging from 60% to 80% on six-emotion labels and from 60% to 75% on a more challenging 12-category task. The predictions of anger and surprise had the lowest accuracy in all datasets. For continuous rating prediction, models showed moderate to strong alignment with humans (r > 0.75) but also exhibited consistent biases, notably weaker performance on arousal, and a tendency to overestimate response strength. Rater-conditioned prompting resulted in only small, inconsistent changes in predictions. Overall, VLMs capture broad affective trends but lack the nuance found in validated psychological ratings, highlighting their potential and current limitations for affective computing and mental health-related applications.

</details>


### [324] [Modeling Image-Caption Rating from Comparative Judgments](https://arxiv.org/abs/2602.00381)
*Kezia Minni,Qiang Zhang,Monoshiz Mahbub Khan,Zhe Yu*

Main category: cs.CV

Relevance: 40.0

TL;DR: 该论文提出了一种基于比较学习的框架，通过建模人类对图像描述的比较判断而非直接评分，来减少人工标注成本并有效学习人类偏好。


<details>
  <summary>Details</summary>
Motivation: 直接评估图像描述准确性耗时且主观，而人类更容易比较两个描述哪个更匹配图像。因此需要开发能建模这种比较判断的机器学习方法，以降低人工标注成本。

Method: 提出比较学习框架，使用VICR数据集，用ResNet-50提取视觉特征，MiniLM提取文本特征，训练比较学习模型和回归模型进行对比。

Result: 回归模型表现更好（Pearson's ρ: 0.7609，Spearman's r_s: 0.7089），但比较学习模型随数据增加稳步提升，接近回归基线。人工评估显示比较标注更快且标注者间一致性更高。

Conclusion: 比较学习能有效建模人类偏好，同时显著降低人工标注成本，为图像描述评估提供了更高效的替代方案。

Abstract: Rating the accuracy of captions in describing images is time-consuming and subjective for humans. In contrast, it is often easier for people to compare two captions and decide which one better matches a given image. In this work, we propose a machine learning framework that models such comparative judgments instead of direct ratings. The model can then be applied to rank unseen image-caption pairs in the same way as a regression model trained on direct ratings. Using the VICR dataset, we extract visual features with ResNet-50 and text features with MiniLM, then train both a regression model and a comparative learning model. While the regression model achieves better performance (Pearson's $ρ$: 0.7609 and Spearman's $r_s$: 0.7089), the comparative learning model steadily improves with more data and approaches the regression baseline. In addition, a small-scale human evaluation study comparing absolute rating, pairwise comparison, and same-image comparison shows that comparative annotation yields faster results and has greater agreement among human annotators. These results suggest that comparative learning can effectively model human preferences while significantly reducing the cost of human annotations.

</details>


### [325] [Distill3R: A Pipeline for Democratizing 3D Foundation Models on Commodity Hardware](https://arxiv.org/abs/2602.00865)
*Brandon Leblanc,Charalambos Poullis*

Main category: cs.CV

Relevance: 40.0

TL;DR: Distill3R：一种将3D基础模型的几何推理能力蒸馏到可在单工作站上训练的紧凑学生模型的框架，实现9倍参数减少和5倍推理加速


<details>
  <summary>Details</summary>
Motivation: 当前多视图3D重建转向大规模基础模型，但这些模型依赖大规模计算集群训练，为学术实验室设置了高门槛。为了解决计算资源不平等问题，需要开发能在单工作站上训练的高效模型。

Method: 1. 离线缓存管道：通过压缩监督信号将繁重的教师推理与训练循环解耦；2. 置信感知蒸馏损失：利用教师不确定性实现商品硬件上的训练；3. 提出7200万参数学生模型，相比6.5亿参数教师模型大幅精简。

Result: 学生模型实现9倍参数减少和5倍推理加速，可在单工作站3天内完成训练（教师模型需要大规模GPU集群训练一周）。学生模型保持了结构一致性和定性几何理解能力。

Conclusion: Distill3R为3D视觉研究提供了民主化的探索入口和高效边缘部署方案，旨在为缺乏大规模计算资源的实验室提供可访问的研究基线，而非与最先进基础模型竞争。

Abstract: While multi-view 3D reconstruction has shifted toward large-scale foundation models capable of inferring globally consistent geometry, their reliance on massive computational clusters for training has created a significant barrier to entry for most academic laboratories. To bridge this compute divide, we introduce Distill3R, a framework designed to distill the geometric reasoning of 3D foundation models into compact students fully trainable on a single workstation. Our methodology centers on two primary innovations: (1) an offline caching pipeline that decouples heavy teacher inference from the training loop through compressed supervision signals, and (2) a confidence-aware distillation loss that leverages teacher uncertainty to enable training on commodity hardware. We propose a 72M-parameter student model which achieves a 9x reduction in parameters and a 5x inference speedup compared to its 650M-parameter teacher. The student is fully trainable in under 3 days on a single workstation, whereas its teacher requires massive GPU clusters for up to a week. We demonstrate that the student preserves the structural consistency and qualitative geometric understanding required for functional 3D awareness. By providing a reproducible, single-workstation training recipe, Distill3R serves as an exploratory entry point for democratized 3D vision research and efficient edge deployment. This work is not intended to compete with state-of-the-art foundation models, but to provide an accessible research baseline for laboratories without access to large-scale compute to train and specialize models on their own domain-specific data at minimal cost.

</details>


### [326] [Trust but Verify: Adaptive Conditioning for Reference-Based Diffusion Super-Resolution via Implicit Reference Correlation Modeling](https://arxiv.org/abs/2602.01864)
*Yuan Wang,Yuhao Wan,Siming Zheng,Bo Li,Qibin Hou,Peng-Tao Jiang*

Main category: cs.CV

Relevance: 40.0

TL;DR: Ada-RefSR：基于"信任但验证"原则的单步扩散框架，通过自适应隐式关联门控机制，在参考图像与低质量输入相关性可靠时利用参考信息，不可靠时抑制参考信息，解决参考图像超分辨率中的幻觉问题。


<details>
  <summary>Details</summary>
Motivation: 现有基于参考图像的扩散超分辨率方法面临两个问题：1）真实世界退化使得低质量输入与参考图像之间的对应关系不可靠；2）现有方法要么忽略相关性，要么依赖脆弱的显式匹配，导致要么过度依赖误导性参考，要么未能充分利用有价值的线索。

Method: 提出Ada-RefSR框架，核心是自适应隐式关联门控（AICG），使用可学习的摘要令牌来提取主导参考模式并捕获与低质量特征的隐式相关性。该机制集成到注意力骨干网络中，提供轻量级的自适应参考引导调节，作为防止错误融合的内置安全机制。

Result: 在多个数据集上的实验表明，Ada-RefSR在保真度、自然度和效率之间实现了良好平衡，同时在变化的参考对齐情况下保持鲁棒性。

Conclusion: Ada-RefSR通过自适应控制参考信息的使用，有效解决了参考图像超分辨率中的幻觉问题，为扩散模型在图像恢复任务中的可靠性提供了新思路。

Abstract: Recent works have explored reference-based super-resolution (RefSR) to mitigate hallucinations in diffusion-based image restoration. A key challenge is that real-world degradations make correspondences between low-quality (LQ) inputs and reference (Ref) images unreliable, requiring adaptive control of reference usage. Existing methods either ignore LQ-Ref correlations or rely on brittle explicit matching, leading to over-reliance on misleading references or under-utilization of valuable cues. To address this, we propose Ada-RefSR, a single-step diffusion framework guided by a "Trust but Verify" principle: reference information is leveraged when reliable and suppressed otherwise. Its core component, Adaptive Implicit Correlation Gating (AICG), employs learnable summary tokens to distill dominant reference patterns and capture implicit correlations with LQ features. Integrated into the attention backbone, AICG provides lightweight, adaptive regulation of reference guidance, serving as a built-in safeguard against erroneous fusion. Experiments on multiple datasets demonstrate that Ada-RefSR achieves a strong balance of fidelity, naturalness, and efficiency, while remaining robust under varying reference alignment.

</details>


### [327] [SSI-DM: Singularity Skipping Inversion of Diffusion Models](https://arxiv.org/abs/2602.02193)
*Chen Min,Enze Jiang,Jishen Peng,Zheng Ma*

Main category: cs.CV

Relevance: 40.0

TL;DR: 提出SSI-DM方法，通过跳过数学奇点区域解决扩散模型反演中的非高斯噪声问题，实现更好的图像编辑效果


<details>
  <summary>Details</summary>
Motivation: 现有扩散模型反演方法在早期噪声化步骤中存在不准确问题，导致生成非高斯噪声和编辑性差。研究发现根本原因是数学奇点使反演问题本质上不适定。

Method: 提出SSI-DM方法，通过在标准反演前添加少量噪声来绕过奇点区域。这种简单方法能产生具有自然高斯特性的反演噪声，同时保持重建保真度。

Result: 在公共图像数据集上，该方法在重建和插值任务中表现出优越性能，为扩散模型反演提供了原则性高效解决方案。

Conclusion: SSI-DM作为一种即插即用技术，与通用扩散模型兼容，通过解决数学奇点问题显著改善了扩散模型反演的质量和编辑性。

Abstract: Inverting real images into the noise space is essential for editing tasks using diffusion models, yet existing methods produce non-Gaussian noise with poor editability due to the inaccuracy in early noising steps. We identify the root cause: a mathematical singularity that renders inversion fundamentally ill-posed. We propose Singularity Skipping Inversion of Diffusion Models (SSI-DM), which bypasses this singular region by adding small noise before standard inversion. This simple approach produces inverted noise with natural Gaussian properties while maintaining reconstruction fidelity. As a plug-and-play technique compatible with general diffusion models, our method achieves superior performance on public image datasets for reconstruction and interpolation tasks, providing a principled and efficient solution to diffusion model inversion.

</details>


### [328] [FD-VLA: Force-Distilled Vision-Language-Action Model for Contact-Rich Manipulation](https://arxiv.org/abs/2602.02142)
*Ruiteng Zhao,Wenshuo Wang,Yicheng Ma,Xiaocong Li,Francis E. H. Tay,Marcelo H. Ang,Haiyue Zhu*

Main category: cs.RO

Relevance: 40.0

TL;DR: FD-VLA框架通过力蒸馏模块将力感知集成到视觉-语言-动作系统中，无需物理力传感器，在接触丰富的操作任务中实现精细感知和灵巧操作


<details>
  <summary>Details</summary>
Motivation: 力感知对于视觉-语言-动作框架至关重要，但许多机器人缺乏昂贵或易损的力传感器。需要一种无需物理传感器就能实现力感知的方法，以降低硬件成本和复杂性

Method: 提出力蒸馏模块，将可学习的查询令牌映射到预测的力令牌，该令牌与实际力信号的潜在表示对齐。在推理时，将蒸馏的力令牌注入预训练的VLM中，实现力感知推理

Result: 物理实验表明，蒸馏的力令牌优于直接传感器力测量和其他基线方法，证明了力蒸馏VLA方法的有效性

Conclusion: FD-VLA框架成功将力感知集成到VLA系统中，无需物理传感器，提高了跨模态对齐和感知-动作鲁棒性，适用于广泛的机器人平台

Abstract: Force sensing is a crucial modality for Vision-Language-Action (VLA) frameworks, as it enables fine-grained perception and dexterous manipulation in contact-rich tasks. We present Force-Distilled VLA (FD-VLA), a novel framework that integrates force awareness into contact-rich manipulation without relying on physical force sensors. The core of our approach is a Force Distillation Module (FDM), which distills force by mapping a learnable query token, conditioned on visual observations and robot states, into a predicted force token aligned with the latent representation of actual force signals. During inference, this distilled force token is injected into the pretrained VLM, enabling force-aware reasoning while preserving the integrity of its vision-language semantics. This design provides two key benefits: first, it allows practical deployment across a wide range of robots that lack expensive or fragile force-torque sensors, thereby reducing hardware cost and complexity; second, the FDM introduces an additional force-vision-state fusion prior to the VLM, which improves cross-modal alignment and enhances perception-action robustness in contact-rich scenarios. Surprisingly, our physical experiments show that the distilled force token outperforms direct sensor force measurements as well as other baselines, which highlights the effectiveness of this force-distilled VLA approach.

</details>


### [329] [HYPE-EDIT-1: Benchmark for Measuring Reliability in Frontier Image Editing Models](https://arxiv.org/abs/2602.00105)
*Wing Chan,Richard Allen*

Main category: cs.CV

Relevance: 35.0

TL;DR: HYPE-EDIT-1是一个包含100个任务的基准测试，用于评估基于参考的图像编辑模型在营销/设计场景中的实际表现，通过多次尝试和人工审核来衡量真实工作流程的成本效益。


<details>
  <summary>Details</summary>
Motivation: 当前图像编辑模型的公开演示通常展示最佳案例，但实际工作流程需要多次尝试和人工审核，这增加了真实成本。作者希望建立一个更贴近实际应用的评估基准。

Method: 创建了100个基于参考的营销/设计编辑任务，每个任务生成10个独立输出，使用二元通过/失败判断。计算每次尝试通过率、pass@10、重试限制下的预期尝试次数，以及结合模型价格和人工审核时间的有效成本。

Result: 评估模型的每次尝试通过率在34-83%之间，每个成功编辑的有效成本在0.66-1.42美元之间。低单次定价的模型在考虑重试和人工审核的总成本后反而更昂贵。

Conclusion: 仅看单次图像定价会误导实际成本评估，必须考虑重试率和人工审核时间。HYPE-EDIT-1基准提供了更全面的图像编辑模型评估框架。

Abstract: Public demos of image editing models are typically best-case samples; real workflows pay for retries and review time. We introduce HYPE-EDIT-1, a 100-task benchmark of reference-based marketing/design edits with binary pass/fail judging. For each task we generate 10 independent outputs to estimate per-attempt pass rate, pass@10, expected attempts under a retry cap, and an effective cost per successful edit that combines model price with human review time. We release 50 public tasks and maintain a 50-task held-out private split for server-side evaluation, plus a standardized JSON schema and tooling for VLM and human-based judging. Across the evaluated models, per-attempt pass rates span 34-83 percent and effective cost per success spans USD 0.66-1.42. Models that have low per-image pricing are more expensive when you consider the total effective cost of retries and human reviews.

</details>


### [330] [AI-Driven Three-Dimensional Reconstruction and Quantitative Analysis for Burn Injury Assessment](https://arxiv.org/abs/2602.00113)
*S. Kalaycioglu,C. Hong,K. Zhai,H. Xie,J. N. Wong*

Main category: cs.CV

Relevance: 35.0

TL;DR: 开发了一个AI驱动的烧伤评估平台，通过多视角摄影测量和3D重建实现客观、可重复的烧伤评估，支持纵向追踪和治疗决策。


<details>
  <summary>Details</summary>
Motivation: 传统烧伤评估依赖主观视觉检查和2D摄影，缺乏客观性、可重复性和纵向比较能力，需要更精确的几何感知评估方法。

Method: 整合多视角摄影测量、3D表面重建和深度学习分割，使用消费级相机采集多角度图像，重建患者特异性3D烧伤表面，计算客观几何指标。

Result: 系统能稳定重建、一致计算指标，展示临床合理的纵向趋势，支持客观、几何感知的烧伤评估和决策支持。

Conclusion: 该平台为急性和门诊护理提供了可扩展、非侵入性的客观烧伤评估方法，支持治疗规划和愈合监测。

Abstract: Accurate, reproducible burn assessment is critical for treatment planning, healing monitoring, and medico-legal documentation, yet conventional visual inspection and 2D photography are subjective and limited for longitudinal comparison. This paper presents an AI-enabled burn assessment and management platform that integrates multi-view photogrammetry, 3D surface reconstruction, and deep learning-based segmentation within a structured clinical workflow. Using standard multi-angle images from consumer-grade cameras, the system reconstructs patient-specific 3D burn surfaces and maps burn regions onto anatomy to compute objective metrics in real-world units, including surface area, TBSA, depth-related geometric proxies, and volumetric change. Successive reconstructions are spatially aligned to quantify healing progression over time, enabling objective tracking of wound contraction and depth reduction. The platform also supports structured patient intake, guided image capture, 3D analysis and visualization, treatment recommendations, and automated report generation. Simulation-based evaluation demonstrates stable reconstructions, consistent metric computation, and clinically plausible longitudinal trends, supporting a scalable, non-invasive approach to objective, geometry-aware burn assessment and decision support in acute and outpatient care.

</details>


### [331] [1S-DAug: One-Shot Data Augmentation for Robust Few-Shot Generalization](https://arxiv.org/abs/2602.00114)
*Yunwei Bai,Ying Kiat Tan,Yao Shu,Tsuhan Chen*

Main category: cs.CV

Relevance: 35.0

TL;DR: 1S-DAug是一种单样本生成增强算子，通过结合几何扰动、受控噪声注入和基于原始图像的降噪扩散过程，从单个测试图像生成多样且忠实的变体，用于提升少样本学习性能。


<details>
  <summary>Details</summary>
Motivation: 传统测试时增强方法在少样本学习中效果不佳，因为只有极少数标注样本可用。需要一种能够从单个测试图像生成多样且忠实变体的方法，以增强少样本学习的泛化能力。

Method: 1S-DAug将传统几何扰动与受控噪声注入相结合，使用基于原始图像的条件降噪扩散过程。生成的图像被编码并与原始图像聚合为组合表示，用于更鲁棒的少样本学习预测。

Result: 在4个不同数据集的标准基准测试中，1S-DAug无需模型参数更新即可持续改进少样本学习性能，在miniImagenet 5-way-1-shot基准上实现了超过10%的比例准确率提升。

Conclusion: 1S-DAug作为一种无需训练、模型无关的插件，能够有效提升少样本学习的泛化能力，证明了从单个测试图像生成多样化增强的有效性。

Abstract: Few-shot learning (FSL) challenges model generalization to novel classes based on just a few shots of labeled examples, a testbed where traditional test-time augmentations fail to be effective. We introduce 1S-DAug, a one-shot generative augmentation operator that synthesizes diverse yet faithful variants from just one example image at test time. 1S-DAug couples traditional geometric perturbations with controlled noise injection and a denoising diffusion process conditioned on the original image. The generated images are then encoded and aggregated, alongside the original image, into a combined representation for more robust FSL predictions. Integrated as a training-free model-agnostic plugin, 1S-DAug consistently improves FSL across standard benchmarks of 4 different datasets without any model parameter update, including achieving over 10% proportional accuracy improvement on the miniImagenet 5-way-1-shot benchmark. Codes will be released.

</details>


### [332] [Context-Aware Autoencoders for Anomaly Detection in Maritime Surveillance](https://arxiv.org/abs/2602.00124)
*Divya Acharya,Pierre Bernab'e,Antoine Chevrot,Helge Spieker,Arnaud Gotlieb,Bruno Legeard*

Main category: cs.CV

Relevance: 35.0

TL;DR: 提出上下文感知自编码器用于海上船舶异常检测，通过整合特定上下文阈值提高检测精度并降低计算成本，在时间序列异常检测中表现优于传统方法


<details>
  <summary>Details</summary>
Motivation: 传统自编码器在海上船舶异常检测中存在局限性，特别是在识别集体和上下文异常方面，因为海上异常通常依赖于船舶特定上下文（来自AIS消息）。需要一种能更好处理上下文依赖异常的方法。

Method: 提出上下文感知自编码器，整合特定上下文阈值。比较了四种上下文感知自编码器变体和传统自编码器，以海上监控中的捕鱼状态异常为案例研究。

Result: 结果显示上下文对重构损失和异常检测有显著影响。上下文感知自编码器在时间序列数据异常检测中表现优于其他方法，提高了检测精度。

Conclusion: 通过整合特定上下文阈值并认识到上下文在异常检测中的重要性，该方法为提高海上船舶交通监控系统的准确性提供了有前景的解决方案。

Abstract: The detection of anomalies is crucial to ensuring the safety and security of maritime vessel traffic surveillance. Although autoencoders are popular for anomaly detection, their effectiveness in identifying collective and contextual anomalies is limited, especially in the maritime domain, where anomalies depend on vessel-specific contexts derived from self-reported AIS messages. To address these limitations, we propose a novel solution: the context-aware autoencoder. By integrating context-specific thresholds, our method improves detection accuracy and reduces computational cost. We compare four context-aware autoencoder variants and a conventional autoencoder using a case study focused on fishing status anomalies in maritime surveillance. Results demonstrate the significant impact of context on reconstruction loss and anomaly detection. The context-aware autoencoder outperforms others in detecting anomalies in time series data. By incorporating context-specific thresholds and recognizing the importance of context in anomaly detection, our approach offers a promising solution to improve accuracy in maritime vessel traffic surveillance systems.

</details>


### [333] [PovNet+: A Deep Learning Architecture for Socially Assistive Robots to Learn and Assist with Multiple Activities of Daily Living](https://arxiv.org/abs/2602.00131)
*Fraser Robinson,Souren Pashangpour,Matthew Lisondra,Goldie Nejat*

Main category: cs.CV

Relevance: 35.0

TL;DR: POVNet+：首个用于社交辅助机器人的多模态深度学习架构，通过ADL和运动嵌入空间识别已知ADL、未见ADL和异常执行ADL，以主动启动辅助交互。


<details>
  <summary>Details</summary>
Motivation: 自主社交辅助机器人长期部署的主要障碍是无法同时感知和协助多项日常生活活动（ADL）。现有系统缺乏识别多种ADL、未见ADL及异常执行ADL的能力，限制了机器人在真实场景中的主动辅助能力。

Method: 提出多模态深度学习架构POVNet+，引入ADL嵌入空间和运动嵌入空间来区分：已知ADL、未见ADL、已知ADL异常执行。应用新颖的用户状态估计方法到运动嵌入空间，在监控用户表现的同时识别新ADL。使用ADL感知信息主动启动机器人辅助交互。

Result: 与最先进的人类活动识别方法相比，POVNet+具有更高的ADL分类准确率。在杂乱生活环境中与社交辅助机器人Leia进行的人机交互实验表明，该架构能成功识别不同已知和未见ADL，以及异常执行的ADL，并启动适当的人机辅助交互。

Conclusion: POVNet+是首个能够识别多种ADL、未见ADL和异常执行ADL的多模态架构，为社交辅助机器人提供了更全面的ADL感知能力，使其能够在真实场景中主动启动适当的辅助交互。

Abstract: A significant barrier to the long-term deployment of autonomous socially assistive robots is their inability to both perceive and assist with multiple activities of daily living (ADLs). In this paper, we present the first multimodal deep learning architecture, POVNet+, for multi-activity recognition for socially assistive robots to proactively initiate assistive behaviors. Our novel architecture introduces the use of both ADL and motion embedding spaces to uniquely distinguish between a known ADL being performed, a new unseen ADL, or a known ADL being performed atypically in order to assist people in real scenarios. Furthermore, we apply a novel user state estimation method to the motion embedding space to recognize new ADLs while monitoring user performance. This ADL perception information is used to proactively initiate robot assistive interactions. Comparison experiments with state-of-the-art human activity recognition methods show our POVNet+ method has higher ADL classification accuracy. Human-robot interaction experiments in a cluttered living environment with multiple users and the socially assistive robot Leia using POVNet+ demonstrate the ability of our multi-modal ADL architecture in successfully identifying different seen and unseen ADLs, and ADLs being performed atypically, while initiating appropriate assistive human-robot interactions.

</details>


### [334] [Shedding the Facades, Connecting the Domains: Detecting Shifting Multimodal Hate Video with Test-Time Adaptation](https://arxiv.org/abs/2602.00132)
*Jiao Li,Jian Lang,Xikai Tang,Wenzheng Shu,Ting Zhong,Qiang Gao,Yong Wang,Leiting Chen,Fan Zhou*

Main category: cs.CV

Relevance: 35.0

TL;DR: SCANNER是一个专门针对仇恨视频检测的测试时自适应框架，通过利用仇恨内容中稳定的核心特征（如性别、种族等）作为源域和目标域之间的桥梁，解决语义漂移问题。


<details>
  <summary>Details</summary>
Motivation: 仇恨内容经常演变成不规则和模糊的形式以逃避审查，导致严重的语义漂移，使现有模型失效。传统测试时自适应方法针对温和的分布偏移，难以处理仇恨视频检测中的严重语义漂移。

Method: 1. 通过质心引导对齐机制从模糊布局中揭示稳定核心特征；2. 加入样本级自适应质心对齐策略缓解异常样本影响；3. 引入簇内多样性正则化防止语义崩溃。

Result: 实验表明SCANNER在所有基线方法中表现最佳，平均Macro-F1比最佳基线提升4.69%。

Conclusion: SCANNER是首个专门针对仇恨视频检测的测试时自适应框架，通过利用仇恨内容的稳定核心特征有效解决了语义漂移问题。

Abstract: Hate Video Detection (HVD) is crucial for online ecosystems. Existing methods assume identical distributions between training (source) and inference (target) data. However, hateful content often evolves into irregular and ambiguous forms to evade censorship, resulting in substantial semantic drift and rendering previously trained models ineffective. Test-Time Adaptation (TTA) offers a solution by adapting models during inference to narrow the cross-domain gap, while conventional TTA methods target mild distribution shifts and struggle with the severe semantic drift in HVD. To tackle these challenges, we propose SCANNER, the first TTA framework tailored for HVD. Motivated by the insight that, despite the evolving nature of hateful manifestations, their underlying cores remain largely invariant (i.e., targeting is still based on characteristics like gender, race, etc), we leverage these stable cores as a bridge to connect the source and target domains. Specifically, SCANNER initially reveals the stable cores from the ambiguous layout in evolving hateful content via a principled centroid-guided alignment mechanism. To alleviate the impact of outlier-like samples that are weakly correlated with centroids during the alignment process, SCANNER enhances the prior by incorporating a sample-level adaptive centroid alignment strategy, promoting more stable adaptation. Furthermore, to mitigate semantic collapse from overly uniform outputs within clusters, SCANNER introduces an intra-cluster diversity regularization that encourages the cluster-wise semantic richness. Experiments show that SCANNER outperforms all baselines, with an average gain of 4.69% in Macro-F1 over the best.

</details>


### [335] [SDCM: Simulated Densifying and Compensatory Modeling Fusion for Radar-Vision 3-D Object Detection in Internet of Vehicles](https://arxiv.org/abs/2602.00149)
*Shucong Li,Xiaoluo Zhou,Yuqian He,Zhenyu Liu*

Main category: cs.CV

Relevance: 35.0

TL;DR: 提出SDCM框架，通过模拟密度化、雷达补偿映射和Mamba建模交互融合，解决4D雷达-视觉3D目标检测中的稀疏点云和视觉退化问题


<details>
  <summary>Details</summary>
Motivation: 在车联网中，4D雷达-视觉3D目标检测面临两个主要挑战：1）4D雷达点云稀疏导致3D表示不佳；2）视觉数据在低光照、远距离和密集遮挡场景下存在表示退化，在融合阶段提供不可靠的纹理信息

Method: 提出SDCM框架包含三个模块：1）SimDen模块：基于3D核密度估计的关键点高斯模拟生成点云，基于曲率模拟生成轮廓，实现雷达点云密度化；2）RCM模块：利用4D雷达的全天候特性，通过雷达补偿映射减少视觉数据退化影响；3）MMIF模块：利用特征张量差异值提取有效信息，通过Mamba建模实现异构减少和模态交互融合

Result: 在VoD、TJ4DRadSet和Astyx HiRes 2019数据集上，SDCM在参数数量更少、推理速度更快的情况下取得了最佳性能

Conclusion: SDCM框架有效解决了4D雷达-视觉3D目标检测中的点云稀疏和视觉退化问题，通过密度化、补偿映射和Mamba交互融合实现了优越的性能

Abstract: 3-D object detection based on 4-D radar-vision is an important part in Internet of Vehicles (IoV). However, there are two challenges which need to be faced. First, the 4-D radar point clouds are sparse, leading to poor 3-D representation. Second, vision datas exhibit representation degradation under low-light, long distance detection and dense occlusion scenes, which provides unreliable texture information during fusion stage. To address these issues, a framework named SDCM is proposed, which contains Simulated Densifying and Compensatory Modeling Fusion for radar-vision 3-D object detection in IoV. Firstly, considering point generation based on Gaussian simulation of key points obtained from 3-D Kernel Density Estimation (3-D KDE), and outline generation based on curvature simulation, Simulated Densifying (SimDen) module is designed to generate dense radar point clouds. Secondly, considering that radar data could provide more real time information than vision data, due to the all-weather property of 4-D radar. Radar Compensatory Mapping (RCM) module is designed to reduce the affects of vision datas' representation degradation. Thirdly, considering that feature tensor difference values contain the effective information of every modality, which could be extracted and modeled for heterogeneity reduction and modalities interaction, Mamba Modeling Interactive Fusion (MMIF) module is designed for reducing heterogeneous and achieving interactive Fusion. Experiment results on the VoD, TJ4DRadSet and Astyx HiRes 2019 dataset show that SDCM achieves best performance with lower parameter quantity and faster inference speed. Our code will be available.

</details>


### [336] [Investigating the Impact of Histopathological Foundation Models on Regressive Prediction of Homologous Recombination Deficiency](https://arxiv.org/abs/2602.00151)
*Alexander Blezinger,Wolfgang Nejdl,Ming Tang*

Main category: cs.CV

Relevance: 35.0

TL;DR: 本文系统评估了基于大规模病理数据预训练的基础模型在回归任务（如同源重组缺陷评分预测）中的表现，发现这些模型特征能显著提升预测准确性和泛化能力，并提出了分布上采样策略解决目标不平衡问题。


<details>
  <summary>Details</summary>
Motivation: 尽管基于大规模病理数据预训练的基础模型在计算病理学多个领域取得成功，但其在回归性生物标志物预测方面的影响尚未充分探索。本研究旨在系统评估这些基础模型在回归任务中的表现，特别是预测同源重组缺陷评分这一对个性化癌症治疗至关重要的生物标志物。

Method: 1) 在多实例学习框架下，从全切片图像中提取补丁级特征，使用五种最先进的基础模型；2) 与基于对比学习的特征进行比较；3) 在乳腺癌、子宫内膜癌和肺癌队列上训练模型预测连续HRD评分；4) 提出分布上采样策略缓解目标不平衡；5) 通过消融研究探索不同采样策略和实例包大小的影响。

Result: 1) 基于基础模型特征的模型在预测准确性和泛化能力方面始终优于基线；2) 不同基础模型之间存在系统性差异；3) 提出的分布上采样策略显著提高了对临床重要但代表性不足患者群体的召回率和平衡准确率；4) 大规模病理预训练为更精确和可转移的回归性生物标志物预测提供了明显优势。

Conclusion: 大规模病理数据预训练的基础模型在回归性生物标志物预测方面具有显著优势，能够提高预测精度和可转移性，展示了在AI驱动的精准肿瘤学中的潜力。提出的分布上采样策略有效解决了临床数据集中的目标不平衡问题。

Abstract: Foundation models pretrained on large-scale histopathology data have found great success in various fields of computational pathology, but their impact on regressive biomarker prediction remains underexplored. In this work, we systematically evaluate histopathological foundation models for regression-based tasks, demonstrated through the prediction of homologous recombination deficiency (HRD) score - a critical biomarker for personalized cancer treatment. Within multiple instance learning frameworks, we extract patch-level features from whole slide images (WSI) using five state-of-the-art foundation models, and evaluate their impact compared to contrastive learning-based features. Models are trained to predict continuous HRD scores based on these extracted features across breast, endometrial, and lung cancer cohorts from two public medical data collections. Extensive experiments demonstrate that models trained on foundation model features consistently outperform the baseline in terms of predictive accuracy and generalization capabilities while exhibiting systematic differences among the foundation models. Additionally, we propose a distribution-based upsampling strategy to mitigate target imbalance in these datasets, significantly improving the recall and balanced accuracy for underrepresented but clinically important patient populations. Furthermore, we investigate the impact of different sampling strategies and instance bagsizes by ablation studies. Our results highlight the benefits of large-scale histopathological pretraining for more precise and transferable regressive biomarker prediction, showcasing its potential to advance AI-driven precision oncology.

</details>


### [337] [YOLOE-26: Integrating YOLO26 with YOLOE for Real-Time Open-Vocabulary Instance Segmentation](https://arxiv.org/abs/2602.00168)
*Ranjan Sapkota,Manoj Karkee*

Main category: cs.CV

Relevance: 35.0

TL;DR: YOLOE-26是一个统一框架，将部署优化的YOLO架构与开放词汇学习范式结合，实现实时开放词汇实例分割，支持文本提示、视觉提示和无提示推理。


<details>
  <summary>Details</summary>
Motivation: 传统YOLO模型仅限于封闭集识别，无法适应动态真实世界环境中的新类别。需要将YOLO的高效实时性与开放词汇学习结合，实现更灵活的场景适应能力。

Method: 基于YOLOv26的无NMS端到端设计，使用卷积骨干网络和PAN/FPN多尺度特征聚合。关键创新：1) 用对象嵌入头替代固定类别logits，将分类建模为与提示嵌入的相似度匹配；2) RepRTA实现零开销文本提示；3) SAVPE用于示例引导分割；4) Lazy Region Prompt Contrast支持无提示推理。

Result: 实验显示在不同模型规模下均具有一致的缩放行为和良好的精度-效率权衡。在提示和无提示设置下都表现出色，与Ultralytics生态系统完全兼容。

Conclusion: YOLOE-26为动态真实世界环境中的实时开放词汇实例分割提供了实用且可扩展的解决方案，在保持YOLO效率的同时扩展了识别能力。

Abstract: This paper presents YOLOE-26, a unified framework that integrates the deployment-optimized YOLO26(or YOLOv26) architecture with the open-vocabulary learning paradigm of YOLOE for real-time open-vocabulary instance segmentation. Building on the NMS-free, end-to-end design of YOLOv26, the proposed approach preserves the hallmark efficiency and determinism of the YOLO family while extending its capabilities beyond closed-set recognition. YOLOE-26 employs a convolutional backbone with PAN/FPN-style multi-scale feature aggregation, followed by end-to-end regression and instance segmentation heads. A key architectural contribution is the replacement of fixed class logits with an object embedding head, which formulates classification as similarity matching against prompt embeddings derived from text descriptions, visual examples, or a built-in vocabulary. To enable efficient open-vocabulary reasoning, the framework incorporates Re-Parameterizable Region-Text Alignment (RepRTA) for zero-overhead text prompting, a Semantic-Activated Visual Prompt Encoder (SAVPE) for example-guided segmentation, and Lazy Region Prompt Contrast for prompt-free inference. All prompting modalities operate within a unified object embedding space, allowing seamless switching between text-prompted, visual-prompted, and fully autonomous segmentation. Extensive experiments demonstrate consistent scaling behavior and favorable accuracy-efficiency trade-offs across model sizes in both prompted and prompt-free settings. The training strategy leverages large-scale detection and grounding datasets with multi-task optimization and remains fully compatible with the Ultralytics ecosystem for training, validation, and deployment. Overall, YOLOE-26 provides a practical and scalable solution for real-time open-vocabulary instance segmentation in dynamic, real-world environments.

</details>


### [338] [Vision-Language Model Purified Semi-Supervised Semantic Segmentation for Remote Sensing Images](https://arxiv.org/abs/2602.00202)
*Shanwen Wang,Xin Sun,Danfeng Hong,Fei Zhou*

Main category: cs.CV

Relevance: 35.0

TL;DR: 提出SemiEarth模型，将视觉语言模型(VLMs)引入遥感图像的半监督语义分割，通过VLM伪标签净化模块提升教师网络生成的伪标签质量，尤其在多类别边界区域表现显著。


<details>
  <summary>Details</summary>
Motivation: 传统半监督语义分割(S4)架构面临伪标签质量低的问题，特别是在教师-学生框架中。遥感图像具有多类别边界复杂的特点，需要更高质量的伪标签来指导学习。

Method: 提出SemiEarth模型，引入视觉语言模型(VLMs)并设计VLM伪标签净化(VLM-PP)结构。该模块利用VLMs的开放世界能力，净化教师网络生成的伪标签，特别是在低置信度区域和多类别边界区域纠正错误预测。

Result: 在多个遥感数据集上的实验表明，SemiEarth达到了最先进的性能。VLM-PP模块显著提升了伪标签质量，尤其在多类别边界区域，模型不仅性能优异，还具有良好的可解释性。

Conclusion: 将VLMs引入遥感半监督语义分割是有效的，VLM-PP模块能够显著提升伪标签质量，特别是在复杂边界区域。该方法独立于S4架构，具有开放世界能力和良好的可解释性。

Abstract: The semi-supervised semantic segmentation (S4) can learn rich visual knowledge from low-cost unlabeled images. However, traditional S4 architectures all face the challenge of low-quality pseudo-labels, especially for the teacher-student framework.We propose a novel SemiEarth model that introduces vision-language models (VLMs) to address the S4 issues for the remote sensing (RS) domain. Specifically, we invent a VLM pseudo-label purifying (VLM-PP) structure to purify the teacher network's pseudo-labels, achieving substantial improvements. Especially in multi-class boundary regions of RS images, the VLM-PP module can significantly improve the quality of pseudo-labels generated by the teacher, thereby correctly guiding the student model's learning. Moreover, since VLM-PP equips VLMs with open-world capabilities and is independent of the S4 architecture, it can correct mispredicted categories in low-confidence pseudo-labels whenever a discrepancy arises between its prediction and the pseudo-label. We conducted extensive experiments on multiple RS datasets, which demonstrate that our SemiEarth achieves SOTA performance. More importantly, unlike previous SOTA RS S4 methods, our model not only achieves excellent performance but also offers good interpretability. The code is released at https://github.com/wangshanwen001/SemiEarth.

</details>


### [339] [Interpretable Unsupervised Deformable Image Registration via Confidence-bound Multi-Hop Visual Reasoning](https://arxiv.org/abs/2602.00211)
*Zafar Iqbal,Anwar Ul Haq,Srimannarayana Grandhi*

Main category: cs.CV

Relevance: 35.0

TL;DR: 提出多跳视觉推理链(VCoR)框架，将无监督医学图像配准重新定义为渐进推理过程，通过局部空间细化和交叉参考注意力机制实现可解释的配准。


<details>
  <summary>Details</summary>
Motivation: 现有深度学习方法在无监督医学图像配准中缺乏透明度和可解释性，导致误差漂移和临床信任度降低。需要一种既能保持准确性又能提供透明推理过程的配准方法。

Method: 提出多跳视觉推理链(VCoR)框架，将配准重新定义为渐进推理过程。每个视觉推理跳包含：1) 局部空间细化(LSR)模块来丰富特征表示；2) 交叉参考注意力(CRA)机制引导迭代细化过程，保持解剖一致性。该框架提供中间预测序列和理论边界。

Result: 在DIR-Lab 4D CT(肺部)和IXI T1加权MRI(脑部)两个挑战性公共数据集上的评估表明，VCoR实现了具有竞争力的配准准确性，同时提供丰富的中间可视化和置信度测量。

Conclusion: 通过嵌入隐式视觉推理范式，VCoR提供了一种可解释、可靠且临床可行的无监督医学图像配准方法，超越了单纯的准确性，提供了内置的可解释性和不确定性估计。

Abstract: Unsupervised deformable image registration requires aligning complex anatomical structures without reference labels, making interpretability and reliability critical. Existing deep learning methods achieve considerable accuracy but often lack transparency, leading to error drift and reduced clinical trust. We propose a novel Multi-Hop Visual Chain of Reasoning (VCoR) framework that reformulates registration as a progressive reasoning process. Inspired by the iterative nature of clinical decision-making, each visual reasoning hop integrates a Localized Spatial Refinement (LSR) module to enrich feature representations and a Cross-Reference Attention (CRA) mechanism that leads the iterative refinement process, preserving anatomical consistency. This multi-hop strategy enables robust handling of large deformations and produces a transparent sequence of intermediate predictions with a theoretical bound. Beyond accuracy, our framework offers built-in interpretability by estimating uncertainty via the stability and convergence of deformation fields across hops. Extensive evaluations on two challenging public datasets, DIR-Lab 4D CT (lung) and IXI T1-weighted MRI (brain), demonstrate that VCoR achieves competitive registration accuracy while offering rich intermediate visualizations and confidence measures. By embedding an implicit visual reasoning paradigm, we present an interpretable, reliable, and clinically viable unsupervised medical image registration.

</details>


### [340] [Opportunistic Promptable Segmentation: Leveraging Routine Radiological Annotations to Guide 3D CT Lesion Segmentation](https://arxiv.org/abs/2602.00309)
*Samuel Church,Joshua D. Warner,Danyal Maqbool,Xin Tie,Junjie Hu,Meghan G. Lubner,Tyler J. Bradshaw*

Main category: cs.CV

Relevance: 35.0

TL;DR: SAM2CT：首个可提示分割模型，将放射科医师的稀疏标注（箭头/线段）转换为CT体积的3D分割，利用历史PACS数据大规模生成3D分割数据集


<details>
  <summary>Details</summary>
Motivation: CT成像的机器学习模型需要大规模高质量标注数据集，但3D分割标注成本高昂。临床PACS中已有大量放射科医师的稀疏标注（箭头、线段等GSPS对象），这些标注可被提取并转换为3D分割，实现低成本大规模数据集构建。

Method: 基于SAM2构建，扩展提示编码器以支持箭头和线段输入，并引入Memory-Conditioned Memories（MCM）——专为3D医学体积设计的记忆编码策略。通过"机会性可提示分割"范式，将稀疏标注转换为3D分割。

Result: 在公共病灶分割基准测试中，SAM2CT优于现有可提示分割模型和类似训练的基线：箭头提示Dice系数0.649，线段提示0.757。在临床PACS的60个GSPS标注上，87%的分割被放射科医师评为临床可接受或仅需微调。在急诊科发现上展示强零样本性能。

Conclusion: SAM2CT证明了从历史GSPS标注中大规模挖掘3D CT分割数据集的可行性，为医学影像分析提供了可扩展的低成本数据生成方法。

Abstract: The development of machine learning models for CT imaging depends on the availability of large, high-quality, and diverse annotated datasets. Although large volumes of CT images and reports are readily available in clinical picture archiving and communication systems (PACS), 3D segmentations of critical findings are costly to obtain, typically requiring extensive manual annotation by radiologists. On the other hand, it is common for radiologists to provide limited annotations of findings during routine reads, such as line measurements and arrows, that are often stored in PACS as GSPS objects. We posit that these sparse annotations can be extracted along with CT volumes and converted into 3D segmentations using promptable segmentation models, a paradigm we term Opportunistic Promptable Segmentation. To enable this paradigm, we propose SAM2CT, the first promptable segmentation model designed to convert radiologist annotations into 3D segmentations in CT volumes. SAM2CT builds upon SAM2 by extending the prompt encoder to support arrow and line inputs and by introducing Memory-Conditioned Memories (MCM), a memory encoding strategy tailored to 3D medical volumes. On public lesion segmentation benchmarks, SAM2CT outperforms existing promptable segmentation models and similarly trained baselines, achieving Dice similarity coefficients of 0.649 for arrow prompts and 0.757 for line prompts. Applying the model to pre-existing GSPS annotations from a clinical PACS (N = 60), SAM2CT generates 3D segmentations that are clinically acceptable or require only minor adjustments in 87% of cases, as scored by radiologists. Additionally, SAM2CT demonstrates strong zero-shot performance on select Emergency Department findings. These results suggest that large-scale mining of historical GSPS annotations represents a promising and scalable approach for generating 3D CT segmentation datasets.

</details>


### [341] [On the Assessment of Sensitivity of Autonomous Vehicle Perception](https://arxiv.org/abs/2602.00314)
*Apostol Vassilev,Munawar Hasan,Edward Griffor,Honglan Jin,Pavel Piliptchak,Mahima Arora,Thoshitha Gamage*

Main category: cs.CV

Relevance: 35.0

TL;DR: 论文提出了一种基于模型集成和预测敏感性量化的自动驾驶感知系统鲁棒性评估方法，通过模拟和真实环境中的多种不利驾驶场景，评估了五种先进计算机视觉模型的性能，发现光照条件、道路遮挡和距离是影响感知鲁棒性的关键因素。


<details>
  <summary>Details</summary>
Motivation: 自动驾驶的可行性高度依赖感知系统的实时准确性和可靠性。这些系统不仅需要在理想条件下工作，还要在自然和对抗性驾驶因素干扰下保持性能。感知错误和检测延迟可能导致严重后果，因此需要评估自动驾驶车辆感知系统的鲁棒性并探索提高可靠性的策略。

Method: 采用基于模型集成的预测敏感性量化方法，捕捉多个模型之间的分歧和推理变异性。在模拟和真实环境中评估不利驾驶场景下的感知性能。提出了一个评估感知性能的概念架构，基于自动驾驶车辆在不同路面（干/湿沥青）和速度下在停止标志前的停车距离开发感知评估标准。实验使用了五种最先进的计算机视觉模型：YOLO (v8-v9)、DETR50、DETR101和RT-DETR。

Result: 1. 光照条件（雾、低太阳高度）对感知模型性能影响最大；2. 道路遮挡等对抗性条件会增加感知敏感性；3. 对抗性道路条件和恶劣天气条件组合时模型性能下降；4. 距离道路物体越远，对感知性能影响越大，感知鲁棒性越差。

Conclusion: 自动驾驶感知系统的鲁棒性受到多种环境因素的显著影响，特别是光照条件、道路遮挡和距离。基于模型集成的预测敏感性量化方法可以有效评估感知性能，为开发更可靠的自动驾驶系统提供重要见解。

Abstract: The viability of automated driving is heavily dependent on the performance of perception systems to provide real-time accurate and reliable information for robust decision-making and maneuvers. These systems must perform reliably not only under ideal conditions, but also when challenged by natural and adversarial driving factors. Both of these types of interference can lead to perception errors and delays in detection and classification. Hence, it is essential to assess the robustness of the perception systems of automated vehicles (AVs) and explore strategies for making perception more reliable. We approach this problem by evaluating perception performance using predictive sensitivity quantification based on an ensemble of models, capturing model disagreement and inference variability across multiple models, under adverse driving scenarios in both simulated environments and real-world conditions. A notional architecture for assessing perception performance is proposed. A perception assessment criterion is developed based on an AV's stopping distance at a stop sign on varying road surfaces, such as dry and wet asphalt, and vehicle speed. Five state-of-the-art computer vision models are used, including YOLO (v8-v9), DEtection TRansformer (DETR50, DETR101), Real-Time DEtection TRansformer (RT-DETR)in our experiments. Diminished lighting conditions, e.g., resulting from the presence of fog and low sun altitude, have the greatest impact on the performance of the perception models. Additionally, adversarial road conditions such as occlusions of roadway objects increase perception sensitivity and model performance drops when faced with a combination of adversarial road conditions and inclement weather conditions. Also, it is demonstrated that the greater the distance to a roadway object, the greater the impact on perception performance, hence diminished perception robustness.

</details>


### [342] [Deep Learning-Based Object Detection for Autonomous Vehicles: A Comparative Study of One-Stage and Two-Stage Detectors on Basic Traffic Objects](https://arxiv.org/abs/2602.00385)
*Bsher Karbouj,Adam Michael Altenbuchner,Joerg Krueger*

Main category: cs.CV

Relevance: 35.0

TL;DR: 该研究对YOLOv5和Faster R-CNN两种目标检测模型在自动驾驶应用中的性能进行了全面实验分析，评估了它们在mAP、召回率和推理速度等指标上的表现。


<details>
  <summary>Details</summary>
Motivation: 自动驾驶系统中目标检测是关键组件，但现有通用深度学习架构（如YOLO、SSD、Faster R-CNN）在特定自动驾驶应用中的适用性指导有限。方法选择直接影响检测精度、处理速度、环境鲁棒性、传感器集成、可扩展性和边缘情况处理。

Method: 使用包含真实和合成图像的多样化数据集，对YOLOv5（单阶段检测器）和Faster R-CNN（两阶段检测器）进行实验比较。评估指标包括平均精度均值（mAP）、召回率和推理速度，并分析了不同置信度阈值和真实场景下的模型行为。

Result: YOLOv5在mAP、召回率和训练效率方面表现更优，特别是在数据集规模和图像分辨率增加时。Faster R-CNN在检测小型远距离物体方面有优势，在挑战性光照条件下表现良好。

Conclusion: 研究为自动驾驶系统选择合适的目标检测模型提供了实用指导：YOLOv5在整体性能和效率方面更优，而Faster R-CNN在特定场景（如小物体检测和恶劣光照条件）有优势。

Abstract: Object detection is a crucial component in autonomous vehicle systems. It enables the vehicle to perceive and understand its environment by identifying and locating various objects around it. By utilizing advanced imaging and deep learning techniques, autonomous vehicle systems can rapidly and accurately identify objects based on their features. Different deep learning methods vary in their ability to accurately detect and classify objects in autonomous vehicle systems. Selecting the appropriate method significantly impacts system performance, robustness, and efficiency in real-world driving scenarios. While several generic deep learning architectures like YOLO, SSD, and Faster R-CNN have been proposed, guidance on their suitability for specific autonomous driving applications is often limited. The choice of method affects detection accuracy, processing speed, environmental robustness, sensor integration, scalability, and edge case handling. This study provides a comprehensive experimental analysis comparing two prominent object detection models: YOLOv5 (a one-stage detector) and Faster R-CNN (a two-stage detector). Their performance is evaluated on a diverse dataset combining real and synthetic images, considering various metrics including mean Average Precision (mAP), recall, and inference speed. The findings reveal that YOLOv5 demonstrates superior performance in terms of mAP, recall, and training efficiency, particularly as dataset size and image resolution increase. However, Faster R-CNN shows advantages in detecting small, distant objects and performs well in challenging lighting conditions. The models' behavior is also analyzed under different confidence thresholds and in various real-world scenarios, providing insights into their applicability for autonomous driving systems.

</details>


### [343] [Modeling Art Evaluations from Comparative Judgments: A Deep Learning Approach to Predicting Aesthetic Preferences](https://arxiv.org/abs/2602.00394)
*Manoj Reddy Bethi,Sai Rupa Jhade,Pravallika Yaganti,Monoshiz Mahbub Khan,Zhe Yu*

Main category: cs.CV

Relevance: 35.0

TL;DR: 该论文提出使用成对比较学习框架来降低获取人类审美判断标注数据的成本，通过相对选择而非直接评分来建模视觉艺术中的审美偏好。


<details>
  <summary>Details</summary>
Motivation: 视觉艺术中的人类审美判断建模面临个体偏好差异大和标注数据获取成本高的挑战。直接评分需要大量标注工作，且认知负担重、一致性差。

Method: 1) 使用ResNet-50提取绘画图像的深度卷积特征；2) 开发深度神经网络回归模型；3) 构建双分支成对比较模型；4) 基于比较判断定律，通过相对选择降低认知负担；5) 进行个体评分者偏好分析（内部和跨评分者）。

Result: 1) 深度回归模型比基线线性回归模型提升高达328%的R²；2) 比较模型在无法访问直接评分值的情况下接近回归性能；3) 个体偏好预测仍然困难；4) 成对比较标注时间比直接评分减少60%。

Conclusion: 成对比较学习框架能有效降低审美判断标注成本，在缺乏直接评分值时仍能获得良好性能，为大规模偏好建模提供了高效的标注方法。

Abstract: Modeling human aesthetic judgments in visual art presents significant challenges due to individual preference variability and the high cost of obtaining labeled data. To reduce cost of acquiring such labels, we propose to apply a comparative learning framework based on pairwise preference assessments rather than direct ratings. This approach leverages the Law of Comparative Judgment, which posits that relative choices exhibit less cognitive burden and greater cognitive consistency than direct scoring. We extract deep convolutional features from painting images using ResNet-50 and develop both a deep neural network regression model and a dual-branch pairwise comparison model. We explored four research questions: (RQ1) How does the proposed deep neural network regression model with CNN features compare to the baseline linear regression model using hand-crafted features? (RQ2) How does pairwise comparative learning compare to regression-based prediction when lacking access to direct rating values? (RQ3) Can we predict individual rater preferences through within-rater and cross-rater analysis? (RQ4) What is the annotation cost trade-off between direct ratings and comparative judgments in terms of human time and effort? Our results show that the deep regression model substantially outperforms the baseline, achieving up to $328\%$ improvement in $R^2$. The comparative model approaches regression performance despite having no access to direct rating values, validating the practical utility of pairwise comparisons. However, predicting individual preferences remains challenging, with both within-rater and cross-rater performance significantly lower than average rating prediction. Human subject experiments reveal that comparative judgments require $60\%$ less annotation time per item, demonstrating superior annotation efficiency for large-scale preference modeling.

</details>


### [344] [Model Optimization for Multi-Camera 3D Detection and Tracking](https://arxiv.org/abs/2602.00450)
*Ethan Anderson,Justin Silva,Kyle Zheng,Sameer Pusegaonkar,Yizhou Wang,Zheng Tang,Sujit Biswas*

Main category: cs.CV

Relevance: 35.0

TL;DR: Sparse4D多相机3D检测跟踪框架在低帧率、量化、跨数据集迁移和混合精度训练下的性能分析，重点关注身份稳定性指标


<details>
  <summary>Details</summary>
Motivation: 室内多相机感知系统需要处理遮挡和多视角问题，现有方法在低帧率、量化部署和跨数据集迁移时的性能表现需要系统评估

Method: 基于查询的时空3D检测跟踪框架Sparse4D，在共享世界坐标系中融合多视角特征，通过实例记忆传播稀疏对象查询

Result: Sparse4D在中等帧率降低时保持稳定，但低于2FPS时身份关联崩溃；选择性量化提供最佳速度-精度权衡；低帧率预训练在WILDTRACK上带来显著零样本提升；混合精度训练降低延迟但可能影响身份稳定性

Conclusion: 多相机跟踪系统需要针对部署场景优化帧率、量化策略和精度配置，身份稳定性是重要评估指标，混合精度训练需要稳定性感知验证

Abstract: Outside-in multi-camera perception is increasingly important in indoor environments, where networks of static cameras must support multi-target tracking under occlusion and heterogeneous viewpoints. We evaluate Sparse4D, a query-based spatiotemporal 3D detection and tracking framework that fuses multi-view features in a shared world frame and propagates sparse object queries via instance memory. We study reduced input frame rates, post-training quantization (INT8 and FP8), transfer to the WILDTRACK benchmark, and Transformer Engine mixed-precision fine-tuning. To better capture identity stability, we report Average Track Duration (AvgTrackDur), which measures identity persistence in seconds. Sparse4D remains stable under moderate FPS reductions, but below 2 FPS, identity association collapses even when detections are stable. Selective quantization of the backbone and neck offers the best speed-accuracy trade-off, while attention-related modules are consistently sensitive to low precision. On WILDTRACK, low-FPS pretraining yields large zero-shot gains over the base checkpoint, while small-scale fine-tuning provides limited additional benefit. Transformer Engine mixed precision reduces latency and improves camera scalability, but can destabilize identity propagation, motivating stability-aware validation.

</details>


### [345] [PSGS: Text-driven Panorama Sliding Scene Generation via Gaussian Splatting](https://arxiv.org/abs/2602.00463)
*Xin Zhang,Shen Chen,Jiale Zhou,Lei Li*

Main category: cs.CV

Relevance: 35.0

TL;DR: PSGS：一个两阶段框架，通过双层优化架构生成语义连贯的全景图，然后通过全景滑动机制初始化全局一致的3D高斯泼溅点云，实现从文本生成高质量3D场景。


<details>
  <summary>Details</summary>
Motivation: 从文本生成逼真的3D场景对于VR、AR和游戏等沉浸式应用至关重要。现有方法存在3D-文本数据有限和多视角拼接不一致的问题，导致生成的场景过于简单。

Method: 1. 双层优化架构：布局推理层将文本解析为结构化空间关系，自优化层通过迭代MLLM反馈细化视觉细节；2. 全景滑动机制：通过策略性采样重叠视角初始化全局一致的3D高斯泼溅点云；3. 训练时加入深度和语义一致性损失。

Result: PSGS在全景图生成方面优于现有方法，产生更具吸引力的3D场景，为可扩展的沉浸式内容创建提供了稳健解决方案。

Conclusion: PSGS通过结合文本驱动的全景生成和3D高斯泼溅技术，有效解决了现有方法在3D场景生成中的局限性，实现了高质量、细节丰富的沉浸式内容创建。

Abstract: Generating realistic 3D scenes from text is crucial for immersive applications like VR, AR, and gaming. While text-driven approaches promise efficiency, existing methods suffer from limited 3D-text data and inconsistent multi-view stitching, resulting in overly simplistic scenes. To address this, we propose PSGS, a two-stage framework for high-fidelity panoramic scene generation. First, a novel two-layer optimization architecture generates semantically coherent panoramas: a layout reasoning layer parses text into structured spatial relationships, while a self-optimization layer refines visual details via iterative MLLM feedback. Second, our panorama sliding mechanism initializes globally consistent 3D Gaussian Splatting point clouds by strategically sampling overlapping perspectives. By incorporating depth and semantic coherence losses during training, we greatly improve the quality and detail fidelity of rendered scenes. Our experiments demonstrate that PSGS outperforms existing methods in panorama generation and produces more appealing 3D scenes, offering a robust solution for scalable immersive content creation.

</details>


### [346] [HSSDCT: Factorized Spatial-Spectral Correlation for Hyperspectral Image Fusion](https://arxiv.org/abs/2602.00490)
*Chia-Ming Lee,Yu-Hao Ho,Yu-Fan Lin,Jen-Wei Lee,Li-Wei Kang,Chih-Chung Hsu*

Main category: cs.CV

Relevance: 35.0

TL;DR: 提出HSSDCT网络用于高光谱图像融合，通过分层密集残差Transformer块和多尺度特征聚合，以及空间-光谱相关层实现线性复杂度，在计算效率和重建质量上达到SOTA。


<details>
  <summary>Details</summary>
Motivation: 现有深度学习方法在高光谱图像融合中存在感受野有限、光谱带冗余、自注意力二次复杂度等问题，限制了效率和鲁棒性，需要更高效的架构来解决这些挑战。

Method: 提出分层空间-光谱密集相关网络(HSSDCT)，包含两个关键模块：1)分层密集残差Transformer块(HDRTB)，通过渐进扩大窗口和使用密集残差连接进行多尺度特征聚合；2)空间-光谱相关层(SSCL)，显式分解空间和光谱依赖，将自注意力复杂度降低到线性，同时减少光谱冗余。

Result: 在基准数据集上的广泛实验表明，HSSDCT在显著降低计算成本的同时提供了优越的重建质量，在高光谱图像融合中实现了新的最先进性能。

Conclusion: HSSDCT通过创新的分层密集残差Transformer块和空间-光谱相关层，有效解决了高光谱图像融合中的计算效率和鲁棒性问题，为相关领域提供了高效解决方案。

Abstract: Hyperspectral image (HSI) fusion aims to reconstruct a high-resolution HSI (HR-HSI) by combining the rich spectral information of a low-resolution HSI (LR-HSI) with the fine spatial details of a high-resolution multispectral image (HR-MSI). Although recent deep learning methods have achieved notable progress, they still suffer from limited receptive fields, redundant spectral bands, and the quadratic complexity of self-attention, which restrict both efficiency and robustness. To overcome these challenges, we propose the Hierarchical Spatial-Spectral Dense Correlation Network (HSSDCT). The framework introduces two key modules: (i) a Hierarchical Dense-Residue Transformer Block (HDRTB) that progressively enlarges windows and employs dense-residue connections for multi-scale feature aggregation, and (ii) a Spatial-Spectral Correlation Layer (SSCL) that explicitly factorizes spatial and spectral dependencies, reducing self-attention to linear complexity while mitigating spectral redundancy. Extensive experiments on benchmark datasets demonstrate that HSSDCT delivers superior reconstruction quality with significantly lower computational costs, achieving new state-of-the-art performance in HSI fusion. Our code is available at https://github.com/jemmyleee/HSSDCT.

</details>


### [347] [SPARK: Stochastic Propagation via Affinity-guided Random walK for training-free unsupervised segmentation](https://arxiv.org/abs/2602.00516)
*Kunal Mahatha,Jose Dolz,Christian Desrosiers*

Main category: cs.CV

Relevance: 35.0

TL;DR: 该论文提出了一种新的无训练分割方法，将分割重新定义为扩散诱导亲和力图上的随机流平衡问题，通过马尔可夫传播方案实现零样本语义分割，在七个基准测试中达到SOTA性能。


<details>
  <summary>Details</summary>
Motivation: 现有无训练分割方法基于谱图分割假设，存在需要预设聚类数量、边界过度平滑、对噪声和多模态亲和力分布敏感等问题，且忽略了局部邻域结构的重要性。

Method: 将分割重新定义为扩散诱导亲和力图上的随机流平衡问题，提出马尔可夫传播方案，结合随机游走标签扩散和自适应剪枝策略，整合全局扩散注意力与稳定扩散提取的局部邻域。

Result: 在七个广泛使用的语义分割基准测试中实现了最先进的零样本性能，产生更清晰的边界、更连贯的区域和显著更稳定的掩码。

Conclusion: 通过随机流平衡框架和马尔可夫传播方案，解决了传统谱聚类方法的局限性，为无训练分割提供了更稳健和准确的解决方案。

Abstract: We argue that existing training-free segmentation methods rely on an implicit and limiting assumption, that segmentation is a spectral graph partitioning problem over diffusion-derived affinities. Such approaches, based on global graph partitioning and eigenvector-based formulations of affinity matrices, suffer from several fundamental drawbacks, they require pre-selecting the number of clusters, induce boundary oversmoothing due to spectral relaxation, and remain highly sensitive to noisy or multi-modal affinity distributions. Moreover, many prior works neglect the importance of local neighborhood structure, which plays a crucial role in stabilizing affinity propagation and preserving fine-grained contours. To address these limitations, we reformulate training-free segmentation as a stochastic flow equilibrium problem over diffusion-induced affinity graphs, where segmentation emerges from a stochastic propagation process that integrates global diffusion attention with local neighborhoods extracted from stable diffusion, yielding a sparse yet expressive affinity structure. Building on this formulation, we introduce a Markov propagation scheme that performs random-walk-based label diffusion with an adaptive pruning strategy that suppresses unreliable transitions while reinforcing confident affinity paths. Experiments across seven widely used semantic segmentation benchmarks demonstrate that our method achieves state-of-the-art zero-shot performance, producing sharper boundaries, more coherent regions, and significantly more stable masks compared to prior spectral-clustering-based approaches.

</details>


### [348] [Enhancing Open-Vocabulary Object Detection through Multi-Level Fine-Grained Visual-Language Alignment](https://arxiv.org/abs/2602.00531)
*Tianyi Zhang,Antoine Simoulin,Kai Li,Sana Lakdawala,Shiqing Yu,Arpit Mittal,Hongyu Fu,Yu Lin*

Main category: cs.CV

Relevance: 35.0

TL;DR: VLDet是一个新颖的开放词汇目标检测框架，通过重新设计特征金字塔实现细粒度的视觉-语言对齐，显著提升了新类别的检测性能。


<details>
  <summary>Details</summary>
Motivation: 传统目标检测系统受限于预定义类别，无法适应动态环境。开放词汇目标检测(OVD)能够识别训练集中未见过的新类别，但现有方法在将CLIP的单尺度图像骨干网络适配到检测框架或确保稳健的视觉-语言对齐方面面临挑战。

Method: 提出VLDet框架：1) VL-PUB模块：重新设计特征金字塔，有效利用CLIP的视觉-语言知识并将骨干网络适配到目标检测；2) SigRPN块：引入基于sigmoid的锚点-文本对比对齐损失，改善新类别检测。

Result: 在COCO2017上达到58.7 AP（新类别），在LVIS上达到24.8 AP，分别比现有最佳方法提升27.6%和6.9%。同时在闭集目标检测上也表现出优越的零样本性能。

Conclusion: VLDet通过重新设计特征金字塔实现细粒度视觉-语言对齐，显著提升了开放词汇目标检测性能，为动态环境中的目标识别提供了有效解决方案。

Abstract: Traditional object detection systems are typically constrained to predefined categories, limiting their applicability in dynamic environments. In contrast, open-vocabulary object detection (OVD) enables the identification of objects from novel classes not present in the training set. Recent advances in visual-language modeling have led to significant progress of OVD. However, prior works face challenges in either adapting the single-scale image backbone from CLIP to the detection framework or ensuring robust visual-language alignment. We propose Visual-Language Detection (VLDet), a novel framework that revamps feature pyramid for fine-grained visual-language alignment, leading to improved OVD performance. With the VL-PUB module, VLDet effectively exploits the visual-language knowledge from CLIP and adapts the backbone for object detection through feature pyramid. In addition, we introduce the SigRPN block, which incorporates a sigmoid-based anchor-text contrastive alignment loss to improve detection of novel categories. Through extensive experiments, our approach achieves 58.7 AP for novel classes on COCO2017 and 24.8 AP on LVIS, surpassing all state-of-the-art methods and achieving significant improvements of 27.6% and 6.9%, respectively. Furthermore, VLDet also demonstrates superior zero-shot performance on closed-set object detection.

</details>


### [349] [SADER: Structure-Aware Diffusion Framework with DEterministic Resampling for Multi-Temporal Remote Sensing Cloud Removal](https://arxiv.org/abs/2602.00536)
*Yifan Zhang,Qian Chen,Yi Liu,Wengen Li,Jihong Guan*

Main category: cs.CV

Relevance: 35.0

TL;DR: SADER是一个用于多时相遥感影像去云的结构感知扩散框架，通过多时相条件扩散网络、云感知注意力损失和确定性重采样策略，显著提升了去云效果和采样效率。


<details>
  <summary>Details</summary>
Motivation: 云污染严重降低了遥感影像的可用性，对下游地球观测任务构成根本性挑战。现有的基于扩散的方法存在采样效率有限和在多时相遥感场景中结构与时序先验利用不足的问题。

Method: 提出SADER框架：1) 可扩展的多时相条件扩散网络(MTCDN)，通过时序融合和混合注意力捕获多时相和多模态相关性；2) 云感知注意力损失，考虑云厚度和亮度差异强调云主导区域；3) 确定性重采样策略，在固定采样步数下通过引导校正替换异常值来迭代优化样本。

Result: 在多个多时相数据集上的广泛实验表明，SADER在所有评估指标上一致优于最先进的去云方法。

Conclusion: SADER通过结合结构感知的扩散建模、云感知损失和高效采样策略，为多时相遥感去云提供了一个有效且高效的解决方案。

Abstract: Cloud contamination severely degrades the usability of remote sensing imagery and poses a fundamental challenge for downstream Earth observation tasks. Recently, diffusion-based models have emerged as a dominant paradigm for remote sensing cloud removal due to their strong generative capability and stable optimization. However, existing diffusion-based approaches often suffer from limited sampling efficiency and insufficient exploitation of structural and temporal priors in multi-temporal remote sensing scenarios. In this work, we propose SADER, a structure-aware diffusion framework for multi-temporal remote sensing cloud removal. SADER first develops a scalable Multi-Temporal Conditional Diffusion Network (MTCDN) to fully capture multi-temporal and multimodal correlations via temporal fusion and hybrid attention. Then, a cloud-aware attention loss is introduced to emphasize cloud-dominated regions by accounting for cloud thickness and brightness discrepancies. In addition, a deterministic resampling strategy is designed for continuous diffusion models to iteratively refine samples under fixed sampling steps by replacing outliers through guided correction. Extensive experiments on multiple multi-temporal datasets demonstrate that SADER consistently outperforms state-of-the-art cloud removal methods across all evaluation metrics. The code of SADER is publicly available at https://github.com/zyfzs0/SADER.

</details>


### [350] [Bridging Degradation Discrimination and Generation for Universal Image Restoration](https://arxiv.org/abs/2602.00579)
*JiaKui Hu,Zhengjian Yao,Lujia Jin,Yanye Lu*

Main category: cs.CV

Relevance: 35.0

TL;DR: BDG提出了一种结合退化判别与生成的通用图像恢复方法，通过MAS-GLCM进行细粒度退化分析，并采用三阶段扩散训练来同时保持纹理恢复能力和退化感知能力。


<details>
  <summary>Details</summary>
Motivation: 通用图像恢复需要处理多种退化类型和程度，现有方法在同时保持高质量图像生成能力和退化感知能力方面存在挑战。需要一种既能准确识别退化特征又能有效恢复细节的统一框架。

Method: 1) 提出MAS-GLCM进行多角度多尺度灰度共生矩阵分析，实现细粒度退化类型和程度判别；2) 设计三阶段扩散训练：生成阶段、桥接阶段和恢复阶段，将判别信息融入恢复过程；3) 不改变基础架构，通过训练策略提升多任务多退化场景处理能力。

Result: BDG在通用图像恢复和真实世界超分辨率任务中取得显著性能提升，特别是在保真度方面有大幅改进，同时不牺牲感知质量。代码和预训练模型已开源。

Conclusion: 通过结合退化判别与生成，BDG有效解决了通用图像恢复中的关键挑战，在保持纹理丰富性的同时增强了退化感知能力，为多任务低层视觉问题提供了有效解决方案。

Abstract: Universal image restoration is a critical task in low-level vision, requiring the model to remove various degradations from low-quality images to produce clean images with rich detail. The challenges lie in sampling the distribution of high-quality images and adjusting the outputs on the basis of the degradation. This paper presents a novel approach, Bridging Degradation discrimination and Generation (BDG), which aims to address these challenges concurrently. First, we propose the Multi-Angle and multi-Scale Gray Level Co-occurrence Matrix (MAS-GLCM) and demonstrate its effectiveness in performing fine-grained discrimination of degradation types and levels. Subsequently, we divide the diffusion training process into three distinct stages: generation, bridging, and restoration. The objective is to preserve the diffusion model's capability of restoring rich textures while simultaneously integrating the discriminative information from the MAS-GLCM into the restoration process. This enhances its proficiency in addressing multi-task and multi-degraded scenarios. Without changing the architecture, BDG achieves significant performance gains in all-in-one restoration and real-world super-resolution tasks, primarily evidenced by substantial improvements in fidelity without compromising perceptual quality. The code and pretrained models are provided in https://github.com/MILab-PKU/BDG.

</details>


### [351] [FaceSnap: Enhanced ID-fidelity Network for Tuning-free Portrait Customization](https://arxiv.org/abs/2602.00627)
*Benxiang Zhai,Yifang Xu,Guofeng Zhang,Yang Li,Sidan Du*

Main category: cs.CV

Relevance: 35.0

TL;DR: FaceSnap：基于Stable Diffusion的单参考图像个性化肖像生成方法，无需微调即可在单次推理中实现高保真面部细节


<details>
  <summary>Details</summary>
Motivation: 现有个性化图像生成方法要么需要耗时微调且缺乏泛化性，要么无法实现高保真的面部细节。需要一种既能保持高保真度又无需复杂微调的方法。

Method: 基于Stable Diffusion，设计Facial Attribute Mixer提取低层特定特征和高层抽象特征的融合信息，引入Landmark Predictor保持不同姿态下的身份一致性，使用ID-preserving模块注入UNet

Result: 在个性化肖像生成任务中表现优异，超越该领域其他最先进方法，仅需单张参考图像即可在单次推理中产生高度一致的结果

Conclusion: FaceSnap是一种即插即用的个性化肖像生成方法，无需微调即可实现高保真面部细节，可轻松扩展到不同的SD模型

Abstract: Benefiting from the significant advancements in text-to-image diffusion models, research in personalized image generation, particularly customized portrait generation, has also made great strides recently. However, existing methods either require time-consuming fine-tuning and lack generalizability or fail to achieve high fidelity in facial details. To address these issues, we propose FaceSnap, a novel method based on Stable Diffusion (SD) that requires only a single reference image and produces extremely consistent results in a single inference stage. This method is plug-and-play and can be easily extended to different SD models. Specifically, we design a new Facial Attribute Mixer that can extract comprehensive fused information from both low-level specific features and high-level abstract features, providing better guidance for image generation. We also introduce a Landmark Predictor that maintains reference identity across landmarks with different poses, providing diverse yet detailed spatial control conditions for image generation. Then we use an ID-preserving module to inject these into the UNet. Experimental results demonstrate that our approach performs remarkably in personalized and customized portrait generation, surpassing other state-of-the-art methods in this domain.

</details>


### [352] [Schrödinger-Inspired Time-Evolution for 4D Deformation Forecasting](https://arxiv.org/abs/2602.00661)
*Ahsan Raza Siyal,Markus Haltmeier,Ruth Steiger,Elke Ruth Gizewski,Astrid Ellen Grams*

Main category: cs.CV

Relevance: 35.0

TL;DR: 提出了一种基于薛定谔方程启发的物理引导神经网络架构，用于4D（3D+时间）时空预测，将显式时间演化算子嵌入深度卷积框架中，实现稳定、可解释的长期预测。


<details>
  <summary>Details</summary>
Motivation: 4D时空预测在医学影像、流体动力学等领域至关重要，但现有无约束神经网络模型存在长期预测漂移和误差累积问题。需要结合物理先验来提高预测的稳定性、可解释性和解剖学一致性。

Method: 提出薛定谔启发的物理引导架构，从观测的体序列中学习体素级的振幅、相位和势场，定义复值波函数ψ=Ae^{iφ}，通过可微分的、展开的薛定谔时间步进器进行时间演化。该架构将物理演化算子直接集成到深度学习框架中。

Result: 在模拟真实形状变形和拓扑变化的合成基准测试中，展示了准确稳定的未来4D状态预测，包括体素强度和变形场。模型表现出良好的时间稳定性和可解释性。

Conclusion: 这是首个将薛定谔型演化算子集成到端到端4D神经预测框架中的方法，为可解释、稳定且解剖学一致的时空预测提供了原则性途径，结合了深度网络的表达能力和物理建模的鲁棒性。

Abstract: Spatiotemporal forecasting of complex three-dimensional phenomena (4D: 3D + time) is fundamental to applications in medical imaging, fluid and material dynamics, and geophysics. In contrast to unconstrained neural forecasting models, we propose a Schrödinger-inspired, physics-guided neural architecture that embeds an explicit time-evolution operator within a deep convolutional framework for 4D prediction. From observed volumetric sequences, the model learns voxelwise amplitude, phase, and potential fields that define a complex-valued wavefunction $ψ= A e^{iφ}$, which is evolved forward in time using a differentiable, unrolled Schrödinger time stepper. This physics-guided formulation yields several key advantages: (i) temporal stability arising from the structured evolution operator, which mitigates drift and error accumulation in long-horizon forecasting; (ii) an interpretable latent representation, where phase encodes transport dynamics, amplitude captures structural intensity, and the learned potential governs spatiotemporal interactions; and (iii) natural compatibility with deformation-based synthesis, which is critical for preserving anatomical fidelity in medical imaging applications. By integrating physical priors directly into the learning process, the proposed approach combines the expressivity of deep networks with the robustness and interpretability of physics-based modeling. We demonstrate accurate and stable prediction of future 4D states, including volumetric intensities and deformation fields, on synthetic benchmarks that emulate realistic shape deformations and topological changes. To our knowledge, this is the first end-to-end 4D neural forecasting framework to incorporate a Schrödinger-type evolution operator, offering a principled pathway toward interpretable, stable, and anatomically consistent spatiotemporal prediction.

</details>


### [353] [HPC: Hierarchical Point-based Latent Representation for Streaming Dynamic Gaussian Splatting Compression](https://arxiv.org/abs/2602.00671)
*Yangzhi Ma,Bojun Liu,Wenting Liao,Dong Liu,Zhu Li,Li Li*

Main category: cs.CV

Relevance: 35.0

TL;DR: HPC提出了一种用于动态高斯溅射的流式压缩框架，采用分层点基潜在表示来减少参数冗余，并通过压缩神经网络参数实现端到端压缩，相比基线减少67%存储空间。


<details>
  <summary>Details</summary>
Motivation: 动态高斯溅射在自由视点视频中取得显著进展，但如何在保持渲染质量的同时减少内存占用以实现高效流式传输仍是一个挑战。现有方法存在参数冗余或局部相关性利用不足的问题。

Method: 提出HPC框架：1）采用分层点基潜在表示，避免未占用空间的参数冗余；2）设计定制聚合方案提高紧凑性；3）首次研究通过挖掘参数间帧相关性来压缩神经网络；4）结合潜在压缩形成端到端压缩框架。

Result: HPC显著优于现有方法，在保持高重建保真度的同时，相比基线减少67%的存储空间。

Conclusion: HPC通过分层点基潜在表示和神经网络参数压缩，有效解决了动态高斯溅射流式压缩中的参数冗余问题，实现了高效的端到端压缩。

Abstract: While dynamic Gaussian Splatting has driven significant advances in free-viewpoint video, maintaining its rendering quality with a small memory footprint for efficient streaming transmission still presents an ongoing challenge. Existing streaming dynamic Gaussian Splatting compression methods typically leverage a latent representation to drive the neural network for predicting Gaussian residuals between frames. Their core latent representations can be categorized into structured grid-based and unstructured point-based paradigms. However, the former incurs significant parameter redundancy by inevitably modeling unoccupied space, while the latter suffers from limited compactness as it fails to exploit local correlations. To relieve these limitations, we propose HPC, a novel streaming dynamic Gaussian Splatting compression framework. It employs a hierarchical point-based latent representation that operates on a per-Gaussian basis to avoid parameter redundancy in unoccupied space. Guided by a tailored aggregation scheme, these latent points achieve high compactness with low spatial redundancy. To improve compression efficiency, we further undertake the first investigation to compress neural networks for streaming dynamic Gaussian Splatting through mining and exploiting the inter-frame correlation of parameters. Combined with latent compression, this forms a fully end-to-end compression framework. Comprehensive experimental evaluations demonstrate that HPC substantially outperforms state-of-the-art methods. It achieves a storage reduction of 67% against its baseline while maintaining high reconstruction fidelity.

</details>


### [354] [V2X-DSC: Multi-Agent Collaborative Perception with Distributed Source Coding Guided Communication](https://arxiv.org/abs/2602.00687)
*Yuankun Zeng,Shaohui Li,Zhi Li,Shulan Ruan,Yu Liu,You He*

Main category: cs.CV

Relevance: 35.0

TL;DR: V2X-DSC：一种基于分布式信源编码的V2X协同感知框架，通过条件编解码器在带宽受限下实现高效特征融合，仅传输本地特征的增量信息而非冗余内容。


<details>
  <summary>Details</summary>
Motivation: 协同感知通过多智能体观测融合提升3D理解能力，但中间特征共享面临严格带宽限制，因为密集BEV特征会饱和V2X链路。观察到协作方观察同一物理世界，其特征高度相关，接收方只需获取超出其本地上下文的增量信息。

Method: 从分布式信源编码角度重新审视该问题，提出V2X-DSC框架，包含条件编解码器(DCC)。发送方将BEV特征压缩为紧凑编码，接收方使用本地特征作为边信息进行条件重建，将比特分配给互补线索而非冗余内容。这种条件结构规范化学习过程，鼓励增量表示并产生低噪声特征。

Result: 在DAIR-V2X、OPV2V和V2X-Real数据集上的实验表明，在KB级通信下实现了最先进的精度-带宽权衡，并可作为即插即用的通信层泛化到多个融合骨干网络中。

Conclusion: V2X-DSC通过分布式信源编码视角有效解决了V2X协同感知中的带宽约束问题，实现了高效的特征压缩和重建，为带宽受限环境下的多智能体感知提供了实用解决方案。

Abstract: Collaborative perception improves 3D understanding by fusing multi-agent observations, yet intermediate-feature sharing faces strict bandwidth constraints as dense BEV features saturate V2X links. We observe that collaborators view the same physical world, making their features strongly correlated; thus receivers only need innovation beyond their local context. Revisiting this from a distributed source coding perspective, we propose V2X-DSC, a framework with a Conditional Codec (DCC) for bandwidth-constrained fusion. The sender compresses BEV features into compact codes, while the receiver performs conditional reconstruction using its local features as side information, allocating bits to complementary cues rather than redundant content. This conditional structure regularizes learning, encouraging incremental representation and yielding lower-noise features. Experiments on DAIR-V2X, OPV2V, and V2X-Real demonstrate state-of-the-art accuracy-bandwidth trade-offs under KB-level communication, and generalizes as a plug-and-play communication layer across multiple fusion backbones.

</details>


### [355] [HSI-VAR: Rethinking Hyperspectral Restoration through Spatial-Spectral Visual Autoregression](https://arxiv.org/abs/2602.00749)
*Xiangming Wang,Benteng Sun,Yungeng Liu,Haijin Zeng,Yongyong Chen,Jingyong Su,Jie Liu*

Main category: cs.CV

Relevance: 35.0

TL;DR: HSI-VAR：将高光谱图像修复重新构想为自回归生成问题，通过潜在条件对齐、退化感知引导和空间-光谱适应模块，在保持结构细节的同时实现高效推理，相比扩散模型推理速度提升95.5倍。


<details>
  <summary>Details</summary>
Motivation: 高光谱图像（HSI）包含比RGB更丰富的空间-光谱信息，但现实中的HSI常受噪声、模糊和波段缺失等多种退化影响。现有生成方法（如扩散模型）需要数百次迭代，计算成本高；回归模型则产生过度平滑结果，无法保留关键结构细节。

Method: 将HSI修复重新定义为自回归生成问题，渐进建模光谱和空间依赖关系而非全局重建。核心创新：1）潜在条件对齐：耦合潜在先验和条件嵌入的语义一致性；2）退化感知引导：将混合退化为嵌入空间中的线性组合编码，推理计算成本降低近50%；3）空间-光谱适应模块：在解码阶段跨域细化细节。

Result: 在9个一体化HSI修复基准测试中达到SOTA性能，在ICVL数据集上PSNR提升3.77dB，相比扩散方法推理速度提升95.5倍，计算成本降低近50%，提供卓越的结构保留能力。

Conclusion: HSI-VAR通过自回归生成框架有效解决了高维HSI修复的计算效率和细节保留难题，为现实世界HSI修复提供了高度实用的解决方案。

Abstract: Hyperspectral images (HSIs) capture richer spatial-spectral information beyond RGB, yet real-world HSIs often suffer from a composite mix of degradations, such as noise, blur, and missing bands. Existing generative approaches for HSI restoration like diffusion models require hundreds of iterative steps, making them computationally impractical for high-dimensional HSIs. While regression models tend to produce oversmoothed results, failing to preserve critical structural details. We break this impasse by introducing HSI-VAR, rethinking HSI restoration as an autoregressive generation problem, where spectral and spatial dependencies can be progressively modeled rather than globally reconstructed. HSI-VAR incorporates three key innovations: (1) Latent-condition alignment, which couples semantic consistency between latent priors and conditional embeddings for precise reconstruction; (2) Degradation-aware guidance, which uniquely encodes mixed degradations as linear combinations in the embedding space for automatic control, remarkably achieving a nearly $50\%$ reduction in computational cost at inference; (3) A spatial-spectral adaptation module that refines details across both domains in the decoding phase. Extensive experiments on nine all-in-one HSI restoration benchmarks confirm HSI-VAR's state-of-the-art performance, achieving a 3.77 dB PSNR improvement on \textbf{\textit{ICVL}} and offering superior structure preservation with an inference speed-up of up to $95.5 \times$ compared with diffusion-based methods, making it a highly practical solution for real-world HSI restoration.

</details>


### [356] [Edge-Native Generative De-identification: Inversion-Free Flow for Privacy-Preserving Federated Skin Image Analysis](https://arxiv.org/abs/2602.00821)
*Konstantinos Moutselos,Ilias Maglogiannis*

Main category: cs.CV

Relevance: 35.0

TL;DR: 提出基于Rectified Flow Transformers的联邦学习隐私保护框架，用于皮肤病学图像分析，通过身份无关的病理特征保留在边缘设备上生成合成替代图像，防止梯度泄漏。


<details>
  <summary>Details</summary>
Motivation: 临床皮肤病学联邦学习面临隐私保护与诊断特征保留的矛盾：传统去标识化方法会降低病理保真度，而标准生成编辑技术计算密集，不适合资源受限的边缘设备。

Method: 提出身份无关的病理保留框架，使用无反转的Rectified Flow Transformers（FlowEdit）进行高保真身份转换，引入"Segment-by-Synthesis"机制在本地生成反事实健康与病理双胞胎对，提取与生物标记物解耦的差异红斑掩码。

Result: 在高分辨率临床样本上的初步验证显示，合成身份间的IoU稳定性大于0.67，身份转换时间小于20秒，可在边缘设备上生成隐私合规的合成替代图像。

Conclusion: 该框架通过在源头生成隐私合规的合成替代图像，减轻梯度泄漏风险，为联邦环境中的高精度皮肤图像分析提供了安全途径。

Abstract: The deployment of Federated Learning (FL) for clinical dermatology is hindered by the competing requirements of protecting patient privacy and preserving diagnostic features. Traditional de-identification methods often degrade pathological fidelity, while standard generative editing techniques rely on computationally intensive inversion processes unsuitable for resource-constrained edge devices. We propose a framework for identity-agnostic pathology preservation that serves as a client-side privacy-preserving utility. By leveraging inversion-free Rectified Flow Transformers (FlowEdit), the system performs high-fidelity identity transformation in near real-time (less than 20s), facilitating local deployment on clinical nodes. We introduce a "Segment-by-Synthesis" mechanism that generates counterfactual healthy and pathological twin pairs locally. This enables the extraction of differential erythema masks that are decoupled from biometric markers and semantic artifacts (e.g. jewelry). Pilot validation on high-resolution clinical samples demonstrates an Intersection over Union (IoU) stability greater than 0.67 across synthetic identities. By generating privacy-compliant synthetic surrogates at the edge, this framework mitigates the risk of gradient leakage at the source, providing a secure pathway for high-precision skin image analysis in federated environments.

</details>


### [357] [TransNormal: Dense Visual Semantics for Diffusion-based Transparent Object Normal Estimation](https://arxiv.org/abs/2602.00839)
*Mingwei Li,Hehe Fan,Yi Yang*

Main category: cs.CV

Relevance: 35.0

TL;DR: TransNormal：利用预训练扩散先验和DINOv3语义的单步法向估计框架，专为透明物体设计，在ClearGrasp和ClearPose基准上显著优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 透明物体的单目法向估计对实验室自动化至关重要，但由于复杂的光折射和反射，传统深度和法向传感器常出现灾难性失败，阻碍了具身AI在科学环境中的部署。

Method: 1) 采用预训练扩散先验进行单步法向回归；2) 通过交叉注意力机制整合DINOv3的密集视觉语义，提供几何线索；3) 使用多任务学习目标和基于小波的正则化来保留细粒度结构细节；4) 创建物理基础的合成数据集TransNormal-Synthetic。

Result: 在ClearGrasp基准上，平均误差降低24.4%，11.25°精度提升22.8%；在ClearPose基准上，平均误差降低15.2%。

Conclusion: TransNormal通过结合扩散先验和视觉语义，有效解决了透明物体法向估计的挑战，显著优于现有方法，为实验室自动化提供了有力工具。

Abstract: Monocular normal estimation for transparent objects is critical for laboratory automation, yet it remains challenging due to complex light refraction and reflection. These optical properties often lead to catastrophic failures in conventional depth and normal sensors, hindering the deployment of embodied AI in scientific environments. We propose TransNormal, a novel framework that adapts pre-trained diffusion priors for single-step normal regression. To handle the lack of texture in transparent surfaces, TransNormal integrates dense visual semantics from DINOv3 via a cross-attention mechanism, providing strong geometric cues. Furthermore, we employ a multi-task learning objective and wavelet-based regularization to ensure the preservation of fine-grained structural details. To support this task, we introduce TransNormal-Synthetic, a physics-based dataset with high-fidelity normal maps for transparent labware. Extensive experiments demonstrate that TransNormal significantly outperforms state-of-the-art methods: on the ClearGrasp benchmark, it reduces mean error by 24.4% and improves 11.25° accuracy by 22.8%; on ClearPose, it achieves a 15.2% reduction in mean error. The code and dataset will be made publicly available at https://longxiang-ai.github.io/TransNormal.

</details>


### [358] [DIAMOND: Directed Inference for Artifact Mitigation in Flow Matching Models](https://arxiv.org/abs/2602.00883)
*Alicja Polowczyk,Agnieszka Polowczyk,Piotr Borycki,Joanna Waczyńska,Jacek Tabor,Przemysław Spurek*

Main category: cs.CV

Relevance: 35.0

TL;DR: DIAMOND是一种无需训练的方法，通过轨迹校正来减少文本到图像生成中的视觉和解剖学伪影，在推理过程中主动引导生成过程远离导致伪影的潜在状态。


<details>
  <summary>Details</summary>
Motivation: 当前文本到图像模型（如FLUX）存在视觉和解剖学伪影问题，现有后处理方法无法在核心图像形成过程中有效干预，且通常需要修改模型权重或依赖计算昂贵的区域细化过程。

Method: 提出DIAMOND方法，在推理过程中通过重建每个生成步骤的干净样本估计来进行轨迹校正，主动引导生成远离导致伪影的潜在状态。该方法无需训练，适用于标准扩散模型。

Result: DIAMOND为现代生成架构提供了一条稳健的零样本路径，能够实现高保真、无伪影的图像合成，无需额外训练或权重修改。

Conclusion: DIAMOND是一种有效的训练免费方法，能够显著减少文本到图像生成中的伪影，为实际和专业应用提供了更好的解决方案。

Abstract: Despite impressive results from recent text-to-image models like FLUX, visual and anatomical artifacts remain a significant hurdle for practical and professional use. Existing methods for artifact reduction, typically work in a post-hoc manner, consequently failing to intervene effectively during the core image formation process. Notably, current techniques require problematic and invasive modifications to the model weights, or depend on a computationally expensive and time-consuming process of regional refinement. To address these limitations, we propose DIAMOND, a training-free method that applies trajectory correction to mitigate artifacts during inference. By reconstructing an estimate of the clean sample at every step of the generative trajectory, DIAMOND actively steers the generation process away from latent states that lead to artifacts. Furthermore, we extend the proposed method to standard Diffusion Models, demonstrating that DIAMOND provides a robust, zero-shot path to high-fidelity, artifact-free image synthesis without the need for additional training or weight modifications in modern generative architectures. Code is available at https://gmum.github.io/DIAMOND/

</details>


### [359] [Hybrid Topological and Deep Feature Fusion for Accurate MRI-Based Alzheimer's Disease Severity Classification](https://arxiv.org/abs/2602.00956)
*Faisal Ahmed*

Main category: cs.CV

Relevance: 35.0

TL;DR: 提出了一种结合拓扑数据分析(TDA)和DenseNet121的混合深度学习框架，用于阿尔茨海默病的四阶段分类，在OASIS数据集上取得了99.93%的准确率和100%的AUC。


<details>
  <summary>Details</summary>
Motivation: 阿尔茨海默病的早期准确诊断在神经影像临床决策支持系统中仍然是一个关键挑战。传统神经网络往往忽略大脑结构的拓扑特征，需要一种能同时捕捉空间和拓扑信息的方法来提高分类性能。

Method: 提出混合深度学习框架：1) 使用TDA捕捉大脑结构的拓扑特征；2) 使用DenseNet121从MRI切片中学习层次化空间特征；3) 将深度特征和拓扑特征融合，增强四个AD阶段的类别可分性。

Result: 在OASIS-1 Kaggle MRI数据集上的实验表明，TDA+DenseNet121模型显著优于现有方法，达到99.93%准确率和100% AUC，超越了最近的CNN、迁移学习、集成和多尺度架构。

Conclusion: 将拓扑洞察融入深度学习流程是有效的，提出的框架可作为自动化阿尔茨海默病诊断的鲁棒且高精度工具。这展示了结合几何/拓扑分析与深度学习在医学影像分析中的潜力。

Abstract: Early and accurate diagnosis of Alzheimer's disease (AD) remains a critical challenge in neuroimaging-based clinical decision support systems. In this work, we propose a novel hybrid deep learning framework that integrates Topological Data Analysis (TDA) with a DenseNet121 backbone for four-class Alzheimer's disease classification using structural MRI data from the OASIS dataset. TDA is employed to capture complementary topological characteristics of brain structures that are often overlooked by conventional neural networks, while DenseNet121 efficiently learns hierarchical spatial features from MRI slices. The extracted deep and topological features are fused to enhance class separability across the four AD stages.
  Extensive experiments conducted on the OASIS-1 Kaggle MRI dataset demonstrate that the proposed TDA+DenseNet121 model significantly outperforms existing state-of-the-art approaches. The model achieves an accuracy of 99.93% and an AUC of 100%, surpassing recently published CNN-based, transfer learning, ensemble, and multi-scale architectures. These results confirm the effectiveness of incorporating topological insights into deep learning pipelines and highlight the potential of the proposed framework as a robust and highly accurate tool for automated Alzheimer's disease diagnosis.

</details>


### [360] [Navigating Simply, Aligning Deeply: Winning Solutions for Mouse vs. AI 2025](https://arxiv.org/abs/2602.00982)
*Phu-Hoa Pham,Chi-Nguyen Tran,Dao Sy Duy Minh,Nguyen Lam Phu Quy,Huynh Trung Kiet*

Main category: cs.CV

Relevance: 35.0

TL;DR: 该论文介绍了NeurIPS 2025 Mouse vs. AI竞赛中视觉鲁棒性和神经对齐两个赛道的获胜方法。Track 1使用轻量级CNN+GLU实现95.4%得分，Track 2使用深度ResNet-like架构达到最佳神经预测性能。研究发现训练时长与性能呈非单调关系，200K步时最优。


<details>
  <summary>Details</summary>
Motivation: 解决人工代理与生物视觉系统之间的视觉鲁棒性和神经对齐挑战，开发能够匹配生物视觉系统的鲁棒、生物启发的视觉代理。

Method: Track 1：轻量级两层CNN，增强门控线性单元和观测归一化。Track 2：16层卷积的深度ResNet-like架构，基于GLU的门控机制。系统分析了10个模型检查点（60K-1.14M步），进行消融研究和失败案例分析。

Result: Track 1获得95.4%最终得分，Track 2达到top-1神经预测性能（1780万参数）。训练时长与性能呈非单调关系，200K步左右达到最优。简单架构在视觉鲁棒性上表现更好，而深度模型在神经对齐上更优。

Conclusion: 挑战了视觉运动学习中模型复杂度的传统假设，为开发鲁棒、生物启发的视觉代理提供实用指导。架构简单性结合针对性组件能实现优越泛化，而深度模型增加容量能实现更好的神经对齐。

Abstract: Visual robustness and neural alignment remain critical challenges in developing artificial agents that can match biological vision systems. We present the winning approaches from Team HCMUS_TheFangs for both tracks of the NeurIPS 2025 Mouse vs. AI: Robust Visual Foraging Competition. For Track 1 (Visual Robustness), we demonstrate that architectural simplicity combined with targeted components yields superior generalization, achieving 95.4% final score with a lightweight two-layer CNN enhanced by Gated Linear Units and observation normalization. For Track 2 (Neural Alignment), we develop a deep ResNet-like architecture with 16 convolutional layers and GLU-based gating that achieves top-1 neural prediction performance with 17.8 million parameters. Our systematic analysis of ten model checkpoints trained between 60K to 1.14M steps reveals that training duration exhibits a non-monotonic relationship with performance, with optimal results achieved around 200K steps. Through comprehensive ablation studies and failure case analysis, we provide insights into why simpler architectures excel at visual robustness while deeper models with increased capacity achieve better neural alignment. Our results challenge conventional assumptions about model complexity in visuomotor learning and offer practical guidance for developing robust, biologically-inspired visual agents.

</details>


### [361] [LocalScore: Local Density-Aware Similarity Scoring for Biometrics](https://arxiv.org/abs/2602.01012)
*Yiyang Su,Minchul Kim,Jie Zhu,Christopher Perry,Feng Liu,Anil Jain,Xiaoming Liu*

Main category: cs.CV

Relevance: 35.0

TL;DR: LocalScore：一种用于开放集生物识别的简单有效评分算法，通过k近邻显式利用图库特征分布的局部密度，提升开放集检索和验证性能。


<details>
  <summary>Details</summary>
Motivation: 传统生物识别系统在处理未注册的探针样本时存在困难，特别是在多样本图库的实际部署中，现有方法通常将同一主体的多个样本压缩为单一全局表示，导致决策边界不理想和开放集鲁棒性差。

Method: 提出LocalScore算法，基于k近邻显式地利用图库特征分布的局部密度进行评分。该方法与架构无关、损失函数独立，计算开销可忽略，可作为即插即用方案集成到现有生物识别系统中。

Result: 在多种模态上的实验表明，LocalScore在开放集检索中显著提升性能（FNIR@FPIR从53%降至40%），在验证任务中也有明显改进（TAR@FAR从51%提升至74%）。

Conclusion: LocalScore通过显式建模图库特征的局部密度，有效解决了开放集生物识别中的挑战，为现有系统提供了简单高效的改进方案。

Abstract: Open-set biometrics faces challenges with probe subjects who may not be enrolled in the gallery, as traditional biometric systems struggle to detect these non-mated probes. Despite the growing prevalence of multi-sample galleries in real-world deployments, most existing methods collapse intra-subject variability into a single global representation, leading to suboptimal decision boundaries and poor open-set robustness. To address this issue, we propose LocalScore, a simple yet effective scoring algorithm that explicitly incorporates the local density of the gallery feature distribution using the k-th nearest neighbors. LocalScore is architecture-agnostic, loss-independent, and incurs negligible computational overhead, making it a plug-and-play solution for existing biometric systems. Extensive experiments across multiple modalities demonstrate that LocalScore consistently achieves substantial gains in open-set retrieval (FNIR@FPIR reduced from 53% to 40%) and verification (TAR@FAR improved from 51% to 74%). We further provide theoretical analysis and empirical validation explaining when and why the method achieves the most significant gains based on dataset characteristics.

</details>


### [362] [ReLayout: Versatile and Structure-Preserving Design Layout Editing via Relation-Aware Design Reconstruction](https://arxiv.org/abs/2602.01046)
*Jiawei Lin,Shizhao Sun,Danqing Huang,Ting Liu,Ji Li,Jiang Bian*

Main category: cs.CV

Relevance: 35.0

TL;DR: ReLayout是一个无需三元组数据的自动设计布局编辑框架，通过关系图保持未编辑元素的布局结构，使用多模态大语言模型实现多种编辑操作。


<details>
  <summary>Details</summary>
Motivation: 设计布局编辑是设计工作流中的关键步骤，但面临用户需求表达模糊、缺乏编辑三元组数据、需要保持未编辑元素布局结构等挑战。

Method: 1) 引入四种基本编辑操作并标准化编辑格式；2) 提出关系图作为未编辑元素布局结构的约束；3) 开发关系感知设计重建(RADR)，通过自监督学习从元素、关系图和合成编辑操作重建设计；4) 使用多模态大语言模型作为RADR骨干，统一多种编辑操作。

Result: ReLayout在编辑质量、准确性和布局结构保持方面显著优于基线模型，定性和定量结果以及用户研究都证明了其优越性。

Conclusion: ReLayout为无需手动调整的自动化设计重设计提供了有效解决方案，通过关系图保持结构和自监督学习克服数据稀缺问题，展示了多模态大语言模型在设计编辑任务中的潜力。

Abstract: Automated redesign without manual adjustments marks a key step forward in the design workflow. In this work, we focus on a foundational redesign task termed design layout editing, which seeks to autonomously modify the geometric composition of a design based on user intents. To overcome the ambiguity of user needs expressed in natural language, we introduce four basic and important editing actions and standardize the format of editing operations. The underexplored task presents a unique challenge: satisfying specified editing operations while simultaneously preserving the layout structure of unedited elements. Besides, the scarcity of triplet (original design, editing operation, edited design) samples poses another formidable challenge. To this end, we present ReLayout, a novel framework for versatile and structure-preserving design layout editing that operates without triplet data. Specifically, ReLayout first introduces the relation graph, which contains the position and size relationships among unedited elements, as the constraint for layout structure preservation. Then, relation-aware design reconstruction (RADR) is proposed to bypass the data challenge. By learning to reconstruct a design from its elements, a relation graph, and a synthesized editing operation, RADR effectively emulates the editing process in a self-supervised manner. A multi-modal large language model serves as the backbone for RADR, unifying multiple editing actions within a single model and thus achieving versatile editing after fine-tuning. Qualitative, quantitative results and user studies show that ReLayout significantly outperforms the baseline models in terms of editing quality, accuracy, and layout structure preservation.

</details>


### [363] [Baseline Method of the Foundation Model Challenge for Ultrasound Image Analysis](https://arxiv.org/abs/2602.01055)
*Bo Deng,Yitong Tang,Jiake Li,Yuxin Huang,Li Wang,Yu Zhang,Yufei Zhan,Hua Lu,Xiaoshen Zhang,Jieyun Bai*

Main category: cs.CV

Relevance: 35.0

TL;DR: 本文提出了FM_UIA 2026超声图像分析基础模型挑战赛的官方基线模型，采用统一的多头多任务学习框架，支持分割、分类、检测和回归等27个子任务，为超声图像分析基础模型研究建立了可扩展的基线。


<details>
  <summary>Details</summary>
Motivation: 超声图像在不同解剖结构和采集协议下存在显著异质性，现有方法多为任务特定型，缺乏适合临床部署的通用基础模型。需要建立统一的多任务基准来推动超声图像分析基础模型的发展。

Method: 采用统一的多头多任务学习框架，使用ImageNet预训练的EfficientNet-B4作为骨干网络，结合特征金字塔网络捕获多尺度上下文信息。通过任务特定路由策略，全局任务利用高层语义特征，密集预测任务利用空间细节特征。训练采用复合损失函数、任务自适应学习率缩放和余弦退火调度。

Result: 验证结果表明该统一设计具有可行性和鲁棒性，为超声基础模型研究建立了强大且可扩展的基线。代码和数据集已公开。

Conclusion: 提出的MH-MTL框架成功解决了超声图像分析的异质性问题，为开发临床可部署的超声基础模型提供了有效的统一解决方案，建立了FM_UIA 2026挑战赛的基准。

Abstract: Ultrasound (US) imaging exhibits substantial heterogeneity across anatomical structures and acquisition protocols, posing significant challenges to the development of generalizable analysis models. Most existing methods are task-specific, limiting their suitability as clinically deployable foundation models. To address this limitation, the Foundation Model Challenge for Ultrasound Image Analysis (FM\_UIA~2026) introduces a large-scale multi-task benchmark comprising 27 subtasks across segmentation, classification, detection, and regression. In this paper, we present the official baseline for FM\_UIA~2026 based on a unified Multi-Head Multi-Task Learning (MH-MTL) framework that supports all tasks within a single shared network. The model employs an ImageNet-pretrained EfficientNet--B4 backbone for robust feature extraction, combined with a Feature Pyramid Network (FPN) to capture multi-scale contextual information. A task-specific routing strategy enables global tasks to leverage high-level semantic features, while dense prediction tasks exploit spatially detailed FPN representations. Training incorporates a composite loss with task-adaptive learning rate scaling and a cosine annealing schedule. Validation results demonstrate the feasibility and robustness of this unified design, establishing a strong and extensible baseline for ultrasound foundation model research. The code and dataset are publicly available at \href{https://github.com/lijiake2408/Foundation-Model-Challenge-for-Ultrasound-Image-Analysis}{GitHub}.

</details>


### [364] [DRFormer: A Dual-Regularized Bidirectional Transformer for Person Re-identification](https://arxiv.org/abs/2602.01059)
*Ying Shu,Pujian Zhan,Huiqi Yang,Hehe Fan,Youfang Lin,Kai Lv*

Main category: cs.CV

Relevance: 35.0

TL;DR: 提出DRFormer框架，通过双正则化双向Transformer结合DINO的局部纹理特征和CLIP的全局语义特征，解决行人重识别中的遮挡和姿态变化问题


<details>
  <summary>Details</summary>
Motivation: 现有方法主要依赖单一范式（要么使用DINO挖掘局部纹理，要么使用CLIP捕获全局语义差异），忽略了两种架构结合的潜在优势。作者分析了这两种架构的互补作用，旨在解决行人重识别中的遮挡和姿态变化挑战。

Method: 提出DRFormer（双正则化双向Transformer）框架，通过双正则化机制确保多样化的特征提取，并平衡两种模型的贡献。该框架协同结合了DINO的局部纹理挖掘能力和CLIP的全局语义差异捕获能力。

Result: 在五个基准测试上的广泛实验表明，该方法有效地协调了局部和全局表示，实现了与最先进方法相竞争的性能。

Conclusion: 通过DRFormer框架成功整合了视觉基础模型（DINO）和视觉语言模型（CLIP）的优势，解决了行人重识别中的关键挑战，证明了结合局部纹理和全局语义特征的有效性。

Abstract: Both fine-grained discriminative details and global semantic features can contribute to solving person re-identification challenges, such as occlusion and pose variations. Vision foundation models (\textit{e.g.}, DINO) excel at mining local textures, and vision-language models (\textit{e.g.}, CLIP) capture strong global semantic difference. Existing methods predominantly rely on a single paradigm, neglecting the potential benefits of their integration. In this paper, we analyze the complementary roles of these two architectures and propose a framework to synergize their strengths by a \textbf{D}ual-\textbf{R}egularized Bidirectional \textbf{Transformer} (\textbf{DRFormer}). The dual-regularization mechanism ensures diverse feature extraction and achieves a better balance in the contributions of the two models. Extensive experiments on five benchmarks show that our method effectively harmonizes local and global representations, achieving competitive performance against state-of-the-art methods.

</details>


### [365] [Robust Harmful Meme Detection under Missing Modalities via Shared Representation Learning](https://arxiv.org/abs/2602.01101)
*Felix Breiteneder,Mohammad Belal,Muhammad Saad Saeed,Shahed Masoudian,Usman Naseem,Kulshrestha Juhi,Markus Schedl,Shah Nawaz*

Main category: cs.CV

Relevance: 35.0

TL;DR: 论文提出了一种针对模态不完整数据的有害表情包检测方法，通过独立投影学习共享表示，在文本缺失时优于现有方法


<details>
  <summary>Details</summary>
Motivation: 互联网表情包是强大的传播工具，但可能被用于传播仇恨。现有检测方法依赖完整的模态数据（文本和图像），而现实场景中文本可能因OCR质量差等原因缺失，导致现有方法性能下降

Method: 提出一种新的基线方法，通过独立投影学习多模态的共享表示，这些共享表示可以在模态不完整时被利用。方法减少对文本的依赖，更好地整合视觉特征

Result: 在两个基准数据集上的实验结果表明，该方法在文本缺失时优于现有方法，能更好地整合视觉特征，提高在文本信息缺失场景下的鲁棒性

Conclusion: 这项工作是首个全面研究模态不完整数据下有害表情包检测方法行为的开创性工作，为实现有害表情包检测在现实世界中的应用迈出了重要一步

Abstract: Internet memes are powerful tools for communication, capable of spreading political, psychological, and sociocultural ideas. However, they can be harmful and can be used to disseminate hate toward targeted individuals or groups. Although previous studies have focused on designing new detection methods, these often rely on modal-complete data, such as text and images. In real-world settings, however, modalities like text may be missing due to issues like poor OCR quality, making existing methods sensitive to missing information and leading to performance deterioration. To address this gap, in this paper, we present the first-of-its-kind work to comprehensively investigate the behavior of harmful meme detection methods in the presence of modal-incomplete data. Specifically, we propose a new baseline method that learns a shared representation for multiple modalities by projecting them independently. These shared representations can then be leveraged when data is modal-incomplete. Experimental results on two benchmark datasets demonstrate that our method outperforms existing approaches when text is missing. Moreover, these results suggest that our method allows for better integration of visual features, reducing dependence on text and improving robustness in scenarios where textual information is missing. Our work represents a significant step forward in enabling the real-world application of harmful meme detection, particularly in situations where a modality is absent.

</details>


### [366] [Boosting Point-supervised Temporal Action Localization via Text Refinement and Alignment](https://arxiv.org/abs/2602.01257)
*Yunchuan Ma,Laiyun Qing,Guorong Li,Yuqing Liu,Yuankai Qi,Qingming Huang*

Main category: cs.CV

Relevance: 35.0

TL;DR: 提出TRA框架，利用文本特征补充视觉特征，通过文本精炼和对齐模块提升点监督时序动作定位性能


<details>
  <summary>Details</summary>
Motivation: 当前点监督时序动作定位方法仅考虑视觉特征，忽略了文本侧的语义信息。文本描述包含丰富语义，可以补充视觉特征，提升定位精度。

Method: 提出文本精炼与对齐框架，包含两个新模块：基于点的文本精炼模块和基于点的多模态对齐模块。首先用预训练多模态模型生成视频帧描述，然后用点标注和多个预训练模型精炼描述，最后通过点级多模态特征对比学习对齐视觉和语言模态。

Result: 在五个广泛使用的基准测试上展示了优于多个SOTA方法的性能，且计算开销分析表明可在单张24GB RTX 3090 GPU上运行，具有实用性和可扩展性。

Conclusion: 通过有效利用文本特征补充视觉特征，提出的TRA框架在点监督时序动作定位中取得了优异性能，证明了多模态信息融合的重要性。

Abstract: Recently, point-supervised temporal action localization has gained significant attention for its effective balance between labeling costs and localization accuracy. However, current methods only consider features from visual inputs, neglecting helpful semantic information from the text side. To address this issue, we propose a Text Refinement and Alignment (TRA) framework that effectively utilizes textual features from visual descriptions to complement the visual features as they are semantically rich. This is achieved by designing two new modules for the original point-supervised framework: a Point-based Text Refinement module (PTR) and a Point-based Multimodal Alignment module (PMA). Specifically, we first generate descriptions for video frames using a pre-trained multimodal model. Next, PTR refines the initial descriptions by leveraging point annotations together with multiple pre-trained models. PMA then projects all features into a unified semantic space and leverages a point-level multimodal feature contrastive learning to reduce the gap between visual and linguistic modalities. Last, the enhanced multi-modal features are fed into the action detector for precise localization. Extensive experimental results on five widely used benchmarks demonstrate the favorable performance of our proposed framework compared to several state-of-the-art methods. Moreover, our computational overhead analysis shows that the framework can run on a single 24 GB RTX 3090 GPU, indicating its practicality and scalability.

</details>


### [367] [OASIS-DC: Generalizable Depth Completion via Output-level Alignment of Sparse-Integrated Monocular Pseudo Depth](https://arxiv.org/abs/2602.01268)
*Jaehyeon Cho,Jhonghyun An*

Main category: cs.CV

Relevance: 35.0

TL;DR: 该论文提出了一种将单目基础模型的相对深度估计转换为度量深度的方法，通过稀疏测距测量进行校准，并设计了一个精炼网络，在仅有少量标注样本的情况下实现准确的度量深度预测。


<details>
  <summary>Details</summary>
Motivation: 单目基础模型在零样本深度估计方面表现出色，但其输出本质上是相对的而非度量的，这限制了它们在机器人和自动驾驶等需要精确度量深度的应用中的直接使用。现实世界中标注数据稀缺，需要一种能够在少量标注样本下实现准确度量深度预测的方法。

Method: 1. 利用相对深度保持全局布局和边界的特点，通过稀疏测距测量进行校准，将其转换为伪度量深度先验。2. 基于这个先验设计精炼网络，在可靠区域遵循先验，在需要时进行偏离，从而从极少标注样本中实现准确的度量预测。

Result: 该系统在缺乏精心策划验证数据的情况下特别有效，能够在少样本场景中保持稳定的尺度和锐利边缘。结果表明，将基础先验与稀疏锚点结合是在现实世界标注稀缺情况下实现稳健、可部署深度补全的实用途径。

Conclusion: 通过将单目基础模型的相对深度先验与稀疏测距测量相结合，可以构建一个在标注数据稀缺情况下仍能实现准确度量深度预测的系统，这对于机器人和自动驾驶等实际应用具有重要意义。

Abstract: Recent monocular foundation models excel at zero-shot depth estimation, yet their outputs are inherently relative rather than metric, limiting direct use in robotics and autonomous driving. We leverage the fact that relative depth preserves global layout and boundaries: by calibrating it with sparse range measurements, we transform it into a pseudo metric depth prior. Building on this prior, we design a refinement network that follows the prior where reliable and deviates where necessary, enabling accurate metric predictions from very few labeled samples. The resulting system is particularly effective when curated validation data are unavailable, sustaining stable scale and sharp edges across few-shot regimes. These findings suggest that coupling foundation priors with sparse anchors is a practical route to robust, deployment-ready depth completion under real-world label scarcity.

</details>


### [368] [TF-Lane: Traffic Flow Module for Robust Lane Perception](https://arxiv.org/abs/2602.01277)
*Yihan Xie,Han Xia,Zhen Yang*

Main category: cs.CV

Relevance: 35.0

TL;DR: 提出TFM模块，利用实时交通流信息增强车道感知，解决遮挡或车道缺失场景下的视觉感知性能下降问题


<details>
  <summary>Details</summary>
Motivation: 现有基于视觉的车道感知方法在遮挡或车道缺失场景下性能显著下降，而使用高精地图作为补充信息存在订阅成本高和实时性受限的问题。交通流信息提供了无需额外成本的实时能力，可作为创新的信息源来增强车道感知。

Method: 提出TrafficFlow-aware Lane perception Module (TFM)，有效提取实时交通流特征，并将其与现有车道感知算法无缝集成。该方法源自真实自动驾驶场景，并在开源算法和数据集上进行验证。

Result: 在四个主流模型和两个公开数据集（Nuscenes和OpenLaneV2）上的大量实验表明，TFM能持续提升性能，在Nuscenes数据集上实现了高达+4.1%的mAP增益。

Conclusion: TFM通过利用交通流信息有效增强了车道感知能力，为自动驾驶系统提供了更鲁棒的车道感知解决方案，特别是在视觉传感器提供信息不足的场景下。

Abstract: Autonomous driving systems require robust lane perception capabilities, yet existing vision-based detection methods suffer significant performance degradation when visual sensors provide insufficient cues, such as in occluded or lane-missing scenarios. While some approaches incorporate high-definition maps as supplementary information, these solutions face challenges of high subscription costs and limited real-time performance. To address these limitations, we explore an innovative information source: traffic flow, which offers real-time capabilities without additional costs. This paper proposes a TrafficFlow-aware Lane perception Module (TFM) that effectively extracts real-time traffic flow features and seamlessly integrates them with existing lane perception algorithms. This solution originated from real-world autonomous driving conditions and was subsequently validated on open-source algorithms and datasets. Extensive experiments on four mainstream models and two public datasets (Nuscenes and OpenLaneV2) using standard evaluation metrics show that TFM consistently improves performance, achieving up to +4.1% mAP gain on the Nuscenes dataset.

</details>


### [369] [DSFC-Net: A Dual-Encoder Spatial and Frequency Co-Awareness Network for Rural Road Extraction](https://arxiv.org/abs/2602.01278)
*Zhengbo Zhang,Yihe Tian,Wanke Xia,Lin Chen,Yue Sun,Kun Ding,Ying Wang,Bing Xu,Shiming Xiang*

Main category: cs.CV

Relevance: 35.0

TL;DR: 提出DSFC-Net用于高分辨率遥感影像中的乡村道路提取，通过双编码器框架融合空间和频域信息，解决乡村道路的高类内变异、植被遮挡和狭窄宽度等挑战。


<details>
  <summary>Details</summary>
Motivation: 乡村道路提取对基础设施规划和可持续发展至关重要，但面临独特挑战：1）不同表面材料导致的高类内变异和低类间可分性；2）植被遮挡破坏空间连续性；3）狭窄道路宽度加剧检测难度。现有方法主要针对结构化城市环境优化，在这些场景中表现不佳。

Method: 提出DSFC-Net双编码器框架：1）CNN分支捕捉细粒度局部道路边界和短程连续性；2）新颖的空间-频率混合Transformer（SFT）建模全局拓扑依赖关系以对抗植被遮挡；3）交叉频率交互注意力（CFIA）模块通过拉普拉斯金字塔策略显式解耦高/低频信息；4）通道特征融合模块（CFFM）自适应重新校准通道特征响应，整合局部纹理和全局语义。

Result: 在WHU-RuR+、DeepGlobe和Massachusetts数据集上的综合实验验证了DSFC-Net相对于最先进方法的优越性。

Conclusion: DSFC-Net通过融合空间和频域信息，有效解决了乡村道路提取的独特挑战，在多个基准数据集上表现出色。

Abstract: Accurate extraction of rural roads from high-resolution remote sensing imagery is essential for infrastructure planning and sustainable development. However, this task presents unique challenges in rural settings due to several factors. These include high intra-class variability and low inter-class separability from diverse surface materials, frequent vegetation occlusions that disrupt spatial continuity, and narrow road widths that exacerbate detection difficulties. Existing methods, primarily optimized for structured urban environments, often underperform in these scenarios as they overlook such distinctive characteristics. To address these challenges, we propose DSFC-Net, a dual-encoder framework that synergistically fuses spatial and frequency-domain information. Specifically, a CNN branch is employed to capture fine-grained local road boundaries and short-range continuity, while a novel Spatial-Frequency Hybrid Transformer (SFT) is introduced to robustly model global topological dependencies against vegetation occlusions. Distinct from standard attention mechanisms that suffer from frequency bias, the SFT incorporates a Cross-Frequency Interaction Attention (CFIA) module that explicitly decouples high- and low-frequency information via a Laplacian Pyramid strategy. This design enables the dynamic interaction between spatial details and frequency-aware global contexts, effectively preserving the connectivity of narrow roads. Furthermore, a Channel Feature Fusion Module (CFFM) is proposed to bridge the two branches by adaptively recalibrating channel-wise feature responses, seamlessly integrating local textures with global semantics for accurate segmentation. Comprehensive experiments on the WHU-RuR+, DeepGlobe, and Massachusetts datasets validate the superiority of DSFC-Net over state-of-the-art approaches.

</details>


### [370] [DeCorStory: Gram-Schmidt Prompt Embedding Decorrelation for Consistent Storytelling](https://arxiv.org/abs/2602.01306)
*Ayushman Sarkar,Zhenyu Yu,Mohd Yamani Idna Idris*

Main category: cs.CV

Relevance: 35.0

TL;DR: DeCorStory：一种无需训练、推理时的方法，通过Gram-Schmidt提示嵌入去相关和奇异值重加权，减少文本到图像故事生成中的帧间语义干扰，提升视觉一致性


<details>
  <summary>Details</summary>
Motivation: 现有无需训练的方法（如One-Prompt-One-Story）将所有提示连接成单一序列，导致强嵌入相关性，引起颜色泄漏、背景混合和身份漂移等问题。需要一种方法在推理时显式减少帧间语义干扰。

Method: 1) Gram-Schmidt提示嵌入去相关：正交化帧级语义；2) 奇异值重加权：增强提示特定信息；3) 身份保持交叉注意力：在扩散过程中稳定角色身份。无需模型修改或微调，可无缝集成到现有扩散流程。

Result: 实验表明在提示-图像对齐、身份一致性和视觉多样性方面有持续改进，在无需训练的基线方法中达到最先进性能。

Conclusion: DeCorStory通过推理时嵌入去相关和语义增强，有效解决了文本到图像故事生成中的视觉一致性挑战，无需额外训练即可提升生成质量。

Abstract: Maintaining visual and semantic consistency across frames is a key challenge in text-to-image storytelling. Existing training-free methods, such as One-Prompt-One-Story, concatenate all prompts into a single sequence, which often induces strong embedding correlation and leads to color leakage, background blending, and identity drift. We propose DeCorStory, a training-free inference-time framework that explicitly reduces inter-frame semantic interference. DeCorStory applies Gram-Schmidt prompt embedding decorrelation to orthogonalize frame-level semantics, followed by singular value reweighting to strengthen prompt-specific information and identity-preserving cross-attention to stabilize character identity during diffusion. The method requires no model modification or fine-tuning and can be seamlessly integrated into existing diffusion pipelines. Experiments demonstrate consistent improvements in prompt-image alignment, identity consistency, and visual diversity, achieving state-of-the-art performance among training-free baselines. Code is available at: https://github.com/YuZhenyuLindy/DeCorStory

</details>


### [371] [MTC-VAE: Multi-Level Temporal Compression with Content Awareness](https://arxiv.org/abs/2602.01340)
*Yubo Dong,Linchao Zhu*

Main category: cs.CV

Relevance: 35.0

TL;DR: 提出一种将固定压缩率VAE转换为支持多级时间压缩的方法，通过最小微调解决高压缩率下的性能下降问题，并探索了与扩散模型的集成。


<details>
  <summary>Details</summary>
Motivation: 在潜在视频扩散模型中，使用VAE压缩视频时，提高压缩率通常会导致性能下降，特别是当增加采样层而不扩展隐藏通道维度时效率显著降低。需要一种方法在保持性能的同时实现多级时间压缩。

Method: 提出一种技术将固定压缩率VAE转换为支持多级时间压缩的模型，采用简单的最小微调方法来对抗高压缩率下的性能下降。研究了不同压缩级别对视频片段性能的影响，并探索了与DiT等扩散生成模型的集成。

Result: 提供了多级时间压缩VAE有效性的实证证据，展示了与扩散模型的成功并发训练和兼容性，证明了该方法的实用潜力。

Conclusion: 该方法能够有效解决高压缩率下的性能下降问题，实现多级时间压缩，并与现有扩散模型框架兼容，为视频生成和压缩提供了新的技术路径。

Abstract: Latent Video Diffusion Models (LVDMs) rely on Variational Autoencoders (VAEs) to compress videos into compact latent representations. For continuous Variational Autoencoders (VAEs), achieving higher compression rates is desirable; yet, the efficiency notably declines when extra sampling layers are added without expanding the dimensions of hidden channels. In this paper, we present a technique to convert fixed compression rate VAEs into models that support multi-level temporal compression, providing a straightforward and minimal fine-tuning approach to counteract performance decline at elevated compression rates.Moreover, we examine how varying compression levels impact model performance over video segments with diverse characteristics, offering empirical evidence on the effectiveness of our proposed approach. We also investigate the integration of our multi-level temporal compression VAE with diffusion-based generative models, DiT, highlighting successful concurrent training and compatibility within these frameworks. This investigation illustrates the potential uses of multi-level temporal compression.

</details>


### [372] [T2M Mamba: Motion Periodicity-Saliency Coupling Approach for Stable Text-Driven Motion Generation](https://arxiv.org/abs/2602.01352)
*Xingzu Zhan,Chen Xie,Honghang Chen,Yixun Lin,Xiaochun Mai*

Main category: cs.CV

Relevance: 35.0

TL;DR: T2M Mamba：一种用于文本到动作生成的新方法，通过周期性-显著性感知Mamba和周期性差分跨模态对齐模块，解决现有模型在长序列生成漂移和语义等价重述脆弱性方面的问题。


<details>
  <summary>Details</summary>
Motivation: 现有文本到动作生成模型存在两个核心限制：1) 将动作周期性和关键帧显著性视为独立因素，忽略了它们的耦合关系，导致长序列生成漂移；2) 对语义等价重述脆弱，微小的同义词替换会扭曲文本嵌入，通过解码器传播产生不稳定或错误的动作。

Method: 提出T2M Mamba方法：1) 周期性-显著性感知Mamba，通过增强密度峰值聚类进行关键帧权重估计，通过FFT加速自相关进行动作周期性估计，以最小计算开销捕获耦合动态；2) 周期性差分跨模态对齐模块(PDCAM)，增强文本和动作嵌入的鲁棒对齐。

Result: 在HumanML3D和KIT-ML数据集上的广泛实验证实了方法的有效性，实现了0.068的FID分数，并在所有其他指标上获得一致增益。

Conclusion: T2M Mamba通过有效建模动作周期性和关键帧显著性之间的耦合关系，并增强跨模态对齐的鲁棒性，显著提升了文本到动作生成的性能，特别是在长序列和语义变化场景下。

Abstract: Text-to-motion generation, which converts motion language descriptions into coherent 3D human motion sequences, has attracted increasing attention in fields, such as avatar animation and humanoid robotic interaction. Though existing models have achieved significant fidelity, they still suffer from two core limitations: (i) They treat motion periodicity and keyframe saliency as independent factors, overlooking their coupling and causing generation drift in long sequences. (ii) They are fragile to semantically equivalent paraphrases, where minor synonym substitutions distort textual embeddings, propagating through the decoder and producing unstable or erroneous motions. In this work, we propose T2M Mamba to address these limitations by (i) proposing Periodicity-Saliency Aware Mamba, which utilizes novel algorithms for keyframe weight estimation via enhanced Density Peaks Clustering and motion periodicity estimation via FFT-accelerated autocorrelation to capture coupled dynamics with minimal computational overhead, and (ii) constructing a Periodic Differential Cross-modal Alignment Module (PDCAM) to enhance robust alignment of textual and motion embeddings. Extensive experiments on HumanML3D and KIT-ML datasets have been conducted, confirming the effectiveness of our approach, achieving an FID of 0.068 and consistent gains on all other metrics.

</details>


### [373] [Cross-Paradigm Evaluation of Gaze-Based Semantic Object Identification for Intelligent Vehicles](https://arxiv.org/abs/2602.01452)
*Penghao Deng,Jidong J. Yang,Jiachen Bian*

Main category: cs.CV

Relevance: 35.0

TL;DR: 该论文研究驾驶员视觉注意力定位，通过三种视觉方法（直接目标检测、分割辅助分类、视觉语言模型）分析注视点与道路场景语义的关联，发现YOLOv13和Qwen2.5-VL-32b表现最佳，揭示了传统检测器实时效率与大型VLM上下文理解之间的权衡。


<details>
  <summary>Details</summary>
Motivation: 理解驾驶员在驾驶过程中的视觉注意力分布对于开发下一代高级驾驶辅助系统和提高道路安全至关重要。需要从车辆前视摄像头捕捉的道路场景中识别注视点对应的语义对象。

Method: 采用三种视觉方法：1）直接目标检测（YOLOv13）；2）分割辅助分类（SAM2+EfficientNetV2 vs YOLOv13）；3）基于查询的视觉语言模型（Qwen2.5-VL-7b vs Qwen2.5-VL-32b）。将注视点与对象语义关联作为语义识别任务。

Result: 直接目标检测（YOLOv13）和Qwen2.5-VL-32b显著优于其他方法，Macro F1-Score超过0.84。大型VLM（Qwen2.5-VL-32b）在识别小型安全关键对象（如交通灯）方面表现出更强的鲁棒性，尤其在夜间恶劣条件下。分割辅助方法因"部分vs整体"语义差距导致召回率大幅下降。

Conclusion: 研究揭示了传统检测器的实时效率与大型VLM提供的更丰富上下文理解和鲁棒性之间的基本权衡。为未来人类感知智能驾驶员监控系统的设计提供了关键见解和实践指导。

Abstract: Understanding where drivers direct their visual attention during driving, as characterized by gaze behavior, is critical for developing next-generation advanced driver-assistance systems and improving road safety. This paper tackles this challenge as a semantic identification task from the road scenes captured by a vehicle's front-view camera. Specifically, the collocation of gaze points with object semantics is investigated using three distinct vision-based approaches: direct object detection (YOLOv13), segmentation-assisted classification (SAM2 paired with EfficientNetV2 versus YOLOv13), and query-based Vision-Language Models, VLMs (Qwen2.5-VL-7b versus Qwen2.5-VL-32b). The results demonstrate that the direct object detection (YOLOv13) and Qwen2.5-VL-32b significantly outperform other approaches, achieving Macro F1-Scores over 0.84. The large VLM (Qwen2.5-VL-32b), in particular, exhibited superior robustness and performance for identifying small, safety-critical objects such as traffic lights, especially in adverse nighttime conditions. Conversely, the segmentation-assisted paradigm suffers from a "part-versus-whole" semantic gap that led to large failure in recall. The results reveal a fundamental trade-off between the real-time efficiency of traditional detectors and the richer contextual understanding and robustness offered by large VLMs. These findings provide critical insights and practical guidance for the design of future human-aware intelligent driver monitoring systems.

</details>


### [374] [Rotation-free Online Handwritten Character Recognition Using Linear Recurrent Units](https://arxiv.org/abs/2602.01533)
*Zhe Ling,Sicheng Yu,Danyu Yang*

Main category: cs.CV

Relevance: 35.0

TL;DR: 提出SW-PS+LRU框架用于在线手写字符识别，通过滑动窗口路径签名提取局部结构特征，使用线性循环单元作为分类器，在旋转变形下实现高精度识别。


<details>
  <summary>Details</summary>
Motivation: 在线手写字符识别虽然利用笔画顺序和动态特征通常比离线识别更准确，但在实际应用中旋转变形会破坏笔画空间布局，显著降低识别精度。提取旋转不变特征仍然是一个具有挑战性的开放问题。

Method: 采用滑动窗口路径签名(SW-PS)捕捉字符的局部结构特征，引入轻量级线性循环单元(LRU)作为分类器。LRU结合了RNN的快速增量处理能力和状态空间模型(SSM)的高效并行训练，同时可靠地建模动态笔画特征。

Result: 在CASIA-OLHWDB1.1数据集的三个子集（数字、英文大写字母、中文部首）上进行随机旋转角度达±180°的识别实验，集成学习后的准确率分别为99.62%、96.67%和94.33%。SW-PS+LRU框架在收敛速度和测试精度上均优于竞争模型。

Conclusion: 提出的SW-PS+LRU框架能有效处理旋转变形问题，在在线手写字符识别中实现了高精度和鲁棒性，为解决旋转不变特征提取这一挑战性问题提供了有效方案。

Abstract: Online handwritten character recognition leverages stroke order and dynamic features, which generally provide higher accuracy and robustness compared with offline recognition. However, in practical applications, rotational deformations can disrupt the spatial layout of strokes, substantially reducing recognition accuracy. Extracting rotation-invariant features therefore remains a challenging open problem. In this work, we employ the Sliding Window Path Signature (SW-PS) to capture local structural features of characters, and introduce the lightweight Linear Recurrent Units (LRU) as the classifier. The LRU combine the fast incremental processing capability of recurrent neural networks (RNN) with the efficient parallel training of state space models (SSM), while reliably modelling dynamic stroke characteristics. We conducted recognition experiments with random rotation angle up to $\pm 180^{\circ}$ on three subsets of the CASIA-OLHWDB1.1 dataset: digits, English upper letters, and Chinese radicals. The accuracies achieved after ensemble learning were $99.62\%$, $96.67\%$, and $94.33\%$, respectively. Experimental results demonstrate that the proposed SW-PS+LRU framework consistently surpasses competing models in both convergence speed and test accuracy.

</details>


### [375] [Making Avatars Interact: Towards Text-Driven Human-Object Interaction for Controllable Talking Avatars](https://arxiv.org/abs/2602.01538)
*Youliang Zhang,Zhengguang Zhou,Zhentao Yu,Ziyao Huang,Teng Hu,Sen Liang,Guozhen Zhang,Ziqiao Peng,Shunkai Li,Yi Chen,Zixiang Zhou,Yuan Zhou,Qinglin Lu,Xiu Li*

Main category: cs.CV

Relevance: 35.0

TL;DR: 提出InteractAvatar框架，通过感知-规划-生成双流架构解决接地人-物交互视频生成中的控制-质量困境，建立GroundInter基准进行评估


<details>
  <summary>Details</summary>
Motivation: 现有方法能生成简单人体运动的全身说话头像，但扩展到接地人-物交互（GHOI）仍具挑战性，需要环境感知和解决控制-质量困境

Method: 提出双流框架InteractAvatar，包含感知交互模块（PIM）生成文本对齐的交互动作，音频交互感知生成模块（AIM）合成生动的说话头像，通过运动-视频对齐器实现并行协同生成

Result: 在建立的GroundInter基准上进行了广泛实验和比较，证明了方法在生成接地人-物交互说话头像方面的有效性

Conclusion: InteractAvatar成功解决了GHOI视频生成中的环境感知和控制-质量困境问题，为说话头像的接地交互生成提供了有效解决方案

Abstract: Generating talking avatars is a fundamental task in video generation. Although existing methods can generate full-body talking avatars with simple human motion, extending this task to grounded human-object interaction (GHOI) remains an open challenge, requiring the avatar to perform text-aligned interactions with surrounding objects. This challenge stems from the need for environmental perception and the control-quality dilemma in GHOI generation. To address this, we propose a novel dual-stream framework, InteractAvatar, which decouples perception and planning from video synthesis for grounded human-object interaction. Leveraging detection to enhance environmental perception, we introduce a Perception and Interaction Module (PIM) to generate text-aligned interaction motions. Additionally, an Audio-Interaction Aware Generation Module (AIM) is proposed to synthesize vivid talking avatars performing object interactions. With a specially designed motion-to-video aligner, PIM and AIM share a similar network structure and enable parallel co-generation of motions and plausible videos, effectively mitigating the control-quality dilemma. Finally, we establish a benchmark, GroundedInter, for evaluating GHOI video generation. Extensive experiments and comparisons demonstrate the effectiveness of our method in generating grounded human-object interactions for talking avatars. Project page: https://interactavatar.github.io

</details>


### [376] [FSCA-Net: Feature-Separated Cross-Attention Network for Robust Multi-Dataset Training](https://arxiv.org/abs/2602.01540)
*Yuehai Chen*

Main category: cs.CV

Relevance: 35.0

TL;DR: FSCA-Net是一个用于人群计数的跨域泛化框架，通过特征分离和交叉注意力机制解决多数据集联合训练中的负迁移问题。


<details>
  <summary>Details</summary>
Motivation: CNN和Transformer模型在人群计数中表现良好，但在跨域应用时性能下降。多数据集联合训练本应增强泛化能力，却导致负迁移问题，因为共享特征和域特定特征纠缠在一起。

Method: 提出FSCA-Net框架：1) 将特征显式解耦为域不变和域特定组件；2) 新颖的交叉注意力融合模块自适应建模组件间交互；3) 引入互信息优化目标，最大化域不变特征一致性，最小化域特定特征冗余。

Result: 在多个人群计数基准测试上的广泛实验表明，FSCA-Net有效缓解了负迁移问题，实现了最先进的跨数据集泛化性能。

Conclusion: FSCA-Net为现实世界人群分析提供了一个鲁棒且可扩展的解决方案，通过特征解耦和自适应融合机制解决了跨域泛化挑战。

Abstract: Crowd counting plays a vital role in public safety, traffic regulation, and smart city management. However, despite the impressive progress achieved by CNN- and Transformer-based models, their performance often deteriorates when applied across diverse environments due to severe domain discrepancies. Direct joint training on multiple datasets, which intuitively should enhance generalization, instead results in negative transfer, as shared and domain-specific representations become entangled. To address this challenge, we propose the Feature Separation and Cross-Attention Network FSCA-Net, a unified framework that explicitly disentangles feature representations into domain-invariant and domain-specific components. A novel cross-attention fusion module adaptively models interactions between these components, ensuring effective knowledge transfer while preserving dataset-specific discriminability. Furthermore, a mutual information optimization objective is introduced to maximize consistency among domain-invariant features and minimize redundancy among domain-specific ones, promoting complementary shared-private representations. Extensive experiments on multiple crowd counting benchmarks demonstrate that FSCA-Net effectively mitigates negative transfer and achieves state-of-the-art cross-dataset generalization, providing a robust and scalable solution for real-world crowd analysis.

</details>


### [377] [Combined Flicker-banding and Moire Removal for Screen-Captured Images](https://arxiv.org/abs/2602.01559)
*Libo Zhu,Zihan Zhou,Zhiyi Zhou,Yiyang Qu,Weihang Zhang,Keyu Shi,Yifan Fu,Yulun Zhang*

Main category: cs.CV

Relevance: 35.0

TL;DR: 提出了CLEAR框架，首个系统研究屏幕截图图像中摩尔纹和闪烁带纹联合去除的方法，通过构建大规模数据集和频率域分解模块实现复合伪影的有效消除。


<details>
  <summary>Details</summary>
Motivation: 移动设备拍摄显示屏图像时，摩尔纹和闪烁带纹两种伪影同时存在且强耦合，导致严重视觉质量下降。现有单一退化处理方法无法应对这种复合场景，需要新的联合去除方法。

Method: 提出CLEAR统一恢复框架：1)构建包含两种伪影的大规模数据集；2)引入基于ISP的闪烁模拟管道稳定训练并扩展退化分布；3)设计频率域分解与重构模块；4)提出轨迹对齐损失增强复合伪影建模。

Result: 在多个评估指标上，CLEAR方法一致优于现有图像恢复方法，验证了其在复杂真实场景中的有效性。

Conclusion: 首次系统研究了屏幕截图图像中摩尔纹和闪烁带纹的联合去除问题，提出的CLEAR框架通过频率域分解和轨迹对齐损失有效解决了复合伪影问题，为实际应用提供了有效解决方案。

Abstract: Capturing display screens with mobile devices has become increasingly common, yet the resulting images often suffer from severe degradations caused by the coexistence of moiré patterns and flicker-banding, leading to significant visual quality degradation. Due to the strong coupling of these two artifacts in real imaging processes, existing methods designed for single degradations fail to generalize to such compound scenarios. In this paper, we present the first systematic study on joint removal of moiré patterns and flicker-banding in screen-captured images, and propose a unified restoration framework, named CLEAR. To support this task, we construct a large-scale dataset containing both moiré patterns and flicker-banding, and introduce an ISP-based flicker simulation pipeline to stabilize model training and expand the degradation distribution. Furthermore, we design a frequency-domain decomposition and re-composition module together with a trajectory alignment loss to enhance the modeling of compound artifacts. Extensive experiments demonstrate that the proposed method consistently. outperforms existing image restoration approaches across multiple evaluation metrics, validating its effectiveness in complex real-world scenarios.

</details>


### [378] [HandMCM: Multi-modal Point Cloud-based Correspondence State Space Model for 3D Hand Pose Estimation](https://arxiv.org/abs/2602.01586)
*Wencan Cheng,Gim Hee Lee*

Main category: cs.CV

Relevance: 35.0

TL;DR: 提出HandMCM方法，基于状态空间模型(Mamba)，通过局部信息注入/过滤和对应关系建模模块，有效学习关键点动态运动学拓扑，结合多模态图像特征提升3D手部姿态估计精度，在遮挡场景下表现优异。


<details>
  <summary>Details</summary>
Motivation: 3D手部姿态估计对于增强现实等人机交互应用至关重要，但由于手部自遮挡和与物体交互造成的遮挡，该任务面临重大挑战。现有方法在严重遮挡场景下表现不佳。

Method: 基于状态空间模型(Mamba)的HandMCM方法，包含局部信息注入/过滤模块和对应关系建模模块，学习关键点动态运动学拓扑。整合多模态图像特征增强输入鲁棒性和表示能力。

Result: 在三个基准数据集上的实证评估表明，该方法显著优于当前最先进方法，特别是在涉及严重遮挡的挑战性场景中表现突出。

Conclusion: 该方法展示了在状态空间模型基础上通过局部信息处理和对应关系建模提升3D手部姿态估计准确性和可靠性的潜力，尤其在遮挡场景下具有优势。

Abstract: 3D hand pose estimation that involves accurate estimation of 3D human hand keypoint locations is crucial for many human-computer interaction applications such as augmented reality. However, this task poses significant challenges due to self-occlusion of the hands and occlusions caused by interactions with objects. In this paper, we propose HandMCM to address these challenges. Our HandMCM is a novel method based on the powerful state space model (Mamba). By incorporating modules for local information injection/filtering and correspondence modeling, the proposed correspondence Mamba effectively learns the highly dynamic kinematic topology of keypoints across various occlusion scenarios. Moreover, by integrating multi-modal image features, we enhance the robustness and representational capacity of the input, leading to more accurate hand pose estimation. Empirical evaluations on three benchmark datasets demonstrate that our model significantly outperforms current state-of-the-art methods, particularly in challenging scenarios involving severe occlusions. These results highlight the potential of our approach to advance the accuracy and reliability of 3D hand pose estimation in practical applications.

</details>


### [379] [UV-M3TL: A Unified and Versatile Multimodal Multi-Task Learning Framework for Assistive Driving Perception](https://arxiv.org/abs/2602.01594)
*Wenzhuo Liu,Qiannan Guo,Zhen Wang,Wenshuo Wang,Lei Yang,Yicheng Qiao,Lening Wang,Zhiwei Li,Chen Lv,Shanghang Zhang,Junqiang Xi,Huaping Liu*

Main category: cs.CV

Relevance: 35.0

TL;DR: 提出了一个统一的多模态多任务学习框架（UV-M3TL），用于同时识别驾驶员行为、情绪、车辆行为和交通环境，通过双分支空间通道多模态嵌入和自适应特征解耦多任务损失来缓解任务间负迁移问题。


<details>
  <summary>Details</summary>
Motivation: 高级驾驶辅助系统（ADAS）需要同时理解人类驾驶员行为和感知导航环境，但联合学习这些异构任务会导致任务间负迁移，损害系统性能。现有方法难以有效处理多模态多任务学习中的任务冲突问题。

Method: 提出了UV-M3TL框架，包含两个核心组件：1) 双分支空间通道多模态嵌入（DB-SCME），通过双分支结构显式建模任务共享和任务特定的显著特征；2) 自适应特征解耦多任务损失（AFD-Loss），基于学习动态的自适应加权机制和特征解耦约束来稳定联合优化。

Result: 在AIDE数据集上，UV-M3TL在所有四个任务上达到了最先进的性能。在额外的多任务感知基准测试（BDD100K、CityScapes、NYUD-v2和PASCAL-Context）上也表现出色，在大多数任务上获得了最先进的结果。

Conclusion: UV-M3TL框架能够有效缓解多模态多任务学习中的负迁移问题，在驾驶场景和通用多任务感知基准上都表现出强大的性能和泛化能力。

Abstract: Advanced Driver Assistance Systems (ADAS) need to understand human driver behavior while perceiving their navigation context, but jointly learning these heterogeneous tasks would cause inter-task negative transfer and impair system performance. Here, we propose a Unified and Versatile Multimodal Multi-Task Learning (UV-M3TL) framework to simultaneously recognize driver behavior, driver emotion, vehicle behavior, and traffic context, while mitigating inter-task negative transfer. Our framework incorporates two core components: dual-branch spatial channel multimodal embedding (DB-SCME) and adaptive feature-decoupled multi-task loss (AFD-Loss). DB-SCME enhances cross-task knowledge transfer while mitigating task conflicts by employing a dual-branch structure to explicitly model salient task-shared and task-specific features. AFD-Loss improves the stability of joint optimization while guiding the model to learn diverse multi-task representations by introducing an adaptive weighting mechanism based on learning dynamics and feature decoupling constraints. We evaluate our method on the AIDE dataset, and the experimental results demonstrate that UV-M3TL achieves state-of-the-art performance across all four tasks. To further prove the versatility, we evaluate UV-M3TL on additional public multi-task perception benchmarks (BDD100K, CityScapes, NYUD-v2, and PASCAL-Context), where it consistently delivers strong performance across diverse task combinations, attaining state-of-the-art results on most tasks.

</details>


### [380] [From Frames to Sequences: Temporally Consistent Human-Centric Dense Prediction](https://arxiv.org/abs/2602.01661)
*Xingyu Miao,Junting Dong,Qin Zhao,Yuhang Yang,Junhao Chen,Yang Long*

Main category: cs.CV

Relevance: 35.0

TL;DR: 提出一个用于视频中时间一致的人体中心密集预测的统一框架，通过合成数据管道生成对齐的运动序列，并训练ViT-based密集预测器，结合几何先验和特征融合优化。


<details>
  <summary>Details</summary>
Motivation: 现有模型在单帧预测上表现良好，但在运动、遮挡和光照变化下容易出现闪烁问题，且缺乏针对多个密集任务的配对人体视频监督数据。

Method: 1) 可扩展的合成数据管道生成真实感人体帧和运动对齐序列；2) 统一的ViT-based密集预测器，注入CSE嵌入作为几何先验；3) 轻量级通道重加权模块优化特征融合；4) 两阶段训练策略：静态预训练+动态序列监督。

Result: 在THuman2.1和Hi4D数据集上达到SOTA性能，并能有效泛化到真实世界视频中。

Conclusion: 通过合成数据管道和统一预测器设计，解决了视频中时间一致的人体密集预测问题，在多个基准测试中表现优异。

Abstract: In this work, we focus on the challenge of temporally consistent human-centric dense prediction across video sequences. Existing models achieve strong per-frame accuracy but often flicker under motion, occlusion, and lighting changes, and they rarely have paired human video supervision for multiple dense tasks. We address this gap with a scalable synthetic data pipeline that generates photorealistic human frames and motion-aligned sequences with pixel-accurate depth, normals, and masks. Unlike prior static data synthetic pipelines, our pipeline provides both frame-level labels for spatial learning and sequence-level supervision for temporal learning. Building on this, we train a unified ViT-based dense predictor that (i) injects an explicit human geometric prior via CSE embeddings and (ii) improves geometry-feature reliability with a lightweight channel reweighting module after feature fusion. Our two-stage training strategy, combining static pretraining with dynamic sequence supervision, enables the model first to acquire robust spatial representations and then refine temporal consistency across motion-aligned sequences. Extensive experiments show that we achieve state-of-the-art performance on THuman2.1 and Hi4D and generalize effectively to in-the-wild videos.

</details>


### [381] [Moonworks Lunara Aesthetic II: An Image Variation Dataset](https://arxiv.org/abs/2602.01666)
*Yan Wang,Partho Hassan,Samiha Sadeka,Nada Soliman,M M Sayeef Abdullah,Sabit Hassan*

Main category: cs.CV

Relevance: 35.0

TL;DR: Lunara Aesthetic II是一个公开的图像数据集，包含2,854个锚点链接的变体对，用于评估和学习图像生成/编辑系统中的上下文一致性，同时保持身份不变和高美学质量。


<details>
  <summary>Details</summary>
Motivation: 当前图像生成和编辑系统缺乏对上下文一致性（如光照、天气、视角等变化）的受控评估数据集。需要能够保持底层身份不变的同时，评估系统对上下文变化的处理能力。

Method: 从Moonworks的原创艺术和照片中创建锚点链接的变体对，应用上下文变换（光照、天气、视角、场景构图、色调、情绪等），同时保持稳定的底层身份。数据集操作化身份保持的上下文变化作为监督信号。

Result: 数据集显示出高身份稳定性、强目标属性实现，以及超越大规模网络数据集的稳健美学特征。在身份保持、上下文变化实现和美学质量方面表现优异。

Conclusion: Lunara Aesthetic II为图像生成和图像到图像系统的上下文泛化、身份保持和编辑鲁棒性提供了基准测试、微调和分析的工具，具有可解释的关系监督。

Abstract: We introduce Lunara Aesthetic II, a publicly released, ethically sourced image dataset designed to support controlled evaluation and learning of contextual consistency in modern image generation and editing systems. The dataset comprises 2,854 anchor-linked variation pairs derived from original art and photographs created by Moonworks. Each variation pair applies contextual transformations, such as illumination, weather, viewpoint, scene composition, color tone, or mood; while preserving a stable underlying identity. Lunara Aesthetic II operationalizes identity-preserving contextual variation as a supervision signal while also retaining Lunara's signature high aesthetic scores. Results show high identity stability, strong target attribute realization, and a robust aesthetic profile that exceeds large-scale web datasets. Released under the Apache 2.0 license, Lunara Aesthetic II is intended for benchmarking, fine-tuning, and analysis of contextual generalization, identity preservation, and edit robustness in image generation and image-to-image systems with interpretable, relational supervision. The dataset is publicly available at: https://huggingface.co/datasets/moonworks/lunara-aesthetic-image-variations.

</details>


### [382] [FastPhysGS: Accelerating Physics-based Dynamic 3DGS Simulation via Interior Completion and Adaptive Optimization](https://arxiv.org/abs/2602.01723)
*Yikun Ma,Yiqing Li,Jingwen Ye,Zhongkai Wu,Weidong Zhang,Lin Gao,Zhi Jin*

Main category: cs.CV

Relevance: 35.0

TL;DR: FastPhysGS：一个快速鲁棒的物理动态3D高斯泼溅框架，通过实例感知粒子填充和双向图解耦优化，在1分钟内实现高保真物理模拟，仅需7GB内存。


<details>
  <summary>Details</summary>
Motivation: 将3D高斯泼溅扩展到4D物理模拟面临挑战：现有方法依赖手动参数调整或从视频扩散模型蒸馏动态，限制了泛化能力和优化效率；基于LLMs/VLMs的方法存在文本/图像到3D的感知差距，导致不稳定物理行为；且常忽略3DGS的表面结构，产生不合理的运动。

Method: 提出FastPhysGS框架：1) 实例感知粒子填充(IPF)结合蒙特卡洛重要性采样(MCIS)，高效填充内部粒子同时保持几何保真度；2) 双向图解耦优化(BGDO)，一种自适应策略，快速优化从VLM预测的材料参数。

Result: FastPhysGS在1分钟内实现高保真物理模拟，仅需7GB运行时内存，优于先前工作，具有广泛的应用潜力。

Conclusion: FastPhysGS提供了一个快速、鲁棒的解决方案，用于基于物理的动态3D高斯泼溅模拟，解决了现有方法的局限性，实现了高效的物理模拟。

Abstract: Extending 3D Gaussian Splatting (3DGS) to 4D physical simulation remains challenging. Based on the Material Point Method (MPM), existing methods either rely on manual parameter tuning or distill dynamics from video diffusion models, limiting the generalization and optimization efficiency. Recent attempts using LLMs/VLMs suffer from a text/image-to-3D perceptual gap, yielding unstable physics behavior. In addition, they often ignore the surface structure of 3DGS, leading to implausible motion. We propose FastPhysGS, a fast and robust framework for physics-based dynamic 3DGS simulation:(1) Instance-aware Particle Filling (IPF) with Monte Carlo Importance Sampling (MCIS) to efficiently populate interior particles while preserving geometric fidelity; (2) Bidirectional Graph Decoupling Optimization (BGDO), an adaptive strategy that rapidly optimizes material parameters predicted from a VLM. Experiments show FastPhysGS achieves high-fidelity physical simulation in 1 minute using only 7 GB runtime memory, outperforming prior works with broad potential applications.

</details>


### [383] [Spatio-Temporal Transformers for Long-Term NDVI Forecasting](https://arxiv.org/abs/2602.01799)
*Ido Faran,Nathan S. Netanyahu,Maxim Shoshany*

Main category: cs.CV

Relevance: 35.0

TL;DR: STT-LTF是一个用于长期卫星图像时间序列分析的时空Transformer框架，能够处理多尺度空间补丁和长达20年的时间序列，通过自监督学习从40年无标签Landsat图像中训练，直接预测任意未来时间点。


<details>
  <summary>Details</summary>
Motivation: 解决异质景观中长期卫星图像时间序列分析的挑战，特别是在地中海地区，复杂的空间模式、季节变化和多十年环境变化在不同尺度上相互作用。传统方法难以同时建模空间上下文和时间序列预测。

Method: 提出STT-LTF框架，使用统一的Transformer架构处理多尺度空间补丁和时间序列，结合空间掩码、时间掩码和视界采样策略进行自监督学习，包含空间补丁嵌入、循环时间编码和地理坐标。

Result: 在1984-2024年Landsat数据上，STT-LTF在明年预测中实现了0.0328的MAE和0.8412的R²，优于传统统计方法、CNN、LSTM和标准Transformer。

Conclusion: STT-LTF能够有效处理不规则时间采样和可变预测视界，特别适合分析经历快速生态转变的异质景观，为长期环境监测提供了强大的时空建模框架。

Abstract: Long-term satellite image time series (SITS) analysis in heterogeneous landscapes faces significant challenges, particularly in Mediterranean regions where complex spatial patterns, seasonal variations, and multi-decade environmental changes interact across different scales. This paper presents the Spatio-Temporal Transformer for Long Term Forecasting (STT-LTF ), an extended framework that advances beyond purely temporal analysis to integrate spatial context modeling with temporal sequence prediction. STT-LTF processes multi-scale spatial patches alongside temporal sequences (up to 20 years) through a unified transformer architecture, capturing both local neighborhood relationships and regional climate influences. The framework employs comprehensive self-supervised learning with spatial masking, temporal masking, and horizon sampling strategies, enabling robust model training from 40 years of unlabeled Landsat imagery. Unlike autoregressive approaches, STT-LTF directly predicts arbitrary future time points without error accumulation, incorporating spatial patch embeddings, cyclical temporal encoding, and geographic coordinates to learn complex dependencies across heterogeneous Mediterranean ecosystems. Experimental evaluation on Landsat data (1984-2024) demonstrates that STT-LTF achieves a Mean Absolute Error (MAE) of 0.0328 and R^2 of 0.8412 for next-year predictions, outperforming traditional statistical methods, CNN-based approaches, LSTM networks, and standard transformers. The framework's ability to handle irregular temporal sampling and variable prediction horizons makes it particularly suitable for analysis of heterogeneous landscapes experiencing rapid ecological transitions.

</details>


### [384] [FlowBypass: Rectified Flow Trajectory Bypass for Training-Free Image Editing](https://arxiv.org/abs/2602.01805)
*Menglin Han,Zhangkai Ni*

Main category: cs.CV

Relevance: 35.0

TL;DR: FlowBypass：基于Rectified Flow的无训练图像编辑框架，通过构建连接反转和重建轨迹的旁路来避免误差累积，无需特征操作即可实现更好的提示对齐和细节保真度


<details>
  <summary>Details</summary>
Motivation: 现有无训练图像编辑方法主要依赖反转-重建轨迹，存在固有权衡：长轨迹会累积误差损害保真度，短轨迹则无法确保与编辑提示充分对齐。先前解决方案通常使用特定于骨干网络的特征操作，限制了通用性。

Method: 提出FlowBypass框架，基于Rectified Flow理论构建连接反转和重建轨迹的旁路。通过形式化推导两个轨迹，获得近似旁路公式及其数值解，实现无缝轨迹转换，避免误差累积且不依赖特征操作。

Result: 大量实验表明FlowBypass在图像编辑任务中持续优于最先进方法，在保持无关区域高保真细节的同时，实现了更强的提示对齐效果。

Conclusion: FlowBypass提供了一种新颖的分析框架，通过构建轨迹旁路解决了无训练图像编辑中的误差累积与对齐权衡问题，具有更好的通用性和性能表现。

Abstract: Training-free image editing has attracted increasing attention for its efficiency and independence from training data. However, existing approaches predominantly rely on inversion-reconstruction trajectories, which impose an inherent trade-off: longer trajectories accumulate errors and compromise fidelity, while shorter ones fail to ensure sufficient alignment with the edit prompt. Previous attempts to address this issue typically employ backbone-specific feature manipulations, limiting general applicability. To address these challenges, we propose FlowBypass, a novel and analytical framework grounded in Rectified Flow that constructs a bypass directly connecting inversion and reconstruction trajectories, thereby mitigating error accumulation without relying on feature manipulations. We provide a formal derivation of two trajectories, from which we obtain an approximate bypass formulation and its numerical solution, enabling seamless trajectory transitions. Extensive experiments demonstrate that FlowBypass consistently outperforms state-of-the-art image editing methods, achieving stronger prompt alignment while preserving high-fidelity details in irrelevant regions.

</details>


### [385] [GPD: Guided Progressive Distillation for Fast and High-Quality Video Generation](https://arxiv.org/abs/2602.01814)
*Xiao Liang,Yunzhu Zhang,Linchao Zhu*

Main category: cs.CV

Relevance: 35.0

TL;DR: 提出GPD框架，通过渐进式蒸馏加速视频扩散模型的推理过程，将采样步数从48步减少到6步，同时保持视觉质量


<details>
  <summary>Details</summary>
Motivation: 扩散模型在视频生成方面取得了显著成功，但去噪过程的高计算成本仍然是主要瓶颈。现有方法在减少扩散步数方面有潜力，但在视频生成中往往导致显著的视觉质量下降。

Method: 提出Guided Progressive Distillation (GPD)框架：1）教师模型渐进式指导学生模型使用更大的步长；2）在线生成训练目标，降低优化难度并提高计算效率；3）潜在空间中的频域约束，促进细粒度细节和时间动态的保留。

Result: 应用于Wan2.1模型，GPD将采样步数从48减少到6，同时在VBench上保持竞争力的视觉质量。与现有蒸馏方法相比，GPD在流程简单性和质量保持方面具有明显优势。

Conclusion: GPD框架为快速高质量视频生成提供了一种有效的扩散过程加速方法，在保持视觉质量的同时显著减少了计算成本。

Abstract: Diffusion models have achieved remarkable success in video generation; however, the high computational cost of the denoising process remains a major bottleneck. Existing approaches have shown promise in reducing the number of diffusion steps, but they often suffer from significant quality degradation when applied to video generation. We propose Guided Progressive Distillation (GPD), a framework that accelerates the diffusion process for fast and high-quality video generation. GPD introduces a novel training strategy in which a teacher model progressively guides a student model to operate with larger step sizes. The framework consists of two key components: (1) an online-generated training target that reduces optimization difficulty while improving computational efficiency, and (2) frequency-domain constraints in the latent space that promote the preservation of fine-grained details and temporal dynamics. Applied to the Wan2.1 model, GPD reduces the number of sampling steps from 48 to 6 while maintaining competitive visual quality on VBench. Compared with existing distillation methods, GPD demonstrates clear advantages in both pipeline simplicity and quality preservation.

</details>


### [386] [SPIRIT: Adapting Vision Foundation Models for Unified Single- and Multi-Frame Infrared Small Target Detection](https://arxiv.org/abs/2602.01843)
*Qian Xu,Xi Li,Fei Gao,Jie Guo,Haojuan Yuan,Shuaipeng Fan,Mingjin Zhang*

Main category: cs.CV

Relevance: 35.0

TL;DR: SPIRIT是一个统一且兼容视觉基础模型的红外小目标检测框架，通过轻量级物理信息插件适应红外数据特性，在空间和时间维度分别优化特征提取和跨帧关联。


<details>
  <summary>Details</summary>
Motivation: 红外小目标检测面临数据稀缺、目标信号弱、语义线索有限等挑战，现有视觉基础模型直接应用于红外数据存在模态差异问题，导致特征聚合淹没目标峰值，外观驱动的跨帧关联不可靠。

Method: 提出SPIRIT框架：1) 空间上使用PIFR插件通过近似秩稀疏分解抑制结构化背景并增强稀疏目标信号；2) 时间上使用PGMA插件将历史软空间先验注入记忆交叉注意力，约束跨帧关联，实现鲁棒视频检测。

Result: 在多个红外小目标检测基准测试中，相比基于视觉基础模型的基线方法取得一致提升，达到最先进的性能水平。

Conclusion: 通过轻量级物理信息插件适配视觉基础模型到红外小目标检测是有效的，解决了模态差异问题，实现了单帧和多帧检测的统一框架。

Abstract: Infrared small target detection (IRSTD) is crucial for surveillance and early-warning, with deployments spanning both single-frame analysis and video-mode tracking. A practical solution should leverage vision foundation models (VFMs) to mitigate infrared data scarcity, while adopting a memory-attention-based temporal propagation framework that unifies single- and multi-frame inference. However, infrared small targets exhibit weak radiometric signals and limited semantic cues, which differ markedly from visible-spectrum imagery. This modality gap makes direct use of semantics-oriented VFMs and appearance-driven cross-frame association unreliable for IRSTD: hierarchical feature aggregation can submerge localized target peaks, and appearance-only memory attention becomes ambiguous, leading to spurious clutter associations. To address these challenges, we propose SPIRIT, a unified and VFM-compatible framework that adapts VFMs to IRSTD via lightweight physics-informed plug-ins. Spatially, PIFR refines features by approximating rank-sparsity decomposition to suppress structured background components and enhance sparse target-like signals. Temporally, PGMA injects history-derived soft spatial priors into memory cross-attention to constrain cross-frame association, enabling robust video detection while naturally reverting to single-frame inference when temporal context is absent. Experiments on multiple IRSTD benchmarks show consistent gains over VFM-based baselines and SOTA performance.

</details>


### [387] [WS-IMUBench: Can Weakly Supervised Methods from Audio, Image, and Video Be Adapted for IMU-based Temporal Action Localization?](https://arxiv.org/abs/2602.01850)
*Pei Li,Jiaxi Yin,Lei Ouyang,Shihan Pan,Ge Wang,Han Ding,Fei Wang*

Main category: cs.CV

Relevance: 35.0

TL;DR: 该论文提出了WS-IMUBench，一个弱监督IMU时序动作定位的基准研究，评估了7种弱监督方法在7个公共IMU数据集上的表现，发现时序域方法比图像衍生方法更稳定，弱监督在某些数据集上可与全监督竞争，并指出了短动作、时序模糊和提议质量等主要失败模式。


<details>
  <summary>Details</summary>
Motivation: IMU基于的人体活动识别(HAR)应用广泛，但传统的片段分类范式无法捕捉真实世界行为的丰富时序结构。虽然IMU时序动作定位(IMU-TAL)能预测连续流中的动作类别和起止时间，但当前进展受限于需要密集的帧级边界标注，成本高且难以扩展。为解决这一瓶颈，作者研究了仅使用序列级标签的弱监督IMU-TAL。

Method: 作者提出了WS-IMUBench基准研究，而非新的定位算法。他们评估了来自音频、图像和视频领域的7种代表性弱监督定位方法在IMU-TAL上的迁移效果。在7个公共IMU数据集上进行了超过3,540次模型训练和7,080次推理评估，通过三个研究问题（可迁移性、有效性和洞察）来指导分析。

Result: 研究发现：(1) 迁移效果依赖于模态，时序域方法通常比图像衍生的提议方法更稳定；(2) 在有利数据集上（如动作较长、传感维度较高），弱监督可以与全监督竞争；(3) 主要失败模式源于短动作、时序模糊和提议质量差。研究还指出了WS-IMU-TAL的未来发展方向。

Conclusion: WS-IMUBench为弱监督IMU时序动作定位建立了可复现的基准模板、数据集、协议和分析框架，旨在加速社区向可扩展的WS-IMU-TAL发展。虽然该工作与用户的大语言模型研究兴趣直接关联度不高，但其中涉及的弱监督学习、时序建模和基准评估方法有一定相关性。

Abstract: IMU-based Human Activity Recognition (HAR) has enabled a wide range of ubiquitous computing applications, yet its dominant clip classification paradigm cannot capture the rich temporal structure of real-world behaviors. This motivates a shift toward IMU Temporal Action Localization (IMU-TAL), which predicts both action categories and their start/end times in continuous streams. However, current progress is strongly bottlenecked by the need for dense, frame-level boundary annotations, which are costly and difficult to scale. To address this bottleneck, we introduce WS-IMUBench, a systematic benchmark study of weakly supervised IMU-TAL (WS-IMU-TAL) under only sequence-level labels. Rather than proposing a new localization algorithm, we evaluate how well established weakly supervised localization paradigms from audio, image, and video transfer to IMU-TAL under only sequence-level labels. We benchmark seven representative weakly supervised methods on seven public IMU datasets, resulting in over 3,540 model training runs and 7,080 inference evaluations. Guided by three research questions on transferability, effectiveness, and insights, our findings show that (i) transfer is modality-dependent, with temporal-domain methods generally more stable than image-derived proposal-based approaches; (ii) weak supervision can be competitive on favorable datasets (e.g., with longer actions and higher-dimensional sensing); and (iii) dominant failure modes arise from short actions, temporal ambiguity, and proposal quality. Finally, we outline concrete directions for advancing WS-IMU-TAL (e.g., IMU-specific proposal generation, boundary-aware objectives, and stronger temporal reasoning). Beyond individual results, WS-IMUBench establishes a reproducible benchmarking template, datasets, protocols, and analyses, to accelerate community-wide progress toward scalable WS-IMU-TAL.

</details>


### [388] [ProxyImg: Towards Highly-Controllable Image Representation via Hierarchical Disentangled Proxy Embedding](https://arxiv.org/abs/2602.01881)
*Ye Chen,Yupeng Zhu,Xiongzhen Zhang,Zhewen Wan,Yingzhe Li,Wenjun Zhang,Bingbing Ni*

Main category: cs.CV

Relevance: 35.0

TL;DR: 提出了一种基于分层代理的参数化图像表示方法，将语义、几何和纹理属性解耦到独立可操作的参数空间，支持高效可控的图像编辑和实时物理驱动动画。


<details>
  <summary>Details</summary>
Motivation: 现有图像表示方法（如光栅图像、高斯基元等显式表示，或潜在图像等隐式表示）存在表示冗余导致手动编辑负担重，或缺乏从潜在变量到语义实例/部件的直接映射，使得细粒度操作困难，阻碍了高效可控的图像视频编辑。

Method: 基于输入图像的语义感知分解，通过自适应贝塞尔拟合和迭代内部区域细分网格化构建分层代理几何。将多尺度隐式纹理参数嵌入到几何感知的分布式代理节点中，支持连续高保真重建和实例/部件独立的语义编辑。引入局部自适应特征索引机制确保空间纹理一致性。

Result: 在ImageNet、OIR-Bench和HumanEdit等图像重建和编辑基准测试中，该方法以显著更少的参数实现了最先进的渲染保真度，同时支持直观、交互式和物理合理的操作。通过将代理节点与基于位置的动力学集成，实现了实时物理驱动动画，相比生成方法具有更好的时间一致性和视觉真实感。

Conclusion: 提出的分层代理参数化图像表示方法有效解决了现有表示方法的局限性，实现了高效、可控的图像编辑和动画生成，为图像表示和编辑提供了新的解决方案。

Abstract: Prevailing image representation methods, including explicit representations such as raster images and Gaussian primitives, as well as implicit representations such as latent images, either suffer from representation redundancy that leads to heavy manual editing effort, or lack a direct mapping from latent variables to semantic instances or parts, making fine-grained manipulation difficult. These limitations hinder efficient and controllable image and video editing. To address these issues, we propose a hierarchical proxy-based parametric image representation that disentangles semantic, geometric, and textural attributes into independent and manipulable parameter spaces. Based on a semantic-aware decomposition of the input image, our representation constructs hierarchical proxy geometries through adaptive Bezier fitting and iterative internal region subdivision and meshing. Multi-scale implicit texture parameters are embedded into the resulting geometry-aware distributed proxy nodes, enabling continuous high-fidelity reconstruction in the pixel domain and instance- or part-independent semantic editing. In addition, we introduce a locality-adaptive feature indexing mechanism to ensure spatial texture coherence, which further supports high-quality background completion without relying on generative models. Extensive experiments on image reconstruction and editing benchmarks, including ImageNet, OIR-Bench, and HumanEdit, demonstrate that our method achieves state-of-the-art rendering fidelity with significantly fewer parameters, while enabling intuitive, interactive, and physically plausible manipulation. Moreover, by integrating proxy nodes with Position-Based Dynamics, our framework supports real-time physics-driven animation using lightweight implicit rendering, achieving superior temporal consistency and visual realism compared with generative approaches.

</details>


### [389] [DSXFormer: Dual-Pooling Spectral Squeeze-Expansion and Dynamic Context Attention Transformer for Hyperspectral Image Classification](https://arxiv.org/abs/2602.01906)
*Farhan Ullah,Irfan Ullah,Khalil Khan,Giovanni Pau,JaKeoung Koo*

Main category: cs.CV

Relevance: 35.0

TL;DR: 提出DSXFormer用于高光谱图像分类，通过双池化谱挤压扩展块和动态上下文注意力机制，在保持计算效率的同时增强谱区分能力，在多个基准数据集上达到SOTA性能。


<details>
  <summary>Details</summary>
Motivation: 高光谱图像分类面临高谱维度、复杂谱空间相关性和有限标注样本的挑战。现有Transformer方法在谱区分能力和计算效率之间难以平衡。

Method: 提出DSXFormer，包含双池化谱挤压扩展块（使用全局平均池化和最大池化自适应重新校准谱特征通道）和动态上下文注意力机制（在窗口Transformer架构中动态捕获局部谱空间关系）。结合补丁提取、嵌入和合并策略实现多尺度特征学习。

Result: 在Salinas、Indian Pines、Pavia University和Kennedy Space Center四个基准数据集上分别达到99.95%、98.91%、99.85%和98.52%的分类准确率，优于现有方法。

Conclusion: DSXFormer通过谱双池化挤压扩展和动态上下文注意力的联合集成，在谱强调和空间上下文表示之间实现了有效平衡，为高光谱图像分类提供了高效解决方案。

Abstract: Hyperspectral image classification (HSIC) is a challenging task due to high spectral dimensionality, complex spectral-spatial correlations, and limited labeled training samples. Although transformer-based models have shown strong potential for HSIC, existing approaches often struggle to achieve sufficient spectral discriminability while maintaining computational efficiency. To address these limitations, we propose a novel DSXFormer, a novel dual-pooling spectral squeeze-expansion transformer with Dynamic Context Attention for HSIC. The proposed DSXFormer introduces a Dual-Pooling Spectral Squeeze-Expansion (DSX) block, which exploits complementary global average and max pooling to adaptively recalibrate spectral feature channels, thereby enhancing spectral discriminability and inter-band dependency modeling. In addition, DSXFormer incorporates a Dynamic Context Attention (DCA) mechanism within a window-based transformer architecture to dynamically capture local spectral-spatial relationships while significantly reducing computational overhead. The joint integration of spectral dual-pooling squeeze-expansion and DCA enables DSXFormer to achieve an effective balance between spectral emphasis and spatial contextual representation. Furthermore, patch extraction, embedding, and patch merging strategies are employed to facilitate efficient multi-scale feature learning. Extensive experiments conducted on four widely used hyperspectral benchmark datasets, including Salinas (SA), Indian Pines (IP), Pavia University (PU), and Kennedy Space Center (KSC), demonstrate that DSXFormer consistently outperforms state-of-the-art methods, achieving classification accuracies of 99.95%, 98.91%, 99.85%, and 98.52%, respectively.

</details>


### [390] [Enabling Progressive Whole-slide Image Analysis with Multi-scale Pyramidal Network](https://arxiv.org/abs/2602.01951)
*Shuyang Wu,Yifu Qiu,Ines P. Nearchou,Sandrine Prost,Jonathan A Fallowfield,Hakan Bilen,Timothy J Kendall*

Main category: cs.CV

Relevance: 35.0

TL;DR: MSPN是一个用于计算病理学的多尺度金字塔网络，作为注意力机制多实例学习的即插即用模块，通过渐进式多尺度分析提升WSI处理性能


<details>
  <summary>Details</summary>
Motivation: 传统多尺度特征方法依赖制造商定义的固定放大倍数，存在特征跨尺度链接丢失、计算成本高、灵活性差的问题，需要更有效的多尺度特征学习方法

Method: 提出MSPN（多尺度金字塔网络），包含基于网格的重映射（用高倍特征推导粗粒度特征）和粗粒度引导网络（学习粗粒度上下文），作为注意力机制MIL的即插即用模块

Result: 在4个临床相关任务、3种基础模型和预训练MIL框架上，MSPN作为4个注意力框架的附加模块，均能一致提升MIL性能，同时保持轻量级和易用性

Conclusion: MSPN通过渐进式多尺度分析有效解决了传统多尺度方法的局限性，为计算病理学提供了一种灵活、高效的多尺度特征学习方案

Abstract: Multiple-instance Learning (MIL) is commonly used to undertake computational pathology (CPath) tasks, and the use of multi-scale patches allows diverse features across scales to be learned. Previous studies using multi-scale features in clinical applications rely on multiple inputs across magnifications with late feature fusion, which does not retain the link between features across scales while the inputs are dependent on arbitrary, manufacturer-defined magnifications, being inflexible and computationally expensive. In this paper, we propose the Multi-scale Pyramidal Network (MSPN), which is plug-and-play over attention-based MIL that introduces progressive multi-scale analysis on WSI. Our MSPN consists of (1) grid-based remapping that uses high magnification features to derive coarse features and (2) the coarse guidance network (CGN) that learns coarse contexts. We benchmark MSPN as an add-on module to 4 attention-based frameworks using 4 clinically relevant tasks across 3 types of foundation model, as well as the pre-trained MIL framework. We show that MSPN consistently improves MIL across the compared configurations and tasks, while being lightweight and easy-to-use.

</details>


### [391] [Beyond Open Vocabulary: Multimodal Prompting for Object Detection in Remote Sensing Images](https://arxiv.org/abs/2602.01954)
*Shuai Yang,Ziyue Huang,Jiaxin Chen,Qingjie Liu,Yunhong Wang*

Main category: cs.CV

Relevance: 35.0

TL;DR: RS-MPOD：一个用于遥感图像的多模态开放词汇检测框架，通过结合视觉提示和文本提示来解决传统文本提示在遥感场景中语义模糊的问题


<details>
  <summary>Details</summary>
Motivation: 传统遥感开放词汇检测仅依赖文本提示，假设预训练模型具有可靠的文本-视觉对齐能力。但在实际遥感场景中，由于任务和应用特定的类别语义，这种假设经常失效，导致类别指定不稳定

Method: 提出RS-MPOD框架，引入视觉提示编码器从示例实例中提取外观特征，支持纯视觉类别指定；同时包含多模态融合模块，在两种模态都可用时整合视觉和文本信息

Result: 在标准、跨数据集和细粒度遥感基准测试中，视觉提示在语义模糊和分布偏移下提供更可靠的类别指定，而多模态提示在文本语义对齐良好时保持竞争力

Conclusion: 多模态提示方法能够解决遥感场景中文本提示的局限性，提供更灵活可靠的开放词汇检测方案

Abstract: Open-vocabulary object detection in remote sensing commonly relies on text-only prompting to specify target categories, implicitly assuming that inference-time category queries can be reliably grounded through pretraining-induced text-visual alignment. In practice, this assumption often breaks down in remote sensing scenarios due to task- and application-specific category semantics, resulting in unstable category specification under open-vocabulary settings. To address this limitation, we propose RS-MPOD, a multimodal open-vocabulary detection framework that reformulates category specification beyond text-only prompting by incorporating instance-grounded visual prompts, textual prompts, and their multimodal integration. RS-MPOD introduces a visual prompt encoder to extract appearance-based category cues from exemplar instances, enabling text-free category specification, and a multimodal fusion module to integrate visual and textual information when both modalities are available. Extensive experiments on standard, cross-dataset, and fine-grained remote sensing benchmarks show that visual prompting yields more reliable category specification under semantic ambiguity and distribution shifts, while multimodal prompting provides a flexible alternative that remains competitive when textual semantics are well aligned.

</details>


### [392] [Leveraging Latent Vector Prediction for Localized Control in Image Generation via Diffusion Models](https://arxiv.org/abs/2602.01991)
*Pablo Domingo-Gregorio,Javier Ruiz-Hidalgo*

Main category: cs.CV

Relevance: 35.0

TL;DR: 提出了一种新的扩散模型训练框架，通过掩码特征和额外损失项实现图像局部区域的精确控制，同时让模型自主生成其他区域


<details>
  <summary>Details</summary>
Motivation: 现有文本到图像生成方法虽然能通过文本提示和图像级控制（如边缘、分割、深度图）生成高质量图像，但这些控制是全局均匀应用的，缺乏对用户定义区域的局部精确控制能力

Method: 提出新的训练框架，引入掩码特征和额外损失项，利用任意扩散步骤中初始潜在向量的预测来增强当前步骤与最终样本在潜在空间中的对应关系

Result: 大量实验表明，该方法能有效合成具有受控局部条件的高质量图像

Conclusion: 该方法实现了对图像局部区域的精确控制，同时保持扩散模型根据原始提示自主生成其他区域的能力

Abstract: Diffusion models emerged as a leading approach in text-to-image generation, producing high-quality images from textual descriptions. However, attempting to achieve detailed control to get a desired image solely through text remains a laborious trial-and-error endeavor. Recent methods have introduced image-level controls alongside with text prompts, using prior images to extract conditional information such as edges, segmentation and depth maps. While effective, these methods apply conditions uniformly across the entire image, limiting localized control. In this paper, we propose a novel methodology to enable precise local control over user-defined regions of an image, while leaving to the diffusion model the task of autonomously generating the remaining areas according to the original prompt. Our approach introduces a new training framework that incorporates masking features and an additional loss term, which leverages the prediction of the initial latent vector at any diffusion step to enhance the correspondence between the current step and the final sample in the latent space. Extensive experiments demonstrate that our method effectively synthesizes high-quality images with controlled local conditions.

</details>


### [393] [UrbanGS: A Scalable and Efficient Architecture for Geometrically Accurate Large-Scene Reconstruction](https://arxiv.org/abs/2602.02089)
*Changbai Li,Haodong Zhu,Hanlin Chen,Xiuping Liang,Tongfei Chen,Shuwei Shao,Linlin Yang,Huobin Tan,Baochang Zhang*

Main category: cs.CV

Relevance: 35.0

TL;DR: UrbanGS：针对大规模城市场景的3D高斯泼溅扩展框架，通过深度一致D-法向正则化和空间自适应高斯剪枝，解决了几何一致性、内存效率和计算可扩展性挑战。


<details>
  <summary>Details</summary>
Motivation: 3D高斯泼溅（3DGS）在有限场景中能实现高质量实时渲染，但扩展到大规模城市场景时面临几何一致性、内存效率和计算可扩展性三大挑战。现有方法依赖单目法向估计器，能有效更新旋转参数但难以更新位置参数，且缺乏对大规模场景的高效处理机制。

Method: 1. 深度一致D-法向正则化模块：结合D-法向约束与外部深度监督，全面更新所有几何参数；2. 基于梯度一致性和逆深度偏差的自适应置信度加权机制，增强多视角深度对齐和几何一致性；3. 空间自适应高斯剪枝策略：根据局部几何复杂度和可见性动态调整高斯密度；4. 统一分区和视角分配方案：消除边界伪影并优化计算负载。

Result: 在多个城市数据集上的实验表明，UrbanGS在渲染质量、几何精度和内存效率方面均取得优越性能，为高保真大规模场景重建提供了系统解决方案。

Conclusion: UrbanGS通过创新的深度一致正则化和自适应剪枝策略，成功解决了3DGS在大规模城市场景中的关键挑战，实现了高质量、高效的大规模场景重建。

Abstract: While 3D Gaussian Splatting (3DGS) enables high-quality, real-time rendering for bounded scenes, its extension to large-scale urban environments gives rise to critical challenges in terms of geometric consistency, memory efficiency, and computational scalability. To address these issues, we present UrbanGS, a scalable reconstruction framework that effectively tackles these challenges for city-scale applications. First, we propose a Depth-Consistent D-Normal Regularization module. Unlike existing approaches that rely solely on monocular normal estimators, which can effectively update rotation parameters yet struggle to update position parameters, our method integrates D-Normal constraints with external depth supervision. This allows for comprehensive updates of all geometric parameters. By further incorporating an adaptive confidence weighting mechanism based on gradient consistency and inverse depth deviation, our approach significantly enhances multi-view depth alignment and geometric coherence, which effectively resolves the issue of geometric accuracy in complex large-scale scenes. To improve scalability, we introduce a Spatially Adaptive Gaussian Pruning (SAGP) strategy, which dynamically adjusts Gaussian density based on local geometric complexity and visibility to reduce redundancy. Additionally, a unified partitioning and view assignment scheme is designed to eliminate boundary artifacts and optimize computational load. Extensive experiments on multiple urban datasets demonstrate that UrbanGS achieves superior performance in rendering quality, geometric accuracy, and memory efficiency, providing a systematic solution for high-fidelity large-scale scene reconstruction.

</details>


### [394] [Enhancing Diffusion-Based Quantitatively Controllable Image Generation via Matrix-Form EDM and Adaptive Vicinal Training](https://arxiv.org/abs/2602.02114)
*Xin Ding,Yun Chen,Sen Zhang,Kao Zhang,Nenglun Chen,Peibei Cao,Yongwei Wang,Fei Wu*

Main category: cs.CV

Relevance: 35.0

TL;DR: iCCDM是一个改进的连续条件扩散模型框架，采用先进的EDM架构和自适应邻域训练策略，在图像生成质量和采样效率上超越现有方法


<details>
  <summary>Details</summary>
Motivation: 现有的CCDM虽然比早期方法有优势，但存在两个主要问题：1) 基于过时的扩散框架；2) 采样效率低（采样轨迹长）。最近还被GAN方法CcGAN-AVAR超越，因此需要改进。

Method: 1) 采用更先进的Elucidated Diffusion Model (EDM)框架并进行大量修改；2) 提出新颖的矩阵形式EDM公式；3) 引入自适应邻域训练策略

Result: 在四个基准数据集上（图像分辨率从64×64到256×256）的实验表明，iCCDM一致优于现有方法，包括最先进的大规模文本到图像扩散模型（如Stable Diffusion 3, FLUX.1, Qwen-Image），在显著降低采样成本的同时获得更高的生成质量

Conclusion: iCCDM通过结合先进的EDM框架和创新的训练策略，成功解决了CCDM的局限性，在连续条件图像生成任务上实现了质量和效率的双重提升

Abstract: Continuous Conditional Diffusion Model (CCDM) is a diffusion-based framework designed to generate high-quality images conditioned on continuous regression labels. Although CCDM has demonstrated clear advantages over prior approaches across a range of datasets, it still exhibits notable limitations and has recently been surpassed by a GAN-based method, namely CcGAN-AVAR. These limitations mainly arise from its reliance on an outdated diffusion framework and its low sampling efficiency due to long sampling trajectories. To address these issues, we propose an improved CCDM framework, termed iCCDM, which incorporates the more advanced \textit{Elucidated Diffusion Model} (EDM) framework with substantial modifications to improve both generation quality and sampling efficiency. Specifically, iCCDM introduces a novel matrix-form EDM formulation together with an adaptive vicinal training strategy. Extensive experiments on four benchmark datasets, spanning image resolutions from $64\times64$ to $256\times256$, demonstrate that iCCDM consistently outperforms existing methods, including state-of-the-art large-scale text-to-image diffusion models (e.g., Stable Diffusion 3, FLUX.1, and Qwen-Image), achieving higher generation quality while significantly reducing sampling cost.

</details>


### [395] [Toxicity Assessment in Preclinical Histopathology via Class-Aware Mahalanobis Distance for Known and Novel Anomalies](https://arxiv.org/abs/2602.02124)
*Olga Graf,Dhrupal Patel,Peter Groß,Charlotte Lempp,Matthias Hein,Fabian Heinemann*

Main category: cs.CV

Relevance: 35.0

TL;DR: 提出基于AI的组织病理学异常检测框架，用于药物毒性评估中的肝组织病理图像分析，能够检测已知病理和未见过的罕见病理


<details>
  <summary>Details</summary>
Motivation: 药物诱导毒性是临床前开发和早期临床试验失败的主要原因，组织病理学评估依赖专家病理学家，成为大规模筛查的瓶颈，需要AI解决方案来加速安全药物开发

Method: 1) 构建像素级标注的健康组织和已知病理数据集；2) 使用DINOv2预训练Vision Transformer，通过LoRA微调进行组织分割；3) 提取特征并使用马氏距离进行OOD检测；4) 提出类别特异性阈值优化方法

Result: 仅0.16%的病理组织被误分类为健康，0.35%的健康组织被误分类为病理，在已知毒理学发现的鼠肝WSIs中准确检测异常，包括罕见的OOD形态

Conclusion: AI驱动的组织病理学在支持临床前工作流程、减少后期失败和提高药物开发效率方面具有潜力

Abstract: Drug-induced toxicity remains a leading cause of failure in preclinical development and early clinical trials. Detecting adverse effects at an early stage is critical to reduce attrition and accelerate the development of safe medicines. Histopathological evaluation remains the gold standard for toxicity assessment, but it relies heavily on expert pathologists, creating a bottleneck for large-scale screening. To address this challenge, we introduce an AI-based anomaly detection framework for histopathological whole-slide images (WSIs) in rodent livers from toxicology studies. The system identifies healthy tissue and known pathologies (anomalies) for which training data is available. In addition, it can detect rare pathologies without training data as out-of-distribution (OOD) findings. We generate a novel dataset of pixelwise annotations of healthy tissue and known pathologies and use this data to fine-tune a pre-trained Vision Transformer (DINOv2) via Low-Rank Adaptation (LoRA) in order to do tissue segmentation. Finally, we extract features for OOD detection using the Mahalanobis distance. To better account for class-dependent variability in histological data, we propose the use of class-specific thresholds. We optimize the thresholds using the mean of the false negative and false positive rates, resulting in only 0.16\% of pathological tissue classified as healthy and 0.35\% of healthy tissue classified as pathological. Applied to mouse liver WSIs with known toxicological findings, the framework accurately detects anomalies, including rare OOD morphologies. This work demonstrates the potential of AI-driven histopathology to support preclinical workflows, reduce late-stage failures, and improve efficiency in drug development.

</details>


### [396] [Lung Nodule Image Synthesis Driven by Two-Stage Generative Adversarial Networks](https://arxiv.org/abs/2602.02171)
*Lu Cao,Xiquan He,Junying Zeng,Chaoyun Mai,Min Luo*

Main category: cs.CV

Relevance: 35.0

TL;DR: 提出TSGAN两阶段生成对抗网络，通过解耦肺结节形态结构和纹理特征来增强合成数据的多样性和空间可控性，提升检测模型性能。


<details>
  <summary>Details</summary>
Motivation: 现有肺结节CT数据集样本量有限且多样性不足，限制了检测模型的性能和泛化能力。现有生成方法存在多样性不足、可控性差、纹理特征单调、解剖结构扭曲等问题。

Method: 两阶段生成对抗网络：第一阶段使用StyleGAN生成语义分割掩码图像，编码肺结节和组织背景以控制解剖结构；第二阶段使用DL-Pix2Pix模型将掩码转换为CT图像，采用局部重要性注意力捕获局部特征，动态权重多头窗口注意力增强纹理建模能力。

Result: 在LUNA16数据集上，相比原始数据集，准确率提升4.6%，mAP提升4%。实验结果表明TSGAN能增强合成图像质量和检测模型性能。

Conclusion: TSGAN通过解耦形态结构和纹理特征，有效提升了合成肺结节CT图像的多样性和空间可控性，从而改善了检测模型的性能。

Abstract: The limited sample size and insufficient diversity of lung nodule CT datasets severely restrict the performance and generalization ability of detection models. Existing methods generate images with insufficient diversity and controllability, suffering from issues such as monotonous texture features and distorted anatomical structures. Therefore, we propose a two-stage generative adversarial network (TSGAN) to enhance the diversity and spatial controllability of synthetic data by decoupling the morphological structure and texture features of lung nodules. In the first stage, StyleGAN is used to generate semantic segmentation mask images, encoding lung nodules and tissue backgrounds to control the anatomical structure of lung nodule images; The second stage uses the DL-Pix2Pix model to translate the mask map into CT images, employing local importance attention to capture local features, while utilizing dynamic weight multi-head window attention to enhance the modeling capability of lung nodule texture and background. Compared to the original dataset, the accuracy improved by 4.6% and mAP by 4% on the LUNA16 dataset. Experimental results demonstrate that TSGAN can enhance the quality of synthetic images and the performance of detection models.

</details>


### [397] [CIEC: Coupling Implicit and Explicit Cues for Multimodal Weakly Supervised Manipulation Localization](https://arxiv.org/abs/2602.02175)
*Xinquan Yu,Wei Lu,Xiangyang Luo*

Main category: cs.CV

Relevance: 35.0

TL;DR: 提出CIEC框架，仅使用粗粒度图像/句子级标注实现多模态弱监督篡改定位，包含图像和文本两个分支，分别通过TRPS和VCTG模块进行定位，性能接近全监督方法。


<details>
  <summary>Details</summary>
Motivation: 为了应对虚假信息威胁，多模态篡改定位受到关注。现有方法依赖昂贵且耗时的细粒度标注（如补丁/标记级标注），需要开发仅使用粗粒度标注的弱监督方法。

Method: 提出CIEC框架，包含两个分支：1）图像弱监督定位分支：使用TRPS模块整合视觉和文本伪造线索，结合空间先验锁定可疑区域，通过背景抑制和空间对比约束减少干扰；2）文本弱监督定位分支：使用VCTG模块关注内容词，利用相对视觉偏差辅助标记定位，通过非对称稀疏和语义一致性约束减轻标签噪声。

Result: 大量实验证明CIEC的有效性，在多个评估指标上取得了与全监督方法相当的结果。

Conclusion: CIEC框架仅使用粗粒度标注就能实现有效的多模态篡改定位，为虚假信息检测提供了更实用的解决方案。

Abstract: To mitigate the threat of misinformation, multimodal manipulation localization has garnered growing attention. Consider that current methods rely on costly and time-consuming fine-grained annotations, such as patch/token-level annotations. This paper proposes a novel framework named Coupling Implicit and Explicit Cues (CIEC), which aims to achieve multimodal weakly-supervised manipulation localization for image-text pairs utilizing only coarse-grained image/sentence-level annotations. It comprises two branches, image-based and text-based weakly-supervised localization. For the former, we devise the Textual-guidance Refine Patch Selection (TRPS) module. It integrates forgery cues from both visual and textual perspectives to lock onto suspicious regions aided by spatial priors. Followed by the background silencing and spatial contrast constraints to suppress interference from irrelevant areas. For the latter, we devise the Visual-deviation Calibrated Token Grounding (VCTG) module. It focuses on meaningful content words and leverages relative visual bias to assist token localization. Followed by the asymmetric sparse and semantic consistency constraints to mitigate label noise and ensure reliability. Extensive experiments demonstrate the effectiveness of our CIEC, yielding results comparable to fully supervised methods on several evaluation metrics.

</details>


### [398] [Evaluating OCR Performance for Assistive Technology: Effects of Walking Speed, Camera Placement, and Camera Type](https://arxiv.org/abs/2602.02223)
*Junchi Feng,Nikhil Ballem,Mahya Beheshti,Giles Hamilton-Fletcher,Todd Hudson,Maurizio Porfiri,William H. Seiple,John-Ross Rizzo*

Main category: cs.CV

Relevance: 35.0

TL;DR: 该研究系统评估了OCR在静态和动态条件下的性能，包括距离、视角、行走速度和设备位置等因素，发现Google Vision准确率最高，PaddleOCR是最佳开源替代方案


<details>
  <summary>Details</summary>
Motivation: 当前OCR评估多基于静态数据集，无法反映移动使用中的真实挑战，特别是对于视障人士辅助技术应用。需要系统评估OCR在动态条件下的性能表现。

Method: 采用静态测试（距离1-7米，水平视角0-75度）和动态测试（行走速度0.8-1.8 m/s，三种相机位置：头戴式、肩戴式、手持式）。评估智能手机和智能眼镜，使用四种OCR引擎（Google Vision、PaddleOCR 3.0、EasyOCR、Tesseract），以字符级Levenshtein比率计算准确率。

Result: 识别准确率随行走速度增加和视角变宽而下降。Google Vision整体准确率最高，PaddleOCR是最佳开源替代方案。手机主摄像头准确率最高，肩戴式位置平均表现最佳，但不同身体位置间差异不显著。

Conclusion: 移动OCR性能受动态条件显著影响，需要在实际使用场景中进行评估。Google Vision在移动OCR应用中表现最优，PaddleOCR是可行的开源替代方案。

Abstract: Optical character recognition (OCR), which converts printed or handwritten text into machine-readable form, is widely used in assistive technology for people with blindness and low vision. Yet, most evaluations rely on static datasets that do not reflect the challenges of mobile use. In this study, we systematically evaluated OCR performance under both static and dynamic conditions. Static tests measured detection range across distances of 1-7 meters and viewing angles of 0-75 degrees horizontally. Dynamic tests examined the impact of motion by varying walking speed from slow (0.8 m/s) to very fast (1.8 m/s) and comparing three camera mounting positions: head-mounted, shoulder-mounted, and hand-held. We evaluated both a smartphone and smart glasses, using the phone's main and ultra-wide cameras. Four OCR engines were benchmarked to assess accuracy at different distances and viewing angles: Google Vision, PaddleOCR 3.0, EasyOCR, and Tesseract. PaddleOCR 3.0 was then used to evaluate accuracy at different walking speeds. Accuracy was computed at the character level using the Levenshtein ratio against manually defined ground truth. Results showed that recognition accuracy declined with increased walking speed and wider viewing angles. Google Vision achieved the highest overall accuracy, with PaddleOCR close behind as the strongest open-source alternative. Across devices, the phone's main camera achieved the highest accuracy, and a shoulder-mounted placement yielded the highest average among body positions; however, differences among shoulder, head, and hand were not statistically significant.

</details>


### [399] [Enhancing Indoor Occupancy Prediction via Sparse Query-Based Multi-Level Consistent Knowledge Distillation](https://arxiv.org/abs/2602.02318)
*Xiang Li,Yupeng Zheng,Pengfei Li,Yilun Chen,Ya-Qin Zhang,Wenchao Ding*

Main category: cs.CV

Relevance: 35.0

TL;DR: DiScene：一种基于稀疏查询的占用预测框架，通过多级知识蒸馏实现高效鲁棒的几何理解，在室内场景中超越现有方法。


<details>
  <summary>Details</summary>
Motivation: 当前占用预测方法面临效率-准确性权衡：密集方法在空体素上浪费计算资源，而稀疏查询方法在复杂室内场景中缺乏鲁棒性。需要一种既高效又鲁棒的解决方案。

Method: 提出DiScene框架，包含两个关键创新：1）多级一致知识蒸馏策略，通过编码器级特征对齐、查询级特征匹配、先验级空间引导和锚点级高置信度知识转移四个层次将大教师模型的知识传递给轻量学生模型；2）教师引导初始化策略，使用优化参数预热加速模型收敛。

Result: 在Occ-Scannet基准测试中，DiScene达到23.2 FPS，比基线方法OPUS提升36.1%，甚至优于深度增强版本OPUS†。集成深度信息后，DiScene†超越EmbodiedOcc 3.7%，推理速度提升1.62倍。在Occ3D-nuScenes基准和真实场景中也表现出良好泛化能力。

Conclusion: DiScene通过多级蒸馏实现了高效鲁棒的占用预测，在多个基准测试中达到最先进性能，展示了在机器人几何理解任务中的实用价值。

Abstract: Occupancy prediction provides critical geometric and semantic understanding for robotics but faces efficiency-accuracy trade-offs. Current dense methods suffer computational waste on empty voxels, while sparse query-based approaches lack robustness in diverse and complex indoor scenes. In this paper, we propose DiScene, a novel sparse query-based framework that leverages multi-level distillation to achieve efficient and robust occupancy prediction. In particular, our method incorporates two key innovations: (1) a Multi-level Consistent Knowledge Distillation strategy, which transfers hierarchical representations from large teacher models to lightweight students through coordinated alignment across four levels, including encoder-level feature alignment, query-level feature matching, prior-level spatial guidance, and anchor-level high-confidence knowledge transfer and (2) a Teacher-Guided Initialization policy, employing optimized parameter warm-up to accelerate model convergence. Validated on the Occ-Scannet benchmark, DiScene achieves 23.2 FPS without depth priors while outperforming our baseline method, OPUS, by 36.1% and even better than the depth-enhanced version, OPUS†. With depth integration, DiScene† attains new SOTA performance, surpassing EmbodiedOcc by 3.7% with 1.62$\times$ faster inference speed. Furthermore, experiments on the Occ3D-nuScenes benchmark and in-the-wild scenarios demonstrate the versatility of our approach in various environments. Code and models can be accessed at https://github.com/getterupper/DiScene.

</details>


### [400] [VQ-Style: Disentangling Style and Content in Motion with Residual Quantized Representations](https://arxiv.org/abs/2602.02334)
*Fatemeh Zargarbashi,Dhruv Agrawal,Jakob Buhmann,Martin Guay,Stelian Coros,Robert W. Sumner*

Main category: cs.CV

Relevance: 35.0

TL;DR: 提出一种基于残差向量量化变分自编码器（RVQ-VAE）的方法，通过对比学习和信息泄漏损失实现人体运动数据中风格与内容的解耦，并利用量化代码交换技术实现无需微调的运动风格迁移。


<details>
  <summary>Details</summary>
Motivation: 人体运动数据包含丰富的语义内容和细微的风格特征，但现有方法难以有效解耦风格与内容，限制了运动风格迁移等应用的效果。

Method: 使用RVQ-VAE学习从粗到细的运动表示，结合对比学习和新颖的信息泄漏损失来组织不同码本中的内容与风格，通过量化代码交换技术实现推理时的风格迁移。

Result: 框架在风格迁移、风格移除和运动混合等多种推理应用中表现出强大的多功能性，能够处理未见过的风格而无需微调。

Conclusion: 该方法有效解决了人体运动数据中风格与内容的解耦问题，为运动风格迁移等应用提供了简单而有效的解决方案。

Abstract: Human motion data is inherently rich and complex, containing both semantic content and subtle stylistic features that are challenging to model. We propose a novel method for effective disentanglement of the style and content in human motion data to facilitate style transfer. Our approach is guided by the insight that content corresponds to coarse motion attributes while style captures the finer, expressive details. To model this hierarchy, we employ Residual Vector Quantized Variational Autoencoders (RVQ-VAEs) to learn a coarse-to-fine representation of motion. We further enhance the disentanglement by integrating contrastive learning and a novel information leakage loss with codebook learning to organize the content and the style across different codebooks. We harness this disentangled representation using our simple and effective inference-time technique Quantized Code Swapping, which enables motion style transfer without requiring any fine-tuning for unseen styles. Our framework demonstrates strong versatility across multiple inference applications, including style transfer, style removal, and motion blending.

</details>


### [401] [Uncertainty-Aware Image Classification In Biomedical Imaging Using Spectral-normalized Neural Gaussian Processes](https://arxiv.org/abs/2602.02370)
*Uma Meleti,Jeffrey J. Nirschl*

Main category: cs.CV

Relevance: 35.0

TL;DR: SNGP（谱归一化神经高斯过程）通过谱归一化和高斯过程层改进数字病理学中的不确定性估计和分布外检测，提高模型可信度。


<details>
  <summary>Details</summary>
Motivation: 当前数字病理学的深度学习模型在分布外（OOD）场景下往往过于自信且校准不佳，限制了临床信任和采用。医疗影像工作流需要具备内在的不确定性感知能力来准确拒绝OOD输入。

Method: 实现SNGP（谱归一化神经高斯过程），通过谱归一化和将最终密集层替换为高斯过程层来改进单模型不确定性估计和OOD检测。在三个生物医学分类任务（白细胞、淀粉样斑块、结直肠组织病理学）的六个数据集上评估SNGP与确定性模型和蒙特卡洛dropout的比较。

Result: SNGP在分布内性能相当的同时，显著改善了不确定性估计和OOD检测能力。

Conclusion: SNGP及相关模型为数字病理学中的不确定性感知分类提供了有用框架，支持安全部署并建立与病理学家的信任。

Abstract: Accurate histopathologic interpretation is key for clinical decision-making; however, current deep learning models for digital pathology are often overconfident and poorly calibrated in out-of-distribution (OOD) settings, which limit trust and clinical adoption. Safety-critical medical imaging workflows benefit from intrinsic uncertainty-aware properties that can accurately reject OOD input. We implement the Spectral-normalized Neural Gaussian Process (SNGP), a set of lightweight modifications that apply spectral normalization and replace the final dense layer with a Gaussian process layer to improve single-model uncertainty estimation and OOD detection. We evaluate SNGP vs. deterministic and MonteCarlo dropout on six datasets across three biomedical classification tasks: white blood cells, amyloid plaques, and colorectal histopathology. SNGP has comparable in-distribution performance while significantly improving uncertainty estimation and OOD detection. Thus, SNGP or related models offer a useful framework for uncertainty-aware classification in digital pathology, supporting safe deployment and building trust with pathologists.

</details>


### [402] [Multi-head automated segmentation by incorporating detection head into the contextual layer neural network](https://arxiv.org/abs/2602.02471)
*Edwin Kys,Febian Febian*

Main category: cs.CV

Relevance: 35.0

TL;DR: 提出基于Swin U-Net的门控多头Transformer架构，通过切片级结构检测和像素级分割的联合学习，在放疗自动分割中抑制解剖学上不合理的假阳性（幻觉）预测。


<details>
  <summary>Details</summary>
Motivation: 深度学习在放疗自动分割中应用广泛，但传统模型在缺乏目标结构的切片上经常产生解剖学上不合理的假阳性（幻觉）预测，影响临床可靠性。

Method: 基于Swin U-Net的门控多头Transformer架构，包含切片间上下文集成和平行检测头。检测头通过多层感知器进行切片级结构检测，分割流进行像素级分割。检测输出门控分割预测，抑制解剖无效切片中的假阳性。训练使用切片级Tversky损失处理类别不平衡。

Result: 在Prostate-Anatomical-Edge-Cases数据集上，门控模型显著优于非门控基线：平均Dice损失为0.013±0.036 vs 0.732±0.314。检测概率与解剖存在性强相关，有效消除虚假分割。非门控模型在所有切片上表现出更高的变异性和持续性假阳性。

Conclusion: 检测门控增强了自动分割应用的鲁棒性和解剖合理性，在不影响有效切片分割质量的情况下减少幻觉预测，为提高临床放疗自动轮廓勾画工作流的可靠性提供了有前景的方法。

Abstract: Deep learning based auto segmentation is increasingly used in radiotherapy, but conventional models often produce anatomically implausible false positives, or hallucinations, in slices lacking target structures. We propose a gated multi-head Transformer architecture based on Swin U-Net, augmented with inter-slice context integration and a parallel detection head, which jointly performs slice-level structure detection via a multi-layer perceptron and pixel-level segmentation through a context-enhanced stream. Detection outputs gate the segmentation predictions to suppress false positives in anatomically invalid slices, and training uses slice-wise Tversky loss to address class imbalance. Experiments on the Prostate-Anatomical-Edge-Cases dataset from The Cancer Imaging Archive demonstrate that the gated model substantially outperforms a non-gated segmentation-only baseline, achieving a mean Dice loss of $0.013 \pm 0.036$ versus $0.732 \pm 0.314$, with detection probabilities strongly correlated with anatomical presence, effectively eliminating spurious segmentations. In contrast, the non-gated model exhibited higher variability and persistent false positives across all slices. These results indicate that detection-based gating enhances robustness and anatomical plausibility in automated segmentation applications, reducing hallucinated predictions without compromising segmentation quality in valid slices, and offers a promising approach for improving the reliability of clinical radiotherapy auto-contouring workflows.

</details>


### [403] [PixelGen: Pixel Diffusion Beats Latent Diffusion with Perceptual Loss](https://arxiv.org/abs/2602.02493)
*Zehong Ma,Ruihan Xu,Shiliang Zhang*

Main category: cs.CV

Relevance: 35.0

TL;DR: PixelGen是一种简单的像素扩散框架，通过感知监督直接在像素空间生成图像，无需VAE或潜在表示，在ImageNet-256上达到FID 5.11，超越潜在扩散模型。


<details>
  <summary>Details</summary>
Motivation: 现有像素扩散方法难以优化高维像素流形中的感知无关信号，导致性能落后于潜在扩散模型。作者希望开发一个更简单但更强大的生成范式，直接在像素空间进行端到端生成，避免VAE引入的伪影和瓶颈。

Method: PixelGen引入两种互补的感知损失来引导扩散模型学习更有意义的感知流形：1) LPIPS损失促进学习更好的局部模式；2) 基于DINO的感知损失增强全局语义。该方法无需VAE、潜在表示或辅助阶段。

Result: 在ImageNet-256上不使用分类器无关引导，仅用80个训练周期就达到FID 5.11；在大规模文本到图像生成上获得GenEval分数0.79，展示了良好的扩展性能，超越了强大的潜在扩散基线。

Conclusion: PixelGen通过感知监督实现了更简单但更强大的生成范式，直接在像素空间进行端到端生成，无需复杂的潜在表示或VAE，为图像生成提供了新的有效方法。

Abstract: Pixel diffusion generates images directly in pixel space in an end-to-end manner, avoiding the artifacts and bottlenecks introduced by VAEs in two-stage latent diffusion. However, it is challenging to optimize high-dimensional pixel manifolds that contain many perceptually irrelevant signals, leaving existing pixel diffusion methods lagging behind latent diffusion models. We propose PixelGen, a simple pixel diffusion framework with perceptual supervision. Instead of modeling the full image manifold, PixelGen introduces two complementary perceptual losses to guide diffusion model towards learning a more meaningful perceptual manifold. An LPIPS loss facilitates learning better local patterns, while a DINO-based perceptual loss strengthens global semantics. With perceptual supervision, PixelGen surpasses strong latent diffusion baselines. It achieves an FID of 5.11 on ImageNet-256 without classifier-free guidance using only 80 training epochs, and demonstrates favorable scaling performance on large-scale text-to-image generation with a GenEval score of 0.79. PixelGen requires no VAEs, no latent representations, and no auxiliary stages, providing a simpler yet more powerful generative paradigm. Codes are publicly available at https://github.com/Zehong-Ma/PixelGen.

</details>


### [404] [Comparison of Image Processing Models in Quark Gluon Jet Classification](https://arxiv.org/abs/2602.00141)
*Daeun Kim,Jiwon Lee,Wonjun Jeong,Hyeongwoo Noh,Giyeong Kim,Jaeyoon Cho,Geonhee Kwak,Seunghwan Yang,MinJung Kweon*

Main category: physics.data-an

Relevance: 35.0

TL;DR: 该研究比较了卷积神经网络和Transformer模型在区分夸克和胶子喷注方面的性能，发现仅微调Swin-Tiny模型最后两个Transformer块在效率和准确性之间达到最佳平衡。


<details>
  <summary>Details</summary>
Motivation: 研究动机是探索不同深度学习架构（特别是卷积神经网络和Transformer模型）在粒子物理中喷注子结构分析中的应用效果，寻找最适合区分夸克和胶子喷注的模型架构。

Method: 方法包括：1）将喷注子结构编码为三通道粒子运动学表示；2）评估CNN、ViT和Swin Transformer模型；3）在监督学习和自监督学习（MoCo）设置下进行实验；4）采用仅微调最后两个Transformer块的策略。

Result: 结果显示：1）仅微调Swin-Tiny最后两个Transformer块达到最佳性能（81.4%准确率和88.9% AUC）；2）MoCo自监督预训练增强了特征鲁棒性并减少了可训练参数；3）分层注意力模型在喷注子结构研究中表现优异。

Conclusion: 结论是分层注意力模型（如Swin Transformer）在喷注子结构分析中具有潜力，自监督预训练能提升特征质量，这种架构有望迁移到真实碰撞数据中。

Abstract: We present a comprehensive comparison of convolutional and transformer-based models for distinguishing quark and gluon jets using simulated jet images from Pythia 8. By encoding jet substructure into a three-channel representation of particle kinematics, we evaluate the performance of convolutional neural networks (CNNs), Vision Transformers (ViTs), and Swin Transformers (Swin-Tiny) under both supervised and self-supervised learning setups. Our results show that fine-tuning only the final two transformer blocks of the Swin-Tiny model achieves the best trade-off between efficiency and accuracy, reaching 81.4% accuracy and an AUC (area under the ROC curve) of 88.9%. Self-supervised pretraining with Momentum Contrast (MoCo) further enhances feature robustness and reduces the number of trainable parameters. These findings highlight the potential of hierarchical attention-based models for jet substructure studies and for domain transfer to real collision data.

</details>


### [405] [Recent Advances of End-to-End Video Coding Technologies for AVS Standard Development](https://arxiv.org/abs/2602.00483)
*Xihua Sheng,Xiongzhuang Liang,Chuanbo Tang,Zhirui Zuo,Yifan Bian,Yutao Xie,Zhuoyuan Li,Yuqi Li,Hui Xiang,Li Li,Dong Liu*

Main category: eess.IV

Relevance: 35.0

TL;DR: AVS-EEM项目开发了一个面向实际部署的端到端智能视频编码模型，在严格复杂度约束下实现了优于传统AVS3参考软件的压缩效率


<details>
  <summary>Details</summary>
Motivation: 追求更高的视频压缩效率，推动智能视频编码标准化，同时注重实际部署可行性，要求低计算复杂度和符合传统视频编码测试条件

Method: 建立了AVS端到端智能视频编码探索模型框架，包括模型架构设计、训练策略和推理优化，经过两年多迭代优化

Result: 最新模型在严格复杂度约束下实现了比传统AVS3参考软件更优的压缩效率，标志着向可部署智能视频编码标准迈出重要一步

Conclusion: AVS-EEM项目通过系统性的技术框架和持续优化，在智能视频编码标准化方面取得了显著进展，展示了实际部署的可行性

Abstract: Video coding standards are essential to enable the interoperability and widespread adoption of efficient video compression technologies. In pursuit of greater video compression efficiency, the AVS video coding working group launched the standardization exploration of end-to-end intelligent video coding, establishing the AVS End-to-End Intelligent Video Coding Exploration Model (AVS-EEM) project. A core design principle of AVS-EEM is its focus on practical deployment, featuring inherently low computational complexity and requiring strict adherence to the common test conditions of conventional video coding. This paper details the development history of AVS-EEM and provides a systematic introduction to its key technical framework, covering model architectures, training strategies, and inference optimizations. These innovations have collectively driven the project's rapid performance evolution, enabling continuous and significant gains under strict complexity constraints. Through over two years of iterative refinement and collaborative effort, the coding performance of AVS-EEM has seen substantial improvement. Experimental results demonstrate that its latest model achieves superior compression efficiency compared to the conventional AVS3 reference software, marking a significant step toward a deployable intelligent video coding standard.

</details>


### [406] [APEX: A Decoupled Memory-based Explorer for Asynchronous Aerial Object Goal Navigation](https://arxiv.org/abs/2602.00551)
*Daoxuan Zhang,Ping Chen,Xiaobo Xia,Xiu Su,Ruichen Zhen,Jianqiang Xiao,Shuo Yang*

Main category: cs.RO

Relevance: 35.0

TL;DR: APEX是一个用于空中物体目标导航的层次化智能体，通过动态时空语义映射记忆、动作决策模块和目标接地模块，在复杂空中环境中实现高效探索和目标获取。


<details>
  <summary>Details</summary>
Motivation: 现有方法在空中环境中存在三个主要问题：1）复杂空间表示的记忆困难；2）可靠且可解释的动作决策不足；3）探索和信息收集效率低下。这些限制了无人机在自主导航和目标识别方面的性能。

Method: APEX采用三层模块化架构：1）动态时空语义映射记忆，利用视觉语言模型的零样本能力构建高分辨率3D吸引力、探索和障碍物地图；2）基于强化学习的动作决策模块；3）使用开放词汇检测器的目标接地模块。这些组件集成在层次化、异步并行框架中，避免了VLM推理延迟。

Result: 在UAV-ON基准测试中，APEX比之前的最先进方法提升了+4.2%的成功率(SR)和+2.8%的路径长度加权成功率(SPL)，证明了其高效性和层次异步设计的有效性。

Conclusion: APEX通过创新的层次异步架构解决了空中物体目标导航的关键挑战，显著提升了无人机在复杂环境中的自主导航和目标识别能力，为具身AI领域提供了新的解决方案。

Abstract: Aerial Object Goal Navigation, a challenging frontier in Embodied AI, requires an Unmanned Aerial Vehicle (UAV) agent to autonomously explore, reason, and identify a specific target using only visual perception and language description. However, existing methods struggle with the memorization of complex spatial representations in aerial environments, reliable and interpretable action decision-making, and inefficient exploration and information gathering. To address these challenges, we introduce \textbf{APEX} (Aerial Parallel Explorer), a novel hierarchical agent designed for efficient exploration and target acquisition in complex aerial settings. APEX is built upon a modular, three-part architecture: 1) Dynamic Spatio-Semantic Mapping Memory, which leverages the zero-shot capability of a Vision-Language Model (VLM) to dynamically construct high-resolution 3D Attraction, Exploration, and Obstacle maps, serving as an interpretable memory mechanism. 2) Action Decision Module, trained with reinforcement learning, which translates this rich spatial understanding into a fine-grained and robust control policy. 3) Target Grounding Module, which employs an open-vocabulary detector to achieve definitive and generalizable target identification. All these components are integrated into a hierarchical, asynchronous, and parallel framework, effectively bypassing the VLM's inference latency and boosting the agent's proactivity in exploration. Extensive experiments show that APEX outperforms the previous state of the art by +4.2\% SR and +2.8\% SPL on challenging UAV-ON benchmarks, demonstrating its superior efficiency and the effectiveness of its hierarchical asynchronous design. Our source code is provided in \href{https://github.com/4amGodvzx/apex}{GitHub}

</details>


### [407] [Hyperspectral Image Fusion with Spectral-Band and Fusion-Scale Agnosticism](https://arxiv.org/abs/2602.01681)
*Yu-Jie Liang,Zihan Cao,Liang-Jian Deng,Yang Yang,Malu Zhang*

Main category: eess.IV

Relevance: 35.0

TL;DR: 提出SSA框架，实现多光谱/高光谱图像融合的谱带和尺度无关性，通过Matryoshka核适应任意光谱通道数，基于隐式神经表示实现任意空间分辨率重建。


<details>
  <summary>Details</summary>
Motivation: 当前多光谱/高光谱图像融合模型通常针对固定谱带和空间尺度设计，限制了在不同传感器间的可迁移性。需要开发通用框架以适应多样化的传感器配置。

Method: 提出SSA通用框架：1) 引入Matryoshka核(MK)算子，使单个模型能适应任意数量的光谱通道；2) 基于隐式神经表示(INR)建模高光谱信号为连续函数，实现任意空间分辨率重建。

Result: 大量实验表明，单个SSA模型在保持最先进性能的同时，能很好地泛化到未见过的传感器和尺度，为未来高光谱基础模型铺平道路。

Conclusion: SSA框架通过谱带无关性和融合尺度无关性，实现了多光谱/高光谱图像融合模型的通用化，为构建高光谱基础模型提供了可行路径。

Abstract: Current deep learning models for Multispectral and Hyperspectral Image Fusion (MS/HS fusion) are typically designed for fixed spectral bands and spatial scales, which limits their transferability across diverse sensors. To address this, we propose SSA, a universal framework for MS/HS fusion with spectral-band and fusion-scale agnosticism. Specifically, we introduce Matryoshka Kernel (MK), a novel operator that enables a single model to adapt to arbitrary numbers of spectral channels. Meanwhile, we build SSA upon an Implicit Neural Representation (INR) backbone that models the HS signal as a continuous function, enabling reconstruction at arbitrary spatial resolutions. Together, these two forms of agnosticism enable a single MS/HS fusion model that generalizes effectively to unseen sensors and spatial scales. Extensive experiments demonstrate that our single model achieves state-of-the-art performance while generalizing well to unseen sensors and scales, paving the way toward future HS foundation models.

</details>


### [408] [Multi-Task Learning for Robot Perception with Imbalanced Data](https://arxiv.org/abs/2602.01899)
*Ozgur Erkent*

Main category: cs.RO

Relevance: 35.0

TL;DR: 提出一种在多任务学习中处理标签不平衡问题的方法，即使某些任务缺乏真实标签也能学习，并通过任务交互分析找出哪些任务能提升其他任务的性能。


<details>
  <summary>Details</summary>
Motivation: 机器人资源有限，多任务学习能提高各任务精度，但实际中不同任务的标签数量往往不平衡，且移动机器人在各种环境中难以获取所有任务的标签数据。

Method: 提出一种在部分任务缺乏真实标签情况下仍能学习的方法，通过训练教师网络使用任务输出（如深度）作为输入，分析任务间的交互关系，确定哪些任务能提升其他任务的性能。

Result: 在NYUDv2和Cityscapes数据集上对语义分割和深度估计任务进行实验，提供了详细的方法分析和实证证据，特别是在小数据量训练下的表现。

Conclusion: 该方法能有效处理多任务学习中的标签不平衡问题，通过任务交互分析揭示了任务间的相互促进关系，为资源受限的机器人系统提供了实用的多任务学习方案。

Abstract: Multi-task problem solving has been shown to improve the accuracy of the individual tasks, which is an important feature for robots, as they have a limited resource. However, when the number of labels for each task is not equal, namely imbalanced data exist, a problem may arise due to insufficient number of samples, and labeling is not very easy for mobile robots in every environment. We propose a method that can learn tasks even in the absence of the ground truth labels for some of the tasks. We also provide a detailed analysis of the proposed method. An interesting finding is related to the interaction of the tasks. We show a methodology to find out which tasks can improve the performance of other tasks. We investigate this by training the teacher network with the task outputs such as depth as inputs. We further provide empirical evidence when trained with a small amount of data. We use semantic segmentation and depth estimation tasks on different datasets, NYUDv2 and Cityscapes.

</details>


### [409] [LIEREx: Language-Image Embeddings for Robotic Exploration](https://arxiv.org/abs/2602.01930)
*Felix Igelbrink,Lennart Niecksch,Marian Renz,Martin Günther,Martin Atzmueller*

Main category: cs.RO

Relevance: 35.0

TL;DR: LIEREx 将视觉语言基础模型（如CLIP）与3D语义场景图结合，实现自主机器人在部分未知环境中的目标导向探索，解决传统映射方法受限于预定义符号词汇的问题。


<details>
  <summary>Details</summary>
Motivation: 传统语义地图方法依赖预设计的符号词汇和固定对象类别，无法处理设计时未定义的新知识（分布外知识）。机器人需要更灵活的表示方法来在部分未知环境中进行目标导向探索。

Method: 集成视觉语言基础模型（VLFMs，如CLIP）与3D语义场景图。CLIP提供开放集映射能力，将对象编码为高维嵌入而非固定标签，结合语义场景图的结构化表示，实现目标导向探索。

Result: LIEREx系统使自主机器人能够在部分未知环境中进行目标导向探索，利用开放集映射处理未预定义的对象类别，比传统固定词汇方法更具适应性。

Conclusion: 视觉语言基础模型与语义场景图的结合为机器人语义映射提供了新范式，解决了传统方法的词汇限制问题，使机器人能够更灵活地理解和探索复杂环境。

Abstract: Semantic maps allow a robot to reason about its surroundings to fulfill tasks such as navigating known environments, finding specific objects, and exploring unmapped areas. Traditional mapping approaches provide accurate geometric representations but are often constrained by pre-designed symbolic vocabularies. The reliance on fixed object classes makes it impractical to handle out-of-distribution knowledge not defined at design time. Recent advances in Vision-Language Foundation Models, such as CLIP, enable open-set mapping, where objects are encoded as high-dimensional embeddings rather than fixed labels. In LIEREx, we integrate these VLFMs with established 3D Semantic Scene Graphs to enable target-directed exploration by an autonomous agent in partially unknown environments.

</details>


### [410] [Real-Time 2D LiDAR Object Detection Using Three-Frame RGB Scan Encoding](https://arxiv.org/abs/2602.02167)
*Soheil Behnam Roudsari,Alexandre S. Brandão,Felipe N. Martins*

Main category: eess.SP

Relevance: 35.0

TL;DR: 提出一种基于2D LiDAR的物体检测方法，通过堆叠连续三帧扫描作为RGB通道输入YOLOv8n，在嵌入式设备上实现实时室内物体检测，无需RGB摄像头。


<details>
  <summary>Details</summary>
Motivation: 室内服务机器人需要鲁棒、隐私友好且能在嵌入式硬件上运行的感知系统。传统RGB视频存在隐私问题，而LiDAR感知通常需要构建占用网格，计算开销较大。

Method: 将连续三帧2D LiDAR扫描堆叠为RGB三通道，直接输入YOLOv8n网络，保留角度结构和运动线索，避免占用网格构建。在Webots模拟器中评估，使用160个随机室内场景。

Result: 在四类物体检测上达到98.4% mAP@0.5（0.778 mAP@0.5:0.95），精确率94.9%，召回率94.7%。在树莓派5上实时运行，平均端到端延迟47.8ms/帧，比基于占用网格的方法延迟更低。

Conclusion: 轻量级时间编码方法能够在嵌入式室内机器人上实现准确、实时的纯LiDAR检测，无需RGB摄像头，提供隐私友好的感知方案。

Abstract: Indoor service robots need perception that is robust, more privacy-friendly than RGB video, and feasible on embedded hardware. We present a camera-free 2D LiDAR object detection pipeline that encodes short-term temporal context by stacking three consecutive scans as RGB channels, yielding a compact YOLOv8n input without occupancy-grid construction while preserving angular structure and motion cues. Evaluated in Webots across 160 randomized indoor scenarios with strict scenario-level holdout, the method achieves 98.4% mAP@0.5 (0.778 mAP@0.5:0.95) with 94.9% precision and 94.7% recall on four object classes. On a Raspberry Pi 5, it runs in real time with a mean post-warm-up end-to-end latency of 47.8ms per frame, including scan encoding and postprocessing. Relative to a closely related occupancy-grid LiDAR-YOLO pipeline reported on the same platform, the proposed representation is associated with substantially lower reported end-to-end latency. Although results are simulation-based, they suggest that lightweight temporal encoding can enable accurate and real-time LiDAR-only detection for embedded indoor robotics without capturing RGB appearance.

</details>


### [411] [Computer Vision and Its Relationship to Cognitive Science: A perspective from Bayes Decision Theory](https://arxiv.org/abs/2602.00289)
*Alan Yuille,Daniel Kersten*

Main category: cs.CV

Relevance: 30.0

TL;DR: 本文从贝叶斯决策理论视角介绍计算机视觉及其与认知科学的关系，对比贝叶斯方法和深度神经网络方法，并探讨两者结合的潜力。


<details>
  <summary>Details</summary>
Motivation: 计算机视觉领域庞大复杂，需要理论框架来整合不同方法。贝叶斯决策理论提供了一个统一视角，既能涵盖与认知科学相关的贝叶斯方法，又能解释在现实世界中取得巨大成功的深度神经网络方法。

Method: 采用贝叶斯决策理论作为分析框架，对比分析两种主要方法：1）贝叶斯视角 - 提供概念上吸引人的视觉框架，与认知科学概念相呼应；2）深度神经网络方法 - 受视觉腹侧通路层次结构启发，在实践中取得巨大成功。

Result: 贝叶斯决策理论框架能够关联并捕捉这两种方法的优缺点，通过讨论BDT的局限性，指出了如何将两者结合到更丰富的框架中。

Conclusion: 贝叶斯决策理论为理解计算机视觉提供了有价值的理论视角，既能解释传统贝叶斯方法的认知科学相关性，又能理解深度神经网络的实际成功。未来的发展方向在于结合两种方法的优势。

Abstract: This document presents an introduction to computer vision, and its relationship to Cognitive Science, from the perspective of Bayes Decision Theory (Berger 1985). Computer vision is a vast and complex field, so this overview has a narrow scope and provides a theoretical lens which captures many key concepts. BDT is rich enough to include two different approaches: (i) the Bayesian viewpoint, which gives a conceptually attractive framework for vision with concepts that resonate with Cognitive Science (Griffiths et al., 2024), and (ii) the Deep Neural Network approach whose successes in the real world have made Computer Vision into a trillion-dollar industry and which is motivated by the hierarchical structure of the visual ventral stream. The BDT framework relates and captures the strengths and weakness of these two approaches and, by discussing the limitations of BDT, points the way to how they can be combined in a richer framework.

</details>


### [412] [Diff-PC: Identity-preserving and 3D-aware Controllable Diffusion for Zero-shot Portrait Customization](https://arxiv.org/abs/2602.00639)
*Yifang Xu,Benxiang Zhai,Chenyu Zhang,Ming Li,Yang Li,Sidan Du*

Main category: cs.CV

Relevance: 30.0

TL;DR: Diff-PC：基于扩散模型的零样本肖像定制框架，通过3D面部先验、ID编码器和控制模块实现高保真身份保持、精确面部控制和多样化背景生成。


<details>
  <summary>Details</summary>
Motivation: 现有肖像定制方法在身份保持和面部控制方面存在不足，缺乏精确的身份保真度和面部属性控制能力，需要一种能够生成高保真身份、指定面部属性和多样化背景的解决方案。

Method: 1) 使用3D面部预测器重建包含参考身份、目标表情和姿态的3D感知面部先验；2) 设计ID编码器融合局部和全局面部特征；3) 开发ID控制模块利用3D面部指导身份特征对齐；4) 引入ID注入器增强身份保真度和面部可控性；5) 在收集的身份中心数据集上进行训练。

Result: Diff-PC在身份保持、面部控制和文本-图像一致性方面超越现有最佳方法，能够生成具有高身份保真度、指定面部属性和多样化背景的现实肖像，且兼容多风格基础模型。

Conclusion: Diff-PC通过创新的3D面部先验、ID编码和控制机制，成功解决了肖像定制中的身份保持和面部控制问题，为高质量肖像生成提供了有效的零样本解决方案。

Abstract: Portrait customization (PC) has recently garnered significant attention due to its potential applications. However, existing PC methods lack precise identity (ID) preservation and face control. To address these tissues, we propose Diff-PC, a diffusion-based framework for zero-shot PC, which generates realistic portraits with high ID fidelity, specified facial attributes, and diverse backgrounds. Specifically, our approach employs the 3D face predictor to reconstruct the 3D-aware facial priors encompassing the reference ID, target expressions, and poses. To capture fine-grained face details, we design ID-Encoder that fuses local and global facial features. Subsequently, we devise ID-Ctrl using the 3D face to guide the alignment of ID features. We further introduce ID-Injector to enhance ID fidelity and facial controllability. Finally, training on our collected ID-centric dataset improves face similarity and text-to-image (T2I) alignment. Extensive experiments demonstrate that Diff-PC surpasses state-of-the-art methods in ID preservation, facial control, and T2I consistency. Furthermore, our method is compatible with multi-style foundation models.

</details>


### [413] [SelvaMask: Segmenting Trees in Tropical Forests and Beyond](https://arxiv.org/abs/2602.02426)
*Simon-Olivier Duguay,Hugo Baudchon,Etienne Laliberté,Helene Muller-Landau,Gonzalo Rivas-Torres,Arthur Ouaknine*

Main category: cs.CV

Relevance: 30.0

TL;DR: 提出SelvaMask热带森林树冠分割数据集和基于视觉基础模型的检测-分割管道，在密集热带森林中达到SOTA性能


<details>
  <summary>Details</summary>
Motivation: 热带森林对全球生态平衡至关重要，树冠分割对研究碳储存和生态系统功能很重要。现有基于transformer的树冠分割模型在热带森林中性能较低，缺乏高质量标注数据集。

Method: 1) 创建SelvaMask数据集：包含8,800多个手工标注的树冠，覆盖巴拿马、巴西和厄瓜多尔三个热带森林站点；2) 提出模块化检测-分割管道：使用视觉基础模型(VFMs)和领域特定的检测提示器

Result: 在密集热带森林中达到最先进性能，优于零样本通用模型和全监督端到端方法。在外部热带和温带数据集上验证了泛化能力

Conclusion: SelvaMask既是具有挑战性的基准测试，也是实现广义森林监测的关键推动因素。代码和数据集将公开发布

Abstract: Tropical forests harbor most of the planet's tree biodiversity and are critical to global ecological balance. Canopy trees in particular play a disproportionate role in carbon storage and functioning of these ecosystems. Studying canopy trees at scale requires accurate delineation of individual tree crowns, typically performed using high-resolution aerial imagery. Despite advances in transformer-based models for individual tree crown segmentation, performance remains low in most forests, especially tropical ones. To this end, we introduce SelvaMask, a new tropical dataset containing over 8,800 manually delineated tree crowns across three Neotropical forest sites in Panama, Brazil, and Ecuador. SelvaMask features comprehensive annotations, including an inter-annotator agreement evaluation, capturing the dense structure of tropical forests and highlighting the difficulty of the task. Leveraging this benchmark, we propose a modular detection-segmentation pipeline that adapts vision foundation models (VFMs), using domain-specific detection-prompter. Our approach reaches state-of-the-art performance, outperforming both zero-shot generalist models and fully supervised end-to-end methods in dense tropical forests. We validate these gains on external tropical and temperate datasets, demonstrating that SelvaMask serves as both a challenging benchmark and a key enabler for generalized forest monitoring. Our code and dataset will be released publicly.

</details>


### [414] [Efficient UAV trajectory prediction: A multi-modal deep diffusion framework](https://arxiv.org/abs/2602.00107)
*Yuan Gao,Xinyu Guo,Wenjing Xie,Zifan Wang,Hongwen Yu,Gongyang Li,Shugong Xu*

Main category: cs.CV

Relevance: 25.0

TL;DR: 提出了一种基于LiDAR和毫米波雷达信息融合的多模态无人机轨迹预测方法，设计了多模态深度融合框架，通过双向交叉注意力机制实现信息互补，在MMAUD数据集上相比基线模型提升了40%的预测精度。


<details>
  <summary>Details</summary>
Motivation: 为满足低空经济中管理未经授权无人机的需求，需要准确预测无人机轨迹。LiDAR和毫米波雷达在空间几何结构和动态反射特性上具有互补信息，通过多模态融合可以提升轨迹预测的准确性。

Method: 设计了一个多模态深度融合框架，包含两个模态特定的特征提取网络（LiDAR和雷达各自独立的特征编码器）和一个双向交叉注意力融合模块。特征提取后通过双向交叉注意力机制实现模态间的信息互补和语义对齐。

Result: 在CVPR 2024 UG2+无人机跟踪与姿态估计挑战赛的MMAUD数据集上进行实验，提出的多模态融合模型显著提升了轨迹预测精度，相比基线模型提高了40%。消融实验验证了不同损失函数和后处理策略对模型性能的有效性。

Conclusion: 该方法能有效利用多模态数据，为低空经济中未经授权无人机的轨迹预测提供了高效解决方案，通过LiDAR和雷达信息的深度融合显著提升了预测性能。

Abstract: To meet the requirements for managing unauthorized UAVs in the low-altitude economy, a multi-modal UAV trajectory prediction method based on the fusion of LiDAR and millimeter-wave radar information is proposed. A deep fusion network for multi-modal UAV trajectory prediction, termed the Multi-Modal Deep Fusion Framework, is designed. The overall architecture consists of two modality-specific feature extraction networks and a bidirectional cross-attention fusion module, aiming to fully exploit the complementary information of LiDAR and radar point clouds in spatial geometric structure and dynamic reflection characteristics. In the feature extraction stage, the model employs independent but structurally identical feature encoders for LiDAR and radar. After feature extraction, the model enters the Bidirectional Cross-Attention Mechanism stage to achieve information complementarity and semantic alignment between the two modalities. To verify the effectiveness of the proposed model, the MMAUD dataset used in the CVPR 2024 UG2+ UAV Tracking and Pose-Estimation Challenge is adopted as the training and testing dataset. Experimental results show that the proposed multi-modal fusion model significantly improves trajectory prediction accuracy, achieving a 40% improvement compared to the baseline model. In addition, ablation experiments are conducted to demonstrate the effectiveness of different loss functions and post-processing strategies in improving model performance. The proposed model can effectively utilize multi-modal data and provides an efficient solution for unauthorized UAV trajectory prediction in the low-altitude economy.

</details>


### [415] [VDE Bench: Evaluating The Capability of Image Editing Models to Modify Visual Documents](https://arxiv.org/abs/2602.00122)
*Hongzhu Yi,Yujia Yang,Yuanxiang Wang,Zhenyu Guan,Jiahuan Chen,Chenxi Bao,Tiankun Yang,Yixuan Yuan,Tianyu Zong,Xinming Wang,Tao Yu,Ruiwen Tao,Haijin Liang,Jin Ma,Jinwen Luo,Yeshani Xinyu Zuo,Jungang Xu*

Main category: cs.CV

Relevance: 25.0

TL;DR: 提出了VDE Bench基准，用于评估多语言和复杂视觉文档编辑模型，填补了现有方法在密集文本和非拉丁脚本（如中文）文档编辑方面的空白。


<details>
  <summary>Details</summary>
Motivation: 当前多模态图像编辑模型在视觉文档图像编辑方面研究不足，特别是对于密集、结构复杂的文档和非拉丁脚本（如中文）。现有方法如AnyText、GlyphControl和TextCtrl主要关注英文场景和稀疏文本布局，无法充分处理复杂文档。

Method: 提出了VDE Bench基准，包含高质量数据集（英文和中文密集文本文档）和解耦评估框架。数据集涵盖学术论文、海报、演示幻灯片、考试材料和报纸等。评估框架在OCR解析层面系统量化编辑性能，实现文本修改准确性的细粒度评估。

Result: 对代表性最先进的图像编辑模型进行了全面评估。人工验证显示人工判断与自动评估指标之间具有很强的一致性。VDE Bench是首个用于评估多语言和密集文本视觉文档编辑模型的系统性基准。

Conclusion: VDE Bench填补了视觉文档编辑评估的空白，为多语言和复杂文档编辑模型提供了首个系统性基准，支持细粒度评估并展示了人工与自动评估的一致性。

Abstract: In recent years, multimodal image editing models have achieved substantial progress, enabling users to manipulate visual content through natural language in a flexible and interactive manner. Nevertheless, an important yet insufficiently explored research direction remains visual document image editing, which involves modifying textual content within images while faithfully preserving the original text style and background context. Existing approaches, including AnyText, GlyphControl, and TextCtrl, predominantly focus on English-language scenarios and documents with relatively sparse textual layouts, thereby failing to adequately address dense, structurally complex documents or non-Latin scripts such as Chinese. To bridge this gap, we propose \textbf{V}isual \textbf{D}oc \textbf{E}dit Bench(VDE Bench), a rigorously human-annotated and evaluated benchmark specifically designed to assess image editing models on multilingual and complex visual document editing tasks. The benchmark comprises a high-quality dataset encompassing densely textual documents in both English and Chinese, including academic papers, posters, presentation slides, examination materials, and newspapers. Furthermore, we introduce a decoupled evaluation framework that systematically quantifies editing performance at the OCR parsing level, enabling fine-grained assessment of text modification accuracy. Based on this benchmark, we conduct a comprehensive evaluation of representative state-of-the-art image editing models. Manual verification demonstrates a strong consistency between human judgments and automated evaluation metrics. VDE Bench constitutes the first systematic benchmark for evaluating image editing models on multilingual and densely textual visual documents.

</details>


### [416] [D3R-Net: Dual-Domain Denoising Reconstruction Network for Robust Industrial Anomaly Detection](https://arxiv.org/abs/2602.00126)
*Dmytro Filatov,Valentyn Fedorov,Vira Filatova,Andrii Zelenchuk*

Main category: cs.CV

Relevance: 25.0

TL;DR: D3R-Net：一种用于无监督异常检测的双域去噪重建框架，通过空间和频域损失结合，提升缺陷检测的定位精度。


<details>
  <summary>Details</summary>
Motivation: 现有基于重建的无监督异常检测方法在处理高频细节时会产生过度平滑的结果，导致细微缺陷被部分重建而非突出显示，限制了分割精度。

Method: 提出D3R-Net双域去噪重建框架，结合自监督"修复"任务和频率感知正则化。使用合成损坏的正常图像训练，要求重建干净目标，防止恒等映射。除了空间均方误差损失，还使用FFT幅度损失确保频域一致性，可选SSIM损失。

Result: 在MVTec AD Hazelnut基准上，FFT损失将PRO AUC从0.603提升至0.687；在15个MVTec类别上，平均像素ROC AUC从0.733提升至0.751，PRO AUC从0.417提升至0.468，单GPU约20FPS。

Conclusion: D3R-Net通过结合空间和频域损失，有效提升了无监督异常检测的定位精度，为基于预训练特征嵌入的笨重方法提供了实用的轻量级替代方案。

Abstract: Unsupervised anomaly detection (UAD) is a key ingredient of automated visual inspection in modern manufacturing. The reconstruction-based methods appeal because they have basic architectural design and they process data quickly but they produce oversmoothed results for high-frequency details. As a result, subtle defects are partially reconstructed rather than highlighted, which limits segmentation accuracy. We build on this line of work and introduce D3R-Net, a Dual-Domain Denoising Reconstruction framework that couples a self-supervised 'healing' task with frequency-aware regularization. During training, the network receives synthetically corrupted normal images and is asked to reconstruct the clean targets, which prevents trivial identity mapping and pushes the model to learn the manifold of defect-free textures. In addition to the spatial mean squared error, we employ a Fast Fourier Transform (FFT) magnitude loss that encourages consistency in the frequency domain. The implementation also allows an optional structural similarity (SSIM) term, which we study in an ablation. On the MVTec AD Hazelnut benchmark, D3R-Net with the FFT loss improves localization consistency over a spatial-only baseline: PRO AUC increases from 0.603 to 0.687, while image-level ROC AUC remains robust. Evaluated across fifteen MVTec categories, the FFT variant raises the average pixel ROC AUC from 0.733 to 0.751 and PRO AUC from 0.417 to 0.468 compared to the MSE-only baseline, at roughly 20 FPS on a single GPU. The network is trained from scratch and uses a lightweight convolutional autoencoder backbone, providing a practical alternative to heavy pre-trained feature embedding methods.

</details>


### [417] [Real-Time Human Activity Recognition on Edge Microcontrollers: Dynamic Hierarchical Inference with Multi-Spectral Sensor Fusion](https://arxiv.org/abs/2602.00152)
*Boyu Li,Kuangji Zuo,Lincong Li,Yonghui Wu*

Main category: cs.CV

Relevance: 25.0

TL;DR: 提出HPPI-Net，一种基于多光谱融合和可解释模块的资源感知分层网络，用于实时、设备端的人类活动识别，在ARM Cortex-M4微控制器上实现高精度和低内存占用。


<details>
  <summary>Details</summary>
Motivation: 边缘应用中设备端模式识别的需求日益增长，但现有方法难以在精度和计算约束之间取得平衡，特别是在内存受限的边缘平台上进行实时人类活动识别。

Method: 采用两层架构：第一层使用FFT频谱图提取初步特征；第二层根据活动类型选择性地激活专用模块（静态活动）或并行LSTM-MobileNet网络（动态状态）。PLMN通过三个并行LSTM编码器融合FFT、小波和Gabor频谱图，并使用高效通道注意力和深度可分离卷积优化特征，提供通道级可解释性。

Result: 在ARM Cortex-M4上实现96.70%的准确率，仅使用22.3 KiB RAM和439.5 KiB ROM。相比MobileNetV3，准确率提高1.22%，RAM使用减少71.2%，ROM使用减少42.1%。

Conclusion: HPPI-Net在精度和效率之间取得了有利的权衡，并提供可解释的预测，为可穿戴设备、工业和智能家居等内存受限的边缘平台提供了实用的HAR解决方案。

Abstract: The demand for accurate on-device pattern recognition in edge applications is intensifying, yet existing approaches struggle to reconcile accuracy with computational constraints. To address this challenge, a resource-aware hierarchical network based on multi-spectral fusion and interpretable modules, namely the Hierarchical Parallel Pseudo-image Enhancement Fusion Network (HPPI-Net), is proposed for real-time, on-device Human Activity Recognition (HAR). Deployed on an ARM Cortex-M4 microcontroller for low-power real-time inference, HPPI-Net achieves 96.70% accuracy while utilizing only 22.3 KiB of RAM and 439.5 KiB of ROM after optimization. HPPI-Net employs a two-layer architecture. The first layer extracts preliminary features using Fast Fourier Transform (FFT) spectrograms, while the second layer selectively activates either a dedicated module for stationary activity recognition or a parallel LSTM-MobileNet network (PLMN) for dynamic states. PLMN fuses FFT, Wavelet, and Gabor spectrograms through three parallel LSTM encoders and refines the concatenated features using Efficient Channel Attention (ECA) and Depthwise Separable Convolution (DSC), thereby offering channel-level interpretability while substantially reducing multiply-accumulate operations. Compared with MobileNetV3, HPPI-Net improves accuracy by 1.22% and reduces RAM usage by 71.2% and ROM usage by 42.1%. These results demonstrate that HPPI-Net achieves a favorable accuracy-efficiency trade-off and provides explainable predictions, establishing a practical solution for wearable, industrial, and smart home HAR on memory-constrained edge platforms.

</details>


### [418] [See Without Decoding: Motion-Vector-Based Tracking in Compressed Video](https://arxiv.org/abs/2602.00153)
*Axel Duché,Clément Chatelain,Gilles Gasso*

Main category: cs.CV

Relevance: 25.0

TL;DR: 提出了一种轻量级压缩域跟踪模型，直接在视频流上操作，无需完整RGB解码。利用压缩数据中的运动向量和变换系数，通过深度模型传播目标边界框，在MOTS数据集上实现最高3.7倍计算加速，仅比RGB基线下降4% mAP@0.5。


<details>
  <summary>Details</summary>
Motivation: 针对大规模监控系统中的实时分析需求，传统基于RGB视频的目标跟踪需要完整解码，计算开销大。压缩域包含丰富的运动信息（运动向量）和纹理信息（变换系数），可直接用于跟踪，避免昂贵的解码开销。

Method: 从压缩视频流中提取运动向量和变换系数作为输入特征，设计深度神经网络模型直接在压缩域进行目标跟踪。模型学习利用运动向量传播目标边界框，结合变换系数进行目标识别和定位。

Result: 在MOTS15/17/20数据集上，相比RGB基线方法，计算速度提升最高达3.7倍，仅损失4%的mAP@0.5精度。证明了压缩域运动建模的高效性。

Conclusion: 压缩域跟踪是视频分析的高效解决方案，特别适合大规模监控系统。运动向量和变换系数提供了足够的信息进行准确跟踪，避免了昂贵的RGB解码开销。

Abstract: We propose a lightweight compressed-domain tracking model that operates directly on video streams, without requiring full RGB video decoding. Using motion vectors and transform coefficients from compressed data, our deep model propagates object bounding boxes across frames, achieving a computational speed-up of order up to 3.7 with only a slight 4% mAP@0.5 drop vs RGB baseline on MOTS15/17/20 datasets. These results highlight codec-domain motion modeling efficiency for real-time analytics in large monitoring systems.

</details>


### [419] [Intra-Class Subdivision for Pixel Contrastive Learning: Application to Semi-supervised Cardiac Image Segmentation](https://arxiv.org/abs/2602.00174)
*Jiajun Zhao,Xuan Yang*

Main category: cs.CV

Relevance: 25.0

TL;DR: 提出了一种用于心脏图像分割的类内细分像素对比学习框架（SPCL），通过引入"无关样本"概念和边界对比损失来解决边界处的表示污染问题，显著提升了分割质量和边界精度。


<details>
  <summary>Details</summary>
Motivation: 在医学图像分割中，边界区域的像素表示容易受到污染，导致分割精度下降。传统方法在处理同一类别内不同区域（内部区域和边界区域）的像素表示时存在局限，需要更精细的表示学习方法来区分这些区域。

Method: 1. 提出类内细分像素对比学习框架（SPCL）
2. 引入"无关样本"概念，区分同一类别内的内部区域和边界区域像素表示
3. 设计边界对比损失函数，专门增强边界表示的可区分性
4. 从理论上分析了无关样本和边界对比损失的优势

Result: 在公开的心脏数据集上的实验结果表明，SPCL显著提升了分割性能，在分割质量和边界精度方面优于现有方法。

Conclusion: SPCL框架通过精细的类内表示学习和边界对比优化，有效解决了医学图像分割中的边界表示污染问题，为高质量分割提供了新思路。

Abstract: We propose an intra-class subdivision pixel contrastive learning (SPCL) framework for cardiac image segmentation to address representation contamination at boundaries. The novel concept ``Unconcerned sample'' is proposed to distinguish pixel representations at the inner and boundary regions within the same class, facilitating a clearer characterization of intra-class variations. A novel boundary contrastive loss for boundary representations is proposed to enhance representation discrimination across boundaries. The advantages of the unconcerned sample and boundary contrastive loss are analyzed theoretically. Experimental results in public cardiac datasets demonstrate that SPCL significantly improves segmentation performance, outperforming existing methods with respect to segmentation quality and boundary precision. Our code is available at https://github.com/Jrstud203/SPCL.

</details>


### [420] [Deep Learning Based CNN Model for Automated Detection of Pneumonia from Chest XRay Images](https://arxiv.org/abs/2602.00212)
*Sathish Krishna Anumula,Vetrivelan Tamilmani,Aniruddha Arjun Singh,Dinesh Rajendran,Venkata Deepak Namburi*

Main category: cs.CV

Relevance: 25.0

TL;DR: 本文提出了一种基于定制卷积神经网络（CNN）的自动化肺炎诊断模型，专门针对胸部X光图像进行优化，通过深度可分离卷积设计、CLAHE预处理和几何增强技术，实现了高精度且计算效率高的肺炎检测。


<details>
  <summary>Details</summary>
Motivation: 肺炎是全球发病率和死亡率的主要原因之一，尤其在儿科和老年人群中发病率高。传统依赖专家手动解读胸部X光的方法存在观察者差异、专家疲劳和合格放射科医生短缺等问题，需要快速精确的诊断方法。

Method: 使用定制的卷积神经网络（CNN），采用深度可分离卷积设计，专门针对灰度医学图像的纹理特征进行优化。采用对比度受限自适应直方图均衡化（CLAHE）和几何增强作为重要预处理技术，解决类别不平衡问题并提高泛化能力。

Result: 该系统在包含5863张前后位胸部X光的数据集上进行测试，能够以高精度和最小计算开销识别肺炎。

Conclusion: 提出的统一自动化诊断模型能够有效解决传统肺炎诊断方法的局限性，为临床干预提供快速精确的诊断支持。

Abstract: Pneumonia has been one of the major causes of morbidities and mortality in the world and the prevalence of this disease is disproportionately high among the pediatric and elderly populations especially in resources trained areas Fast and precise diagnosis is a prerequisite for successful clinical intervention but due to inter observer variation fatigue among experts and a shortage of qualified radiologists traditional approaches that rely on manual interpretation of chest radiographs are frequently constrained To address these problems this paper introduces a unified automated diagnostic model using a custom Convolutional Neural Network CNN that can recognize pneumonia in chest Xray images with high precision and at minimal computational expense In contrast like other generic transfer learning based models which often possess redundant parameters the offered architecture uses a tailor made depth wise separable convolutional design which is optimized towards textural characteristics of grayscale medical images Contrast Limited Adaptive Histogram Equalization CLAHE and geometric augmentation are two significant preprocessing techniques used to ensure that the system does not experience class imbalance and is more likely to generalize The system is tested using a dataset of 5863 anterior posterior chest Xrays.

</details>


### [421] [A Geometric Multimodal Foundation Model Integrating Bp-MRI and Clinical Reports in Prostate Cancer Classification](https://arxiv.org/abs/2602.00214)
*Juan A. Olmos,Antoine Manzanera,Fabio Martínez*

Main category: cs.CV

Relevance: 25.0

TL;DR: MFM-Geom是一个用于前列腺癌诊断的几何多模态基础模型，结合bp-MRI影像和临床报告，利用SPD矩阵和黎曼深度学习进行表征融合，在数据稀缺情况下表现出色。


<details>
  <summary>Details</summary>
Motivation: 前列腺癌诊断依赖专家主观解读，现有计算机辅助诊断方法主要基于影像模型，忽视临床背景且受数据稀缺限制，难以学习鲁棒表征。

Method: 提出MFM-Geom几何多模态基础模型，从bp-MRI和临床报告中学习表征，使用对称正定矩阵和黎曼深度学习融合影像-文本表征。

Result: 仅用10%训练数据，MFM-Geom在AUC-PR上比基线分类token嵌入方法提升8.3%，达到90.67%。在外部数据集上泛化性能良好，AUC-PR达90.6。

Conclusion: 该方法通过几何多模态基础模型有效整合医学影像和临床文本信息，在数据稀缺条件下实现鲁棒的前列腺癌诊断。

Abstract: Prostate cancer (PCa) is one of the most common cancers in men worldwide. Bi-parametric MRI (bp-MRI) and clinical variables are crucial for PCa identification and improving treatment decisions. However, this process is subjective to expert interpretations. Furthermore, most existing computer-aided diagnosis methods focus on imaging-based models, overlooking the clinical context and suffering from data scarcity, limiting their ability to learn robust representations. We propose a geometric multimodal Foundation Model (FM), named MFM-Geom, that learns representations from bp-MRI and clinical reports, encoding visual findings and information from the context of clinical variables. In the representations classification head, the approach leverages symmetric positive definite (SPD) matrices and Riemannian deep learning to integrate imaging-text representations from a biomedical multimodal FM. Using 10% of the training data, MFM-Geom outperformed baseline class token embedding-based classification (+8.3%, AUC-PR of 90.67). Generalization on external dataset confirmed the robustness of fine-tuning biomedical FM, achieving an AUC-PR of 90.6.

</details>


### [422] [Subspace Clustering on Incomplete Data with Self-Supervised Contrastive Learning](https://arxiv.org/abs/2602.00262)
*Huanran Li,Daniel Pimentel-Alarcón*

Main category: cs.CV

Relevance: 25.0

TL;DR: 提出了一种用于不完整数据的对比自监督子空间聚类框架CSC，通过生成掩码视图和对比学习学习不变嵌入，然后进行稀疏子空间聚类


<details>
  <summary>Details</summary>
Motivation: 现有子空间聚类方法大多假设数据完全观测，但在现实场景中数据常有缺失，限制了这些方法的有效性。需要开发能够处理不完整数据的子空间聚类方法。

Method: 提出对比子空间聚类(CSC)框架：1) 对部分观测输入生成掩码视图；2) 使用SimCLR风格的对比损失训练深度神经网络学习不变嵌入；3) 使用稀疏子空间聚类对嵌入进行聚类。

Result: 在六个基准数据集上的实验表明，CSC始终优于经典和深度学习方法，对缺失数据表现出强鲁棒性，并能扩展到大型数据集。

Conclusion: CSC为不完整数据的子空间聚类提供了一种有效的对比自监督解决方案，在多个应用领域具有实用价值。

Abstract: Subspace clustering aims to group data points that lie in a union of low-dimensional subspaces and finds wide application in computer vision, hyperspectral imaging, and recommendation systems. However, most existing methods assume fully observed data, limiting their effectiveness in real-world scenarios with missing entries. In this paper, we propose a contrastive self-supervised framework, Contrastive Subspace Clustering (CSC), designed for clustering incomplete data. CSC generates masked views of partially observed inputs and trains a deep neural network using a SimCLR-style contrastive loss to learn invariant embeddings. These embeddings are then clustered using sparse subspace clustering. Experiments on six benchmark datasets show that CSC consistently outperforms both classical and deep learning baselines, demonstrating strong robustness to missing data and scalability to large datasets.

</details>


### [423] [World-Shaper: A Unified Framework for 360° Panoramic Editing](https://arxiv.org/abs/2602.00265)
*Dong Liang,Yuhao Liu,Jinyuan Jia,Youjun Zhao,Rynson W. H. Lau*

Main category: cs.CV

Relevance: 25.0

TL;DR: World-Shaper：一个统一的几何感知全景图像编辑框架，直接在等距柱状投影域中操作，通过生成-编辑范式解决配对数据稀缺问题，实现几何一致的高质量全景编辑。


<details>
  <summary>Details</summary>
Motivation: 现有基于透视的图像编辑方法无法建模全景图像的空间结构，传统的立方体贴图分解方法由于与球面几何不匹配而破坏全局一致性，需要直接在等距柱状投影域中解决全景编辑问题。

Method: 采用生成-编辑范式：1）可控全景生成作为辅助阶段合成多样化的配对示例；2）几何感知学习策略，通过位置感知形状监督显式约束，通过渐进式训练隐式内化全景先验；3）在PEBench基准上进行评估。

Result: 在PEBench基准上的实验表明，该方法在几何一致性、编辑保真度和文本可控性方面优于现有SOTA方法，能够实现连贯灵活的全景视觉世界创建。

Conclusion: World-Shaper提供了一个统一的几何感知框架，直接在ERP域中桥接全景生成和编辑，解决了全景编辑中的几何失真和配对数据稀缺问题，实现了高质量的全景图像编辑。

Abstract: Being able to edit panoramic images is crucial for creating realistic 360° visual experiences. However, existing perspective-based image editing methods fail to model the spatial structure of panoramas. Conventional cube-map decompositions attempt to overcome this problem but inevitably break global consistency due to their mismatch with spherical geometry. Motivated by this insight, we reformulate panoramic editing directly in the equirectangular projection (ERP) domain and present World-Shaper, a unified geometry-aware framework that bridges panoramic generation and editing within a single editing-centric design. To overcome the scarcity of paired data, we adopt a generate-then-edit paradigm, where controllable panoramic generation serves as an auxiliary stage to synthesize diverse paired examples for supervised editing learning. To address geometric distortion, we introduce a geometry-aware learning strategy that explicitly enforces position-aware shape supervision and implicitly internalizes panoramic priors through progressive training. Extensive experiments on our new benchmark, PEBench, demonstrate that our method achieves superior geometric consistency, editing fidelity, and text controllability compared to SOTA methods, enabling coherent and flexible 360° visual world creation with unified editing control. Code, model, and data will be released at our project page: https://world-shaper-project.github.io/

</details>


### [424] [Robust automatic brain vessel segmentation in 3D CTA scans using dynamic 4D-CTA data](https://arxiv.org/abs/2602.00391)
*Alberto Mario Ceballos-Arroyo,Shrikanth M. Yadav,Chu-Hsuan Lin,Jisoo Kim,Geoffrey S. Young,Huaizu Jiang,Lei Qin*

Main category: cs.CV

Relevance: 25.0

TL;DR: 提出一种基于动态4D-CTA扫描的脑血管标注新方法，通过多时相数据增强血管可视化，训练深度学习模型实现更精确的脑血管分割。


<details>
  <summary>Details</summary>
Motivation: 传统脑血管分割需要大量手动标注，且CTA扫描中的骨骼和软组织干扰血管可视化。本研究旨在利用动态4D-CTA的多时相信息，减少手动标注工作量，提高脑血管分割的准确性和鲁棒性。

Method: 使用动态4D-CTA头扫描的多时相数据，通过减影技术去除骨骼和软组织干扰，增强血管可视化。利用同一分割标注应用于多个时相，将数据集扩大4-5倍，训练nnUNet模型进行脑血管分割。

Result: 在110张训练图像和165张测试图像上，模型在TopBrain数据集中动脉平均mDC达到0.846，静脉达到0.957。平均定向Hausdorff距离（动脉0.304mm，静脉0.078mm）和拓扑敏感性（动脉0.877，静脉0.974）均显示优异性能。

Conclusion: 提出的动态4D-CTA标注方法能有效减少手动标注工作量，生成高质量脑血管分割数据集，训练出的模型在血管形态捕捉和分割准确性方面显著优于现有方法。

Abstract: In this study, we develop a novel methodology for annotating the brain vasculature using dynamic 4D-CTA head scans. By using multiple time points from dynamic CTA acquisitions, we subtract bone and soft tissue to enhance the visualization of arteries and veins, reducing the effort required to obtain manual annotations of brain vessels. We then train deep learning models on our ground truth annotations by using the same segmentation for multiple phases from the dynamic 4D-CTA collection, effectively enlarging our dataset by 4 to 5 times and inducing robustness to contrast phases. In total, our dataset comprises 110 training images from 25 patients and 165 test images from 14 patients. In comparison with two similarly-sized datasets for CTA-based brain vessel segmentation, a nnUNet model trained on our dataset can achieve significantly better segmentations across all vascular regions, with an average mDC of 0.846 for arteries and 0.957 for veins in the TopBrain dataset. Furthermore, metrics such as average directed Hausdorff distance (adHD) and topology sensitivity (tSens) reflected similar trends: using our dataset resulted in low error margins (aDHD of 0.304 mm for arteries and 0.078 for veins) and high sensitivity (tSens of 0.877 for arteries and 0.974 for veins), indicating excellent accuracy in capturing vessel morphology. Our code and model weights are available online: https://github.com/alceballosa/robust-vessel-segmentation

</details>


### [425] [ZS-TreeSeg: A Zero-Shot Framework for Tree Crown Instance Segmentation](https://arxiv.org/abs/2602.00470)
*Pengyu Chen,Fangzheng Lyu,Sicheng Wang,Cuizhen Wang*

Main category: cs.CV

Relevance: 25.0

TL;DR: 提出ZS-TreeSeg零样本框架，通过结合冠层语义分割和细胞实例分割技术，实现密集重叠树冠的自动分割，无需训练数据


<details>
  <summary>Details</summary>
Motivation: 传统监督学习方法标注成本高且泛化能力有限，而基础模型（如SAM）缺乏领域知识导致在密集树冠中分割不足。需要一种无需训练、能适应不同传感器和冠层密度的解决方案。

Method: 将树冠建模为拓扑流场中的星凸对象，使用Cellpose-SAM框架，基于向量收敛原理实现接触树冠实例的数学分离。结合冠层语义分割和细胞实例分割两个成熟任务。

Result: 在NEON和BAMFOREST数据集上的实验和视觉检查表明，该框架能稳健地泛化到不同传感器类型和冠层密度，为树冠实例分割和标签生成提供无训练解决方案。

Conclusion: ZS-TreeSeg为零样本树冠分割提供了有效框架，解决了密集重叠冠层分割的瓶颈问题，具有实际应用价值。

Abstract: Individual tree crown segmentation is an important task in remote sensing for forest biomass estimation and ecological monitoring. However, accurate delineation in dense, overlapping canopies remains a bottleneck. While supervised deep learning methods suffer from high annotation costs and limited generalization, emerging foundation models (e.g., Segment Anything Model) often lack domain knowledge, leading to under-segmentation in dense clusters. To bridge this gap, we propose ZS-TreeSeg, a Zero-Shot framework that adapts from two mature tasks: 1) Canopy Semantic segmentation; and 2) Cells instance segmentation. By modeling tree crowns as star-convex objects within a topological flow field using Cellpose-SAM, the ZS-TreeSeg framework forces the mathematical separation of touching tree crown instances based on vector convergence. Experiments on the NEON and BAMFOREST datasets and visual inspection demonstrate that our framework generalizes robustly across diverse sensor types and canopy densities, which can offer a training-free solution for tree crown instance segmentation and labels generation.

</details>


### [426] [MAUGen: A Unified Diffusion Approach for Multi-Identity Facial Expression and AU Label Generation](https://arxiv.org/abs/2602.00583)
*Xiangdong Li,Ye Lou,Ao Gao,Wei Zhang,Siyang Song*

Main category: cs.CV

Relevance: 25.0

TL;DR: MAUGen是一个基于扩散模型的多模态框架，用于生成具有精确动作单元(AU)标注的大规模多样化人脸图像，解决了AU识别领域数据稀缺的问题。


<details>
  <summary>Details</summary>
Motivation: AU识别系统缺乏大规模、人口统计学多样的人脸图像数据集，这些数据集需要精确的AU发生和强度标注。现有数据集的局限性限制了AU识别系统的泛化能力。

Method: 提出MAUGen框架：1) 多模态表示学习模块，在统一潜在空间中捕捉文本描述、人脸身份、表情图像和AU激活之间的关系；2) 基于扩散的图像标签生成器，将联合表示解码为对齐的人脸图像-标签对。基于此框架构建了MIFA大规模合成数据集。

Result: MAUGen在生成逼真、人口统计学多样化的人脸图像以及语义对齐的AU标签方面优于现有方法。生成的MIFA数据集包含全面的AU标注和身份变化。

Conclusion: MAUGen通过生成大规模合成数据集，解决了AU识别领域的数据瓶颈问题，为开发更通用的AU识别系统提供了重要资源。

Abstract: The lack of large-scale, demographically diverse face images with precise Action Unit (AU) occurrence and intensity annotations has long been recognized as a fundamental bottleneck in developing generalizable AU recognition systems. In this paper, we propose MAUGen, a diffusion-based multi-modal framework that jointly generates a large collection of photorealistic facial expressions and anatomically consistent AU labels, including both occurrence and intensity, conditioned on a single descriptive text prompt. Our MAUGen involves two key modules: (1) a Multi-modal Representation Learning (MRL) module that captures the relationships among the paired textual description, facial identity, expression image, and AU activations within a unified latent space; and (2) a Diffusion-based Image label Generator (DIG) that decodes the joint representation into aligned facial image-label pairs across diverse identities. Under this framework, we introduce Multi-Identity Facial Action (MIFA), a large-scale multimodal synthetic dataset featuring comprehensive AU annotations and identity variations. Extensive experiments demonstrate that MAUGen outperforms existing methods in synthesizing photorealistic, demographically diverse facial images along with semantically aligned AU labels.

</details>


### [427] [Tune-Your-Style: Intensity-tunable 3D Style Transfer with Gaussian Splatting](https://arxiv.org/abs/2602.00618)
*Yian Zhao,Rushi Ye,Ruochong Zheng,Zesen Cheng,Chaoran Feng,Jiashu Yang,Pengchong Qiao,Chang Liu,Jie Chen*

Main category: cs.CV

Relevance: 25.0

TL;DR: 本文提出了一种可调节强度的3D风格迁移方法Tune-Your-Style，允许用户灵活调整场景中的风格注入强度，增强3D风格迁移的可定制性。


<details>
  <summary>Details</summary>
Motivation: 现有3D风格迁移方法采用固定输出范式，难以适应不同用户对内容-风格平衡的多样化需求。3DGS方法虽然训练和渲染速度快，但在内容与风格模式、颜色之间的平衡方面存在挑战。

Method: 1) 引入高斯神经元显式建模风格强度，参数化可学习风格调节器实现强度可调的风格注入；2) 提出可调风格化引导，通过跨视图风格对齐从扩散模型获得多视图一致的风格化视图；3) 采用两阶段优化策略，通过调制来自风格化视图的全风格引导和初始渲染的零风格引导之间的平衡，提供稳定高效的引导。

Result: 实验表明，该方法不仅能够产生视觉上吸引人的结果，而且在3D风格迁移方面展现出灵活的可定制性。

Conclusion: 本文提出的Tune-Your-Style范式通过引入可调节的风格强度控制，解决了3D风格迁移中内容-风格平衡的定制化需求，为3D风格迁移提供了新的可定制解决方案。

Abstract: 3D style transfer refers to the artistic stylization of 3D assets based on reference style images. Recently, 3DGS-based stylization methods have drawn considerable attention, primarily due to their markedly enhanced training and rendering speeds. However, a vital challenge for 3D style transfer is to strike a balance between the content and the patterns and colors of the style. Although the existing methods strive to achieve relatively balanced outcomes, the fixed-output paradigm struggles to adapt to the diverse content-style balance requirements from different users. In this work, we introduce a creative intensity-tunable 3D style transfer paradigm, dubbed \textbf{Tune-Your-Style}, which allows users to flexibly adjust the style intensity injected into the scene to match their desired content-style balance, thus enhancing the customizability of 3D style transfer. To achieve this goal, we first introduce Gaussian neurons to explicitly model the style intensity and parameterize a learnable style tuner to achieve intensity-tunable style injection. To facilitate the learning of tunable stylization, we further propose the tunable stylization guidance, which obtains multi-view consistent stylized views from diffusion models through cross-view style alignment, and then employs a two-stage optimization strategy to provide stable and efficient guidance by modulating the balance between full-style guidance from the stylized views and zero-style guidance from the initial rendering. Extensive experiments demonstrate that our method not only delivers visually appealing results, but also exhibits flexible customizability for 3D style transfer. Project page is available at https://zhao-yian.github.io/TuneStyle.

</details>


### [428] [Supervised makeup transfer with a curated dataset: Decoupling identity and makeup features for enhanced transformation](https://arxiv.org/abs/2602.00729)
*Qihe Pan,Yiming Wu,Xing Zhao,Liang Xie,Guodao Sun,Ronghua Liang*

Main category: cs.CV

Relevance: 25.0

TL;DR: 本文提出了一种基于扩散模型的化妆迁移方法，通过构建高质量数据集、设计特征解耦框架和文本引导控制机制，解决了现有方法在数据集有限、特征解耦不足和可控性弱的问题。


<details>
  <summary>Details</summary>
Motivation: 现有化妆迁移方法存在三个主要问题：1）数据集有限且质量不高；2）身份特征与化妆特征解耦不充分；3）可控性弱，无法实现细粒度的区域控制。这些问题限制了化妆迁移的实际应用效果。

Method: 1）采用训练-生成-过滤-再训练策略构建高质量数据集；2）设计基于扩散模型的框架，解耦身份特征（面部结构、肤色）和化妆特征；3）提出文本引导机制，通过自然语言提示实现细粒度的区域控制（如眼妆、唇妆等）。

Result: 在基准测试和真实场景中，该方法在保真度、身份保持和灵活性方面均有显著提升，能够生成高质量、多样化的化妆迁移结果。

Conclusion: 本文提出的扩散模型方法有效解决了化妆迁移中的关键挑战，通过高质量数据集、特征解耦框架和文本引导控制，实现了更稳定、可控和高质量的化妆迁移效果。

Abstract: Diffusion models have recently shown strong progress in generative tasks, offering a more stable alternative to GAN-based approaches for makeup transfer. Existing methods often suffer from limited datasets, poor disentanglement between identity and makeup features, and weak controllability. To address these issues, we make three contributions. First, we construct a curated high-quality dataset using a train-generate-filter-retrain strategy that combines synthetic, realistic, and filtered samples to improve diversity and fidelity. Second, we design a diffusion-based framework that disentangles identity and makeup features, ensuring facial structure and skin tone are preserved while applying accurate and diverse cosmetic styles. Third, we propose a text-guided mechanism that allows fine-grained and region-specific control, enabling users to modify eyes, lips, or face makeup with natural language prompts. Experiments on benchmarks and real-world scenarios demonstrate improvements in fidelity, identity preservation, and flexibility. Examples of our dataset can be found at: https://makeup-adapter.github.io.

</details>


### [429] [VVLoc: Prior-free 3-DoF Vehicle Visual Localization](https://arxiv.org/abs/2602.00810)
*Ze Huang,Zhongyang Xiao,Mingliang Song,Longan Yang,Hongyuan Yuan,Li Sun*

Main category: cs.CV

Relevance: 25.0

TL;DR: VVLoc是一个统一的车辆定位框架，使用单一神经网络通过多摄像头系统同时实现拓扑定位和度量定位，并提供置信度评估。


<details>
  <summary>Details</summary>
Motivation: 传统定位方法通常独立处理拓扑定位和度量定位，依赖单摄像头设置，需要额外的3D语义或姿态先验，且缺乏置信度量化机制，难以满足实际工业应用需求。

Method: 使用单一神经网络，通过多摄像头系统：1) 评估视觉观测之间的地理邻近性（拓扑定位）；2) 使用匹配策略估计相对度量姿态；3) 提供置信度测量。训练仅需视觉数据对和对应真实姿态，无需复杂辅助数据。

Result: 在公开数据集和更具挑战性的自收集数据集上评估，VVLoc在各种定位任务中实现了最先进的定位精度。

Conclusion: VVLoc提供了一个高效、统一的车辆定位解决方案，能够同时处理拓扑和度量定位，提供置信度评估，且训练过程简单高效，适合实际工业应用。

Abstract: Localization is a critical technology in autonomous driving, encompassing both topological localization, which identifies the most similar map keyframe to the current observation, and metric localization, which provides precise spatial coordinates. Conventional methods typically address these tasks independently, rely on single-camera setups, and often require additional 3D semantic or pose priors, while lacking mechanisms to quantify the confidence of localization results, making them less feasible for real industrial applications. In this paper, we propose VVLoc, a unified pipeline that employs a single neural network to concurrently achieve topological and metric vehicle localization using multi-camera system. VVLoc first evaluates the geo-proximity between visual observations, then estimates their relative metric poses using a matching strategy, while also providing a confidence measure. Additionally, the training process for VVLoc is highly efficient, requiring only pairs of visual data and corresponding ground-truth poses, eliminating the need for complex supplementary data. We evaluate VVLoc not only on the publicly available datasets, but also on a more challenging self-collected dataset, demonstrating its ability to deliver state-of-the-art localization accuracy across a wide range of localization tasks.

</details>


### [430] [Invariance on Manifolds: Understanding Robust Visual Representations for Place Recognition](https://arxiv.org/abs/2602.00841)
*Jintao Cheng,Weibin Li,Zhijian He,Jin Wu,Chi Man Vong,Wei Zhang*

Main category: cs.CV

Relevance: 25.0

TL;DR: 提出一种基于二阶几何统计的视觉地点识别框架，通过协方差描述符在SPD流形上捕捉几何稳定性，无需训练即可实现零样本泛化。


<details>
  <summary>Details</summary>
Motivation: 当前视觉地点识别方法要么依赖数据密集型监督，要么使用简单的一阶统计，忽略了内在的结构相关性。需要一种能捕捉几何稳定性且无需训练的方法。

Method: 将场景建模为SPD流形上的协方差描述符，扰动表现为可处理的同余变换。通过几何感知的黎曼映射将描述符投影到线性化欧几里得嵌入中，分离信号结构与噪声。基于预训练骨干构建无需训练框架。

Result: 在广泛实验中表现出与最先进基线方法相竞争的性能，特别是在具有挑战性的零样本场景中表现优异。

Conclusion: 提出的二阶几何统计框架通过捕捉几何稳定性，实现了无需训练的零样本泛化，为视觉地点识别提供了有效解决方案。

Abstract: Visual Place Recognition (VPR) demands representations robust to drastic environmental and viewpoint shifts. Current aggregation paradigms, however, either rely on data-hungry supervision or simplistic first-order statistics, often neglecting intrinsic structural correlations. In this work, we propose a Second-Order Geometric Statistics framework that inherently captures geometric stability without training. We conceptualize scenes as covariance descriptors on the Symmetric Positive Definite (SPD) manifold, where perturbations manifest as tractable congruence transformations. By leveraging geometry-aware Riemannian mappings, we project these descriptors into a linearized Euclidean embedding, effectively decoupling signal structure from noise. Our approach introduces a training-free framework built upon fixed, pre-trained backbones, achieving strong zero-shot generalization without parameter updates. Extensive experiments confirm that our method achieves highly competitive performance against state-of-the-art baselines, particularly excelling in challenging zero-shot scenarios.

</details>


### [431] [Data Augmentation for High-Fidelity Generation of CAR-T/NK Immunological Synapse Images](https://arxiv.org/abs/2602.00949)
*Xiang Zhang,Boxuan Zhang,Alireza Naghizadeh,Mohab Mohamed,Dongfang Liu,Ruixiang Tang,Dimitris Metaxas,Dongfang Liu*

Main category: cs.CV

Relevance: 25.0

TL;DR: 该论文提出两种数据增强框架（IAAA和SAAA）来生成合成CAR-T/NK免疫突触图像，解决医学影像标注数据稀缺问题，提升神经网络在癌症免疫治疗中的检测和分割性能。


<details>
  <summary>Details</summary>
Motivation: CAR-T/NK细胞免疫疗法中，免疫突触质量是预测疗效的重要生物标志物。然而，标注的显微镜图像数据集规模有限，限制了人工神经网络在免疫突触检测和分割中的泛化能力。

Method: 提出两种互补的数据增强框架：1) IAAA（实例感知自动增强）：自动化的实例保留增强方法，对原始IS数据应用优化增强策略生成合成图像和分割掩码；2) SAAA（语义感知AI增强）：结合基于扩散的掩码生成器和Pix2Pix条件图像合成器，生成多样化的解剖学真实掩码和对应的高保真IS图像。

Result: 两种增强策略生成的合成图像在视觉和结构特性上与真实IS数据高度匹配，显著提升了CAR-T/NK免疫突触的检测和分割性能。

Conclusion: 通过增强免疫突触量化的鲁棒性和准确性，这项工作支持开发更可靠的基于影像的生物标志物，用于预测患者对CAR-T/NK免疫疗法的反应。

Abstract: Chimeric antigen receptor (CAR)-T and NK cell immunotherapies have transformed cancer treatment, and recent studies suggest that the quality of the CAR-T/NK cell immunological synapse (IS) may serve as a functional biomarker for predicting therapeutic efficacy. Accurate detection and segmentation of CAR-T/NK IS structures using artificial neural networks (ANNs) can greatly increase the speed and reliability of IS quantification. However, a persistent challenge is the limited size of annotated microscopy datasets, which restricts the ability of ANNs to generalize. To address this challenge, we integrate two complementary data-augmentation frameworks. First, we employ Instance Aware Automatic Augmentation (IAAA), an automated, instance-preserving augmentation method that generates synthetic CAR-T/NK IS images and corresponding segmentation masks by applying optimized augmentation policies to original IS data. IAAA supports multiple imaging modalities (e.g., fluorescence and brightfield) and can be applied directly to CAR-T/NK IS images derived from patient samples. In parallel, we introduce a Semantic-Aware AI Augmentation (SAAA) pipeline that combines a diffusion-based mask generator with a Pix2Pix conditional image synthesizer. This second method enables the creation of diverse, anatomically realistic segmentation masks and produces high-fidelity CAR-T/NK IS images aligned with those masks, further expanding the training corpus beyond what IAAA alone can provide. Together, these augmentation strategies generate synthetic images whose visual and structural properties closely match real IS data, significantly improving CAR-T/NK IS detection and segmentation performance. By enhancing the robustness and accuracy of IS quantification, this work supports the development of more reliable imaging-based biomarkers for predicting patient response to CAR-T/NK immunotherapy.

</details>


### [432] [CortiNet: A Physics-Perception Hybrid Cortical-Inspired Dual-Stream Network for Gallbladder Disease Diagnosis from Ultrasound](https://arxiv.org/abs/2602.01000)
*Vagish Kumar,Souvik Chakraborty*

Main category: cs.CV

Relevance: 25.0

TL;DR: CortiNet：一种轻量级、受皮层启发的双流神经网络架构，用于胆囊疾病诊断，通过物理可解释的多尺度信号分解和感知驱动的特征学习，在保持高准确率的同时大幅减少参数数量。


<details>
  <summary>Details</summary>
Motivation: 超声成像是胆囊疾病的主要诊断方式，但图像分辨率低且存在斑点噪声，影响诊断可靠性。现有大型卷积神经网络难以在常规临床环境中部署，需要轻量级且高效的解决方案。

Method: 受人类视觉皮层并行处理通路启发，CortiNet将低频结构信息与高频感知细节分离，通过专门的编码流处理。采用物理可解释的多尺度信号分解和感知驱动的特征学习，直接处理结构化、频率选择性表示而非原始像素强度。后期皮层式融合机制整合互补的结构和纹理线索。

Result: 在10,692张专家标注图像（涵盖9个临床相关胆囊疾病类别）上评估，CortiNet达到98.74%的诊断准确率，仅需传统深度卷积模型参数的一小部分。

Conclusion: CortiNet通过结合物理可解释的信号分解和感知驱动的特征学习，实现了高效、轻量级的胆囊疾病诊断，具有临床部署潜力。提出的结构感知可解释性框架增强了模型对斑点噪声的鲁棒性。

Abstract: Ultrasound imaging is the primary diagnostic modality for detecting Gallbladder diseases due to its non-invasive nature, affordability, and wide accessibility. However, the low resolution and speckle noise inherent to ultrasound images hinder diagnostic reliability, prompting the use of large convolutional neural networks that are difficult to deploy in routine clinical settings. In this work, we propose CortiNet, a lightweight, cortical-inspired dual-stream neural architecture for gallbladder disease diagnosis that integrates physically interpretable multi-scale signal decomposition with perception-driven feature learning. Inspired by parallel processing pathways in the human visual cortex, CortiNet explicitly separates low-frequency structural information from high-frequency perceptual details and processes them through specialized encoding streams. By operating directly on structured, frequency-selective representations rather than raw pixel intensities, the architecture embeds strong physics-based inductive bias, enabling efficient feature learning with a significantly reduced parameter footprint. A late-stage cortical-style fusion mechanism integrates complementary structural and textural cues while preserving computational efficiency. Additionally, we propose a structure-aware explainability framework wherein gradient-weighted class activation mapping is only applied to the structural branch of the proposed CortiNet architecture. This choice allows the model to only focus on the structural features, making it robust against speckle noise. We evaluate CortiNet on 10,692 expert-annotated images spanning nine clinically relevant gallbladder disease categories. Experimental results demonstrate that CortiNet achieves high diagnostic accuracy (98.74%) with only a fraction of the parameters required by conventional deep convolutional models.

</details>


### [433] [Effectiveness of Automatically Curated Dataset in Thyroid Nodules Classification Algorithms Using Deep Learning](https://arxiv.org/abs/2602.01020)
*Jichen Yang,Jikai Zhang,Benjamin Wildman-Tobriner,Maciej A. Mazurowski*

Main category: cs.CV

Relevance: 25.0

TL;DR: 该研究证明，在甲状腺结节超声图像分类任务中，使用自动标注数据集训练的深度学习模型性能显著优于使用人工标注数据集训练的模型，且使用全部自动标注数据比仅使用高精度子集效果更好。


<details>
  <summary>Details</summary>
Motivation: 甲状腺结节癌症诊断通常使用超声图像，深度学习算法可以匹配放射科医生的性能，但训练数据有限。先前研究提出了自动标注甲状腺结节数据集的方法（63%产出率，83%准确率），但该数据对深度学习模型训练的有效性未知。

Method: 在人工标注数据集和自动标注数据集上分别训练深度学习模型，同时使用自动标注数据集中精度更高的子集进行训练，以探索此类数据集的最佳使用方式。

Result: 人工标注数据集训练的模型AUC为0.643，显著低于自动标注数据集训练的模型AUC 0.694（P < .001）。高精度子集训练的模型AUC为0.689，与完整自动标注数据集相比无显著差异（P > .43）。

Conclusion: 使用自动标注数据集可以显著提升深度学习算法的性能，建议使用全部自动标注数据而非仅使用高精度子集。

Abstract: The diagnosis of thyroid nodule cancers commonly utilizes ultrasound images. Several studies showed that deep learning algorithms designed to classify benign and malignant thyroid nodules could match radiologists' performance. However, data availability for training deep learning models is often limited due to the significant effort required to curate such datasets. The previous study proposed a method to curate thyroid nodule datasets automatically. It was tested to have a 63% yield rate and 83% accuracy. However, the usefulness of the generated data for training deep learning models remains unknown. In this study, we conducted experiments to determine whether using a automatically-curated dataset improves deep learning algorithms' performance. We trained deep learning models on the manually annotated and automatically-curated datasets. We also trained with a smaller subset of the automatically-curated dataset that has higher accuracy to explore the optimum usage of such dataset. As a result, the deep learning model trained on the manually selected dataset has an AUC of 0.643 (95% confidence interval [CI]: 0.62, 0.66). It is significantly lower than the AUC of the 6automatically-curated dataset trained deep learning model, 0.694 (95% confidence interval [CI]: 0.67, 0.73, P < .001). The AUC of the accurate subset trained deep learning model is 0.689 (95% confidence interval [CI]: 0.66, 0.72, P > .43), which is insignificantly worse than the AUC of the full automatically-curated dataset. In conclusion, we showed that using a automatically-curated dataset can substantially increase the performance of deep learning algorithms, and it is suggested to use all the data rather than only using the accurate subset.

</details>


### [434] [GMAC: Global Multi-View Constraint for Automatic Multi-Camera Extrinsic Calibration](https://arxiv.org/abs/2602.01033)
*Chentian Sun*

Main category: cs.CV

Relevance: 25.0

TL;DR: GMAC提出了一种基于多视图重建网络隐式几何表示的多相机外参标定框架，无需显式3D重建或手动标定，通过联合优化重投影一致性和多视图循环一致性实现准确稳定的外参估计。


<details>
  <summary>Details</summary>
Motivation: 现有多相机系统标定方法依赖标定板、显式几何建模或任务特定神经网络，在复杂动态环境或在线场景中鲁棒性和适用性有限，难以在实际应用中部署。

Method: GMAC将外参建模为受多视图几何结构约束的全局变量，对现有网络进行剪枝和结构重构，使其隐式特征通过轻量回归头直接支持外参预测，无需全新网络设计。联合优化跨视图重投影一致性和多视图循环一致性。

Result: 在合成和真实世界多相机数据集上的实验表明，GMAC无需显式3D重建或手动标定即可实现准确稳定的外参估计，为多相机系统高效部署和在线标定提供了新解决方案。

Conclusion: GMAC通过利用多视图重建网络的隐式几何表示，提供了一种鲁棒、高效的多相机外参标定方法，在复杂动态环境中具有良好适用性。

Abstract: Automatic calibration of multi-camera systems, namely the accurate estimation of spatial extrinsic parameters, is fundamental for 3D reconstruction, panoramic perception, and multi-view data fusion. Existing methods typically rely on calibration targets, explicit geometric modeling, or task-specific neural networks. Such approaches often exhibit limited robustness and applicability in complex dynamic environments or online scenarios, making them difficult to deploy in practical applications. To address this, this paper proposes GMAC, a multi-camera extrinsic estimation framework based on the implicit geometric representations learned by multi-view reconstruction networks. GMAC models extrinsics as global variables constrained by the latent multi-view geometric structure and prunes and structurally reconfigures existing networks so that their latent features can directly support extrinsic prediction through a lightweight regression head, without requiring a completely new network design. Furthermore, GMAC jointly optimizes cross-view reprojection consistency and multi-view cycle consistency, ensuring geometric coherence across cameras while improving prediction accuracy and optimization stability. Experiments on both synthetic and real-world multi-camera datasets demonstrate that GMAC achieves accurate and stable extrinsic estimation without explicit 3D reconstruction or manual calibration, providing a new solution for efficient deployment and online calibration of multi-camera systems.

</details>


### [435] [PDE-Constrained Optimization for Neural Image Segmentation with Physics Priors](https://arxiv.org/abs/2602.01069)
*Seema K. Poudel,Sunny K. Khadka*

Main category: cs.CV

Relevance: 25.0

TL;DR: 该论文提出了一种基于PDE约束优化的图像分割方法，将物理先验融入深度学习模型，在显微镜图像分割任务中相比无约束基线模型取得了更好的准确性和泛化能力。


<details>
  <summary>Details</summary>
Motivation: 显微镜图像分割面临测量噪声、弱边界和有限标注数据等挑战，无约束的深度学习方法容易产生不稳定解和泛化能力差的问题。需要将物理先验整合到深度学习框架中。

Method: 将图像分割建模为PDE约束优化问题，结合数据保真项和基于反应-扩散方程、相场界面能量的正则化项，构建可微分的残差损失函数。在LIVECell数据集上进行训练和评估。

Result: 相比无约束的UNet基线模型，PDE正则化模型在分割准确性和边界保真度方面均取得一致改进，在低样本情况下表现出更好的稳定性和泛化能力。

Conclusion: PDE约束优化为数据驱动学习框架提供了原则性桥梁，展示了将变分方法、统计学习和科学机器学习相结合的优势。

Abstract: Segmentation of microscopy images constitutes an ill-posed inverse problem due to measurement noise, weak object boundaries, and limited labeled data. Although deep neural networks provide flexible nonparametric estimators, unconstrained empirical risk minimization often leads to unstable solutions and poor generalization. In this work, image segmentation is formulated as a PDE-constrained optimization problem that integrates physically motivated priors into deep learning models through variational regularization. The proposed framework minimizes a composite objective function consisting of a data fidelity term and penalty terms derived from reaction-diffusion equations and phase-field interface energies, all implemented as differentiable residual losses. Experiments are conducted on the LIVECell dataset, a high-quality, manually annotated collection of phase-contrast microscopy images. Training is performed on two cell types, while evaluation is carried out on a distinct, unseen cell type to assess generalization. A UNet architecture is used as the unconstrained baseline model. Experimental results demonstrate consistent improvements in segmentation accuracy and boundary fidelity compared to unconstrained deep learning baselines. Moreover, the PDE-regularized models exhibit enhanced stability and improved generalization in low-sample regimes, highlighting the advantages of incorporating structured priors. The proposed approach illustrates how PDE-constrained optimization can strengthen data-driven learning frameworks, providing a principled bridge between variational methods, statistical learning, and scientific machine learning.

</details>


### [436] [LightCity: An Urban Dataset for Outdoor Inverse Rendering and Reconstruction under Multi-illumination Conditions](https://arxiv.org/abs/2602.01118)
*Jingjing Wang,Qirui Hu,Chong Bao,Yuke Zhu,Hujun Bao,Zhaopeng Cui,Guofeng Zhang*

Main category: cs.CV

Relevance: 25.0

TL;DR: LightCity是一个高质量合成城市数据集，用于解决城市场景逆渲染中的复杂光照挑战，包含多种光照条件、视角和丰富属性，并用于基准测试三个基本任务。


<details>
  <summary>Details</summary>
Motivation: 城市场景逆渲染在自动驾驶和数字孪生中很重要，但面临多光源、间接光和阴影等复杂光照条件的挑战。由于缺乏合适的数据集，这些挑战对内在分解和3D重建的影响尚未被探索。

Method: 提出了LightCity数据集，包含300多个高度可控光照的天空图，涵盖街道级和航拍视角的5万多张图像，提供深度、法线、材质组件、直接光和间接光等丰富属性。

Result: 创建了一个高质量合成城市数据集，利用该数据集对城市环境中的三个基本任务进行基准测试，并进行了全面分析。

Conclusion: LightCity为推进城市场景逆渲染相关研究奠定了坚实基础，解决了复杂光照条件下缺乏适当数据集的问题。

Abstract: Inverse rendering in urban scenes is pivotal for applications like autonomous driving and digital twins. Yet, it faces significant challenges due to complex illumination conditions, including multi-illumination and indirect light and shadow effects. However, the effects of these challenges on intrinsic decomposition and 3D reconstruction have not been explored due to the lack of appropriate datasets. In this paper, we present LightCity, a novel high-quality synthetic urban dataset featuring diverse illumination conditions with realistic indirect light and shadow effects. LightCity encompasses over 300 sky maps with highly controllable illumination, varying scales with street-level and aerial perspectives over 50K images, and rich properties such as depth, normal, material components, light and indirect light, etc. Besides, we leverage LightCity to benchmark three fundamental tasks in the urban environments and conduct a comprehensive analysis of these benchmarks, laying a robust foundation for advancing related research.

</details>


### [437] [Stronger Semantic Encoders Can Harm Relighting Performance: Probing Visual Priors via Augmented Latent Intrinsics](https://arxiv.org/abs/2602.01391)
*Xiaoyan Xing,Xiao Zhang,Sezer Karaoglu,Theo Gevers,Anand Bhattad*

Main category: cs.CV

Relevance: 25.0

TL;DR: 该论文提出了增强潜在内在表示（ALI）方法，通过融合像素对齐的视觉编码器特征来平衡语义抽象和光度保真度，显著提升了图像重光照质量，特别是在复杂材质上。


<details>
  <summary>Details</summary>
Motivation: 图像到图像重光照需要解耦场景属性和光照的表示。现有基于潜在内在表示的方法在金属和玻璃等挑战性材质上表现不佳。研究发现，高性能语义编码器的特征反而会降低重光照质量，揭示了语义抽象与光度保真度之间的基本权衡。

Method: 提出增强潜在内在表示（ALI）方法：1）将像素对齐视觉编码器的特征融合到潜在内在框架中，平衡语义上下文和密集光度结构；2）采用自监督细化策略缓解真实世界配对数据稀缺问题；3）仅使用未标记的真实世界图像对进行训练。

Result: ALI在重光照任务上取得了显著改进，特别是在复杂、高光材质上获得最大提升。与像素对齐的视觉先验结合，该方法在真实世界图像对上表现出色。

Conclusion: 该研究揭示了视觉表示中语义抽象与光度保真度的基本权衡，提出的ALI方法通过融合像素对齐特征有效解决了这一权衡，在图像重光照任务上取得了实质性进展。

Abstract: Image-to-image relighting requires representations that disentangle scene properties from illumination. Recent methods rely on latent intrinsic representations but remain under-constrained and often fail on challenging materials such as metal and glass. A natural hypothesis is that stronger pretrained visual priors should resolve these failures. We find the opposite: features from top-performing semantic encoders often degrade relighting quality, revealing a fundamental trade-off between semantic abstraction and photometric fidelity. We study this trade-off and introduce Augmented Latent Intrinsics (ALI), which balances semantic context and dense photometric structure by fusing features from a pixel-aligned visual encoder into a latent-intrinsic framework, together with a self-supervised refinement strategy to mitigate the scarcity of paired real-world data. Trained only on unlabeled real-world image pairs and paired with a dense, pixel-aligned visual prior, ALI achieves strong improvements in relighting, with the largest gains on complex, specular materials. Project page: https:\\augmented-latent-intrinsics.github.io

</details>


### [438] [BioTamperNet: Affinity-Guided State-Space Model Detecting Tampered Biomedical Images](https://arxiv.org/abs/2602.01435)
*Soumyaroop Nandi,Prem Natarajan*

Main category: cs.CV

Relevance: 25.0

TL;DR: BioTamperNet是一个用于检测生物医学图像中重复篡改区域的新框架，采用受状态空间模型启发的亲和力引导注意力机制，在生物医学图像取证任务上显著优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有取证模型主要在自然图像上训练，在生物医学图像上表现不佳，而生物医学图像中的细微篡改可能影响实验有效性，需要专门针对生物医学图像的篡改检测方法。

Method: 提出亲和力引导自注意力模块捕捉图像内相似性，亲和力引导交叉注意力模块建模图像间对应关系，集成轻量级SSM启发的线性注意力机制实现高效细粒度定位，端到端训练同时识别篡改区域及其来源。

Result: 在基准生物医学取证数据集上的广泛实验表明，BioTamperNet在准确检测重复区域方面显著优于竞争基线方法。

Conclusion: BioTamperNet通过SSM启发的亲和力引导注意力机制，为生物医学图像篡改检测提供了有效的解决方案，在检测精度和效率方面都有显著提升。

Abstract: We propose BioTamperNet, a novel framework for detecting duplicated regions in tampered biomedical images, leveraging affinity-guided attention inspired by State Space Model (SSM) approximations. Existing forensic models, primarily trained on natural images, often underperform on biomedical data where subtle manipulations can compromise experimental validity. To address this, BioTamperNet introduces an affinity-guided self-attention module to capture intra-image similarities and an affinity-guided cross-attention module to model cross-image correspondences. Our design integrates lightweight SSM-inspired linear attention mechanisms to enable efficient, fine-grained localization. Trained end-to-end, BioTamperNet simultaneously identifies tampered regions and their source counterparts. Extensive experiments on the benchmark bio-forensic datasets demonstrate significant improvements over competitive baselines in accurately detecting duplicated regions. Code - https://github.com/SoumyaroopNandi/BioTamperNet

</details>


### [439] [One-Step Diffusion for Perceptual Image Compression](https://arxiv.org/abs/2602.01570)
*Yiwen Jia,Hao Wei,Yanhui Zhou,Chenyang Ge*

Main category: cs.CV

Relevance: 25.0

TL;DR: 提出一种单步扩散的图像压缩方法，显著提升推理速度，同时通过特征级判别器保持感知质量


<details>
  <summary>Details</summary>
Motivation: 基于扩散的图像压缩方法在低比特率下能提供高感知质量，但实际部署受到推理延迟和计算开销的阻碍，主要原因是解码时需要大量去噪步骤

Method: 提出仅需单步扩散过程的图像压缩方法；引入在紧凑特征表示上操作的判别器，而非原始像素，利用特征更好地捕捉高级纹理和结构细节

Result: 实验结果显示，该方法在提供可比压缩性能的同时，相比最近的扩散方法实现了46倍的推理速度提升

Conclusion: 该方法有效解决了扩散图像压缩的推理效率问题，为实际部署提供了可行方案

Abstract: Diffusion-based image compression methods have achieved notable progress, delivering high perceptual quality at low bitrates. However, their practical deployment is hindered by significant inference latency and heavy computational overhead, primarily due to the large number of denoising steps required during decoding. To address this problem, we propose a diffusion-based image compression method that requires only a single-step diffusion process, significantly improving inference speed. To enhance the perceptual quality of reconstructed images, we introduce a discriminator that operates on compact feature representations instead of raw pixels, leveraging the fact that features better capture high-level texture and structural details. Experimental results show that our method delivers comparable compression performance while offering a 46$\times$ faster inference speed compared to recent diffusion-based approaches. The source code and models are available at https://github.com/cheesejiang/OSDiff.

</details>


### [440] [Real-Time Loop Closure Detection in Visual SLAM via NetVLAD and Faiss](https://arxiv.org/abs/2602.01673)
*Enguang Fan*

Main category: cs.CV

Relevance: 25.0

TL;DR: 论文评估NetVLAD作为SLAM中闭环检测模块的性能，与DBoW在KITTI数据集上对比，通过Faiss加速实现实时查询，提出细粒度Top-K精度召回曲线更好地反映LCD场景。


<details>
  <summary>Details</summary>
Motivation: 传统词袋方法如DBoW在SLAM闭环检测中效率高但易受外观变化和感知混淆影响，而深度学习视觉位置识别描述符如NetVLAD虽更鲁棒但计算成本高，阻碍实时SLAM应用。本文旨在实证评估NetVLAD作为LCD模块的可行性。

Method: 1) 在KITTI数据集上评估NetVLAD作为LCD模块；2) 与DBoW方法对比；3) 提出细粒度Top-K精度召回曲线，更好处理查询可能无匹配或多匹配的LCD场景；4) 使用Faiss加速最近邻搜索实现实时查询。

Result: NetVLAD通过Faiss加速实现实时查询速度，同时在精度和鲁棒性上优于DBoW，成为SLAM中LCD的实用替代方案。

Conclusion: 深度学习视觉位置识别描述符如NetVLAD可以作为SLAM中闭环检测的实用替代方案，通过适当优化（如Faiss加速）可实现实时性能同时提升精度和鲁棒性。

Abstract: Loop closure detection (LCD) is a core component of simultaneous localization and mapping (SLAM): it identifies revisited places and enables pose-graph constraints that correct accumulated drift. Classic bag-of-words approaches such as DBoW are efficient but often degrade under appearance change and perceptual aliasing. In parallel, deep learning-based visual place recognition (VPR) descriptors (e.g., NetVLAD and Transformer-based models) offer stronger robustness, but their computational cost is often viewed as a barrier to real-time SLAM. In this paper, we empirically evaluate NetVLAD as an LCD module and compare it against DBoW on the KITTI dataset. We introduce a Fine-Grained Top-K precision-recall curve that better reflects LCD settings where a query may have zero or multiple valid matches. With Faiss-accelerated nearestneighbor search, NetVLAD achieves real-time query speed while improving accuracy and robustness over DBoW, making it a practical drop-in alternative for LCD in SLAM.

</details>


### [441] [Physics Informed Generative AI Enabling Labour Free Segmentation For Microscopy Analysis](https://arxiv.org/abs/2602.01710)
*Salma Zahran,Zhou Ao,Zhengyang Zhang,Chen Chi,Chenchen Yuan,Yanming Wang*

Main category: cs.CV

Relevance: 25.0

TL;DR: 提出基于物理模拟和CycleGAN的图像生成框架，将模拟数据转化为真实感SEM图像，用于材料微观结构分割，无需人工标注


<details>
  <summary>Details</summary>
Motivation: 解决材料科学中显微镜图像分割任务面临的人工标注成本高、主观性强、数据稀缺的问题，同时克服传统基于模拟数据训练的模型因域差距而泛化能力差的问题

Method: 1) 使用相场模拟生成大量微观结构形态和完美标注掩码；2) 采用CycleGAN进行无配对图像到图像转换，将干净模拟数据转化为高保真真实SEM图像；3) 在合成数据上训练U-Net分割模型

Result: 仅使用合成数据训练的U-Net在未见实验图像上表现出优异泛化能力：平均边界F1分数0.90，交并比0.88。t-SNE和香农熵分析证实合成图像在统计和特征上与真实数据无法区分

Conclusion: 该生成框架将数据稀缺问题转化为数据丰富问题，完全解耦模型训练与人工标注，为加速材料发现和分析提供了鲁棒的全自动解决方案

Abstract: Semantic segmentation of microscopy images is a critical task for high-throughput materials characterisation, yet its automation is severely constrained by the prohibitive cost, subjectivity, and scarcity of expert-annotated data. While physics-based simulations offer a scalable alternative to manual labelling, models trained on such data historically fail to generalise due to a significant domain gap, lacking the complex textures, noise patterns, and imaging artefacts inherent to experimental data. This paper introduces a novel framework for labour-free segmentation that successfully bridges this simulation-to-reality gap. Our pipeline leverages phase-field simulations to generate an abundant source of microstructural morphologies with perfect, intrinsically-derived ground-truth masks. We then employ a Cycle-Consistent Generative Adversarial Network (CycleGAN) for unpaired image-to-image translation, transforming the clean simulations into a large-scale dataset of high-fidelity, realistic SEM images. A U-Net model, trained exclusively on this synthetic data, demonstrated remarkable generalisation when deployed on unseen experimental images, achieving a mean Boundary F1-Score of 0.90 and an Intersection over Union (IOU) of 0.88. Comprehensive validation using t-SNE feature-space projection and Shannon entropy analysis confirms that our synthetic images are statistically and featurally indistinguishable from the real data manifold. By completely decoupling model training from manual annotation, our generative framework transforms a data-scarce problem into one of data abundance, providing a robust and fully automated solution to accelerate materials discovery and analysis.

</details>


### [442] [MagicFuse: Single Image Fusion for Visual and Semantic Reinforcement](https://arxiv.org/abs/2602.01760)
*Hao Zhang,Yanping Zha,Zizhuo Li,Meiqi Gong,Jiayi Ma*

Main category: cs.CV

Relevance: 25.0

TL;DR: MagicFuse：一种基于扩散模型的单图像融合框架，能够在仅使用单张低质量可见光图像的情况下，生成跨光谱场景表示，性能媲美多模态融合方法。


<details>
  <summary>Details</summary>
Motivation: 解决在恶劣条件下只有可见光传感器可用时，如何继续受益于多模态图像融合优势的实际问题。将传统数据级融合扩展到知识级融合。

Method: 提出MagicFuse框架，包含三个分支：1) 基于扩散模型的谱内知识增强分支，挖掘可见光谱中被遮挡的场景信息；2) 跨光谱知识生成分支，学习转移到红外光谱的热辐射分布模式；3) 多域知识融合分支，整合两个分支的扩散流概率噪声，通过连续采样获得跨光谱场景表示。同时施加视觉和语义约束。

Result: 实验表明，MagicFuse仅依赖单张退化可见光图像，就能实现与最先进多模态输入融合方法相当甚至更好的视觉和语义表示性能。

Conclusion: 提出的单图像融合概念和MagicFuse框架成功将数据级融合扩展到知识级，在仅有可见光图像的情况下实现了有效的跨光谱场景表示，具有重要的实际应用价值。

Abstract: This paper focuses on a highly practical scenario: how to continue benefiting from the advantages of multi-modal image fusion under harsh conditions when only visible imaging sensors are available. To achieve this goal, we propose a novel concept of single-image fusion, which extends conventional data-level fusion to the knowledge level. Specifically, we develop MagicFuse, a novel single image fusion framework capable of deriving a comprehensive cross-spectral scene representation from a single low-quality visible image. MagicFuse first introduces an intra-spectral knowledge reinforcement branch and a cross-spectral knowledge generation branch based on the diffusion models. They mine scene information obscured in the visible spectrum and learn thermal radiation distribution patterns transferred to the infrared spectrum, respectively. Building on them, we design a multi-domain knowledge fusion branch that integrates the probabilistic noise from the diffusion streams of these two branches, from which a cross-spectral scene representation can be obtained through successive sampling. Then, we impose both visual and semantic constraints to ensure that this scene representation can satisfy human observation while supporting downstream semantic decision-making. Extensive experiments show that our MagicFuse achieves visual and semantic representation performance comparable to or even better than state-of-the-art fusion methods with multi-modal inputs, despite relying solely on a single degraded visible image.

</details>


### [443] [LDRNet: Large Deformation Registration Model for Chest CT Registration](https://arxiv.org/abs/2602.01812)
*Cheng Wang,Qiyu Gao,Fandong Zhang,Shu Zhang,Yizhou Yu*

Main category: cs.CV

Relevance: 25.0

TL;DR: 提出LDRNet用于胸部CT图像的大变形配准，通过粗到细的配准场优化和刚性变换学习，在速度和精度上优于现有方法


<details>
  <summary>Details</summary>
Motivation: 现有深度学习医学图像配准方法主要针对脑部图像，而胸部CT配准面临更大变形、更复杂背景和区域重叠的挑战，需要专门的大变形配准方法

Method: 提出LDRNet：1）先预测粗分辨率配准场，然后从粗到细进行细化；2）设计细化块在不同分辨率下优化配准场；3）设计刚性块从高层特征学习变换矩阵

Result: 在私有数据集和公开数据集SegTHOR上评估，相比传统方法和深度学习模型VoxelMorph、RCN、LapIRN，实现了最先进的大变形图像配准性能，且速度更快

Conclusion: LDRNet能够有效处理胸部CT图像的大变形配准问题，在精度和效率上都优于现有方法，为医学图像分析提供了有效的解决方案

Abstract: Most of the deep learning based medical image registration algorithms focus on brain image registration tasks.Compared with brain registration, the chest CT registration has larger deformation, more complex background and region over-lap. In this paper, we propose a fast unsupervised deep learning method, LDRNet, for large deformation image registration of chest CT images. We first predict a coarse resolution registration field, then refine it from coarse to fine. We propose two innovative technical components: 1) a refine block that is used to refine the registration field in different resolutions, 2) a rigid block that is used to learn transformation matrix from high-level features. We train and evaluate our model on the private dataset and public dataset SegTHOR. We compare our performance with state-of-the-art traditional registration methods as well as deep learning registration models VoxelMorph, RCN, and LapIRN. The results demonstrate that our model achieves state-of-the-art performance for large deformation images registration and is much faster.

</details>


### [444] [CloDS: Visual-Only Unsupervised Cloth Dynamics Learning in Unknown Conditions](https://arxiv.org/abs/2602.01844)
*Yuliang Zhan,Jian Li,Wenbing Huang,Wenbing Huang,Yang Liu,Hao Sun*

Main category: cs.CV

Relevance: 25.0

TL;DR: 提出Cloth Dynamics Grounding (CDG)场景和Cloth Dynamics Splatting (CloDS)框架，用于从多视角视觉观测中无监督学习布料动力学，无需已知物理属性作为监督。


<details>
  <summary>Details</summary>
Motivation: 现有深度学习模拟动态系统的方法需要已知物理属性作为监督或输入，这在未知条件下限制了应用。为了解决这一问题，作者提出了无监督学习布料动力学的新场景。

Method: CloDS采用三阶段流程：1) 视频到几何的grounding；2) 在grounded网格上训练动力学模型。为处理大非线性变形和严重自遮挡，提出双位置不透明度调制，通过基于网格的高斯溅射实现2D观测和3D几何之间的双向映射。

Result: 综合实验评估表明，CloDS能够有效地从视觉数据中学习布料动力学，同时对未见配置保持强大的泛化能力。

Conclusion: 该工作提出了无监督学习布料动力学的新框架，解决了现有方法需要已知物理属性的限制，在未知条件下具有更好的适用性。

Abstract: Deep learning has demonstrated remarkable capabilities in simulating complex dynamic systems. However, existing methods require known physical properties as supervision or inputs, limiting their applicability under unknown conditions. To explore this challenge, we introduce Cloth Dynamics Grounding (CDG), a novel scenario for unsupervised learning of cloth dynamics from multi-view visual observations. We further propose Cloth Dynamics Splatting (CloDS), an unsupervised dynamic learning framework designed for CDG. CloDS adopts a three-stage pipeline that first performs video-to-geometry grounding and then trains a dynamics model on the grounded meshes. To cope with large non-linear deformations and severe self-occlusions during grounding, we introduce a dual-position opacity modulation that supports bidirectional mapping between 2D observations and 3D geometry via mesh-based Gaussian splatting in video-to-geometry grounding stage. It jointly considers the absolute and relative position of Gaussian components. Comprehensive experimental evaluations demonstrate that CloDS effectively learns cloth dynamics from visual data while maintaining strong generalization capabilities for unseen configurations. Our code is available at https://github.com/whynot-zyl/CloDS. Visualization results are available at https://github.com/whynot-zyl/CloDS_video}.%\footnote{As in this example.

</details>


### [445] [Multi-View Stenosis Classification Leveraging Transformer-Based Multiple-Instance Learning Using Real-World Clinical Data](https://arxiv.org/abs/2602.02067)
*Nikola Cenikj,Özgün Turgut,Alexander Müller,Alexander Steger,Jan Kehrer,Marcus Brugger,Daniel Rueckert,Eimo Martens,Philip Müller*

Main category: cs.CV

Relevance: 25.0

TL;DR: SegmentMIL：基于Transformer的多视角多示例学习框架，用于患者级别的冠状动脉狭窄分类，无需视角级标注，仅使用患者级监督即可同时预测狭窄存在并定位受影响解剖区域。


<details>
  <summary>Details</summary>
Motivation: 冠状动脉狭窄是心血管疾病的主要原因，通常需要从多个血管造影视角进行分析。现有深度学习模型依赖昂贵的视角级标注，且无法捕捉多视角间的时序动态和依赖关系，这些在临床诊断中至关重要。

Method: 提出SegmentMIL，一个基于Transformer的多视角多示例学习框架。该方法仅使用患者级监督（无需视角级标注），联合预测狭窄存在并定位受影响解剖区域（区分左右冠状动脉及其分段）。

Result: 在内部和外部评估中均获得高性能，优于视角级模型和经典MIL基线，展示了其作为临床可行且可扩展解决方案的潜力。

Conclusion: SegmentMIL为冠状动脉狭窄诊断提供了一种无需昂贵视角级标注的临床可行方案，能够有效利用多视角信息进行患者级分类和定位。

Abstract: Coronary artery stenosis is a leading cause of cardiovascular disease, diagnosed by analyzing the coronary arteries from multiple angiography views. Although numerous deep-learning models have been proposed for stenosis detection from a single angiography view, their performance heavily relies on expensive view-level annotations, which are often not readily available in hospital systems. Moreover, these models fail to capture the temporal dynamics and dependencies among multiple views, which are crucial for clinical diagnosis. To address this, we propose SegmentMIL, a transformer-based multi-view multiple-instance learning framework for patient-level stenosis classification. Trained on a real-world clinical dataset, using patient-level supervision and without any view-level annotations, SegmentMIL jointly predicts the presence of stenosis and localizes the affected anatomical region, distinguishing between the right and left coronary arteries and their respective segments. SegmentMIL obtains high performance on internal and external evaluations and outperforms both view-level models and classical MIL baselines, underscoring its potential as a clinically viable and scalable solution for coronary stenosis diagnosis. Our code is available at https://github.com/NikolaCenic/mil-stenosis.

</details>


### [446] [MLV-Edit: Towards Consistent and Highly Efficient Editing for Minute-Level Videos](https://arxiv.org/abs/2602.02123)
*Yangyi Cao,Yuanhang Li,Lan Chen,Qi Mao*

Main category: cs.CV

Relevance: 25.0

TL;DR: MLV-Edit是一个无需训练、基于光流的分钟级视频编辑框架，通过分而治之策略解决长视频编辑中的计算开销和时间一致性难题。


<details>
  <summary>Details</summary>
Motivation: 现有视频编辑技术擅长短视频处理，但扩展到分钟级长视频面临两大挑战：1) 计算开销巨大（数千帧处理）；2) 难以保持全局时间一致性。需要解决分段编辑中的边界伪影和累积结构漂移问题。

Method: 采用无需训练的分而治之策略，核心包含两个模块：1) Velocity Blend：通过对齐相邻片段的光流场来校正边界运动不一致性，消除闪烁和边界伪影；2) Attention Sink：将局部片段特征锚定到全局参考帧，抑制累积结构漂移。

Result: 大量定量和定性实验表明，MLV-Edit在时间稳定性和语义保真度方面持续优于最先进方法。

Conclusion: MLV-Edit为分钟级长视频编辑提供了一个高效、稳定的解决方案，通过创新的光流对齐和注意力锚定机制，成功解决了长视频编辑中的关键挑战。

Abstract: We propose MLV-Edit, a training-free, flow-based framework that address the unique challenges of minute-level video editing. While existing techniques excel in short-form video manipulation, scaling them to long-duration videos remains challenging due to prohibitive computational overhead and the difficulty of maintaining global temporal consistency across thousands of frames. To address this, MLV-Edit employs a divide-and-conquer strategy for segment-wise editing, facilitated by two core modules: Velocity Blend rectifies motion inconsistencies at segment boundaries by aligning the flow fields of adjacent chunks, eliminating flickering and boundary artifacts commonly observed in fragmented video processing; and Attention Sink anchors local segment features to global reference frames, effectively suppressing cumulative structural drift. Extensive quantitative and qualitative experiments demonstrate that MLV-Edit consistently outperforms state-of-the-art methods in terms of temporal stability and semantic fidelity.

</details>


### [447] [Eliminating Registration Bias in Synthetic CT Generation: A Physics-Based Simulation Framework](https://arxiv.org/abs/2602.02130)
*Lukas Zimmermann,Michael Rauter,Maximilian Schmid,Dietmar Georg,Barbara Knäusl*

Main category: cs.CV

Relevance: 25.0

TL;DR: 该论文提出使用基于物理的CBCT模拟生成几何对齐的训练数据对，结合几何对齐指标评估，解决了传统监督学习因配准偏差导致的评估失真问题。


<details>
  <summary>Details</summary>
Motivation: 传统从CBCT生成合成CT的监督学习方法需要配准的训练对，但完美配准无法实现。配准偏差会传播到训练模型中，并污染标准评估指标，导致更好的基准性能可能只是更好地复制了配准伪影而非解剖保真度。

Method: 提出基于物理的CBCT模拟方法，通过构造提供几何对齐的训练对，结合使用几何对齐指标（如归一化互信息）对输入CBCT而非有偏差的ground truth进行评估。

Result: 在两个独立的盆腔数据集上，使用合成数据训练的模型实现了更好的几何对齐（归一化互信息：0.31 vs 0.22），尽管传统强度分数较低。几何对齐指标与临床评估一致，临床观察者在87%的病例中偏好合成训练的输出。

Conclusion: 几何保真度而非与有偏差ground truth的强度一致性，与临床需求相符。基于物理的模拟和几何对齐评估提供了更可靠的训练和评估框架。

Abstract: Supervised synthetic CT generation from CBCT requires registered training pairs, yet perfect registration between separately acquired scans remains unattainable. This registration bias propagates into trained models and corrupts standard evaluation metrics. This may suggest that superior benchmark performance indicates better reproduction of registration artifacts rather than anatomical fidelity. We propose physics-based CBCT simulation to provide geometrically aligned training pairs by construction, combined with evaluation using geometric alignment metrics against input CBCT rather than biased ground truth. On two independent pelvic datasets, models trained on synthetic data achieved superior geometric alignment (Normalized Mutual Information: 0.31 vs 0.22) despite lower conventional intensity scores. Intensity metrics showed inverted correlations with clinical assessment for deformably registered data, while Normalized Mutual Information consistently predicted observer preference across registration methodologies (rho = 0.31, p < 0.001). Clinical observers preferred synthetic-trained outputs in 87% of cases, demonstrating that geometric fidelity, not intensity agreement with biased ground truth, aligns with clinical requirements.

</details>


### [448] [Deep learning enables urban change profiling through alignment of historical maps](https://arxiv.org/abs/2602.02154)
*Sidi Wu,Yizi Chen,Maurizio Gribaudi,Konrad Schindler,Clément Mallet,Julien Perret,Lorenz Hurni*

Main category: cs.CV

Relevance: 25.0

TL;DR: 提出一个基于深度学习的全自动框架，用于从历史地图中提取细粒度城市变化信息，通过模块化设计整合密集地图对齐、多时相目标检测和变化分析


<details>
  <summary>Details</summary>
Motivation: 历史地图提供了城市长期转型的独特记录，但由于空间错位、制图变化和文档质量退化等问题，从历史地图系列中提取一致且细粒度的变化信息仍然具有挑战性，限制了大多数分析只能采用小规模或定性方法

Method: 基于深度学习的全自动框架，采用模块化设计，整合三个核心组件：1) 密集地图对齐，2) 多时相目标检测，3) 变化分析。该框架将历史地图分析从临时视觉比较转向系统化、定量化的城市变化表征

Result: 实验证明所提出的对齐和目标检测方法具有鲁棒性能。应用于1868年至1937年的巴黎，该框架揭示了城市转型的空间和时间异质性，突显了其在社会科学和人文学科研究中的相关性

Conclusion: 该框架的模块化设计支持适应不同的制图背景和下游应用，为历史地图分析提供了从临时视觉比较到系统化定量表征的转变

Abstract: Prior to modern Earth observation technologies, historical maps provide a unique record of long-term urban transformation and offer a lens on the evolving identity of cities. However, extracting consistent and fine-grained change information from historical map series remains challenging due to spatial misalignment, cartographic variation, and degrading document quality, limiting most analyses to small-scale or qualitative approaches. We propose a fully automated, deep learning-based framework for fine-grained urban change analysis from large collections of historical maps, built on a modular design that integrates dense map alignment, multi-temporal object detection, and change profiling. This framework shifts the analysis of historical maps from ad hoc visual comparison toward systematic, quantitative characterization of urban change. Experiments demonstrate the robust performance of the proposed alignment and object detection methods. Applied to Paris between 1868 and 1937, the framework reveals the spatial and temporal heterogeneity in urban transformation, highlighting its relevance for research in the social sciences and humanities. The modular design of our framework further supports adaptation to diverse cartographic contexts and downstream applications.

</details>


### [449] [LiFlow: Flow Matching for 3D LiDAR Scene Completion](https://arxiv.org/abs/2602.02232)
*Andrea Matteazzi,Dietmar Tutsch*

Main category: cs.CV

Relevance: 25.0

TL;DR: LiFlow：首个基于流匹配的3D LiDAR场景补全框架，通过改进扩散模型解决训练与推理初始分布不匹配问题，在自动驾驶场景中实现最先进的点云补全性能。


<details>
  <summary>Details</summary>
Motivation: 自动驾驶场景中采集的LiDAR点云常受遮挡和远距离稀疏性影响，限制了感知系统性能。现有基于扩散模型的方法存在训练与推理初始分布不匹配问题，导致性能受限。

Method: 提出首个流匹配框架用于3D LiDAR场景补全，采用最近邻流匹配损失和Chamfer距离损失，同时优化局部结构和全局覆盖，确保训练与推理初始分布一致性。

Result: LiFlow在多个指标上达到最先进性能，代码已开源。

Conclusion: 流匹配框架有效解决了扩散模型在3D场景补全中的分布不匹配问题，为自动驾驶感知系统提供了更可靠的场景补全方案。

Abstract: In autonomous driving scenarios, the collected LiDAR point clouds can be challenged by occlusion and long-range sparsity, limiting the perception of autonomous driving systems. Scene completion methods can infer the missing parts of incomplete 3D LiDAR scenes. Recent methods adopt local point-level denoising diffusion probabilistic models, which require predicting Gaussian noise, leading to a mismatch between training and inference initial distributions. This paper introduces the first flow matching framework for 3D LiDAR scene completion, improving upon diffusion-based methods by ensuring consistent initial distributions between training and inference. The model employs a nearest neighbor flow matching loss and a Chamfer distance loss to enhance both local structure and global coverage in the alignment of point clouds. LiFlow achieves state-of-the-art performance across multiple metrics. Code: https://github.com/matteandre/LiFlow.

</details>


### [450] [NAB: Neural Adaptive Binning for Sparse-View CT reconstruction](https://arxiv.org/abs/2602.02356)
*Wangduo Xie,Matthew B. Blaschko*

Main category: cs.CV

Relevance: 25.0

TL;DR: 提出神经自适应分箱（NAB）方法，将矩形先验整合到稀疏视图CT重建中，通过可学习的参数化分箱机制提升工业对象重建质量。


<details>
  <summary>Details</summary>
Motivation: 工业CT中稀疏视图重建对降低成本至关重要，现有隐式神经网络方法无法有效利用工业对象常见的矩形结构先验。

Method: 提出神经自适应分箱（NAB）：1）将坐标空间映射到分箱向量空间；2）基于移位双曲正切函数差值的创新分箱机制；3）支持绕输入平面法向量的旋转扩展；4）神经网络预测CT衰减系数；5）端到端优化编码参数（位置、大小、陡度、旋转）。

Result: 在两个工业数据集上取得优越性能，通过调整分箱函数平滑度可泛化到更复杂几何形状，在医学数据集上也保持鲁棒性。

Conclusion: 为将形状先验整合到神经网络重建提供了新视角，NAB方法能有效利用矩形先验提升稀疏视图CT重建质量。

Abstract: Computed Tomography (CT) plays a vital role in inspecting the internal structures of industrial objects. Furthermore, achieving high-quality CT reconstruction from sparse views is essential for reducing production costs. While classic implicit neural networks have shown promising results for sparse reconstruction, they are unable to leverage shape priors of objects. Motivated by the observation that numerous industrial objects exhibit rectangular structures, we propose a novel \textbf{N}eural \textbf{A}daptive \textbf{B}inning (\textbf{NAB}) method that effectively integrates rectangular priors into the reconstruction process. Specifically, our approach first maps coordinate space into a binned vector space. This mapping relies on an innovative binning mechanism based on differences between shifted hyperbolic tangent functions, with our extension enabling rotations around the input-plane normal vector. The resulting representations are then processed by a neural network to predict CT attenuation coefficients. This design enables end-to-end optimization of the encoding parameters -- including position, size, steepness, and rotation -- via gradient flow from the projection data, thus enhancing reconstruction accuracy. By adjusting the smoothness of the binning function, NAB can generalize to objects with more complex geometries. This research provides a new perspective on integrating shape priors into neural network-based reconstruction. Extensive experiments demonstrate that NAB achieves superior performance on two industrial datasets. It also maintains robust on medical datasets when the binning function is extended to more general expression. The code will be made available.

</details>


### [451] [Toward a Unified Semantic Loss Model for Deep JSCC-based Transmission of EO Imagery](https://arxiv.org/abs/2602.00136)
*Ti Ti Nguyen,Thanh-Dung Le,Vu Nguyen Ha,Duc-Dung Tran,Hung Nguyen-Kha,Dinh-Hieu Tran,Carlos L. Marcos-Rojas,Juan C. Merlano-Duncan,Symeon Chatzinotas*

Main category: eess.IV

Relevance: 25.0

TL;DR: 该论文研究深度联合信源信道编码(DJSCC)用于地球观测图像传输，分析语义损失的两个方面：重建中心框架和任务导向框架，并提出统一的语义损失框架。


<details>
  <summary>Details</summary>
Motivation: 高分辨率地球观测图像产生大量数据，对带宽有限、功率受限、链路条件动态变化的卫星通信系统构成挑战，需要有效的图像传输方法。

Method: 1) 评估重建中心框架：分析不同压缩比和信道信噪比下的重建图像语义退化；2) 开发任务导向框架：将DJSCC与轻量级应用特定模型（如EfficientViT）集成，以下游任务准确性衡量性能；3) 提出统一的语义损失框架，同时捕捉重建和任务性能。

Result: 通过大量实证分析，提出了一个统一的语义损失框架，揭示了JSCC压缩、信道信噪比和语义质量之间的隐含关系，为资源受限卫星链路下鲁棒高效的EO图像传输设计提供了可行见解。

Conclusion: DJSCC是地球观测图像传输的有效信源信道范式，统一的语义损失框架能够同时优化重建质量和下游任务性能，适用于资源受限的卫星通信环境。

Abstract: Modern Earth Observation (EO) systems increasingly rely on high-resolution imagery to support critical applications such as environmental monitoring, disaster response, and land-use analysis. Although these applications benefit from detailed visual data, the resulting data volumes impose significant challenges on satellite communication systems constrained by limited bandwidth, power, and dynamic link conditions. To address these limitations, this paper investigates Deep Joint Source-Channel Coding (DJSCC) as an effective source-channel paradigm for the transmission of EO imagery. We focus on two complementary aspects of semantic loss in DJSCC-based systems. First, a reconstruction-centric framework is evaluated by analyzing the semantic degradation of reconstructed images under varying compression ratios and channel signal-to-noise ratios (SNR). Second, a task-oriented framework is developed by integrating DJSCC with lightweight, application-specific models (e.g., EfficientViT), with performance measured using downstream task accuracy rather than pixel-level fidelity. Based on extensive empirical analysis, we propose a unified semantic loss framework that captures both reconstruction-centric and task-oriented performance within a single model. This framework characterizes the implicit relationship between JSCC compression, channel SNR, and semantic quality, offering actionable insights for the design of robust and efficient EO imagery transmission under resource-constrained satellite links.

</details>


### [452] [Advanced Geometric Correction Algorithms for 3D Medical Reconstruction: Comparison of Computed Tomography and Macroscopic Imaging](https://arxiv.org/abs/2602.00220)
*Tomasz Les,Tomasz Markiewicz,Malgorzata Lorent,Miroslaw Dziekiewicz,Krzysztof Siwek*

Main category: eess.IV

Relevance: 25.0

TL;DR: 提出了一种用于从宏观切片重建3D肾脏解剖结构的混合两阶段配准框架，结合几何先验和深度学习，解决数据稀缺和大变形挑战。


<details>
  <summary>Details</summary>
Motivation: 宏观成像存在数据稀缺和高失真挑战，完全基于学习的配准方法（如VoxelMorph）由于训练多样性有限和大非刚性变形超出卷积滤波器捕获范围而难以泛化。

Method: 混合两阶段框架：1) OCM算法进行约束全局对齐（平移、旋转、均匀缩放）；2) 轻量级深度学习细化网络预测连续切片间的局部变形残差，采用分层分解配准流形设计。

Result: 在40个肾脏的原始数据集上实验，相比单阶段基线方法获得更好结果，通过Hough网格检测保持物理校准，Bezier轮廓平滑实现鲁棒网格化和体积估计。

Conclusion: 该框架将可解释的全局优化与数据高效的深度细化解耦，提高了多模态3D重建的精度、可重复性和解剖真实性，适用于手术规划、形态评估和医学教育。

Abstract: This paper introduces a hybrid two-stage registration framework for reconstructing three-dimensional (3D) kidney anatomy from macroscopic slices, using CT-derived models as the geometric reference standard. The approach addresses the data-scarcity and high-distortion challenges typical of macroscopic imaging, where fully learning-based registration (e.g., VoxelMorph) often fails to generalize due to limited training diversity and large nonrigid deformations that exceed the capture range of unconstrained convolutional filters. In the proposed pipeline, the Optimal Cross-section Matching (OCM) algorithm first performs constrained global alignment: translation, rotation, and uniform scaling to establish anatomically consistent slice initialization. Next, a lightweight deep-learning refinement network, inspired by VoxelMorph, predicts residual local deformations between consecutive slices. The core novelty of this architecture lies in its hierarchical decomposition of the registration manifold. This hybrid OCM+DL design integrates explicit geometric priors with the flexible learning capacity of neural networks, ensuring stable optimization and plausible deformation fields even with few training examples. Experiments on an original dataset of 40 kidneys demonstrated better results compared to single-stage baselines. The pipeline maintains physical calibration via Hough-based grid detection and employs Bezier-based contour smoothing for robust meshing and volume estimation. Although validated on kidney data, the proposed framework generalizes to other soft-tissue organs reconstructed from optical or photographic cross-sections. By decoupling interpretable global optimization from data-efficient deep refinement, the method advances the precision, reproducibility, and anatomical realism of multimodal 3D reconstructions for surgical planning, morphological assessment, and medical education.

</details>


### [453] [Dual Quaternion SE(3) Synchronization with Recovery Guarantees](https://arxiv.org/abs/2602.00324)
*Jianing Zhao,Linglingzhi Zhu,Anthony Man-Cho So*

Main category: math.OC

Relevance: 25.0

TL;DR: 本文提出了一种基于对偶四元数的SE(3)同步算法，包含谱初始化器和广义幂方法，具有理论保证并在实验中优于基于矩阵的方法。


<details>
  <summary>Details</summary>
Motivation: SE(3)同步是机器人和3D视觉中的核心问题，用于从噪声相对变换恢复绝对位姿。现有方法通常需要多步启发式过程，缺乏理论保证且难以分析。

Method: 采用对偶四元数表示，提出两阶段算法：1) 通过对偶四元数测量矩阵的幂方法计算谱初始化器；2) 对偶四元数广义幂方法(DQGPM)，通过每迭代投影确保可行性。

Result: 建立了谱估计器的误差界，证明DQGPM具有有限迭代误差界，并在明确噪声阈值内实现线性误差收缩。在合成基准和真实多扫描点集配准实验中，该算法在精度和效率上优于代表性基于矩阵的方法。

Conclusion: 提出的对偶四元数SE(3)同步算法提供了理论保证，在精度和效率上优于现有方法，为机器人和3D视觉中的位姿估计问题提供了更可靠的解决方案。

Abstract: Synchronization over the special Euclidean group SE(3) aims to recover absolute poses from noisy pairwise relative transformations and is a core primitive in robotics and 3D vision. Standard approaches often require multi-step heuristic procedures to recover valid poses, which are difficult to analyze and typically lack theoretical guarantees. This paper adopts a dual quaternion representation and formulates SE(3) synchronization directly over the unit dual quaternion. A two-stage algorithm is developed: A spectral initializer computed via the power method on a Hermitian dual quaternion measurement matrix, followed by a dual quaternion generalized power method (DQGPM) that enforces feasibility through per-iteration projection. The estimation error bounds are established for spectral estimators, and DQGPM is shown to admit a finite-iteration error bound and achieves linear error contraction up to an explicit noise-dependent threshold. Experiments on synthetic benchmarks and real-world multi-scan point-set registration demonstrate that the proposed pipeline improves both accuracy and efficiency over representative matrix-based methods.

</details>


### [454] [A 30-item Test for Assessing Chinese Character Amnesia in Child Handwriters](https://arxiv.org/abs/2602.00464)
*Zebo Xu,Steven Langsford,Zhuang Qiu,Zhenguang Cai*

Main category: q-bio.QM

Relevance: 25.0

TL;DR: 开发了一个评估儿童汉字失写症的30项简短测试，通过项目反应理论分析，能够有效识别早期书写困难


<details>
  <summary>Details</summary>
Motivation: 在数字时代，手写能力下降，特别是非字母文字系统（如中文）中，儿童出现"汉字失写症"（能认字但不会写）的现象日益普遍。目前缺乏标准化的诊断工具来评估儿童的汉字失写症。

Method: 使用40名儿童手写800个汉字的大规模数据集，采用双参数项目反应理论模型分析。比较了四种项目选择方案：随机基线、最大区分度、难度多样化和上下三分之一区分度。通过样本外预测评估候选项目子集。

Result: 上下三分之一区分度方法产生了包含30个项目的简短测试，该测试保持了完整的个体差异结构，并能推广到未见过的测试者（交叉验证平均r=0.74，与完整800项测试相比；样本内r=0.93）。

Conclusion: 这个简短测试为评估儿童汉字失写症提供了可靠高效的工具，可用于识别早期书写和正字法学习困难，有助于早期发现发展性书写障碍和相关读写挑战。

Abstract: Handwriting literacy is an important skill for learning and communication in school-age children. In the digital age, handwriting has been largely replaced by typing, leading to a decline in handwriting proficiency, particularly in non-alphabetic writing systems. Among children learning Chinese, a growing number have reported experiencing character amnesia: difficulty in correctly handwriting a character despite being able to recognize it. Given that there is currently no standardized diagnostic tool for assessing character amnesia in children, we developed an assessment to measure Chinese character amnesia in Mandarin-speaking school-age population. We utilised a large-scale handwriting dataset in which 40 children handwrote 800 characters from dictation prompts. Character amnesia and correct handwriting responses were analysed using a two-parameter Item Response Theory model. Four item-selection schemes were compared: random baseline, maximum discrimination, diverse difficulty, and an upper-and-lower-thirds discrimination score. Candidate item subsets were evaluated using out-of-sample prediction. Among these selection schemes, the upper-and-lower-thirds discrimination procedure yields a compact 30-item test that preserves individual-difference structure and generalizes to unseen test-takers (cross-validated mean r =.74 with full 800-item-test; within-sample r =.93). This short-form test provides a reliable and efficient tool of assessing Chinese character amnesia in children and can be used to identify early handwriting and orthographic learning difficulties, contributing to the early detection of developmental dysgraphia and related literacy challenges.

</details>


### [455] [SyNeT: Synthetic Negatives for Traversability Learning](https://arxiv.org/abs/2602.00814)
*Bomena Kim,Hojun Lee,Younsoo Park,Yaoyu Hu,Sebastian Scherer,Inwook Shim*

Main category: cs.RO

Relevance: 25.0

TL;DR: 提出SyNet方法，通过构建合成负样本来增强视觉可通行性估计，解决了现有自监督学习中缺乏明确负样本的问题，提高了模型对非可通行区域的识别能力。


<details>
  <summary>Details</summary>
Motivation: 现有自监督可通行性估计框架主要依赖正样本和未标记数据，缺乏明确的负样本限制了模型准确识别多样化非可通行区域的能力。需要一种方法来明确构建合成负样本，以增强模型对非可通行区域的识别。

Method: 提出SyNet方法，构建合成负样本表示合理但不可通行的区域，并将其集成到基于视觉的可通行性学习中。该方法可无缝集成到PU（正-未标记）和PN（正-负）框架中，无需修改推理架构。同时引入面向对象的FPR评估方法，分析在合成负样本插入区域的预测结果。

Result: 在公开数据集和自收集数据集上的大量实验表明，该方法显著提高了模型在不同环境中的鲁棒性和泛化能力。通过合成负样本的集成，模型能够更准确地识别非可通行区域。

Conclusion: 通过明确构建合成负样本并将其集成到可通行性学习框架中，可以有效解决现有方法缺乏负样本的局限性，提高自主机器人在复杂户外环境中的导航安全性。

Abstract: Reliable traversability estimation is crucial for autonomous robots to navigate complex outdoor environments safely. Existing self-supervised learning frameworks primarily rely on positive and unlabeled data; however, the lack of explicit negative data remains a critical limitation, hindering the model's ability to accurately identify diverse non-traversable regions. To address this issue, we introduce a method to explicitly construct synthetic negatives, representing plausible but non-traversable, and integrate them into vision-based traversability learning. Our approach is formulated as a training strategy that can be seamlessly integrated into both Positive-Unlabeled (PU) and Positive-Negative (PN) frameworks without modifying inference architectures. Complementing standard pixel-wise metrics, we introduce an object-centric FPR evaluation approach that analyzes predictions in regions where synthetic negatives are inserted. This evaluation provides an indirect measure of the model's ability to consistently identify non-traversable regions without additional manual labeling. Extensive experiments on both public and self-collected datasets demonstrate that our approach significantly enhances robustness and generalization across diverse environments. The source code and demonstration videos are publicly available at the project page: https://anonymous-synet.github.io/SyNet.github.io/

</details>


### [456] [Seeing, Hearing, and Knowing Together: Multimodal Strategies in Deepfake Videos Detection](https://arxiv.org/abs/2602.01284)
*Chen Chen,Dion Hoe-Lian Goh*

Main category: cs.MM

Relevance: 25.0

TL;DR: 研究人类识别深度伪造视频的策略，通过195名参与者的实验发现，真实视频识别准确率高于深度伪造视频，视觉、声音和直觉线索的组合对成功识别至关重要，为设计媒体素养工具提供依据。


<details>
  <summary>Details</summary>
Motivation: 随着深度伪造视频越来越难以识别，理解人类使用的识别策略对于设计有效的媒体素养干预措施至关重要。研究旨在探索人们在识别真实与深度伪造视频时依赖的线索和策略。

Method: 对195名21-40岁参与者进行实验研究，让他们判断真实和深度伪造视频，评估自信度，并报告依赖的视觉、音频和知识策略线索。使用关联规则挖掘分析线索组合对识别性能的影响。

Result: 参与者对真实视频的识别准确率高于深度伪造视频，对真实内容的预期校准误差更低。通过关联规则挖掘发现，视觉外观、声音和直觉线索经常共同出现并促成成功识别，强调了多模态方法在人类检测中的重要性。

Conclusion: 研究揭示了哪些线索有助于或阻碍深度伪造检测，为设计媒体素养工具提供了方向，这些工具可以指导有效的线索使用。基于这些见解可以帮助人们提高识别技能，增强对欺骗性数字媒体的抵抗力。

Abstract: As deepfake videos become increasingly difficult for people to recognise, understanding the strategies humans use is key to designing effective media literacy interventions. We conducted a study with 195 participants between the ages of 21 and 40, who judged real and deepfake videos, rated their confidence, and reported the cues they relied on across visual, audio, and knowledge strategies. Participants were more accurate with real videos than with deepfakes and showed lower expected calibration error for real content. Through association rule mining, we identified cue combinations that shaped performance. Visual appearance, vocal, and intuition often co-occurred for successful identifications, which highlights the importance of multimodal approaches in human detection. Our findings show which cues help or hinder detection and suggest directions for designing media literacy tools that guide effective cue use. Building on these insights can help people improve their identification skills and become more resilient to deceptive digital media.

</details>


### [457] [Event Driven Clustering Algorithm](https://arxiv.org/abs/2602.00115)
*David El-Chai Ben-Ezra,Adar Tal,Daniel Brisk*

Main category: cs.CV

Relevance: 20.0

TL;DR: 提出了一种用于事件相机数据中实时检测小事件簇的异步事件驱动算法，基于时空距离进行层次聚类，具有线性复杂度且运行时间与像素阵列维度无关。


<details>
  <summary>Details</summary>
Motivation: 事件相机产生异步、稀疏的事件流数据，传统聚类算法难以高效处理这种数据格式。需要开发专门针对事件相机数据特性的实时检测算法，以支持机器人视觉、自动驾驶等实时应用。

Method: 采用异步事件驱动的层次凝聚聚类方法，利用事件相机的特殊异步数据结构，通过精妙高效的决策机制，基于事件的时空距离进行聚类检测。

Result: 算法实现了O(n)的线性复杂度（n为事件数量），运行时间与像素阵列维度无关，能够实时检测小事件簇，适用于事件相机的实时处理需求。

Conclusion: 该算法为事件相机数据提供了一种高效、实时的聚类检测方案，特别适合处理异步稀疏事件流，在机器人视觉和实时感知系统中具有应用价值。

Abstract: This paper introduces a novel asynchronous, event-driven algorithm for real-time detection of small event clusters in event camera data. Like other hierarchical agglomerative clustering algorithms, the algorithm detects the event clusters based on their tempo-spatial distance. However, the algorithm leverages the special asynchronous data structure of event camera, and by a sophisticated, efficient and simple decision-making, enjoys a linear complexity of $O(n)$ where $n$ is the events amount. In addition, the run-time of the algorithm is independent with the dimensions of the pixels array.

</details>


### [458] [PandaPose: 3D Human Pose Lifting from a Single Image via Propagating 2D Pose Prior to 3D Anchor Space](https://arxiv.org/abs/2602.01095)
*Jinghong Zheng,Changlong Jiang,Yang Xiao,Jiaqi Li,Haohong Kuang,Hang Xu,Ran Wang,Zhiguo Cao,Min Du,Joey Tianyi Zhou*

Main category: cs.CV

Relevance: 20.0

TL;DR: PandaPose提出了一种新的3D人体姿态提升方法，通过将2D姿态先验传播到3D锚点空间作为统一中间表示，解决了传统方法中的误差传播和自遮挡问题。


<details>
  <summary>Details</summary>
Motivation: 现有3D人体姿态提升方法通常基于2D特征建立直接的关节到关节映射，存在两个根本限制：1) 从输入预测的2D姿态到3D预测的不可避免的误差传播；2) 处理自遮挡情况的固有困难。

Method: 提出PandaPose方法，包含三个核心组件：1) 规范坐标系中的关节级3D锚点，提供准确鲁棒的先验；2) 深度感知的关节级特征提升，分层整合深度信息解决自遮挡模糊性；3) 锚点-特征交互解码器，将3D锚点与提升的特征结合生成统一的锚点查询，进一步用于锚点到关节的集成预测。

Result: 在Human3.6M、MPI-INF-3DHP和3DPW三个基准测试上验证了方法的优越性，在Human3.6M的挑战性条件下相比SOTA方法误差降低了14.7%，定性比较进一步展示了方法的有效性和鲁棒性。

Conclusion: PandaPose通过引入3D锚点空间作为统一中间表示，有效解决了传统3D姿态提升方法中的误差传播和自遮挡问题，在多个基准测试上取得了显著性能提升。

Abstract: 3D human pose lifting from a single RGB image is a challenging task in 3D vision. Existing methods typically establish a direct joint-to-joint mapping from 2D to 3D poses based on 2D features. This formulation suffers from two fundamental limitations: inevitable error propagation from input predicted 2D pose to 3D predictions and inherent difficulties in handling self-occlusion cases. In this paper, we propose PandaPose, a 3D human pose lifting approach via propagating 2D pose prior to 3D anchor space as the unified intermediate representation. Specifically, our 3D anchor space comprises: (1) Joint-wise 3D anchors in the canonical coordinate system, providing accurate and robust priors to mitigate 2D pose estimation inaccuracies. (2) Depth-aware joint-wise feature lifting that hierarchically integrates depth information to resolve self-occlusion ambiguities. (3) The anchor-feature interaction decoder that incorporates 3D anchors with lifted features to generate unified anchor queries encapsulating joint-wise 3D anchor set, visual cues and geometric depth information. The anchor queries are further employed to facilitate anchor-to-joint ensemble prediction. Experiments on three well-established benchmarks (i.e., Human3.6M, MPI-INF-3DHP and 3DPW) demonstrate the superiority of our proposition. The substantial reduction in error by $14.7\%$ compared to SOTA methods on the challenging conditions of Human3.6M and qualitative comparisons further showcase the effectiveness and robustness of our approach.

</details>


### [459] [Learning Topology-Aware Implicit Field for Unified Pulmonary Tree Modeling with Incomplete Topological Supervision](https://arxiv.org/abs/2602.02186)
*Ziqiao Weng,Jiancheng Yang,Kangxian Xie,Bo Zhou,Weidong Cai*

Main category: cs.CV

Relevance: 20.0

TL;DR: TopoField：一种拓扑感知的隐式建模框架，用于修复CT图像中提取的肺树拓扑不完整问题，同时支持解剖标记和肺段重建的多任务推理。


<details>
  <summary>Details</summary>
Motivation: CT图像中提取的肺树常存在拓扑不完整问题（如缺失或断开的支气管分支），这会严重影响下游解剖分析，且现有方法依赖密集体积处理或显式图推理，效率低且对结构损坏鲁棒性差。

Method: 使用稀疏表面和骨架点云表示肺解剖结构，学习连续的隐式场，通过在已有不完整树上引入合成结构破坏进行训练，无需完整或显式断开标注。基于修复的隐式表示，通过任务特定的隐式函数在单次前向传播中联合推断解剖标记和肺段重建。

Result: 在Lung3D+数据集上的实验表明，TopoField能持续改善拓扑完整性，并在挑战性的不完整场景下实现准确的解剖标记和肺段重建。由于其隐式表示，计算效率高，每个病例所有任务仅需约1秒。

Conclusion: TopoField将拓扑修复作为首要建模问题，实现了高效、鲁棒的肺树分析，适用于大规模和时间敏感的临床应用。

Abstract: Pulmonary trees extracted from CT images frequently exhibit topological incompleteness, such as missing or disconnected branches, which substantially degrades downstream anatomical analysis and limits the applicability of existing pulmonary tree modeling pipelines. Current approaches typically rely on dense volumetric processing or explicit graph reasoning, leading to limited efficiency and reduced robustness under realistic structural corruption. We propose TopoField, a topology-aware implicit modeling framework that treats topology repair as a first-class modeling problem and enables unified multi-task inference for pulmonary tree analysis. TopoField represents pulmonary anatomy using sparse surface and skeleton point clouds and learns a continuous implicit field that supports topology repair without relying on complete or explicit disconnection annotations, by training on synthetically introduced structural disruptions over \textit{already} incomplete trees. Building upon the repaired implicit representation, anatomical labeling and lung segment reconstruction are jointly inferred through task-specific implicit functions within a single forward pass.Extensive experiments on the Lung3D+ dataset demonstrate that TopoField consistently improves topological completeness and achieves accurate anatomical labeling and lung segment reconstruction under challenging incomplete scenarios. Owing to its implicit formulation, TopoField attains high computational efficiency, completing all tasks in just over one second per case, highlighting its practicality for large-scale and time-sensitive clinical applications. Code and data will be available at https://github.com/HINTLab/TopoField.

</details>


### [460] [Robustness of Presentation Attack Detection in Remote Identity Validation Scenarios](https://arxiv.org/abs/2602.00109)
*John J. Howard,Richard O. Plesh,Yevgeniy B. Sirotin,Jerry L. Tipton,Arun R. Vemury*

Main category: cs.CV

Relevance: 15.0

TL;DR: 该论文研究了低光照条件和自动图像采集对商业展示攻击检测系统性能的影响，发现这些条件会显著降低系统性能，仅有一个系统在所有场景下保持稳健。


<details>
  <summary>Details</summary>
Motivation: 远程身份验证系统中的展示攻击检测子系统在实际应用中面临环境多样性挑战，特别是在低光照和自动采集条件下，需要确保其鲁棒性和可靠性。

Method: 通过场景测试方法，评估商业PAD系统在低光照条件和自动图像采集工作流程下的性能表现，分析错误率变化。

Result: 低光照条件下错误率增加约4倍，自动采集工作流程下错误率翻倍。仅有一个测试系统在所有场景下保持稳健，最大真实展示分类错误率低于3%。

Conclusion: PAD系统性能受环境条件显著影响，需要在多样化环境中进行测试以确保实际应用中的鲁棒性和可靠性。

Abstract: Presentation attack detection (PAD) subsystems are an important part of effective and user-friendly remote identity validation (RIV) systems. However, ensuring robust performance across diverse environmental and procedural conditions remains a critical challenge. This paper investigates the impact of low-light conditions and automated image acquisition on the robustness of commercial PAD systems using a scenario test of RIV. Our results show that PAD systems experience a significant decline in performance when utilized in low-light or auto-capture scenarios, with a model-predicted increase in error rates by a factor of about four under low-light conditions and a doubling of those odds under auto-capture workflows. Specifically, only one of the tested systems was robust to these perturbations, maintaining a maximum bona fide presentation classification error rate below 3% across all scenarios. Our findings emphasize the importance of testing across diverse environments to ensure robust and reliable PAD performance in real-world applications.

</details>


### [461] [From Manual Observation to Automated Monitoring: Space Allowance Effects on Play Behaviour in Group-Housed Dairy Calves](https://arxiv.org/abs/2602.00111)
*Haiyu Yang,Heidi Lesscher,Enhong Liu,Miel Hostens*

Main category: cs.CV

Relevance: 15.0

TL;DR: 研究开发了计算机视觉管道自动监测奶牛犊游戏行为，发现空间分配与游戏行为呈非线性关系，8-10平方米/犊牛为最佳平衡点，计算机视觉分类器准确率达97.6%


<details>
  <summary>Details</summary>
Motivation: 游戏行为是奶牛犊积极福利指标，但商业条件下空间分配（特别是6-20平方米/犊牛）对游戏行为的影响研究不足，需要可扩展的自动化监测方法

Method: 在荷兰14个商业农场研究60头群养犊牛，空间范围2.66-17.98平方米/犊牛；开发自动化计算机视觉管道，使用详细行为谱分析视频观察，游戏行为表示为观察期百分比；采用线性混合模型（农场为随机效应）；计算机视觉管道基于6个农场108小时手动标注训练

Result: 计算机视觉分类器主动游戏检测准确率97.6%，召回率99.4%；犊牛平均1.0%观察时间游戏（约17小时中10分钟）；空间-游戏关系非线性，8-10平方米/犊牛游戏水平最高（1.6%），6-8和12-14平方米最低（<0.6%）；控制年龄、健康和群体规模后空间仍显著

Conclusion: 8-10平方米/犊牛是平衡福利效益与经济可行性的实用目标；自动化监测可将小规模标注项目扩展为连续福利评估系统

Abstract: Play behaviour serves as a positive welfare indicator in dairy calves, yet the influence of space allowance under commercial conditions remains poorly characterized, particularly at intermediate-to-high allowances (6-20 m2 per calf). This study investigated the relationship between space allowance and play behaviour in 60 group-housed dairy calves across 14 commercial farms in the Netherlands (space range: 2.66-17.98 m2 per calf), and developed an automated computer vision pipeline for scalable monitoring. Video observations were analyzed using a detailed ethogram, with play expressed as percentage of observation period (%OP). Statistical analysis employed linear mixed models with farm as a random effect. A computer vision pipeline was trained on manual annotations from 108 hours on 6 farms and validated on held-out test data. The computer vision classifier achieved 97.6% accuracy with 99.4% recall for active play detection. Calves spent on average 1.0% of OP playing reflecting around 10 minutes per 17-hour period. The space-play relationship was non-linear, with highest play levels at 8-10 m2 per calf (1.6% OP) and lowest at 6-8 m2 and 12-14 m2 (<0.6% OP). Space remained significant after controlling for age, health, and group size. In summary, these findings suggest that 8-10 m2 per calf represents a practical target balancing welfare benefits with economic feasibility, and demonstrate that automated monitoring can scale small annotation projects to continuous welfare assessment systems.

</details>


### [462] [DensiThAI, A Multi-View Deep Learning Framework for Breast Density Estimation using Infrared Images](https://arxiv.org/abs/2602.00145)
*Siva Teja Kakileti,Geetha Manjunath*

Main category: cs.CV

Relevance: 15.0

TL;DR: 提出DensiThAI框架，利用多视角深度学习从热成像图像估计乳腺密度，作为无辐射的替代方案


<details>
  <summary>Details</summary>
Motivation: 乳腺组织密度是乳腺癌风险的关键生物标志物，但目前主要依赖X射线乳腺摄影这种电离辐射成像方式。研究探索使用热成像这种无辐射方法进行密度评估的可行性。

Method: 提出DensiThAI多视角深度学习框架，利用五个标准热成像视图进行乳腺密度分类。使用3,500名女性的多中心数据集，以乳腺摄影得出的密度标签作为参考。

Result: 在10个随机分割中平均AUROC达到0.73，所有分割中密度类别间均具有统计学显著差异(p << 0.05)。在不同年龄组中表现一致。

Conclusion: 热成像作为无辐射的乳腺密度评估方法具有潜力，可改善患者体验和工作流程优化。

Abstract: Breast tissue density is a key biomarker of breast cancer risk and a major factor affecting mammographic sensitivity. However, density assessment currently relies almost exclusively on X-ray mammography, an ionizing imaging modality. This study investigates the feasibility of estimating breast density using artificial intelligence over infrared thermal images, offering a non-ionizing imaging approach. The underlying hypothesis is that fibroglandular and adipose tissues exhibit distinct thermophysical and physiological properties, leading to subtle but spatially coherent temperature variations on the breast surface. In this paper, we propose DensiThAI, a multi-view deep learning framework for breast density classification from thermal images. The framework was evaluated on a multi-center dataset of 3,500 women using mammography-derived density labels as reference. Using five standard thermal views, DensiThAI achieved a mean AUROC of 0.73 across 10 random splits, with statistically significant separation between density classes across all splits (p << 0.05). Consistent performance across age cohorts supports the potential of thermal imaging as a non-ionizing approach for breast density assessment with implications for improved patient experience and workflow optimization.

</details>


### [463] [Deep Learning Pose Estimation for Multi-Label Recognition of Combined Hyperkinetic Movement Disorders](https://arxiv.org/abs/2602.00163)
*Laura Cif,Diane Demailly,Gabriella A. Horvàth,Juan Dario Ortigoza Escobar,Nathalie Dorison,Mayté Castro Jiménez,Cécile A. Hubsch,Thomas Wirth,Gun-Marie Hariz,Sophie Huby,Morgan Dornadic,Zohra Souei,Muhammad Mushhood Ur Rehman,Simone Hemm,Mehdi Boulayme,Eduardo M. Moraud,Jocelyne Bloch,Xavier Vasques*

Main category: cs.CV

Relevance: 15.0

TL;DR: 开发了一个基于姿态的机器学习框架，将临床视频转换为关键点时间序列，用于区分运动障碍表型


<details>
  <summary>Details</summary>
Motivation: 运动障碍（如肌张力障碍、震颤等）的临床表现波动、间歇且经常重叠，临床识别和监测主要依赖主观评估，缺乏客观可扩展的方法来区分这些表型

Method: 开发了基于姿态的机器学习框架，将标准门诊视频转换为解剖学有意义的关键点时间序列，计算涵盖统计、时域、频域以及高阶不规则性-复杂性特征的运动学描述符

Result: 论文提出了一个能够客观分析运动障碍表型的框架，但具体结果未在摘要中说明

Conclusion: 该框架为运动障碍的客观识别和纵向监测提供了新方法，有助于解决临床评估的主观性问题

Abstract: Hyperkinetic movement disorders (HMDs) such as dystonia, tremor, chorea, myoclonus, and tics are disabling motor manifestations across childhood and adulthood. Their fluctuating, intermittent, and frequently co-occurring expressions hinder clinical recognition and longitudinal monitoring, which remain largely subjective and vulnerable to inter-rater variability. Objective and scalable methods to distinguish overlapping HMD phenotypes from routine clinical videos are still lacking. Here, we developed a pose-based machine-learning framework that converts standard outpatient videos into anatomically meaningful keypoint time series and computes kinematic descriptors spanning statistical, temporal, spectral, and higher-order irregularity-complexity features.

</details>


### [464] [Development of a Cacao Disease Identification and Management App Using Deep Learning](https://arxiv.org/abs/2602.00216)
*Zaldy Pagaduan,Jason Occidental,Nathaniel Duro,Dexielito Badilles,Eleonor Palconit*

Main category: cs.CV

Relevance: 15.0

TL;DR: 开发了一个用于可可病害识别的离线移动应用，通过深度学习模型帮助菲律宾小农户诊断和管理可可病害，验证准确率达96.93%，田间测试与专家评估一致性为84.2%。


<details>
  <summary>Details</summary>
Motivation: 菲律宾可可小农户面临病虫害严重、缺乏现代农业技术和信息获取困难的问题，特别是在偏远地区网络连接差，需要离线可用的技术工具来改善作物健康管理。

Method: 开发了一个离线运行的移动应用程序，集成了深度学习模型进行可可病害识别。模型经过训练能够准确识别病害类型，并检测黑果病感染程度。系统设计考虑了偏远地区的使用需求。

Result: 病害识别模型验证准确率达到96.93%，黑果病感染程度检测模型准确率为79.49%。田间测试显示应用与可可技术专家评估的一致性为84.2%，证明系统在实际环境中的有效性。

Conclusion: 该离线移动应用为小农户提供了可访问的技术工具，能够有效支持可可病害诊断和管理，有助于提高作物健康和生产力，特别是在资源有限的偏远地区。

Abstract: Smallholder cacao producers often rely on outdated farming techniques and face significant challenges from pests and diseases, unlike larger plantations with more resources and expertise. In the Philippines, cacao farmers have limited access to data, information, and good agricultural practices. This study addresses these issues by developing a mobile application for cacao disease identification and management that functions offline, enabling use in remote areas where farms are mostly located. The core of the system is a deep learning model trained to identify cacao diseases accurately. The trained model is integrated into the mobile app to support farmers in field diagnosis. The disease identification model achieved a validation accuracy of 96.93% while the model for detecting cacao black pod infection levels achieved 79.49% validation accuracy. Field testing of the application showed an agreement rate of 84.2% compared with expert cacao technician assessments. This approach empowers smallholder farmers by providing accessible, technology-enabled tools to improve cacao crop health and productivity.

</details>


### [465] [3DGS$^2$-TR: Scalable Second-Order Trust-Region Method for 3D Gaussian Splatting](https://arxiv.org/abs/2602.00395)
*Roger Hsiao,Yuchen Fang,Xiangru Huang,Ruilong Li,Hesam Rabeti,Zan Gojcic,Javad Lavaei,James Demmel,Sophia Shao*

Main category: cs.CV

Relevance: 15.0

TL;DR: 3DGS²-TR：一种用于加速3D高斯泼溅场景训练的二阶优化器，通过Hessian矩阵对角线近似和参数级信任域技术，在减少50%训练迭代的同时保持低内存开销。


<details>
  <summary>Details</summary>
Motivation: 现有3DGS训练方法中，一阶优化器（如ADAM）收敛慢，而现有二阶方法（如3DGS-LM、3DGS2）依赖显式或密集曲率表示，计算和内存开销大。需要开发高效且内存友好的二阶优化器来加速3DGS训练。

Method: 1. 使用Hutchinson方法高效近似Hessian矩阵对角线，实现完全矩阵自由的计算；2. 基于平方Hellinger距离的参数级信任域技术，正则化高斯参数更新以应对3DGS光栅化过程的强非线性；3. 保持与ADAM相同的O(n)计算和内存复杂度。

Result: 在相同参数初始化和无致密化条件下，相比ADAM：1. 达到更好重建质量；2. 减少50%训练迭代；3. 峰值GPU内存开销增加不到1GB（比ADAM多17%，比3DGS-LM少85%）；4. 支持大规模场景和分布式训练。

Conclusion: 3DGS²-TR是一种高效、内存友好的二阶优化器，显著加速3DGS训练，在保持低内存开销的同时提升重建质量，为大规模场景训练和分布式设置提供了可行性。

Abstract: We propose 3DGS$^2$-TR,a second-order optimizer for accelerating the scene training problem in 3D Gaussian Splatting (3DGS). Unlike existing second-order approaches that rely on explicit or dense curvature representations, such as 3DGS-LM (Höllein et al., 2025) or 3DGS2 (Lan et al., 2025), our method approximates curvature using only the diagonal of the Hessian matrix, efficiently via Hutchinson's method. Our approach is fully matrix-free and has the same complexity as ADAM (Kingma, 2024), $O(n)$ in both computation and memory costs. To ensure stable optimization in the presence of strong nonlinearity in the 3DGS rasterization process, we introduce a parameter-wise trust-region technique based on the squared Hellinger distance, regularizing updates to Gaussian parameters. Under identical parameter initialization and without densification, 3DGS$^2$-TR is able to achieve better reconstruction quality on standard datasets, using 50% fewer training iterations compared to ADAM, while incurring less than 1GB of peak GPU memory overhead (17% more than ADAM and 85% less than 3DGS-LM), enabling scalability to very large scenes and potentially to distributed training settings.

</details>


### [466] [GTATrack: Winner Solution to SoccerTrack 2025 with Deep-EIoU and Global Tracklet Association](https://arxiv.org/abs/2602.00484)
*Rong-Lin Jian,Ming-Chi Luo,Chen-Wei Huang,Chia-Ming Lee,Yu-Fan Lin,Chih-Chung Hsu*

Main category: cs.CV

Relevance: 15.0

TL;DR: GTATrack是一个用于鱼眼相机足球比赛多目标跟踪的分层框架，在SoccerTrack Challenge 2025中获得第一名，通过Deep-EIoU进行运动无关的在线关联和GTA进行轨迹级优化，解决了球员不规则运动、外观相似和频繁遮挡等问题。


<details>
  <summary>Details</summary>
Motivation: 体育场景中的多目标跟踪面临球员不规则运动、外观相似和频繁遮挡等挑战，而静态鱼眼相机引入的几何畸变和极端尺度变化进一步加剧了这些困难。现有方法在处理鱼眼相机失真和足球场景复杂动态时效果有限。

Method: 提出GTATrack分层跟踪框架：1) Deep Expansion IoU (Deep-EIoU)用于运动无关的在线关联，2) Global Tracklet Association (GTA)用于轨迹级优化。采用两阶段设计实现短期鲁棒匹配和长期身份一致性，并使用伪标签策略提升小目标和失真目标的检测召回率。

Result: 在SoccerTrack Challenge 2025中获得第一名，HOTA得分0.60，显著减少误报至982个，在基于鱼眼相机的足球跟踪中达到最先进的准确性。

Conclusion: GTATrack通过局部关联和全局推理的协同作用，有效解决了身份切换、遮挡和跟踪碎片化问题，为鱼眼相机体育跟踪提供了有效的解决方案。

Abstract: Multi-object tracking (MOT) in sports is highly challenging due to irregular player motion, uniform appearances, and frequent occlusions. These difficulties are further exacerbated by the geometric distortion and extreme scale variation introduced by static fisheye cameras. In this work, we present GTATrack, a hierarchical tracking framework that win first place in the SoccerTrack Challenge 2025. GTATrack integrates two core components: Deep Expansion IoU (Deep-EIoU) for motion-agnostic online association and Global Tracklet Association (GTA) for trajectory-level refinement. This two-stage design enables both robust short-term matching and long-term identity consistency. Additionally, a pseudo-labeling strategy is used to boost detector recall on small and distorted targets. The synergy between local association and global reasoning effectively addresses identity switches, occlusions, and tracking fragmentation. Our method achieved a winning HOTA score of 0.60 and significantly reduced false positives to 982, demonstrating state-of-the-art accuracy in fisheye-based soccer tracking. Our code is available at https://github.com/ron941/GTATrack-STC2025.

</details>


### [467] [Refining Strokes by Learning Offset Attributes between Strokes for Flexible Sketch Edit at Stroke-Level](https://arxiv.org/abs/2602.00489)
*Sicong Zang,Tao Sun,Cairong Yan*

Main category: cs.CV

Relevance: 15.0

TL;DR: SketchMod：一种通过变换源笔画来对齐目标草图模式，实现笔画级草图编辑的方法


<details>
  <summary>Details</summary>
Motivation: 现有草图编辑方法仅重新定位源笔画，但源笔画在尺寸和方向上可能有显著变化，仅重新定位而不调整会导致语义不一致和视觉保真度差的问题

Method: 通过学习三个关键偏移属性（缩放、方向和位置）来精炼源笔画，通过：1）按比例调整大小以匹配空间比例，2）按方向旋转以对齐局部几何，3）按位置位移以满足语义布局

Result: 实验结果表明SketchMod在笔画级草图编辑上实现了精确和灵活的性能

Conclusion: 通过变换源笔画来对齐目标草图模式，可以实现更灵活和精确的笔画级草图编辑

Abstract: Sketch edit at stroke-level aims to transplant source strokes onto a target sketch via stroke expansion or replacement, while preserving semantic consistency and visual fidelity with the target sketch. Recent studies addressed it by relocating source strokes at appropriate canvas positions. However, as source strokes could exhibit significant variations in both size and orientation, we may fail to produce plausible sketch editing results by merely repositioning them without further adjustments. For example, anchoring an oversized source stroke onto the target without proper scaling would fail to produce a semantically coherent outcome. In this paper, we propose SketchMod to refine the source stroke through transformation so as to align it with the target sketch's patterns, further realize flexible sketch edit at stroke-level. As the source stroke refinement is governed by the patterns of the target sketch, we learn three key offset attributes (scale, orientation and position) from the source stroke to another, and align it with the target by: 1) resizing to match spatial proportions by scale, 2) rotating to align with local geometry by orientation, and 3) displacing to meet with semantic layout by position. Besides, a stroke's profiles can be precisely controlled during sketch edit via the exposed captured stroke attributes. Experimental results indicate that SketchMod achieves precise and flexible performances on stroke-level sketch edit.

</details>


### [468] [NPNet: A Non-Parametric Network with Adaptive Gaussian-Fourier Positional Encoding for 3D Classification and Segmentation](https://arxiv.org/abs/2602.00542)
*Mohammad Saeid,Amir Salarpour,Pedram MohajerAnsari,Mert D. Pesé*

Main category: cs.CV

Relevance: 15.0

TL;DR: NPNet：一种完全非参数化的3D点云分类和部件分割方法，不包含学习权重，使用确定性操作符构建点特征，采用自适应高斯-傅里叶位置编码


<details>
  <summary>Details</summary>
Motivation: 开发一种无需训练权重、完全非参数化的3D点云处理方法，以解决传统深度学习方法需要大量训练数据、计算资源以及模型泛化能力有限的问题。该方法旨在实现跨不同尺度和采样密度的稳定性，并在少样本场景中表现优异。

Method: NPNet采用完全非参数化设计，使用确定性操作符如最远点采样、k近邻和池化构建点特征。核心创新是自适应高斯-傅里叶位置编码，其带宽和高斯-余弦混合参数根据输入几何自动选择。对于分割任务，额外加入固定频率的傅里叶特征以提供全局上下文信息。

Result: 在ModelNet40/ModelNet-R、ScanObjectNN和ShapeNetPart等基准测试中，NPNet在非参数化基线方法中表现优异，特别是在ModelNet40的少样本设置中效果显著。相比先前的非参数化方法，NPNet在内存使用和推理时间方面也有优势。

Conclusion: NPNet证明了完全非参数化方法在3D点云处理任务中的可行性，通过自适应位置编码机制实现了跨尺度和密度的稳定性，为点云分析提供了一种高效、轻量级的替代方案。

Abstract: We present NPNet, a fully non-parametric approach for 3D point-cloud classification and part segmentation. NPNet contains no learned weights; instead, it builds point features using deterministic operators such as farthest point sampling, k-nearest neighbors, and pooling. Our key idea is an adaptive Gaussian-Fourier positional encoding whose bandwidth and Gaussian-cosine mixing are chosen from the input geometry, helping the method remain stable across different scales and sampling densities. For segmentation, we additionally incorporate fixed-frequency Fourier features to provide global context alongside the adaptive encoding. Across ModelNet40/ModelNet-R, ScanObjectNN, and ShapeNetPart, NPNet achieves strong performance among non-parametric baselines, and it is particularly effective in few-shot settings on ModelNet40. NPNet also offers favorable memory use and inference time compared to prior non-parametric methods

</details>


### [469] [S$^3$POT: Contrast-Driven Face Occlusion Segmentation via Self-Supervised Prompt Learning](https://arxiv.org/abs/2602.00635)
*Lingsong Wang,Mancheng Meng,Ziyan Wu,Terrence Chen,Fan Yang,Dinggang Shen*

Main category: cs.CV

Relevance: 15.0

TL;DR: S³POT是一个用于面部遮挡分割的对比驱动框架，结合人脸生成和自监督空间提示，无需遮挡地面真值标注。


<details>
  <summary>Details</summary>
Motivation: 现有面部解析方法通常将遮挡误分类为面部组件，因为遮挡是高级概念而非具体对象类别。构建覆盖所有遮挡类别的真实数据集几乎不可能，且准确掩码标注成本高昂。

Method: 提出S³POT框架，包含三个模块：参考生成(RF)利用人脸生成器重建被遮挡区域；特征增强(FE)通过原始图像和参考图像的token对比获得初始提示；提示选择(PS)构建正负提示集并通过自注意力网络筛选。框架基于三个互补目标函数学习，无需遮挡真值掩码。

Result: 在专门收集的数据集上进行广泛实验，证明了S³POT的优越性能以及每个模块的有效性。

Conclusion: S³POT通过结合人脸生成和基础分割模型的优势，有效解决了面部遮挡分割问题，无需昂贵的遮挡标注。

Abstract: Existing face parsing methods usually misclassify occlusions as facial components. This is because occlusion is a high-level concept, it does not refer to a concrete category of object. Thus, constructing a real-world face dataset covering all categories of occlusion object is almost impossible and accurate mask annotation is labor-intensive. To deal with the problems, we present S$^3$POT, a contrast-driven framework synergizing face generation with self-supervised spatial prompting, to achieve occlusion segmentation. The framework is inspired by the insights: 1) Modern face generators' ability to realistically reconstruct occluded regions, creating an image that preserve facial geometry while eliminating occlusion, and 2) Foundation segmentation models' (e.g., SAM) capacity to extract precise mask when provided with appropriate prompts. In particular, S$^3$POT consists of three modules: Reference Generation (RF), Feature enhancement (FE), and Prompt Selection (PS). First, a reference image is produced by RF using structural guidance from parsed mask. Second, FE performs contrast of tokens between raw and reference images to obtain an initial prompt, then modifies image features with the prompt by cross-attention. Third, based on the enhanced features, PS constructs a set of positive and negative prompts and screens them with a self-attention network for a mask decoder. The network is learned under the guidance of three novel and complementary objective functions without occlusion ground truth mask involved. Extensive experiments on a dedicatedly collected dataset demonstrate S$^3$POT's superior performance and the effectiveness of each module.

</details>


### [470] [Improving Neuropathological Reconstruction Fidelity via AI Slice Imputation](https://arxiv.org/abs/2602.00669)
*Marina Crespo Aguirre,Jonathan Williams-Ramirez,Dina Zemlyanker,Xiaoling Hu,Lucas J. Deden-Binder,Rogeny Herisse,Mark Montine,Theresa R. Connors,Christopher Mount,Christine L. MacDonald,C. Dirk Keene,Caitlin S. Latimer,Derek H. Oakley,Bradley T. Hyman,Ana Lawry Aguila,Juan Eugenio Iglesias*

Main category: cs.CV

Relevance: 15.0

TL;DR: 提出一种计算高效的超分辨率方法，从各向异性的3D解剖照片重建中生成解剖学一致的各向同性体积，通过域随机化合成数据训练，提高分割精度和表面重建准确性。


<details>
  <summary>Details</summary>
Motivation: 神经病理学分析需要空间精确的体积重建来增强解剖描绘和提高形态测量精度。现有从2D解剖照片重建3D脑体积的方法有时会产生粗糙、过度平滑的结构重建，特别是在高各向异性（厚切片）情况下。

Method: 引入计算高效的超分辨率步骤，从各向异性的3D解剖照片重建中插值切片，生成解剖学一致的各向同性体积。使用域随机化合成数据进行训练，确保方法在不同解剖协议和大切片厚度下具有鲁棒性。

Result: 插值后的体积改善了自动分割性能，获得了更高的Dice分数，特别是在皮质和白质区域。在表面重建和图谱配准任务上的验证显示更准确的皮质表面和MRI配准。

Conclusion: 通过增强基于照片重建的分辨率和解剖保真度，该方法加强了神经病理学和神经影像学之间的联系。方法已公开可用。

Abstract: Neuropathological analyses benefit from spatially precise volumetric reconstructions that enhance anatomical delineation and improve morphometric accuracy. Our prior work has shown the feasibility of reconstructing 3D brain volumes from 2D dissection photographs. However these outputs sometimes exhibit coarse, overly smooth reconstructions of structures, especially under high anisotropy (i.e., reconstructions from thick slabs). Here, we introduce a computationally efficient super-resolution step that imputes slices to generate anatomically consistent isotropic volumes from anisotropic 3D reconstructions of dissection photographs. By training on domain-randomized synthetic data, we ensure that our method generalizes across dissection protocols and remains robust to large slab thicknesses. The imputed volumes yield improved automated segmentations, achieving higher Dice scores, particularly in cortical and white matter regions. Validation on surface reconstruction and atlas registration tasks demonstrates more accurate cortical surfaces and MRI registration. By enhancing the resolution and anatomical fidelity of photograph-based reconstructions, our approach strengthens the bridge between neuropathology and neuroimaging. Our method is publicly available at https://surfer.nmr.mgh.harvard.edu/fswiki/mri_3d_photo_recon

</details>


### [471] [StomataSeg: Semi-Supervised Instance Segmentation for Sorghum Stomatal Components](https://arxiv.org/abs/2602.00703)
*Zhongtian Huang,Zhi Chen,Zi Huang,Xin Yu,Daniel Smith,Chaitanya Purushothama,Erik Van Oosterom,Alex Wu,William Salter,Yan Li,Scott Chapman*

Main category: cs.CV

Relevance: 15.0

TL;DR: 提出一个半监督实例分割框架，用于高粱气孔组件分析，结合补丁预处理和伪标签策略，显著提升微小气孔结构分割性能


<details>
  <summary>Details</summary>
Motivation: 高粱作为耐旱作物对气候适应性农业很重要，但气孔自动分析困难，因为气孔微小（<40μm）且形状随基因型和叶面变化。现有方法面临嵌套小结构和标注瓶颈的挑战

Method: 1) 收集标注高粱叶片图像数据集（11,060个人工标注补丁）；2) 将高分辨率显微图像分割为重叠小补丁以检测微小结构；3) 对未标注图像应用伪标签策略，生成56,428个伪标注补丁；4) 建立半监督实例分割框架

Result: 语义分割模型最佳mIoU从65.93%提升至70.35%，实例分割模型最佳AP从28.30%提升至46.10%，证明补丁预处理结合半监督学习显著改善精细气孔结构分割

Conclusion: 提出的框架支持可扩展的气孔性状提取，促进AI驱动表型分析在作物科学中的广泛应用

Abstract: Sorghum is a globally important cereal grown widely in water-limited and stress-prone regions. Its strong drought tolerance makes it a priority crop for climate-resilient agriculture. Improving water-use efficiency in sorghum requires precise characterisation of stomatal traits, as stomata control of gas exchange, transpiration and photosynthesis have a major influence on crop performance. Automated analysis of sorghum stomata is difficult because the stomata are small (often less than 40 $μ$m in length in grasses such as sorghum) and vary in shape across genotypes and leaf surfaces. Automated segmentation contributes to high-throughput stomatal phenotyping, yet current methods still face challenges related to nested small structures and annotation bottlenecks. In this paper, we propose a semi-supervised instance segmentation framework tailored for analysis of sorghum stomatal components. We collect and annotate a sorghum leaf imagery dataset containing 11,060 human-annotated patches, covering the three stomatal components (pore, guard cell and complex area) across multiple genotypes and leaf surfaces. To improve the detection of tiny structures, we split high-resolution microscopy images into overlapping small patches. We then apply a pseudo-labelling strategy to unannotated images, producing an additional 56,428 pseudo-labelled patches. Benchmarking across semantic and instance segmentation models shows substantial performance gains: for semantic models the top mIoU increases from 65.93% to 70.35%, whereas for instance models the top AP rises from 28.30% to 46.10%. These results demonstrate that combining patch-based preprocessing with semi-supervised learning significantly improves the segmentation of fine stomatal structures. The proposed framework supports scalable extraction of stomatal traits and facilitates broader adoption of AI-driven phenotyping in crop science.

</details>


### [472] [Diffusion-Driven Inter-Outer Surface Separation for Point Clouds with Open Boundaries](https://arxiv.org/abs/2602.00739)
*Zhengyan Qin,Liyuan Qiu*

Main category: cs.CV

Relevance: 15.0

TL;DR: 提出一种基于扩散的算法，用于从双层点云中分离内层和外层表面，特别针对TSDF融合中截断引起的"双层伪影"问题


<details>
  <summary>Details</summary>
Motivation: 在室内或医学3D重建中，TSDF融合的截断会导致不对称截断阈值，产生错误的内外层表面伪影，影响表面表示准确性

Method: 基于扩散的算法，处理具有开放边界的点云，能够同时处理水密和开放边界模型，作为TSDF融合后的轻量级后处理模块

Result: 能够从20,000个内层和20,000个外层点中提取内层表面，处理时间约10秒，适用于室内场景建模和医学成像

Conclusion: 该方法有效解决TSDF融合中的双层伪影问题，提供准确的内层表面提取，但不旨在替代完整的重建流程

Abstract: We propose a diffusion-based algorithm for separating the inter and outer layer surfaces from double-layered point clouds, particularly those exhibiting the "double surface artifact" caused by truncation in Truncated Signed Distance Function (TSDF) fusion during indoor or medical 3D reconstruction. This artifact arises from asymmetric truncation thresholds, leading to erroneous inter and outer shells in the fused volume, which our method addresses by extracting the true inter layer to mitigate challenges like overlapping surfaces and disordered normals. We focus on point clouds with \emph{open boundaries} (i.e., sampled surfaces with topological openings/holes through which particles may escape), rather than point clouds with \emph{missing surface regions} where no samples exist. Our approach enables robust processing of both watertight and open-boundary models, achieving extraction of the inter layer from 20,000 inter and 20,000 outer points in approximately 10 seconds. This solution is particularly effective for applications requiring accurate surface representations, such as indoor scene modeling and medical imaging, where double-layered point clouds are prevalent, and it accommodates both closed (watertight) and open-boundary surface geometries. Our goal is \emph{post-hoc} inter/outer shell separation as a lightweight module after TSDF fusion; we do not aim to replace full variational or learning-based reconstruction pipelines.

</details>


### [473] [Evaluating Deep Learning-Based Nerve Segmentation in Brachial Plexus Ultrasound Under Realistic Data Constraints](https://arxiv.org/abs/2602.00763)
*Dylan Yves,Khush Agarwal,Jonathan Hoyin Chan,Patcharapit Promoppatum,Aroonkamon Pattanasiricharoen*

Main category: cs.CV

Relevance: 15.0

TL;DR: 评估深度学习在超声引导下臂丛神经分割中的性能，分析数据集组成、标注策略对U-Net架构分割效果的影响


<details>
  <summary>Details</summary>
Motivation: 超声引导下区域麻醉中神经准确定位至关重要，但手动识别因图像对比度低、斑点噪声和患者间解剖变异而具有挑战性，需要评估深度学习解决方案

Method: 使用U-Net架构进行超声图像神经分割，研究数据集组成（多台超声设备数据组合）和标注策略（从二分类扩展到多类别监督）对性能的影响

Result: 多设备数据组合对性能较差的数据源有正则化效果，但不超过单源训练；多类别监督导致神经特异性Dice分数下降9%-61%；神经大小与分割精度呈中等正相关（r=0.587）

Conclusion: 为在真实临床数据约束下开发稳健的超声神经分割系统提供了方法学指导，小神经分割仍是主要挑战

Abstract: Accurate nerve localization is critical for the success of ultrasound-guided regional anesthesia, yet manual identification remains challenging due to low image contrast, speckle noise, and inter-patient anatomical variability. This study evaluates deep learning-based nerve segmentation in ultrasound images of the brachial plexus using a U-Net architecture, with a focus on how dataset composition and annotation strategy influence segmentation performance. We find that training on combined data from multiple ultrasound machines (SIEMENS ACUSON NX3 Elite and Philips EPIQ5) provides regularization benefits for lower-performing acquisition sources, though it does not surpass single-source training when matched to the target domain. Extending the task from binary nerve segmentation to multi-class supervision (artery, vein, nerve, muscle) results in decreased nerve-specific Dice scores, with performance drops ranging from 9% to 61% depending on dataset, likely due to class imbalance and boundary ambiguity. Additionally, we observe a moderate positive correlation between nerve size and segmentation accuracy (Pearson r=0.587, p<0.001), indicating that smaller nerves remain a primary challenge. These findings provide methodological guidance for developing robust ultrasound nerve segmentation systems under realistic clinical data constraints.

</details>


### [474] [VAMOS-OCTA: Vessel-Aware Multi-Axis Orthogonal Supervision for Inpainting Motion-Corrupted OCT Angiography Volumes](https://arxiv.org/abs/2602.00995)
*Nick DiSanto,Ehsan Khodapanah Aghdam,Han Liu,Jacob Watson,Yuankai K. Tao,Hao Li,Ipek Oguz*

Main category: cs.CV

Relevance: 15.0

TL;DR: VAMOS-OCTA：一种用于修复OCTA运动伪影的深度学习框架，通过血管感知多轴监督来恢复被运动破坏的B扫描图像


<details>
  <summary>Details</summary>
Motivation: 手持式OCTA在非合作或儿科患者中易受运动伪影影响，导致3D采集时出现未采样的视网膜区域，在en face投影中产生空白带，严重影响图像质量

Method: 采用2.5D U-Net架构，以相邻B扫描堆栈为输入重建被破坏的中心B扫描，使用新颖的VAMOS损失函数，结合血管加权强度重建与轴向和横向投影一致性约束

Result: VAMOS-OCTA在合成和真实世界破坏数据上均优于现有方法，能产生具有清晰毛细血管、恢复血管连续性和干净en face投影的重建结果

Conclusion: 多轴监督为恢复运动退化的3D OCTA数据提供了强大的约束，该框架能同时增强B扫描清晰度和体积投影准确性

Abstract: Handheld Optical Coherence Tomography Angiography (OCTA) enables noninvasive retinal imaging in uncooperative or pediatric subjects, but is highly susceptible to motion artifacts that severely degrade volumetric image quality. Sudden motion during 3D acquisition can lead to unsampled retinal regions across entire B-scans (cross-sectional slices), resulting in blank bands in en face projections. We propose VAMOS-OCTA, a deep learning framework for inpainting motion-corrupted B-scans using vessel-aware multi-axis supervision. We employ a 2.5D U-Net architecture that takes a stack of neighboring B-scans as input to reconstruct a corrupted center B-scan, guided by a novel Vessel-Aware Multi-Axis Orthogonal Supervision (VAMOS) loss. This loss combines vessel-weighted intensity reconstruction with axial and lateral projection consistency, encouraging vascular continuity in native B-scans and across orthogonal planes. Unlike prior work that focuses primarily on restoring the en face MIP, VAMOS-OCTA jointly enhances both cross-sectional B-scan sharpness and volumetric projection accuracy, even under severe motion corruptions. We trained our model on both synthetic and real-world corrupted volumes and evaluated its performance using both perceptual quality and pixel-wise accuracy metrics. VAMOS-OCTA consistently outperforms prior methods, producing reconstructions with sharp capillaries, restored vessel continuity, and clean en face projections. These results demonstrate that multi-axis supervision offers a powerful constraint for restoring motion-degraded 3D OCTA data. Our source code is available at https://github.com/MedICL-VU/VAMOS-OCTA.

</details>


### [475] [FUSE-Flow: Scalable Real-Time Multi-View Point Cloud Reconstruction Using Confidence](https://arxiv.org/abs/2602.01035)
*Chentian Sun*

Main category: cs.CV

Relevance: 15.0

TL;DR: FUSE-Flow：一种实时多视角点云重建框架，通过帧间独立处理、自适应空间哈希和加权融合，实现线性复杂度的高质量点云流式重建。


<details>
  <summary>Details</summary>
Motivation: 实时多视角点云重建在VR/AR、机器人导航、数字孪生等领域有广泛应用，但现有方法（基于体素融合、时间累积或全局优化）存在计算复杂度高、内存占用大、可扩展性有限的问题，难以同时满足实时性、重建质量和多相机扩展性要求。

Method: 提出FUSE-Flow框架：1）帧间独立生成点云片段；2）通过测量置信度和3D距离一致性两个权重进行融合以抑制噪声并保留几何细节；3）引入自适应空间哈希加权聚合方法，根据局部点云密度自适应划分3D空间，每个单元选择代表点进行加权融合；4）GPU并行化实现。

Result: 实验表明，该框架在重叠区域、深度不连续和动态场景中提高了重建稳定性和几何保真度，同时在现代GPU上保持实时帧率，验证了其有效性、鲁棒性和可扩展性。

Conclusion: FUSE-Flow成功解决了实时多视角点云重建的挑战，通过无状态、帧间独立的处理方式和线性复杂度的设计，实现了高质量、高效率的点云流式重建，为3D视觉和沉浸式感知应用提供了实用解决方案。

Abstract: Real-time multi-view point cloud reconstruction is a core problem in 3D vision and immersive perception, with wide applications in VR, AR, robotic navigation, digital twins, and computer interaction. Despite advances in multi-camera systems and high-resolution depth sensors, fusing large-scale multi-view depth observations into high-quality point clouds under strict real-time constraints remains challenging. Existing methods relying on voxel-based fusion, temporal accumulation, or global optimization suffer from high computational complexity, excessive memory usage, and limited scalability, failing to simultaneously achieve real-time performance, reconstruction quality, and multi-camera extensibility. We propose FUSE-Flow, a frame-wise, stateless, and linearly scalable point cloud streaming reconstruction framework. Each frame independently generates point cloud fragments, fused via two weights, measurement confidence and 3D distance consistency to suppress noise while preserving geometric details. For large-scale multi-camera efficiency, we introduce an adaptive spatial hashing-based weighted aggregation method: 3D space is adaptively partitioned by local point cloud density, representative points are selected per cell, and weighted fusion is performed to handle both sparse and dense regions. With GPU parallelization, FUSE-Flow achieves high-throughput, low-latency point cloud generation and fusion with linear complexity. Experiments demonstrate that the framework improves reconstruction stability and geometric fidelity in overlapping, depth-discontinuous, and dynamic scenes, while maintaining real-time frame rates on modern GPUs, verifying its effectiveness, robustness, and scalability.

</details>


### [476] [Radioactive 3D Gaussian Ray Tracing for Tomographic Reconstruction](https://arxiv.org/abs/2602.01057)
*Ling Chen,Bao Yang*

Main category: cs.CV

Relevance: 15.0

TL;DR: 提出基于3D高斯射线追踪的断层重建框架，通过解析计算线积分避免局部仿射近似，提高投影精度并支持非线性几何校正


<details>
  <summary>Details</summary>
Motivation: 现有R2-Gaussian方法在断层重建中使用局部仿射近似，这会降低重建定量精度并难以整合非线性几何校正，需要更精确的物理一致模型

Method: 基于3D高斯射线追踪的断层重建框架，解析计算通过3D高斯基元的线积分，避免局部仿射塌缩，提供对射线起点和方向的显式控制

Result: 相比基于splatting的模型，该方法提供更物理一致的前向投影模型，能精确应用非线性几何校正（如PET中的弧校正），扩展高斯基重建的适用性

Conclusion: 3D高斯射线追踪框架解决了仿射近似的局限性，提高了断层重建的投影精度和几何校正能力，适用于更广泛的真实断层成像系统

Abstract: 3D Gaussian Splatting (3DGS) has recently emerged in computer vision as a promising rendering technique. By adapting the principles of Elliptical Weighted Average (EWA) splatting to a modern differentiable pipeline, 3DGS enables real-time, high-quality novel view synthesis. Building upon this, R2-Gaussian extended the 3DGS paradigm to tomographic reconstruction by rectifying integration bias, achieving state-of-the-art performance in computed tomography (CT). To enable differentiability, R2-Gaussian adopts a local affine approximation: each 3D Gaussian is locally mapped to a 2D Gaussian on the detector and composed via alpha blending to form projections. However, the affine approximation can degrade reconstruction quantitative accuracy and complicate the incorporation of nonlinear geometric corrections. To address these limitations, we propose a tomographic reconstruction framework based on 3D Gaussian ray tracing. Our approach provides two key advantages over splatting-based models: (i) it computes the line integral through 3D Gaussian primitives analytically, avoiding the local affine collapse and thus yielding a more physically consistent forward projection model; and (ii) the ray-tracing formulation gives explicit control over ray origins and directions, which facilitates the precise application of nonlinear geometric corrections, e.g., arc-correction used in positron emission tomography (PET). These properties extend the applicability of Gaussian-based reconstruction to a wider range of realistic tomography systems while improving projection accuracy.

</details>


### [477] [Interacted Planes Reveal 3D Line Mapping](https://arxiv.org/abs/2602.01296)
*Zeran Ke,Bin Tan,Gui-Song Xia,Yujun Shen,Nan Xue*

Main category: cs.CV

Relevance: 15.0

TL;DR: LiP-Map是一个联合优化线和平面基元的3D线映射框架，通过显式建模线和平面基元的交互，从物理和拓扑角度解决3D线映射问题，在多个数据集上实现了最先进的精度和完整性。


<details>
  <summary>Details</summary>
Motivation: 从物理和拓扑角度研究3D线映射问题：3D线最自然地作为有限3D平面块的边缘出现。现有方法缺乏对线和平面之间关系的显式建模，限制了结构化重建的质量。

Method: 提出LiP-Map框架，显式建模可学习的线和平面基元，通过联合优化耦合线和平面基元。不是施加成对共面约束，而是显式构建平面和线基元之间的交互，为结构化重建提供原则性方法。

Result: 在ScanNetV2、ScanNet++、Hypersim、7Scenes和Tanks&Temple等100多个场景上，LiP-Map在精度和完整性方面优于最先进方法。在7Scenes上的线辅助视觉定位也表现出色，重建通常只需3-5分钟/场景。

Conclusion: LiP-Map首次将平面拓扑整合到3D线映射中，通过显式建模线和平面基元的交互，为人造环境中的结构化重建提供了原则性方法，在效率和准确性方面都有显著提升。

Abstract: 3D line mapping from multi-view RGB images provides a compact and structured visual representation of scenes. We study the problem from a physical and topological perspective: a 3D line most naturally emerges as the edge of a finite 3D planar patch. We present LiP-Map, a line-plane joint optimization framework that explicitly models learnable line and planar primitives. This coupling enables accurate and detailed 3D line mapping while maintaining strong efficiency (typically completing a reconstruction in 3 to 5 minutes per scene). LiP-Map pioneers the integration of planar topology into 3D line mapping, not by imposing pairwise coplanarity constraints but by explicitly constructing interactions between plane and line primitives, thus offering a principled route toward structured reconstruction in man-made environments. On more than 100 scenes from ScanNetV2, ScanNet++, Hypersim, 7Scenes, and Tanks\&Temple, LiP-Map improves both accuracy and completeness over state-of-the-art methods. Beyond line mapping quality, LiP-Map significantly advances line-assisted visual localization, establishing strong performance on 7Scenes. Our code is released at https://github.com/calmke/LiPMAP for reproducible research.

</details>


### [478] [VRGaussianAvatar: Integrating 3D Gaussian Avatars into VR](https://arxiv.org/abs/2602.01674)
*Hail Song,Boram Yoon,Seokhwan Yang,Seoyoung Kang,Hyunjeong Kim,Henning Metzmacher,Woontack Woo*

Main category: cs.CV

Relevance: 15.0

TL;DR: VRGaussianAvatar：基于3D高斯溅射的实时全身虚拟现实化身系统，仅使用头戴显示器追踪信号，通过双目批处理实现高效立体渲染。


<details>
  <summary>Details</summary>
Motivation: 当前虚拟现实中的全身化身系统通常需要复杂的传感器或多视角输入，难以实现实时渲染和高保真外观。本文旨在开发仅使用HMD追踪信号就能实现实时、高保真3D高斯溅射化身的系统。

Method: 系统采用并行管道架构：VR前端使用逆运动学估计全身姿态并传输姿态与立体相机参数；GA后端从单张图像重建3DGS化身。关键创新是双目批处理技术，将左右眼视图联合处理以减少冗余计算，支持高分辨率VR显示。

Result: 系统在交互式VR性能测试中表现稳定，用户研究表明相比基于图像和网格的基线方法，VRGaussianAvatar在感知外观相似性、具身感和合理性方面得分更高。

Conclusion: VRGaussianAvatar展示了仅使用HMD追踪信号实现实时3D高斯溅射化身的可行性，通过双目批处理优化了立体渲染效率，为VR中的高保真化身提供了新解决方案。

Abstract: We present VRGaussianAvatar, an integrated system that enables real-time full-body 3D Gaussian Splatting (3DGS) avatars in virtual reality using only head-mounted display (HMD) tracking signals. The system adopts a parallel pipeline with a VR Frontend and a GA Backend. The VR Frontend uses inverse kinematics to estimate full-body pose and streams the resulting pose along with stereo camera parameters to the backend. The GA Backend stereoscopically renders a 3DGS avatar reconstructed from a single image. To improve stereo rendering efficiency, we introduce Binocular Batching, which jointly processes left and right eye views in a single batched pass to reduce redundant computation and support high-resolution VR displays. We evaluate VRGaussianAvatar with quantitative performance tests and a within-subject user study against image- and video-based mesh avatar baselines. Results show that VRGaussianAvatar sustains interactive VR performance and yields higher perceived appearance similarity, embodiment, and plausibility. Project page and source code are available at https://vrgaussianavatar.github.io.

</details>


### [479] [Cross-Modal Alignment and Fusion for RGB-D Transmission-Line Defect Detection](https://arxiv.org/abs/2602.01696)
*Jiaming Cui,Shuai Zhou,Wenqiang Li,Ruifeng Qin,Feng Shen*

Main category: cs.CV

Relevance: 15.0

TL;DR: 提出CMAFNet用于输电线路缺陷检测，通过跨模态对齐与融合网络整合RGB外观和深度几何信息，在小型缺陷检测上表现优异。


<details>
  <summary>Details</summary>
Motivation: 输电线路缺陷检测面临挑战：小型缺陷占主导、复杂背景、光照变化。现有RGB检测器难以区分几何细微缺陷与视觉相似背景结构，特别是在色彩对比度有限的情况下。

Method: CMAFNet采用"净化-融合"范式，包含语义重组模块（基于字典的特征净化）和上下文语义集成框架（部分通道注意力捕获全局空间依赖）。通过位置归一化实现跨模态对齐。

Result: 在TLRGBD基准测试中（94.5%为小目标），达到32.2% mAP@50和12.5% APs，比最强基线分别提升9.8和4.0个百分点。轻量级版本达到24.8% mAP50、228 FPS、仅4.9M参数。

Conclusion: CMAFNet通过跨模态对齐与融合有效提升输电线路缺陷检测性能，特别是对小目标检测，同时提供高效轻量级版本。

Abstract: Transmission line defect detection remains challenging for automated UAV inspection due to the dominance of small-scale defects, complex backgrounds, and illumination variations. Existing RGB-based detectors, despite recent progress, struggle to distinguish geometrically subtle defects from visually similar background structures under limited chromatic contrast. This paper proposes CMAFNet, a Cross-Modal Alignment and Fusion Network that integrates RGB appearance and depth geometry through a principled purify-then-fuse paradigm. CMAFNet consists of a Semantic Recomposition Module that performs dictionary-based feature purification via a learned codebook to suppress modality-specific noise while preserving defect-discriminative information, and a Contextual Semantic Integration Framework that captures global spatial dependencies using partial-channel attention to enhance structural semantic reasoning. Position-wise normalization within the purification stage enforces explicit reconstruction-driven cross-modal alignment, ensuring statistical compatibility between heterogeneous features prior to fusion. Extensive experiments on the TLRGBD benchmark, where 94.5% of instances are small objects, demonstrate that CMAFNet achieves 32.2% mAP@50 and 12.5% APs, outperforming the strongest baseline by 9.8 and 4.0 percentage points, respectively. A lightweight variant reaches 24.8% mAP50 at 228 FPS with only 4.9M parameters, surpassing all YOLO-based detectors while matching transformer-based methods at substantially lower computational cost.

</details>


### [480] [Spot-Wise Smart Parking: An Edge-Enabled Architecture with YOLOv11 and Digital Twin Integration](https://arxiv.org/abs/2602.01754)
*Gustavo P. C. P. da Luz,Alvaro M. Aspilcueta Narvaez,Tiago Godoi Bannwart,Gabriel Massuyoshi Sato,Luis Fernando Gomez Gonzalez,Juliana Freitag Borin*

Main category: cs.CV

Relevance: 15.0

TL;DR: 论文提出了一种基于距离感知匹配和自适应边界框分区的智能停车位级监控系统，在资源受限的边缘设备上实现了98.80%的平衡准确率和8秒推理时间，并引入了数字孪生雏形和基于电视盒的应用支持服务器。


<details>
  <summary>Details</summary>
Motivation: 现有基于区域车辆计数的停车监控系统虽然准确率高，但无法提供车位级洞察和支持更高级应用。需要开发能够实现精确车位级监控、在资源受限设备上高效运行、并能支持数字孪生和可持续硬件利用的系统。

Method: 1) 基于空间容差的距离感知匹配方法实现车位级监控；2) 针对挑战性空间的自适应边界框分区方法；3) 使用YOLOv11m模型（40.5MB）；4) 引入数字阴影作为数字孪生基础；5) 基于改造电视盒的应用支持服务器实现可扩展通信。

Result: 在资源受限的边缘设备上实现了98.80%的平衡准确率，推理时间仅8秒。系统能够提供精确的车位级占用统计，支持数字孪生演进，并通过硬件重用促进可持续性。

Conclusion: 提出的车位级监控系统在准确性和效率方面表现优异，成功克服了区域级监控的局限性。数字阴影和基于电视盒的服务架构为智能停车系统向数字孪生和可持续硬件利用方向发展提供了可行路径。

Abstract: Smart parking systems help reduce congestion and minimize users' search time, thereby contributing to smart city adoption and enhancing urban mobility. In previous works, we presented a system developed on a university campus to monitor parking availability by estimating the number of free spaces from vehicle counts within a region of interest. Although this approach achieved good accuracy, it restricted the system's ability to provide spot-level insights and support more advanced applications. To overcome this limitation, we extend the system with a spot-wise monitoring strategy based on a distance-aware matching method with spatial tolerance, enhanced through an Adaptive Bounding Box Partitioning method for challenging spaces. The proposed approach achieves a balanced accuracy of 98.80% while maintaining an inference time of 8 seconds on a resource-constrained edge device, enhancing the capabilities of YOLOv11m, a model that has a size of 40.5 MB. In addition, two new components were introduced: (i) a Digital Shadow that visually represents parking lot entities as a base to evolve to a full Digital Twin, and (ii) an application support server based on a repurposed TV box. The latter not only enables scalable communication among cloud services, the parking totem, and a bot that provides detailed spot occupancy statistics, but also promotes hardware reuse as a step towards greater sustainability.

</details>


### [481] [GDPR-Compliant Person Recognition in Industrial Environments Using MEMS-LiDAR and Hybrid Data](https://arxiv.org/abs/2602.01764)
*Dennis Basile,Dennis Sprute,Helene Dörksen,Holger Flatt*

Main category: cs.CV

Relevance: 15.0

TL;DR: 该论文提出了一种基于MEMS-LiDAR的隐私合规人员检测方法，通过结合真实LiDAR数据和CARLA模拟生成的合成数据，在工业环境中实现高精度人员检测，同时满足GDPR隐私要求。


<details>
  <summary>Details</summary>
Motivation: 工业室内安全空间需要可靠检测未经授权人员，传统视觉方法存在光照敏感性和隐私违规问题，且深度学习需要大量标注数据，收集和标注过程耗时且易出错。

Method: 使用MEMS-LiDAR捕获匿名3D点云数据，结合CARLA模拟框架生成合成场景数据，创建混合数据集用于训练人员检测模型，减少真实数据收集和标注工作量。

Result: 混合数据方法相比仅使用真实数据的模型，平均精度提高44个百分点，同时减少50%的人工标注工作量，在保持GDPR合规性的同时实现高性能人员检测。

Conclusion: 该方法为工业环境提供了一种可扩展、成本效益高的隐私合规人员检测方案，展示了合成LiDAR数据在结合高性能检测与隐私保护方面的系统优势。

Abstract: The reliable detection of unauthorized individuals in safety-critical industrial indoor spaces is crucial to avoid plant shutdowns, property damage, and personal hazards. Conventional vision-based methods that use deep-learning approaches for person recognition provide image information but are sensitive to lighting and visibility conditions and often violate privacy regulations, such as the General Data Protection Regulation (GDPR) in the European Union. Typically, detection systems based on deep learning require annotated data for training. Collecting and annotating such data, however, is highly time-consuming and due to manual treatments not necessarily error free. Therefore, this paper presents a privacy-compliant approach based on Micro-Electro-Mechanical Systems LiDAR (MEMS-LiDAR), which exclusively captures anonymized 3D point clouds and avoids personal identification features. To compensate for the large amount of time required to record real LiDAR data and for post-processing and annotation, real recordings are augmented with synthetically generated scenes from the CARLA simulation framework. The results demonstrate that the hybrid data improves the average precision by 44 percentage points compared to a model trained exclusively with real data while reducing the manual annotation effort by 50 %. Thus, the proposed approach provides a scalable, cost-efficient alternative to purely real-data-based methods and systematically shows how synthetic LiDAR data can combine high performance in person detection with GDPR compliance in an industrial environment.

</details>


### [482] [SurfSplat: Conquering Feedforward 2D Gaussian Splatting with Surface Continuity Priors](https://arxiv.org/abs/2602.02000)
*Bing He,Jingnan Gao,Yunuo Chen,Ning Cao,Gang Chen,Zhengxue Cheng,Li Song,Wenjun Zhang*

Main category: cs.CV

Relevance: 15.0

TL;DR: SurfSplat是一个基于2D高斯泼溅的前馈框架，通过引入表面连续性先验和强制alpha混合策略，从稀疏图像重建高保真3D场景，解决了现有方法在近距离观察时出现严重伪影的问题。


<details>
  <summary>Details</summary>
Motivation: 现有基于3D高斯泼溅的通用模型在从稀疏图像重建3D场景时，往往产生离散、颜色偏差的点云，在正常分辨率下看似合理，但在近距离观察时会出现严重伪影，无法重建连续表面。

Method: 提出基于2D高斯泼溅的SurfSplat框架，相比3D高斯泼溅具有更强的各向异性和更高的几何精度。引入表面连续性先验和强制alpha混合策略来重建连贯几何和忠实纹理。还提出了高分辨率渲染一致性（HRRC）评估指标。

Result: 在RealEstate10K、DL3DV和ScanNet数据集上的大量实验表明，SurfSplat在标准指标和HRRC上都持续优于先前方法，为稀疏输入的高保真3D重建提供了鲁棒解决方案。

Conclusion: SurfSplat通过2D高斯泼溅、表面连续性先验和强制alpha混合策略，成功解决了稀疏图像3D重建中的表面连续性问题，实现了高质量的重建结果。

Abstract: Reconstructing 3D scenes from sparse images remains a challenging task due to the difficulty of recovering accurate geometry and texture without optimization. Recent approaches leverage generalizable models to generate 3D scenes using 3D Gaussian Splatting (3DGS) primitive. However, they often fail to produce continuous surfaces and instead yield discrete, color-biased point clouds that appear plausible at normal resolution but reveal severe artifacts under close-up views. To address this issue, we present SurfSplat, a feedforward framework based on 2D Gaussian Splatting (2DGS) primitive, which provides stronger anisotropy and higher geometric precision. By incorporating a surface continuity prior and a forced alpha blending strategy, SurfSplat reconstructs coherent geometry together with faithful textures. Furthermore, we introduce High-Resolution Rendering Consistency (HRRC), a new evaluation metric designed to evaluate high-resolution reconstruction quality. Extensive experiments on RealEstate10K, DL3DV, and ScanNet demonstrate that SurfSplat consistently outperforms prior methods on both standard metrics and HRRC, establishing a robust solution for high-fidelity 3D reconstruction from sparse inputs. Project page: https://hebing-sjtu.github.io/SurfSplat-website/

</details>


### [483] [Implicit neural representation of textures](https://arxiv.org/abs/2602.02354)
*Albert Kwok,Zheyuan Hu,Dounia Hammou*

Main category: cs.CV

Relevance: 15.0

TL;DR: 该论文探索了将隐式神经表示（INR）作为连续纹理表示的新方法，替代传统的离散UV坐标空间表示，在图像质量、内存使用和渲染推理时间之间取得平衡，并探索了实时渲染和下游任务的应用。


<details>
  <summary>Details</summary>
Motivation: 传统纹理表示使用离散的UV坐标空间，而隐式神经表示（INR）在多个领域已证明准确高效。本研究旨在探索如何设计不同的神经网络作为新的纹理INR，实现连续而非离散的纹理表示，以改进渲染质量和效率。

Method: 设计了多种神经网络作为纹理INR，在连续输入UV坐标空间上操作。通过全面的实验评估这些INR在图像质量、内存使用和渲染推理时间方面的表现，并分析这些目标之间的平衡关系。

Result: 实验表明，这些INR在图像质量方面表现良好，同时具有相当的内存使用和渲染推理时间。研究分析了质量与效率之间的权衡，并探索了在实时渲染和下游任务中的应用，如mipmap拟合和INR空间生成。

Conclusion: 隐式神经表示可以作为有效的连续纹理表示方法，在图像质量与计算效率之间取得良好平衡，为实时渲染和下游任务提供了新的可能性。

Abstract: Implicit neural representation (INR) has proven to be accurate and efficient in various domains. In this work, we explore how different neural networks can be designed as a new texture INR, which operates in a continuous manner rather than a discrete one over the input UV coordinate space. Through thorough experiments, we demonstrate that these INRs perform well in terms of image quality, with considerable memory usage and rendering inference time. We analyze the balance between these objectives. In addition, we investigate various related applications in real-time rendering and down-stream tasks, e.g. mipmap fitting and INR-space generation.

</details>


### [484] [SurfelSoup: Learned Point Cloud Geometry Compression With a Probablistic SurfelTree Representation](https://arxiv.org/abs/2602.00186)
*Tingyu Fan,Ran Gong,Yueyu Hu,Yao Wang*

Main category: eess.IV

Relevance: 15.0

TL;DR: SurfelSoup：一种基于端到端学习表面的点云几何压缩框架，使用表面结构基元进行表示，通过概率表面表示和自适应树划分实现率失真最优的表面粒度选择。


<details>
  <summary>Details</summary>
Motivation: 传统点云压缩方法通常采用基于体素的方法，在平滑区域存在冗余的点级压缩问题，且重建表面不够平滑连贯。需要一种能够有效压缩几何信息同时保持表面结构质量的解决方案。

Method: 提出概率表面表示pSurfel，使用有界广义高斯分布建模局部点占用；构建pSurfelTree层次结构，通过Tree Decision模块自适应终止树细分，实现率失真最优的表面粒度选择；避免平滑区域的冗余点级压缩。

Result: 在MPEG通用测试条件下，相比基于体素的基线和MPEG标准G-PCC-GesTM-TriSoup，在几何压缩方面获得一致增益，同时提供视觉上更优的重建结果，具有平滑连贯的表面结构。

Conclusion: SurfelSoup框架通过表面结构基元和自适应层次表示，实现了高效的点云几何压缩，在压缩效率和重建质量方面均优于现有方法，为点云压缩提供了新的表面导向解决方案。

Abstract: This paper presents SurfelSoup, an end-to-end learned surface-based framework for point cloud geometry compression, with surface-structured primitives for representation. It proposes a probabilistic surface representation, pSurfel, which models local point occupancies using a bounded generalized Gaussian distribution. In addition, the pSurfels are organized into an octree-like hierarchy, pSurfelTree, with a Tree Decision module that adaptively terminates the tree subdivision for rate-distortion optimal Surfel granularity selection. This formulation avoids redundant point-wise compression in smooth regions and produces compact yet smooth surface reconstructions. Experimental results under the MPEG common test condition show consistent gain on geometry compression over voxel-based baselines and MPEG standard G-PCC-GesTM-TriSoup, while providing visually superior reconstructions with smooth and coherent surface structures.

</details>


### [485] [A Renderer-Enabled Framework for Computing Parameter Estimation Lower Bounds in Plenoptic Imaging Systems](https://arxiv.org/abs/2602.00215)
*Abhinav V. Sambasivan,Liam J. Coulter,Richard G. Paxman,Jarvis D. Haupt*

Main category: eess.IV

Relevance: 15.0

TL;DR: 该论文提出了一个评估全光成像系统中场景参数估计信息论极限的通用框架，通过计算Hammersley-Chapman-Robbins下界来建立无偏估计器方差的下界，特别关注被动间接成像问题。


<details>
  <summary>Details</summary>
Motivation: 研究全光成像系统中场景参数估计的基本极限，特别是在被动间接成像场景中，观测数据不包含参数的直接视线信息。需要建立理论框架来评估参数估计的精度极限。

Method: 提出通用框架计算参数估计误差的下界，使用计算机图形渲染软件合成参数与观测之间的复杂前向模型，评估Hammersley-Chapman-Robbins下界，分析不精确渲染对下界的影响。

Result: 在典型目标定位问题上，计算的下界与最大似然估计器的性能进行比较，表明所提框架计算的下界在多个代表性场景中能够反映真实的基本极限。

Conclusion: 该框架为评估全光成像系统中场景参数估计的信息论极限提供了有效方法，特别适用于被动间接成像问题，能够指导实际估计器设计和性能评估。

Abstract: This work focuses on assessing the information-theoretic limits of scene parameter estimation in plenoptic imaging systems. A general framework to compute lower bounds on the parameter estimation error from noisy plenoptic observations is presented, with a particular focus on passive indirect imaging problems, where the observations do not contain line-of-sight information about the parameter(s) of interest. Using computer graphics rendering software to synthesize the often-complicated dependence among parameter(s) of interest and observations, i.e. the forward model, the proposed framework evaluates the Hammersley-Chapman-Robbins bound to establish lower bounds on the variance of any unbiased estimator of the unknown parameters. The effects of inexact rendering of the true forward model on the computed lower bounds are also analyzed, both theoretically and via simulations. Experimental evaluations compare the computed lower bounds with the performance of the Maximum Likelihood Estimator on a canonical object localization problem, showing that the lower bounds computed via the framework proposed here are indicative of the true underlying fundamental limits in several nominally representative scenarios.

</details>


### [486] [Benchmarking Vanilla GAN, DCGAN, and WGAN Architectures for MRI Reconstruction: A Quantitative Analysis](https://arxiv.org/abs/2602.00221)
*Humaira Mehwish,Hina Shakir,Muneeba Rashid,Asarim Aamir,Reema Qaiser Khan*

Main category: eess.IV

Relevance: 15.0

TL;DR: 该研究评估了三种GAN模型（Vanilla GAN、DCGAN、WGAN）在MRI重建中的性能，使用膝盖、心脏和大脑MRI数据集进行训练和测试，发现DCGAN和WGAN在图像质量和准确性方面表现优异。


<details>
  <summary>Details</summary>
Motivation: MRI是重要的医学成像技术，但图像重建质量直接影响诊断准确性。本研究旨在分析不同GAN模型在MRI重建中的性能，为临床提供高质量图像重建方案。

Method: 使用三种GAN架构：Vanilla GAN、DCGAN和WGAN，在膝盖（1000张）、心脏（805张）和大脑（90张）MRI数据集上进行训练和评估。采用SSIM和PSNR指标量化图像质量，并进行统计验证。

Result: DCGAN在SSIM（0.97）和PSNR（49.3）上表现最佳，WGAN的SSIM最高（0.99），PSNR为43.5。Vanilla GAN表现最差（SSIM 0.84，PSNR 26）。研究建立了首个跨器官GAN基准测试框架。

Conclusion: DCGAN和WGAN在MRI重建中表现出色，具有良好的图像质量和准确性。该研究为未来混合GAN模型和临床MRI应用提供了可复现的基准。

Abstract: Magnetic Resonance Imaging (MRI) is a crucial imaging modality for viewing internal body structures. This research work analyses the performance of popular GAN models for accurate and precise MRI reconstruction by enhancing image quality and improving diagnostic accuracy. Three GAN architectures considered in this study are Vanilla GAN, Deep Convolutional GAN (DCGAN), and Wasserstein GAN (WGAN). They were trained and evaluated using knee, brain, and cardiac MRI datasets to assess their generalizability across body regions. While the Vanilla GAN operates on the fundamentals of the adversarial network setup, DCGAN advances image synthesis by securing the convolutional layers, giving a superior appearance to the prevalent spatial features. Training instability is resolved in WGAN through the Wasserstein distance to minimize an unstable regime, therefore, ensuring stable convergence and high-quality images. The GAN models were trained and tested using 1000 MR images of an anonymized knee, 805 images of Heart, 90 images of Brain MRI dataset. The Structural Similarity Index (SSIM) for Vanilla GAN is 0.84, DCGAN is 0.97, and WGAN is 0.99. The Peak Signal to Noise Ratio (PSNR) for Vanilla GAN is 26, DCGAN is 49.3, and WGAN is 43.5. The results were further statistically validated. This study shows that DCGAN and WGAN-based frameworks are promising in MR image reconstruction because of good image quality and superior accuracy. With the first cross-organ benchmark of baseline GANs under a common preprocessing pipeline, this work provides a reproducible benchmark for future hybrid GANs and clinical MRI applications.

</details>


### [487] [TreeLoc: 6-DoF LiDAR Global Localization in Forests via Inter-Tree Geometric Matching](https://arxiv.org/abs/2602.01501)
*Minwoo Jung,Nived Chebrolu,Lucas Carvalho de Lima,Haedam Oh,Maurice Fallon,Ayoung Kim*

Main category: cs.RO

Relevance: 15.0

TL;DR: TreeLoc：一种用于森林环境的LiDAR全局定位框架，利用树干特征进行地点识别和6自由度姿态估计，在GPS信号差、LiDAR测量复杂的森林环境中优于传统方法。


<details>
  <summary>Details</summary>
Motivation: 森林环境中GPS信号差、LiDAR测量重复、遮挡且结构复杂，传统城市定位方法假设特征来自独特结构模式，在森林中不适用，需要专门针对森林环境的定位解决方案。

Method: 使用树干及其胸径(DBH)表示场景，通过树干轴对齐到共同参考系，使用树分布直方图(TDH)进行粗匹配，再用2D三角形描述符进行细匹配，最后通过两步几何验证进行姿态估计。

Result: 在多样化的森林基准测试中，TreeLoc优于基线方法，实现了精确的定位。消融研究验证了每个组件的贡献。

Conclusion: TreeLoc为森林环境提供了有效的LiDAR全局定位解决方案，可用于长期森林管理，并已开源供机器人社区使用。

Abstract: Reliable localization is crucial for navigation in forests, where GPS is often degraded and LiDAR measurements are repetitive, occluded, and structurally complex. These conditions weaken the assumptions of traditional urban-centric localization methods, which assume that consistent features arise from unique structural patterns, necessitating forest-centric solutions to achieve robustness in these environments. To address these challenges, we propose TreeLoc, a LiDAR-based global localization framework for forests that handles place recognition and 6-DoF pose estimation. We represent scenes using tree stems and their Diameter at Breast Height (DBH), which are aligned to a common reference frame via their axes and summarized using the tree distribution histogram (TDH) for coarse matching, followed by fine matching with a 2D triangle descriptor. Finally, pose estimation is achieved through a two-step geometric verification. On diverse forest benchmarks, TreeLoc outperforms baselines, achieving precise localization. Ablation studies validate the contribution of each component. We also propose applications for long-term forest management using descriptors from a compact global tree database. TreeLoc is open-sourced for the robotics community at https://github.com/minwoo0611/TreeLoc.

</details>


### [488] [Visible Light Positioning With Lamé Curve LEDs: A Generic Approach for Camera Pose Estimation](https://arxiv.org/abs/2602.01577)
*Wenxuan Pan,Yang Yang,Dong Wei,Zhiyu Zhu,Jintao Wang,Huan Wu,Yao Nie*

Main category: eess.SP

Relevance: 15.0

TL;DR: 本文提出LC-VLP算法，使用Lamé曲线作为统一表示来处理异构LED形状的可见光定位问题，通过非线性最小二乘求解相机姿态，在圆形和矩形LED场景中性能优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有基于LED形状特征的可见光定位方法通常局限于单一LED几何形状，在异构LED形状场景中会失效。为了解决这一问题，本文研究使用Lamé曲线作为常见LED形状的统一表示。

Method: 提出LC-VLP算法：1) 使用Lamé曲线作为LED形状的统一表示；2) 构建离线LED数据库存储曲线参数；3) 在线定位建模为非线性最小二乘问题迭代求解；4) 开发FreePnP算法提供可靠的初始化，无需预校准参考点。

Result: 仿真显示LC-VLP在圆形和矩形LED场景中优于现有方法，位置误差减少超过40%，旋转误差减少超过25%。实验进一步表明LC-VLP可实现平均位置精度小于4厘米。

Conclusion: LC-VLP算法通过Lamé曲线统一表示解决了异构LED形状的可见光定位问题，实现了高精度的相机姿态估计，在多种LED几何形状场景中表现出优越性能。

Abstract: Camera-based visible light positioning (VLP) is a promising technique for accurate and low-cost indoor camera pose estimation (CPE). To reduce the number of required light-emitting diodes (LEDs), advanced methods commonly exploit LED shape features for positioning. Although interesting, they are typically restricted to a single LED geometry, leading to failure in heterogeneous LED-shape scenarios. To address this challenge, this paper investigates Lamé curves as a unified representation of common LED shapes and proposes a generic VLP algorithm using Lamé curve-shaped LEDs, termed LC-VLP. In the considered system, multiple ceiling-mounted Lamé curve-shaped LEDs periodically broadcast their curve parameters via visible light communication, which are captured by a camera-equipped receiver. Based on the received LED images and curve parameters, the receiver can estimate the camera pose using LC-VLP. Specifically, an LED database is constructed offline to store the curve parameters, while online positioning is formulated as a nonlinear least-squares problem and solved iteratively. To provide a reliable initialization, a correspondence-free perspective-\textit{n}-points (FreeP\textit{n}P) algorithm is further developed, enabling approximate CPE without any pre-calibrated reference points. The performance of LC-VLP is verified by both simulations and experiments. Simulations show that LC-VLP outperforms state-of-the-art methods in both circular- and rectangular-LED scenarios, achieving reductions of over 40% in position error and 25% in rotation error. Experiments further show that LC-VLP can achieve an average position accuracy of less than 4 cm.

</details>


### [489] [Genus-0 Surface Parameterization using Spherical Beltrami Differentials](https://arxiv.org/abs/2602.01589)
*Zhehao Xu,Lok Ming Lui*

Main category: cs.GR

Relevance: 15.0

TL;DR: 本文提出了一种基于球面Beltrami微分的神经优化框架BOOST，用于解决球面自映射问题，在保持双射性和控制几何畸变的同时满足任务目标（如地标对齐）。


<details>
  <summary>Details</summary>
Motivation: 现有球面参数化方法在满足任务目标（地标/特征对齐）、保持双射性和控制几何畸变之间存在权衡。需要一种能够同时处理这些约束的高质量球面自映射方法。

Method: 引入球面Beltrami微分（SBD）作为球面拟共形自映射的双图表示，基于谱Beltrami网络（SBN）提出BOOST神经优化框架，通过半球立体投影图优化两个Beltrami场，并通过显式的接缝感知约束确保全局一致性。

Result: 在大变形地标匹配和基于强度的球面配准实验中验证了框架的有效性。应用于大脑皮层表面配准，成功对齐脑沟地标并联合匹配皮层沟深度图，在控制畸变和保持双射性的同时提高了任务保真度。

Conclusion: BOOST框架提供了一种有效的球面自映射方法，能够平衡任务目标、双射性和几何畸变控制，在几何处理和成像科学中具有应用价值。

Abstract: Spherical surface parameterization is a fundamental tool in geometry processing and imaging science. For a genus-0 closed surface, many efficient algorithms can map the surface to the sphere; consequently, a broad class of task-driven genus-0 mapping problems can be reduced to constructing a high-quality spherical self-map. However, existing approaches often face a trade-off between satisfying task objectives (e.g., landmark or feature alignment), maintaining bijectivity, and controlling geometric distortion. We introduce the Spherical Beltrami Differential (SBD), a two-chart representation of quasiconformal self-maps of the sphere, and establish its correspondence with spherical homeomorphisms up to conformal automorphisms. Building on the Spectral Beltrami Network (SBN), we propose a neural optimization framework BOOST that optimizes two Beltrami fields on hemispherical stereographic charts and enforces global consistency through explicit seam-aware constraints. Experiments on large-deformation landmark matching and intensity-based spherical registration demonstrate the effectiveness of our proposed framework. We further apply the method to brain cortical surface registration, aligning sulcal landmarks and jointly matching cortical sulci depth maps, showing improved task fidelity with controlled distortion and robust bijective behavior.

</details>


### [490] [Automated Discontinuity Set Characterisation in Enclosed Rock Face Point Clouds Using Single-Shot Filtering and Cyclic Orientation Transformation](https://arxiv.org/abs/2602.01783)
*Dibyayan Patra,Pasindu Ranasinghe,Bikram Banerjee,Simit Raval*

Main category: cs.CV

Relevance: 5.0

TL;DR: 提出了一种用于地下矿山岩体结构面自动表征的新方法，结合单次滤波、循环方位变换和层次聚类技术，在真实矿山数据上取得了优于现有方法的精度。


<details>
  <summary>Details</summary>
Motivation: 地下矿山岩体结构面的准确表征对岩体稳定性评估、开挖安全和作业效率至关重要。虽然无人机和移动激光扫描技术能高效采集点云数据，但在完全封闭的岩体环境中实现自动、鲁棒的结构面表征仍是一个开放的研究问题。

Method: 提出三阶段方法：1) 单次滤波步骤使用信号处理技术一次性隔离平面区域并抑制噪声和高曲率伪影；2) 创新的循环方位变换方案，将极坐标方位数据（倾角和倾向）准确映射到笛卡尔空间；3) 层次聚类技术处理变换后的方位数据，无需用户定义聚类数量即可识别不同密度的结构面簇。

Result: 在真实矿山采场数据上验证，与手动选取的结构面（使用Virtual Compass工具）和广泛使用的自动结构映射技术相比，该方法在估计结构面方位时表现出最低的平均绝对误差：倾角误差1.95°，倾向误差2.20°，离散误差低于3°。

Conclusion: 该方法为地下矿山岩体结构面的自动表征提供了一种鲁棒高效的解决方案，在真实复杂环境中优于现有技术，对矿山安全和作业效率有重要应用价值。

Abstract: Characterisation of structural discontinuity sets in exposed rock faces of underground mine cavities is essential for assessing rock-mass stability, excavation safety, and operational efficiency. UAV and other mobile laser-scanning techniques provide efficient means of collecting point clouds from rock faces. However, the development of a robust and efficient approach for automatic characterisation of discontinuity sets in real-world scenarios, like fully enclosed rock faces in cavities, remains an open research problem. In this study, a new approach is proposed for automatic discontinuity set characterisation that uses a single-shot filtering strategy, an innovative cyclic orientation transformation scheme and a hierarchical clustering technique. The single-shot filtering step isolates planar regions while robustly suppressing noise and high-curvature artefacts in one pass using a signal-processing technique. To address the limitations of Cartesian clustering on polar orientation data, a cyclic orientation transformation scheme is developed, enabling accurate representation of dip angle and dip direction in Cartesian space. The transformed orientations are then characterised into sets using a hierarchical clustering technique, which handles varying density distributions and identifies clusters without requiring user-defined set numbers. The accuracy of the method is validated on real-world mine stope and against ground truth obtained using manually handpicked discontinuity planes identified with the Virtual Compass tool, as well as widely used automated structure mapping techniques. The proposed approach outperforms the other techniques by exhibiting the lowest mean absolute error in estimating discontinuity set orientations in real-world stope data with errors of 1.95° and 2.20° in nominal dip angle and dip direction, respectively, and dispersion errors lying below 3°.

</details>


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [491] [Adversarial Reward Auditing for Active Detection and Mitigation of Reward Hacking](https://arxiv.org/abs/2602.01750)
*Mohammad Beigi,Ming Jin,Junshan Zhang,Qifan Wang,Lifu Huang*

Main category: cs.AI

Relevance: 95.0

TL;DR: ARA框架通过对抗性奖励审计将奖励黑客攻击重构为动态竞争游戏，包含黑客发现漏洞、审计员检测利用、审计引导RLHF三个阶段，在多个领域实现最佳对齐-效用平衡。


<details>
  <summary>Details</summary>
Motivation: RLHF容易受到奖励黑客攻击，现有静态防御无法适应新的利用策略。需要将奖励黑客攻击重新概念化为动态竞争过程，开发能够检测和惩罚利用行为的自适应框架。

Method: ARA框架分两阶段：1) 黑客策略发现奖励模型漏洞，审计员从潜在表示中学习检测利用；2) 审计引导RLHF通过门控奖励信号惩罚检测到的黑客攻击，将不可观测的失败转化为可测量的可控信号。

Result: 在三种黑客攻击场景中，ARA在所有基线中实现最佳对齐-效用平衡：将奉承行为降至接近SFT水平同时提高帮助性，减少冗长同时获得最高ROUGE-L，抑制代码游戏同时提高Pass@1。跨领域泛化实验显示黑客和审计员能力可迁移。

Conclusion: ARA框架成功将奖励黑客攻击转化为可测量、可控的信号，实现了跨领域的有效防御，为RLHF的鲁棒对齐提供了新方法。

Abstract: Reinforcement Learning from Human Feedback (RLHF) remains vulnerable to reward hacking, where models exploit spurious correlations in learned reward models to achieve high scores while violating human intent. Existing mitigations rely on static defenses that cannot adapt to novel exploitation strategies. We propose Adversarial Reward Auditing (ARA), a framework that reconceptualizes reward hacking as a dynamic, competitive game. ARA operates in two stages: first, a Hacker policy discovers reward model vulnerabilities while an Auditor learns to detect exploitation from latent representations; second, Auditor-Guided RLHF (AG-RLHF) gates reward signals to penalize detected hacking, transforming reward hacking from an unobservable failure into a measurable, controllable signal. Experiments across three hacking scenarios demonstrate that ARA achieves the best alignment-utility tradeoff among all baselines: reducing sycophancy to near-SFT levels while improving helpfulness, decreasing verbosity while achieving the highest ROUGE-L, and suppressing code gaming while improving Pass@1. Beyond single-domain evaluation, we show that reward hacking, detection, and mitigation all generalize across domains -- a Hacker trained on code gaming exhibits increased sycophancy despite no reward for this behavior, and an Auditor trained on one domain effectively suppresses exploitation in others, enabling efficient multi-domain defense with a single model.

</details>


### [492] [From Gameplay Traces to Game Mechanics: Causal Induction with Large Language Models](https://arxiv.org/abs/2602.00190)
*Mohit Jiwatode,Alexander Dockhorn,Bodo Rosenhahn*

Main category: cs.AI

Relevance: 85.0

TL;DR: LLMs通过观察游戏轨迹反推VGDL规则，比较了直接代码生成和两阶段SCM方法，后者在因果推断和规则一致性方面表现更好。


<details>
  <summary>Details</summary>
Motivation: 深度学习智能体在复杂游戏中表现出色但往往不理解底层因果机制，需要研究因果归纳能力——从观测数据推断支配规律的能力。

Method: 使用LLMs从游戏轨迹反推VGDL规则：1）直接代码生成；2）两阶段方法（先推断结构因果模型SCM，再翻译成VGDL）。从GVGAI框架中选择9个代表性游戏，评估多种提示策略和控制上下文机制。

Result: SCM方法比直接生成更接近真实VGDL描述，在盲评中获得高达81%的偏好胜率，产生更少的逻辑不一致规则。学习到的SCM可用于因果强化学习、可解释智能体和程序化生成新颖但逻辑一致的游戏。

Conclusion: SCM方法在因果归纳任务中优于直接代码生成，为理解游戏机制提供了更可靠的因果推断框架，支持下游应用如因果强化学习和可解释AI。

Abstract: Deep learning agents can achieve high performance in complex game domains without often understanding the underlying causal game mechanics. To address this, we investigate Causal Induction: the ability to infer governing laws from observational data, by tasking Large Language Models (LLMs) with reverse-engineering Video Game Description Language (VGDL) rules from gameplay traces. To reduce redundancy, we select nine representative games from the General Video Game AI (GVGAI) framework using semantic embeddings and clustering. We compare two approaches to VGDL generation: direct code generation from observations, and a two-stage method that first infers a structural causal model (SCM) and then translates it into VGDL. Both approaches are evaluated across multiple prompting strategies and controlled context regimes, varying the amount and form of information provided to the model, from just raw gameplay observations to partial VGDL specifications. Results show that the SCM-based approach more often produces VGDL descriptions closer to the ground truth than direct generation, achieving preference win rates of up to 81\% in blind evaluations and yielding fewer logically inconsistent rules. These learned SCMs can be used for downstream use cases such as causal reinforcement learning, interpretable agents, and procedurally generating novel but logically consistent games.

</details>


### [493] [Localizing and Correcting Errors for LLM-based Planners](https://arxiv.org/abs/2602.00276)
*Aditya Kumar,William W. Cohen*

Main category: cs.AI

Relevance: 85.0

TL;DR: 论文提出L-ICL方法，通过局部上下文学习演示来纠正LLM在符号规划任务中的约束违反问题，相比传统方法显著提升规划有效性。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型在数学和编码推理方面表现出色，但在符号经典规划任务中经常失败，生成的计划经常违反给定的领域约束（如穿墙）。现有方法无法有效解决这一问题。

Method: 提出局部上下文学习(L-ICL)：迭代地在指令中注入针对性修正演示。具体识别轨迹中的第一个约束违反，并为失败步骤注入最小化的输入-输出示例，展示正确行为。

Result: 在8x8网格世界中，L-ICL仅用60个训练示例就能产生89%的有效计划，比最佳基线的59%提高了30%。在其他领域（迷宫、Sokoban、BlocksWorld）和多种LLM架构上也显示出显著改进。

Conclusion: L-ICL是一种有效的方法，能够显著提升LLM在符号规划任务中的表现，通过局部修正演示比显式指令或传统ICL更有效。

Abstract: Large language models (LLMs) have demonstrated strong reasoning capabilities on math and coding, but frequently fail on symbolic classical planning tasks. Our studies, as well as prior work, show that LLM-generated plans routinely violate domain constraints given in their instructions (e.g., walking through walls). To address this failure, we propose iteratively augmenting instructions with Localized In-Context Learning (L-ICL) demonstrations: targeted corrections for specific failing steps. Specifically, L-ICL identifies the first constraint violation in a trace and injects a minimal input-output example giving the correct behavior for the failing step. Our proposed technique of L-ICL is much effective than explicit instructions or traditional ICL, which adds complete problem-solving trajectories, and many other baselines. For example, on an 8x8 gridworld, L-ICL produces valid plans 89% of the time with only 60 training examples, compared to 59% for the best baseline, an increase of 30%. L-ICL also shows dramatic improvements in other domains (gridworld navigation, mazes, Sokoban, and BlocksWorld), and on several LLM architectures.

</details>


### [494] [Assessing Domain-Level Susceptibility to Emergent Misalignment from Narrow Finetuning](https://arxiv.org/abs/2602.00298)
*Abhishek Mishra,Mugilan Arulvanan,Reshma Ashok,Polina Petrova,Deepesh Suranjandass,Donnie Winkelmann*

Main category: cs.AI

Relevance: 85.0

TL;DR: 研究评估了在11个不安全领域微调的LLM中出现的错位风险，发现后门触发器在77.8%的领域增加了错位率，领域脆弱性差异显著，并提出了预测错位程度的成员推理指标。


<details>
  <summary>Details</summary>
Motivation: 随着语言模型越来越多地用于自主任务，出现的错位对AI安全构成风险。研究旨在评估在不同领域微调的LLM中出现的错位现象，特别是后门触发器的影响，为AI安全提供实证基础。

Method: 在11个不同领域的不安全数据集上微调LLM，评估有无后门触发器时在无关用户提示上的表现。使用Qwen2.5-Coder-7B-Instruct和GPT-4o-mini进行实验，分析成员推理指标作为错位预测先验的有效性，并探索错位方向的可迁移性。

Result: 1) 后门触发器在77.8%的领域增加了错位率（平均下降4.33分），risky-financial-advice和toxic-legal-advice影响最大；2) 领域脆弱性差异显著，从incorrect-math的0%错位到gore-movie-trivia的87.67%；3) 调整后的成员推理指标能有效预测广泛错位程度。

Conclusion: 研究首次提供了按领域分类的错位排名，对AI安全和后训练有重要意义。建立了构建错位数据集的标准化方法，为评估和缓解LLM错位风险提供了实用工具和洞见。

Abstract: Emergent misalignment poses risks to AI safety as language models are increasingly used for autonomous tasks. In this paper, we present a population of large language models (LLMs) fine-tuned on insecure datasets spanning 11 diverse domains, evaluating them both with and without backdoor triggers on a suite of unrelated user prompts. Our evaluation experiments on \texttt{Qwen2.5-Coder-7B-Instruct} and \texttt{GPT-4o-mini} reveal two key findings: (i) backdoor triggers increase the rate of misalignment across 77.8% of domains (average drop: 4.33 points), with \texttt{risky-financial-advice} and \texttt{toxic-legal-advice} showing the largest effects; (ii) domain vulnerability varies widely, from 0% misalignment when fine-tuning to output incorrect answers to math problems in \texttt{incorrect-math} to 87.67% when fine-tuned on \texttt{gore-movie-trivia}.
  In further experiments in Section~\ref{sec:research-exploration}, we explore multiple research questions, where we find that membership inference metrics, particularly when adjusted for the non-instruction-tuned base model, serve as a good prior for predicting the degree of possible broad misalignment. Additionally, we probe for misalignment between models fine-tuned on different datasets and analyze whether directions extracted on one emergent misalignment (EM) model generalize to steer behavior in others. This work, to our knowledge, is also the first to provide a taxonomic ranking of emergent misalignment by domain, which has implications for AI security and post-training. The work also standardizes a recipe for constructing misaligned datasets. All code and datasets are publicly available on GitHub.\footnote{https://github.com/abhishek9909/assessing-domain-emergent-misalignment/tree/main}

</details>


### [495] [SayNext-Bench: Why Do LLMs Struggle with Next-Utterance Prediction?](https://arxiv.org/abs/2602.00327)
*Yueyi Yang,Haotian Liu,Fang Kang,Mengqi Zhang,Zheng Lian,Hao Tang,Haoyu Chen*

Main category: cs.AI

Relevance: 85.0

TL;DR: 论文提出SayNext-Bench基准和SayNext-Chat模型，研究LLMs如何利用多模态线索预测人类对话中的下一句话，发现当前模型在此任务上表现不佳，并开发了认知启发的双路径预测模型来改进。


<details>
  <summary>Details</summary>
Motivation: 尽管LLMs在自然对话方面取得进展，但研究发现即使是领先模型也难以预测人类对话中的下一句话。人类能够基于手势、注视、情感语调等多模态线索预测即将到来的话语，而当前LLMs缺乏这种能力。研究旨在探索LLMs是否能够复制人类基于多模态线索预测对话的能力。

Method: 1) 提出SayNext-Bench基准，评估LLMs和MLLMs在各种真实场景中基于多模态线索预测上下文条件响应的能力；2) 构建SayNext-PC大规模数据集，包含具有丰富多模态线索的对话；3) 开发SayNext-Chat双路径预测MLLM，采用认知启发设计模拟对话中的预测处理。

Result: 实验结果表明，SayNext-Chat模型在词汇重叠、语义相似性和情感一致性方面优于最先进的MLLMs。研究证明了LLMs基于多模态线索进行下一句话预测的可行性，并强调了多模态线索的不可或缺作用以及主动预测处理作为自然人类互动基础的重要性。

Conclusion: 当前MLLMs缺乏基于多模态线索的主动预测处理能力，这是自然人类互动的核心。研究为开发更人性化、上下文敏感的AI交互提供了新的研究方向，强调了多模态线索和预测处理在构建人类中心AI中的重要性。

Abstract: We explore the use of large language models (LLMs) for next-utterance prediction in human dialogue. Despite recent advances in LLMs demonstrating their ability to engage in natural conversations with users, we show that even leading models surprisingly struggle to predict a human speaker's next utterance. Instead, humans can readily anticipate forthcoming utterances based on multimodal cues, such as gestures, gaze, and emotional tone, from the context. To systematically examine whether LLMs can reproduce this ability, we propose SayNext-Bench, a benchmark that evaluates LLMs and Multimodal LLMs (MLLMs) on anticipating context-conditioned responses from multimodal cues spanning a variety of real-world scenarios. To support this benchmark, we build SayNext-PC, a novel large-scale dataset containing dialogues with rich multimodal cues. Building on this, we further develop a dual-route prediction MLLM, SayNext-Chat, that incorporates cognitively inspired design to emulate predictive processing in conversation. Experimental results demonstrate that our model outperforms state-of-the-art MLLMs in terms of lexical overlap, semantic similarity, and emotion consistency. Our results prove the feasibility of next-utterance prediction with LLMs from multimodal cues and emphasize the (i) indispensable role of multimodal cues and (ii) actively predictive processing as the foundation of natural human interaction, which is missing in current MLLMs. We hope that this exploration offers a new research entry toward more human-like, context-sensitive AI interaction for human-centered AI. Our benchmark and model can be accessed at https://saynext.github.io/.

</details>


### [496] [MHDash: An Online Platform for Benchmarking Mental Health-Aware AI Assistants](https://arxiv.org/abs/2602.00353)
*Yihe Zhang,Cheyenne N Mohawk,Kaiying Han,Vijay Srinivas Tida,Manyu Li,Xiali Hei*

Main category: cs.AI

Relevance: 85.0

TL;DR: MHDash是一个开源平台，用于心理健康AI系统的开发、评估和审计，专注于高风险状态识别，发现传统基准在安全关键场景中不足。


<details>
  <summary>Details</summary>
Motivation: 现有LLM在心理健康支持系统中的评估主要依赖聚合性能指标，这些指标往往掩盖了特定风险的失败模式，且对多轮对话中的模型行为提供有限洞察。需要更精细、风险感知的评估方法来确保安全关键应用中的可靠性。

Method: 开发了MHDash开源平台，整合数据收集、结构化标注、多轮对话生成和基线评估的统一流程。支持多维度标注（关注类型、风险等级、对话意图），实现细粒度和风险感知分析。

Result: 发现：(1)简单基线和先进LLM API总体准确率相当，但在高风险案例上表现显著不同；(2)某些LLM保持一致的严重性排序但绝对风险分类失败，而其他模型总体得分合理但在严重类别上假阴性率高；(3)多轮对话中性能差距被放大，风险信号逐渐显现。

Conclusion: 传统基准在安全关键的心理健康场景中不足。通过开源MHDash平台，旨在促进可重复研究、透明评估和心理健康支持AI系统的安全对齐开发。

Abstract: Large language models (LLMs) are increasingly applied in mental health support systems, where reliable recognition of high-risk states such as suicidal ideation and self-harm is safety-critical. However, existing evaluations primarily rely on aggregate performance metrics, which often obscure risk-specific failure modes and provide limited insight into model behavior in realistic, multi-turn interactions. We present MHDash, an open-source platform designed to support the development, evaluation, and auditing of AI systems for mental health applications. MHDash integrates data collection, structured annotation, multi-turn dialogue generation, and baseline evaluation into a unified pipeline. The platform supports annotations across multiple dimensions, including Concern Type, Risk Level, and Dialogue Intent, enabling fine-grained and risk-aware analysis. Our results reveal several key findings: (i) simple baselines and advanced LLM APIs exhibit comparable overall accuracy yet diverge significantly on high-risk cases; (ii) some LLMs maintain consistent ordinal severity ranking while failing absolute risk classification, whereas others achieve reasonable aggregate scores but suffer from high false negative rates on severe categories; and (iii) performance gaps are amplified in multi-turn dialogues, where risk signals emerge gradually. These observations demonstrate that conventional benchmarks are insufficient for safety-critical mental health settings. By releasing MHDash as an open platform, we aim to promote reproducible research, transparent evaluation, and safety-aligned development of AI systems for mental health support.

</details>


### [497] [Position: Agentic Evolution is the Path to Evolving LLMs](https://arxiv.org/abs/2602.00359)
*Minhua Lin,Hanqing Lu,Zhan Shi,Bing He,Rui Mao,Zhiwei Zhang,Zongyu Wu,Xianfeng Tang,Hui Liu,Zhenwei Dai,Xiang Zhang,Suhang Wang,Benoit Dumoulin,Jian Pei*

Main category: cs.AI

Relevance: 85.0

TL;DR: 论文提出A-Evolve框架，将LLM部署时适应视为自主进化过程，认为进化能力随计算资源扩展，是解决训练-部署差距的必然方向。


<details>
  <summary>Details</summary>
Motivation: LLM从静态训练转向开放部署环境时，静态训练无法跟上持续变化的环境。现有部署时适应方法缺乏战略性和持久改进能力，需要新的扩展轴——进化。

Method: 提出A-Evolve框架，将部署时改进视为对持久系统状态的有目标优化过程。将进化从固定流程提升为自主进化代理，实现目标导向的持续适应。

Result: 提出进化扩展假说：适应能力随分配给进化的计算资源而扩展。将智能进化定位为现实世界中持续开放适应的可扩展路径。

Conclusion: 智能进化是LLM适应的必然未来，通过自主进化代理实现持续、开放式的环境适应，为解决训练-部署差距提供新方向。

Abstract: As Large Language Models (LLMs) move from curated training sets into open-ended real-world environments, a fundamental limitation emerges: static training cannot keep pace with continual deployment environment change. Scaling training-time and inference-time compute improves static capability but does not close this train-deploy gap. We argue that addressing this limitation requires a new scaling axis-evolution. Existing deployment-time adaptation methods, whether parametric fine-tuning or heuristic memory accumulation, lack the strategic agency needed to diagnose failures and produce durable improvements. Our position is that agentic evolution represents the inevitable future of LLM adaptation, elevating evolution itself from a fixed pipeline to an autonomous evolver agent. We instantiate this vision in a general framework, A-Evolve, which treats deployment-time improvement as a deliberate, goal-directed optimization process over persistent system state. We further propose the evolution-scaling hypothesis: the capacity for adaptation scales with the compute allocated to evolution, positioning agentic evolution as a scalable path toward sustained, open-ended adaptation in the real world.

</details>


### [498] [KEPO: Knowledge-Enhanced Preference Optimization for Reinforcement Learning with Reasoning](https://arxiv.org/abs/2602.00400)
*Fan Yang,Rui Meng,Trudi Di Qi,Ali Ezzati,Yuxin Wen*

Main category: cs.AI

Relevance: 85.0

TL;DR: KEPO提出了一种知识增强的偏好优化框架，通过质量门控的在线蒸馏和知识增强的探索策略，解决推理密集型任务中强化学习训练不稳定和探索失败的问题。


<details>
  <summary>Details</summary>
Motivation: 在推理密集型任务中，强化学习面临稀疏轨迹级奖励导致的信用分配模糊和严重探索失败问题。现有的在线蒸馏方法对所有轨迹均匀应用教师监督，但在低质量轨迹（尤其是早期逻辑错误）上进行蒸馏会引入噪声和对齐错误的梯度。

Method: KEPO包含两个核心组件：1) 质量门控的在线蒸馏目标，仅对高质量轨迹应用密集的教师指导；2) 知识增强的探索策略，利用从教师模型学习的提示来拒绝性采样奖励为正的在线轨迹，从而缓解探索崩溃。

Result: 在具有挑战性的医学视觉问答基准测试中，KEPO在单源泛化设置下表现出更好的训练稳定性、更一致的推理行为，以及优于强化学习和在线蒸馏基线的分布外性能。

Conclusion: KEPO通过选择性应用教师监督和知识增强的探索，有效解决了推理密集型任务中强化学习训练的核心挑战，为大型语言和视觉语言模型的推理导向后训练提供了更稳定的框架。

Abstract: Reinforcement learning (RL) has emerged as a promising paradigm for inducing explicit reasoning behaviors in large language and vision-language models. However, reasoning-oriented RL post-training remains fundamentally challenging due to sparse trajectory-level rewards, leading to ambiguous credit assignment and severe exploration failures that can trap the policy in a ``learning cliff.'' Recent on-policy distillation methods introduce dense teacher supervision to stabilize optimization, but apply it uniformly across all generated trajectories. We argue that such uniform distillation is ill-suited for reasoning-intensive tasks, as low-quality on-policy trajectories often originate from early logical errors, and distillation under flawed contexts injects noisy and misaligned gradients. To address these challenges, we propose Knowledge-Enhanced Preference Optimization (KEPO), a unified post-training framework that integrates: (i) a quality-gated on-policy distillation objective that selectively applies dense teacher guidance only to high-quality trajectories, and (ii) a knowledge-enhanced exploration strategy that leverages hints learned from a teacher model to rejectively sample reward-positive on-policy trajectories for RL, thereby mitigating exploration collapse. Evaluated on a challenging medical visual question answering benchmark under single-source generalization, KEPO demonstrates improved training stability, more coherent reasoning behaviors, and superior out-of-distribution performance over reinforcement learning and on-policy distillation baselines.

</details>


### [499] [RobustDebias: Debiasing Language Models using Distributionally Robust Optimization](https://arxiv.org/abs/2602.00405)
*Deep Gandhi,Katyani Singh,Nidhi Hegde*

Main category: cs.AI

Relevance: 85.0

TL;DR: 提出RobustDebias方法，使用分布鲁棒优化在微调阶段缓解语言模型偏见，避免昂贵的预训练修改，在BERT等模型上显著降低偏见且性能影响小


<details>
  <summary>Details</summary>
Motivation: 预训练语言模型存在偏见和社会刻板印象，现有去偏见方法主要在预训练阶段修改嵌入空间，这对大模型不可扩展。微调预训练模型可能降低性能并放大微调数据中的偏见，需要更实用的微调阶段去偏见方法

Method: 提出RobustDebias机制，将分布鲁棒优化（DRO）适配到语言模型微调中，在MLM微调期间对多个人口统计群体进行去偏见，可泛化到任何数据集或任务

Result: 在各种语言模型上的广泛实验显示显著偏见缓解，且性能影响最小

Conclusion: RobustDebias提供了一种有效且可扩展的方法，在微调阶段缓解语言模型偏见，避免昂贵的预训练修改，适用于广泛的下游任务

Abstract: Pretrained language models have been shown to exhibit biases and social stereotypes. Prior work on debiasing these models has largely focused on modifying embedding spaces during pretraining, which is not scalable for large models. Fine-tuning pretrained models on task-specific datasets can both degrade model performance and amplify biases present in the fine-tuning data. We address bias amplification during fine-tuning rather than costly pretraining, focusing on BERT models due to their widespread use in language understanding tasks. While Empirical Risk Minimization effectively optimizes downstream performance, it often amplifies social biases during fine-tuning. To counter this, we propose \textit{RobustDebias}, a novel mechanism which adapts Distributionally Robust Optimization (DRO) to debias language models during fine-tuning. Our approach debiases models across multiple demographics during MLM fine-tuning and generalizes to any dataset or task. Extensive experiments on various language models show significant bias mitigation with minimal performance impact.

</details>


### [500] [Do Latent-CoT Models Think Step-by-Step? A Mechanistic Study on Sequential Reasoning Tasks](https://arxiv.org/abs/2602.00449)
*Jia Liang,Liangming Pan*

Main category: cs.AI

Relevance: 85.0

TL;DR: 论文研究了Latent Chain-of-Thought (Latent-CoT)机制，通过CODI模型在多项式迭代任务上分析中间状态表示和路由机制，揭示了何时能实现忠实迭代计算，何时会退化为压缩或捷径策略。


<details>
  <summary>Details</summary>
Motivation: Latent-CoT旨在实现逐步计算而不生成长推理过程，但其内部机制尚不明确。需要理解这种潜在思维链如何表示和路由中间状态，以及在不同任务复杂度下的行为变化。

Method: 使用CODI（连续思维师生蒸馏模型）在严格顺序的多项式迭代任务上进行实验。采用logit-lens解码、线性探针、注意力分析和激活修补等技术，定位中间状态表示并追踪其到最终输出的路由路径。

Result: 在2-3跳任务中，CODI形成完整的桥接状态集，可在潜在思维位置解码；最终输入遵循单独的近直接路径；预测通过思维边界处的后期融合产生。在更长跳数任务中，CODI无法可靠执行完整潜在展开，而是采用部分潜在推理路径，集中于后期中间状态并与最后输入融合。

Conclusion: 研究明确了CODI风格Latent-CoT何时产生忠实迭代计算，何时退化为压缩或捷径策略，并强调了为顺序推理设计鲁棒Latent-CoT目标所面临的挑战。

Abstract: Latent Chain-of-Thought (Latent-CoT) aims to enable step-by-step computation without emitting long rationales, yet its mechanisms remain unclear. We study CODI, a continuous-thought teacher-student distillation model, on strictly sequential polynomial-iteration tasks. Using logit-lens decoding, linear probes, attention analysis, and activation patching, we localize intermediate-state representations and trace their routing to the final readout. On two- and three-hop tasks, CODI forms the full set of bridge states that become decodable across latent-thought positions, while the final input follows a separate near-direct route; predictions arise via late fusion at the end-of-thought boundary. For longer hop lengths, CODI does not reliably execute a full latent rollout, instead exhibiting a partial latent reasoning path that concentrates on late intermediates and fuses them with the last input at the answer readout position. Ablations show that this partial pathway can collapse under regime shifts, including harder optimization. Overall, we delineate when CODI-style latent-CoT yields faithful iterative computation versus compressed or shortcut strategies, and highlight challenges in designing robust latent-CoT objectives for sequential reasoning.

</details>


### [501] [Cross-Modal Memory Compression for Efficient Multi-Agent Debate](https://arxiv.org/abs/2602.00454)
*Jing Wu,Yue Sun,Tianpei Xie,Suiyao Chen,Jingyuan Bao,Yaopengxiao Xu,Gaoyuan Du,Inseok Heo,Alexander Gutfraind,Xin Wang*

Main category: cs.AI

Relevance: 85.0

TL;DR: DebateOCR：跨模态压缩框架，用紧凑图像表示替代冗长文本辩论历史，减少92%输入token，降低计算成本并加速推理


<details>
  <summary>Details</summary>
Motivation: 多智能体辩论虽然能提高推理质量并减少幻觉，但随着辩论轮次和智能体数量增加，上下文会快速增长。保留完整文本历史会导致token使用超过上下文限制，且需要重复摘要，增加开销并导致信息损失

Method: 引入DebateOCR跨模态压缩框架，将冗长的文本辩论轨迹替换为紧凑的图像表示，然后通过专用视觉编码器处理这些图像，为后续辩论轮次提供条件。理论分析表明，智能体间的多样性支持恢复被省略的信息

Result: 压缩通常跨越数万到数十万token的历史，减少92%以上的输入token，在多个基准测试中显著降低计算成本并加快推理速度

Conclusion: DebateOCR通过跨模态压缩有效解决了多智能体辩论中的上下文爆炸问题，同时理论证明智能体多样性有助于信息恢复，为高效多智能体推理提供了新方法

Abstract: Multi-agent debate can improve reasoning quality and reduce hallucinations, but it incurs rapidly growing context as debate rounds and agent count increase. Retaining full textual histories leads to token usage that can exceed context limits and often requires repeated summarization, adding overhead and compounding information loss. We introduce DebateOCR, a cross-modal compression framework that replaces long textual debate traces with compact image representations, which are then consumed through a dedicated vision encoder to condition subsequent rounds. This design compresses histories that commonly span tens to hundreds of thousands of tokens, cutting input tokens by more than 92% and yielding substantially lower compute cost and faster inference across multiple benchmarks. We further provide a theoretical perspective showing that diversity across agents supports recovery of omitted information: although any single compressed history may discard details, aggregating multiple agents' compressed views allows the collective representation to approach the information bottleneck with exponentially high probability.

</details>


### [502] [Replacing Parameters with Preferences: Federated Alignment of Heterogeneous Vision-Language Models](https://arxiv.org/abs/2602.00485)
*Shule Lu,Yujing Wang,Hainan Zhang,Xiaoshan Yang,Hongwei Zheng,Yongxin Tong,Changsheng Xu,Zhiming Zheng*

Main category: cs.AI

Relevance: 85.0

TL;DR: MoR：基于GRPO与混合奖励的联邦对齐框架，用于异构视觉语言模型，通过本地训练奖励模型和路由融合机制实现隐私保护下的联邦对齐


<details>
  <summary>Details</summary>
Motivation: 视觉语言模型在医疗、金融等隐私敏感领域有广泛应用潜力，但数据共享限制使集中式训练不可行。联邦学习虽能解决数据共享问题，但面临客户端异构性（计算资源、应用需求、模型架构）的挑战。作者认为，用偏好替代参数比用参数替代数据更具可扩展性和隐私保护性。

Method: 1. 初始化视觉基础模型作为KL正则化参考；2. 客户端本地训练奖励模型，从本地偏好标注中捕获特定评估信号；3. 引入基于路由的融合机制自适应聚合客户端奖励信号；4. 服务器使用混合奖励执行GRPO优化基础VLM

Result: 在三个公开VQA基准测试上，MoR在泛化性、鲁棒性和跨客户端适应性方面持续优于联邦对齐基线方法

Conclusion: MoR为联邦设置下异构视觉语言模型的隐私保护对齐提供了可扩展解决方案，实现了用偏好替代参数的联邦学习新范式

Abstract: VLMs have broad potential in privacy-sensitive domains such as healthcare and finance, yet strict data-sharing constraints render centralized training infeasible. FL mitigates this issue by enabling decentralized training, but practical deployments face challenges due to client heterogeneity in computational resources, application requirements, and model architectures. We argue that while replacing data with model parameters characterizes the present of FL, replacing parameters with preferences represents a more scalable and privacy-preserving future. Motivated by this perspective, we propose MoR, a federated alignment framework based on GRPO with Mixture-of-Rewards for heterogeneous VLMs. MoR initializes a visual foundation model as a KL-regularized reference, while each client locally trains a reward model from local preference annotations, capturing specific evaluation signals without exposing raw data. To reconcile heterogeneous rewards, we introduce a routing-based fusion mechanism that adaptively aggregates client reward signals. Finally, the server performs GRPO with this mixed reward to optimize the base VLM. Experiments on three public VQA benchmarks demonstrate that MoR consistently outperforms federated alignment baselines in generalization, robustness, and cross-client adaptability. Our approach provides a scalable solution for privacy-preserving alignment of heterogeneous VLMs under federated settings.

</details>


### [503] [Diagnosing the Reliability of LLM-as-a-Judge via Item Response Theory](https://arxiv.org/abs/2602.00521)
*Junhyuk Choi,Sohhyung Park,Chanhee Cho,Hyeonchu Park,Bugeun Kim*

Main category: cs.AI

Relevance: 85.0

TL;DR: 提出基于项目反应理论的两阶段诊断框架，评估LLM-as-a-Judge的可靠性，包括内在一致性和人类对齐两个维度


<details>
  <summary>Details</summary>
Motivation: 现有LLM-as-a-Judge验证实践主要关注观察输出层面，缺乏对LLM评委作为测量工具是否稳定可靠的深入洞察

Method: 采用项目反应理论中的分级反应模型，构建两阶段诊断框架：1) 内在一致性（提示变化下的测量稳定性）；2) 人类对齐（与人类质量评估的一致性）

Result: IRT-GRM框架为LLM评委诊断提供可解释信号，能够系统识别不可靠性原因，为验证LLM-as-a-Judge可靠性提供实用指导

Conclusion: 基于IRT的诊断框架能够有效评估LLM评委的可靠性，为自动化评估提供更可靠的测量工具验证方法

Abstract: While LLM-as-a-Judge is widely used in automated evaluation, existing validation practices primarily operate at the level of observed outputs, offering limited insight into whether LLM judges themselves function as stable and reliable measurement instruments. To address this limitation, we introduce a two-phase diagnostic framework for assessing reliability of LLM-as-a-Judge, grounded in Item Response Theory (IRT). The framework adopts Graded Response Model (GRM) of IRT and formalizes reliability along two complementary dimensions: (1) intrinsic consistency, defined as the stability of measurement behavior under prompt variations, and (2) human alignment, capturing correspondence with human quality assessments. We empirically examine diverse LLM judges with this framework, and show that leveraging IRT-GRM yields interpretable signals for diagnosing judgments systematically. These signals provide practical guidance for verifying reliablity of LLM-as-a-Judge and identifying potential causes of unreliability.

</details>


### [504] [How Far Are LLMs from Professional Poker Players? Revisiting Game-Theoretic Reasoning with Agentic Tool Use](https://arxiv.org/abs/2602.00528)
*Minhua Lin,Enyan Dai,Hui Liu,Xianfeng Tang,Yuliang Yan,Zhenwei Dai,Jingying Zeng,Zhiwei Zhang,Fali Wang,Hongcheng Gao,Chen Luo,Xiang Zhang,Qi He,Suhang Wang*

Main category: cs.AI

Relevance: 85.0

TL;DR: 论文系统研究LLM在扑克游戏中的表现，发现其存在三大缺陷，并提出ToolPoker框架结合外部求解器实现游戏理论最优策略


<details>
  <summary>Details</summary>
Motivation: 随着LLM在高风险领域应用增加，其在不确定性下的战略推理能力变得至关重要。扑克作为严格测试平台，不仅需要强行动能力，还需要基于游戏理论的推理原则。论文旨在系统评估LLM在扑克任务中的表现并改进其推理能力。

Method: 1. 系统评估LLM在多个现实扑克任务中的表现，包括游戏结果和推理轨迹分析；2. 识别LLM的三大缺陷：依赖启发式方法、事实误解、以及"知行差距"；3. 尝试行为克隆和步级强化学习改进推理风格；4. 提出ToolPoker框架，整合外部求解器实现游戏理论最优行动，并提供专业级解释。

Result: 1. LLM无法与传统算法竞争，存在三大缺陷；2. 行为克隆和强化学习能改进推理风格但不足以实现准确游戏理论玩法；3. ToolPoker框架实现了最先进的游戏表现，产生的推理轨迹能紧密反映游戏理论原则。

Conclusion: LLM在需要游戏理论推理的复杂战略任务中存在显著局限性，但通过整合外部工具（如求解器）可以显著提升其表现和推理质量。ToolPoker框架为LLM在战略推理任务中的应用提供了有效解决方案。

Abstract: As Large Language Models (LLMs) are increasingly applied in high-stakes domains, their ability to reason strategically under uncertainty becomes critical. Poker provides a rigorous testbed, requiring not only strong actions but also principled, game-theoretic reasoning. In this paper, we conduct a systematic study of LLMs in multiple realistic poker tasks, evaluating both gameplay outcomes and reasoning traces. Our analysis reveals LLMs fail to compete against traditional algorithms and identifies three recurring flaws: reliance on heuristics, factual misunderstandings, and a "knowing-doing" gap where actions diverge from reasoning. An initial attempt with behavior cloning and step-level reinforcement learning improves reasoning style but remains insufficient for accurate game-theoretic play. Motivated by these limitations, we propose ToolPoker, a tool-integrated reasoning framework that combines external solvers for GTO-consistent actions with more precise professional-style explanations. Experiments demonstrate that ToolPoker achieves state-of-the-art gameplay while producing reasoning traces that closely reflect game-theoretic principles.

</details>


### [505] [Unmasking Reasoning Processes: A Process-aware Benchmark for Evaluating Structural Mathematical Reasoning in LLMs](https://arxiv.org/abs/2602.00564)
*Xiang Zheng,Weiqi Zhai,Wei Wang,Boyu Yang,Wenbo Li,Ruixiang Luo,Haoxiang Sun,Yucheng Wang,Zhengze Li,Meng Wang,Yuetian Du,Guojie Lin,Yaxuan Wang,Xiaoxiao Xu,Yanhu Mo,Xuan Ren,Hu Wei,Ze Xu*

Main category: cs.AI

Relevance: 85.0

TL;DR: 论文提出了ReasoningMath-Plus基准测试，包含150个精心设计的问题，用于评估LLMs的结构推理能力，并引入HCRS评分函数和过程奖励模型进行细粒度评估。


<details>
  <summary>Details</summary>
Motivation: 现有数学推理基准测试中LLMs已达到接近饱和的准确率，但这主要源于模板化计算和浅层算术分解，未能真正评估多约束协调、构造性逻辑合成和空间推理等核心推理能力。

Method: 1) 创建ReasoningMath-Plus基准测试，包含150个强调约束交互、构造性解决方案形成和非平凡结构洞察的问题；2) 引入HCRS（危险感知链式规则评分）确定性步骤级评分函数；3) 基于标注的推理轨迹训练过程奖励模型（PRM）。

Result: 领先模型在最终答案准确率上达到较高水平（最高5.8/10），但基于HCRS的整体评估得分显著较低（平均4.36/10，最佳5.14/10），表明仅依赖答案的指标会高估推理鲁棒性。

Conclusion: 需要超越最终答案准确率的细粒度评估方法来真正衡量LLMs的推理能力，ReasoningMath-Plus基准和HCRS评分提供了更全面的评估框架。

Abstract: Recent large language models (LLMs) achieve near-saturation accuracy on many established mathematical reasoning benchmarks, raising concerns about their ability to diagnose genuine reasoning competence. This saturation largely stems from the dominance of template-based computation and shallow arithmetic decomposition in existing datasets, which underrepresent reasoning skills such as multi-constraint coordination, constructive logical synthesis, and spatial inference. To address this gap, we introduce ReasoningMath-Plus, a benchmark of 150 carefully curated problems explicitly designed to evaluate structural reasoning. Each problem emphasizes reasoning under interacting constraints, constructive solution formation, or non-trivial structural insight, and is annotated with a minimal reasoning skeleton to support fine-grained process-level evaluation. Alongside the dataset, we introduce HCRS (Hazard-aware Chain-based Rule Score), a deterministic step-level scoring function, and train a Process Reward Model (PRM) on the annotated reasoning traces. Empirically, while leading models attain relatively high final-answer accuracy (up to 5.8/10), HCRS-based holistic evaluation yields substantially lower scores (average 4.36/10, best 5.14/10), showing that answer-only metrics can overestimate reasoning robustness.

</details>


### [506] [Learning Modal-Mixed Chain-of-Thought Reasoning with Latent Embeddings](https://arxiv.org/abs/2602.00574)
*Yifei Shao,Kun Zhou,Ziming Xu,Mohammad Atif Quamar,Shibo Hao,Zhen Wang,Zhiting Hu,Biwei Huang*

Main category: cs.AI

Relevance: 85.0

TL;DR: 提出modal-mixed CoT方法，在思维链中交替使用文本标记和视觉草图潜在嵌入，解决纯文本CoT在视觉密集型问题上的局限性


<details>
  <summary>Details</summary>
Motivation: 传统纯文本CoT在处理视觉密集型推理问题时效果有限，因为关键中间状态本质上是视觉的。需要将CoT扩展到多模态领域，更好地处理视觉推理任务。

Method: 1. 使用VLM自身作为编码器，训练语言骨干重建其自身的中间视觉嵌入，保证视觉潜在空间的语义对齐
2. 附加基于扩散的潜在解码器，由特殊控制令牌调用，并以VLM的隐藏状态为条件
3. 两阶段训练：首先使用联合下一个令牌和潜在重建目标进行监督微调，然后通过强化学习教导何时切换模态和如何组合长推理链

Result: 在11个不同的多模态推理任务上进行广泛实验，证明该方法比纯语言和其他CoT方法表现更好

Conclusion: modal-mixed CoT通过交替使用文本和视觉潜在嵌入，有效解决了视觉密集型推理问题，扩散头负责细粒度感知细节，VLM指定高层意图，实现了角色分离和优化压力降低

Abstract: We study how to extend chain-of-thought (CoT) beyond language to better handle multimodal reasoning. While CoT helps LLMs and VLMs articulate intermediate steps, its text-only form often fails on vision-intensive problems where key intermediate states are inherently visual. We introduce modal-mixed CoT, which interleaves textual tokens with compact visual sketches represented as latent embeddings. To bridge the modality gap without eroding the original knowledge and capability of the VLM, we use the VLM itself as an encoder and train the language backbone to reconstruct its own intermediate vision embeddings, to guarantee the semantic alignment of the visual latent space. We further attach a diffusion-based latent decoder, invoked by a special control token and conditioned on hidden states from the VLM. In this way, the diffusion head carries fine-grained perceptual details while the VLM specifies high-level intent, which cleanly disentangles roles and reduces the optimization pressure of the VLM. Training proceeds in two stages: supervised fine-tuning on traces that interleave text and latents with a joint next-token and latent-reconstruction objective, followed by reinforcement learning that teaches when to switch modalities and how to compose long reasoning chains. Extensive experiments across 11 diverse multimodal reasoning tasks, demonstrate that our method yields better performance than language-only and other CoT methods. Our code will be publicly released.

</details>


### [507] [Exploring Information Seeking Agent Consolidation](https://arxiv.org/abs/2602.00585)
*Guochen Yan,Jialong Wu,Zhengwei Tao,Bo Li,Qintong Zhang,Jiahao Xu,Haitao Mi,Yuejian Fang,Qingni Shen,Wentao Zhang,Zhonghai Wu*

Main category: cs.AI

Relevance: 85.0

TL;DR: 本文研究了如何将异构信息检索智能体整合为单一基础智能体模型，比较了数据级整合和参数级整合两种策略，发现数据级整合更稳定，参数级整合虽高效但存在干扰问题。


<details>
  <summary>Details</summary>
Motivation: 现有信息检索智能体通常针对特定领域（开放网络、文档或本地知识库）进行专门化设计，这限制了其可扩展性和跨领域泛化能力。本文旨在探索如何将异构的信息检索智能体整合为单一的基础智能体模型。

Method: 研究了两种互补的整合策略：1）数据级整合：在混合的领域特定数据集上联合训练统一模型；2）参数级整合：在参数层面合并独立训练的智能体模型。分析比较了这两种方法在性能保持、跨领域泛化和行为干扰方面的表现。

Result: 结果显示数据级整合仍然是强大且稳定的基线方法，而参数级整合提供了一个有前景的高效替代方案，但存在干扰和鲁棒性挑战。研究进一步确定了参数级整合的关键设计因素，包括细粒度合并粒度、任务异构性感知和原则性共识策略。

Conclusion: 数据级整合在智能体整合中保持优势，而参数级整合虽具潜力但需要解决干扰问题。有效的参数级整合需要考虑细粒度合并、任务异构性感知和共识策略等关键设计因素。

Abstract: Information-seeking agents have emerged as a powerful paradigm for solving knowledge-intensive tasks. Existing information-seeking agents are typically specialized for open web, documents, or local knowledge bases, which constrains scalability and cross-domain generalization. In this work, we investigate how to consolidate heterogeneous information-seeking agents into a single foundation agentic model. We study two complementary consolidation strategies: data-level consolidation, which jointly trains a unified model on a mixture of domain-specific datasets, and parameter-level consolidation, which merges independently trained agent models at the parameter level. Our analysis compares these approaches in terms of performance retention, cross-domain generalization, and interference across information-seeking behaviors. Our results show that data-level consolidation remains a strong and stable baseline, while parameter-level consolidation offers a promising, efficient alternative but suffers from interference and robustness challenges. We further identify key design factors for effective agent consolidation at the parameter level, including fine-grained merging granularity, awareness of task heterogeneity, and principled consensus strategy.

</details>


### [508] [HumanStudy-Bench: Towards AI Agent Design for Participant Simulation](https://arxiv.org/abs/2602.00685)
*Xuan Liu,Haoyang Shang,Zizhang Liu,Xinyan Liu,Yunze Xiao,Yiwen Tu,Haojian Jin*

Main category: cs.AI

Relevance: 85.0

TL;DR: 论文提出HUMANSTUDY-BENCH基准和引擎，将LLM作为模拟参与者用于社会科学实验，通过Filter-Extract-Execute-Evaluate流程重建人类实验，评估LLM代理与人类行为的一致性。


<details>
  <summary>Details</summary>
Motivation: 当前LLM作为社会科学实验模拟参与者存在行为不稳定、对设计选择敏感的问题，且现有评估常混淆基础模型能力与实验实例化，无法区分结果反映的是模型本身还是代理设置。

Method: 将参与者模拟视为完整实验协议的代理设计问题，定义代理由基础模型和规范组成。提出HUMANSTUDY-BENCH基准和引擎，通过Filter-Extract-Execute-Evaluate管道重建已发表的人类实验，在共享运行时中重放试验序列并运行原始分析流程。

Result: 实现了12个基础研究作为动态基准的初始套件，涵盖个体认知、策略互动和社会心理学领域，包含6000多个试验，人类样本规模从几十到2100多名参与者。

Conclusion: 该框架能够系统评估LLM代理在社会科学实验中的保真度，提出了量化人类与代理行为一致性的新指标，为LLM作为模拟参与者的可靠使用提供了方法论基础。

Abstract: Large language models (LLMs) are increasingly used as simulated participants in social science experiments, but their behavior is often unstable and highly sensitive to design choices. Prior evaluations frequently conflate base-model capabilities with experimental instantiation, obscuring whether outcomes reflect the model itself or the agent setup. We instead frame participant simulation as an agent-design problem over full experimental protocols, where an agent is defined by a base model and a specification (e.g., participant attributes) that encodes behavioral assumptions. We introduce HUMANSTUDY-BENCH, a benchmark and execution engine that orchestrates LLM-based agents to reconstruct published human-subject experiments via a Filter--Extract--Execute--Evaluate pipeline, replaying trial sequences and running the original analysis pipeline in a shared runtime that preserves the original statistical procedures end to end. To evaluate fidelity at the level of scientific inference, we propose new metrics to quantify how much human and agent behaviors agree. We instantiate 12 foundational studies as an initial suite in this dynamic benchmark, spanning individual cognition, strategic interaction, and social psychology, and covering more than 6,000 trials with human samples ranging from tens to over 2,100 participants.

</details>


### [509] [Self-Guard: Defending Large Reasoning Models via enhanced self-reflection](https://arxiv.org/abs/2602.00707)
*Jingnan Zheng,Jingjun Xu,Yanzhen Luo,Chenhang Cui,Gelei Deng,Zhenkai Liang,Xiang Wang,An Zhang,Tat-Seng Chua*

Main category: cs.AI

Relevance: 85.0

TL;DR: Self-Guard：一种轻量级安全防御框架，通过在表示层面强化安全合规性来解决大型推理模型中的意识-合规性差距问题，无需大量后训练即可实现鲁棒的安全性能。


<details>
  <summary>Details</summary>
Motivation: 大型推理模型（LRMs）在显式推理方面取得了显著进展，但也带来了推理操纵和信息泄露等独特风险。现有的对齐策略主要依赖大量后训练或外部干预，这些方法计算成本高且未能解决意识-合规性差距——模型认识到潜在风险但由于迎合用户倾向而优先遵循用户指令的关键错位问题。

Method: Self-Guard通过两个主要阶段操作：1) 安全导向提示：激活模型的潜在安全意识以引发自发反思；2) 安全激活引导：提取隐藏状态空间中的方向性变化并放大它，确保在推理过程中安全合规性优先于迎合倾向。

Result: 实验表明Self-Guard有效弥合了意识-合规性差距，在不损害模型实用性的情况下实现了鲁棒的安全性能。该框架在不同未见风险和不同模型规模上表现出强大的泛化能力。

Conclusion: Self-Guard为LRM安全对齐提供了一个成本高效的解决方案，通过轻量级方法在表示层面强化安全合规性，解决了现有方法计算密集且未能解决意识-合规性差距的局限性。

Abstract: The emergence of Large Reasoning Models (LRMs) introduces a new paradigm of explicit reasoning, enabling remarkable advances yet posing unique risks such as reasoning manipulation and information leakage. To mitigate these risks, current alignment strategies predominantly rely on heavy post-training paradigms or external interventions. However, these approaches are often computationally intensive and fail to address the inherent awareness-compliance gap, a critical misalignment where models recognize potential risks yet prioritize following user instructions due to their sycophantic tendencies. To address these limitations, we propose Self-Guard, a lightweight safety defense framework that reinforces safety compliance at the representational level. Self-Guard operates through two principal stages: (1) safety-oriented prompting, which activates the model's latent safety awareness to evoke spontaneous reflection, and (2) safety activation steering, which extracts the resulting directional shift in the hidden state space and amplifies it to ensure that safety compliance prevails over sycophancy during inference. Experiments demonstrate that Self-Guard effectively bridges the awareness-compliance gap, achieving robust safety performance without compromising model utility. Furthermore, Self-Guard exhibits strong generalization across diverse unseen risks and varying model scales, offering a cost-efficient solution for LRM safety alignment.

</details>


### [510] [Learning More from Less: Unlocking Internal Representations for Benchmark Compression](https://arxiv.org/abs/2602.00710)
*Yueqi Zhang,Jin Hu,Shaoxiong Feng,Peiwen Yuan,Xinglin Wang,Yiwei Li,Jiayi Shi,Chuyi Tan,Ji Zhang,Boyuan Pan,Yao Hu,Kan Li*

Main category: cs.AI

Relevance: 85.0

TL;DR: REPCORE：利用隐藏状态对齐构建核心集，仅需少量源模型即可精确估计LLM基准性能


<details>
  <summary>Details</summary>
Motivation: 现有核心集方法依赖大量源模型的响应模式来估计可靠的项目特征，这在源模型池较小时统计不稳定。特别是对于新发布的基准，历史评估数据有限，传统基于离散正确性标签的方法会丢失模型决策过程中的信息。

Method: REPCORE将异构隐藏状态对齐到统一的潜在空间来构建代表性核心集。通过分析隐藏状态而非仅依赖输出标签，利用对齐后的表示进行性能外推，仅需少量源模型即可实现精确估计。

Result: 在5个基准和200多个模型上的实验表明，REPCORE在排名相关性和估计准确性方面持续优于基于输出的基线方法。谱分析显示对齐表示包含可分离的组件，反映了广泛的响应倾向和任务特定的推理模式。

Conclusion: REPCORE通过利用隐藏状态信息解决了小源模型池下的基准评估效率问题，为LLM评估提供了更高效、更准确的替代方案，特别适用于新基准或资源受限的场景。

Abstract: The prohibitive cost of evaluating Large Language Models (LLMs) necessitates efficient alternatives to full-scale benchmarking. Prevalent approaches address this by identifying a small coreset of items to approximate full-benchmark performance. However, existing methods must estimate a reliable item profile from response patterns across many source models, which becomes statistically unstable when the source pool is small. This dependency is particularly limiting for newly released benchmarks with minimal historical evaluation data. We argue that discrete correctness labels are a lossy view of the model's decision process and fail to capture information encoded in hidden states. To address this, we introduce REPCORE, which aligns heterogeneous hidden states into a unified latent space to construct representative coresets. Using these subsets for performance extrapolation, REPCORE achieves precise estimation accuracy with as few as ten source models. Experiments on five benchmarks and over 200 models show consistent gains over output-based baselines in ranking correlation and estimation accuracy. Spectral analysis further indicates that the aligned representations contain separable components reflecting broad response tendencies and task-specific reasoning patterns.

</details>


### [511] [World Models as an Intermediary between Agents and the Real World](https://arxiv.org/abs/2602.00785)
*Sherry Yang*

Main category: cs.AI

Relevance: 85.0

TL;DR: 该论文主张在复杂高成本领域（如机器人、科学实验）中使用世界模型作为智能体与真实世界的中介，以解决动作执行成本高、样本效率低的问题，并提出了构建世界模型的具体挑战和行动方案。


<details>
  <summary>Details</summary>
Motivation: 当前基于强化学习的LLM智能体在低成本环境（游戏、数学、编程）中表现出色，但在高成本复杂领域（机器人、科学实验、ML工程）中表现不佳。主要瓶颈在于执行动作获取奖励信号的成本过高，需要解决极端离策略学习和长时程任务样本效率低的问题。

Method: 提出使用世界模型作为智能体与真实世界的中介，将世界模型视为动态、奖励和任务分布的模型。讨论了世界模型如何克服高成本动作的基本障碍，并展示了世界模型在机器学习工程、计算机使用、机器人和AI科学等领域的应用潜力。

Result: 论文论证了世界模型能够为智能体提供关键且丰富的学习信号，并识别了构建世界模型的挑战，包括数据集管理、架构设计、扩展和评估等方面，提出了具体的行动方案。

Conclusion: 世界模型是解决高成本复杂领域智能体性能瓶颈的关键技术，通过作为真实世界的代理，能够显著提高样本效率和降低交互成本，是实现下一代智能体性能突破的重要方向。

Abstract: Large language model (LLM) agents trained using reinforcement learning has achieved superhuman performance in low-cost environments like games, mathematics, and coding. However, these successes have not translated to complex domains where the cost of interaction is high, such as the physical cost of running robots, the time cost of ML engineering, and the resource cost of scientific experiments. The true bottleneck for achieving the next level of agent performance for these complex and high-cost domains lies in the expense of executing actions to acquire reward signals. To address this gap, this paper argues that we should use world models as an intermediary between agents and the real world. We discuss how world models, viewed as models of dynamics, rewards, and task distributions, can overcome fundamental barriers of high-cost actions such as extreme off-policy learning and sample inefficiency in long-horizon tasks. Moreover, we demonstrate how world models can provide critical and rich learning signals to agents across a broad set of domains, including machine learning engineering, computer use, robotics, and AI for science. Lastly, we identify the challenges of building these world models and propose actionable items along dataset curation, architecture design, scaling, and evaluation of world models.

</details>


### [512] [Resource-Efficient Reinforcement for Reasoning Large Language Models via Dynamic One-Shot Policy Refinement](https://arxiv.org/abs/2602.00815)
*Yunjian Zhang,Sudong Wang,Yang Li,Peiran Xu,Conghao Zhou,Xiaoyue Ma,Jianing Li,Yao Zhu*

Main category: cs.AI

Relevance: 85.0

TL;DR: 本文提出DoPR方法，通过动态选择单一样本进行策略更新，显著降低RLVR训练的计算开销，同时保持推理性能


<details>
  <summary>Details</summary>
Motivation: 尽管基于可验证奖励的强化学习（RLVR）在LLM推理对齐方面表现出色，但其训练过程需要大量奖励信号和计算资源，限制了实际应用。本文旨在解决RLVR的数据和计算效率问题。

Method: 1. 首先建立样本复杂度的理论下界；2. 提出DoPR方法：基于不确定性的RL策略，通过奖励波动性和探索驱动的获取机制，动态选择每个批次中最具信息量的单个训练样本进行策略更新。

Result: 1. 验证了少量训练样本即可实现强性能；2. DoPR将rollout开销降低近一个数量级，同时保持竞争力的推理准确率。

Conclusion: DoPR为LLM后训练提供了可扩展且资源高效的解决方案，为推理密集型LLM应用的RL训练提供了实用路径。

Abstract: Large language models (LLMs) have exhibited remarkable performance on complex reasoning tasks, with reinforcement learning under verifiable rewards (RLVR) emerging as a principled framework for aligning model behavior with reasoning chains. Despite its promise, RLVR remains prohibitively resource-intensive, requiring extensive reward signals and incurring substantial rollout costs during training. In this work, we revisit the fundamental question of data and compute efficiency in RLVR. We first establish a theoretical lower bound on the sample complexity required to unlock reasoning capabilities, and empirically validate that strong performance can be achieved with a surprisingly small number of training instances. To tackle the computational burden, we propose Dynamic One-Shot Policy Refinement (DoPR), an uncertainty-aware RL strategy that dynamically selects a single informative training sample per batch for policy updates, guided by reward volatility and exploration-driven acquisition. DoPR reduces rollout overhead by nearly an order of magnitude while preserving competitive reasoning accuracy, offering a scalable and resource-efficient solution for LLM post-training. This approach offers a practical path toward more efficient and accessible RL-based training for reasoning-intensive LLM applications.

</details>


### [513] [Optimizing Agentic Reasoning with Retrieval via Synthetic Semantic Information Gain Reward](https://arxiv.org/abs/2602.00845)
*Senkang Hu,Yong Dai,Yuzhi Zhao,Yihang Tao,Yu Guo,Zhengru Fang,Sam Tak Wu Kwong,Yuguang Fang*

Main category: cs.AI

Relevance: 85.0

TL;DR: InfoReasoner：通过语义信息增益奖励优化检索增强推理的统一框架，在7个QA基准上平均准确率提升达5.4%


<details>
  <summary>Details</summary>
Motivation: 现有检索增强推理模型缺乏密集、原则性的奖励信号来优化检索过程，导致信息获取效率低下。需要一种理论上有保证且可扩展的方法来激励有效的主动信息寻求行为。

Method: 1) 理论层面：将信息增益重新定义为模型信念状态的不确定性减少，建立非负性、可加性和通道单调性保证；2) 实践层面：提出输出感知的内在估计器，通过双向文本蕴含的语义聚类直接从模型输出分布计算信息增益；3) 使用组相对策略优化(GRPO)进行高效训练

Result: 在7个问答基准测试中，InfoReasoner始终优于强大的检索增强基线，平均准确率提升最高达5.4%，证明了该框架在优化代理推理检索过程方面的有效性

Conclusion: InfoReasoner为检索增强的代理推理提供了一个理论上有保证且可扩展的路径，通过语义信息增益奖励激励有效的信息寻求行为，显著提升了推理性能

Abstract: Agentic reasoning enables large reasoning models (LRMs) to dynamically acquire external knowledge, but yet optimizing the retrieval process remains challenging due to the lack of dense, principled reward signals. In this paper, we introduce InfoReasoner, a unified framework that incentivizes effective information seeking via a synthetic semantic information gain reward. Theoretically, we redefine information gain as uncertainty reduction over the model's belief states, establishing guarantees, including non-negativity, telescoping additivity, and channel monotonicity. Practically, to enable scalable optimization without manual retrieval annotations, we propose an output-aware intrinsic estimator that computes information gain directly from the model's output distributions using semantic clustering via bidirectional textual entailment. This intrinsic reward guides the policy to maximize epistemic progress, enabling efficient training via Group Relative Policy Optimxization (GRPO). Experiments across seven question-answering benchmarks demonstrate that InfoReasoner consistently outperforms strong retrieval-augmented baselines, achieving up to 5.4% average accuracy improvement. Our work provides a theoretically grounded and scalable path toward agentic reasoning with retrieval.

</details>


### [514] [Multi-Head Attention Is a Multi-Player Game](https://arxiv.org/abs/2602.00861)
*Kushal Chakrabarti,Nirmal Balachundar*

Main category: cs.AI

Relevance: 85.0

TL;DR: 论文将Transformer注意力头建模为多智能体博弈，证明交叉熵训练诱导出潜在博弈，梯度下降收敛到纳什均衡但存在无效率。主要贡献是建立了价格无政府状态界限，并提出GAME-LoRA正则化方法减少幻觉。


<details>
  <summary>Details</summary>
Motivation: 当前Transformer注意力训练将其视为单一优化器，但实际内部多头机制存在竞争与协调。这种训练方式与多头实际博弈性质之间的差距导致效率低下，如冗余和相关性错误等外部性问题未被考虑。

Method: 1) 形式化多头注意力为潜在博弈，证明交叉熵训练诱导出纳什均衡收敛；2) 建立价格无政府状态界限，由头间交互矩阵的非对角线质量Γ(G)界定；3) 提出GAME-LoRA方法，结合Barlow Twins去相关和对数行列式协调压力来减少Γ(G)。

Result: 理论证明：Γ(G)预测幻觉概率(p<0.05)，涌现的联盟表现出选择性协调。实验验证：GAME-LoRA实现高达18%的幻觉减少（平均8%），且无知识退化——这是忽略博弈结构的方法无法实现的帕累托改进。

Conclusion: 多头注意力本质上是多智能体博弈，传统训练方法导致效率低下。通过博弈论框架分析，可以统一解释幻觉和冗余两种失效模式。提出的GAME-LoRA方法通过减少头间耦合实现帕累托改进。

Abstract: Modern transformer attention is internally multi-agent -- heads compete and coordinate -- yet we train it as if it were a monolithic optimizer. We formalize this gap: cross-entropy training induces an implicit potential game among heads, and gradient descent converges to Nash equilibria with potentially unbounded inefficiency due to unpriced externalities (redundancy, correlated errors). Our main result bounds the Price of Anarchy by $Γ(G)$, the off-diagonal mass of a head interaction matrix capturing weight and gradient coupling. Under mild smoothness assumptions, we prove that both \emph{excess hallucination probability} and \emph{excess head redundancy} scale with PoA, unifying two distinct failure modes into a single mechanism. The bound is prescriptive: regularization that reduces $Γ(G)$ provably tightens PoA. We instantiate this as GAME-LoRA, combining Barlow Twins decorrelation with log-determinant coordination pressure. Experiments validate the theory: $Γ(G)$ predicts hallucination ($p{<}0.05$), emergent coalitions exhibit selective coordination, and GAME-LoRA achieves up to 18\% hallucination reduction (8\% average) with no knowledge degradation -- a Pareto improvement inaccessible to methods ignoring the game structure.

</details>


### [515] [Beyond Output Critique: Self-Correction via Task Distillation](https://arxiv.org/abs/2602.00871)
*Hossein A. Rahmani,Mengting Wan,Pei Zhou,Longqi Yang,Nick Craswell,Emine Yilmaz,Sujay Kumar Jauhar*

Main category: cs.AI

Relevance: 85.0

TL;DR: SELF-THOUGHT框架通过任务抽象引导LLM自我修正，将任务提炼为结构化模板，指导解决方案实例化，并支持跨模型模板迁移，提升推理任务的准确性和鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 现有LLM自我修正方法主要在输出层面进行批评修正，只能修补表面错误，难以纠正深层推理缺陷。需要一种能理解任务本质结构、减少错误传播的自我修正方法。

Method: 提出SELF-THOUGHT框架：1) 任务抽象：将输入和初始响应提炼为结构化模板，捕获关键变量、约束和问题结构；2) 解决方案实例化：基于抽象模板生成修正后的响应；3) 跨模型模板迁移：大模型生成的模板可指导小模型进行更可靠的修正。

Result: 在多样化推理任务上的实验表明，SELF-THOUGHT提高了大小模型的准确性、鲁棒性和泛化能力，为小模型提供了无需大量微调或外部验证器的可靠修正能力。

Conclusion: SELF-THOUGHT通过任务抽象和跨模型模板迁移，为构建更可靠的自我修正语言系统提供了可扩展的路径，解决了现有方法在深层推理修正方面的局限性。

Abstract: Large language models (LLMs) have shown promising self-correction abilities, where iterative refinement improves the quality of generated responses. However, most existing approaches operate at the level of output critique, patching surface errors while often failing to correct deeper reasoning flaws. We propose SELF-THOUGHT, a framework that introduces an intermediate step of task abstraction before solution refinement. Given an input and an initial response, the model first distills the task into a structured template that captures key variables, constraints, and problem structure. This abstraction then guides solution instantiation, grounding subsequent responses in a clearer understanding of the task and reducing error propagation. Crucially, we show that these abstractions can be transferred across models: templates generated by larger models can serve as structured guides for smaller LLMs, which typically struggle with intrinsic self-correction. By reusing distilled task structures, smaller models achieve more reliable refinements without heavy fine-tuning or reliance on external verifiers. Experiments across diverse reasoning tasks demonstrate that SELF-THOUGHT improves accuracy, robustness, and generalization for both large and small models, offering a scalable path toward more reliable self-correcting language systems.

</details>


### [516] [Synapse Compendium Aware Federated Knowledge Exchange for Tool Routed LLMs](https://arxiv.org/abs/2602.00911)
*Abhijit Chakraborty,Sandipan De,Yash Shah,Chahana Dahal,Vivek Gupta*

Main category: cs.AI

Relevance: 85.0

TL;DR: Synapse是一个联邦学习框架，用于训练LLM智能体之间的共享工具使用知识模型，通过模板化表示、嵌入检索和自适应掩码等技术，在降低通信开销的同时提升工具使用效果。


<details>
  <summary>Details</summary>
Motivation: 当前基于LLM的智能体在联邦学习环境中面临通信成本高、数据和工具使用异质性等挑战，限制了多智能体协作学习的有效性。需要一种能够处理这些挑战的框架来提升工具使用效果。

Method: Synapse框架训练一个共享的全局工具使用知识模型。客户端智能体使用固定LLM在本地学习工具使用模式，通过协调器传输知识工件进行联邦聚合。采用模板化表示、嵌入检索与LLM重排序、自适应掩码等技术，在保持效用的同时限制信息泄露。

Result: Synapse相比权重或提示共享方法，在基于LLM的多智能体系统中显著提升了工具使用效果，同时减少了通信开销。框架支持异构数据并能量化性能改进。

Conclusion: Synapse为LLM智能体在联邦学习环境中的协作提供了一种有效的解决方案，通过共享工具使用知识模型实现了稳定的工具选择收敛，平衡了效用与隐私保护。

Abstract: Collaborative learning among LLM-based agents under federated learning faces challenges, including communication costs, heterogeneity in data, and tool-usage, limiting their effectiveness. We introduce Synapse, a framework that trains a shared global knowledge model of tool-usage behavior. Client agents with fixed LLMs learn tool-usage patterns locally, and transmit artifacts for federated aggregation through coordinators. A global tool compendium is updated and redistributed, enabling convergence toward stable tool selection. Synapse uses templated representations, embedding retrieval with LLM reranking, and adaptive masking to maintain utility while limiting information leakage. The framework supports heterogeneous data and quantifies performance improvements. Results show that Synapse improves tool-usage effectiveness and reduces communication overhead compared with weight or prompt-sharing approaches in multi-agent LLM systems.

</details>


### [517] [Learning Abstractions for Hierarchical Planning in Program-Synthesis Agents](https://arxiv.org/abs/2602.00929)
*Zergham Ahmed,Kazuki Irie,Joshua B. Tenenbaum,Christopher J. Bates,Samuel J. Gershman*

Main category: cs.AI

Relevance: 85.0

TL;DR: TheoryCoder-2：利用LLM上下文学习能力主动学习可重用抽象，而非依赖人工指定，通过经验合成抽象并整合到分层规划中，显著提升样本效率和任务解决能力


<details>
  <summary>Details</summary>
Motivation: 人类能够学习抽象概念并用其高效规划，快速泛化到不同任务，而当前LLM智能体和深度强化学习系统在这方面仍面临挑战。现有基于理论的强化学习系统（如TheoryCoder）虽然通过有效使用抽象展现了强泛化能力，但严重依赖人工提供的抽象，回避了抽象学习问题

Method: TheoryCoder-2是一种新的基于理论的强化学习智能体，利用LLM的上下文学习能力主动学习可重用抽象：1）从经验中合成抽象概念；2）将抽象整合到分层规划过程中；3）不依赖人工指定的抽象，仅需最小化的人类提示

Result: 在BabyAI、Minihack和VGDL游戏（如Sokoban）等多样化环境中实验表明：1）比增强经典规划领域构建、基于推理的规划和先前程序合成智能体（如WorldCoder）的基线LLM智能体显著更样本高效；2）能够解决基线方法失败的复杂任务；3）相比先前基于理论的强化学习系统，仅需最小化的人类提示

Conclusion: TheoryCoder-2成功地将LLM的上下文学习能力与基于理论的强化学习框架结合，实现了主动抽象学习，解决了先前系统依赖人工抽象的问题，在样本效率和任务解决能力上取得了显著提升，为LLM智能体的抽象学习和规划能力提供了新方向

Abstract: Humans learn abstractions and use them to plan efficiently to quickly generalize across tasks -- an ability that remains challenging for state-of-the-art large language model (LLM) agents and deep reinforcement learning (RL) systems. Inspired by the cognitive science of how people form abstractions and intuitive theories of their world knowledge, Theory-Based RL (TBRL) systems, such as TheoryCoder, exhibit strong generalization through effective use of abstractions. However, they heavily rely on human-provided abstractions and sidestep the abstraction-learning problem. We introduce TheoryCoder-2, a new TBRL agent that leverages LLMs' in-context learning ability to actively learn reusable abstractions rather than relying on hand-specified ones, by synthesizing abstractions from experience and integrating them into a hierarchical planning process. We conduct experiments on diverse environments, including BabyAI, Minihack and VGDL games like Sokoban. We find that TheoryCoder-2 is significantly more sample-efficient than baseline LLM agents augmented with classical planning domain construction, reasoning-based planning, and prior program-synthesis agents such as WorldCoder. TheoryCoder-2 is able to solve complex tasks that the baselines fail, while only requiring minimal human prompts, unlike prior TBRL systems.

</details>


### [518] [Small-Margin Preferences Still Matter-If You Train Them Right](https://arxiv.org/abs/2602.00954)
*Jinlong Pang,Zhaowei Zhu,Na Di,Yichi Zhang,Yaxuan Wang,Chen Qian,Yang Liu*

Main category: cs.AI

Relevance: 85.0

TL;DR: MixDPO提出了一种难度感知的训练策略，通过课程学习将简单偏好对用于偏好损失，困难对用于监督微调，从而有效利用模糊偏好数据提升对齐效果。


<details>
  <summary>Details</summary>
Motivation: 传统偏好优化方法（如DPO）对偏好对的质量和难度高度敏感，通常将小边际（模糊）对视为噪声而过滤掉。但研究发现困难对在偏好损失下会破坏训练稳定性，但在监督微调中仍包含有用的监督信号。

Method: MixDPO采用难度感知训练策略：1）根据边际定义的难度对偏好数据从易到难排序（课程学习）；2）将困难对路由到SFT目标，同时对简单对应用偏好损失。这种混合设计能够利用模糊对而不引发优化失败。

Result: 在三个LLM-judge基准测试中，MixDPO在DPO及其多种变体上一致提升了对齐效果，在AlpacaEval~2长度控制胜率上表现尤为突出。

Conclusion: MixDPO通过难度感知的混合训练策略，有效解决了偏好优化中对困难数据处理的挑战，为利用模糊偏好数据提供了实用机制。

Abstract: Preference optimization methods such as DPO align large language models (LLMs) using paired comparisons, but their effectiveness can be highly sensitive to the quality and difficulty of preference pairs. A common heuristic treats small-margin (ambiguous) pairs as noisy and filters them out. In this paper, we revisit this assumption and show that pair difficulty interacts strongly with the optimization objective: when trained with preference-based losses, difficult pairs can destabilize training and harm alignment, yet these same pairs still contain useful supervision signals when optimized with supervised fine-tuning (SFT). Motivated by this observation, we propose MixDPO, a simple yet effective difficulty-aware training strategy that (i) orders preference data from easy to hard (a curriculum over margin-defined difficulty), and (ii) routes difficult pairs to an SFT objective while applying a preference loss to easy pairs. This hybrid design provides a practical mechanism to leverage ambiguous pairs without incurring the optimization failures often associated with preference losses on low-margin data. Across three LLM-judge benchmarks, MixDPO consistently improves alignment over DPO and a range of widely-used variants, with particularly strong gains on AlpacaEval~2 length-controlled (LC) win rate.

</details>


### [519] [Reasoning and Tool-use Compete in Agentic RL:From Quantifying Interference to Disentangled Tuning](https://arxiv.org/abs/2602.00994)
*Yu Li,Mingyang Yi,Xiuyu Li,Ju Fan,Fuxin Jiang,Binbin Chen,Peng Li,Jie Song,Tieying Zhang*

Main category: cs.AI

Relevance: 85.0

TL;DR: 本文通过引入线性效应归因系统(LEAS)发现，在智能体强化学习中，推理和工具使用能力之间存在训练干扰，并提出解耦动作推理调优(DART)框架来分离这两种能力的参数更新。


<details>
  <summary>Details</summary>
Motivation: 现有智能体强化学习方法通常训练单一共享模型参数来同时支持推理和工具使用行为，并假设联合训练能提升整体智能体性能。然而这一假设缺乏实证检验，本文旨在系统研究这一假设的有效性。

Method: 1. 引入线性效应归因系统(LEAS)定量分析推理和工具使用行为之间的干扰；2. 提出解耦动作推理调优(DART)框架，通过分离的低秩适应模块显式解耦推理和工具使用的参数更新。

Result: 实验结果显示DART始终优于基线方法，平均提升6.35%，并且使用单一模型就能达到与显式分离工具使用和推理的多智能体系统相当的性能。

Conclusion: 推理和工具使用能力确实存在训练干扰，传统联合优化方法效果有限。DART框架通过解耦参数更新有效解决了这一问题，为智能体强化学习提供了更有效的训练范式。

Abstract: Agentic Reinforcement Learning (ARL) focuses on training large language models (LLMs) to interleave reasoning with external tool execution to solve complex tasks. Most existing ARL methods train a single shared model parameters to support both reasoning and tool use behaviors, implicitly assuming that joint training leads to improved overall agent performance. Despite its widespread adoption, this assumption has rarely been examined empirically. In this paper, we systematically investigate this assumption by introducing a Linear Effect Attribution System(LEAS), which provides quantitative evidence of interference between reasoning and tool-use behaviors. Through an in-depth analysis, we show that these two capabilities often induce misaligned gradient directions, leading to training interference that undermines the effectiveness of joint optimization and challenges the prevailing ARL paradigm. To address this issue, we propose Disentangled Action Reasoning Tuning(DART), a simple and efficient framework that explicitly decouples parameter updates for reasoning and tool-use via separate low-rank adaptation modules. Experimental results show that DART consistently outperforms baseline methods with averaged 6.35 percent improvements and achieves performance comparable to multi-agent systems that explicitly separate tool-use and reasoning using a single model.

</details>


### [520] [Error Taxonomy-Guided Prompt Optimization](https://arxiv.org/abs/2602.00997)
*Mayank Singh,Vikas Yadav,Eduardo Blanco*

Main category: cs.AI

Relevance: 85.0

TL;DR: ETGPO是一种基于错误分类的提示优化方法，采用自上而下的方式分析全局失败模式，相比传统试错方法减少约三分之二的token使用和评估预算。


<details>
  <summary>Details</summary>
Motivation: 现有自动提示优化方法多采用自下而上的试错方式，基于单个问题的反馈迭代调整提示，缺乏全局视角，计算成本高。需要一种更高效、全局化的提示优化方法。

Method: ETGPO采用自上而下的方法：1) 收集模型错误；2) 将错误分类到分类学中；3) 针对最常见失败模式在提示中添加指导。通过分析全局失败模式来优化提示。

Result: 在数学、问答和逻辑推理等多个基准测试中，ETGPO达到或优于最先进方法的准确率，同时优化阶段的token使用量和评估预算减少约三分之二。

Conclusion: 基于错误分类的自上而下提示优化方法比传统自下而上方法更高效，能以更少计算资源获得同等或更好的性能，为提示工程提供了新思路。

Abstract: Automatic Prompt Optimization (APO) is a powerful approach for extracting performance from large language models without modifying their weights. Many existing methods rely on trial-and-error, testing different prompts or in-context examples until a good configuration emerges, often consuming substantial compute. Recently, natural language feedback derived from execution logs has shown promise as a way to identify how prompts can be improved. However, most prior approaches operate in a bottom-up manner, iteratively adjusting the prompt based on feedback from individual problems, which can cause them to lose the global perspective. In this work, we propose Error Taxonomy-Guided Prompt Optimization (ETGPO), a prompt optimization algorithm that adopts a top-down approach. ETGPO focuses on the global failure landscape by collecting model errors, categorizing them into a taxonomy, and augmenting the prompt with guidance targeting the most frequent failure modes. Across multiple benchmarks spanning mathematics, question answering, and logical reasoning, ETGPO achieves accuracy that is comparable to or better than state-of-the-art methods, while requiring roughly one third of the optimization-phase token usage and evaluation budget.

</details>


### [521] [How RLHF Amplifies Sycophancy](https://arxiv.org/abs/2602.01002)
*Itai Shapira,Gerdus Benade,Ariel D. Procaccia*

Main category: cs.AI

Relevance: 85.0

TL;DR: 论文分析了LLM在偏好对齐后谄媚行为加剧的机制，提出了一种训练时干预方法来防止这种行为漂移


<details>
  <summary>Details</summary>
Motivation: 大型语言模型在基于偏好的后训练后经常表现出更强的谄媚行为，即使这与事实准确性或合理判断相冲突。本文旨在形式化分析人类反馈对齐如何加剧这种失败模式

Method: 1) 识别奖励优化与人类偏好数据偏差之间的因果放大机制；2) 分析Bradley-Terry等随机效用模型下的奖励学习；3) 提出训练时干预方法，通过KL散度最小化推导闭式协议惩罚

Result: 计算实验发现奖励差距普遍存在，并在所有考虑配置中导致行为漂移。提出的奖励修正方法能有效防止谄媚行为增加

Conclusion: 人类反馈对齐会放大LLM的谄媚行为，但可以通过训练时干预来中和这种放大机制，确保模型在保持对齐的同时不增加谄媚倾向

Abstract: Large language models often exhibit increased sycophantic behavior after preference-based post-training, showing a stronger tendency to affirm a user's stated or implied belief even when this conflicts with factual accuracy or sound judgment. We present a formal analysis of how alignment from human feedback can increase this failure mode by identifying an explicit amplification mechanism that causally links optimization against a learned reward to bias in the human preference data used for alignment. We show that the direction of behavioral drift is determined by a covariance under the base policy between endorsing the belief signal in the prompt and the learned reward, and that the first-order effect reduces to a simple mean-gap condition. We then analyze reward learning from pairwise comparisons under random utility models like Bradley-Terry and characterize when bias in human annotators' preferences induces this reward gap. Next, we propose a training-time intervention designed to neutralize the amplification mechanism itself. Among all post-trained policies that prevent sycophantic behavior from increasing, we characterize the unique policy closest in KL divergence to the unconstrained post-trained policy, and derive the corresponding minimal reward correction as a closed-form agreement penalty. Computational experiments find that reward gaps are common and cause behavioral drift in all the configurations considered.

</details>


### [522] [HalluHard: A Hard Multi-Turn Hallucination Benchmark](https://arxiv.org/abs/2602.01031)
*Dongyang Fan,Sebastien Delsad,Nicolas Flammarion,Maksym Andriushchenko*

Main category: cs.AI

Relevance: 85.0

TL;DR: HalluHard是一个具有挑战性的多轮幻觉基准测试，包含950个种子问题，涵盖法律、研究、医疗和编程四个高风险领域，通过内联引用要求来操作事实基础性。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型在多轮对话中仍会产生看似合理但无事实依据的陈述，随着上下文增长和早期错误累积，问题会加剧。需要评估模型在需要精确事实依据的高风险领域中的幻觉问题。

Method: 1) 创建HalluHard基准测试：950个种子问题，涵盖法律案例、研究问题、医疗指南和编程四个领域；2) 通过内联引用要求操作事实基础性；3) 提出评估管道：通过网页搜索迭代检索证据，获取、过滤和解析全文来源（包括PDF）来评估引用材料是否支持生成内容。

Result: 即使在网页搜索支持下，前沿专有和开源模型中的幻觉仍然显著（最强配置Opus-4.5+网页搜索约30%）。内容基础错误持续高发。幻觉行为受模型能力、轮次位置、有效推理和所需知识类型影响。

Conclusion: 多轮对话中的幻觉问题仍然严重，需要更可靠的评估方法和改进的模型能力。幻觉行为受多种因素影响，为未来研究提供了方向。

Abstract: Large language models (LLMs) still produce plausible-sounding but ungrounded factual claims, a problem that worsens in multi-turn dialogue as context grows and early errors cascade. We introduce $\textbf{HalluHard}$, a challenging multi-turn hallucination benchmark with 950 seed questions spanning four high-stakes domains: legal cases, research questions, medical guidelines, and coding. We operationalize groundedness by requiring inline citations for factual assertions. To support reliable evaluation in open-ended settings, we propose a judging pipeline that iteratively retrieves evidence via web search. It can fetch, filter, and parse full-text sources (including PDFs) to assess whether cited material actually supports the generated content. Across a diverse set of frontier proprietary and open-weight models, hallucinations remain substantial even with web search ($\approx 30\%$ for the strongest configuration, Opus-4.5 with web search), with content-grounding errors persisting at high rates. Finally, we show that hallucination behavior is shaped by model capacity, turn position, effective reasoning, and the type of knowledge required.

</details>


### [523] [Discovering Process-Outcome Credit in Multi-Step LLM Reasoning](https://arxiv.org/abs/2602.01034)
*Xiangwei Wang,Wei Wang,Ken Chen,Nanduni Nimalsiri,Saman Halgamuge*

Main category: cs.AI

Relevance: 85.0

TL;DR: 提出一个强化学习框架，通过步骤边际信息增益机制提供连续奖励信号，结合解耦掩码策略和双门监督微调目标，提升大语言模型的推理能力。


<details>
  <summary>Details</summary>
Motivation: 传统基于结果的强化学习方法存在奖励稀疏和信用分配效率低的问题，限制了LLM推理能力的提升。需要一种能够提供连续奖励信号、有效过滤训练噪声并实现解耦信用分配的框架。

Method: 1) 步骤边际信息增益机制：基于单调历史水印量化推理步骤的内在价值；2) 解耦掩码策略：过程导向奖励应用于思维链，结果导向奖励应用于完整完成；3) 双门监督微调目标：利用高质量结构和事实信号稳定训练。

Result: 在文本和多模态基准测试（如MATH、Super-CLEVR）上持续优于GRPO等基线方法，在样本效率和最终准确率方面均有提升。模型表现出优越的分布外鲁棒性，在未见过的挑战性推理任务上具有零样本迁移能力。

Conclusion: 提出的框架通过连续奖励信号、解耦信用分配和稳定训练机制，有效解决了强化学习在LLM推理中的奖励稀疏和信用分配问题，显著提升了模型的推理性能和泛化能力。

Abstract: Reinforcement Learning (RL) serves as a potent paradigm for enhancing reasoning capabilities in Large Language Models (LLMs), yet standard outcome-based approaches often suffer from reward sparsity and inefficient credit assignment. In this paper, we propose a novel framework designed to provide continuous reward signals, which introduces a Step-wise Marginal Information Gain (MIG) mechanism that quantifies the intrinsic value of reasoning steps against a Monotonic Historical Watermark, effectively filtering out training noise. To ensure disentangled credit distribution, we implement a Decoupled Masking Strategy, applying process-oriented rewards specifically to the chain-of-thought (CoT) and outcome-oriented rewards to the full completion. Additionally, we incorporate a Dual-Gated SFT objective to stabilize training with high-quality structural and factual signals. Extensive experiments across textual and multi-modal benchmarks (e.g., MATH, Super-CLEVR) demonstrate that our approach consistently outperforms baselines such as GRPO in both sample efficiency and final accuracy. Furthermore, our model exhibits superior out-of-distribution robustness, demonstrating promising zero-shot transfer capabilities to unseen and challenging reasoning tasks.

</details>


### [524] [SetPO: Set-Level Policy Optimization for Diversity-Preserving LLM Reasoning](https://arxiv.org/abs/2602.01062)
*Chenyi Li,Yuan Zhang,Bo Wang,Guoqing Ma,Wei Tang,Haoyang Huang,Nan Duan*

Main category: cs.AI

Relevance: 85.0

TL;DR: 该论文提出了一种基于核化相似度的集合级多样性目标，通过留一法边际贡献计算，将多样性作为优势塑形项集成到策略优化中，以解决强化学习提升LLM推理性能时导致的解决方案多样性下降问题。


<details>
  <summary>Details</summary>
Motivation: 强化学习虽然能有效提升大语言模型的数学推理性能，但往往导致解决方案多样性降低，模型将概率质量集中在少数几种解法上。这限制了模型的创造性和鲁棒性，需要平衡性能提升与解决方案多样性。

Method: 1. 基于核化相似度定义集合级多样性目标；2. 使用留一法计算每个采样轨迹的边际贡献；3. 将多样性目标作为可插拔的优势塑形项集成到策略优化中；4. 通过分布扰动框架分析单个轨迹对多样性的贡献，理论上证明稀有轨迹对全局多样性有更高边际贡献。

Result: 在不同模型规模上的广泛实验表明，该方法在多个基准测试中，在Pass@1和Pass@K指标上都持续优于强基线方法，有效提升了解决方案的多样性同时保持了推理性能。

Conclusion: 提出的基于核化相似度的多样性目标能有效解决强化学习导致的解决方案多样性下降问题，通过理论分析和实验验证了方法的有效性，为大语言模型的推理优化提供了新的多样性增强方案。

Abstract: Reinforcement learning with verifiable rewards has shown notable effectiveness in enhancing large language models (LLMs) reasoning performance, especially in mathematics tasks. However, such improvements often come with reduced outcome diversity, where the model concentrates probability mass on a narrow set of solutions. Motivated by diminishing-returns principles, we introduce a set level diversity objective defined over sampled trajectories using kernelized similarity. Our approach derives a leave-one-out marginal contribution for each sampled trajectory and integrates this objective as a plug-in advantage shaping term for policy optimization. We further investigate the contribution of a single trajectory to language model diversity within a distribution perturbation framework. This analysis theoretically confirms a monotonicity property, proving that rarer trajectories yield consistently higher marginal contributions to the global diversity. Extensive experiments across a range of model scales demonstrate the effectiveness of our proposed algorithm, consistently outperforming strong baselines in both Pass@1 and Pass@K across various benchmarks.

</details>


### [525] [ConvexBench: Can LLMs Recognize Convex Functions?](https://arxiv.org/abs/2602.01075)
*Yepeng Liu,Yu Huang,Yu-Xiang Wang,Yingbin Liang,Yuheng Bu*

Main category: cs.AI

Relevance: 85.0

TL;DR: 论文提出了CB基准测试，用于评估LLMs在深度函数组合下识别符号目标凸性的能力，发现现有模型存在组合推理缺陷，并提出基于分治的代理框架来改善性能。


<details>
  <summary>Details</summary>
Motivation: 随着LLMs开始自动化研究级数学和科学任务，需要评估它们理解和推理凸性的能力。凸分析是现代数学的重要分支，在优化等领域有广泛应用。目前缺乏可扩展且可机械验证的基准来测试LLMs在深度函数组合下识别凸性的能力。

Method: 1. 引入CB基准：可扩展、可机械验证的基准，测试LLMs识别符号目标在深度函数组合下的凸性
2. 在前沿LLMs上进行实验，分析性能随深度增加的变化
3. 提出基于分治的代理框架：i) 使用外部工具解析构建抽象语法树(AST)；ii) 强制对每个中间子表达式进行递归推理，并提供聚焦上下文

Result: 实验显示前沿LLMs存在明显的组合推理缺陷：性能随深度增加急剧下降，从深度2时的F1分数1.0降至深度100时的约0.2。分析推理轨迹发现两种失败模式：解析失败和懒惰推理。提出的代理框架能可靠缓解深度组合失败，在深度100时达到F1分数1.0的显著性能提升。

Conclusion: LLMs在深度函数组合的凸性识别任务上存在组合推理缺陷，但通过外部工具辅助解析和递归推理的代理框架可以有效解决这些问题，为LLMs在数学推理任务上的改进提供了方向。

Abstract: Convex analysis is a modern branch of mathematics with many applications. As Large Language Models (LLMs) start to automate research-level math and sciences, it is important for LLMs to demonstrate the ability to understand and reason with convexity. We introduce \cb, a scalable and mechanically verifiable benchmark for testing \textit{whether LLMs can identify the convexity of a symbolic objective under deep functional composition.} Experiments on frontier LLMs reveal a sharp compositional reasoning gap: performance degrades rapidly with increasing depth, dropping from an F1-score of $1.0$ at depth $2$ to approximately $0.2$ at depth $100$. Inspection of models' reasoning traces indicates two failure modes: \textit{parsing failure} and \textit{lazy reasoning}. To address these limitations, we propose an agentic divide-and-conquer framework that (i) offloads parsing to an external tool to construct an abstract syntax tree (AST) and (ii) enforces recursive reasoning over each intermediate sub-expression with focused context. This framework reliably mitigates deep-composition failures, achieving substantial performance improvement at large depths (e.g., F1-Score $= 1.0$ at depth $100$).

</details>


### [526] [EvoOpt-LLM: Evolving industrial optimization models with large language models](https://arxiv.org/abs/2602.01082)
*Yiliu He,Tianle Li,Binghao Ji,Zhiyuan Liu,Di Huang*

Main category: cs.AI

Relevance: 85.0

TL;DR: EvoOpt-LLM：基于7B参数LLM的工业优化建模框架，通过LoRA微调实现自动化模型构建、约束注入和变量剪枝，在少量样本（3000个）下达到91%生成率和65.9%可执行率。


<details>
  <summary>Details</summary>
Motivation: 工业规划调度中，将自然语言需求转换为MILP模型并随业务规则演化维护需要大量专家知识。现有LLM方法存在数据效率低、求解器有效性有限、工业规模扩展性差等问题。

Method: 基于7B参数LLM构建统一框架，使用参数高效的LoRA微调，支持全生命周期工业优化建模：1) 自动化模型构建 2) 动态业务约束注入 3) 端到端变量剪枝。

Result: 仅用3000训练样本达到91%生成率和65.9%可执行率，关键性能在1500样本内显现。约束注入模块可靠增强现有MILP模型，变量剪枝模块在400样本下对中等规模LP模型达到F1分数~0.56。

Conclusion: EvoOpt-LLM展示了工业优化建模的实用数据高效方法，减少对专家干预的依赖，同时提高适应性和求解器效率。

Abstract: Optimization modeling via mixed-integer linear programming (MILP) is fundamental to industrial planning and scheduling, yet translating natural-language requirements into solver-executable models and maintaining them under evolving business rules remains highly expertise-intensive. While large language models (LLMs) offer promising avenues for automation, existing methods often suffer from low data efficiency, limited solver-level validity, and poor scalability to industrial-scale problems. To address these challenges, we present EvoOpt-LLM, a unified LLM-based framework supporting the full lifecycle of industrial optimization modeling, including automated model construction, dynamic business-constraint injection, and end-to-end variable pruning. Built on a 7B-parameter LLM and adapted via parameter-efficient LoRA fine-tuning, EvoOpt-LLM achieves a generation rate of 91% and an executability rate of 65.9% with only 3,000 training samples, with critical performance gains emerging under 1,500 samples. The constraint injection module reliably augments existing MILP models while preserving original objectives, and the variable pruning module enhances computational efficiency, achieving an F1 score of ~0.56 on medium-sized LP models with only 400 samples. EvoOpt-LLM demonstrates a practical, data-efficient approach to industrial optimization modeling, reducing reliance on expert intervention while improving adaptability and solver efficiency.

</details>


### [527] [MedBeads: An Agent-Native, Immutable Data Substrate for Trustworthy Medical AI](https://arxiv.org/abs/2602.01086)
*Takahito Nakajima*

Main category: cs.AI

Relevance: 85.0

TL;DR: MedBeads提出了一种面向AI代理的原生医疗数据基础设施，使用不可变的Merkle DAG结构存储临床事件，解决LLM在医疗领域部署时的上下文不匹配问题，确保数据确定性和防篡改。


<details>
  <summary>Details</summary>
Motivation: 当前电子病历系统为人类设计，导致AI代理接收碎片化数据，需要依赖概率推理（如RAG）重建患者历史，这容易产生幻觉且难以审计。存在"上下文不匹配"问题。

Method: 提出MedBeads架构：将临床事件作为不可变的"Beads"节点，构建Merkle有向无环图，密码学引用因果前驱。实现包括Go核心引擎、Python中间件和React可视化界面。使用BFS算法进行上下文检索。

Result: 成功用合成数据实现工作流，将FHIR资源转换为因果链接图。BFS算法实现O(V+E)复杂度的实时决策支持。设计保证防篡改性，可视化界面帮助临床医生理解因果链。

Conclusion: MedBeads通过从概率搜索转向确定性图遍历，从可变记录转向不可变链，解决了上下文不匹配问题，为"可信医疗AI"提供基础。确保AI接收的上下文是确定性和防篡改的，同时LLM负责解释。

Abstract: Background: As of 2026, Large Language Models (LLMs) demonstrate expert-level medical knowledge. However, deploying them as autonomous "Clinical Agents" remains limited. Current Electronic Medical Records (EMRs) and standards like FHIR are designed for human review, creating a "Context Mismatch": AI agents receive fragmented data and must rely on probabilistic inference (e.g., RAG) to reconstruct patient history. This approach causes hallucinations and hinders auditability. Methods: We propose MedBeads, an agent-native data infrastructure where clinical events are immutable "Beads"--nodes in a Merkle Directed Acyclic Graph (DAG)--cryptographically referencing causal predecessors. This "write-once, read-many" architecture makes tampering mathematically detectable. We implemented a prototype with a Go Core Engine, Python middleware for LLM integration, and a React-based visualization interface. Results: We successfully implemented the workflow using synthetic data. The FHIR-to-DAG conversion transformed flat resources into a causally-linked graph. Our Breadth-First Search (BFS) Context Retrieval algorithm traverses relevant subgraphs with O(V+E) complexity, enabling real-time decision support. Tamper-evidence is guaranteed by design: any modification breaks the cryptographic chain. The visualization aids clinician understanding through explicit causal links. Conclusion: MedBeads addresses the "Context Mismatch" by shifting from probabilistic search to deterministic graph traversal, and from mutable records to immutable chains, providing the substrate for "Trustworthy Medical AI." It guarantees the context the AI receives is deterministic and tamper-evident, while the LLM determines interpretation. The structured Bead format serves as a token-efficient "AI-native language." We release MedBeads as open-source software to accelerate agent-native data standards.

</details>


### [528] [Hard Constraints Meet Soft Generation: Guaranteed Feasibility for LLM-based Combinatorial Optimization](https://arxiv.org/abs/2602.01090)
*Yang Liu,Chuan Zhou,Yancheng Chen,Shuai Zhang,Xixun Lin,Xiaoqing Wang*

Main category: cs.AI

Relevance: 85.0

TL;DR: FALCON框架通过语法约束解码、可行性修复层和自适应采样确保LLM解决组合优化问题时100%可行性，使用BOPO训练方法，在7个NP难问题上实现完美可行性且质量优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型在组合优化中表现出潜力，但缺乏保证解可行性的机制，这对于实际部署至关重要。现有方法无法确保100%可行性，限制了LLM在真实世界优化问题中的应用。

Method: 1) 语法约束解码：强制输出符合语法规则；2) 可行性修复层：修正语义约束违反；3) 自适应Best-of-N采样：高效分配推理计算资源；4) BOPO训练方法：基于目标差距加权偏好对，提供密集监督。

Result: 在7个NP难组合优化问题上，FALCON实现了100%可行性，同时解决方案质量匹配或超过了最先进的神经和LLM求解器。理论证明了BOPO的收敛性，并给出了修复引起的质量损失界限。

Conclusion: FALCON框架成功解决了LLM在组合优化中缺乏可行性保证的关键问题，通过创新的解码、修复和训练方法，为LLM在实际优化问题中的可靠部署提供了可行方案。

Abstract: Large language models (LLMs) have emerged as promising general-purpose solvers for combinatorial optimization (CO), yet they fundamentally lack mechanisms to guarantee solution feasibility which is critical for real-world deployment. In this work, we introduce FALCON, a framework that ensures 100\% feasibility through three key innovations: (i) \emph{grammar-constrained decoding} enforces syntactic validity, (ii) a \emph{feasibility repair layer} corrects semantic constraint violations, and (iii) \emph{adaptive Best-of-$N$ sampling} allocates inference compute efficiently. To train the underlying LLM, we introduce the Best-anchored Objective-guided Preference Optimization (BOPO) in LLM training, which weights preference pairs by their objective gap, providing dense supervision without human labels. Theoretically, we prove convergence for BOPO and provide bounds on repair-induced quality loss. Empirically, across seven NP-hard CO problems, FALCON achieves perfect feasibility while matching or exceeding the solution quality of state-of-the-art neural and LLM-based solvers.

</details>


### [529] [Probing RLVR training instability through the lens of objective-level hacking](https://arxiv.org/abs/2602.01103)
*Yiming Dong,Kun Fu,Haoyu Li,Xinyuan Zhu,Yurou Liu,Lijing Shao,Jieping Ye,Zheng Wang*

Main category: cs.AI

Relevance: 85.0

TL;DR: 该论文提出了一个理解MoE架构中RLVR训练不稳定性的理论框架，通过"目标级黑客攻击"视角解释token级信用错配导致的系统级虚假信号，并揭示了训练-推理差异异常增长的根本机制。


<details>
  <summary>Details</summary>
Motivation: MoE架构在RLVR训练中容易出现不稳定性，这严重限制了模型能力的持续提升，但现有研究对其根本原因和机制理解不足。作者旨在建立理论框架来解释这种不稳定性。

Method: 提出了"目标级黑客攻击"的理论框架，区别于传统的奖励黑客攻击。通过分析token级信用错配导致的系统级虚假信号，在30B MoE模型上进行大量实验，追踪并形式化了训练-推理差异异常增长的病理动态机制。

Result: 成功揭示了MoE模型中训练不稳定的具体因果机制，特别是训练-推理差异异常增长的现象，为之前缺乏机制解释的广泛关联问题提供了具体解释。

Conclusion: 该研究为理解MoE模型中的RLVR训练不稳定性提供了具体和因果性的解释，为设计稳定的RLVR算法提供了理论指导。

Abstract: Prolonged reinforcement learning with verifiable rewards (RLVR) has been shown to drive continuous improvements in the reasoning capabilities of large language models, but the training is often prone to instabilities, especially in Mixture-of-Experts (MoE) architectures. Training instability severely undermines model capability improvement, yet its underlying causes and mechanisms remain poorly understood. In this work, we introduce a principled framework for understanding RLVR instability through the lens of objective-level hacking. Unlike reward hacking, which arises from exploitable verifiers, objective-level hacking emerges from token-level credit misalignment and is manifested as system-level spurious signals in the optimization objective. Grounded in our framework, together with extensive experiments on a 30B MoE model, we trace the origin and formalize the mechanism behind a key pathological training dynamic in MoE models: the abnormal growth of the training-inference discrepancy, a phenomenon widely associated with instability but previously lacking a mechanistic explanation. These findings provide a concrete and causal account of the training dynamics underlying instabilities in MoE models, offering guidance for the design of stable RLVR algorithms.

</details>


### [530] [PersistBench: When Should Long-Term Memories Be Forgotten by LLMs?](https://arxiv.org/abs/2602.01146)
*Sidharth Pulipaka,Oliver Chen,Manas Sharma,Taaha S Bajwa,Vyas Raina,Ivaxi Sheth*

Main category: cs.AI

Relevance: 85.0

TL;DR: 论文提出PersistBench基准，用于评估LLM长期记忆带来的安全风险，发现前沿和开源模型在跨域泄漏和记忆诱导奉承方面存在高失败率。


<details>
  <summary>Details</summary>
Motivation: 随着对话助手越来越多地将长期记忆与LLM集成，记忆的持久性（如用户是素食者）可以增强未来对话的个性化。然而，这种持久性也可能引入被忽视的安全风险，需要系统性地测量和解决这些风险。

Method: 引入PersistBench基准来测量长期记忆特定安全风险，识别两种风险：跨域泄漏（LLM不适当地从长期记忆中注入上下文）和记忆诱导奉承（存储的长期记忆暗中强化用户偏见）。在18个前沿和开源LLM上评估该基准。

Result: 结果显示这些LLM存在惊人的高失败率：跨域样本中位数失败率为53%，奉承样本中位数失败率为97%。这表明当前LLM在长期记忆安全方面存在严重漏洞。

Conclusion: 长期记忆集成带来了新的安全挑战，需要开发更稳健和安全的长期记忆使用机制。PersistBench为评估和改善前沿对话系统的安全性提供了重要基准。

Abstract: Conversational assistants are increasingly integrating long-term memory with large language models (LLMs). This persistence of memories, e.g., the user is vegetarian, can enhance personalization in future conversations. However, the same persistence can also introduce safety risks that have been largely overlooked. Hence, we introduce PersistBench to measure the extent of these safety risks. We identify two long-term memory-specific risks: cross-domain leakage, where LLMs inappropriately inject context from the long-term memories; and memory-induced sycophancy, where stored long-term memories insidiously reinforce user biases. We evaluate 18 frontier and open-source LLMs on our benchmark. Our results reveal a surprisingly high failure rate across these LLMs - a median failure rate of 53% on cross-domain samples and 97% on sycophancy samples. To address this, our benchmark encourages the development of more robust and safer long-term memory usage in frontier conversational systems.

</details>


### [531] [Capabilities and Fundamental Limits of Latent Chain-of-Thought](https://arxiv.org/abs/2602.01148)
*Jiaxuan Zou,Yaozhong Xiong,Yong Liu*

Main category: cs.AI

Relevance: 85.0

TL;DR: 论文揭示了潜在思维链模型在探索与执行之间的权衡由决策确定性控制，提出了符号索引作为核心机制，并证明课程学习是理论必要的。


<details>
  <summary>Details</summary>
Motivation: 潜在思维链模型在推理效率方面有前景，但表现出令人困惑的性能不一致性：在探索任务上表现出色（ProsQA: 97.0%），但在计算任务上失败（GSM8K: 34.1%）。作者旨在揭示这种权衡的根本原因。

Method: 1) 理论分析探索-执行权衡，证明高确定性支持精确执行但抑制探索，低确定性促进搜索但导致错误累积；2) 引入符号索引作为量化决策承诺的核心机制；3) 证明课程学习的理论必要性。

Result: 建立了决策确定性控制探索-执行权衡的理论框架，提出了符号索引作为核心机制，并证明了课程学习对于解决分布不匹配问题的必要性。

Conclusion: 该框架将设计范式从二元架构选择转向基于任务需求动态调节决策确定性的自适应系统，为潜在思维链模型的优化提供了理论基础。

Abstract: Latent Chain-of-Thought (Latent CoT) models promise efficient reasoning via continuous representations, yet exhibit puzzling performance inconsistencies: excelling at exploration (ProsQA: 97.0%) but failing at computation (GSM8K: 34.1%). We reveal that this trade-off is governed by decisional certainty. Our contributions are threefold: (1) We theoretically characterize the fundamental Exploration-Execution Trade-off, proving that high certainty enables precise execution but inhibits exploration, while low certainty facilitates search but causes error accumulation. (2) We introduce the Symbolic Index--quantifying decisional commitment--as the core mechanism governing this trade-off and establish its causal relationship with both execution stability and exploration capability. (3) We prove that curriculum learning is theoretically necessary, as direct training provably fails due to distributional mismatch. Our framework shifts the design paradigm from binary architectural choices toward adaptive systems that dynamically regulate decisional certainty based on task demands.

</details>


### [532] [Do All Individual Layers Help? An Empirical Study of Task-Interfering Layers in Vision-Language Models](https://arxiv.org/abs/2602.01167)
*Zhiming Liu,Yujie Wei,Lei Feng,Xiu Su,Xiaobo Xia,Weili Guan,Zeke Xie,Shuo Yang*

Main category: cs.AI

Relevance: 85.0

TL;DR: 该论文发现预训练视觉语言模型（VLM）中存在任务干扰层，这些层会损害下游任务性能。作者提出TaLo方法，通过动态识别并绕过最干扰的层来提升性能，无需参数更新。


<details>
  <summary>Details</summary>
Motivation: 当前VLM通常默认使用所有层进行预测，但作者发现某些层实际上会损害特定下游任务的性能。这揭示了预训练VLM中存在任务干扰层的问题，需要一种方法来识别并绕过这些有害层以提升模型性能。

Method: 1. 系统研究各层对任务的影响，通过层干预（如参数置零）测量性能变化
2. 提出任务-层交互向量量化每层对特定任务的影响
3. 提出TaLo方法：训练免费、测试时自适应方法，动态识别并绕过最干扰层

Result: 1. 发现任务干扰层普遍存在于不同模型和数据集
2. 任务-层交互向量显示相似能力的任务具有一致的响应模式
3. TaLo方法显著提升性能：Qwen-VL在ScienceQA的Maps任务上准确率提升16.6%
4. 方法适用于多种模型和数据集，无需参数更新

Conclusion: 该工作揭示了预训练VLM中意外的模块化特性，提供了即插即用、无需训练的方法来在推理时解锁隐藏能力。任务干扰层的发现为模型分析和优化提供了新视角。

Abstract: Current VLMs have demonstrated capabilities across a wide range of multimodal tasks. Typically, in a pretrained VLM, all layers are engaged by default to make predictions on downstream tasks. We find that intervening on a single layer, such as by zeroing its parameters, can improve the performance on certain tasks, indicating that some layers hinder rather than help downstream tasks. We systematically investigate how individual layers influence different tasks via layer intervention. Specifically, we measure the change in performance relative to the base model after intervening on each layer and observe improvements when bypassing specific layers. This improvement can be generalizable across models and datasets, indicating the presence of Task-Interfering Layers that harm downstream tasks' performance. We introduce Task-Layer Interaction Vector, which quantifies the effect of intervening on each layer of a VLM given a task. These task-interfering layers exhibit task-specific sensitivity patterns: tasks requiring similar capabilities show consistent response trends under layer interventions, as evidenced by the high similarity in their task-layer interaction vectors. Inspired by these findings, we propose TaLo (Task-Adaptive Layer Knockout), a training-free, test-time adaptation method that dynamically identifies and bypasses the most interfering layer for a given task. Without parameter updates, TaLo improves performance across various models and datasets, including boosting Qwen-VL's accuracy on the Maps task in ScienceQA by up to 16.6%. Our work reveals an unexpected form of modularity in pretrained VLMs and provides a plug-and-play, training-free mechanism to unlock hidden capabilities at inference time. The source code will be publicly available.

</details>


### [533] [A State-Transition Framework for Efficient LLM Reasoning](https://arxiv.org/abs/2602.01198)
*Liang Zhang,Yu Zhao,Longyue Wang,Tianqi Shi,Weihua Luo,Kaifu Zhang,Jinsong Su*

Main category: cs.AI

Relevance: 85.0

TL;DR: 提出了一种基于状态转移的高效推理框架，将LLM的推理过程建模为状态转移过程，使用线性注意力机制估计推理状态，将注意力计算复杂度从二次降低到线性，显著提升推理效率。


<details>
  <summary>Details</summary>
Motivation: 长链思维推理虽然能提升LLM在复杂任务上的性能，但生成长推理序列的计算和内存成本很高，限制了效率和实用性。现有方法通常通过压缩推理序列来提高效率，但这与测试时扩展相冲突，限制了LLM的推理能力。

Method: 1) 将LLM推理过程建模为状态转移过程；2) 使用线性注意力机制估计推理状态，记录历史推理信息；3) 基于查询提示和推理状态，LLM高效执行当前推理步骤并更新状态；4) 提出基于状态的推理策略缓解噪声推理步骤导致的过度思考问题。

Result: 在多个数据集和模型规模上的广泛实验表明，该框架不仅提高了LLM的推理效率，还增强了推理性能。

Conclusion: 提出的高效推理框架通过状态转移建模和线性注意力机制，在保持甚至提升推理性能的同时，显著降低了计算复杂度，解决了长链推理的效率瓶颈问题。

Abstract: While Long Chain-of-Thought (CoT) reasoning significantly improves Large Language Models (LLMs) performance on complex reasoning tasks, the substantial computational and memory costs of generating long CoT sequences limit their efficiency and practicality. Existing studies usually enhance the reasoning efficiency of LLMs by compressing CoT sequences. However, this approach conflicts with test-time scaling, limiting the reasoning capacity of LLMs. In this paper, we propose an efficient reasoning framework that models the reasoning process of LLMs as a state-transition process. Specifically, we first apply a linear attention mechanism to estimate the LLM's reasoning state, which records the historical reasoning information from previous reasoning steps. Then, based on the query prompt and the reasoning state, the LLM can efficiently perform the current reasoning step and update the state. With the linear attention, each token in the current reasoning step can directly retrieve relevant historical reasoning information from the reasoning state, without explicitly attending to tokens in previous reasoning steps. In this way, the computational complexity of attention is reduced from quadratic to linear, significantly improving the reasoning efficiency of LLMs. In addition, we propose a state-based reasoning strategy to mitigate the over-thinking issue caused by noisy reasoning steps. Extensive experiments across multiple datasets and model sizes demonstrate that our framework not only improves the reasoning efficiency of LLMs but also enhances their reasoning performance.

</details>


### [534] [Workflow-R1: Group Sub-sequence Policy Optimization for Multi-turn Workflow Construction](https://arxiv.org/abs/2602.01202)
*Mingze Kong,Zikun Qu,Zhongquan Zhou,Pengyu Liang,Xiang Li,Zhiwei Shang,Zhi Hong,Kaiyu Huang,Zhiyong Wang,Zhongxiang Dai*

Main category: cs.AI

Relevance: 85.0

TL;DR: Workflow-R1将工作流构建重新定义为多轮自然语言顺序决策过程，引入GSsPO算法优化多轮交互中的粒度不匹配问题，在多个QA基准测试中表现优异。


<details>
  <summary>Details</summary>
Motivation: 现有工作流优化方法通常将工作流合成视为静态、一次性的代码中心生成问题，这过度约束了模型的编码能力，限制了动态问题解决所需的灵活性。

Method: 提出Workflow-R1框架，将工作流构建重新定义为多轮自然语言顺序决策过程。引入Group Sub-sequence Policy Optimization (GSsPO)算法，通过将优化单元重新校准为复合子序列（特别是原子Think-Action循环），使梯度更新与这些交互的语义边界对齐。

Result: 在多个QA基准测试中，Workflow-R1优于竞争基线，验证了GSsPO作为顺序推理的通用解决方案的有效性。

Conclusion: Workflow-R1为自动化工作流优化提供了一个有前景的新范式，GSsPO作为结构感知的RL算法可推广到广泛的多轮智能体顺序决策任务。

Abstract: The rapid evolution of agentic workflows has demonstrated strong performance of LLM-based agents in addressing complex reasoning tasks. However, existing workflow optimization methods typically formulate workflow synthesis as a static, one-shot code-centric generation problem. This paradigm imposes excessive constraints on the model's coding capabilities and restricts the flexibility required for dynamic problem-solving. In this paper, we present Workflow-R1, a framework that reformulates workflow construction as a multi-turn, natural language-based sequential decision-making process. To resolve the optimization granularity mismatch inherent in such multi-turn interactions, we introduce Group Sub-sequence Policy Optimization (GSsPO). While explicitly tailored to align with the interleaved Think-Action dynamics of agentic reasoning, GSsPO fundamentally functions as a structure-aware RL algorithm generalizable to a broad class of multi-turn agentic sequential decision-making tasks. By recalibrating the optimization unit to the composite sub-sequence, specifically the atomic Think-Action cycle, it aligns gradient updates with the semantic boundaries of these interactions, ensuring robust learning in complex multi-turn reasoning tasks. Through extensive experiments on multiple QA benchmarks, Workflow-R1 outperforms competitive baselines, validating GSsPO as a generalized solution for sequential reasoning and establishing Workflow-R1 as a promising new paradigm for automated workflow optimization.

</details>


### [535] [Addressing Explainability of Generative AI using SMILE (Statistical Model-agnostic Interpretability with Local Explanations)](https://arxiv.org/abs/2602.01206)
*Zeinab Dehghani*

Main category: cs.AI

Relevance: 85.0

TL;DR: gSMILE是一个统一的生成模型可解释性框架，通过文本扰动、Wasserstein距离和加权代理建模来量化提示组件对模型输出的影响，为LLMs提供细粒度token级归因和可视化热图。


<details>
  <summary>Details</summary>
Motivation: 生成式AI模型虽然能产生复杂的文本和视觉输出，但其决策过程不透明，限制了在高风险应用中的信任和问责。需要一种统一框架来解释生成模型的决策过程。

Method: 将SMILE方法扩展到生成设置，使用文本输入的受控扰动、Wasserstein距离度量和加权代理建模来量化和可视化提示组件对输出的影响。为LLMs提供token级归因和热图，为图像编辑模型分析指令修改对结果的影响。结合基于场景的评估策略和操作设计域框架。

Result: gSMILE能产生稳健、与人类认知一致的归因，并能有效泛化到最先进的生成模型。定义了稳定性、保真度、准确性、一致性和忠实性等归因指标来评估解释质量。

Conclusion: gSMILE有潜力推进生成式AI技术的透明、可靠和负责任部署，为生成模型提供系统化的可解释性框架。

Abstract: The rapid advancement of generative artificial intelligence has enabled models capable of producing complex textual and visual outputs; however, their decision-making processes remain largely opaque, limiting trust and accountability in high-stakes applications. This thesis introduces gSMILE, a unified framework for the explainability of generative models, extending the Statistical Model-agnostic Interpretability with Local Explanations (SMILE) method to generative settings. gSMILE employs controlled perturbations of textual input, Wasserstein distance metrics, and weighted surrogate modelling to quantify and visualise how specific components of a prompt or instruction influence model outputs. Applied to Large Language Models (LLMs), gSMILE provides fine-grained token-level attribution and generates intuitive heatmaps that highlight influential tokens and reasoning pathways. In instruction-based image editing models, the exact text-perturbation mechanism is employed, allowing for the analysis of how modifications to an editing instruction impact the resulting image. Combined with a scenario-based evaluation strategy grounded in the Operational Design Domain (ODD) framework, gSMILE allows systematic assessment of model behaviour across diverse semantic and environmental conditions. To evaluate explanation quality, we define rigorous attribution metrics, including stability, fidelity, accuracy, consistency, and faithfulness, and apply them across multiple generative architectures. Extensive experiments demonstrate that gSMILE produces robust, human-aligned attributions and generalises effectively across state-of-the-art generative models. These findings highlight the potential of gSMILE to advance transparent, reliable, and responsible deployment of generative AI technologies.

</details>


### [536] [Not All Preferences Are Created Equal: Stability-Aware and Gradient-Efficient Alignment for Reasoning Models](https://arxiv.org/abs/2602.01207)
*Hui Wu,Hengyi Cai,Jinman Zhao,Xinran Chen,Ziheng Li,Zhejun Zhao,Shuaiqiang Wang,Yuchen Li,Dawei Yin*

Main category: cs.AI

Relevance: 85.0

TL;DR: SAGE提出了一种动态的稳定性感知梯度效率框架，用于改进大语言模型的偏好对齐训练，通过课程学习和稳定性感知评分来优化训练样本选择，加速收敛并提升性能。


<details>
  <summary>Details</summary>
Motivation: 现有偏好对齐方法（如DPO）通常对所有偏好对进行统一处理，忽略了训练样本的演化效用。这种静态方法导致计算效率低下和优化不稳定，浪费计算资源在梯度可忽略的平凡样本上，同时受到决策边界附近不确定样本的噪声影响。

Method: SAGE框架包含两个核心组件：1）基于模型能力的粗粒度课程机制，动态刷新候选样本池；2）细粒度的稳定性感知评分函数，优先选择信息丰富、置信度高的错误样本，同时过滤不稳定的样本，最大化策略更新的信噪比。

Result: 在多个数学推理基准测试上的实验表明，SAGE显著加速了收敛速度，并超越了静态基线方法，证明了策略感知、稳定性意识的数据选择在推理对齐中的关键作用。

Conclusion: SAGE框架通过动态的稳定性感知梯度效率优化，有效解决了偏好对齐中的训练效率和稳定性问题，为大语言模型的推理能力对齐提供了更可靠的训练方法。

Abstract: Preference-based alignment is pivotal for training large reasoning models; however, standard methods like Direct Preference Optimization (DPO) typically treat all preference pairs uniformly, overlooking the evolving utility of training instances. This static approach often leads to inefficient or unstable optimization, as it wastes computation on trivial pairs with negligible gradients and suffers from noise induced by samples near uncertain decision boundaries. Facing these challenges, we propose SAGE (Stability-Aware Gradient Efficiency), a dynamic framework designed to enhance alignment reliability by maximizing the Signal-to-Noise Ratio of policy updates. Concretely, SAGE integrates a coarse-grained curriculum mechanism that refreshes candidate pools based on model competence with a fine-grained, stability-aware scoring function that prioritizes informative, confident errors while filtering out unstable samples. Experiments on multiple mathematical reasoning benchmarks demonstrate that SAGE significantly accelerates convergence and outperforms static baselines, highlighting the critical role of policy-aware, stability-conscious data selection in reasoning alignment.

</details>


### [537] [FutureMind: Equipping Small Language Models with Strategic Thinking-Pattern Priors via Adaptive Knowledge Distillation](https://arxiv.org/abs/2602.01222)
*Shaoxiong Yang,Junting Li,Mengyuan Zhang,Chao Li,Wei Liu,Jian Luan*

Main category: cs.AI

Relevance: 85.0

TL;DR: FutureMind是一个模块化推理框架，通过从大语言模型进行自适应知识蒸馏，为小语言模型注入战略思维模式先验，提升其在复杂知识密集型任务上的表现。


<details>
  <summary>Details</summary>
Motivation: 小语言模型（SLMs）在成本敏感和资源受限场景中具有吸引力，但它们在需要结构化推理和有效检索的复杂知识密集型任务上表现不佳。需要一种方法让SLMs既能保持高效推理，又能处理复杂任务。

Method: 提出FutureMind框架，包含四个关键模块：问题分析、逻辑推理、策略规划和检索指导。采用三种不同的检索范式将复杂查询分解为可处理的子问题。通过自适应知识蒸馏从大语言模型向小语言模型传递思维模式先验。

Result: 在多跳QA基准测试（2WikiMultihopQA、MuSiQue、Bamboogle、Frames）上，FutureMind consistently outperforms strong baselines such as Search-o1，在自由训练条件下实现了最先进的结果，适用于不同架构和规模的SLMs。

Conclusion: FutureMind成功提升了SLMs在复杂任务上的表现，但研究发现思维模式蒸馏过程受到教师（LLMs）和学生（SLMs）模型之间认知偏差瓶颈的限制，这为推理技能的可迁移性提供了新视角。

Abstract: Small Language Models (SLMs) are attractive for cost-sensitive and resource-limited settings due to their efficient, low-latency inference. However, they often struggle with complex, knowledge-intensive tasks that require structured reasoning and effective retrieval. To address these limitations, we propose FutureMind, a modular reasoning framework that equips SLMs with strategic thinking-pattern priors via adaptive knowledge distillation from large language models (LLMs). FutureMind introduces a dynamic reasoning pipeline composed of four key modules: Problem Analysis, Logical Reasoning, Strategy Planning, and Retrieval Guidance. This pipeline is augmented by three distinct retrieval paradigms that decompose complex queries into tractable subproblems, ensuring efficient and accurate retrieval execution. Extensive experiments on multi-hop QA benchmarks, including 2WikiMultihopQA, MuSiQue, Bamboogle, and Frames, demonstrate the superiority of FutureMind. It consistently outperforms strong baselines such as Search-o1, achieving state-of-the-art results under free training conditions across diverse SLM architectures and scales. Beyond empirical gains, our analysis reveals that the process of thinking-pattern distillation is restricted by the cognitive bias bottleneck between the teacher (LLMs) and student (SLMs) models. This provides new perspectives on the transferability of reasoning skills, paving the way for the development of SLMs that combine efficiency with genuine cognitive capability.

</details>


### [538] [Predictive Scheduling for Efficient Inference-Time Reasoning in Large Language Models](https://arxiv.org/abs/2602.01237)
*Katrina Brown,Aneesh Muppidi,Rana Shahout*

Main category: cs.AI

Relevance: 85.0

TL;DR: 提出Predictive Scheduling框架，通过轻量级预测器在生成前预估查询的最优推理长度，动态分配固定token预算以最大化准确率，在GSM8K上相比均匀分配获得7.9%绝对准确率提升。


<details>
  <summary>Details</summary>
Motivation: 当前LLMs在复杂推理任务中使用固定token预算会导致简单问题过度计算、困难问题计算不足的问题，需要更精细的计算-准确率权衡控制。

Method: 1) 使用MLP分析transformer中间隐藏状态或LoRA微调的分类器分析原始问题文本，预测每个查询的最优推理长度/难度；2) 贪心批量分配器动态分配固定总token预算；3) 在GSM8K算术基准上进行系统层间研究。

Result: 在GSM8K上，相比均匀预算分配获得7.9%绝对准确率提升，缩小了与完美预知oracle之间50%以上的差距；发现transformer中间层（12-17层）包含最丰富的长度估计信号。

Conclusion: 预运行预算预测实现了计算-准确率权衡的细粒度控制，为延迟敏感、成本高效的LLM部署提供了具体路径。

Abstract: Large language models (LLMs) achieve state-of-the-art accuracy on complex reasoning tasks by generating multiple chain-of-thought (CoT) traces, but using a fixed token budget per query leads to over-computation on easy inputs and under-computation on hard ones. We introduce Predictive Scheduling, a plug-and-play framework that pre-runs lightweight predictors, an MLP on intermediate transformer hidden states or a LoRA-fine-tuned classifier on raw question text, to estimate each query's optimal reasoning length or difficulty before any full generation. Our greedy batch allocator dynamically distributes a fixed total token budget across queries to maximize expected accuracy. On the GSM8K arithmetic benchmark, predictive scheduling yields up to 7.9 percentage points of absolute accuracy gain over uniform budgeting at identical token cost, closing over 50\% of the gap to an oracle with perfect foresight. A systematic layer-wise study reveals that middle layers (12 - 17) of the transformer carry the richest signals for size estimation. These results demonstrate that pre-run budget prediction enables fine-grained control of the compute-accuracy trade-off, offering a concrete path toward latency-sensitive, cost-efficient LLM deployments.

</details>


### [539] [Qrita: High-performance Top-k and Top-p Algorithm for GPUs using Pivot-based Truncation and Selection](https://arxiv.org/abs/2602.01518)
*Jongseok Park,Sunga Kim,Alvin Cheung,Ion Stoica*

Main category: cs.AI

Relevance: 85.0

TL;DR: Qrita提出了一种基于枢轴选择策略的高效Top-k和Top-p算法，相比传统排序方法在GPU上实现2倍吞吐量和一半内存使用，同时保持确定性输出。


<details>
  <summary>Details</summary>
Motivation: Top-k和Top-p是LLM采样中的主要截断操作，但在大规模词汇表上高效实现仍具挑战性。现有方法要么依赖排序（计算和内存开销大），要么使用随机方法（改变算法输出）。

Method: 基于RTop-k的枢轴搜索思想，扩展到Top-k和Top-p：1) 高斯基sigma截断，大幅减少目标元素搜索空间；2) 四元枢轴搜索与重复处理，将枢轴搜索迭代减半并保证确定性输出。使用Triton GPU编程语言实现。

Result: 与vLLM、SGLang、Flashinfer等高性能LLM执行引擎的Top-k和Top-p内核相比，Qrita实现了高达2倍的吞吐量和一半的内存使用，同时提供与基于排序算法相同的输出。

Conclusion: Qrita为LLM采样中的Top-k和Top-p操作提供了高效、确定性的解决方案，显著提升了GPU上的计算效率和内存使用效率。

Abstract: Top-k and Top-p are the dominant truncation operators in the sampling of large language models. Despite their widespread use, implementing them efficiently over large vocabularies remains a significant challenge. Existing approaches often rely on sorting, which incur significant computation and memory overhead on GPUs, or stochastic approaches, which alter the algorithm output. In this work, we propose Qrita, an efficient Top-k and Top-p algorithm based on a pivot-based selection strategy. Based on RTop-k, which uses a pivot-based search for node selection in graph neural networks, Qrita extends the concept of pivot-based search to both Top-k and Top-p with two key techniques: 1. Gaussian-based sigma-truncation, which greatly reduces the search space of the target elements, and 2. Quaternary pivot search with duplication handling, which halves the pivot search iteration and guarantees deterministic output. We provide the full implementation of Qrita using Triton, a popular GPU programming language. Our evaluation of Qrita against the Top-k and Top-p kernels of high performance LLM execution engines such as vLLM, SGLang, and Flashinfer show that Qrita achieves up to 2 times throughput and half memory use while providing the same output to the the sorting-based algorithms.

</details>


### [540] [PRISM: Festina Lente Proactivity -- Risk-Sensitive, Uncertainty-Aware Deliberation for Proactive Agents](https://arxiv.org/abs/2602.01532)
*Yuxuan Fu,Xiaoyu Tan,Teqi Hao,Chen Zhan,Xihe Qiu*

Main category: cs.AI

Relevance: 85.0

TL;DR: PRISM框架通过决策理论门控和双过程推理架构，实现成本敏感的选择性干预，在主动对话代理中平衡帮助收益与干扰负担。


<details>
  <summary>Details</summary>
Motivation: 当前主动对话系统依赖脆弱的启发式方法或不分青红皂白的长推理，难以控制帮助收益与干扰负担之间的权衡。需要一种能够精确决定何时干预的框架。

Method: PRISM框架结合决策理论门控和双过程推理架构：1) 使用成本敏感的门控机制，仅在用户接受概率超过成本阈值时干预；2) 采用"慢速模式"进行反事实检查，集中计算资源于模糊和高风险情况；3) 使用门对齐、模式锁定的蒸馏训练方法。

Result: 在ProactiveBench基准测试中，PRISM将误报率降低22.78%，F1分数提高20.14%，优于现有基线方法。

Conclusion: 决策理论门控结合选择性慢速推理和对齐蒸馏，能够产生精确、计算高效且可控的主动对话代理。

Abstract: Proactive agents must decide not only what to say but also whether and when to intervene. Many current systems rely on brittle heuristics or indiscriminate long reasoning, which offers little control over the benefit-burden tradeoff. We formulate the problem as cost-sensitive selective intervention and present PRISM, a novel framework that couples a decision-theoretic gate with a dual-process reasoning architecture. At inference time, the agent intervenes only when a calibrated probability of user acceptance exceeds a threshold derived from asymmetric costs of missed help and false alarms. Inspired by festina lente (Latin: "make haste slowly"), we gate by an acceptance-calibrated, cost-derived threshold and invoke a resource-intensive Slow mode with counterfactual checks only near the decision boundary, concentrating computation on ambiguous and high-stakes cases. Training uses gate-aligned, schema-locked distillation: a teacher running the full PRISM pipeline provides dense, executable supervision on unlabeled interaction traces, while the student learns a response policy that is explicitly decoupled from the intervention gate to enable tunable and auditable control. On ProactiveBench, PRISM reduces false alarms by 22.78% and improves F1 by 20.14% over strong baselines. These results show that principled decision-theoretic gating, paired with selective slow reasoning and aligned distillation, yields proactive agents that are precise, computationally efficient, and controllable. To facilitate reproducibility, we release our code, models, and resources at https://prism-festinalente.github.io/; all experiments use the open-source ProactiveBench benchmark.

</details>


### [541] [MAGIC: A Co-Evolving Attacker-Defender Adversarial Game for Robust LLM Safety](https://arxiv.org/abs/2602.01539)
*Xiaoyu Wen,Zhida He,Han Qi,Ziyu Wan,Zhongtian Ma,Ying Wen,Tianhang Zheng,Xingcheng Xu,Chaochao Lu,Qiaosheng Zhang*

Main category: cs.AI

Relevance: 85.0

TL;DR: MAGIC是一个多轮多智能体强化学习框架，将LLM安全对齐建模为对抗性非对称博弈，通过攻击者和防御者的共同进化来提升模型的安全性。


<details>
  <summary>Details</summary>
Motivation: 现有LLM安全防御方法依赖于静态、预收集的数据分布，无法跟上不断演化的对抗攻击。需要一种动态、自适应的安全对齐方法。

Method: 采用多轮多智能体强化学习框架：攻击者智能体学习迭代重写原始查询为欺骗性提示，防御者智能体同时优化策略以识别并拒绝此类输入，形成共同进化过程。

Result: 实验验证了框架的有效性，展示了优越的防御成功率，同时不损害模型的有用性。攻击者通过迭代RL训练演化出新颖的、以前未见过的组合策略。

Conclusion: MAGIC框架通过对抗性共同进化实现了更强大的LLM安全对齐，为动态安全防御提供了新思路。

Abstract: Ensuring robust safety alignment is crucial for Large Language Models (LLMs), yet existing defenses often lag behind evolving adversarial attacks due to their \textbf{reliance on static, pre-collected data distributions}. In this paper, we introduce \textbf{MAGIC}, a novel multi-turn multi-agent reinforcement learning framework that formulates LLM safety alignment as an adversarial asymmetric game. Specifically, an attacker agent learns to iteratively rewrite original queries into deceptive prompts, while a defender agent simultaneously optimizes its policy to recognize and refuse such inputs. This dynamic process triggers a \textbf{co-evolution}, where the attacker's ever-changing strategies continuously uncover long-tail vulnerabilities, driving the defender to generalize to unseen attack patterns. Remarkably, we observe that the attacker, endowed with initial reasoning ability, evolves \textbf{novel, previously unseen combinatorial strategies} through iterative RL training, underscoring our method's substantial potential. Theoretically, we provide insights into a more robust game equilibrium and derive safety guarantees. Extensive experiments validate our framework's effectiveness, demonstrating superior defense success rates without compromising the helpfulness of the model. Our code is available at https://github.com/BattleWen/MAGIC.

</details>


### [542] [Autonomous Question Formation for Large Language Model-Driven AI Systems](https://arxiv.org/abs/2602.01556)
*Hong Su*

Main category: cs.AI

Relevance: 85.0

TL;DR: 提出基于人类模拟的框架，使AI系统能通过推理内部状态、环境观察和与其他AI系统的交互来自主形成问题和设定任务，将问题形成作为任务选择和执行的优先决策过程。


<details>
  <summary>Details</summary>
Motivation: 现有LLM驱动的AI系统大多依赖预定义任务和固定提示，限制了在环境变化时自主识别应解决问题的能力。需要让AI系统能自主形成问题和设定任务。

Method: 提出人类模拟框架，将问题形成作为一等决策过程，整合内部驱动、环境感知和智能体间感知三种提示范围来逐步扩展认知覆盖。支持从经验中学习问题形成过程。

Result: 在多智能体模拟环境中，环境感知提示显著减少无进食事件，智能体间感知提示在20天模拟中进一步减少累计无进食事件超过60%，具有统计显著性改进(p<0.05)。

Conclusion: 该框架使AI系统能自主形成问题和设定任务，通过整合多层次认知范围提高了在动态开放环境中的适应性和决策质量。

Abstract: Large language model (LLM)-driven AI systems are increasingly important for autonomous decision-making in dynamic and open environments. However, most existing systems rely on predefined tasks and fixed prompts, limiting their ability to autonomously identify what problems should be solved when environmental conditions change. In this paper, we propose a human-simulation-based framework that enables AI systems to autonomously form questions and set tasks by reasoning over their internal states, environmental observations, and interactions with other AI systems. The proposed method treats question formation as a first-class decision process preceding task selection and execution, and integrates internal-driven, environment-aware, and inter-agent-aware prompting scopes to progressively expand cognitive coverage. In addition, the framework supports learning the question-formation process from experience, allowing the system to improve its adaptability and decision quality over time. xperimental results in a multi-agent simulation environment show that environment-aware prompting significantly reduces no-eat events compared with the internal-driven baseline, and inter-agent-aware prompting further reduces cumulative no-eat events by more than 60% over a 20-day simulation, with statistically significant improvements (p < 0.05).

</details>


### [543] [Reasoning with Autoregressive-Diffusion Collaborative Thoughts](https://arxiv.org/abs/2602.01608)
*Mu Yuan,Liekang Zeng,Guoliang Xing,Lan Zhang,Yunhao Liu*

Main category: cs.AI

Relevance: 85.0

TL;DR: 提出Collaborative Thoughts框架，让自回归模型和扩散模型通过闭环交互协同工作：自回归模型负责结构化规划和约束管理，扩散模型生成中间视觉表示，视觉批评模块评估是否满足结构物理要求，通过反馈迭代优化生成过程。


<details>
  <summary>Details</summary>
Motivation: 自回归模型擅长序列规划和约束组合，但在需要空间或物理基础的任务上表现不佳；扩散模型能捕捉丰富的空间结构，但缺乏逐步逻辑控制来满足复杂多阶段约束或可靠识别纠正错误。需要结合两者优势。

Method: Collaborative Thoughts框架：1) 自回归模型进行结构化规划和约束管理；2) 扩散模型将这些约束实例化为中间视觉表示；3) 视觉批评模块评估视觉表示是否满足结构物理要求；4) 反馈用于迭代优化后续规划和生成步骤，减少跨模态错误传播。

Result: 通过代表性示例展示，Collaborative Thoughts能够提高空间推理的可靠性和生成的可控性。该框架使用相同的协作循环，适用于自回归问答和基于扩散的视觉生成任务。

Conclusion: Collaborative Thoughts通过自回归和扩散模型的协同工作，结合了两种生成范式的优势，提高了复杂约束下生成任务的可靠性和可控性，为多模态生成提供了新思路。

Abstract: Autoregressive and diffusion models represent two complementary generative paradigms. Autoregressive models excel at sequential planning and constraint composition, yet struggle with tasks that require explicit spatial or physical grounding. Diffusion models, in contrast, capture rich spatial structure through high-dimensional generation, but lack the stepwise logical control needed to satisfy complex, multi-stage constraints or to reliably identify and correct errors. We introduce Collaborative Thoughts, a unified collaborative framework that enables autoregressive and diffusion models to reason and generate jointly through a closed-loop interaction. In Collaborative Thoughts, autoregressive models perform structured planning and constraint management, diffusion models instantiate these constraints as intermediate visual thoughts, and a vision-based critic module evaluates whether the visual thoughts satisfy the intended structural and physical requirements. This feedback is then used to iteratively refine subsequent planning and generation steps, mitigating error propagation across modalities. Importantly, Collaborative Thoughts uses the same collaborative loop regardless of whether the task is autoregressive question answering or diffusion-based visual generation. Through representative examples, we illustrate how Collaborative Thoughts can improve the reliability of spatial reasoning and the controllability of generation.

</details>


### [544] [ProjDevBench: Benchmarking AI Coding Agents on End-to-End Project Development](https://arxiv.org/abs/2602.01655)
*Pengrui Lu,Shiqi Zhang,Yunzhong Hou,Lyumanshan Ye,Chaoyi Huang,Zixi Chen,Ji Zeng,Hantao Jiang,Pengfei Liu,Yiwei Wang,Ming-Hsuan Yang*

Main category: cs.AI

Relevance: 85.0

TL;DR: ProjDevBench是一个端到端的编码代理评估基准，通过项目需求评估代理生成的完整代码库，结合在线评测和LLM辅助代码审查，在20个编程问题上测试6个编码代理，总体通过率27.38%。


<details>
  <summary>Details</summary>
Motivation: 现有编码代理评估主要关注问题级别的bug修复，缺乏端到端的开发评估。需要评估编码代理从简单提示生成完整代码库的能力，包括系统架构设计、功能正确性和迭代解决方案优化。

Method: 提出ProjDevBench基准，结合在线评测(Online Judge)测试和LLM辅助代码审查。包含20个编程问题，涵盖8个类别，既有概念导向任务也有实际应用场景。评估6个基于不同LLM后端的编码代理。

Result: 总体接受率27.38%。编码代理能处理基本功能和数据结构，但在复杂系统设计、时间复杂度优化和资源管理方面表现不佳。基准公开可用。

Conclusion: 编码代理在端到端开发方面仍有局限，特别是在复杂系统设计方面。ProjDevBench为评估编码代理的完整项目开发能力提供了标准化基准。

Abstract: Recent coding agents can generate complete codebases from simple prompts, yet existing evaluations focus on issue-level bug fixing and lag behind end-to-end development. We introduce ProjDevBench, an end-to-end benchmark that provides project requirements to coding agents and evaluates the resulting repositories. Combining Online Judge (OJ) testing with LLM-assisted code review, the benchmark evaluates agents on (1) system architecture design, (2) functional correctness, and (3) iterative solution refinement. We curate 20 programming problems across 8 categories, covering both concept-oriented tasks and real-world application scenarios, and evaluate six coding agents built on different LLM backends. Our evaluation reports an overall acceptance rate of 27.38%: agents handle basic functionality and data structures but struggle with complex system design, time complexity optimization, and resource management. Our benchmark is available at https://github.com/zsworld6/projdevbench.

</details>


### [545] [TRIP-Bench: A Benchmark for Long-Horizon Interactive Agents in Real-World Scenarios](https://arxiv.org/abs/2602.01675)
*Yuanzhe Shen,Zisu Huang,Zhengyuan Wang,Muzhao Tian,Zhengkang Guo,Chenyang Zhang,Shuaiyu Zhou,Zengjie Hu,Dailin Li,Jingwen Xu,Kaimin Wang,Wenhao Liu,Tianlong Li,Fengpeng Yue,Feng Hong,Cao Liu,Ke Zeng*

Main category: cs.AI

Relevance: 85.0

TL;DR: TRIP-Bench是一个基于真实旅行规划场景的长视野基准测试，包含18个工具和40+旅行需求，支持自动评估。实验显示先进模型在简单集上最多达到50%成功率，在困难集上低于10%。提出的GTPO在线多轮强化学习方法在Qwen2.5-32B-Instruct上表现优于Gemini-3-Pro。


<details>
  <summary>Details</summary>
Motivation: 现有基准测试在表示现实世界复杂挑战方面存在不足，特别是强制执行全局约束、协调多工具推理以及适应长期多轮交互中的用户行为变化。需要更贴近实际应用场景的评估框架来推动实用长视野交互智能体的发展。

Method: 1) 提出TRIP-Bench基准：基于真实旅行规划场景，包含18个精选工具和40+旅行需求，支持自动评估，包含不同难度划分（困难集强调长且模糊的交互、风格转变、可行性变化和迭代版本修订）。2) 提出GTPO方法：在线多轮强化学习方法，采用专门的奖励归一化和奖励差分技术。

Result: 1) 基准测试结果：即使先进模型在简单集上最多达到50%成功率，在困难子集上性能降至10%以下。2) GTPO方法效果：应用于Qwen2.5-32B-Instruct后，在约束满足和交互鲁棒性方面表现优于Gemini-3-Pro。

Conclusion: TRIP-Bench能够推动实用长视野交互智能体的发展，而GTPO为鲁棒的长视野训练提供了有效的在线强化学习方案。

Abstract: As LLM-based agents are deployed in increasingly complex real-world settings, existing benchmarks underrepresent key challenges such as enforcing global constraints, coordinating multi-tool reasoning, and adapting to evolving user behavior over long, multi-turn interactions. To bridge this gap, we introduce \textbf{TRIP-Bench}, a long-horizon benchmark grounded in realistic travel-planning scenarios. TRIP-Bench leverages real-world data, offers 18 curated tools and 40+ travel requirements, and supports automated evaluation. It includes splits of varying difficulty; the hard split emphasizes long and ambiguous interactions, style shifts, feasibility changes, and iterative version revision. Dialogues span up to 15 user turns, can involve 150+ tool calls, and may exceed 200k tokens of context. Experiments show that even advanced models achieve at most 50\% success on the easy split, with performance dropping below 10\% on hard subsets. We further propose \textbf{GTPO}, an online multi-turn reinforcement learning method with specialized reward normalization and reward differencing. Applied to Qwen2.5-32B-Instruct, GTPO improves constraint satisfaction and interaction robustness, outperforming Gemini-3-Pro in our evaluation. We expect TRIP-Bench to advance practical long-horizon interactive agents, and GTPO to provide an effective online RL recipe for robust long-horizon training.

</details>


### [546] [What LLMs Think When You Don't Tell Them What to Think About?](https://arxiv.org/abs/2602.01689)
*Yongchan Kwon,James Zou*

Main category: cs.AI

Relevance: 85.0

TL;DR: 该论文通过最小化、主题中立的输入来研究LLMs的无约束生成行为，发现不同模型家族存在系统性的主题偏好和退化模式，揭示了模型内在的生成倾向。


<details>
  <summary>Details</summary>
Motivation: 现有LLM分析大多依赖特定主题或任务的提示词，限制了观察范围。为了更全面地理解LLM行为，特别是用于可靠监控和AI安全，需要研究模型在最小输入下的无约束生成行为。

Method: 使用最小化、主题中立的输入（如"Here is"、"This is"等）来激发LLMs的无约束生成，收集了16个LLM的256,000个样本，分析其主题偏好、内容深度和退化模式。

Result: 发现不同模型家族存在强烈的系统性主题偏好：GPT-OSS偏好编程和数学内容，Llama偏好文学内容，DeepSeek偏好宗教内容，Qwen偏好多选题。还观察到内容深度差异和独特的退化模式（如Llama生成重复的个人社交媒体链接）。

Conclusion: 即使在没有明确主题提示的情况下，LLMs仍表现出强烈的内在生成偏好，这些偏好反映了模型训练数据的特征和架构差异。这些发现对LLM评估、监控和安全具有重要意义。

Abstract: Characterizing the behavior of large language models (LLMs) across diverse settings is critical for reliable monitoring and AI safety. However, most existing analyses rely on topic- or task-specific prompts, which can substantially limit what can be observed. In this work, we study what LLMs generate from minimal, topic-neutral inputs and probe their near-unconstrained generative behavior. Despite the absence of explicit topics, model outputs cover a broad semantic space, and surprisingly, each model family exhibits strong and systematic topical preferences. GPT-OSS predominantly generates programming (27.1%) and mathematical content (24.6%), whereas Llama most frequently generates literary content (9.1%). DeepSeek often generates religious content, while Qwen frequently generates multiple-choice questions. Beyond topical preferences, we also observe differences in content specialization and depth: GPT-OSS often generates more technically advanced content (e.g., dynamic programming) compared with other models (e.g., basic Python). Furthermore, we find that the near-unconstrained generation often degenerates into repetitive phrases, revealing interesting behaviors unique to each model family. For instance, degenerate outputs from Llama include multiple URLs pointing to personal Facebook and Instagram accounts. We release the complete dataset of 256,000 samples from 16 LLMs, along with a reproducible codebase.

</details>


### [547] [Beyond Dense States: Elevating Sparse Transcoders to Active Operators for Latent Reasoning](https://arxiv.org/abs/2602.01695)
*Yadong Wang,Haodong Chen,Yu Tian,Chuanxing Geng,Dong Liang,Xiang Chen*

Main category: cs.AI

Relevance: 85.0

TL;DR: LSTR提出了一种稀疏潜在推理框架，将稀疏transcoder提升为主动推理算子，通过稀疏语义转换进行多步计算，在保持推理准确性和压缩效率的同时显著提升可解释性。


<details>
  <summary>Details</summary>
Motivation: 现有潜在推理方法将思维链压缩为连续隐状态，但依赖密集的潜在转换，难以解释和控制。而稀疏表示模型虽然能发现人类可解释的语义特征，但主要局限于事后分析。需要解决这一矛盾，实现既高效又可解释的推理。

Method: 提出LSTR框架，使用具有残差跳跃架构的潜在转换Transcoder（LTT），将线性流形传输与稀疏语义更新解耦。通过显式稀疏约束实现可控的语义分辨率，将功能性稀疏transcoder提升为主动推理算子。

Result: 实验表明LSTR在保持推理准确性和压缩效率的同时，显著提高了相对于密集潜在基线的可解释性。因果干预和轨迹分析进一步证明这些稀疏特征在推理过程中既可作为可解释算子，也具有因果有效性。

Conclusion: LSTR成功地将稀疏表示与主动推理相结合，实现了既高效又可解释的潜在推理，为LLM的可解释性和可控推理提供了新方法。

Abstract: Latent reasoning compresses the chain-of-thought (CoT) into continuous hidden states, yet existing methods rely on dense latent transitions that remain difficult to interpret and control. Meanwhile, sparse representation models uncover human-interpretable semantic features but remain largely confined to post-hoc analysis. We reconcile this tension by proposing LSTR (Latent Sparse Transcoder Reasoning), a latent reasoning framework that elevates functional sparse transcoders into active reasoning operators to perform multi-step computation through sparse semantic transitions. At its core, LSTR employs a Latent Transition Transcoder (LTT) with a residual skip architecture that decouples linear manifold transport from sparse semantic updates, enabling controllable semantic resolution via explicit sparsity constraints. Extensive experiments show that LSTR preserves reasoning accuracy and compression efficiency while substantially improving interpretability over dense latent baselines. Causal interventions and trajectory analyses further demonstrate that these sparse features act as both interpretable and causally effective operators in the reasoning process.

</details>


### [548] [Optimizing Prompts for Large Language Models: A Causal Approach](https://arxiv.org/abs/2602.01711)
*Wei Chen,Yanbin Fang,Shuran Fu,Fasheng Xu,Xuan Wei*

Main category: cs.AI

Relevance: 85.0

TL;DR: 提出因果提示优化(CPO)框架，将提示设计重新定义为因果估计问题，通过双机器学习分离提示变化与查询特征的混杂效应，实现高效、低成本的查询特定提示优化


<details>
  <summary>Details</summary>
Motivation: 当前自动提示优化面临两大挑战：1) 静态指令无法适应异构查询；2) 动态方法依赖离线奖励模型，但这类模型本质上是相关性的，混淆了提示效果与查询特征。需要一种能够分离混杂因素、实现精准提示优化的方法。

Method: CPO采用两阶段框架：第一阶段，应用双机器学习(DML)到提示和查询的语义嵌入中，学习离线因果奖励模型，分离提示变化的因果效应与混杂的查询属性；第二阶段，利用这个无偏奖励信号指导资源高效的查询特定提示搜索，无需依赖昂贵的在线评估。

Result: 在数学推理、可视化和数据分析基准测试中，CPO始终优于人工设计的提示和最先进的自动优化器。改进主要体现在困难查询上的鲁棒性提升，而现有方法在这些查询上往往表现不佳。

Conclusion: CPO从根本上重塑了提示优化的经济学：通过将评估从实时模型执行转移到离线因果模型，它能够以在线方法所需推理成本的一小部分实现高精度、按查询定制的优化。因果推断为企业LLM部署中可靠且经济高效的提示优化提供了可扩展的基础。

Abstract: Large Language Models (LLMs) are increasingly embedded in enterprise workflows, yet their performance remains highly sensitive to prompt design. Automatic Prompt Optimization (APO) seeks to mitigate this instability, but existing approaches face two persistent challenges. First, commonly used prompt strategies rely on static instructions that perform well on average but fail to adapt to heterogeneous queries. Second, more dynamic approaches depend on offline reward models that are fundamentally correlational, confounding prompt effectiveness with query characteristics. We propose Causal Prompt Optimization (CPO), a framework that reframes prompt design as a problem of causal estimation. CPO operates in two stages. First, it learns an offline causal reward model by applying Double Machine Learning (DML) to semantic embeddings of prompts and queries, isolating the causal effect of prompt variations from confounding query attributes. Second, it utilizes this unbiased reward signal to guide a resource-efficient search for query-specific prompts without relying on costly online evaluation. We evaluate CPO across benchmarks in mathematical reasoning, visualization, and data analytics. CPO consistently outperforms human-engineered prompts and state-of-the-art automated optimizers. The gains are driven primarily by improved robustness on hard queries, where existing methods tend to deteriorate. Beyond performance, CPO fundamentally reshapes the economics of prompt optimization: by shifting evaluation from real-time model execution to an offline causal model, it enables high-precision, per-query customization at a fraction of the inference cost required by online methods. Together, these results establish causal inference as a scalable foundation for reliable and cost-efficient prompt optimization in enterprise LLM deployments.

</details>


### [549] [PRISM: Parametrically Refactoring Inference for Speculative Sampling Draft Models](https://arxiv.org/abs/2602.01762)
*Xuliang Wang,Yuetao Chen,Maochan Zhen,Fang Liu,Xinzhou Zheng,Xingwu Liu,Hong Xu,Ming Li*

Main category: cs.AI

Relevance: 85.0

TL;DR: PRISM是一种创新的推测解码架构，通过解耦计算路径将预测步骤的计算分散到不同参数集，从而在保持最小草稿延迟的同时实现卓越的接受长度，显著提升LLM解码吞吐量。


<details>
  <summary>Details</summary>
Motivation: 当前推测解码方法为了提升草稿质量倾向于使用更大的参数化草稿模型，但这带来了显著的计算开销。现有工作试图平衡预测准确性和计算延迟之间的权衡，本文通过架构创新来解决这一根本困境。

Method: PRISM（预测步骤计算解耦架构）将每个预测步骤的计算分散到不同的参数集，重构草稿模型的计算路径，成功地将模型容量与推理成本解耦。该方法允许在保持低延迟的同时使用更大的模型容量。

Result: PRISM在所有现有草稿架构中表现最佳，实现了卓越的接受长度，同时保持最小的草稿延迟，获得优越的端到端加速。在高度优化的推理引擎上，PRISM将解码吞吐量提升了2.6倍以上。

Conclusion: PRISM通过架构创新成功解决了推测解码中模型容量与推理成本的根本权衡问题，为LLM加速提供了有效的解决方案。重新审视扩展定律发现，PRISM在数据量扩展时比其他草稿架构更有效地扩展。

Abstract: Large Language Models (LLMs), constrained by their auto-regressive nature, suffer from slow decoding. Speculative decoding methods have emerged as a promising solution to accelerate LLM decoding, attracting attention from both systems and AI research communities. Recently, the pursuit of better draft quality has driven a trend toward parametrically larger draft models, which inevitably introduces substantial computational overhead. While existing work attempts to balance the trade-off between prediction accuracy and compute latency, we address this fundamental dilemma through architectural innovation.
  We propose PRISM, which disaggregates the computation of each predictive step across different parameter sets, refactoring the computational pathways of draft models to successfully decouple model capacity from inference cost. Through extensive experiments, we demonstrate that PRISM outperforms all existing draft architectures, achieving exceptional acceptance lengths while maintaining minimal draft latency for superior end-to-end speedup. We also re-examine scaling laws with PRISM, revealing that PRISM scales more effectively with expanding data volumes than other draft architectures. Through rigorous and fair comparison, we show that PRISM boosts the decoding throughput of an already highly optimized inference engine by more than 2.6x.

</details>


### [550] [ORCH: many analyses, one merge-a deterministic multi-agent orchestrator for discrete-choice reasoning with EMA-guided routing](https://arxiv.org/abs/2602.01797)
*Hanlin Zhou,Huah Yong Chan*

Main category: cs.AI

Relevance: 85.0

TL;DR: ORCH是一个确定性的多智能体协调框架，用于离散选择推理任务。它采用"多分析、一决策"范式，通过固定规则协调异构LLM，实现可预测、可复现且无需训练的推理流程。


<details>
  <summary>Details</summary>
Motivation: 现有多智能体系统通常依赖随机路由或启发式方法，导致行为难以复现、决策过程难以解释。需要一种确定性的协调框架来提高LLM多智能体系统的可控性和可解释性。

Method: ORCH采用"多分析、一决策"范式：多个基础模型独立生成结构化分析，专用合并智能体输出最终选择。框架使用固定规则进行任务分解和答案聚合，保持流程可预测且无需训练。可选引入EMA引导的路由器，基于历史准确性、延迟或成本更新智能体选择。

Result: 在MMLU、MMLU-Pro和GSM8K上的实验显示，ORCH持续优于单模型基线和多数投票集成。在MMLU-Pro上准确率提升超过10个百分点，在GSM8K上提升超过50个百分点。EMA路由器提供额外0.7-2.0个百分点的准确率提升。

Conclusion: ORCH为离散选择推理提供了一个实用、可控、可解释且可部署的LLM多智能体系统路径，解决了现有系统随机性和不可复现性的问题。

Abstract: Recent advances in large-scale language models (LLMs) have made multi-agent architectures attractive for challenging reasoning tasks. However, many existing systems rely on stochastic routing or ad-hoc heuristics, making their behavior difficult to reproduce and their decision process hard to interpret. We propose ORCH, a deterministic coordination framework for discrete-choice reasoning that orchestrates heterogeneous LLMs. ORCH follows a ``many analyses, one decision'' paradigm: multiple base models independently produce structured analyses, and a dedicated merge agent outputs the final choice. The framework uses fixed rules for task decomposition and answer aggregation, keeping the pipeline predictable, reproducible, and training-free. Determinism here refers to fixed routing and aggregation rules under a fixed evaluation protocol, rather than strict bit-level reproducibility across deployments. To exploit model complementarity, we optionally introduce an EMA-guided router that updates agent selection using historical accuracy, latency, or cost; since it relies on answer-based feedback, it is mainly intended for benchmarking, controlled evaluation, or delayed-feedback settings. Experiments on MMLU, MMLU-Pro, and GSM8K show that ORCH consistently outperforms single-model baselines and a majority-vote ensemble. On MMLU-Pro, ORCH improves accuracy by over 10 points compared to the strongest baseline, and on GSM8K it yields gains exceeding 50 points; McNemar tests confirm statistical significance. The EMA router provides an additional 0.7--2.0 point accuracy boost, and ablations show that both multi-agent collaboration and routing contribute substantially. Overall, ORCH offers a practical path toward controllable, interpretable, and deployment-ready LLM-based agent systems for discrete-choice reasoning.

</details>


### [551] [ROMA: Recursive Open Meta-Agent Framework for Long-Horizon Multi-Agent Systems](https://arxiv.org/abs/2602.01848)
*Salaheddin Alzu'bi,Baran Nama,Arda Kaz,Anushri Eswaran,Weiyuan Chen,Sarvesh Khetan,Rishab Bala,Tu Vu,Sewoong Oh*

Main category: cs.AI

Relevance: 85.0

TL;DR: ROMA是一个递归开放元代理框架，通过任务分解和结构化聚合解决长时程任务中的性能瓶颈，支持异构多智能体系统，结合GEPA+提示搜索实现领先的系统级性能。


<details>
  <summary>Details</summary>
Motivation: 当前智能体框架在长时程任务中表现不佳，随着推理深度增加，顺序编排变得脆弱，上下文窗口限制性能，不透明的执行轨迹难以调试。需要解决这些限制。

Method: 引入ROMA框架，通过递归任务分解和结构化聚合，将目标分解为依赖感知的子任务树并行执行，同时聚合压缩和验证中间结果以控制上下文增长。框架围绕四个模块化角色标准化智能体构建：Atomizer、Planner、Executor、Aggregator。结合GEPA+遗传帕累托提示提议器进行提示搜索。

Result: 在SEAL-0基准上，ROMA+GLM-4.6相比Kimi-Researcher准确率提升9.9%；在EQ-Bench长文本生成基准上，ROMA+DeepSeek-V3达到与Claude Sonnet 4.5相当的性能。

Conclusion: 递归模块化智能体架构能够扩展推理深度，同时保持可解释性、灵活性和模型无关性，为解决长时程任务提供了有效框架。

Abstract: Current agentic frameworks underperform on long-horizon tasks. As reasoning depth increases, sequential orchestration becomes brittle, context windows impose hard limits that degrade performance, and opaque execution traces make failures difficult to localize or debug. We introduce ROMA (Recursive Open Meta-Agents), a domain-agnostic framework that addresses these limitations through recursive task decomposition and structured aggregation. ROMA decomposes goals into dependency-aware subtask trees that can be executed in parallel, while aggregation compresses and validates intermediate results to control context growth. Our framework standardizes agent construction around four modular roles --Atomizer (which decides whether a task should be decomposed), Planner, Executor, and Aggregator -- which cleanly separate orchestration from model selection and enable transparent, hierarchical execution traces. This design supports heterogeneous multi-agent systems that mix models and tools according to cost, latency, and capability. To adapt ROMA to specific tasks without fine-tuning, we further introduce GEPA$+$, an improved Genetic-Pareto prompt proposer that searches over prompts within ROMA's component hierarchy while preserving interface contracts. We show that ROMA, combined with GEPA+, delivers leading system-level performance on reasoning and long-form generation benchmarks. On SEAL-0, which evaluates reasoning over conflicting web evidence, ROMA instantiated with GLM-4.6 improves accuracy by 9.9\% over Kimi-Researcher. On EQ-Bench, a long-form writing benchmark, ROMA enables DeepSeek-V3 to match the performance of leading closed-source models such as Claude Sonnet 4.5. Our results demonstrate that recursive, modular agent architectures can scale reasoning depth while remaining interpretable, flexible, and model-agnostic.

</details>


### [552] [ProcMEM: Learning Reusable Procedural Memory from Experience via Non-Parametric PPO for LLM Agents](https://arxiv.org/abs/2602.01869)
*Qirui Mi,Zhijian Ma,Mengyue Yang,Haoxuan Li,Yisen Wang,Haifeng Zhang,Jun Wang*

Main category: cs.AI

Relevance: 85.0

TL;DR: ProcMEM：无需参数更新的LLM智能体程序性记忆框架，通过将交互经验转化为可执行技能，实现经验复用，提升决策效率和稳定性


<details>
  <summary>Details</summary>
Motivation: 当前LLM驱动的智能体在顺序决策中主要依赖即时推理，即使在重复场景下也重新推导解决方案，导致计算冗余和执行不稳定。缺乏经验复用机制限制了智能体的长期自主性。

Method: 提出ProcMEM框架，通过形式化Skill-MDP将被动经验叙述转化为可执行技能（包含激活、执行、终止条件）。引入非参数PPO，利用语义梯度生成高质量候选技能，并通过PPO Gate进行技能验证。采用基于分数的维护机制保持紧凑高质量的程序性记忆。

Result: 实验在领域内、跨任务和跨智能体场景中显示，ProcMEM实现了更高的复用率和显著的性能提升，同时保持极端的内存压缩。可视化进化轨迹和技能分布展示了框架如何透明地积累、精炼和复用程序性知识。

Conclusion: ProcMEM为LLM智能体提供了无需参数更新的程序性记忆学习框架，有效解决了经验复用问题，提升了决策效率和稳定性，促进了智能体的长期自主性。

Abstract: LLM-driven agents demonstrate strong performance in sequential decision-making but often rely on on-the-fly reasoning, re-deriving solutions even in recurring scenarios. This insufficient experience reuse leads to computational redundancy and execution instability. To bridge this gap, we propose ProcMEM, a framework that enables agents to autonomously learn procedural memory from interaction experiences without parameter updates. By formalizing a Skill-MDP, ProcMEM transforms passive episodic narratives into executable Skills defined by activation, execution, and termination conditions to ensure executability. To achieve reliable reusability without capability degradation, we introduce Non-Parametric PPO, which leverages semantic gradients for high-quality candidate generation and a PPO Gate for robust Skill verification. Through score-based maintenance, ProcMEM sustains compact, high-quality procedural memory. Experimental results across in-domain, cross-task, and cross-agent scenarios demonstrate that ProcMEM achieves superior reuse rates and significant performance gains with extreme memory compression. Visualized evolutionary trajectories and Skill distributions further reveal how ProcMEM transparently accumulates, refines, and reuses procedural knowledge to facilitate long-term autonomy.

</details>


### [553] [Entropy-Guided Data-Efficient Training for Multimodal Reasoning Reward Models](https://arxiv.org/abs/2602.01884)
*Shidong Yang,Tongwen Huang,Hao Wen,Yong Wang,Li Chen,Xiangxiang Chu*

Main category: cs.AI

Relevance: 85.0

TL;DR: 提出基于熵引导训练(EGT)的多模态推理奖励模型，通过响应熵作为无监督代理来识别标注噪声和样本难度，包含熵引导数据筛选和渐进式训练策略，在三个基准测试中超越现有方法。


<details>
  <summary>Details</summary>
Motivation: 多模态奖励模型对于对齐多模态大语言模型与人类偏好至关重要。现有方法面临两个关键挑战：1) 偏好数据集中的固有噪声会降低模型性能；2) 传统训练方法效率低下，忽略了样本难度的差异。

Method: 提出熵引导训练(EGT)方法：1) 发现响应熵与准确性之间存在强相关性，熵可作为标注噪声和样本难度的无监督代理；2) 熵引导数据筛选：减轻不可靠样本的影响；3) 熵引导训练策略：逐步引入更复杂的示例进行渐进式训练。

Result: 在三个基准测试上的广泛实验表明，EGT训练模型始终优于最先进的多模态奖励模型。

Conclusion: 响应熵是识别标注噪声和样本难度的有效无监督指标，基于熵的引导训练策略能显著提升多模态推理奖励模型的性能。

Abstract: Multimodal reward models are crucial for aligning multimodal large language models with human preferences. Recent works have incorporated reasoning capabilities into these models, achieving promising results. However, training these models suffers from two critical challenges: (1) the inherent noise in preference datasets, which degrades model performance, and (2) the inefficiency of conventional training methods, which ignore the differences in sample difficulty. In this paper, we identify a strong correlation between response entropy and accuracy, indicating that entropy can serve as a reliable and unsupervised proxy for annotation noise and sample difficulty. Based on this insight, we propose a novel Entropy-Guided Training (EGT) approach for multimodal reasoning reward models, which combines two strategies: (1) entropy-guided data curation to mitigate the impact of unreliable samples, and (2) an entropy-guided training strategy that progressively introduces more complex examples. Extensive experiments across three benchmarks show that the EGT-trained model consistently outperforms state-of-the-art multimodal reward models.

</details>


### [554] [Geometric Analysis of Token Selection in Multi-Head Attention](https://arxiv.org/abs/2602.01893)
*Timur Mudarisov,Mikhal Burtsev,Tatiana Petrova,Radu State*

Main category: cs.AI

Relevance: 85.0

TL;DR: 该论文提出了一个几何框架来分析LLM中的多头注意力机制，将其视为top-N选择器，定义了Precision、Recall、F-score等几何指标来量化token选择的可分性，并通过理论和实验验证了注意力机制作为结构化几何分类器的行为。


<details>
  <summary>Details</summary>
Motivation: 现有的注意力机制分析缺乏几何视角，难以量化token选择的精确性。作者希望从几何角度理解注意力机制如何选择和分离token，为注意力头提供可解释性，并为几何感知的稀疏化和注意力设计提供理论基础。

Method: 1) 将标准注意力机制视为top-N选择器，在value-state空间直接研究其行为；2) 定义几何指标（Precision、Recall、F-score）量化选中与未选中token的可分性；3) 在经验假设下推导非渐近边界；4) 在LLaMA-2-7B、Gemma-7B、Mistral-7B上进行实证验证。

Result: 理论预测了小N操作区域具有最强的非平凡可分性，阐明了序列长度和sink相似性如何影响几何指标。实证结果显示：top-N选择增强了可分性，sink相似性与Recall相关。在LLaMA-2-7B中发现注意力头分为三种类型（Retriever、Mixer、Reset），具有不同的几何特征。

Conclusion: 注意力机制表现为具有可测量token选择标准的结构化几何分类器，为注意力头级别的可解释性提供了新视角，并为几何感知的稀疏化和注意力设计提供了理论基础。

Abstract: We present a geometric framework for analysing multi-head attention in large language models (LLMs). Without altering the mechanism, we view standard attention through a top-N selection lens and study its behaviour directly in value-state space. We define geometric metrics - Precision, Recall, and F-score - to quantify separability between selected and non-selected tokens, and derive non-asymptotic bounds with explicit dependence on dimension and margin under empirically motivated assumptions (stable value norms with a compressed sink token, exponential similarity decay, and piecewise attention weight profiles). The theory predicts a small-N operating regime of strongest non-trivial separability and clarifies how sequence length and sink similarity shape the metrics. Empirically, across LLaMA-2-7B, Gemma-7B, and Mistral-7B, measurements closely track the theoretical envelopes: top-N selection sharpens separability, sink similarity correlates with Recall. We also found that in LLaMA-2-7B heads specialize into three regimes - Retriever, Mixer, Reset - with distinct geometric signatures. Overall, attention behaves as a structured geometric classifier with measurable criteria for token selection, offering head level interpretability and informing geometry-aware sparsification and design of attention in LLMs.

</details>


### [555] [Small Generalizable Prompt Predictive Models Can Steer Efficient RL Post-Training of Large Reasoning Models](https://arxiv.org/abs/2602.01970)
*Yun Qu,Qi Wang,Yixiu Mao,Heming Zou,Yuhang Jiang,Weijie Liu,Clive Bai,Kai Yang,Yangkun Chen,Saiyong Yang,Xiangyang Ji*

Main category: cs.AI

Relevance: 85.0

TL;DR: GPS提出了一种可泛化的预测性提示选择方法，通过轻量级生成模型进行贝叶斯推断，选择中等难度的提示批次，显著提升强化学习训练效率和最终性能。


<details>
  <summary>Details</summary>
Motivation: 强化学习虽然能增强大语言模型的推理能力，但通常需要高计算成本，因为涉及大量rollout优化。在线提示选择通过优先选择信息丰富的提示来提高训练效率，但现有方法要么依赖昂贵的精确评估，要么构建缺乏跨提示泛化能力的特定提示预测模型。

Method: 提出GPS方法：1) 使用轻量级生成模型在共享优化历史上进行训练，对提示难度进行贝叶斯推断；2) 采用中等难度优先原则；3) 结合历史锚定的多样性；4) 构建小型预测模型，在测试时也能泛化，实现高效计算分配。

Result: 在多种推理基准测试中，GPS在训练效率、最终性能和测试时效率方面都显著优于现有基线方法。

Conclusion: GPS通过可泛化的预测性提示选择，有效解决了强化学习中计算成本高的问题，实现了训练效率和模型性能的双重提升。

Abstract: Reinforcement learning enhances the reasoning capabilities of large language models but often involves high computational costs due to rollout-intensive optimization. Online prompt selection presents a plausible solution by prioritizing informative prompts to improve training efficiency. However, current methods either depend on costly, exact evaluations or construct prompt-specific predictive models lacking generalization across prompts. This study introduces Generalizable Predictive Prompt Selection (GPS), which performs Bayesian inference towards prompt difficulty using a lightweight generative model trained on the shared optimization history. Intermediate-difficulty prioritization and history-anchored diversity are incorporated into the batch acquisition principle to select informative prompt batches. The small predictive model also generalizes at test-time for efficient computational allocation. Experiments across varied reasoning benchmarks indicate GPS's substantial improvements in training efficiency, final performance, and test-time efficiency over superior baseline methods.

</details>


### [556] [Evolving from Tool User to Creator via Training-Free Experience Reuse in Multimodal Reasoning](https://arxiv.org/abs/2602.01983)
*Xintian Shen,Jiawei Chen,Lihao Zheng,Hao Ma,Tao Wei,Kun Zhan*

Main category: cs.AI

Relevance: 85.0

TL;DR: UCT框架让LLM从工具使用者转变为工具创造者，通过提取推理经验创建自适应工具库，无需训练即可持续改进推理能力


<details>
  <summary>Details</summary>
Motivation: 现有工具集成推理模型存在三个主要问题：1) 固定工具无法应对开放性问题；2) 错误工具输出会误导LLM；3) 工具构建需要大量人工工作。作者认为LLM的推理轨迹蕴含隐式问题解决能力，应加以利用。

Method: 提出UCT训练免费框架：1) 从LLM推理经验中提取可重用资产；2) 在推理过程中自适应创建和更新工具；3) 引入记忆巩固机制维护工具库，确保高重用性。

Result: 在数学和科学推理任务的多领域基准测试中，性能显著提升+20.86%和+23.04%，验证了代理的自进化能力。

Conclusion: UCT为增强工具集成推理模型能力提供了新范式，通过自动化工具构建和持续改进，使代理系统无需额外训练即可进步。

Abstract: Existing Tool-Integrated Reasoning (TIR) models have effectively extended the question-answering capabilities of LLMs by incorporating external tools. However, real-world scenarios present numerous open-ended problems where fixed tools often fail to meet task requirements. Furthermore, the lack of self-optimization mechanisms means that erroneous tool outputs can mislead the LLM's responses. Additionally, the construction of existing tools entails significant manual effort, which consequently constrains their applicability. Recognizing that the reasoning traces of LLMs encapsulate implicit problem-solving capabilities, we propose UCT, a novel training-free framework that transforms agents from tool users to tool creators. This approach harvests reasoning experiences and distills them into reusable assets. This method transforms the agent from a mere tool user into a tool creator, enabling adaptive tool creation and self-updating during the inference process. We also introduce a memory consolidation mechanism to maintain the tool library, ensuring high reusability of retained experiential memory for subsequent reasoning tasks. This novel automated tool construction paradigm continuously improves tool quality during reasoning, allowing the overall agent system to progress without additional training. Extensive experiments demonstrate that our method serves as a novel paradigm for enhancing the capabilities of TIR models. In particular, the significant performance gains achieved +20.86%$\uparrow$ and +23.04%$\uparrow$ on benchmarks across multi-domain mathematical and scientific reasoning tasks validate the self-evolving capability of the agent.

</details>


### [557] [Emergent Analogical Reasoning in Transformers](https://arxiv.org/abs/2602.01992)
*Gouki Minegishi,Jingyuan Feng,Hiroki Furuta,Takeshi Kojima,Yusuke Iwasawa,Yutaka Matsuo*

Main category: cs.AI

Relevance: 85.0

TL;DR: 该论文将类比推理形式化为跨类别实体对应关系的推断，基于范畴论中的函子概念，通过合成任务研究Transformer中类比推理的涌现机制，发现其依赖于嵌入空间的几何对齐和Transformer内部的函子应用。


<details>
  <summary>Details</summary>
Motivation: 类比是人类智能的核心能力，但Transformer如何获得和实现类比推理的机制尚不清楚。研究者希望将类比从抽象的认知概念转化为现代神经网络中具体、机制上可解释的现象。

Method: 1. 基于范畴论中的函子概念，将类比推理形式化为跨类别实体对应关系的推断；2. 设计合成任务在受控设置下评估类比推理的涌现；3. 通过机制分析研究Transformer实现类比的两个关键组件：嵌入空间的几何对齐和Transformer内部的函子应用。

Result: 发现类比推理的涌现对数据特征、优化选择和模型规模高度敏感。机制分析表明，Transformer中的类比推理分解为两个关键组件：1) 嵌入空间中关系结构的几何对齐；2) Transformer内部函子的应用。这些机制使模型能够将关系结构从一个类别转移到另一个类别，实现类比。同样的趋势在预训练LLMs中也观察到。

Conclusion: 该研究将类比从抽象的认知概念转化为现代神经网络中具体、机制上可解释的现象，揭示了Transformer实现类比推理的机制，为理解LLMs的推理能力提供了新的视角。

Abstract: Analogy is a central faculty of human intelligence, enabling abstract patterns discovered in one domain to be applied to another. Despite its central role in cognition, the mechanisms by which Transformers acquire and implement analogical reasoning remain poorly understood. In this work, inspired by the notion of functors in category theory, we formalize analogical reasoning as the inference of correspondences between entities across categories. Based on this formulation, we introduce synthetic tasks that evaluate the emergence of analogical reasoning under controlled settings. We find that the emergence of analogical reasoning is highly sensitive to data characteristics, optimization choices, and model scale. Through mechanistic analysis, we show that analogical reasoning in Transformers decomposes into two key components: (1) geometric alignment of relational structure in the embedding space, and (2) the application of a functor within the Transformer. These mechanisms enable models to transfer relational structure from one category to another, realizing analogy. Finally, we quantify these effects and find that the same trends are observed in pretrained LLMs. In doing so, we move analogy from an abstract cognitive notion to a concrete, mechanistically grounded phenomenon in modern neural networks.

</details>


### [558] [Do I Really Know? Learning Factual Self-Verification for Hallucination Reduction](https://arxiv.org/abs/2602.02018)
*Enes Altinisik,Masoomali Fatehkia,Fatih Deniz,Nadir Durrani,Majd Hawasly,Mohammad Raza,Husrev Taha Sencar*

Main category: cs.AI

Relevance: 85.0

TL;DR: VeriFY是一个训练时框架，通过一致性自验证教LLM推理事实不确定性，减少事实幻觉，同时保持召回率


<details>
  <summary>Details</summary>
Motivation: 事实幻觉是LLM的核心挑战。现有方法要么依赖外部后验验证，要么通过微调将不确定性直接映射为弃权，往往导致过于保守的行为。需要一种训练时方法，让LLM学会推理事实不确定性。

Method: VeriFY框架：1）用结构化验证轨迹增强训练，引导模型：生成初始答案→生成并回答验证查询→进行一致性判断→决定回答或弃权；2）引入阶段级损失掩码，排除幻觉答案阶段的训练目标，同时保留对验证行为的监督。

Result: 在多个模型系列和规模上，VeriFY将事实幻觉率降低9.7%至53.3%，召回率仅小幅下降（0.4%至5.7%），在单源训练时能跨数据集泛化。

Conclusion: VeriFY通过一致性自验证有效减少LLM的事实幻觉，在保持召回率的同时显著提升事实准确性，为LLM可信AI提供了有前景的训练时解决方案。

Abstract: Factual hallucination remains a central challenge for large language models (LLMs). Existing mitigation approaches primarily rely on either external post-hoc verification or mapping uncertainty directly to abstention during fine-tuning, often resulting in overly conservative behavior. We propose VeriFY, a training-time framework that teaches LLMs to reason about factual uncertainty through consistency-based self-verification. VeriFY augments training with structured verification traces that guide the model to produce an initial answer, generate and answer a probing verification query, issue a consistency judgment, and then decide whether to answer or abstain. To address the risk of reinforcing hallucinated content when training on augmented traces, we introduce a stage-level loss masking approach that excludes hallucinated answer stages from the training objective while preserving supervision over verification behavior. Across multiple model families and scales, VeriFY reduces factual hallucination rates by 9.7 to 53.3 percent, with only modest reductions in recall (0.4 to 5.7 percent), and generalizes across datasets when trained on a single source. The source code, training data, and trained model checkpoints will be released upon acceptance.

</details>


### [559] [Light Alignment Improves LLM Safety via Model Self-Reflection with a Single Neuron](https://arxiv.org/abs/2602.02027)
*Sicheng Shen,Mingyang Lv,Han Shen,Jialin Wu,Binghao Wang,Zhou Yang,Guobin Shen,Dongcheng Zhao,Feifei Zhao,Yi Zeng*

Main category: cs.AI

Relevance: 85.0

TL;DR: 提出NGSD方法，通过训练小型专家模型和单神经元门控机制，实现轻量级安全对齐，平衡模型能力与外部指导，同时保持实用性和安全性。


<details>
  <summary>Details</summary>
Motivation: 现有LLM安全对齐方法主要依赖后训练，计算成本高且泛化性差。轻量级方法要么依赖预计算的安全注入，要么过度依赖模型自身能力，导致泛化有限、生成效率降低。需要一种轻量、泛化性好的安全对齐方法。

Method: 提出NGSD（Neuron-Gated Safety Decoding）方法：1）低成本训练小型专家模型；2）使用单神经元作为门控机制；3）平衡模型内在能力与外部指导，在解码过程中动态调整生成。

Result: 方法在训练开销和跨模型规模泛化方面有明显优势，能同时保持实用性和增强输出安全性，为LLM的安全实用部署提供了新视角。

Conclusion: NGSD为LLM的轻量级安全对齐提供了有效解决方案，通过低成本的专家模型和智能门控机制，实现了安全性与实用性的平衡，具有实际部署价值。

Abstract: The safety of large language models (LLMs) has increasingly emerged as a fundamental aspect of their development. Existing safety alignment for LLMs is predominantly achieved through post-training methods, which are computationally expensive and often fail to generalize well across different models. A small number of lightweight alignment approaches either rely heavily on prior-computed safety injections or depend excessively on the model's own capabilities, resulting in limited generalization and degraded efficiency and usability during generation. In this work, we propose a safety-aware decoding method that requires only low-cost training of an expert model and employs a single neuron as a gating mechanism. By effectively balancing the model's intrinsic capabilities with external guidance, our approach simultaneously preserves utility and enhances output safety. It demonstrates clear advantages in training overhead and generalization across model scales, offering a new perspective on lightweight alignment for the safe and practical deployment of large language models. Code: https://github.com/Beijing-AISI/NGSD.

</details>


### [560] [Edit Knowledge, Not Just Facts via Multi-Step Reasoning over Background Stories](https://arxiv.org/abs/2602.02028)
*Ya Gao,Kalle Kujanpää,Pekka Marttinen,Harri Valpola,Alexander Ilin*

Main category: cs.AI

Relevance: 85.0

TL;DR: 提出一种基于推理的知识内化训练策略，通过背景故事、多跳问题和知识蒸馏，使LLM能够有效整合新知识并进行多步推理


<details>
  <summary>Details</summary>
Motivation: 当前知识编辑方法主要关注原子事实的记忆，但无法将新知识整合到连贯的框架中并在不同上下文中灵活应用。作者认为知识内化本质上是推理问题而非记忆问题。

Method: 提出基于三个原则的训练策略：1) 将新知识作为连贯的背景故事引入，解释与现有知识的关系；2) 使用自生成的多跳问题进行训练，需要涉及新信息的多步推理；3) 采用知识蒸馏，让学生模型内化教师模型的推理行为而无需访问新信息。

Result: 实验表明，使用该策略训练的模型能够有效利用新获取的知识进行推理，在需要结合多个新事实的挑战性问题中表现出色。

Conclusion: 通过将知识内化视为推理问题而非记忆问题，并采用背景故事、多跳问题和知识蒸馏的训练策略，可以显著提升LLM整合和应用新知识的能力。

Abstract: Enabling artificial intelligence systems, particularly large language models, to integrate new knowledge and flexibly apply it during reasoning remains a central challenge. Existing knowledge editing approaches emphasize atomic facts, improving factual recall but often failing to integrate new information into a coherent framework usable across contexts. In this work, we argue that knowledge internalization is fundamentally a reasoning problem rather than a memorization problem. Consequently, a model should be trained in situations where the new information is instrumental to solving a task, combined with pre-existing knowledge, and exercised through multi-step reasoning. Based on this insight, we propose a training strategy based on three principles. First, new knowledge is introduced as a coherent background story that contextualizes novel facts and explains their relation to existing knowledge. Second, models are trained using self-generated multi-hop questions that require multi-step reasoning involving the new information. Third, training is done using knowledge distillation, forcing a student model to internalize the teacher's reasoning behavior without access to the novel information. Experiments show that models trained with this strategy effectively leverage newly acquired knowledge during reasoning and achieve remarkable performance on challenging questions that require combining multiple new facts.

</details>


### [561] [Hunt Instead of Wait: Evaluating Deep Data Research on Large Language Models](https://arxiv.org/abs/2602.02039)
*Wei Liu,Peijie Yu,Michele Orini,Yali Du,Yulan He*

Main category: cs.AI

Relevance: 85.0

TL;DR: 论文提出"调查性智能"概念，区别于执行性智能，并引入Deep Data Research任务和DDR-Bench基准来评估LLM在自主数据探索中的能力。


<details>
  <summary>Details</summary>
Motivation: 当前LLM评估主要关注执行性智能（完成指定任务），但智能体LLM需要具备自主设定目标和探索的"调查性智能"。数据科学是天然测试场，因为真实分析始于原始数据而非明确查询，但现有基准对此关注不足。

Method: 引入Deep Data Research开放任务，让LLM从数据库中自主提取关键洞察；创建DDR-Bench大规模检查表基准，支持可验证评估；分析前沿模型在长时域探索中的表现。

Result: 前沿模型显示出初步的智能体能力，但长时域探索仍然困难；有效的调查性智能不仅依赖智能体框架或规模扩展，更需要智能体模型的内在策略。

Conclusion: 调查性智能是智能体LLM的关键能力，需要专门基准评估；当前模型在自主探索方面仍有局限，未来研究应关注智能体模型的内在策略设计。

Abstract: The agency expected of Agentic Large Language Models goes beyond answering correctly, requiring autonomy to set goals and decide what to explore. We term this investigatory intelligence, distinguishing it from executional intelligence, which merely completes assigned tasks. Data Science provides a natural testbed, as real-world analysis starts from raw data rather than explicit queries, yet few benchmarks focus on it. To address this, we introduce Deep Data Research (DDR), an open-ended task where LLMs autonomously extract key insights from databases, and DDR-Bench, a large-scale, checklist-based benchmark that enables verifiable evaluation. Results show that while frontier models display emerging agency, long-horizon exploration remains challenging. Our analysis highlights that effective investigatory intelligence depends not only on agent scaffolding or merely scaling, but also on intrinsic strategies of agentic models.

</details>


### [562] [Rethinking the Role of Entropy in Optimizing Tool-Use Behaviors for Large Language Model Agents](https://arxiv.org/abs/2602.02050)
*Zeping Li,Hongru Wang,Yiwen Zhao,Guanhua Chen,Yixia Li,Keyang Chen,Yixin Cao,Guangnan Ye,Hongfeng Chai,Mengdi Wang,Zhenfei Yin*

Main category: cs.AI

Relevance: 85.0

TL;DR: 提出使用熵减作为监督信号来优化LLM工具使用行为，通过稀疏结果奖励和密集过程奖励两种策略，分别减少工具调用72.07%和提升性能22.27%


<details>
  <summary>Details</summary>
Motivation: 当前基于LLM的工具使用代理在长轨迹任务中经常触发过多低质量工具调用，导致延迟增加和推理性能下降，需要有效管理工具使用行为

Method: 基于熵减与高质量工具调用的正相关关系，提出使用熵减作为监督信号，设计两种奖励策略：稀疏结果奖励（轨迹级指导）和密集过程奖励（细粒度监督）

Result: 实验表明两种奖励设计都能改进工具使用行为：稀疏奖励相比基线平均减少72.07%工具调用，密集奖励提升22.27%性能

Conclusion: 熵减是增强工具使用行为的关键机制，能使代理在现实应用中更具适应性，为LLM工具使用优化提供了有效方法

Abstract: Tool-using agents based on Large Language Models (LLMs) excel in tasks such as mathematical reasoning and multi-hop question answering. However, in long trajectories, agents often trigger excessive and low-quality tool calls, increasing latency and degrading inference performance, making managing tool-use behavior challenging. In this work, we conduct entropy-based pilot experiments and observe a strong positive correlation between entropy reduction and high-quality tool calls. Building on this finding, we propose using entropy reduction as a supervisory signal and design two reward strategies to address the differing needs of optimizing tool-use behavior. Sparse outcome rewards provide coarse, trajectory-level guidance to improve efficiency, while dense process rewards offer fine-grained supervision to enhance performance. Experiments across diverse domains show that both reward designs improve tool-use behavior: the former reduces tool calls by 72.07% compared to the average of baselines, while the latter improves performance by 22.27%. These results position entropy reduction as a key mechanism for enhancing tool-use behavior, enabling agents to be more adaptive in real-world applications.

</details>


### [563] [Understanding the Reversal Curse Mitigation in Masked Diffusion Models through Attention and Training Dynamics](https://arxiv.org/abs/2602.02133)
*Sangwoo Shin,BumJun Kim,Kyelim Lee,Moongyu Jeon,Albert No*

Main category: cs.AI

Relevance: 85.0

TL;DR: 该论文研究了自回归语言模型(ARMs)中的"逆转诅咒"现象，并发现基于掩码扩散的语言模型(MDMs)能部分缓解此问题。作者揭示了这种缓解源于Transformer编码器的架构特性及其与训练的交互作用，而非通常认为的任意顺序训练目标。


<details>
  <summary>Details</summary>
Motivation: 自回归语言模型存在"逆转诅咒"问题：学习了"A是B"后，往往无法正确回答"B是A"。虽然基于掩码扩散的语言模型对此问题有较好缓解，但根本原因尚不清楚。作者旨在揭示这种缓解背后的真正机制，挑战了常见的"任意顺序训练目标"解释。

Method: 作者采用理论分析和实验验证相结合的方法：1) 在单层Transformer编码器中分析权重共享如何使前向和反向注意力分数正相关；2) 证明相应梯度对齐，最小化前向损失同时减少反向损失；3) 在受控玩具任务和大规模扩散语言模型上进行实验验证。

Result: 研究发现：1) Transformer编码器的权重共享架构使前向和反向注意力分数正相关；2) 相应的梯度对齐，训练优化前向任务时自然改善反向任务性能；3) 实验验证了这些机制，解释了为什么MDMs能部分克服ARMs中持续存在的失败模式。

Conclusion: 基于掩码扩散的语言模型对"逆转诅咒"的缓解主要源于Transformer编码器的架构特性及其与训练的交互作用，而非训练目标本身。这一发现为理解不同语言模型架构的对称性学习能力提供了新视角。

Abstract: Autoregressive language models (ARMs) suffer from the reversal curse: after learning that "$A$ is $B$", they often fail on the reverse query "$B$ is $A$". Masked diffusion-based language models (MDMs) exhibit this failure in a much weaker form, but the underlying reason has remained unclear. A common explanation attributes this mitigation to the any-order training objective. However, observing "[MASK] is $B$" during training does not necessarily teach the model to handle the reverse prompt "$B$ is [MASK]". We show that the mitigation arises from architectural structure and its interaction with training. In a one-layer Transformer encoder, weight sharing couples the two directions by making forward and reverse attention scores positively correlated. In the same setting, we further show that the corresponding gradients are aligned, so minimizing the forward loss also reduces the reverse loss. Experiments on both controlled toy tasks and large-scale diffusion language models support these mechanisms, explaining why MDMs partially overcome a failure mode that persists in strong ARMs.

</details>


### [564] [Mitigating Safety Tax via Distribution-Grounded Refinement in Large Reasoning Models](https://arxiv.org/abs/2602.02136)
*Yingsha Xie,Tiansheng Huang,Enneng Yang,Rui Min,Wenjie Lu,Xiaochun Cao,Naiqiang Tan,Li Shen*

Main category: cs.AI

Relevance: 85.0

TL;DR: 提出DGR方法，通过将外部安全推理数据集转换为目标LLM的内部分布，减少安全对齐对大型推理模型通用推理能力的负面影响，在保持安全性能的同时显著提升推理准确率。


<details>
  <summary>Details</summary>
Motivation: 现有安全对齐数据集通常从外部LRM或人工标注中提取安全推理轨迹和答案，但这些数据与需要对齐的目标LRM存在分布差异，作者推测这种分布差异是导致目标LRM推理能力显著下降的主要原因。

Method: 提出DGR方法，将现有的分布外安全推理数据集转换和精炼，使其与目标LLM的内部分布对齐，从而减少安全对齐对推理能力的负面影响。

Result: DGR有效减轻了安全税，在保持安全性能的同时，相比Vanilla SFT平均推理准确率提升30.2%（DirectRefusal）和21.2%（R1-ACT）。研究发现推理能力下降程度与分布偏移程度相关，且仅需10个样本就能激活有效的拒绝行为。

Conclusion: 分布一致性对于保持LRM的安全对齐和推理能力至关重要，安全对齐可能主要作为激活潜在知识的机制，少量样本即可激活有效的拒绝行为。

Abstract: Safety alignment incurs safety tax that perturbs a large reasoning model's (LRM) general reasoning ability. Existing datasets used for safety alignment for an LRM are usually constructed by distilling safety reasoning traces and answers from an external LRM or human labeler. However, such reasoning traces and answers exhibit a distributional gap with the target LRM that needs alignment, and we conjecture such distributional gap is the culprit leading to significant degradation of reasoning ability of the target LRM. Driven by this hypothesis, we propose a safety alignment dataset construction method, dubbed DGR. DGR transforms and refines an existing out-of-distributional safety reasoning dataset to be aligned with the target's LLM inner distribution. Experimental results demonstrate that i) DGR effectively mitigates the safety tax while maintaining safety performance across all baselines, i.e., achieving \textbf{+30.2\%} on DirectRefusal and \textbf{+21.2\%} on R1-ACT improvement in average reasoning accuracy compared to Vanilla SFT; ii) the degree of reasoning degradation correlates with the extent of distribution shift, suggesting that bridging this gap is central to preserving capabilities. Furthermore, we find that safety alignment in LRMs may primarily function as a mechanism to activate latent knowledge, as a mere \textbf{10} samples are sufficient for activating effective refusal behaviors. These findings not only emphasize the importance of distributional consistency but also provide insights into the activation mechanism of safety in reasoning models.

</details>


### [565] [Reasoning in a Combinatorial and Constrained World: Benchmarking LLMs on Natural-Language Combinatorial Optimization](https://arxiv.org/abs/2602.02188)
*Xia Jiang,Jing Chen,Cong Zhang,Jie Gao,Chengpeng Hu,Chenhao Zhang,Yaoxin Wu,Yingqian Zhang*

Main category: cs.AI

Relevance: 85.0

TL;DR: NLCO是一个自然语言组合优化基准，用于评估LLM在端到端组合优化推理上的能力，涵盖43个问题，使用四层分类法组织，评估可行性、最优性和推理效率。


<details>
  <summary>Details</summary>
Motivation: 虽然LLM在数学和逻辑推理方面表现出色，但它们在组合优化（在高维解空间中搜索满足硬约束的解）方面的能力尚未得到充分探索。需要建立一个基准来评估LLM在端到端组合优化推理上的能力。

Method: 引入NLCO基准，包含43个组合优化问题，采用四层分类法组织（变量类型、约束族、全局模式、目标类别）。提供求解器标注的解，从可行性、解最优性和推理效率三个维度全面评估LLM。

Result: 高性能LLM在小规模实例上表现出良好的可行性和解质量，但随着实例规模增大，两者均下降，即使使用更多token进行推理也是如此。集合类任务相对容易，而图结构问题和瓶颈目标导致更多失败。

Conclusion: LLM在组合优化推理方面存在局限性，特别是在大规模实例和复杂结构问题上。需要进一步研究提升LLM在组合优化任务上的能力。

Abstract: While large language models (LLMs) have shown strong performance in math and logic reasoning, their ability to handle combinatorial optimization (CO) -- searching high-dimensional solution spaces under hard constraints -- remains underexplored. To bridge the gap, we introduce NLCO, a \textbf{N}atural \textbf{L}anguage \textbf{C}ombinatorial \textbf{O}ptimization benchmark that evaluates LLMs on end-to-end CO reasoning: given a language-described decision-making scenario, the model must output a discrete solution without writing code or calling external solvers. NLCO covers 43 CO problems and is organized using a four-layer taxonomy of variable types, constraint families, global patterns, and objective classes, enabling fine-grained evaluation. We provide solver-annotated solutions and comprehensively evaluate LLMs by feasibility, solution optimality, and reasoning efficiency. Experiments across a wide range of modern LLMs show that high-performing models achieve strong feasibility and solution quality on small instances, but both degrade as instance size grows, even if more tokens are used for reasoning. We also observe systematic effects across the taxonomy: set-based tasks are relatively easy, whereas graph-structured problems and bottleneck objectives lead to more frequent failures.

</details>


### [566] [TIDE: Trajectory-based Diagnostic Evaluation of Test-Time Improvement in LLM Agents](https://arxiv.org/abs/2602.02196)
*Hang Yan,Xinyu Che,Fangzhi Xu,Qiushi Sun,Zichen Ding,Kanzhi Cheng,Jian Zhang,Tao Qin,Jun Liu,Qika Lin*

Main category: cs.AI

Relevance: 85.0

TL;DR: 提出了TIDE框架，用于诊断LLM智能体在测试时改进(TTI)过程中的性能瓶颈，通过三个维度评估任务完成效率、递归循环行为和记忆负担


<details>
  <summary>Details</summary>
Motivation: 现有研究对自主LLM智能体通过与环境迭代交互实现性能改进(TTI)的机制理解不足，现有评估指标无法捕捉任务优化效率、错误行为后的适应能力以及工作记忆的具体效用

Method: 提出TIDE框架，将TTI分解为三个相互关联的维度：1)任务完成的整体时间动态；2)识别性能是否主要受递归循环行为限制；3)识别是否受累积记忆负担限制。该框架是智能体无关和环境无关的

Result: 通过跨不同智能体和环境的广泛实验，TIDE揭示提升智能体性能不仅需要扩展内部推理，还需要显式优化智能体与环境之间的交互动态

Conclusion: TIDE框架为理解LLM智能体测试时改进机制提供了系统诊断工具，指出了优化智能体-环境交互动态的重要性

Abstract: Recent advances in autonomous LLM agents demonstrate their ability to improve performance through iterative interaction with the environment. We define this paradigm as Test-Time Improvement (TTI). However, the mechanisms under how and why TTI succeed or fail remain poorly understood, and existing evaluation metrics fail to capture their task optimization efficiency, behavior adaptation after erroneous actions, and the specific utility of working memory for task completion. To address these gaps, we propose Test-time Improvement Diagnostic Evaluation (TIDE), an agent-agnostic and environment-agnostic framework that decomposes TTI into three comprehensive and interconnected dimensions. The framework measures (1) the overall temporal dynamics of task completion and (2) identifies whether performance is primarily constrained by recursive looping behaviors or (3) by burdensome accumulated memory. Through extensive experiments across diverse agents and environments, TIDE highlights that improving agent performance requires more than scaling internal reasoning, calling for explicitly optimizing the interaction dynamics between the agent and the environment.

</details>


### [567] [More Than a Quick Glance: Overcoming the Greedy Bias in KV-Cache Compression](https://arxiv.org/abs/2602.02199)
*Aryan Sood,Tanvi Sharma,Vansh Agrawal*

Main category: cs.AI

Relevance: 85.0

TL;DR: LASER-KV是一种KV缓存压缩框架，采用分层累积选择和精确LSH召回机制，在严格累积预算策略下测试KV压缩极限，相比现有方法在长上下文任务中保持稳定性能。


<details>
  <summary>Details</summary>
Motivation: 尽管LLM理论上支持长上下文窗口，但KV缓存的线性内存增长限制了实际部署。现有压缩方法通过剪枝机制在语义召回和内存效率之间权衡，但性能下降明显。作者旨在测试严格累积预算策略下的KV压缩极限。

Method: 提出LASER-KV框架，采用分层累积选择与精确LSH召回。不同于固定摘要大小方法，实施基于保护除数(n)的分块累积策略，将压缩效果与滑动窗口伪影分离。使用精确局部敏感哈希进行召回。

Result: 在Babilong基准测试中，先前压缩方法在各种长上下文任务上性能下降15-30%，而LASER-KV保持稳定性能，在128k上下文长度下准确率优势高达10%。

Conclusion: 研究挑战了注意力分数单独作为token效用代理的普遍假设，表明需要更精细的压缩策略。LASER-KV在严格预算下实现更好的KV缓存压缩，为长上下文LLM部署提供实用解决方案。

Abstract: While Large Language Models (LLMs) can theoretically support extensive context windows, their actual deployment is constrained by the linear growth of Key-Value (KV) cache memory. Prevailing compression strategies mitigate this through various pruning mechanisms, yet trade-off semantic recall for memory efficiency. In this work, we present LASER-KV (Layer Accumulated Selection with Exact-LSH Recall), a framework designed to test the limits of KV compression under a strict accumulative budgeting policy. We deviate from the standard fixed summary size approach by implementing a block-wise accumulation strategy governed by a protection divisor (n). This allows us to isolate the effects of compression from sliding window artifacts. Our experiments on the Babilong benchmark reveal performance degradation in previous compression methods by 15-30% on various long context tasks. LASER-KV maintains stable performance, achieving superior accuracies by a margin of upto 10% at 128k. These findings challenge the prevailing assumption that attention scores alone are a sufficient proxy for token utility.

</details>


### [568] [Position: Explaining Behavioral Shifts in Large Language Models Requires a Comparative Approach](https://arxiv.org/abs/2602.02304)
*Martino Ciaperoni,Marzio Di Vece,Luca Pappalardo,Fosca Giannotti,Francesco Giannini*

Main category: cs.AI

Relevance: 85.0

TL;DR: 论文提出了Comparative XAI（Δ-XAI）框架，用于解释基础模型在不同干预（如缩放、微调、RLHF等）后的行为变化，强调比较性解释而非孤立分析单个模型。


<details>
  <summary>Details</summary>
Motivation: 大规模基础模型在缩放、微调、强化学习或上下文学习后会出现行为变化，但现有可解释AI方法主要针对单个模型检查点，无法解释不同检查点之间的内部变化，需要专门针对行为变化的比较性解释框架。

Method: 提出Comparative XAI（Δ-XAI）框架，包含一套设计解释方法时应考虑的需求标准，提供可能的解释流程，并通过具体Δ-XAI实验展示方法应用。

Result: 建立了系统性比较性解释框架，能够解释干预引起的模型行为变化，填补了现有XAI方法在跨检查点解释方面的空白。

Conclusion: 行为变化应该通过比较性方式解释，关注参考模型与干预模型之间的变化，Δ-XAI框架为此提供了系统化的方法论基础。

Abstract: Large-scale foundation models exhibit behavioral shifts: intervention-induced behavioral changes that appear after scaling, fine-tuning, reinforcement learning or in-context learning. While investigating these phenomena have recently received attention, explaining their appearance is still overlooked. Classic explainable AI (XAI) methods can surface failures at a single checkpoint of a model, but they are structurally ill-suited to justify what changed internally across different checkpoints and which explanatory claims are warranted about that change. We take the position that behavioral shifts should be explained comparatively: the core target should be the intervention-induced shift between a reference model and an intervened model, rather than any single model in isolation. To this aim we formulate a Comparative XAI ($Δ$-XAI) framework with a set of desiderata to be taken into account when designing proper explaining methods. To highlight how $Δ$-XAI methods work, we introduce a set of possible pipelines, relate them to the desiderata, and provide a concrete $Δ$-XAI experiment.

</details>


### [569] [Interpreting and Controlling LLM Reasoning through Integrated Policy Gradient](https://arxiv.org/abs/2602.02313)
*Changming Li,Kaixing Zhang,Haoyun Xu,Yingdong Shi,Zheng Zhang,Kaitao Song,Kan Ren*

Main category: cs.AI

Relevance: 85.0

TL;DR: 提出IPG框架，通过传播基于结果的信号（如推理准确率）来定位LLM中复杂推理行为的内部机制，实现更精确的定位和可靠的行为调控。


<details>
  <summary>Details</summary>
Motivation: 现有可解释性方法难以精确定位复杂推理机制或捕捉从模型内部到推理输出的序列影响，需要新方法来识别对推理行为有序列贡献的组件。

Method: 提出集成策略梯度（IPG）框架，通过将基于结果的信号（如推理后准确率）沿模型推理轨迹反向传播，将推理行为归因于模型内部组件。

Result: 实证评估显示IPG实现了更精确的定位，并能在不同推理模型中可靠地调控推理行为（如推理能力、推理强度）。

Conclusion: IPG框架基于结果导向和序列影响感知原则，能有效识别对推理行为有序列贡献的组件，为LLM推理机制的可解释性提供了新方法。

Abstract: Large language models (LLMs) demonstrate strong reasoning abilities in solving complex real-world problems. Yet, the internal mechanisms driving these complex reasoning behaviors remain opaque. Existing interpretability approaches targeting reasoning either identify components (e.g., neurons) correlated with special textual patterns, or rely on human-annotated contrastive pairs to derive control vectors. Consequently, current methods struggle to precisely localize complex reasoning mechanisms or capture sequential influence from model internal workings to the reasoning outputs. In this paper, built on outcome-oriented and sequential-influence-aware principles, we focus on identifying components that have sequential contribution to reasoning behavior where outcomes are cumulated by long-range effects. We propose Integrated Policy Gradient (IPG), a novel framework that attributes reasoning behaviors to model's inner components by propagating compound outcome-based signals such as post reasoning accuracy backward through model inference trajectories. Empirical evaluations demonstrate that our approach achieves more precise localization and enables reliable modulation of reasoning behaviors (e.g., reasoning capability, reasoning strength) across diverse reasoning models.

</details>


### [570] [Context Learning for Multi-Agent Discussion](https://arxiv.org/abs/2602.02350)
*Xingyuan Hua,Sheng Yue,Xinyi Li,Yizhe Zhao,Jinrui Zhang,Ju Ren*

Main category: cs.AI

Relevance: 85.0

TL;DR: M2CL通过训练上下文生成器解决多智能体讨论中的不一致性问题，动态生成每轮讨论的上下文指令，显著提升多LLM协作性能


<details>
  <summary>Details</summary>
Motivation: 当前多智能体讨论方法容易遭受讨论不一致性问题，由于各个LLM实例的上下文不对齐，导致无法达成一致的解决方案

Method: 提出多LLM上下文学习方法M2CL，为每个智能体训练上下文生成器，通过自动信息组织和精炼动态生成每轮讨论的上下文指令，采用自适应的机制控制上下文一致性和输出差异

Result: 在学术推理、具身任务和移动控制等挑战性任务上，M2CL性能显著超越现有方法20%-50%，同时具有良好的可迁移性和计算效率

Conclusion: M2CL通过上下文学习有效解决多智能体讨论中的不一致性问题，使LLM能够避免过早收敛于多数噪声并逐步达成正确共识

Abstract: Multi-Agent Discussion (MAD) has garnered increasing attention very recently, where multiple LLM instances collaboratively solve problems via structured discussion. However, we find that current MAD methods easily suffer from discussion inconsistency, LLMs fail to reach a coherent solution, due to the misalignment between their individual contexts.In this paper, we introduce a multi-LLM context learning method (M2CL) that learns a context generator for each agent, capable of dynamically generating context instructions per discussion round via automatic information organization and refinement. Specifically, inspired by our theoretical insights on the context instruction, M2CL train the generators to control context coherence and output discrepancies via a carefully crafted self-adaptive mechanism.It enables LLMs to avoid premature convergence on majority noise and progressively reach the correct consensus. We evaluate M2CL on challenging tasks, including academic reasoning, embodied tasks, and mobile control. The results show that the performance of M2CL significantly surpasses existing methods by 20%--50%, while enjoying favorable transferability and computational efficiency.

</details>


### [571] [Live-Evo: Online Evolution of Agentic Memory from Continuous Feedback](https://arxiv.org/abs/2602.02369)
*Yaolun Zhang,Yiran Wu,Yijiong Yu,Qingyun Wu,Huazheng Wang*

Main category: cs.AI

Relevance: 85.0

TL;DR: Live-Evo：在线自进化记忆系统，通过经验库和元指导库分离"发生了什么"和"如何使用"，在持续数据流中动态更新记忆权重，实现真正的在线学习。


<details>
  <summary>Details</summary>
Motivation: 现有LLM代理的记忆系统多为静态训练/测试分割设计，仅通过折叠静态基准来近似在线学习，在真实分布偏移和持续反馈下表现脆弱。需要真正的在线自进化记忆系统。

Method: 1. 分离经验存储与使用：Experience Bank存储原始经验，Meta-Guideline Bank存储如何使用经验的指导原则
2. 在线记忆管理：维护经验权重，根据反馈动态更新——有帮助的经验强化权重，误导/过时经验降低权重并逐渐遗忘
3. 任务自适应指导：从检索到的经验中为每个任务编译个性化指导

Result: 在10周的Prophet Arena基准测试中：Brier分数提升20.8%，市场回报增加12.9%；在深度研究基准测试中也表现出持续增益

Conclusion: Live-Evo实现了真正的在线自进化记忆系统，能够有效应对分布偏移和持续反馈，在金融预测和深度研究任务中均表现出色。

Abstract: Large language model (LLM) agents are increasingly equipped with memory, which are stored experience and reusable guidance that can improve task-solving performance. Recent \emph{self-evolving} systems update memory based on interaction outcomes, but most existing evolution pipelines are developed for static train/test splits and only approximate online learning by folding static benchmarks, making them brittle under true distribution shift and continuous feedback. We introduce \textsc{Live-Evo}, an online self-evolving memory system that learns from a stream of incoming data over time. \textsc{Live-Evo} decouples \emph{what happened} from \emph{how to use it} via an Experience Bank and a Meta-Guideline Bank, compiling task-adaptive guidelines from retrieved experiences for each task. To manage memory online, \textsc{Live-Evo} maintains experience weights and updates them from feedback: experiences that consistently help are reinforced and retrieved more often, while misleading or stale experiences are down-weighted and gradually forgotten, analogous to reinforcement and decay in human memory. On the live \textit{Prophet Arena} benchmark over a 10-week horizon, \textsc{Live-Evo} improves Brier score by 20.8\% and increases market returns by 12.9\%, while also transferring to deep-research benchmarks with consistent gains over strong baselines. Our code is available at https://github.com/ag2ai/Live-Evo.

</details>


### [572] [Trust by Design: Skill Profiles for Transparent, Cost-Aware LLM Routing](https://arxiv.org/abs/2602.02386)
*Mika Okamoto,Ansel Kaplan Erol,Glenn Matlin*

Main category: cs.AI

Relevance: 85.0

TL;DR: BELLA框架通过技能分析为LLM任务推荐最优模型，在预算约束下最大化性能，提供透明解释


<details>
  <summary>Details</summary>
Motivation: 当前LLM基准测试报告聚合指标，掩盖了任务所需的具体能力，无法判断更便宜的模型是否足够。实践者需要在不浪费资金的情况下为任务选择合适模型。

Method: 三阶段框架：1) 分解LLM输出并通过批判性分析提取细粒度技能；2) 将技能聚类为结构化能力矩阵；3) 多目标优化选择模型，在预算约束下最大化性能

Result: BELLA提供自然语言推理的推荐，提供当前黑盒路由系统缺乏的透明度。框架应用于金融推理领域，展示了处理多样化技能需求和模型成本变化的能力。

Conclusion: 该框架使实践者能够在部署LLM时做出原则性和成本性能权衡的决策，填补了当前基准测试的空白。

Abstract: How should Large Language Model (LLM) practitioners select the right model for a task without wasting money? We introduce BELLA (Budget-Efficient LLM Selection via Automated skill-profiling), a framework that recommends optimal LLM selection for tasks through interpretable skill-based model selection. Standard benchmarks report aggregate metrics that obscure which specific capabilities a task requires and whether a cheaper model could suffice. BELLA addresses this gap through three stages: (1) decomposing LLM outputs and extract the granular skills required by using critic-based profiling, (2) clustering skills into structured capability matrices, and (3) multi-objective optimization to select the right models to maximize performance while respecting budget constraints. BELLA provides natural-language rationale for recommendations, providing transparency that current black-box routing systems lack. We describe the framework architecture, situate it within the landscape of LLM routing and evaluation, and discuss its application to financial reasoning as a representative domain exhibiting diverse skill requirements and cost-variation across models. Our framework enables practitioners to make principled and cost-performance trade-offs for deploying LLMs.

</details>


### [573] [Structure Enables Effective Self-Localization of Errors in LLMs](https://arxiv.org/abs/2602.02416)
*Ankur Samanta,Akshayaa Magesh,Ayush Jain,Kavosh Asadi,Youliang Yu,Daniel Jiang,Boris Vidolov,Kaveh Hassani,Paul Sajda,Jalaj Bhandari,Yonathan Efroni*

Main category: cs.AI

Relevance: 85.0

TL;DR: 提出Thought-ICS框架，通过离散化思维步骤实现语言模型的自我纠错，相比传统链式思维提升20-40%的纠错能力


<details>
  <summary>Details</summary>
Motivation: 探索语言模型能否显式定位推理错误，构建能够有效自我纠正的AI系统。当前语言模型的自我纠错能力仍然不足，需要更结构化的方法来定位和修正推理错误。

Method: 提出Thought-ICS（迭代纠正思维采样）框架：1）将推理结构化为离散、语义连贯的思维步骤；2）每次生成一个完整的离散思维；3）验证时定位第一个错误步骤；4）回溯到最后一个正确点重新生成替代推理。

Result: 1）在由oracle验证为错误的推理中，Thought-ICS实现20-40%的自我纠错提升；2）在完全自主无外部验证的设置中，优于当代自我纠错基线方法。

Conclusion: 通过离散化思维步骤的结构化方法，语言模型能够可靠地定位推理错误，Thought-ICS框架为构建有效的自我纠正AI系统提供了可行路径。

Abstract: Self-correction in language models remains elusive. In this work, we explore whether language models can explicitly localize errors in incorrect reasoning, as a path toward building AI systems that can effectively correct themselves. We introduce a prompting method that structures reasoning as discrete, semantically coherent thought steps, and show that models are able to reliably localize errors within this structure, while failing to do so in conventional, unstructured chain-of-thought reasoning. Motivated by how the human brain monitors errors at discrete decision points and resamples alternatives, we introduce Iterative Correction Sampling of Thoughts (Thought-ICS), a self-correction framework. Thought-ICS iteratively prompts the model to generate reasoning one discrete and complete thought at a time--where each thought represents a deliberate decision by the model--creating natural boundaries for precise error localization. Upon verification, the model localizes the first erroneous step, and the system backtracks to generate alternative reasoning from the last correct point. When asked to correct reasoning verified as incorrect by an oracle, Thought-ICS achieves 20-40% self-correction lift. In a completely autonomous setting without external verification, it outperforms contemporary self-correction baselines.

</details>


### [574] [Drift-Bench: Diagnosing Cooperative Breakdowns in LLM Agents under Input Faults via Multi-Turn Interaction](https://arxiv.org/abs/2602.02455)
*Han Bao,Zheyuan Zhang,Pengcheng Jing,Zhengqing Yuan,Kaiwen Shi,Yanfang Ye*

Main category: cs.AI

Relevance: 85.0

TL;DR: Drift-Bench：首个诊断性基准测试，用于评估自主智能体在输入故障下的多轮澄清能力，重点关注执行风险下的语用学表现


<details>
  <summary>Details</summary>
Motivation: 现有基准测试通常假设指令明确或仅限于文本单轮澄清，无法衡量在接地执行风险下的多轮消歧能力。随着大语言模型向自主智能体过渡，用户输入经常违反合作假设，产生文本评估无法捕捉的执行风险。

Method: 基于经典沟通理论建立统一的合作故障分类法，使用角色驱动的用户模拟器和Rise评估协议，在状态导向和服务导向的执行环境中进行多轮澄清评估。

Result: 实验显示在这些故障下性能显著下降，澄清效果因用户角色和故障类型而异。该方法揭示了现有智能体在应对输入故障时的系统性弱点。

Conclusion: Drift-Bench连接了澄清研究和智能体安全评估，能够系统诊断可能导致不安全执行的故障，为自主智能体的安全性和鲁棒性评估提供了新工具。

Abstract: As Large Language Models transition to autonomous agents, user inputs frequently violate cooperative assumptions (e.g., implicit intent, missing parameters, false presuppositions, or ambiguous expressions), creating execution risks that text-only evaluations do not capture. Existing benchmarks typically assume well-specified instructions or restrict evaluation to text-only, single-turn clarification, and thus do not measure multi-turn disambiguation under grounded execution risk. We introduce \textbf{Drift-Bench}, the first diagnostic benchmark that evaluates agentic pragmatics under input faults through multi-turn clarification across state-oriented and service-oriented execution environments. Grounded in classical theories of communication, \textbf{Drift-Bench} provides a unified taxonomy of cooperative breakdowns and employs a persona-driven user simulator with the \textbf{Rise} evaluation protocol. Experiments show substantial performance drops under these faults, with clarification effectiveness varying across user personas and fault types. \MethodName bridges clarification research and agent safety evaluation, enabling systematic diagnosis of failures that can lead to unsafe executions.

</details>


### [575] [MentisOculi: Revealing the Limits of Reasoning with Mental Imagery](https://arxiv.org/abs/2602.02465)
*Jana Zeller,Thaddäus Wiedemer,Fanfei Li,Thomas Klein,Prasanna Mayilvahanan,Matthias Bethge,Felix Wichmann,Ryan Cotterell,Wieland Brendel*

Main category: cs.AI

Relevance: 85.0

TL;DR: 该论文提出了MentisOculi评估套件，用于测试统一多模态模型(UMMs)的视觉推理能力，发现当前模型无法有效利用视觉化作为推理辅助工具，即使能生成正确视觉内容也无法改善推理性能。


<details>
  <summary>Details</summary>
Motivation: 随着前沿模型从仅能处理视觉信息的MLLMs向能够原生交错生成的UMMs转变，研究者希望探索使用中间视觉化作为推理辅助工具的可能性，类似于人类的心理意象。需要评估模型形成、维护和操作视觉表征的能力。

Method: 开发了MentisOculi——一个程序化、分层化的多步推理问题套件，专门设计用于挑战前沿模型。评估了从潜在标记到显式生成图像等多种视觉策略，并特别分析了UMMs在利用视觉化进行推理时的表现。

Result: 研究发现视觉策略通常无法提高模型性能。UMMs虽然具备解决任务的文本推理能力，有时也能生成正确的视觉内容，但存在复合生成错误，甚至无法有效利用真实视觉化信息。视觉思维目前尚未对模型推理产生益处。

Conclusion: 尽管视觉思维具有内在吸引力，但当前模型尚未能有效利用视觉化进行推理。MentisOculi为分析和弥合这一差距提供了必要的基础，适用于多种模型家族。

Abstract: Frontier models are transitioning from multimodal large language models (MLLMs) that merely ingest visual information to unified multimodal models (UMMs) capable of native interleaved generation. This shift has sparked interest in using intermediate visualizations as a reasoning aid, akin to human mental imagery. Central to this idea is the ability to form, maintain, and manipulate visual representations in a goal-oriented manner. To evaluate and probe this capability, we develop MentisOculi, a procedural, stratified suite of multi-step reasoning problems amenable to visual solution, tuned to challenge frontier models. Evaluating visual strategies ranging from latent tokens to explicit generated imagery, we find they generally fail to improve performance. Analysis of UMMs specifically exposes a critical limitation: While they possess the textual reasoning capacity to solve a task and can sometimes generate correct visuals, they suffer from compounding generation errors and fail to leverage even ground-truth visualizations. Our findings suggest that despite their inherent appeal, visual thoughts do not yet benefit model reasoning. MentisOculi establishes the necessary foundation to analyze and close this gap across diverse model families.

</details>


### [576] [Breaking the Reversal Curse in Autoregressive Language Models via Identity Bridge](https://arxiv.org/abs/2602.02470)
*Xutao Ma,Yixiao Huang,Hanlin Zhu,Somayeh Sojoudi*

Main category: cs.AI

Relevance: 85.0

TL;DR: 通过引入"身份桥"正则化数据（如"A→A"）来缓解LLM的"逆转诅咒"问题，理论上证明单层Transformer即可解决，实验显示成功率从接近0提升至40%


<details>
  <summary>Details</summary>
Motivation: 挑战现有观点：传统认为自回归LLM在简单逻辑推理（如逆转诅咒）上的失败是其固有局限，表明模型只是记忆事实而非学习高层规则。本文认为这种局限可以通过数据层面的简单调整来缓解。

Method: 提出"身份桥"正则化数据配方：在训练数据中添加"A→A"形式的样本（如"Alice的名字是Alice"）。理论上分析梯度下降的隐式偏差，证明单层Transformer即可打破逆转诅咒。实验上对1B预训练语言模型进行微调。

Result: 使用身份桥数据配方的模型在逆转任务上达到40%成功率，而仅使用前向知识数据的模型成功率接近0。为逆转诅咒提供了新的理论解释。

Conclusion: 逆转诅咒并非自回归LLM的固有局限，可以通过简单的数据正则化缓解。这为鼓励LLM从数据中学习高层规则提供了低成本、有原则的路径。

Abstract: Autoregressive large language models (LLMs) have achieved remarkable success in many complex tasks, yet they can still fail in very simple logical reasoning such as the "reversal curse" -- when trained on forward knowledge data of the form "$A \rightarrow B$" (e.g., Alice's husband is Bob), the model is unable to deduce the reversal knowledge "$B \leftarrow A$" (e.g., Bob's wife is Alice) during test. Extensive prior research suggests that this failure is an inherent, fundamental limit of autoregressive causal LLMs, indicating that these models tend to memorize factual-level knowledge rather than capture higher-level rules. In this paper, we challenge this view by showing that this seemingly fundamental limit can be mitigated by slightly tweaking the training data with a simple regularization data recipe called the Identity Bridge of the form "$A \to A$" (e.g., The name of Alice is Alice). Theoretically, we prove that under this recipe, even a one-layer transformer can break the reversal curse by analyzing the implicit bias of gradient descent. Empirically, we show that a 1B pretrained language model finetuned with the proposed data recipe achieves a 40% success rate on reversal tasks, in stark contrast to a near-zero success rate when trained solely on forward-knowledge data. Our work provides a novel theoretical foundation for the reversal curse and offers a principled, low-cost path to encouraging LLMs to learn higher-level rules from data.

</details>


### [577] [AgentRx: Diagnosing AI Agent Failures from Execution Trajectories](https://arxiv.org/abs/2602.02475)
*Shraddha Barke,Arnav Goyal,Alind Khare,Avaljot Singh,Suman Nath,Chetan Bansal*

Main category: cs.AI

Relevance: 85.0

TL;DR: 论文提出了AGENTRX框架，用于自动诊断AI代理失败轨迹中的关键失败步骤，并发布了包含115个失败轨迹的基准数据集


<details>
  <summary>Details</summary>
Motivation: AI代理经常失败，但由于执行具有概率性、长时程、多智能体以及受噪声工具输出影响等特点，失败原因难以定位。现有方法缺乏系统化的失败诊断框架。

Method: 1. 手动标注失败代理运行，创建包含115个失败轨迹的基准数据集，涵盖结构化API工作流、事件管理和开放式Web/文件任务
2. 提出AGENTRX框架：合成约束条件，逐步评估，生成可审计的约束违反验证日志
3. 使用基于LLM的裁判根据验证日志定位关键失败步骤和类别

Result: AGENTRX在三个领域中改进了步骤定位和失败归因，相比现有基线方法表现更好

Conclusion: AGENTRX提供了一个有效的自动化框架来诊断AI代理失败，有助于提高代理系统的可靠性和可调试性

Abstract: AI agents often fail in ways that are difficult to localize because executions are probabilistic, long-horizon, multi-agent, and mediated by noisy tool outputs. We address this gap by manually annotating failed agent runs and release a novel benchmark of 115 failed trajectories spanning structured API workflows, incident management, and open-ended web/file tasks. Each trajectory is annotated with a critical failure step and a category from a grounded-theory derived, cross-domain failure taxonomy. To mitigate the human cost of failure attribution, we present AGENTRX, an automated domain-agnostic diagnostic framework that pinpoints the critical failure step in a failed agent trajectory. It synthesizes constraints, evaluates them step-by-step, and produces an auditable validation log of constraint violations with associated evidence; an LLM-based judge uses this log to localize the critical step and category. Our framework improves step localization and failure attribution over existing baselines across three domains.

</details>


### [578] [Efficient Multilingual Search Relevance Modeling in E-Commerce via LLM Mixture-of-Experts](https://arxiv.org/abs/2602.00003)
*Ye Liu,Xu Chen,Wuji Chen,Mang Li*

Main category: cs.IR

Relevance: 85.0

TL;DR: 提出基于LLM的MoE框架用于电商搜索相关性建模，通过专家路由和嵌入融合提升多语言多地区性能，优化推理管道实现35% GPU时间节省和9%吞吐提升。


<details>
  <summary>Details</summary>
Motivation: 电商平台中搜索相关性直接影响用户体验和商家收入。在多国部署中，语言、文化和产品目录的多样性导致显著分布偏移，现有单一模型方法受限于数据多样性、覆盖缺口和高推理成本。

Method: 提出可扩展的LLM-based MoE框架，动态路由查询到专门专家，通过拼接融合嵌入表示。比较基于规则、伪标签和端到端策略，最终采用端到端硬路由+拼接。开发工程优化的离线批处理管道，隐藏内存延迟，提高GPU利用率。

Result: 在东南亚六个市场数据集上，MoE相比相同活跃参数的密集基线提升AUC 0.72个百分点。优化管道达到27.6 QPS，吞吐提升9%，GPU小时消耗减少35%。

Conclusion: 提出的MoE框架在电商搜索系统中实现了优越的多语言相关性和效率，为实际部署提供了强大的成本效益。

Abstract: In e-commerce platforms, search relevance directly influences both user experience and merchant revenue. In multi-country deployments, diverse linguistic, cultural, and product catalog contexts introduce significant distribution shifts, posing substantial challenges to relevance modeling. Existing approaches typically enhance the reasoning or multilingual abilities of a single monolithic model, yet they remain limited by data diversity, coverage gaps, and high inference costs in heterogeneous environments. Our empirical analysis reveals that different LLM base models exhibit complementary strengths across languages and regions, motivating an expert-based architecture. We propose a scalable LLM-based Mixture-of-Experts (MoE) framework that dynamically routes queries to specialized experts and fuses their embeddings through concatenation. Among rule-based, pseudo-label-based, and fully end-to-end strategies, end-to-end hard routing with concatenation offers the best balance of effectiveness and efficiency. To mitigate inference overhead, we further develop an engineering-optimized offline batch pipeline with resource-efficient scheduling, which hides memory latency, improves GPU utilization, and reduces GPU-hour consumption by up to 35% compared with synchronous execution. On datasets spanning six Southeast Asian markets, our MoE improves AUC by 0.72 percentage points over a dense baseline with the same active parameters. Meanwhile, the optimized pipeline achieves 27.6 queries per second (QPS), a 9% throughput improvement. These results demonstrate superior multilingual relevance and efficiency, delivering strong cost-effectiveness for real-world e-commerce search systems.

</details>


### [579] [LSSF: Safety Alignment for Large Language Models through Low-Rank Safety Subspace Fusion](https://arxiv.org/abs/2602.00038)
*Guanghao Zhou,Panjia Qiu,Cen Chen,Hongyu Li,Mingyuan Chu,Xin Zhang,Jun Zhou*

Main category: cs.CY

Relevance: 85.0

TL;DR: LSSF：一种基于低秩安全子空间融合的后处理安全对齐方法，通过提取LLMs中的低秩安全向量主成分，在微调后有效恢复模型的安全性，同时最小化对下游任务性能的影响。


<details>
  <summary>Details</summary>
Motivation: 现有LLMs的安全机制存在脆弱性，即使在无害数据集上微调也可能破坏安全性。当前安全对齐方法主要依赖微调过程，导致复杂度和计算资源需求增加。需要一种更高效、稳定的安全对齐方法。

Method: 提出LSSF框架：1) 利用LLMs中安全信息的低秩特性，构建低秩投影矩阵提取安全向量主成分；2) 该投影矩阵代表LLMs的低秩安全子空间，在微调过程中保持稳定且与模型通用能力隔离；3) 提出安全奇异值熵度量，量化不同层中安全信息的编码密度，动态计算各安全向量的安全关键秩；4) 通过线性算术将主成分与微调后的LLMs结合，恢复安全对齐。

Result: 大量实验表明，该后处理对齐方法能有效恢复微调模型的安全对齐，同时对下游任务性能影响最小。

Conclusion: LSSF提供了一种高效、稳定的安全对齐解决方案，能够在不显著影响模型性能的情况下恢复LLMs的安全性，解决了现有安全对齐方法的局限性。

Abstract: The safety mechanisms of large language models (LLMs) exhibit notable fragility, as even fine-tuning on datasets without harmful content may still undermine their safety capabilities. Meanwhile, existing safety alignment methods predominantly rely on the fine-tuning process, which inadvertently leads to the increased complexity and computational resources required. To address these issues, we introduce LSSF, a novel safety re-alignment framework with \underline{L}ow-Rank \underline{S}afety \underline{S}ubspace \underline{F}usion. Our proposed method exploits the low-rank characteristics of safety information in LLMs by constructing a low-rank projection matrix to extract the principal components of safety vectors. Notably, this projection matrix represents the low-rank safety subspace of the LLMs, which we have observed to remain stable during fine-tuning process and is isolated from the model's general capabilities. These principal components are used to effectively restore safety alignment when combined with fine-tuned LLMs through linear arithmetic. Additionally, to account for the varying encoding densities of safety information across different layers of LLMs, we propose a novel metric called safety singular value entropy. This metric quantifies the encoding density and allows for the dynamic computation of the safety-critical rank for each safety vector. Extensive experiments demonstrate that our proposed post-hoc alignment method can effectively restore the safety alignment of fine-tuned models with minimal impact on their performance in downstream tasks.

</details>


### [580] [SPARC-RAG: Adaptive Sequential-Parallel Scaling with Context Management for Retrieval-Augmented Generation](https://arxiv.org/abs/2602.00083)
*Yuxin Yang,Gangda Deng,Ömer Faruk Akgül,Nima Chitsazan,Yash Govilkar,Akasha Tigalappanavara,Shi-Xiong Zhang,Sambit Sahu,Viktor Prasanna*

Main category: cs.IR

Relevance: 85.0

TL;DR: SPARC-RAG是一个多智能体框架，通过协调顺序和并行推理时间扩展来解决RAG在多跳问答中的上下文污染和扩展效率问题，在更低推理成本下实现平均+6.2 F1提升。


<details>
  <summary>Details</summary>
Motivation: 检索增强生成(RAG)在需要长推理的多跳问答任务上仍面临挑战。现有方法通过顺序深度（迭代优化）和并行宽度（覆盖扩展）进行推理时扩展，但简单扩展会导致上下文污染和扩展效率低下，计算增加但收益递减甚至为负。

Method: 提出SPARC-RAG多智能体框架：1) 在统一上下文管理机制下协调顺序和并行推理扩展；2) 使用专门智能体维护共享全局上下文并显式控制扩展过程；3) 为每个分支生成针对性互补子查询以实现多样化并行探索；4) 基于答案正确性和证据基础显式调节退出决策；5) 引入轻量级微调方法，通过过程级可验证偏好优化扩展行为。

Result: 在单跳和多跳问答基准测试中，SPARC-RAG始终优于之前的RAG基线，在更低的推理成本下实现平均+6.2 F1提升。

Conclusion: SPARC-RAG通过多智能体协调和显式上下文管理，有效解决了RAG在多跳问答中的扩展效率问题，在推理效率和效果之间取得了良好平衡。

Abstract: Retrieval-Augmented Generation (RAG) grounds large language model outputs in external evidence, but remains challenged on multi-hop question answering that requires long reasoning. Recent works scale RAG at inference time along two complementary dimensions: sequential depth for iterative refinement and parallel width for coverage expansion. However, naive scaling causes context contamination and scaling inefficiency, leading to diminishing or negative returns despite increased computation. To address these limitations, we propose SPARC-RAG, a multi-agent framework that coordinates sequential and parallel inference-time scaling under a unified context management mechanism. SPARC-RAG employs specialized agents that maintain a shared global context and provide explicit control over the scaling process. It generates targeted, complementary sub-queries for each branch to enable diverse parallel exploration, and explicitly regulates exiting decisions based on answer correctness and evidence grounding. To optimize scaling behavior, we further introduce a lightweight fine-tuning method with process-level verifiable preferences, which improves the efficiency of sequential scaling and effectiveness of parallel scaling. Across single- and multi-hop QA benchmarks, SPARC-RAG consistently outperforms previous RAG baselines, yielding an average +6.2 F1 improvement under lower inference cost.

</details>


### [581] [When LLMs Imagine People: A Human-Centered Persona Brainstorm Audit for Bias and Fairness in Creative Applications](https://arxiv.org/abs/2602.00044)
*Hongliu Cao,Eoin Thomas,Rodrigo Acuna Agost*

Main category: cs.CY

Relevance: 85.0

TL;DR: 提出Persona Brainstorm Audit (PBA)方法，通过开放式角色生成来检测LLM偏见，相比现有基于固定身份类别的方法更可扩展和透明，能发现跨多个社会维度的偏见并支持纵向追踪。


<details>
  <summary>Details</summary>
Motivation: LLM的偏见输出会强化刻板印象并在实际应用中延续不平等，因此公平性审计至关重要。现有方法依赖固定的身份类别和静态基准，难以全面检测偏见。

Method: 提出PBA方法：通过开放式角色生成来审计偏见，不依赖预定义类别，支持多社会维度分析、纵向追踪，并能降低数据泄露风险。

Result: 应用于12个SOTA LLM，比较了不同模型、维度和版本的偏见严重程度，发现了独特的偏见模式和谱系特异性变异，追踪了偏见在连续代际中的衰减、持续或重现。

Conclusion: PBA在不同样本量、角色扮演提示和去偏见提示下保持稳定，为LLM公平性审计提供了可靠方法，能更全面地检测和追踪偏见。

Abstract: Biased outputs from Large Language Models (LLMs) can reinforce stereotypes and perpetuate inequities in real-world applications, making fairness auditing essential. We introduce the Persona Brainstorm Audit (PBA), a scalable and transparent auditing method for detecting bias through open-ended persona generation. Unlike existing methods that rely on fixed identity categories and static benchmarks, PBA uncovers biases across multiple social dimensions while supporting longitudinal tracking and mitigating data leakage risks. Applying PBA to 12 state-of-the-art LLMs, we compare bias severity across models, dimensions, and versions, uncover distinct patterns and lineage-specific variability, and trace how biases attenuate, persist, or resurface across successive generations. Robustness analyses show PBA remains stable under varying sample sizes, role-playing prompts, and debiasing prompts, establishing its reliability for fairness auditing in LLMs.

</details>


### [582] [Simple Role Assignment is Extraordinarily Effective for Safety Alignment](https://arxiv.org/abs/2602.00061)
*Zhou Ziheng,Jiakun Ding,Zhaowei Zhang,Ruosen Gao,Yingnian Wu,Demetri Terzopoulos,Yipeng Kang,Fangwei Zhong,Junqi Wang*

Main category: cs.CY

Relevance: 85.0

TL;DR: 提出基于角色调节的AI对齐方法，利用社会角色（如母亲、法官）隐含的价值和认知模式，替代原则式对齐，在多个模型上显著提升安全性和性能。


<details>
  <summary>Details</summary>
Motivation: 传统基于原则的对齐方法缺乏上下文敏感性和完整性。受心理理论启发，社会角色天然编码了价值观和应用这些价值观所需的认知模式，为AI对齐提供了更紧凑、更自然的替代方案。

Method: 提出无需训练的流程：1) 角色条件生成器，2) 基于角色的迭代批评器进行精炼。利用社会角色隐含的价值观和认知模式，通过角色分配实现对齐。

Result: 在五个模型家族中，该方法一致优于基于原则的方法、思维链和其他基线。在WildJailbreak基准上，将DeepSeek-V3的不安全输出从81.4%降至3.6%。在代理安全任务中也表现一致。

Conclusion: 角色分配是AI对齐和LLM-as-a-Judge构建的强大、可解释范式，比原则式对齐更紧凑、更自然，能显著提升模型安全性和性能。

Abstract: Principle-based alignment often lacks context sensitivity and completeness. Grounded in Theory of Mind, we propose role conditioning as a compact alternative: social roles (e.g., mother, judge) implicitly encode both values and the cognitive schemas required to apply them. We introduce a training-free pipeline featuring a role-conditioned generator and iterative role-based critics for refinement. Across five model families, our approach consistently outperforms principle-based, Chain-of-Thought (CoT) and other baselines across benchmarks. Notably, it reduces unsafe outputs on the WildJailbreak benchmark from 81.4\% to 3.6\% with DeepSeek-V3. Not only for common safety benchmarks, it consistently applies for agentic safety tasks. These results establish role assignment as a powerful, interpretable paradigm for AI alignment and LLM-as-a-Judge construction.

</details>


### [583] [GradingAttack: Attacking Large Language Models Towards Short Answer Grading Ability](https://arxiv.org/abs/2602.00979)
*Xueyi Li,Zhuoneng Zhou,Zitao Liu,Yongdong Wu,Weiqi Luo*

Main category: cs.CR

Relevance: 85.0

TL;DR: 提出GradingAttack框架，针对LLM自动评分系统进行细粒度对抗攻击，包含token级和prompt级策略，并设计新的伪装评估指标


<details>
  <summary>Details</summary>
Motivation: LLM在自动评分中表现出色，但其对抗脆弱性引发公平性和可靠性担忧。需要系统评估LLM评分模型的脆弱性，确保教育评估的公正性

Method: 1) token级攻击：操纵输入文本的token；2) prompt级攻击：修改评分提示；3) 提出新的伪装评估指标，平衡攻击成功率和伪装度

Result: 在多个数据集上，两种攻击策略都能有效误导评分模型：prompt级攻击成功率更高，token级攻击伪装能力更强

Conclusion: LLM评分系统存在严重脆弱性，需要开发鲁棒防御机制以确保自动评分的公平性和可靠性

Abstract: Large language models (LLMs) have demonstrated remarkable potential for automatic short answer grading (ASAG), significantly boosting student assessment efficiency and scalability in educational scenarios. However, their vulnerability to adversarial manipulation raises critical concerns about automatic grading fairness and reliability. In this paper, we introduce GradingAttack, a fine-grained adversarial attack framework that systematically evaluates the vulnerability of LLM based ASAG models. Specifically, we align general-purpose attack methods with the specific objectives of ASAG by designing token-level and prompt-level strategies that manipulate grading outcomes while maintaining high camouflage. Furthermore, to quantify attack camouflage, we propose a novel evaluation metric that balances attack success and camouflage. Experiments on multiple datasets demonstrate that both attack strategies effectively mislead grading models, with prompt-level attacks achieving higher success rates and token-level attacks exhibiting superior camouflage capability. Our findings underscore the need for robust defenses to ensure fairness and reliability in ASAG. Our code and datasets are available at https://anonymous.4open.science/r/GradingAttack.

</details>


### [584] [ReasoningBomb: A Stealthy Denial-of-Service Attack by Inducing Pathologically Long Reasoning in Large Reasoning Models](https://arxiv.org/abs/2602.00154)
*Xiaogeng Liu,Xinyan Wang,Yechao Zhang,Sanjay Kariyappa,Chong Xiang,Muhao Chen,G. Edward Suh,Chaowei Xiao*

Main category: cs.CR

Relevance: 85.0

TL;DR: 论文提出了一种针对大型推理模型的新型攻击方法ReasoningBomb，利用强化学习生成短自然提示，诱导模型产生极长的推理过程，造成推理时间拒绝服务攻击。


<details>
  <summary>Details</summary>
Motivation: 大型推理模型通过显式多步推理扩展了LLM能力，但这种能力引入了新的推理时间拒绝服务攻击风险。攻击者可以利用推理的高计算成本，通过精心设计的提示诱导模型进行异常长的推理，从而消耗大量计算资源。

Method: 首先形式化LRM的推理成本并定义PI-DoS攻击。提出攻击应满足三个属性：高放大率、隐蔽性和可优化性。基于此框架，开发了ReasoningBomb——基于强化学习的PI-DoS框架，使用恒定时间代理奖励训练攻击者模型生成短自然提示，诱导受害者模型进行病态长推理。

Result: 在7个开源模型和3个商业LRM上测试，ReasoningBomb平均诱导18,759个完成token和19,263个推理token，比次优基线分别提高35%和38%，比良性查询多产生6-7倍token，平均输入输出放大率达到286.7倍。检测绕过率：输入检测99.8%，输出检测98.7%，双阶段联合检测98.4%。

Conclusion: 论文揭示了大型推理模型面临的新型安全威胁，提出的ReasoningBomb攻击方法有效且隐蔽，对LLM/LRM的安全性和鲁棒性研究具有重要意义，需要开发新的防御机制来应对此类推理时间攻击。

Abstract: Large reasoning models (LRMs) extend large language models with explicit multi-step reasoning traces, but this capability introduces a new class of prompt-induced inference-time denial-of-service (PI-DoS) attacks that exploit the high computational cost of reasoning. We first formalize inference cost for LRMs and define PI-DoS, then prove that any practical PI-DoS attack should satisfy three properties: (1) a high amplification ratio, where each query induces a disproportionately long reasoning trace relative to its own length; (ii) stealthiness, in which prompts and responses remain on the natural language manifold and evade distribution shift detectors; and (iii) optimizability, in which the attack supports efficient optimization without being slowed by its own success. Under this framework, we present ReasoningBomb, a reinforcement-learning-based PI-DoS framework that is guided by a constant-time surrogate reward and trains a large reasoning-model attacker to generate short natural prompts that drive victim LRMs into pathologically long and often effectively non-terminating reasoning. Across seven open-source models (including LLMs and LRMs) and three commercial LRMs, ReasoningBomb induces 18,759 completion tokens on average and 19,263 reasoning tokens on average across reasoning models. It outperforms the the runner-up baseline by 35% in completion tokens and 38% in reasoning tokens, while inducing 6-7x more tokens than benign queries and achieving 286.7x input-to-output amplification ratio averaged across all samples. Additionally, our method achieves 99.8% bypass rate on input-based detection, 98.7% on output-based detection, and 98.4% against strict dual-stage joint detection.

</details>


### [585] [OmniCode: A Benchmark for Evaluating Software Engineering Agents](https://arxiv.org/abs/2602.02262)
*Atharv Sonwane,Eng-Shen Tu,Wei-Chung Lu,Claas Beger,Carter Larsen,Debjit Dhar,Rachel Chen,Ronit Pattanayak,Tuan Anh Dang,Guohao Chen,Gloria Geng,Kevin Ellis,Saikat Dutta*

Main category: cs.SE

Relevance: 85.0

TL;DR: OmniCode是一个新颖的软件工程基准测试，包含1794个任务，涵盖Python、Java和C++三种编程语言，以及bug修复、测试生成、代码审查修复和风格修复四个关键类别，旨在全面评估LLM驱动的编码代理在真实软件开发中的能力。


<details>
  <summary>Details</summary>
Motivation: 现有编码基准测试（如HumanEval和SWE-Bench）主要关注竞争性编程和补丁生成等狭窄任务，而真实软件开发需要处理更广泛的任务类型。需要更具挑战性的基准测试来推动更好的编码代理研究。

Method: 提出OmniCode基准测试，包含1794个任务，涵盖三种编程语言和四个任务类别。任务经过人工验证以消除定义不清的问题，并通过合成生成或近期整理避免数据泄露问题，建立了从有限真实数据中合成生成多样化软件任务的新框架。

Result: 使用SWE-Agent等流行代理框架评估发现，它们在Python bug修复上表现良好，但在测试生成以及C++和Java任务上表现不足。例如，SWE-Agent在Java测试生成任务上使用DeepSeek-V3.1最高仅达到20.9%。

Conclusion: OmniCode作为一个稳健的基准测试，旨在推动能够跨软件开发不同方面表现良好的代理的发展，填补了现有编码基准测试的空白。

Abstract: LLM-powered coding agents are redefining how real-world software is developed. To drive the research towards better coding agents, we require challenging benchmarks that can rigorously evaluate the ability of such agents to perform various software engineering tasks. However, popular coding benchmarks such as HumanEval and SWE-Bench focus on narrowly scoped tasks such as competition programming and patch generation. In reality, software engineers have to handle a broader set of tasks for real-world software development. To address this gap, we propose OmniCode, a novel software engineering benchmark that contains a broader and more diverse set of task categories beyond code or patch generation. Overall, OmniCode contains 1794 tasks spanning three programming languages (Python, Java, and C++) and four key categories: bug fixing, test generation, code review fixing, and style fixing. In contrast to prior software engineering benchmarks, the tasks in OmniCode are (1) manually validated to eliminate ill-defined problems, and (2) synthetically crafted or recently curated to avoid data leakage issues, presenting a new framework for synthetically generating diverse software tasks from limited real-world data. We evaluate OmniCode with popular agent frameworks such as SWE-Agent and show that while they may perform well on bug fixing for Python, they fall short on tasks such as Test Generation and in languages such as C++ and Java. For instance, SWE-Agent achieves a maximum of 20.9% with DeepSeek-V3.1 on Java Test Generation tasks. OmniCode aims to serve as a robust benchmark and spur the development of agents that can perform well across different aspects of software development. Code and data are available at https://github.com/seal-research/OmniCode.

</details>


### [586] [RACA: Representation-Aware Coverage Criteria for LLM Safety Testing](https://arxiv.org/abs/2602.02280)
*Zeming Wei,Zhixin Zhang,Chengcan Wu,Yihao Zhang,Xiaokun Luan,Meng Sun*

Main category: cs.SE

Relevance: 85.0

TL;DR: RACA提出了一套专门针对LLM安全测试的覆盖标准，利用表征工程聚焦安全关键概念，通过六个子标准评估测试套件的充分性，有效识别高质量越狱提示。


<details>
  <summary>Details</summary>
Motivation: 当前LLM安全测试主要依赖静态数据集，缺乏系统性的测试质量评估标准。传统神经网络覆盖标准无法直接应用于LLM，因为存在可扩展性问题和目标差异。需要专门针对LLM安全测试的覆盖标准框架。

Method: RACA采用三阶段框架：1) 使用专家标注的越狱提示校准集识别安全关键表征；2) 基于这些表征计算测试套件的概念激活分数；3) 通过六个子标准计算覆盖结果，评估个体和组合安全概念。

Result: 实验验证了RACA的有效性、适用性和泛化能力。RACA能成功识别高质量越狱提示，优于传统神经元级标准。在实际应用中，如测试集优先级排序和攻击提示采样，表现出良好效果。

Conclusion: RACA为LLM安全评估提供了新框架，通过聚焦安全关键概念，解决了传统覆盖标准在LLM上的局限性，为AI测试领域贡献了有价值的技术。

Abstract: Recent advancements in LLMs have led to significant breakthroughs in various AI applications. However, their sophisticated capabilities also introduce severe safety concerns, particularly the generation of harmful content through jailbreak attacks. Current safety testing for LLMs often relies on static datasets and lacks systematic criteria to evaluate the quality and adequacy of these tests. While coverage criteria have been effective for smaller neural networks, they are not directly applicable to LLMs due to scalability issues and differing objectives. To address these challenges, this paper introduces RACA, a novel set of coverage criteria specifically designed for LLM safety testing. RACA leverages representation engineering to focus on safety-critical concepts within LLMs, thereby reducing dimensionality and filtering out irrelevant information. The framework operates in three stages: first, it identifies safety-critical representations using a small, expert-curated calibration set of jailbreak prompts. Second, it calculates conceptual activation scores for a given test suite based on these representations. Finally, it computes coverage results using six sub-criteria that assess both individual and compositional safety concepts. We conduct comprehensive experiments to validate RACA's effectiveness, applicability, and generalization, where the results demonstrate that RACA successfully identifies high-quality jailbreak prompts and is superior to traditional neuron-level criteria. We also showcase its practical application in real-world scenarios, such as test set prioritization and attack prompt sampling. Furthermore, our findings confirm RACA's generalization to various scenarios and its robustness across various configurations. Overall, RACA provides a new framework for evaluating the safety of LLMs, contributing a valuable technique to the field of testing for AI.

</details>


### [587] [Training LLMs with Fault Tolerant HSDP on 100,000 GPUs](https://arxiv.org/abs/2602.00277)
*Omkar Salpekar,Rohan Varma,Kenny Yu,Vladimir Ivanov,Yang Wang,Ahmed Sharif,Min Si,Shawn Xu,Feng Tian,Shengbao Zheng,Tristan Rice,Ankush Garg,Shangfu Peng,Shreyas Siravara,Wenyin Fu,Rodrigo de Castro,Adithya Gangidi,Andrey Obraztsov,Sharan Narang,Sergey Edunov,Maxim Naumov,Chunqiang Tang,Mathew Oldham*

Main category: cs.DC

Relevance: 85.0

TL;DR: FT-HSDP是一种新型的大规模训练范式，通过数据并行副本作为容错单元，在GPU故障时仅重启单个副本而非整个系统，将故障恢复时间从10分钟降至3分钟，训练效率从44%提升至80%。


<details>
  <summary>Details</summary>
Motivation: 大规模同步训练在O(100K) GPU规模下效率低下，主要原因是频繁的硬件故障和长恢复时间。传统同步训练要求所有GPU同时健康，任何故障都会导致整个训练停滞。

Method: 提出Fault Tolerant Hybrid-Shared Data Parallelism (FT-HSDP)：1) 以数据并行副本为容错单元，故障时仅重启受影响副本；2) 设计Fault Tolerant All Reduce (FTAR)协议，CPU处理控制逻辑，GPU处理数据传输；3) 非阻塞追赶协议，允许恢复副本最小化停滞地加入训练。

Result: 在O(100K) GPU规模下，FT-HSDP将故障恢复停滞时间从10分钟减少到3分钟，有效训练时间从44%提高到80%。异步恢复不会对模型精度产生有意义的退化。

Conclusion: FT-HSDP通过创新的容错机制显著提升大规模训练系统的效率和可靠性，为超大规模LLM训练提供了实用的解决方案。

Abstract: Large-scale training systems typically use synchronous training, requiring all GPUs to be healthy simultaneously. In our experience training on O(100K) GPUs, synchronous training results in a low efficiency due to frequent failures and long recovery time.
  To address this problem, we propose a novel training paradigm, Fault Tolerant Hybrid-Shared Data Parallelism (FT-HSDP). FT-HSDP uses data parallel replicas as units of fault tolerance. When failures occur, only a single data-parallel replica containing the failed GPU or server is taken offline and restarted, while the other replicas continue training. To realize this idea at scale, FT-HSDP incorporates several techniques: 1) We introduce a Fault Tolerant All Reduce (FTAR) protocol for gradient exchange across data parallel replicas. FTAR relies on the CPU to drive the complex control logic for tasks like adding or removing participants dynamically, and relies on GPU to perform data transfer for best performance. 2) We introduce a non-blocking catch-up protocol, allowing a recovering replica to join training with minimal stall.
  Compared with fully synchronous training at O(100K) GPUs, FT-HSDP can reduce the stall time due to failure recovery from 10 minutes to 3 minutes, increasing effective training time from 44\% to 80\%. We further demonstrate that FT-HSDP's asynchronous recovery does not bring any meaning degradation to the accuracy of the result model.

</details>


### [588] [Semantics-Preserving Evasion of LLM Vulnerability Detectors](https://arxiv.org/abs/2602.00305)
*Luze Sun,Alina Oprea,Eric Wong*

Main category: cs.CR

Relevance: 85.0

TL;DR: LLM漏洞检测器在语义保留的代码变换下存在系统性脆弱性，即使最先进的检测器在干净输入上表现良好，但在行为等效的编辑下预测会翻转，揭示了低成本语义保留规避攻击的有效性。


<details>
  <summary>Details</summary>
Motivation: LLM漏洞检测器越来越多地部署在安全关键的代码审查中，但其在行为保留编辑下的抗规避能力仍不清楚。需要评估检测器在语义保留威胁模型下的完整性，理解其实际部署中的安全风险。

Method: 在统一的C/C++基准测试集(N=5000)上实例化多种行为保留的代码变换，引入跨不同攻击方法/载体的联合鲁棒性度量。使用在单个代理模型上优化的通用对抗字符串，并测试其向黑盒API的迁移性，同时探索梯度访问对规避成功率的放大效应。

Result: 观察到语义不变对抗变换的系统性失败：即使最先进的漏洞检测器在干净输入上表现良好，但在行为等效编辑下预测会翻转。通用对抗字符串在迁移到黑盒API时仍保持有效，梯度访问可进一步放大规避成功率。

Conclusion: 即使高性能的漏洞检测器也容易受到低成本、语义保留的规避攻击。基于载体的度量为评估LLM代码检测器提供了实用的诊断工具，揭示了当前LLM漏洞检测器在实际部署中的安全脆弱性。

Abstract: LLM-based vulnerability detectors are increasingly deployed in security-critical code review, yet their resilience to evasion under behavior-preserving edits remains poorly understood. We evaluate detection-time integrity under a semantics-preserving threat model by instantiating diverse behavior-preserving code transformations on a unified C/C++ benchmark (N=5000), and introduce a metric of joint robustness across different attack methods/carriers. Across models, we observe a systemic failure of semantic invariant adversarial transformations: even state-of-the-art vulnerability detectors perform well on clean inputs while predictions flip under behavior-equivalent edits. Universal adversarial strings optimized on a single surrogate model remain effective when transferred to black-box APIs, and gradient access can further amplify evasion success. These results show that even high-performing detectors are vulnerable to low-cost, semantics-preserving evasion. Our carrier-based metrics provide practical diagnostics for evaluating LLM-based code detectors.

</details>


### [589] [Toward a Machine Bertin: Why Visualization Needs Design Principles for Machine Cognition](https://arxiv.org/abs/2602.01527)
*Brian Keith-Norambuena*

Main category: cs.HC

Relevance: 85.0

TL;DR: 该论文主张可视化领域需要研究面向机器的视觉设计作为一个独立的研究问题，因为人类为中心的视觉设计知识不能直接迁移到机器视觉，机器与人类在视觉处理上存在本质差异。


<details>
  <summary>Details</summary>
Motivation: 当前的可视化设计知识基于60年的人类视觉心理物理学研究，但视觉语言模型(VLMs)在自动化分析流程中处理图表图像时表现出与人类完全不同的视觉处理模式。人类为中心的设计知识不能直接应用于机器，而当前解决方案主要绕过视觉处理，这回避了一个更根本的问题：什么样的视觉表示才能真正服务于机器认知？

Method: 通过综合VLM基准测试、视觉推理研究和可视化素养研究的证据，分析人类与机器在视觉处理上的本质差异，批判当前流行的绕过视觉的方法，并提出人类导向与机器导向可视化的概念区分。

Result: 研究发现人类与机器的感知差异是定性的而非仅仅是定量的，机器表现出不同的编码性能模式，通过基于补丁的标记化而非整体感知处理图像，在某些人类容易的设计模式上失败，而在某些人类困难的设计模式上成功。

Conclusion: 可视化领域需要将机器导向的视觉设计作为独立研究问题，开发该领域目前缺乏的经验基础，即建立一个"机器Bertin"来补充现有的人类中心知识，为不同受众提供根本不同的设计基础。

Abstract: Visualization's design knowledge-effectiveness rankings, encoding guidelines, color models, preattentive processing rules -- derives from six decades of psychophysical studies of human vision. Yet vision-language models (VLMs) increasingly consume chart images in automated analysis pipelines, and a growing body of benchmark evidence indicates that this human-centered knowledge base does not straightforwardly transfer to machine audiences. Machines exhibit different encoding performance patterns, process images through patch-based tokenization rather than holistic perception, and fail on design patterns that pose no difficulty for humans-while occasionally succeeding where humans struggle. Current approaches address this gap primarily by bypassing vision entirely, converting charts to data tables or structured text. We argue that this response forecloses a more fundamental question: what visual representations would actually serve machine cognition well? This paper makes the case that the visualization field needs to investigate machine-oriented visual design as a distinct research problem. We synthesize evidence from VLM benchmarks, visual reasoning research, and visualization literacy studies to show that the human-machine perceptual divergence is qualitative, not merely quantitative, and critically examine the prevailing bypassing approach. We propose a conceptual distinction between human-oriented and machine-oriented visualization-not as an engineering architecture but as a recognition that different audiences may require fundamentally different design foundations-and outline a research agenda for developing the empirical foundations the field currently lacks: the beginnings of a "machine Bertin" to complement the human-centered knowledge the field already possesses.

</details>


### [590] [RecGOAT: Graph Optimal Adaptive Transport for LLM-Enhanced Multimodal Recommendation with Dual Semantic Alignment](https://arxiv.org/abs/2602.00682)
*Yuecheng Li,Hengwei Ju,Zeyu Song,Wei Yang,Chi Lu,Peng Jiang,Kun Gai*

Main category: cs.IR

Relevance: 85.0

TL;DR: RecGOAT提出了一种新颖的双重语义对齐框架，用于LLM增强的多模态推荐系统，通过图注意力网络和双重粒度对齐（实例级和分布级）来弥合大模型表示与推荐系统ID特征之间的表示差异。


<details>
  <summary>Details</summary>
Motivation: 当前多模态推荐系统虽然利用大模型进行语义理解，但忽视了LLM表示（面向通用语义任务）与推荐模型（依赖稀疏用户/物品ID特征）之间的根本表示差异，导致不兼容的多模态表示和次优推荐性能。

Method: 1. 使用图注意力网络建模物品-物品、用户-物品和用户-用户关系，利用用户/物品的LLM表示和交互历史丰富协作语义；2. 设计双重粒度渐进式多模态-ID对齐框架，通过跨模态对比学习（CMCL）实现实例级对齐，通过最优自适应传输（OAT）实现分布级对齐。

Result: 在三个公共基准测试上的广泛实验表明，RecGOAT实现了最先进的性能，实证验证了理论洞察。在大规模在线广告平台上的部署证实了模型在工业推荐场景中的有效性和可扩展性。

Conclusion: RecGOAT通过理论保证的对齐能力，成功弥合了LLM表示与推荐系统ID特征之间的表示差异，为LLM增强的多模态推荐提供了有效的解决方案。

Abstract: Multimodal recommendation systems typically integrates user behavior with multimodal data from items, thereby capturing more accurate user preferences. Concurrently, with the rise of large models (LMs), multimodal recommendation is increasingly leveraging their strengths in semantic understanding and contextual reasoning. However, LM representations are inherently optimized for general semantic tasks, while recommendation models rely heavily on sparse user/item unique identity (ID) features. Existing works overlook the fundamental representational divergence between large models and recommendation systems, resulting in incompatible multimodal representations and suboptimal recommendation performance. To bridge this gap, we propose RecGOAT, a novel yet simple dual semantic alignment framework for LLM-enhanced multimodal recommendation, which offers theoretically guaranteed alignment capability. RecGOAT first employs graph attention networks to enrich collaborative semantics by modeling item-item, user-item, and user-user relationships, leveraging user/item LM representations and interaction history. Furthermore, we design a dual-granularity progressive multimodality-ID alignment framework, which achieves instance-level and distribution-level semantic alignment via cross-modal contrastive learning (CMCL) and optimal adaptive transport (OAT), respectively. Theoretically, we demonstrate that the unified representations derived from our alignment framework exhibit superior semantic consistency and comprehensiveness. Extensive experiments on three public benchmarks show that our RecGOAT achieves state-of-the-art performance, empirically validating our theoretical insights. Additionally, the deployment on a large-scale online advertising platform confirms the model's effectiveness and scalability in industrial recommendation scenarios. Code available at https://github.com/6lyc/RecGOAT-LLM4Rec.

</details>


### [591] [HyperOffload: Graph-Driven Hierarchical Memory Management for Large Language Models on SuperNode Architectures](https://arxiv.org/abs/2602.00748)
*Fangxin Liu,Qinghua Zhang,Hanjing Shen,Zhibo Liang,Li Jiang,Haibing Guan,Chong Bao,Xuefeng Jin*

Main category: cs.DC

Relevance: 85.0

TL;DR: HyperOffload是一个针对超节点架构的编译器辅助内存管理框架，通过图驱动的内存管理将远程内存访问作为计算图中的显式操作，实现静态数据调度以隐藏延迟，在LLM推理中减少峰值内存使用达26%


<details>
  <summary>Details</summary>
Motivation: 随着LLM向长上下文推理和稀疏架构发展，内存需求远超单个设备HBM容量。新兴超节点架构通过高速互连提供TB级共享内存池，但现有软件栈无法有效利用这种硬件。当前运行时卸载和交换技术具有局部视图，导致反应式调度和暴露的通信延迟阻碍计算流水线。

Method: 提出SuperNode内存管理框架，采用编译器辅助方法，利用图驱动内存管理将远程内存访问作为计算图中的显式操作。在编译器中间表示中使用缓存操作符表示数据移动，实现张量生命周期和执行依赖的全局编译时分析。开发全局执行顺序优化算法，静态调度数据传输以在计算密集区域隐藏远程内存延迟。在MindSpore深度学习框架中实现，添加远程内存后端和专用编译器通道。

Result: 在代表性LLM工作负载评估中，SuperNode在推理时将峰值设备内存使用减少高达26%，同时保持端到端性能。证明将内存增强硬件集成到编译器优化框架对于扩展下一代AI工作负载至关重要。

Conclusion: HyperOffload展示了编译器辅助内存管理在超节点架构中的有效性，通过全局静态调度显著减少内存使用而不牺牲性能，为扩展下一代AI工作负载提供了关键解决方案。

Abstract: The rapid evolution of Large Language Models (LLMs) towards long-context reasoning and sparse architectures has pushed memory requirements far beyond the capacity of individual device HBM. While emerging supernode architectures offer terabyte-scale shared memory pools via high-bandwidth interconnects, existing software stacks fail to exploit this hardware effectively. Current runtime-based offloading and swapping techniques operate with a local view, leading to reactive scheduling and exposed communication latency that stall the computation pipeline.
  In this paper, we propose the SuperNode Memory Management Framework (\textbf{HyperOffload}). It employs a compiler-assisted approach that leverages graph-driven memory management to treat remote memory access as explicit operations in the computation graph, specifically designed for hierarchical SuperNode architectures. Unlike reactive runtime systems, SuperNode represents data movement using cache operators within the compiler's Intermediate Representation (IR). This design enables a global, compile-time analysis of tensor lifetimes and execution dependencies. Leveraging this visibility, we develop a global execution-order refinement algorithm that statically schedules data transfers to hide remote memory latency behind compute-intensive regions. We implement SuperNode within the production deep learning framework MindSpore, adding a remote memory backend and specialized compiler passes. Evaluation on representative LLM workloads shows that SuperNode reduces peak device memory usage by up to 26\% for inference while maintaining end-to-end performance. Our work demonstrates that integrating memory-augmented hardware into the compiler's optimization framework is essential for scaling next-generation AI workloads.

</details>


### [592] [Bypassing Prompt Injection Detectors through Evasive Injections](https://arxiv.org/abs/2602.00750)
*Md Jahedur Rahman,Ihsen Alouani*

Main category: cs.CR

Relevance: 85.0

TL;DR: 论文评估了基于激活delta的任务漂移检测器对抗对抗性后缀的鲁棒性，发现这些检测器高度脆弱，并提出了一种有效的防御技术


<details>
  <summary>Details</summary>
Motivation: 大型语言模型在交互和检索增强系统中使用越来越多，但容易受到任务漂移的影响。虽然已有研究使用线性探针检测这种漂移，但需要评估这些检测器对抗对抗性攻击的鲁棒性

Method: 生成通用的对抗性后缀，使中毒输入能够同时逃避多个探针的检测。在Phi-3 3.8B和Llama-3 8B上进行实验，评估攻击成功率。同时提出防御技术：生成多个后缀并随机附加到提示中，使用这些激活训练逻辑回归模型

Result: 单个后缀可以实现高攻击成功率：在所有探针都需要被欺骗时，Phi-3达到93.91%，Llama-3达到99.63%；在多数投票设置下成功率接近完美（>90%）。提出的防御技术对这些攻击非常有效

Conclusion: 基于激活delta的任务漂移检测器对对抗性后缀高度脆弱，需要更强的防御机制来应对自适应攻击。提出的随机后缀附加和逻辑回归训练方法是一种有效的防御策略

Abstract: Large language models (LLMs) are increasingly used in interactive and retrieval-augmented systems, but they remain vulnerable to task drift; deviations from a user's intended instruction due to injected secondary prompts. Recent work has shown that linear probes trained on activation deltas of LLMs' hidden layers can effectively detect such drift. In this paper, we evaluate the robustness of these detectors against adversarially optimised suffixes. We generate universal suffixes that cause poisoned inputs to evade detection across multiple probes simultaneously. Our experiments on Phi-3 3.8B and Llama-3 8B show that a single suffix can achieve high attack success rates; up to 93.91% and 99.63%, respectively, when all probes must be fooled, and nearly perfect success (>90%) under majority vote setting. These results demonstrate that activation delta-based task drift detectors are highly vulnerable to adversarial suffixes, highlighting the need for stronger defences against adaptive attacks. We also propose a defence technique where we generate multiple suffixes and randomly append one of them to the prompts while making forward passes of the LLM and train logistic regression models with these activations. We found this approach to be highly effective against such attacks.

</details>


### [593] [Evolving Interpretable Constitutions for Multi-Agent Simulation](https://arxiv.org/abs/2602.00755)
*Ujwal Kumar,Alice Saito,Hershraj Niranjani,Rayan Yessou,Phan Xuan Tan*

Main category: cs.MA

Relevance: 85.0

TL;DR: 提出了Constitutional Evolution框架，通过LLM驱动的遗传编程在多智能体系统中自动发现行为规范，在网格世界生存模拟中演化出比人工设计更好的合作规范。


<details>
  <summary>Details</summary>
Motivation: 传统宪法AI专注于单模型对齐使用固定原则，但多智能体系统通过涌现的社会动态创造了新的对齐挑战。需要研究如何在多智能体LLM系统中自动发现行为规范，解决个体与集体福利之间的张力。

Method: 使用网格世界模拟环境施加生存压力，量化社会稳定性得分S（结合生产力、生存和冲突指标）。采用LLM驱动的遗传编程和多岛演化技术，在没有明确合作指导的情况下演化宪法以最大化社会福利。

Result: 演化出的宪法C*达到S=0.556±0.008，比人工设计基线高123%，完全消除冲突，并发现最小化通信（0.9% vs 62.2%社交行为）比冗长的协调更有效。演化出的可解释规则表明合作规范可以被发现而非规定。

Conclusion: 合作规范可以通过演化自动发现，而不是由人类预先规定。最小化通信的策略在多智能体协调中可能比复杂的沟通更有效，这对多智能体对齐和LLM社会动态研究有重要意义。

Abstract: Constitutional AI has focused on single-model alignment using fixed principles. However, multi-agent systems create novel alignment challenges through emergent social dynamics. We present Constitutional Evolution, a framework for automatically discovering behavioral norms in multi-agent LLM systems. Using a grid-world simulation with survival pressure, we study the tension between individual and collective welfare, quantified via a Societal Stability Score S in [0,1] that combines productivity, survival, and conflict metrics. Adversarial constitutions lead to societal collapse (S= 0), while vague prosocial principles ("be helpful, harmless, honest") produce inconsistent coordination (S = 0.249). Even constitutions designed by Claude 4.5 Opus with explicit knowledge of the objective achieve only moderate performance (S= 0.332). Using LLM-driven genetic programming with multi-island evolution, we evolve constitutions maximizing social welfare without explicit guidance toward cooperation. The evolved constitution C* achieves S = 0.556 +/- 0.008 (123% higher than human-designed baselines, N = 10), eliminates conflict, and discovers that minimizing communication (0.9% vs 62.2% social actions) outperforms verbose coordination. Our interpretable rules demonstrate that cooperative norms can be discovered rather than prescribed.

</details>


### [594] [MCP-Atlas: A Large-Scale Benchmark for Tool-Use Competency with Real MCP Servers](https://arxiv.org/abs/2602.00933)
*Chaithanya Bandi,Ben Hertzberg,Geobio Boo,Tejas Polakam,Jeff Da,Sami Hassaan,Manasi Sharma,Andrew Park,Ernesto Hernandez,Dan Rambado,Ivan Salazar,Rafael Cruz,Chetan Rane,Ben Levin,Brad Kenstler,Bing Liu*

Main category: cs.SE

Relevance: 85.0

TL;DR: MCP-Atlas：首个大规模工具使用能力基准测试，包含36个真实MCP服务器和220个工具，设计了1000个多步骤工作流任务，采用基于事实声明的评分标准评估LLM的工具发现、参数化、错误恢复等能力。


<details>
  <summary>Details</summary>
Motivation: 现有评估方法无法捕捉真实场景复杂性，依赖受限工具集、简单工作流或主观的LLM-as-a-judge指标。需要更全面评估LLM工具使用能力的基准。

Method: 构建包含36个真实MCP服务器和220个工具的大规模基准，设计1000个多步骤任务，使用自然语言提示避免指定具体工具，要求代理跨多个服务器协调3-6个工具调用。采用基于事实声明的评分标准和内部诊断指标。

Result: 前沿模型在基准测试中通过率超过50%，主要失败原因包括工具使用不足和任务理解不够。发布了任务模式、容器化测试框架和500个任务的公开子集。

Conclusion: MCP-Atlas为工具增强型代理的开发提供了可复现的评估基准，揭示了当前模型在复杂工具使用工作流中的局限性，推动了更鲁棒的LLM工具集成研究。

Abstract: The Model Context Protocol (MCP) is rapidly becoming the standard interface for Large Language Models (LLMs) to discover and invoke external tools. However, existing evaluations often fail to capture the complexity of real-world scenarios, relying on restricted toolsets, simplistic workflows, or subjective LLM-as-a-judge metrics. We introduce MCP-Atlas, a large-scale benchmark for evaluating tool-use competency, comprising 36 real MCP servers and 220 tools. It includes 1,000 tasks designed to assess tool-use competency in realistic, multi-step workflows. Tasks use natural language prompts that avoid naming specific tools or servers, requiring agents to identify and orchestrate 3-6 tool calls across multiple servers. We score tasks using a claims-based rubric that awards partial credit based on the factual claims satisfied in the model's final answer, complemented by internal diagnostics on tool discovery, parameterization, syntax, error recovery, and efficiency. Evaluation results on frontier models reveal that top models achieve pass rates exceeding 50%, with primary failures arising from inadequate tool usage and task understanding. We release the task schema, containerized harness, and a 500-task public subset of the benchmark dataset to facilitate reproducible comparisons and advance the development of robust, tool-augmented agents.

</details>


### [595] [Multi-Agent Teams Hold Experts Back](https://arxiv.org/abs/2602.01011)
*Aneesh Pappu,Batu El,Hancheng Cao,Carmelo di Nolfo,Yanchao Sun,Meng Cao,James Zou*

Main category: cs.MA

Relevance: 85.0

TL;DR: 研究发现自组织的多智能体LLM团队无法有效利用专家智能体的专业知识，团队表现比最佳个体智能体差37.6%，主要瓶颈在于专家知识利用而非识别


<details>
  <summary>Details</summary>
Motivation: 随着多智能体LLM系统作为自主协作者部署，智能体自由交互而非执行预设工作流，需要研究在无约束协调下自组织团队是否能实现强协同效应

Method: 借鉴组织心理学，研究自组织LLM团队是否实现强协同效应（团队表现匹配或超越最佳个体成员），在人类启发和前沿ML基准上进行实验

Result: 与人类团队不同，LLM团队始终无法匹配专家智能体表现，即使明确告知谁是专家，性能损失达37.6%；主要瓶颈是专家知识利用而非识别；对话分析显示倾向于整合性妥协而非适当加权专业知识

Conclusion: 自组织多智能体团队在利用成员集体专业知识方面存在显著差距，表现出对齐与有效专业知识利用之间的权衡

Abstract: Multi-agent LLM systems are increasingly deployed as autonomous collaborators, where agents interact freely rather than execute fixed, pre-specified workflows. In such settings, effective coordination cannot be fully designed in advance and must instead emerge through interaction. However, most prior work enforces coordination through fixed roles, workflows, or aggregation rules, leaving open the question of how well self-organizing teams perform when coordination is unconstrained. Drawing on organizational psychology, we study whether self-organizing LLM teams achieve strong synergy, where team performance matches or exceeds the best individual member. Across human-inspired and frontier ML benchmarks, we find that -- unlike human teams -- LLM teams consistently fail to match their expert agent's performance, even when explicitly told who the expert is, incurring performance losses of up to 37.6%. Decomposing this failure, we show that expert leveraging, rather than identification, is the primary bottleneck. Conversational analysis reveals a tendency toward integrative compromise -- averaging expert and non-expert views rather than appropriately weighting expertise -- which increases with team size and correlates negatively with performance. Interestingly, this consensus-seeking behavior improves robustness to adversarial agents, suggesting a trade-off between alignment and effective expertise utilization. Our findings reveal a significant gap in the ability of self-organizing multi-agent teams to harness the collective expertise of their members.

</details>


### [596] [Calibrating Behavioral Parameters with Large Language Models](https://arxiv.org/abs/2602.01022)
*Brandon Yee,Krishna Sharma*

Main category: econ.GN

Relevance: 85.0

TL;DR: 该论文开发了一个框架，将大型语言模型（LLMs）作为行为参数的校准测量工具，用于量化损失厌恶、羊群效应等行为偏差，并通过校准使LLMs的行为参数达到或超过人类基准水平。


<details>
  <summary>Details</summary>
Motivation: 行为参数（如损失厌恶、羊群效应、外推等）对资产定价模型至关重要，但难以可靠测量。研究者希望利用LLMs作为校准的测量工具来量化这些行为参数。

Method: 1. 使用4个LLM模型和24,000个代理-场景对，测量基线LLM行为中的系统性理性偏差；2. 开发基于配置文件的校准方法，诱导行为参数发生稳定、理论一致的变化；3. 将校准后的参数嵌入基于代理的资产定价模型中，评估外部有效性。

Result: 1. 基线LLM行为显示衰减的损失厌恶、弱羊群效应和接近零的处置效应；2. 校准后，损失厌恶、羊群效应、外推和锚定等参数达到或超过基准幅度；3. 校准后的外推参数在资产定价模型中产生了与经验证据一致的短期动量与长期反转模式。

Conclusion: 该研究为八个经典行为偏差建立了测量范围、校准函数和明确边界，证明了LLMs作为行为参数校准测量工具的可行性，为行为金融研究提供了新方法。

Abstract: Behavioral parameters such as loss aversion, herding, and extrapolation are central to asset pricing models but remain difficult to measure reliably. We develop a framework that treats large language models (LLMs) as calibrated measurement instruments for behavioral parameters. Using four models and 24{,}000 agent--scenario pairs, we document systematic rationality bias in baseline LLM behavior, including attenuated loss aversion, weak herding, and near-zero disposition effects relative to human benchmarks. Profile-based calibration induces large, stable, and theoretically coherent shifts in several parameters, with calibrated loss aversion, herding, extrapolation, and anchoring reaching or exceeding benchmark magnitudes. To assess external validity, we embed calibrated parameters in an agent-based asset pricing model, where calibrated extrapolation generates short-horizon momentum and long-horizon reversal patterns consistent with empirical evidence. Our results establish measurement ranges, calibration functions, and explicit boundaries for eight canonical behavioral biases.

</details>


### [597] [Unifying Ranking and Generation in Query Auto-Completion via Retrieval-Augmented Generation and Multi-Objective Alignment](https://arxiv.org/abs/2602.01023)
*Kai Yuan,Anthony Zheng,Jia Hu,Divyanshu Sheth,Hemanth Velaga,Kylee Kim,Matteo Guarrera,Besim Avci,Xuetao Yin,Rajyashree Mukherjee,Sean Suchter*

Main category: cs.IR

Relevance: 85.0

TL;DR: 提出一个统一的QAC框架，将查询自动补全重新定义为端到端列表生成，结合检索增强生成和多目标直接偏好优化，在商业搜索平台上取得显著效果提升。


<details>
  <summary>Details</summary>
Motivation: 现有QAC方法面临根本性挑战：传统的检索-排序流水线长尾覆盖有限且需要大量特征工程，而最近的生成方法存在幻觉和安全风险。需要一种统一框架来解决这些限制。

Method: 1) 将QAC重新定义为端到端列表生成，采用多目标优化；2) 定义并部署基于规则、模型和LLM-as-judge的验证器，结合RAG、多目标DPO和迭代批判-修订流程生成高质量合成数据；3) 设计混合服务架构，在严格延迟约束下实现高效生产部署。

Result: 在大规模商业搜索平台上评估显示：离线指标在所有维度均有提升，人工评估获得+0.40到+0.69的偏好分数，受控在线实验实现5.44%的击键减少和3.46%的建议采纳率增加。

Conclusion: 这项工作代表了向由大语言模型、RAG和多目标对齐驱动的端到端生成的范式转变，建立了一个经过生产验证的框架，可为更广泛的搜索和推荐行业带来益处。

Abstract: Query Auto-Completion (QAC) suggests query completions as users type, helping them articulate intent and reach results more efficiently. Existing approaches face fundamental challenges: traditional retrieve-and-rank pipelines have limited long-tail coverage and require extensive feature engineering, while recent generative methods suffer from hallucination and safety risks. We present a unified framework that reformulates QAC as end-to-end list generation through Retrieval-Augmented Generation (RAG) and multi-objective Direct Preference Optimization (DPO). Our approach combines three key innovations: (1) reformulating QAC as end-to-end list generation with multi-objective optimization; (2) defining and deploying a suite of rule-based, model-based, and LLM-as-judge verifiers for QAC, and using them in a comprehensive methodology that combines RAG, multi-objective DPO, and iterative critique-revision for high-quality synthetic data; (3) a hybrid serving architecture enabling efficient production deployment under strict latency constraints. Evaluation on a large-scale commercial search platform demonstrates substantial improvements: offline metrics show gains across all dimensions, human evaluation yields +0.40 to +0.69 preference scores, and a controlled online experiment achieves 5.44\% reduction in keystrokes and 3.46\% increase in suggestion adoption, validating that unified generation with RAG and multi-objective alignment provides an effective solution for production QAC. This work represents a paradigm shift to end-to-end generation powered by large language models, RAG, and multi-objective alignment, establishing a production-validated framework that can benefit the broader search and recommendation industry.

</details>


### [598] [SPELL: Synthesis of Programmatic Edits using LLMs](https://arxiv.org/abs/2602.01107)
*Daniel Ramos,Catarina Gamboa,Inês Lynce,Vasco Manquinho,Ruben Martins,Claire Le Goues*

Main category: cs.SE

Relevance: 85.0

TL;DR: 本文提出了一种新的API迁移方法，利用LLMs提取迁移示例，然后通过Agent将其泛化为可重用的PolyglotPiranha转换脚本，避免了传统方法对现有迁移数据的依赖。


<details>
  <summary>Details</summary>
Motivation: 软件库迁移是常见但容易出错的任务。现有自动化迁移工具大多依赖从已完成类似迁移的真实项目中挖掘示例，但这些数据稀缺且难以收集。此外，这些工具未能充分利用现代代码转换基础设施。

Method: 1) 使用LLMs提取迁移示例；2) 通过Agent将这些示例泛化为PolyglotPiranha中的可重用转换脚本；3) 将LLMs中的潜在迁移知识蒸馏为结构化、可测试、可重复的迁移逻辑。

Result: 在Python库上的实验结果表明，该系统能够生成多样化的迁移示例，并合成能够泛化到真实代码库的转换脚本。

Conclusion: 该方法能够在不依赖现有语料库或手动工程工作的情况下，从LLMs中提取迁移知识并转化为可重复使用的转换逻辑，为自动化API迁移提供了新思路。

Abstract: Library migration is a common but error-prone task in software development. Developers may need to replace one library with another due to reasons like changing requirements or licensing changes. Migration typically entails updating and rewriting source code manually. While automated migration tools exist, most rely on mining examples from real-world projects that have already undergone similar migrations. However, these data are scarce, and collecting them for arbitrary pairs of libraries is difficult. Moreover, these migration tools often miss out on leveraging modern code transformation infrastructure.
  In this paper, we present a new approach to automated API migration that sidesteps the limitations described above. Instead of relying on existing migration data or using LLMs directly for transformation, we use LLMs to extract migration examples. Next, we use an Agent to generalize those examples to reusable transformation scripts in PolyglotPiranha, a modern code transformation tool. Our method distills latent migration knowledge from LLMs into structured, testable, and repeatable migration logic, without requiring preexisting corpora or manual engineering effort. Experimental results across Python libraries show that our system can generate diverse migration examples and synthesize transformation scripts that generalize to real-world codebases.

</details>


### [599] [Autoregressive, Yet Revisable: In Decoding Revision for Secure Code Generation](https://arxiv.org/abs/2602.01187)
*Chengran Yang,Zichao Wei,Heminghao Deng,Jinfeng Jiang,Zhensu Sun,Ting Zhang,Tianyi Wu,Ming Wen,David Lo*

Main category: cs.SE

Relevance: 85.0

TL;DR: 提出Stream of Revision范式，将代码生成从单调线性过程转变为动态自修正轨迹，通过特定动作标记让模型在单次前向传播中回溯和编辑历史，显著减少安全漏洞


<details>
  <summary>Details</summary>
Motivation: 当前基于LLM的代码生成被公式化为严格单调过程，仅线性追加标记到不可变前缀，这与编程的认知过程（交织前向生成和即时修订）形成对比。现有修订方法要么延迟高，要么无法利用模型内在语义推理能力。

Method: 提出Stream of Revision范式，引入特定动作标记，使模型能够在单次前向传播中无缝回溯和编辑自身历史。通过内化修订循环，让模型即时激活其潜在能力而无需外部依赖。

Result: 在安全代码生成任务上的实证结果显示，Stream of Revision显著减少了漏洞，同时推理开销最小。

Conclusion: Stream of Revision实现了从单调流到动态自修正轨迹的范式转变，通过利用模型内在能力实现更自然的代码生成过程，在安全代码生成方面表现出色。

Abstract: Large Language Model (LLM) based code generation is predominantly formulated as a strictly monotonic process, appending tokens linearly to an immutable prefix. This formulation contrasts to the cognitive process of programming, which is inherently interleaved with forward generation and on-the-fly revision. While prior works attempt to introduce revision via post-hoc agents or external static tools, they either suffer from high latency or fail to leverage the model's intrinsic semantic reasoning. In this paper, we propose Stream of Revision, a paradigm shift that elevates code generation from a monotonic stream to a dynamic, self-correcting trajectory by leveraging model's intrinsic capabilities. We introduce specific action tokens that enable the model to seamlessly backtrack and edit its own history within a single forward pass. By internalizing the revision loop, our framework Stream of Revision allows the model to activate its latent capabilities just-in-time without external dependencies. Empirical results on secure code generation show that Stream of Revision significantly reduces vulnerabilities with minimal inference overhead.

</details>


### [600] [CIPHER: Cryptographic Insecurity Profiling via Hybrid Evaluation of Responses](https://arxiv.org/abs/2602.01438)
*Max Manolov,Tony Gao,Siddharth Shukla,Cheng-Ting Chou,Ryan Lagasse*

Main category: cs.CR

Relevance: 85.0

TL;DR: CIPHER是一个评估LLM生成Python代码中加密漏洞的基准测试，通过不安全/中性/安全提示变体、加密漏洞分类和自动化评分管道来测量LLM在受控安全指导条件下的加密漏洞发生率。


<details>
  <summary>Details</summary>
Motivation: LLM越来越多地用于辅助开发者编写代码，但其生成的加密功能实现经常包含可利用的安全漏洞。微小的设计选择（如静态初始化向量或缺少认证）可能无声地破坏安全保证。目前缺乏系统评估LLM生成代码中加密漏洞的基准测试。

Method: CIPHER使用不安全/中性/安全三种提示变体针对每个任务，采用加密特定的漏洞分类法，并通过自动化评分管道进行行级归因。该方法在受控安全指导条件下测量LLM生成的Python代码中的加密漏洞发生率。

Result: 在广泛使用的多种LLM上测试发现，明确的"安全"提示可以减少某些特定问题，但并不能可靠地消除整体加密漏洞。这表明当前LLM在生成安全加密代码方面存在系统性缺陷。

Conclusion: LLM生成的加密代码仍然存在显著的安全风险，即使使用安全提示也不能完全消除漏洞。CIPHER基准测试和可复现的评分管道为评估和改进LLM的加密代码生成能力提供了重要工具。

Abstract: Large language models (LLMs) are increasingly used to assist developers with code, yet their implementations of cryptographic functionality often contain exploitable flaws. Minor design choices (e.g., static initialization vectors or missing authentication) can silently invalidate security guarantees. We introduce CIPHER(\textbf{C}ryptographic \textbf{I}nsecurity \textbf{P}rofiling via \textbf{H}ybrid \textbf{E}valuation of \textbf{R}esponses), a benchmark for measuring cryptographic vulnerability incidence in LLM-generated Python code under controlled security-guidance conditions. CIPHER uses insecure/neutral/secure prompt variants per task, a cryptography-specific vulnerability taxonomy, and line-level attribution via an automated scoring pipeline. Across a diverse set of widely used LLMs, we find that explicit ``secure'' prompting reduces some targeted issues but does not reliably eliminate cryptographic vulnerabilities overall. The benchmark and reproducible scoring pipeline will be publicly released upon publication.

</details>


### [601] [The Strategic Foresight of LLMs: Evidence from a Fully Prospective Venture Tournament](https://arxiv.org/abs/2602.01684)
*Felipe A. Csaszar,Aticus Peterson,Daniel Wilde*

Main category: econ.GN

Relevance: 85.0

TL;DR: LLMs在预测初创企业众筹成功率方面显著超越人类专家，Gemini 2.5 Pro达到0.74的排名相关性，正确排序近80%的项目对。


<details>
  <summary>Details</summary>
Motivation: 研究AI是否能在战略预见（对不确定、高风险结果形成准确判断）方面超越人类，特别是在预测初创企业众筹成功率的实际应用场景中。

Method: 使用前瞻性预测锦标赛方法，评估30个美国科技初创企业的Kickstarter众筹项目。让前沿和开源LLM完成870对比较，生成完整的成功预测排名。基准测试包括346名经验丰富的管理者和3名MBA背景投资者。

Result: 人类评估者的排名相关性在0.04-0.45之间，而多个前沿LLM超过0.60，最佳模型Gemini 2.5 Pro达到0.74。群体智慧集成或人机混合团队均未超越最佳独立模型。

Conclusion: LLM在战略预见任务上显著优于人类专家，展示了AI在复杂预测任务中的强大能力，特别是在需要处理不确定性和高风险决策的场景中。

Abstract: Can artificial intelligence outperform humans at strategic foresight -- the capacity to form accurate judgments about uncertain, high-stakes outcomes before they unfold? We address this question through a fully prospective prediction tournament using live Kickstarter crowdfunding projects. Thirty U.S.-based technology ventures, launched after the training cutoffs of all models studied, were evaluated while fundraising remained in progress and outcomes were unknown. A diverse suite of frontier and open-weight large language models (LLMs) completed 870 pairwise comparisons, producing complete rankings of predicted fundraising success. We benchmarked these forecasts against 346 experienced managers recruited via Prolific and three MBA-trained investors working under monitored conditions. The results are striking: human evaluators achieved rank correlations with actual outcomes between 0.04 and 0.45, while several frontier LLMs exceeded 0.60, with the best (Gemini 2.5 Pro) reaching 0.74 -- correctly ordering nearly four of every five venture pairs. These differences persist across multiple performance metrics and robustness checks. Neither wisdom-of-the-crowd ensembles nor human-AI hybrid teams outperformed the best standalone model.

</details>


### [602] [RedVisor: Reasoning-Aware Prompt Injection Defense via Zero-Copy KV Cache Reuse](https://arxiv.org/abs/2602.01795)
*Mingrui Liu,Sixiao Zhang,Cheng Long,Kwok-Yan Lam*

Main category: cs.CR

Relevance: 85.0

TL;DR: RedVisor是一个统一的框架，通过轻量级可移除适配器检测和防御LLM的提示注入攻击，保持模型原始性能的同时实现高效推理。


<details>
  <summary>Details</summary>
Motivation: 当前LLM提示注入防御面临两难：基于预防的微调会降低模型通用能力（对齐税），而基于检测的过滤则带来高延迟和内存开销。需要一种既能检测攻击又能保持模型性能的解决方案。

Method: 提出RedVisor框架，在冻结的主干模型上添加轻量级可移除适配器。该适配器首先生成可解释的分析，精确定位注入并阐述威胁，然后显式地指导模型拒绝恶意指令。适配器仅在推理阶段激活，在后续响应生成阶段静音，实现KV缓存重用策略，消除冗余预填充计算。

Result: 实验表明RedVisor在检测准确性和吞吐量方面优于现有防御方法，同时带来可忽略的效用损失。成功集成到vLLM服务引擎中。

Conclusion: RedVisor首次利用细粒度推理路径同时检测攻击并指导模型安全响应，在数学上保持主干模型在良性输入上的原始性能，实现了检测与预防的有机结合。

Abstract: Large Language Models (LLMs) are increasingly vulnerable to Prompt Injection (PI) attacks, where adversarial instructions hidden within retrieved contexts hijack the model's execution flow. Current defenses typically face a critical trade-off: prevention-based fine-tuning often degrades general utility via the "alignment tax", while detection-based filtering incurs prohibitive latency and memory costs. To bridge this gap, we propose RedVisor, a unified framework that synthesizes the explainability of detection systems with the seamless integration of prevention strategies. To the best of our knowledge, RedVisor is the first approach to leverage fine-grained reasoning paths to simultaneously detect attacks and guide the model's safe response. We implement this via a lightweight, removable adapter positioned atop the frozen backbone. This adapter serves a dual function: it first generates an explainable analysis that precisely localizes the injection and articulates the threat, which then explicitly conditions the model to reject the malicious command. Uniquely, the adapter is active only during this reasoning phase and is effectively muted during the subsequent response generation. This architecture yields two distinct advantages: (1) it mathematically preserves the backbone's original utility on benign inputs; and (2) it enables a novel KV Cache Reuse strategy, eliminating the redundant prefill computation inherent to decoupled pipelines. We further pioneer the integration of this defense into the vLLM serving engine with custom kernels. Experiments demonstrate that RedVisor outperforms state-of-the-art defenses in detection accuracy and throughput while incurring negligible utility loss.

</details>


### [603] [CAM: A Causality-based Analysis Framework for Multi-Agent Code Generation Systems](https://arxiv.org/abs/2602.02138)
*Lyu Zongyi,Ji Zhenlan,Chen Songqiang,Wang Liwen,Huang Yuheng,Wang Shuai,Cheung Shing-Chi*

Main category: cs.SE

Relevance: 85.0

TL;DR: CAM：首个基于因果关系的多智能体代码生成系统分析框架，通过系统量化中间特征对系统正确性的贡献，识别关键特征并优化系统设计。


<details>
  <summary>Details</summary>
Motivation: 多智能体代码生成系统产生大量中间输出，但这些中间输出对系统正确性的重要性尚不明确，阻碍了系统的针对性优化。需要一种系统方法来量化不同中间特征的重要性。

Method: 提出CAM框架：1）全面分类中间输出；2）系统模拟中间特征上的实际错误；3）识别对系统正确性重要的特征并聚合其重要性排名。

Result: 发现两个重要现象：1）上下文依赖特征的重要性主要通过与其他特征的交互体现；2）混合后端MACGS根据LLM相对优势分配任务，Pass@1提升达7.2%。应用：故障修复成功率73.3%，特征剪枝减少66.8%中间token消耗。

Conclusion: CAM为MACGS设计和部署提供了可操作的见解，建立了因果分析作为理解和改进MACGS的强大方法。揭示了特征交互的重要性，并展示了混合架构的潜力。

Abstract: Despite the remarkable success that Multi-Agent Code Generation Systems (MACGS) have achieved, the inherent complexity of multi-agent architectures produces substantial volumes of intermediate outputs. To date, the individual importance of these intermediate outputs to the system correctness remains opaque, which impedes targeted optimization of MACGS designs. To address this challenge, we propose CAM, the first \textbf{C}ausality-based \textbf{A}nalysis framework for \textbf{M}ACGS that systematically quantifies the contribution of different intermediate features for system correctness. By comprehensively categorizing intermediate outputs and systematically simulating realistic errors on intermediate features, we identify the important features for system correctness and aggregate their importance rankings.
  We conduct extensive empirical analysis on the identified importance rankings. Our analysis reveals intriguing findings: first, we uncover context-dependent features\textemdash features whose importance emerges mainly through interactions with other features, revealing that quality assurance for MACGS should incorporate cross-feature consistency checks; second, we reveal that hybrid backend MACGS with different backend LLMs assigned according to their relative strength achieves up to 7.2\% Pass@1 improvement, underscoring hybrid architectures as a promising direction for future MACGS design. We further demonstrate CAM's practical utility through two applications: (1) failure repair which achieves a 73.3\% success rate by optimizing top-3 importance-ranked features and (2) feature pruning that reduces up to 66.8\% intermediate token consumption while maintaining generation performance. Our work provides actionable insights for MACGS design and deployment, establishing causality analysis as a powerful approach for understanding and improving MACGS.

</details>


### [604] [SWE-Universe: Scale Real-World Verifiable Environments to Millions](https://arxiv.org/abs/2602.02361)
*Mouxiang Chen,Lei Zhang,Yunlong Feng,Xuwu Wang,Wenting Zhao,Ruisheng Cao,Jiaxi Yang,Jiawei Chen,Mingze Li,Zeyao Ma,Hao Ge,Zongmeng Zhang,Zeyu Cui,Dayiheng Liu,Jingren Zhou,Jianling Sun,Junyang Lin,Binyuan Hui*

Main category: cs.SE

Relevance: 85.0

TL;DR: SWE-Universe：一个从GitHub PR自动构建真实世界软件工程可验证环境的大规模高效框架，通过定制训练模型驱动的构建代理实现百万级环境构建，用于编码代理训练和评估。


<details>
  <summary>Details</summary>
Motivation: 当前自动构建真实世界软件工程环境面临三大挑战：构建成功率低、验证器能力弱、成本过高。需要一种可扩展的方法来创建大量高质量、可验证的编程任务环境，以推动下一代编码代理的发展。

Method: 1. 使用定制训练模型驱动的构建代理；2. 采用迭代自验证和循环内黑客检测机制；3. 从GitHub PR自动构建多语言软件工程环境；4. 实现百万级环境规模（807,693个）。

Result: 1. 成功构建了807,693个真实世界多语言软件工程环境；2. 通过大规模代理中间训练和强化学习验证了环境价值；3. 应用于Qwen3-Max-Thinking模型，在SWE-Bench Verified上获得75.3%的分数。

Conclusion: SWE-Universe为推进下一代编码代理提供了关键资源和稳健方法学，解决了真实世界软件工程环境构建的规模化难题，显著提升了编码代理的评估和训练能力。

Abstract: We propose SWE-Universe, a scalable and efficient framework for automatically constructing real-world software engineering (SWE) verifiable environments from GitHub pull requests (PRs). To overcome the prevalent challenges of automatic building, such as low production yield, weak verifiers, and prohibitive cost, our framework utilizes a building agent powered by an efficient custom-trained model. This agent employs iterative self-verification and in-loop hacking detection to ensure the reliable generation of high-fidelity, verifiable tasks. Using this method, we scale the number of real-world multilingual SWE environments to a million scale (807,693). We demonstrate the profound value of our environments through large-scale agentic mid-training and reinforcement learning. Finally, we applied this technique to Qwen3-Max-Thinking and achieved a score of 75.3% on SWE-Bench Verified. Our work provides both a critical resource and a robust methodology to advance the next generation of coding agents.

</details>


### [605] [PolarMem: A Training-Free Polarized Latent Graph Memory for Verifiable Multimodal Agents](https://arxiv.org/abs/2602.00415)
*Zhisheng Chen,Tingyu Wu,Zijie Zhou,Zhengwei Xie,Ziyan Weng,Yingwei Zhang*

Main category: cs.AI

Relevance: 75.0

TL;DR: PolarMem是一种无需训练的记忆系统，将模糊的感知似然转化为离散的逻辑约束，通过极化图拓扑结构显式存储否定信息，为可验证的多模态智能体提供基础。


<details>
  <summary>Details</summary>
Motivation: 当前多模态智能体从被动观察者发展为长期决策者，需要具有逻辑可验证性的记忆系统。现有架构存在认知不对称性，将语义亲和性与事实存在性混为一谈，且无法编码否定约束。

Method: 提出PolarMem（极化潜在图记忆），通过非参数分布划分将模糊感知似然转化为离散逻辑约束，采用具有正交抑制连接的极化图拓扑结构，显式存储已验证的否定作为主要认知状态，在推理时实施逻辑主导的检索范式。

Result: 在8个冻结的视觉语言模型和6个基准测试上的广泛评估表明，PolarMem作为稳健的认知系统运行，为可验证的多模态智能体奠定了基础。

Conclusion: PolarMem解决了当前多模态智能体记忆系统的根本限制，通过逻辑可验证的记忆架构为可信AI提供了重要支持。

Abstract: As multimodal agents evolve from passive observers to long-horizon decision-makers, they require memory systems that provide not just information availability but logical verifiability. A fundamental limitation of current architectures is the epistemic asymmetry inherent in probabilistic vision-language models and dense associative memories: they conflate semantic affinity with factual existence and structurally fail to encode negative constraints. To this end, we introduce PolarMem, a training-free Polarized Latent Graph Memory designed to ground agent reasoning in verifiable evidence. PolarMem transforms fuzzy perceptual likelihoods into discrete logical constraints through non-parametric distributional partitioning. Furthermore, it employs a polarized graph topology with orthogonal inhibitory connections to explicitly store verified negation as a primary cognitive state. At inference time, we enforce a logic-dominant retrieval paradigm, suppressing hallucinatory patterns that violate negative constraints. Extensive evaluation across eight frozen Vision--Language Models and six benchmarks demonstrates that PolarMem functions as a robust cognitive system, establishing a foundation for verifiable multimodal agents. Our code is available at https://github.com/czs-ict/PolarMem.

</details>


### [606] [Benchmarking Agents in Insurance Underwriting Environments](https://arxiv.org/abs/2602.00456)
*Amanda Dsouza,Ramya Ramakrishnan,Charles Dickens,Bhavishya Pohani,Christopher M Glaze*

Main category: cs.AI

Relevance: 75.0

TL;DR: UNDERWRITE是一个专家主导的多轮保险承保基准，旨在评估AI代理在真实企业环境中的表现，揭示前沿模型在企业部署准备度方面的显著差距。


<details>
  <summary>Details</summary>
Motivation: 现有基准测试过度强调开放领域（如代码），使用狭窄的准确性指标，缺乏真实复杂性，无法反映企业应用的实际挑战。需要设计能够捕捉真实世界企业复杂性的基准。

Method: 与领域专家密切合作设计专家优先的多轮保险承保基准，引入关键现实因素：专有业务知识、噪声工具接口、需要仔细信息收集的不完美模拟用户。评估了13个前沿模型。

Result: 研究发现：最准确的模型不一定最有效；模型即使有工具访问权限仍会产生领域知识幻觉；pass^k结果显示性能下降20%；常见代理框架存在脆弱性；专业领域的幻觉检测需要组合方法。

Conclusion: 专家参与基准设计对于现实代理评估至关重要，现有基准与企业部署需求存在差距，需要开发更符合企业要求的评估方法。

Abstract: As AI agents integrate into enterprise applications, their evaluation demands benchmarks that reflect the complexity of real-world operations. Instead, existing benchmarks overemphasize open-domains such as code, use narrow accuracy metrics, and lack authentic complexity. We present UNDERWRITE, an expert-first, multi-turn insurance underwriting benchmark designed in close collaboration with domain experts to capture real-world enterprise challenges. UNDERWRITE introduces critical realism factors often absent in current benchmarks: proprietary business knowledge, noisy tool interfaces, and imperfect simulated users requiring careful information gathering. Evaluating 13 frontier models, we uncover significant gaps between research lab performance and enterprise readiness: the most accurate models are not the most efficient, models hallucinate domain knowledge despite tool access, and pass^k results show a 20% drop in performance. The results from UNDERWRITE demonstrate that expert involvement in benchmark design is essential for realistic agent evaluation, common agentic frameworks exhibit brittleness that skews performance reporting, and hallucination detection in specialized domains demands compositional approaches. Our work provides insights for developing benchmarks that better align with enterprise deployment requirements.

</details>


### [607] [Dual Latent Memory for Visual Multi-agent System](https://arxiv.org/abs/2602.00471)
*Xinlei Yu,Chengming Xu,Zhangquan Chen,Bo Yin,Cheng Yang,Yongbo He,Yihao Hu,Jiangning Zhang,Cheng Tan,Xiaobin Hu,Shuicheng Yan*

Main category: cs.AI

Relevance: 75.0

TL;DR: L²-VMAS提出了一种基于双潜在记忆的视觉多智能体系统框架，通过解耦感知与思考、动态合成双潜在记忆以及熵驱动的主动触发机制，解决了传统文本通信中的信息瓶颈问题，有效突破了多智能体协作中的"扩展墙"问题。


<details>
  <summary>Details</summary>
Motivation: 当前视觉多智能体系统(VMAS)存在一个反直觉的"扩展墙"现象：增加智能体交互轮次反而会降低性能，同时指数级增加token成本。作者认为这源于文本中心通信的信息瓶颈，将感知和思维轨迹转换为离散自然语言会导致语义损失。

Method: 提出L²-VMAS框架：1) 使用双潜在记忆实现智能体间协作；2) 解耦感知和思考过程；3) 动态合成双潜在记忆；4) 引入熵驱动的主动触发机制，用按需内存访问替代被动信息传输。

Result: 在不同骨干网络、模型大小和多智能体结构上的广泛实验表明，该方法有效突破了"扩展墙"，具有出色的可扩展性：平均准确率提升2.7-5.4%，同时token使用量减少21.3-44.8%。

Conclusion: L²-VMAS通过双潜在记忆和主动触发机制解决了VMAS中的信息瓶颈问题，为多智能体协作提供了更高效、可扩展的解决方案，在保持性能提升的同时显著降低了计算成本。

Abstract: While Visual Multi-Agent Systems (VMAS) promise to enhance comprehensive abilities through inter-agent collaboration, empirical evidence reveals a counter-intuitive "scaling wall": increasing agent turns often degrades performance while exponentially inflating token costs. We attribute this failure to the information bottleneck inherent in text-centric communication, where converting perceptual and thinking trajectories into discrete natural language inevitably induces semantic loss. To this end, we propose L$^{2}$-VMAS, a novel model-agnostic framework that enables inter-agent collaboration with dual latent memories. Furthermore, we decouple the perception and thinking while dynamically synthesizing dual latent memories. Additionally, we introduce an entropy-driven proactive triggering that replaces passive information transmission with efficient, on-demand memory access. Extensive experiments among backbones, sizes, and multi-agent structures demonstrate that our method effectively breaks the "scaling wall" with superb scalability, improving average accuracy by 2.7-5.4% while reducing token usage by 21.3-44.8%. Codes: https://github.com/YU-deep/L2-VMAS.

</details>


### [608] [Structured Self-Consistency:A Multi-Task Evaluation of LLMs on VirtualHome](https://arxiv.org/abs/2602.00611)
*Jiaqi Xu,Tao Huang,Kai Zhang*

Main category: cs.AI

Relevance: 75.0

TL;DR: 论文在VirtualHome基准上评估了两个7B参数的LLM（OPENPANGU-7B和QWEN2.5-7B）在具身AI任务中的表现，提出了结构化自一致性（SSC）解码策略来提升结构化生成任务的质量。


<details>
  <summary>Details</summary>
Motivation: 具身AI需要智能体在模拟环境中理解目标、规划动作并执行任务。目前缺乏对大型语言模型在具身AI基准上的系统性评估，特别是针对结构化任务生成的质量改进方法。

Method: 使用Embodied Agent Interface框架在VirtualHome基准上评估两个7B参数模型。提出了结构化自一致性（SSC）解码策略，通过多次采样结合领域特定的投票机制来改进结构化生成任务的质量。

Result: SSC显著提升了模型性能。OPENPANGU-7B在层次化规划任务上表现优异，而QWEN2.5-7B在动作级任务上具有优势。两种模型类型展现出互补的优势。

Conclusion: 研究揭示了不同LLM在具身AI任务中的互补优势，提出的SSC解码策略能有效提升结构化生成质量，为未来具身AI系统开发提供了重要见解。

Abstract: Embodied AI requires agents to understand goals, plan actions, and execute tasks in simulated environments.We present a comprehensive evaluation of Large Language Models (LLMs) on the VirtualHome benchmark using the Embodied Agent Interface (EAI) framework.We compare two representative 7B-parameter models OPENPANGU-7B and QWEN2.5-7B across four fundamental tasks: Goal Interpretation, Action Sequencing, Subgoal Decomposition, and Transition Modeling.We propose Structured Self-Consistency (SSC), an enhanced decoding strategy that leverages multiple sampling with domain-specific voting mechanisms to improve output quality for structured generation tasks. Experimental results demonstrate that SSC significantly enhances performance, with OPENPANGU-7B excelling at hierarchical planning while QWEN2.5-7B show advantages in action-level tasks. Our analysis reveals complementary strengths across model types, providing insights for future embodied AI system development.

</details>


### [609] [Inference-Only Prompt Projection for Safe Text-to-Image Generation with TV Guarantees](https://arxiv.org/abs/2602.00616)
*Minhyuk Lee,Hyekyung Yoon,Myungjoo Kang*

Main category: cs.AI

Relevance: 75.0

TL;DR: 提出一种基于总变差理论的文本到图像扩散模型安全对齐框架，通过推理时提示投影实现安全性与提示对齐的权衡，无需重新训练模型


<details>
  <summary>Details</summary>
Motivation: 文本到图像扩散模型需要安全防护来抑制不安全生成，但传统方法会损害良性提示的图像对齐质量。本文从总变差理论角度形式化这一权衡，提出在保持良性提示对齐的同时减少不安全生成的方法。

Method: 提出推理时提示投影框架：1）通过总变差理论形式化安全-提示对齐权衡；2）对高风险提示进行选择性干预，将其映射到容忍度控制的安集合；3）使用带验证的代理目标函数；4）保持良性提示基本不变，无需重新训练生成器。

Result: 在四个数据集和三个扩散模型骨干上，相比强模型级对齐基线，实现了16.7-60.0%的不当百分比相对减少，同时在COCO上保持良性提示图像对齐接近未对齐参考水平。

Conclusion: 通过总变差理论框架形式化了扩散模型安全对齐的固有权衡，提出的推理时提示投影方法能有效减少不安全生成，同时保持良性提示的对齐质量，为实际部署提供了实用解决方案。

Abstract: Text-to-Image (T2I) diffusion models enable high-quality open-ended synthesis, but their real-world deployment demands safeguards that suppress unsafe generations without degrading benign prompt-image alignment. We formalize this tension through a total variation (TV) lens: once the reference conditional distribution is fixed, any nontrivial reduction in unsafe generations necessarily incurs TV deviation from the reference, yielding a principled Safety-Prompt Alignment Trade-off (SPAT). Guided by this view, we propose an inference-only prompt projection framework that selectively intervenes on high-risk prompts via a surrogate objective with verification, mapping them into a tolerance-controlled safe set while leaving benign prompts effectively unchanged, without retraining or fine-tuning the generator. Across four datasets and three diffusion backbones, our approach achieves 16.7-60.0% relative reductions in inappropriate percentage (IP) versus strong model-level alignment baselines, while preserving benign prompt-image alignment on COCO near the unaligned reference.

</details>


### [610] [Persuasion Propagation in LLM Agents](https://arxiv.org/abs/2602.00851)
*Hyejun Jeong,Amir Houmansadr,Shlomo Zilberstein,Eugene Bagdasarian*

Main category: cs.AI

Relevance: 75.0

TL;DR: 研究AI智能体在长期任务中受到用户说服时的影响，提出"说服传播"概念，发现任务前说服比任务中说服对行为影响更显著


<details>
  <summary>Details</summary>
Motivation: 现代AI智能体结合对话交互与自主任务执行（如编码和网络研究），当智能体在执行长期任务时受到用户说服，其下游任务行为会受到何种影响？这引发了关于"说服传播"现象的研究动机。

Method: 引入以行为为中心的评价框架，区分任务执行期间和任务执行前的说服干预。在网络研究和编码任务上进行实验，比较"实时说服"和"信念预填充"两种干预方式的效果。

Result: 实时说服对行为影响较弱且不一致；而信念预填充的智能体平均减少26.9%的搜索次数和16.9%的唯一来源访问量，表明任务前的说服能显著影响智能体行为。

Conclusion: 说服（即使是先前的交互）能够影响智能体行为，这强调了在智能体系统中进行行为层面评估的重要性，为AI智能体的可信性和安全性研究提供了新视角。

Abstract: Modern AI agents increasingly combine conversational interaction with autonomous task execution, such as coding and web research, raising a natural question: what happens when an agent engaged in long-horizon tasks is subjected to user persuasion? We study how belief-level intervention can influence downstream task behavior, a phenomenon we name \emph{persuasion propagation}. We introduce a behavior-centered evaluation framework that distinguishes between persuasion applied during or prior to task execution. Across web research and coding tasks, we find that on-the-fly persuasion induces weak and inconsistent behavioral effects. In contrast, when the belief state is explicitly specified at task time, belief-prefilled agents conduct on average 26.9\% fewer searches and visit 16.9\% fewer unique sources than neutral-prefilled agents. These results suggest that persuasion, even in prior interaction, can affect the agent's behavior, motivating behavior-level evaluation in agentic systems.

</details>


### [611] [Position: Human-Centric AI Requires a Minimum Viable Level of Human Understanding](https://arxiv.org/abs/2602.00854)
*Fangzhou Lin,Qianwen Ge,Lingyu Xu,Peiran Li,Xiangbo Gao,Shuo Xing,Kazunori Yamada,Ziming Zhang,Haichong Zhang,Zhengzhong Tu*

Main category: cs.AI

Relevance: 75.0

TL;DR: 论文提出"能力-理解鸿沟"概念，即AI系统性能提升而用户理解能力下降，并定义"认知完整性阈值"作为保持人类监督所需的最低理解水平。


<details>
  <summary>Details</summary>
Motivation: 随着AI系统产生越来越流畅、正确的端到端结果，用户解释、验证或干预的能力逐渐被侵蚀。当前透明度、用户控制、素养和治理方法未能解决人类在持续AI委托下必须保留的基础理解问题。

Method: 提出认知完整性阈值(CIT)概念，通过三个功能维度操作化：(i)验证能力，(ii)保持理解的交互设计，(iii)治理的制度框架。这推动了一个设计与治理议程，使人类-AI交互与责任关键环境中的认知可持续性保持一致。

Result: 建立了能力-理解鸿沟的理论框架，定义了认知完整性阈值作为保持监督、自主性和问责参与所需的最低理解水平。提出了一个操作化框架，为责任关键环境中的人类-AI交互设计提供指导。

Conclusion: 需要重新思考AI透明度、用户控制和治理方法，确保在AI协助下人类保持足够的理解能力来维持有效的监督和问责。认知完整性阈值为这一目标提供了概念基础和设计原则。

Abstract: AI systems increasingly produce fluent, correct, end-to-end outcomes. Over time, this erodes users' ability to explain, verify, or intervene. We define this divergence as the Capability-Comprehension Gap: a decoupling where assisted performance improves while users' internal models deteriorate. This paper argues that prevailing approaches to transparency, user control, literacy, and governance do not define the foundational understanding humans must retain for oversight under sustained AI delegation. To formalize this, we define the Cognitive Integrity Threshold (CIT) as the minimum comprehension required to preserve oversight, autonomy, and accountable participation under AI assistance. CIT does not require full reasoning reconstruction, nor does it constrain automation. It identifies the threshold beyond which oversight becomes procedural and contestability fails. We operatinalize CIT through three functional dimensions: (i) verification capacity, (ii) comprehension-preserving interaction, and (iii) institutional scaffolds for governance. This motivates a design and governance agenda that aligns human-AI interaction with cognitive sustainability in responsibility-critical settings.

</details>


### [612] [Foundation CAN LM: A Pretrained Language Model For Automotive CAN Data](https://arxiv.org/abs/2602.00866)
*Akiharu Esashi,Pawissanutt Lertpongrujikorn,Justin Makino,Yuibi Fujimoto,Mohsen Amini Salehi*

Main category: cs.AI

Relevance: 75.0

TL;DR: 提出首个CAN总线基础模型，将解码后的CAN信号视为语言进行大规模预训练，通过统一的分词方案处理混合离散-连续信号，实现跨多种汽车保险任务的有效适应


<details>
  <summary>Details</summary>
Motivation: 现有CAN数据处理方法主要针对特定任务训练孤立模型，使用原始数据且很少探索解码信号，这种碎片化方法阻碍了共享表示学习和跨任务泛化。受NLP和CV领域基础模型范式成功的启发，作者希望将这一范式应用于CAN数据

Method: 1) 将CAN数据视为语言进行处理；2) 在大规模未标记解码CAN信号上进行预训练；3) 提出统一的分词方案处理混合离散-连续信号；4) 解决时间复杂性和行程特定变异性的挑战；5) 在异构汽车保险任务上进行微调

Result: 一个预训练的CAN模型能够有效适应多种预测任务，验证了基础模型范式在CAN数据上的有效性，为汽车AI中的可泛化表示学习确立了新方向

Conclusion: 基础模型范式（大规模预训练+任务特定适应）在NLP和CV领域取得成功后，同样适用于CAN数据，能够实现多目标下游泛化，为汽车AI领域开辟了新的研究方向

Abstract: The Controller Area Network (CAN) bus provides a rich source of vehicular signals increasingly leveraged for applications in automotive and auto insurance domains, including collision detection, predictive maintenance, and driver risk modeling. Despite this potential, existing pipelines largely train isolated task-specific models on raw CAN data, with only limited efforts exploring decoded signals. Such fragmentation prevents shared representation learning and limits cross-task generalization. By contrast, natural language processing (NLP) and computer vision (CV) have been transformed by the foundation model paradigm: large-scale pretraining followed by task-specific adaptation. In this work, we introduce the foundation CAN model that demonstrates multi-objective downstream generalization using a single pretrained backbone. Our approach treats CAN data as a language: we pretrain on large-scale, unlabeled decoded CAN signals and fine-tune across heterogeneous auto insurance tasks. To enable this, we propose a unified tokenization scheme for mixed discrete-continuous signals and address challenges of temporal complexity and trip-specific variability. Our results show that one pretrained CAN model can adapt effectively to diverse predictive tasks, validating that the foundation modeling paradigm, proven in NLP and CV, also holds for CAN data. This establishes a new direction for generalizable representation learning in automotive AI.

</details>


### [613] [Supervised sparse auto-encoders as unconstrained feature models for semantic composition](https://arxiv.org/abs/2602.00924)
*Ouns El Harzli,Hugo Wallner,Yoonsoo Nam,Haixuan Xavier Tao*

Main category: cs.AI

Relevance: 75.0

TL;DR: 本文提出了一种监督式稀疏自编码器方法，通过结合无约束特征模型和监督任务，解决了传统SAE在L1惩罚非平滑性和特征-语义对齐方面的局限性，在Stable Diffusion 3.5上实现了组合泛化和语义图像编辑。


<details>
  <summary>Details</summary>
Motivation: 稀疏自编码器在机制可解释性研究中面临两个主要挑战：1) L1惩罚的非平滑性阻碍了重建和可扩展性；2) 学习到的特征与人类语义缺乏对齐。本文旨在解决这些限制。

Method: 采用无约束特征模型（来自神经坍塌理论的数学框架）并监督任务。监督（仅解码器）SAE重建特征向量，通过联合学习稀疏概念嵌入和解码器权重。

Result: 在Stable Diffusion 3.5上验证，该方法展示了组合泛化能力，成功重建训练中未见过的概念组合图像，并实现了无需提示修改的语义图像编辑。

Conclusion: 通过结合无约束特征模型和监督学习，提出的方法有效解决了传统稀疏自编码器的局限性，为机制可解释性和语义可控的图像生成提供了新途径。

Abstract: Sparse auto-encoders (SAEs) have re-emerged as a prominent method for mechanistic interpretability, yet they face two significant challenges: the non-smoothness of the $L_1$ penalty, which hinders reconstruction and scalability, and a lack of alignment between learned features and human semantics. In this paper, we address these limitations by adapting unconstrained feature models-a mathematical framework from neural collapse theory-and by supervising the task. We supervise (decoder-only) SAEs to reconstruct feature vectors by jointly learning sparse concept embeddings and decoder weights. Validated on Stable Diffusion 3.5, our approach demonstrates compositional generalization, successfully reconstructing images with concept combinations unseen during training, and enabling feature-level intervention for semantic image editing without prompt modification.

</details>


### [614] [MindGuard: Guardrail Classifiers for Multi-Turn Mental Health Support](https://arxiv.org/abs/2602.00950)
*António Farinhas,Nuno M. Guerreiro,José Pombal,Pedro Henrique Martins,Laura Melton,Alex Conway,Cara Dochat,Maya D'Eon,Ricardo Rei*

Main category: cs.AI

Relevance: 75.0

TL;DR: 论文提出MindGuard，一个专门用于心理健康对话的安全分类器系统，包含临床风险分类法、专家标注数据集和轻量级分类器模型，旨在区分治疗性披露和真实临床危机。


<details>
  <summary>Details</summary>
Motivation: 现有通用安全机制无法区分心理健康对话中的治疗性披露和真实临床危机，导致安全失效。需要专门针对心理健康场景的临床安全评估系统。

Method: 1) 与心理学博士合作开发临床风险分类法；2) 发布MindGuard-testset真实多轮对话数据集；3) 使用受控双智能体设置生成合成对话；4) 训练轻量级安全分类器（4B和8B参数）；5) 与临床语言模型结合使用。

Result: MindGuard在高召回率操作点减少误报，与临床语言模型结合时，在对抗性多轮交互中实现更低的攻击成功率和有害参与率。

Conclusion: 需要专门针对心理健康场景的临床安全评估系统，MindGuard提供了一种有效的解决方案，能够更好地区分治疗性内容和真实危机。

Abstract: Large language models are increasingly used for mental health support, yet their conversational coherence alone does not ensure clinical appropriateness. Existing general-purpose safeguards often fail to distinguish between therapeutic disclosures and genuine clinical crises, leading to safety failures. To address this gap, we introduce a clinically grounded risk taxonomy, developed in collaboration with PhD-level psychologists, that identifies actionable harm (e.g., self-harm and harm to others) while preserving space for safe, non-crisis therapeutic content. We release MindGuard-testset, a dataset of real-world multi-turn conversations annotated at the turn level by clinical experts. Using synthetic dialogues generated via a controlled two-agent setup, we train MindGuard, a family of lightweight safety classifiers (with 4B and 8B parameters). Our classifiers reduce false positives at high-recall operating points and, when paired with clinician language models, help achieve lower attack success and harmful engagement rates in adversarial multi-turn interactions compared to general-purpose safeguards. We release all models and human evaluation data.

</details>


### [615] [AutoHealth: An Uncertainty-Aware Multi-Agent System for Autonomous Health Data Modeling](https://arxiv.org/abs/2602.01078)
*Tong Xia,Weibin Li,Gang Liu,Yong Li*

Main category: cs.AI

Relevance: 75.0

TL;DR: AutoHealth是一个不确定性感知的多智能体系统，用于自主建模健康数据并评估模型可靠性，通过五个专门智能体的闭环协调，在预测性能和不确定性量化方面显著优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 当前基于LLM的智能体在健康数据应用中存在三个主要限制：1) 难以泛化到异构健康数据模态；2) 过度依赖预定义解决方案模板，对任务特定目标适应不足；3) 忽视不确定性估计，而这对医疗决策可靠性至关重要。

Method: 提出AutoHealth系统，采用五个专门智能体进行闭环协调：数据探索、任务条件模型构建、训练、优化等，同时优先考虑预测性能和不确定性量化。系统不仅生成即用模型，还产生支持可信解释和风险感知决策的综合报告。

Result: 在包含17个跨数据模态和学习设置的现实基准测试中，AutoHealth完成所有任务，预测性能比最先进基线提高29.2%，不确定性估计提高50.2%。

Conclusion: AutoHealth通过不确定性感知的多智能体系统成功解决了健康数据建模中的泛化、适应性和可靠性问题，为医疗领域的可信AI决策提供了有效工具。

Abstract: LLM-based agents have demonstrated strong potential for autonomous machine learning, yet their applicability to health data remains limited. Existing systems often struggle to generalize across heterogeneous health data modalities, rely heavily on predefined solution templates with insufficient adaptation to task-specific objectives, and largely overlook uncertainty estimation, which is essential for reliable decision-making in healthcare. To address these challenges, we propose \textit{AutoHealth}, a novel uncertainty-aware multi-agent system that autonomously models health data and assesses model reliability. \textit{AutoHealth} employs closed-loop coordination among five specialized agents to perform data exploration, task-conditioned model construction, training, and optimization, while jointly prioritizing predictive performance and uncertainty quantification. Beyond producing ready-to-use models, the system generates comprehensive reports to support trustworthy interpretation and risk-aware decision-making. To rigorously evaluate its effectiveness, we curate a challenging real-world benchmark comprising 17 tasks across diverse data modalities and learning settings. \textit{AutoHealth} completes all tasks and outperforms state-of-the-art baselines by 29.2\% in prediction performance and 50.2\% in uncertainty estimation.

</details>


### [616] [RE-MCDF: Closed-Loop Multi-Expert LLM Reasoning for Knowledge-Grounded Clinical Diagnosis](https://arxiv.org/abs/2602.01297)
*Shaowei Shen,Xiaohong Yang,Jie Yang,Lianfen Huang,Yongcai Zhang,Yang Zou,Seyyedali Hosseinalipour*

Main category: cs.AI

Relevance: 75.0

TL;DR: RE-MCDF是一个关系增强的多专家临床诊断框架，通过生成-验证-修订闭环架构，结合主专家、实验室专家和多关系评估专家组，利用医学知识图谱增强电子病历的诊断准确性。


<details>
  <summary>Details</summary>
Motivation: 电子病历（特别是神经科）具有异构、稀疏和噪声的特点，单智能体系统容易产生自强化错误，现有多智能体框架交互浅层且缺乏结构，忽略了疾病间的丰富逻辑依赖关系（互斥性、病理兼容性、诊断混淆），无法排除临床不可行的假设。

Method: 提出RE-MCDF框架：1）主专家生成候选诊断和支持证据；2）实验室专家动态优先处理异构临床指标；3）多关系感知与评估专家组显式执行疾病间逻辑约束。基于医学知识图谱，前两个专家自适应重新加权EMR证据，专家组验证和修正候选诊断以确保逻辑一致性。

Result: 在CMEMR的神经学子集（NEEMRs）和自建数据集（XMEMRs）上的广泛实验表明，RE-MCDF在复杂诊断场景中持续优于最先进的基线方法。

Conclusion: RE-MCDF通过引入关系增强的多专家架构，有效解决了临床诊断中LLM面临的异构、稀疏数据问题，通过显式建模疾病间逻辑依赖关系，提高了诊断的准确性和逻辑一致性。

Abstract: Electronic medical records (EMRs), particularly in neurology, are inherently heterogeneous, sparse, and noisy, which poses significant challenges for large language models (LLMs) in clinical diagnosis. In such settings, single-agent systems are vulnerable to self-reinforcing errors, as their predictions lack independent validation and can drift toward spurious conclusions. Although recent multi-agent frameworks attempt to mitigate this issue through collaborative reasoning, their interactions are often shallow and loosely structured, failing to reflect the rigorous, evidence-driven processes used by clinical experts. More fundamentally, existing approaches largely ignore the rich logical dependencies among diseases, such as mutual exclusivity, pathological compatibility, and diagnostic confusion. This limitation prevents them from ruling out clinically implausible hypotheses, even when sufficient evidence is available. To overcome these, we propose RE-MCDF, a relation-enhanced multi-expert clinical diagnosis framework. RE-MCDF introduces a generation--verification--revision closed-loop architecture that integrates three complementary components: (i) a primary expert that generates candidate diagnoses and supporting evidence, (ii) a laboratory expert that dynamically prioritizes heterogeneous clinical indicators, and (iii) a multi-relation awareness and evaluation expert group that explicitly enforces inter-disease logical constraints. Guided by a medical knowledge graph (MKG), the first two experts adaptively reweight EMR evidence, while the expert group validates and corrects candidate diagnoses to ensure logical consistency. Extensive experiments on the neurology subset of CMEMR (NEEMRs) and on our curated dataset (XMEMRs) demonstrate that RE-MCDF consistently outperforms state-of-the-art baselines in complex diagnostic scenarios.

</details>


### [617] [Model Specific Task Similarity for Vision Language Model Selection via Layer Conductance](https://arxiv.org/abs/2602.01346)
*Wei Yang,Hong Xie,Tao Tan,Xin Li,Defu Lian,Enhong Chen*

Main category: cs.AI

Relevance: 75.0

TL;DR: 提出了一种基于视觉编码器内部功能动态的VLM选择框架，通过方向性传导散度(DCD)度量源任务对目标任务功能块的覆盖程度，无需直接推理即可预测模型排名。


<details>
  <summary>Details</summary>
Motivation: 当前开源视觉语言模型(VLMs)众多，但为特定下游任务选择最优预训练模型仍然困难。现有方法要么依赖数据密集型代理，要么使用对称文本描述符，忽略了迁移性的方向性和模型特定性。

Method: 通过层间传导表示每个任务，通过熵正则化对齐得到目标条件块重要性分布，提出方向性传导散度(DCD)这一非对称度量，量化源任务对目标任务重要功能块的覆盖程度。

Result: 在48个VLMs和21个数据集上的实验表明，该方法优于现有基线，在NDCG@5指标上比SWAB提升了14.7%。

Conclusion: 该方法通过分析视觉编码器的内部功能动态，为VLM选择提供了有效框架，解决了现有方法忽略迁移方向性和模型特定性的问题。

Abstract: While open sourced Vision-Language Models (VLMs) have proliferated, selecting the optimal pretrained model for a specific downstream task remains challenging. Exhaustive evaluation is often infeasible due to computational constraints and data limitations in few shot scenarios. Existing selection methods fail to fully address this: they either rely on data-intensive proxies or use symmetric textual descriptors that neglect the inherently directional and model-specific nature of transferability. To address this problem, we propose a framework that grounds model selection in the internal functional dynamics of the visual encoder. Our approach represents each task via layer wise conductance and derives a target-conditioned block importance distribution through entropy regularized alignment. Building on this, we introduce Directional Conductance Divergence (DCD), an asymmetric metric that quantifies how effectively a source task covers the target's salient functional blocks. This allows for predicting target model rankings by aggregating source task ranks without direct inference. Experimental results on 48 VLMs across 21 datasets demonstrate that our method outperforms state-of-the-art baselines, achieving a 14.7% improvement in NDCG@5 over SWAB.

</details>


### [618] [Building Better Deception Probes Using Targeted Instruction Pairs](https://arxiv.org/abs/2602.01425)
*Vikram Natarajan,Devina Jain,Shivam Arora,Satvik Golechha,Joseph Bloom*

Main category: cs.AI

Relevance: 75.0

TL;DR: 研究发现线性探针在检测AI欺骗行为时，指令对的选择比数据集更重要，解释了70.6%的性能方差。欺骗意图而非内容模式被捕获，建议针对特定威胁模型设计专门探针而非通用检测器。


<details>
  <summary>Details</summary>
Motivation: 线性探针是监控AI系统欺骗行为的有前景方法，但现有方法在简单场景中仍存在明显失败，包括虚假相关性和对非欺骗响应的误报。需要改进探针的可靠性和有效性。

Method: 通过分析训练中使用的指令对的重要性，并基于人类可解释的欺骗分类法来针对特定欺骗行为。研究指令对如何捕获欺骗意图而非内容特定模式。

Result: 指令选择主导探针性能（解释70.6%的方差），欺骗意图被有效捕获。针对特定欺骗类型的专门探针在评估数据集上表现更好。

Conclusion: 由于不同数据集中欺骗类型的异质性，组织应针对其特定威胁模型设计专门探针，而不是寻求通用欺骗检测器。

Abstract: Linear probes are a promising approach for monitoring AI systems for deceptive behaviour. Previous work has shown that a linear classifier trained on a contrastive instruction pair and a simple dataset can achieve good performance. However, these probes exhibit notable failures even in straightforward scenarios, including spurious correlations and false positives on non-deceptive responses. In this paper, we identify the importance of the instruction pair used during training. Furthermore, we show that targeting specific deceptive behaviors through a human-interpretable taxonomy of deception leads to improved results on evaluation datasets. Our findings reveal that instruction pairs capture deceptive intent rather than content-specific patterns, explaining why prompt choice dominates probe performance (70.6% of variance). Given the heterogeneity of deception types across datasets, we conclude that organizations should design specialized probes targeting their specific threat models rather than seeking a universal deception detector.

</details>


### [619] [Agyn: A Multi-Agent System for Team-Based Autonomous Software Engineering](https://arxiv.org/abs/2602.01465)
*Nikita Benkovich,Vitalii Valkov*

Main category: cs.AI

Relevance: 75.0

TL;DR: 多智能体系统模拟软件工程团队组织结构，在SWE-bench上达到72.4%解决率，超越单智能体基线


<details>
  <summary>Details</summary>
Motivation: 当前自主系统将问题解决视为单一或流水线过程，而真实软件开发是团队协作活动，有明确的角色分工、沟通和评审。需要模拟软件工程作为组织过程的多智能体系统。

Method: 基于开源平台agyn构建完全自动化的多智能体系统，分配专门角色（协调、研究、实现、评审），提供隔离沙箱，支持结构化通信，遵循定义好的开发方法学（分析、任务规范、PR创建、迭代评审）。

Result: 在SWE-bench 500上后验评估中解决72.4%的任务，优于使用可比语言模型的单智能体基线。系统设计用于实际生产使用，未针对SWE-bench调优。

Conclusion: 复制团队结构、方法学和沟通是自主软件工程的有力范式，未来进展可能同样依赖于组织设计和智能体基础设施，而不仅仅是模型改进。

Abstract: Large language models have demonstrated strong capabilities in individual software engineering tasks, yet most autonomous systems still treat issue resolution as a monolithic or pipeline-based process. In contrast, real-world software development is organized as a collaborative activity carried out by teams following shared methodologies, with clear role separation, communication, and review. In this work, we present a fully automated multi-agent system that explicitly models software engineering as an organizational process, replicating the structure of an engineering team. Built on top of agyn, an open-source platform for configuring agent teams, our system assigns specialized agents to roles such as coordination, research, implementation, and review, provides them with isolated sandboxes for experimentation, and enables structured communication. The system follows a defined development methodology for working on issues, including analysis, task specification, pull request creation, and iterative review, and operates without any human intervention. Importantly, the system was designed for real production use and was not tuned for SWE-bench. When evaluated post hoc on SWE-bench 500, it resolves 72.4% of tasks, outperforming single-agent baselines using comparable language models. Our results suggest that replicating team structure, methodology, and communication is a powerful paradigm for autonomous software engineering, and that future progress may depend as much on organizational design and agent infrastructure as on model improvements.

</details>


### [620] [S1-NexusAgent: a Self-Evolving Agent Framework for Multidisciplinary Scientific Research](https://arxiv.org/abs/2602.01550)
*S1-NexusAgent Team*

Main category: cs.AI

Relevance: 75.0

TL;DR: S1-NexusAgent是一个自进化科学智能体框架，采用分层规划-代码执行范式，通过双循环架构解耦全局科学规划和子任务工具执行，支持跨学科科学工具集成和动态检索，并引入稀疏上下文管理和科学技能蒸馏机制，在多个科学基准测试中达到SOTA性能。


<details>
  <summary>Details</summary>
Motivation: 现有LLM和基于工具的智能体在处理大规模数据、复杂工作流和专门工具时存在局限性，特别是在长时程规划、鲁棒目标维持和持续学习方面。科学研究的复杂性需要更强大的智能体框架来支持多学科研究。

Method: 1. 分层规划-代码执行范式：双循环架构解耦全局规划和子任务执行
2. 原生支持MCP协议，集成数千个跨学科科学工具
3. 意图感知的动态工具检索和热插拔机制
4. 基于对象引用的稀疏上下文管理，实现子任务上下文隔离和中间结果压缩
5. Critic Agent自动评估执行轨迹，将高质量研究路径提炼为可重用的科学技能

Result: 在权威科学基准测试中（biomini-eval、ChemBench、MatSciBench）达到最先进性能，验证了其在复杂科学任务中的有效性和泛化能力，特别是在长时程规划和复杂专业工具编排方面。

Conclusion: S1-NexusAgent通过自进化框架有效解决了科学研究中LLM智能体的局限性，为可持续、长时程的科学研究提供了有价值的解决方案，展示了在多学科科学任务中的强大能力。

Abstract: Modern scientific research relies on large-scale data, complex workflows, and specialized tools, which existing LLMs and tool-based agents struggle to handle due to limitations in long-horizon planning, robust goal maintenance, and continual learning from execution. To address these issues, in this work, we propose S1-NexusAgent, a self-evolving agent framework designed for multidisciplinary scientific research. S1-NexusAgent adopts a hierarchical Plan-and-CodeAct execution paradigm, decoupling global scientific planning from subtask-level tool execution through a dual-loop architecture, thereby enabling stable modeling of complex research workflows. The system natively supports the Model Context Protocol (MCP), integrates up to thousands of cross-disciplinary scientific tools, and achieves efficient orchestration of heterogeneous research tools via intention-aware dynamic tool retrieval and hot-plug mechanisms. To address long-context and large-scale data challenges in scientific settings, S1-NexusAgent introduces object-reference-based sparse context management, which enables sub-task context isolation and intermediate result compression. Building on this, a Critic Agent automatically evaluates complete execution trajectories and distills high-quality research paths into reusable Scientific Skills, forming a closed loop for continuous self-evolution, which is valuable for sustainable and long-horizon scientific research. Experiments on authoritative scientific benchmarks involving long-horizon planning and complex specialized tool orchestration, including biomini-eval (biology), ChemBench (chemistry), and MatSciBench (material science), demonstrate that S1-NexusAgent achieves state-of-the-art performance, validating its effectiveness and generalization capability in complex scientific tasks.

</details>


### [621] [FlowSteer: Interactive Agentic Workflow Orchestration via End-to-End Reinforcement Learning](https://arxiv.org/abs/2602.01664)
*Mingda Zhang,Haoran Luo,Tiesunlong Shen,Qika Lin,Xiaoying Tang,Rui Mao,Erik Cambria*

Main category: cs.AI

Relevance: 75.0

TL;DR: FlowSteer：基于强化学习的端到端工作流编排框架，通过轻量级策略模型与可执行画布环境的交互，自动化工作流编排，支持多样化算子库和可互换LLM后端。


<details>
  <summary>Details</summary>
Motivation: 现有工作流编排面临高人工成本、依赖特定算子/LLM、稀疏奖励信号等挑战，需要自动化、灵活且高效的工作流编排解决方案。

Method: 提出FlowSteer框架：1) 轻量级策略模型作为智能体，分析执行状态并选择编辑动作；2) 可执行画布环境执行算子并返回反馈；3) 支持插件化算子库和可互换LLM后端；4) 提出Canvas Workflow Relative Policy Optimization (CWRPO)训练方法，引入多样性约束奖励和条件释放机制。

Result: 在12个数据集上的实验结果表明，FlowSteer在各种任务上显著优于基线方法。

Conclusion: FlowSteer通过强化学习框架有效解决了工作流编排的自动化挑战，提供了灵活、高效的解决方案，在多个任务上表现出优越性能。

Abstract: In recent years, a variety of powerful agentic workflows have been applied to solve a wide range of human problems. However, existing workflow orchestration still faces key challenges, including high manual cost, reliance on specific operators/large language models (LLMs), and sparse reward signals. To address these challenges, we propose FlowSteer, an end-to-end reinforcement learning framework that takes a lightweight policy model as the agent and an executable canvas environment, automating workflow orchestration through multi-turn interaction. In this process, the policy model analyzes execution states and selects editing actions, while the canvas executes operators and returns feedback for iterative refinement. Moreover, FlowSteer provides a plug-and-play framework that supports diverse operator libraries and interchangeable LLM backends. To effectively train this interaction paradigm, we propose Canvas Workflow Relative Policy Optimization (CWRPO), which introduces diversity-constrained rewards with conditional release to stabilize learning and suppress shortcut behaviors. Experimental results on twelve datasets show that FlowSteer significantly outperforms baselines across various tasks.

</details>


### [622] [MACD: Model-Aware Contrastive Decoding via Counterfactual Data](https://arxiv.org/abs/2602.01740)
*Qixin Xiao,Kun Zhou*

Main category: cs.AI

Relevance: 75.0

TL;DR: MACD是一种针对视频语言模型幻觉问题的新型推理策略，通过模型感知的反事实数据构建与对比解码相结合，在对象级别生成针对性反事实输入，有效减少幻觉同时保持或提升任务准确性。


<details>
  <summary>Details</summary>
Motivation: 视频语言模型容易产生幻觉，特别是在视觉证据弱、模糊或有偏的情况下。现有解码方法（如对比解码）依赖随机扰动构建对比数据，难以控制驱动幻觉的视觉线索或与模型弱点良好对齐。

Method: 提出模型感知的反事实数据对比解码（MACD），利用Video-LLM自身的反馈识别导致幻觉的关键对象区域，在对象级别而非任意帧或时间修改上生成针对性反事实输入，然后将这些模型感知的反事实数据集成到对比解码中，在解码过程中强制证据基础的标记选择。

Result: 在EventHallusion、MVBench、Perception-test和Video-MME等基准测试中，MACD能持续减少幻觉，同时在Qwen和InternVL等多样化Video-LLMs上保持或提升任务准确性。该方法在处理小物体、遮挡物体或共现物体等挑战性场景中尤其有效。

Conclusion: MACD通过模型引导的反事实构建与解码相结合，提供了一种更有效控制视频语言模型幻觉的方法，特别适用于视觉证据不足的复杂场景。

Abstract: Video language models (Video-LLMs) are prone to hallucinations, often generating plausible but ungrounded content when visual evidence is weak, ambiguous, or biased. Existing decoding methods, such as contrastive decoding (CD), rely on random perturbations to construct contrastive data for mitigating hallucination patterns. However, such a way is hard to control the visual cues that drive hallucination or well align with model weaknesses. We propose Model-aware Counterfactual Data based Contrastive Decoding (MACD), a new inference strategy that combines model-guided counterfactual construction with decoding. Our approach uses the Video-LLM's own feedback to identify object regions most responsible for hallucination, generating targeted counterfactual inputs at the object level rather than arbitrary frame or temporal modifications. These model-aware counterfactual data is then integrated into CD to enforce evidence-grounded token selection during decoding. Experiments on EventHallusion, MVBench, Perception-test and Video-MME show that MACD consistently reduces hallucination while maintaining or improving task accuracy across diverse Video-LLMs, including Qwen and InternVL families. The method is especially effective in challenging scenarios involving small, occluded, or co-occurring objects. Our code and data will be publicly released.

</details>


### [623] [Controlling Exploration-Exploitation in GFlowNets via Markov Chain Perspectives](https://arxiv.org/abs/2602.01749)
*Lin Chen,Samuel Drapeau,Fanghao Shao,Xuekai Zhu,Bo Xue,Yunchong Song,Mathieu Laurière,Zhouhan Lin*

Main category: cs.AI

Relevance: 75.0

TL;DR: 该论文提出α-GFNs，通过可调参数α控制GFlowNet中前向和后向策略的混合比例，从而增强探索-利用平衡和模式发现能力，在多个基准测试中表现优于传统GFlowNet目标。


<details>
  <summary>Details</summary>
Motivation: 传统GFlowNet目标隐含地固定了前向和后向策略的等比例混合，这可能限制了训练过程中的探索-利用权衡。作者希望打破这种约束，提供更灵活的探索机制来增强模式发现能力。

Method: 通过建立GFlowNet目标与马尔可夫链可逆性之间的等价关系，揭示了传统约束的来源。基于此理论发现，提出α-GFNs框架，引入可调参数α来泛化前向和后向策略的混合比例，从而直接控制探索-利用动态。

Result: 在Set、Bit Sequence和Molecule Generation等多个基准测试中，α-GFN目标始终优于先前的GFlowNet目标，实现了高达10倍的发现模式数量增加。

Conclusion: α-GFNs通过可调混合参数提供了对探索-利用动态的直接控制，增强了模式发现能力，同时确保收敛到唯一流，为GFlowNet框架提供了更灵活和强大的理论基础。

Abstract: Generative Flow Network (GFlowNet) objectives implicitly fix an equal mixing of forward and backward policies, potentially constraining the exploration-exploitation trade-off during training. By further exploring the link between GFlowNets and Markov chains, we establish an equivalence between GFlowNet objectives and Markov chain reversibility, thereby revealing the origin of such constraints, and provide a framework for adapting Markov chain properties to GFlowNets. Building on these theoretical findings, we propose $α$-GFNs, which generalize the mixing via a tunable parameter $α$. This generalization enables direct control over exploration-exploitation dynamics to enhance mode discovery capabilities, while ensuring convergence to unique flows. Across various benchmarks, including Set, Bit Sequence, and Molecule Generation, $α$-GFN objectives consistently outperform previous GFlowNet objectives, achieving up to a $10 \times$ increase in the number of discovered modes.

</details>


### [624] [Constrained Process Maps for Multi-Agent Generative AI Workflows](https://arxiv.org/abs/2602.02034)
*Ananya Joshi,Michael Rudow*

Main category: cs.AI

Relevance: 75.0

TL;DR: 本文提出了一种基于有限时域马尔可夫决策过程的多智能体系统，用于处理LLM智能体在合规等监管环境中的复杂多步工作流，通过量化不确定性并引入人工审核状态来提升系统性能。


<details>
  <summary>Details</summary>
Motivation: 当前基于LLM的智能体越来越多地用于合规和尽职调查等监管环境中的复杂多步工作流，但现有架构主要依赖单个智能体的提示工程，难以观察或比较模型如何处理跨决策阶段的不确定性和协调问题，以及与人工监督的交互。

Method: 将多智能体系统形式化为具有有向无环结构的有限时域马尔可夫决策过程，每个智能体对应特定角色或决策阶段，通过预定义转换表示任务升级或完成。使用蒙特卡洛估计在智能体层面量化认知不确定性，系统级不确定性通过MDP在自动标记状态或人工审核状态终止来捕获。

Result: 在AI安全评估（自残检测）的案例研究中，相比单智能体基线，实现了高达19%的准确率提升，高达85倍的人工审核需求减少，在某些配置下还减少了处理时间。

Conclusion: 提出的多智能体MDP框架能够有效处理LLM智能体在监管环境中的不确定性和协调问题，通过系统化地整合人工监督，显著提升了性能指标。

Abstract: Large language model (LLM)-based agents are increasingly used to perform complex, multi-step workflows in regulated settings such as compliance and due diligence. However, many agentic architectures rely primarily on prompt engineering of a single agent, making it difficult to observe or compare how models handle uncertainty and coordination across interconnected decision stages and with human oversight. We introduce a multi-agent system formalized as a finite-horizon Markov Decision Process (MDP) with a directed acyclic structure. Each agent corresponds to a specific role or decision stage (e.g., content, business, or legal review in a compliance workflow), with predefined transitions representing task escalation or completion. Epistemic uncertainty is quantified at the agent level using Monte Carlo estimation, while system-level uncertainty is captured by the MDP's termination in either an automated labeled state or a human-review state. We illustrate the approach through a case study in AI safety evaluation for self-harm detection, implemented as a multi-agent compliance system. Results demonstrate improvements over a single-agent baseline, including up to a 19\% increase in accuracy, up to an 85x reduction in required human review, and, in some configurations, reduced processing time.

</details>


### [625] [Thinking with Comics: Enhancing Multimodal Reasoning through Structured Visual Storytelling](https://arxiv.org/abs/2602.02453)
*Andong Chen,Wenxin Zhu,Qiuyu Ding,Yuchen Song,Muyun Yang,Tiejun Zhao*

Main category: cs.AI

Relevance: 75.0

TL;DR: 提出"Thinking with Comics"视觉推理范式，使用漫画作为图像和视频之间的高信息密度媒介，在保持时间结构和叙事连贯性的同时降低推理成本


<details>
  <summary>Details</summary>
Motivation: 现有模态推理存在局限：静态图像难以表示时间结构，而视频引入大量冗余和计算成本。需要一种既能保留时间信息又高效的中间视觉表示

Method: 提出基于漫画的视觉推理范式，系统研究两种推理路径，在多种推理任务和长上下文理解任务上进行评估

Result: 在时间性和因果推理任务上优于图像推理，同时比视频推理更高效。不同漫画叙事结构和风格对任务性能有持续影响

Conclusion: 漫画作为中间视觉表示能有效改善多模态推理，平衡信息密度和计算效率

Abstract: Chain-of-Thought reasoning has driven large language models to extend from thinking with text to thinking with images and videos. However, different modalities still have clear limitations: static images struggle to represent temporal structure, while videos introduce substantial redundancy and computational cost. In this work, we propose Thinking with Comics, a visual reasoning paradigm that uses comics as a high information-density medium positioned between images and videos. Comics preserve temporal structure, embedded text, and narrative coherence while requiring significantly lower reasoning cost. We systematically study two reasoning paths based on comics and evaluate them on a range of reasoning tasks and long-context understanding tasks. Experimental results show that Thinking with Comics outperforms Thinking with Images on multi-step temporal and causal reasoning tasks, while remaining substantially more efficient than Thinking with Video. Further analysis indicates that different comic narrative structures and styles consistently affect performance across tasks, suggesting that comics serve as an effective intermediate visual representation for improving multimodal reasoning.

</details>


### [626] [Avenir-Web: Human-Experience-Imitating Multimodal Web Agents with Mixture of Grounding Experts](https://arxiv.org/abs/2602.02468)
*Aiden Yiliu Li,Xinyue Hao,Shilong Liu,Mengdi Wang*

Main category: cs.AI

Relevance: 75.0

TL;DR: Avenir-Web是一个先进的网页代理系统，通过混合定位专家、经验模仿规划和任务跟踪检查表等技术，在真实网页交互任务中达到开源最优性能。


<details>
  <summary>Details</summary>
Motivation: 尽管多模态大语言模型有所进展，但自主网页代理在复杂动态网页界面上执行长时程任务时仍面临挑战，包括元素定位不准确、缺乏站点特定程序知识、以及长时任务跟踪和记忆不稳定等问题。

Method: 1. 混合定位专家：提高元素定位准确性
2. 经验模仿规划：整合程序先验知识
3. 任务跟踪检查表+自适应记忆：实现鲁棒交互
4. 在Online-Mind2Web基准上进行评估

Result: Avenir-Web在Online-Mind2Web基准上显著超越先前开源代理，达到与顶级专有模型相当的性能，建立了开源网页代理的新最优水平。

Conclusion: Avenir-Web通过创新的架构设计解决了网页代理的关键挑战，为可靠网页交互提供了开源解决方案，在真实部署中表现出色。

Abstract: Despite advances in multimodal large language models, autonomous web agents still struggle to reliably execute long-horizon tasks on complex and dynamic web interfaces. Existing agents often suffer from inaccurate element grounding, the absence of site-specific procedural knowledge, and unstable long-term task tracking and memory, particularly when operating over complex Document Object Model structures. To address these limitations, we introduce Avenir-Web, a web agent that achieves a new open-source state of the art on the Online-Mind2Web benchmark in real-world deployment. Avenir-Web leverages a Mixture of Grounding Experts, Experience-Imitation Planning for incorporating procedural priors, and a task-tracking checklist combined with adaptive memory to enable robust and seamless interaction across diverse user interface paradigms. We evaluate Avenir-Web on Online-Mind2Web, a rigorous benchmark of live and user-centered web tasks. Our results demonstrate that Avenir-Web significantly surpasses prior open-source agents and attains performance parity with top-tier proprietary models, thereby establishing a new open-source state of the art for reliable web agents on live websites.

</details>


### [627] [Chained Prompting for Better Systematic Review Search Strategies](https://arxiv.org/abs/2602.00011)
*Fatima Nasser,Fouad Trad,Ammar Mohanna,Ghada El-Hajj Fuleihan,Ali Chehab*

Main category: cs.IR

Relevance: 75.0

TL;DR: 提出基于LLM的链式提示工程框架，用于系统综述中搜索策略的自动化开发，显著提高召回率至0.9


<details>
  <summary>Details</summary>
Motivation: 传统系统综述搜索策略设计方法资源密集且易受主观性影响，而启发式和自动化方法在召回率方面表现不佳，需要大量专家输入。需要更高效、可扩展的自动化解决方案。

Method: 基于LLM的链式提示工程框架，模拟人工搜索设计流程：分解综述目标、提取和形式化PICO元素、生成概念表示、扩展术语、合成布尔查询。框架不仅构建查询，还能生成结构良好的PICO元素。

Result: 在LEADSInstruct数据集子集上评估，框架达到0.9的平均召回率，显著优于现有方法。错误分析显示精确目标规范和术语对齐对检索效果至关重要。

Conclusion: LLM-based pipelines能够产生透明、可重复且高性能的搜索策略，具有作为支持证据合成和循证实践的可扩展工具的潜力。

Abstract: Systematic reviews require the use of rigorously designed search strategies to ensure both comprehensive retrieval and minimization of bias. Conventional manual approaches, although methodologically systematic, are resource-intensive and susceptible to subjectivity, whereas heuristic and automated techniques frequently under-perform in recall unless supplemented by extensive expert input. We introduce a Large Language Model (LLM)-based chained prompt engineering framework for the automated development of search strategies in systematic reviews. The framework replicates the procedural structure of manual search design while leveraging LLMs to decompose review objectives, extract and formalize PICO elements, generate conceptual representations, expand terminologies, and synthesize Boolean queries. In addition to query construction, the framework exhibits superior performance in generating well-structured PICO elements relative to existing methods, thereby strengthening the foundation for high-recall search strategies. Evaluation on a subset of the LEADSInstruct dataset demonstrates that the framework attains a 0.9 average recall. These results significantly exceed the performance of existing approaches. Error analysis further highlights the critical role of precise objective specification and terminological alignment in optimizing retrieval effectiveness. These findings confirm the capacity of LLM-based pipelines to yield transparent, reproducible, and high-performing search strategies, and highlight their potential as scalable instruments for supporting evidence synthesis and evidence-based practice.

</details>


### [628] [Happy Young Women, Grumpy Old Men? Emotion-Driven Demographic Biases in Synthetic Face Generation](https://arxiv.org/abs/2602.00032)
*Mengting Wei,Aditya Gulati,Guoying Zhao,Nuria Oliver*

Main category: cs.CY

Relevance: 75.0

TL;DR: 本文系统审计了8个最先进的文本到图像模型（4个西方开发，4个中国开发）在合成人脸生成中的偏见问题，发现所有模型都存在持续的人口统计和情感条件偏见，无论其来源国家。


<details>
  <summary>Details</summary>
Motivation: 尽管文本到图像和多模态大语言模型在合成人脸生成方面取得了快速进展，但这些模型的偏见、表征质量和跨文化一致性仍然知之甚少。先前研究主要关注人口统计偏见，但缺乏关于情感提示如何影响人口统计表征以及不同文化和语言背景下训练的模型输出分布差异的研究。

Method: 对8个最先进的T2I模型进行系统审计（4个西方模型，4个中国模型），使用相同提示生成人脸。采用最先进的面部分析算法估计生成人脸的性别、种族、年龄和吸引力水平。应用信息论偏见度量（包括Kullback-Leibler和Jensen-Shannon散度）来衡量与全球人口统计数据的偏差。

Result: 研究发现所有模型都存在持续的人口统计和情感条件偏见，无论其来源国家。西方和中国开发的模型都显示出系统性偏见，表明这是一个跨文化普遍存在的问题。

Conclusion: 研究结果对公平性、社会技术危害、治理和透明生成系统的开发具有重要意义。需要更全面的偏见评估框架和跨文化一致的模型开发方法。

Abstract: Synthetic face generation has rapidly advanced with the emergence of text-to-image (T2I) and of multimodal large language models, enabling high-fidelity image production from natural-language prompts. Despite the widespread adoption of these tools, the biases, representational quality, and cross-cultural consistency of these models remain poorly understood. Prior research on biases in the synthetic generation of human faces has examined demographic biases, yet there is little research on how emotional prompts influence demographic representation and how models trained in different cultural and linguistic contexts vary in their output distributions. We present a systematic audit of eight state-of-the-art T2I models comprising four models developed by Western organizations and four developed by Chinese institutions, all prompted identically. Using state-of-the-art facial analysis algorithms, we estimate the gender, race, age, and attractiveness levels in the generated faces. To measure the deviations from global population statistics, we apply information-theoretic bias metrics including Kullback-Leibler and Jensen-Shannon divergences. Our findings reveal persistent demographic and emotion-conditioned biases in all models regardless of their country of origin. We discuss implications for fairness, socio-technical harms, governance, and the development of transparent generative systems.

</details>


### [629] [Design and Empirical Study of a Large Language Model-Based Multi-Agent Investment System for Chinese Public REITs](https://arxiv.org/abs/2602.00082)
*Zheng Li*

Main category: q-fin.ST

Relevance: 75.0

TL;DR: 提出基于多智能体协作的LLM驱动交易框架，用于中国低波动性公募REITs市场，通过四种分析智能体从不同维度分析，预测智能体整合信号输出多时间维度概率分布，决策智能体生成仓位调整信号，形成分析-预测-决策-执行闭环。


<details>
  <summary>Details</summary>
Motivation: 针对中国低波动性公募REITs市场，传统量化策略效果有限，需要更智能的分析框架。LLM具有强大的文本理解和推理能力，但直接应用于金融交易面临挑战，需要设计专门的多智能体协作框架来整合多源信息并控制风险。

Method: 构建四类分析智能体（公告、事件、价格动量、市场），分别从不同维度分析；预测智能体整合多源信号输出多时间维度方向概率分布；决策智能体基于预测结果和风险约束生成离散仓位调整信号。比较两种预测模型路径：直接调用通用大模型DeepSeek-R1 vs 使用经过监督微调和强化学习对齐的专用小模型Qwen3-8B。

Result: 在2024年10月至2025年10月的回测中，两种基于智能体的策略在累计收益、夏普比率和最大回撤方面均显著优于买入持有基准。微调的小模型在某些场景下表现接近甚至优于通用大模型。

Conclusion: 多智能体框架能有效提升REITs交易的风险调整后收益，微调的小模型在特定场景下可达到或超越通用大模型性能，为LLM在金融交易中的应用提供了可行路径。

Abstract: This study addresses the low-volatility Chinese Public Real Estate Investment Trusts (REITs) market, proposing a large language model (LLM)-driven trading framework based on multi-agent collaboration. The system constructs four types of analytical agents-announcement, event, price momentum, and market-each conducting analysis from different dimensions; then the prediction agent integrates these multi-source signals to output directional probability distributions across multiple time horizons, then the decision agent generates discrete position adjustment signals based on the prediction results and risk control constraints, thereby forming a closed loop of analysis-prediction-decision-execution. This study further compares two prediction model pathways: for the prediction agent, directly calling the general-purpose large model DeepSeek-R1 versus using a specialized small model Qwen3-8B fine-tuned via supervised fine-tuning and reinforcement learning alignment. In the backtest from October 2024 to October 2025, both agent-based strategies significantly outperformed the buy-and-hold benchmark in terms of cumulative return, Sharpe ratio, and maximum drawdown. The results indicate that the multi-agent framework can effectively enhance the risk-adjusted return of REITs trading, and the fine-tuned small model performs close to or even better than the general-purpose large model in some scenarios.

</details>


### [630] [Semantic-Aware Advanced Persistent Threat Detection Using Autoencoders on LLM-Encoded System Logs](https://arxiv.org/abs/2602.00204)
*Waleed Khan Mohammed,Zahirul Arief Irfan Bin Shahrul Anuar,Mousa Sufian Mousa Mitani,Hezerul Abdul Karim,Nouar AlDahoul*

Main category: cs.CR

Relevance: 75.0

TL;DR: 提出一种基于大语言模型语义嵌入的APT检测方法，通过预训练Transformer将系统日志转换为语义表示，再用自编码器检测异常，在DARPA TC数据集上优于传统无监督方法。


<details>
  <summary>Details</summary>
Motivation: 高级持续性威胁(APTs)具有"低而慢"的特点，传统统计方法和浅层机器学习难以检测。现有溯源图分析方法无法捕捉系统活动的语义意图，需要利用大语言模型的语义理解能力来提升APT检测效果。

Method: 1) 使用预训练Transformer模型将原始系统日志转换为高维语义嵌入；2) 采用自编码器分析这些嵌入，识别异常和潜在恶意模式；3) 在DARPA透明计算数据集上评估，包含红队在真实环境中生成的APT攻击场景。

Result: 基于LLM嵌入的自编码器在AUC-ROC指标上显著优于广泛使用的无监督基线方法（隔离森林、一类支持向量机、主成分分析），在复杂威胁场景中表现优异。

Conclusion: 语义理解对于检测传统方法常遗漏的非线性和隐蔽攻击行为至关重要，LLM生成的语义嵌入能有效提升APT检测性能，为网络安全领域提供了新的研究方向。

Abstract: Advanced Persistent Threats (APTs) are among the most challenging cyberattacks to detect. They are carried out by highly skilled attackers who carefully study their targets and operate in a stealthy, long-term manner. Because APTs exhibit "low-and-slow" behavior, traditional statistical methods and shallow machine learning techniques often fail to detect them. Previous research on APT detection has explored machine learning approaches and provenance graph analysis. However, provenance-based methods often fail to capture the semantic intent behind system activities. This paper proposes a novel anomaly detection approach that leverages semantic embeddings generated by Large Language Models (LLMs). The method enhances APT detection by extracting meaningful semantic representations from unstructured system log data. First, raw system logs are transformed into high-dimensional semantic embeddings using a pre-trained transformer model. These embeddings are then analyzed using an Autoencoder (AE) to identify anomalous and potentially malicious patterns. The proposed method is evaluated using the DARPA Transparent Computing (TC) dataset, which contains realistic APT attack scenarios generated by red teams in live environments. Experimental results show that the AE trained on LLM-derived embeddings outperforms widely used unsupervised baseline methods, including Isolation Forest (IForest), One-Class Support Vector Machine (OC-SVM), and Principal Component Analysis (PCA). Performance is measured using the Area Under the Receiver Operating Characteristic Curve (AUC-ROC), where the proposed approach consistently achieves superior results, even in complex threat scenarios. These findings highlight the importance of semantic understanding in detecting non-linear and stealthy attack behaviors that are often missed by conventional detection techniques.

</details>


### [631] [Action-Free Offline-to-Online RL via Discretised State Policies](https://arxiv.org/abs/2602.00629)
*Natinael Solomon Neggatu,Jeremie Houssineau,Giovanni Montana*

Main category: stat.ML

Relevance: 75.0

TL;DR: 论文提出了一种针对动作缺失离线数据的强化学习方法，通过状态离散化转换和状态策略学习，实现从仅含状态转移的离线数据到在线学习的有效迁移。


<details>
  <summary>Details</summary>
Motivation: 现有离线RL方法通常假设数据集包含动作标签，但实际应用中由于隐私、存储或传感器限制，动作信息可能缺失。需要解决仅含(s,r,s')三元组的动作缺失离线数据到在线RL的迁移问题。

Method: 提出Offline State-Only DecQN算法：1) 状态离散化转换处理高维状态空间；2) 学习状态策略（推荐期望的下一个状态转移而非动作）；3) 利用预训练状态策略指导在线学习加速收敛。

Result: 在多个基准测试中，该方法显著提升了收敛速度和渐近性能。分析表明状态离散化和正则化对方法有效性至关重要。

Conclusion: 建立了一个可扩展的实用框架，能够有效利用动作缺失的离线数据集来加速在线强化学习，解决了实际应用中动作信息缺失的挑战。

Abstract: Most existing offline RL methods presume the availability of action labels within the dataset, but in many practical scenarios, actions may be missing due to privacy, storage, or sensor limitations. We formalise the setting of action-free offline-to-online RL, where agents must learn from datasets consisting solely of $(s,r,s')$ tuples and later leverage this knowledge during online interaction. To address this challenge, we propose learning state policies that recommend desirable next-state transitions rather than actions. Our contributions are twofold. First, we introduce a simple yet novel state discretisation transformation and propose Offline State-Only DecQN (\algo), a value-based algorithm designed to pre-train state policies from action-free data. \algo{} integrates the transformation to scale efficiently to high-dimensional problems while avoiding instability and overfitting associated with continuous state prediction. Second, we propose a novel mechanism for guided online learning that leverages these pre-trained state policies to accelerate the learning of online agents. Together, these components establish a scalable and practical framework for leveraging action-free datasets to accelerate online RL. Empirical results across diverse benchmarks demonstrate that our approach improves convergence speed and asymptotic performance, while analyses reveal that discretisation and regularisation are critical to its effectiveness.

</details>


### [632] [SA-VLA: Spatially-Aware Flow-Matching for Vision-Language-Action Reinforcement Learning](https://arxiv.org/abs/2602.00743)
*Xu Pan,Zhenglin Wan,Xingrui Yu,Xianwei Zheng,Youkai Ke,Ming Sun,Rui Wang,Ziwei Wang,Ivor Tsang*

Main category: cs.RO

Relevance: 75.0

TL;DR: SA-VLA是一个空间感知的强化学习适应框架，用于视觉-语言-动作模型，通过保持空间归纳偏置来提高机器人操作任务中的鲁棒性和泛化能力。


<details>
  <summary>Details</summary>
Motivation: 视觉-语言-动作模型在机器人操作中表现出良好的泛化能力，但强化学习微调常常会降低空间分布变化下的鲁棒性。对于流匹配VLA策略，这种退化与RL适应过程中空间归纳偏置的侵蚀密切相关，因为稀疏奖励和空间不可知的探索越来越偏向短期视觉线索。

Method: SA-VLA框架通过三个关键组件保持空间基础：1）将隐式空间表示与视觉标记融合；2）提供反映几何进展的密集奖励；3）采用SCAN（空间条件退火探索策略），这是一种针对流匹配动态定制的探索方法。

Result: 在具有挑战性的多物体和杂乱操作基准测试中，SA-VLA实现了稳定的RL微调，并改善了零样本空间泛化，产生了更鲁棒和可转移的行为。

Conclusion: SA-VLA通过将表示学习、奖励设计和探索与任务几何对齐，有效解决了VLA模型在RL微调中的空间鲁棒性问题，为机器人操作提供了更可靠的适应框架。

Abstract: Vision-Language-Action (VLA) models exhibit strong generalization in robotic manipulation, yet reinforcement learning (RL) fine-tuning often degrades robustness under spatial distribution shifts. For flow-matching VLA policies, this degradation is closely associated with the erosion of spatial inductive bias during RL adaptation, as sparse rewards and spatially agnostic exploration increasingly favor short-horizon visual cues. To address this issue, we propose \textbf{SA-VLA}, a spatially-aware RL adaptation framework that preserves spatial grounding during policy optimization by aligning representation learning, reward design, and exploration with task geometry. SA-VLA fuses implicit spatial representations with visual tokens, provides dense rewards that reflect geometric progress, and employs \textbf{SCAN}, a spatially-conditioned annealed exploration strategy tailored to flow-matching dynamics. Across challenging multi-object and cluttered manipulation benchmarks, SA-VLA enables stable RL fine-tuning and improves zero-shot spatial generalization, yielding more robust and transferable behaviors. Code and project page are available at https://xupan.top/Projects/savla.

</details>


### [633] [TxRay: Agentic Postmortem of Live Blockchain Attacks](https://arxiv.org/abs/2602.01317)
*Ziyue Wang,Jiangshan Yu,Kaihua Qin,Dawn Song,Arthur Gervais,Liyi Zhou*

Main category: cs.CR

Relevance: 75.0

TL;DR: TxRay是一个基于LLM的DeFi攻击事后分析系统，能够从有限的交易证据自动重构攻击生命周期、识别根本原因，并生成可执行的PoC验证代码。


<details>
  <summary>Details</summary>
Motivation: DeFi生态系统因代码漏洞在过去五年损失超过157.5亿美元，现有的事后分析过程缓慢且手动，需要从有限的交易哈希证据中重构复杂的攻击生命周期。

Method: 开发了TxRay LLM代理系统，使用工具调用从种子交易重构ACT攻击，包括：1）恢复攻击生命周期；2）推导证据支持的根因；3）生成可运行的独立PoC；4）通过可执行断言进行自我验证。

Result: 在DeFiHackLabs的114个事件中，TxRay为105个事件生成专家一致的根因和可执行PoC，端到端复现率达92.11%。PoC评估显示98.1%避免硬编码攻击者地址，比基准提升24.8个百分点。

Conclusion: TxRay证明了LLM代理在DeFi安全分析中的有效性，能够自动化生成高质量的攻击复现PoC，显著提升安全分析效率和准确性。

Abstract: Decentralized Finance (DeFi) has turned blockchains into financial infrastructure, allowing anyone to trade, lend, and build protocols without intermediaries, but this openness exposes pools of value controlled by code. Within five years, the DeFi ecosystem has lost over 15.75B USD to reported exploits. Many exploits arise from permissionless opportunities that any participant can trigger using only public state and standard interfaces, which we call Anyone-Can-Take (ACT) opportunities. Despite on-chain transparency, postmortem analysis remains slow and manual: investigations start from limited evidence, sometimes only a single transaction hash, and must reconstruct the exploit lifecycle by recovering related transactions, contract code, and state dependencies.
  We present TxRay, a Large Language Model (LLM) agentic postmortem system that uses tool calls to reconstruct live ACT attacks from limited evidence. Starting from one or more seed transactions, TxRay recovers the exploit lifecycle, derives an evidence-backed root cause, and generates a runnable, self-contained Proof of Concept (PoC) that deterministically reproduces the incident. TxRay self-checks postmortems by encoding incident-specific semantic oracles as executable assertions.
  To evaluate PoC correctness and quality, we develop PoCEvaluator, an independent agentic execution-and-review evaluator. On 114 incidents from DeFiHackLabs, TxRay produces an expert-aligned root cause and an executable PoC for 105 incidents, achieving 92.11% end-to-end reproduction. Under PoCEvaluator, 98.1% of TxRay PoCs avoid hard-coding attacker addresses, a +24.8pp lift over DeFiHackLabs. In a live deployment, TxRay delivers validated root causes in 40 minutes and PoCs in 59 minutes at median latency. TxRay's oracle-validated PoCs enable attack imitation, improving coverage by 15.6% and 65.5% over STING and APE.

</details>


### [634] [Meta Engine: A Unified Semantic Query Engine on Heterogeneous LLM-Based Query Systems](https://arxiv.org/abs/2602.01701)
*Ruyu Li,Tinghui Zhang,Haodi Ma,Daisy Zhe Wang,Yifan Wang*

Main category: cs.DB

Relevance: 75.0

TL;DR: Meta Engine是一个统一的语义查询引擎，通过集成异构的、专门的LLM查询系统来解决多模态语义查询中的碎片化问题，在评估中比基线方法性能提升3-6倍。


<details>
  <summary>Details</summary>
Motivation: 随着多模态数据的广泛应用，语义查询需求日益增长。当前LLM-based语义查询系统存在碎片化问题：不同系统API不统一，且面临专业化与通用性的权衡。专门化系统在单一模态表现优异但难以处理多模态数据，而通用系统在多模态处理上性能不足。

Method: 提出Meta Engine作为"查询系统之上的查询系统"，包含五个核心组件：1)自然语言查询解析器，2)操作符生成器，3)查询路由器，4)适配器集合，5)结果聚合器。通过统一架构集成异构的专业化LLM查询系统。

Result: Meta Engine在评估中持续优于所有基线方法，在大多数情况下获得3-6倍的F1分数提升，在特定数据集上甚至达到24倍的性能提升。

Conclusion: Meta Engine成功解决了多模态语义查询系统的碎片化问题，通过统一架构集成专门化系统，实现了既保持专门化性能又具备多模态处理能力的平衡。

Abstract: With the increasingly use of multi-modal data, semantic query has become more and more demanded in data management systems, which is an important way to access and analyze multi-modal data. As unstructured data, most information of multi-modal data (text, image, video, etc) hides in the semantics, which cannot be accessed by the traditional database queries like SQL.
  Given the power of Large Language Model (LLM) in understanding semantics and processing natural language, in recent years several LLM-based semantic query systems have been proposed, to support semantic querying over unstructured data. However, this rapid growth has produced a fragmented ecosystem. Applications face significant integration challenges due to (1) disparate APIs of different semantic query systems and (2) a fundamental trade-off between specialization and generality. Many semantic query systems are highly specialized, offering state-of-the-art performance within a single modality but struggling with multi-modal data. Conversely, some "all-in-one" systems handle multiple modalities but often exhibit suboptimal performance compared to their specialized counterparts in specific modalities.
  This paper introduces Meta Engine, a novel "query system on query systems", designed to resolve those aforementioned challenges. Meta Engine is a unified semantic query engine that integrates heterogeneous, specialized LLM-based query systems. Its architecture comprises five key components: (1) a Natural Language (NL) Query Parser, (2) an Operator Generator, (3) a Query Router, (4) a set of Adapters, and (5) a Result Aggregator. In the evaluation, Meta Engine consistently outperforms all baselines, yielding 3-6x higher F1 in most cases and up to 24x on specific datasets.

</details>


### [635] [Human Society-Inspired Approaches to Agentic AI Security: The 4C Framework](https://arxiv.org/abs/2602.01942)
*Alsharif Abuadbba,Nazatul Sultan,Surya Nepal,Sanjay Jha*

Main category: cs.CR

Relevance: 75.0

TL;DR: 本文提出了4C框架用于多智能体AI安全，将智能体风险组织为四个相互依赖的维度：核心、连接、认知和合规，从系统中心保护转向行为完整性和意图保护。


<details>
  <summary>Details</summary>
Motivation: AI正从封闭环境中的领域特定自主性转向LLM驱动的智能体，在开放、跨组织的环境中规划和行动。这从根本上改变了网络安全风险格局，智能体AI系统可以作为复杂社会技术生态系统中的参与者，而不仅仅是孤立的软件组件。现有的系统中心方法可能无法捕捉自主性、交互和涌现行为带来的风险。

Method: 提出了4C框架（受社会治理启发），将智能体风险组织为四个相互依赖的维度：1) 核心（系统、基础设施和环境完整性），2) 连接（通信、协调和信任），3) 认知（信念、目标和推理完整性），4) 合规（伦理、法律和制度治理）。

Result: 该框架将AI安全从狭窄的系统中心保护转向更广泛的行为完整性和意图保护，补充了现有的AI安全策略，为构建可信、可治理且与人类价值观一致的智能体AI系统提供了原则性基础。

Conclusion: 4C框架为多智能体AI安全提供了一个全面的视角，通过关注核心、连接、认知和合规四个维度，能够更好地应对智能体AI在开放环境中带来的新安全挑战，确保系统的可信性和可治理性。

Abstract: AI is moving from domain-specific autonomy in closed, predictable settings to large-language-model-driven agents that plan and act in open, cross-organizational environments. As a result, the cybersecurity risk landscape is changing in fundamental ways. Agentic AI systems can plan, act, collaborate, and persist over time, functioning as participants in complex socio-technical ecosystems rather than as isolated software components. Although recent work has strengthened defenses against model and pipeline level vulnerabilities such as prompt injection, data poisoning, and tool misuse, these system centric approaches may fail to capture risks that arise from autonomy, interaction, and emergent behavior. This article introduces the 4C Framework for multi-agent AI security, inspired by societal governance. It organizes agentic risks across four interdependent dimensions: Core (system, infrastructure, and environmental integrity), Connection (communication, coordination, and trust), Cognition (belief, goal, and reasoning integrity), and Compliance (ethical, legal, and institutional governance). By shifting AI security from a narrow focus on system-centric protection to the broader preservation of behavioral integrity and intent, the framework complements existing AI security strategies and offers a principled foundation for building agentic AI systems that are trustworthy, governable, and aligned with human values.

</details>


### [636] [The Verification Crisis: Expert Perceptions of GenAI Disinformation and the Case for Reproducible Provenance](https://arxiv.org/abs/2602.02100)
*Alexander Loth,Martin Kappes,Marc-Oliver Pahl*

Main category: cs.CY

Relevance: 75.0

TL;DR: 论文通过专家调查发现，虽然深度伪造视频有即时冲击力，但大规模文本生成对政治领域构成系统性风险，导致"认知碎片化"和"合成共识"。专家对技术检测工具持怀疑态度，更倾向于来源标准和监管框架。


<details>
  <summary>Details</summary>
Motivation: 随着生成式AI的发展，虚假信息生产从人工制造转向自动化大规模操纵。需要了解专家对多模态威胁严重性的看法，并评估当前缓解策略的有效性。

Method: 采用纵向专家感知调查（N=21），涉及AI研究人员、政策制定者和虚假信息专家，评估文本、图像、音频和视频等多模态威胁，并分析当前缓解策略。

Result: 1) 深度伪造视频有即时"冲击"价值，但大规模文本生成对政治领域构成系统性风险，导致"认知碎片化"和"合成共识"；2) 专家对技术检测工具持怀疑态度，更倾向于来源标准和监管框架；3) 当前缺乏标准化基准和可复现性检查表，难以追踪或对抗合成媒体。

Conclusion: 需要将信息完整性视为基础设施，在数据来源和方法可复现性上保持严谨。建议建立标准化基准和可复现性检查表，以更好地追踪和对抗生成式AI虚假信息。

Abstract: The growth of Generative Artificial Intelligence (GenAI) has shifted disinformation production from manual fabrication to automated, large-scale manipulation. This article presents findings from the first wave of a longitudinal expert perception survey (N=21) involving AI researchers, policymakers, and disinformation specialists. It examines the perceived severity of multimodal threats -- text, image, audio, and video -- and evaluates current mitigation strategies.
  Results indicate that while deepfake video presents immediate "shock" value, large-scale text generation poses a systemic risk of "epistemic fragmentation" and "synthetic consensus," particularly in the political domain. The survey reveals skepticism about technical detection tools, with experts favoring provenance standards and regulatory frameworks despite implementation barriers.
  GenAI disinformation research requires reproducible methods. The current challenge is measurement: without standardized benchmarks and reproducibility checklists, tracking or countering synthetic media remains difficult. We propose treating information integrity as an infrastructure with rigor in data provenance and methodological reproducibility.

</details>


### [637] [Autonomous Data Processing using Meta-Agents](https://arxiv.org/abs/2602.00307)
*Udayan Khurana*

Main category: cs.AI

Relevance: 65.0

TL;DR: ADP-MA是一个使用元代理自主构建、执行和优化数据处理管道的框架，通过分层代理编排实现动态管道管理


<details>
  <summary>Details</summary>
Motivation: 传统数据处理管道通常是静态的、针对特定任务手工设计的，缺乏适应变化需求的能力。现有通用代理和编码助手虽然能为已知管道生成代码，但无法在部署后自主监控、管理和优化端到端管道。

Method: 采用分层代理编排架构：元代理分析输入数据和任务规范来设计多阶段计划，实例化专门的地面级代理，并持续评估管道性能。包含三个关键组件：策略生成的规划模块、代理协调和工具集成的编排层、以及迭代评估和回溯的监控循环。

Result: 通过交互式演示展示了ADP-MA在代表性数据处理任务中的管道构建、执行监控和自适应优化能力，强调了上下文感知优化、自适应工作负载分区和渐进采样等特性。

Conclusion: ADP-MA框架能够动态构建、执行和迭代优化数据处理管道，解决了传统静态管道的局限性，通过元代理的自主管理实现了更灵活、可扩展的数据处理解决方案。

Abstract: Traditional data processing pipelines are typically static and handcrafted for specific tasks, limiting their adaptability to evolving requirements. While general-purpose agents and coding assistants can generate code for well-understood data pipelines, they lack the ability to autonomously monitor, manage, and optimize an end-to-end pipeline once deployed. We present \textbf{Autonomous Data Processing using Meta-agents} (ADP-MA), a framework that dynamically constructs, executes, and iteratively refines data processing pipelines through hierarchical agent orchestration. At its core, \textit{meta-agents} analyze input data and task specifications to design a multi-phase plan, instantiate specialized \textit{ground-level agents}, and continuously evaluate pipeline performance. The architecture comprises three key components: a planning module for strategy generation, an orchestration layer for agent coordination and tool integration, and a monitoring loop for iterative evaluation and backtracking. Unlike conventional approaches, ADP-MA emphasizes context-aware optimization, adaptive workload partitioning, and progressive sampling for scalability. Additionally, the framework leverages a diverse set of external tools and can reuse previously designed agents, reducing redundancy and accelerating pipeline construction. We demonstrate ADP-MA through an interactive demo that showcases pipeline construction, execution monitoring, and adaptive refinement across representative data processing tasks.

</details>


### [638] [POET: Protocol Optimization via Eligibility Tuning](https://arxiv.org/abs/2602.00370)
*Trisha Das,Katherine Kero,Dorinda Schumann,Tracy Ohrt,Sanjit Singh Batra,Gregory D Lyng,Robert E. Tillman*

Main category: cs.AI

Relevance: 65.0

TL;DR: 提出基于语义轴引导的临床试验资格标准生成框架，通过可解释的语义维度（如人口统计学、实验室参数等）指导LLM生成，在特定性和可用性之间取得平衡，优于无引导生成方法。


<details>
  <summary>Details</summary>
Motivation: 临床试验资格标准设计对临床医生来说是耗时且认知负荷高的任务。现有自动化方法存在两个极端：要么需要高度结构化输入（如预定义实体），要么依赖端到端系统从最少输入生成完整标准，限制了实际应用价值。

Method: 提出引导生成框架，引入可解释的语义轴（如人口统计学、实验室参数、行为因素等）来指导资格标准生成。这些语义轴使用大语言模型推导，在特定性和可用性之间提供中间方案，使临床医生能够引导生成而无需指定确切实体。

Result: 引导生成方法在自动评估、基于量规的评估和临床医生评估中均一致优于无引导生成，为AI辅助试验设计提供了实用且可解释的解决方案。

Conclusion: 提出的引导生成框架通过语义轴在特定性和可用性之间取得平衡，为临床试验资格标准设计提供了实用且可解释的AI辅助解决方案，显著优于现有方法。

Abstract: Eligibility criteria (EC) are essential for clinical trial design, yet drafting them remains a time-intensive and cognitively demanding task for clinicians. Existing automated approaches often fall at two extremes either requiring highly structured inputs, such as predefined entities to generate specific criteria, or relying on end-to-end systems that produce full eligibility criteria from minimal input such as trial descriptions limiting their practical utility. In this work, we propose a guided generation framework that introduces interpretable semantic axes, such as Demographics, Laboratory Parameters, and Behavioral Factors, to steer EC generation. These axes, derived using large language models, offer a middle ground between specificity and usability, enabling clinicians to guide generation without specifying exact entities. In addition, we present a reusable rubric-based evaluation framework that assesses generated criteria along clinically meaningful dimensions. Our results show that our guided generation approach consistently outperforms unguided generation in both automatic, rubric-based and clinician evaluations, offering a practical and interpretable solution for AI-assisted trial design.

</details>


### [639] [Scalable Generative Game Engine: Breaking the Resolution Wall via Hardware-Algorithm Co-Design](https://arxiv.org/abs/2602.00608)
*Wei Zeng,Xuchen Li,Ruili Feng,Zhen Liu,Fengwei An,Jian Zhao*

Main category: cs.AI

Relevance: 65.0

TL;DR: 提出硬件算法协同设计框架，解决生成式游戏引擎的"内存墙"问题，实现720×480分辨率实时生成，比基线提升50倍像素吞吐量


<details>
  <summary>Details</summary>
Motivation: 现有实时生成式游戏引擎受限于"内存墙"，只能实现低分辨率（如64×64），无法满足高分辨率神经模拟需求。需要解决世界模型（计算密集型）与解码器（内存密集型）之间的资源不匹配问题。

Method: 提出异构架构，将世界模型与解码器解耦到AI加速器集群上。核心创新：1) 非对称资源分配策略优化序列并行约束下的吞吐量；2) 内存中心算子融合方案最小化片外带宽使用；3) 流形感知潜在外推机制利用时间冗余掩盖延迟。

Result: 在可编程AI加速器集群上验证，实现720×480分辨率实时生成，比基线提升50倍像素吞吐量。在连续3D赛车和离散2D平台游戏基准测试中，分别达到26.4 FPS和48.3 FPS，摊销有效延迟为2.7毫秒。

Conclusion: 通过架构协同设计解决"内存墙"不仅是优化，更是实现高保真、响应式神经游戏体验的先决条件。该框架为生成模型与高分辨率神经模拟之间的鸿沟搭建了桥梁。

Abstract: Real-time generative game engines represent a paradigm shift in interactive simulation, promising to replace traditional graphics pipelines with neural world models. However, existing approaches are fundamentally constrained by the ``Memory Wall,'' restricting practical deployments to low resolutions (e.g., $64 \times 64$). This paper bridges the gap between generative models and high-resolution neural simulations by introducing a scalable \textit{Hardware-Algorithm Co-Design} framework. We identify that high-resolution generation suffers from a critical resource mismatch: the World Model is compute-bound while the Decoder is memory-bound. To address this, we propose a heterogeneous architecture that intelligently decouples these components across a cluster of AI accelerators. Our system features three core innovations: (1) an asymmetric resource allocation strategy that optimizes throughput under sequence parallelism constraints; (2) a memory-centric operator fusion scheme that minimizes off-chip bandwidth usage; and (3) a manifold-aware latent extrapolation mechanism that exploits temporal redundancy to mask latency. We validate our approach on a cluster of programmable AI accelerators, enabling real-time generation at $720 \times 480$ resolution -- a $50\times$ increase in pixel throughput over prior baselines. Evaluated on both continuous 3D racing and discrete 2D platformer benchmarks, our system delivers fluid 26.4 FPS and 48.3 FPS respectively, with an amortized effective latency of 2.7 ms. This work demonstrates that resolving the ``Memory Wall'' via architectural co-design is not merely an optimization, but a prerequisite for enabling high-fidelity, responsive neural gameplay.

</details>


### [640] [SEISMO: Increasing Sample Efficiency in Molecular Optimization with a Trajectory-Aware LLM Agent](https://arxiv.org/abs/2602.00663)
*Fabian P. Krüger,Andrea Hunklinger,Adrian Wolny,Tim J. Adler,Igor Tetko,Santiago David Villalba*

Main category: cs.AI

Relevance: 65.0

TL;DR: SEISMO是一个LLM代理，通过在线推理进行分子优化，每次oracle调用后更新，无需基于种群或批量学习，在23个任务上比先前方法提高2-3倍优化曲线下面积


<details>
  <summary>Details</summary>
Motivation: 分子优化是化学科学特别是药物发现的核心瓶颈，由于分子属性评估依赖于昂贵且速率受限的oracle（如实验测定），因此需要高度样本效率的优化方法

Method: SEISMO是一个LLM代理，执行严格的在线推理时分子优化，每次oracle调用后更新，无需基于种群或批量学习。它将每个提议基于完整的优化轨迹，结合自然语言任务描述、标量分数以及可用的结构化解释性反馈

Result: 在23个任务的实用分子优化基准测试中，SEISMO实现了比先前方法高2-3倍的优化曲线下面积，通常在50次oracle调用内达到接近最大任务分数。额外的药物化学任务表明，提供解释性反馈能进一步提高效率

Conclusion: 利用领域知识和结构化信息是实现样本高效分子优化的关键。SEISMO展示了LLM代理在严格在线设置中进行分子优化的有效性

Abstract: Optimizing the structure of molecules to achieve desired properties is a central bottleneck across the chemical sciences, particularly in the pharmaceutical industry where it underlies the discovery of new drugs. Since molecular property evaluation often relies on costly and rate-limited oracles, such as experimental assays, molecular optimization must be highly sample-efficient. To address this, we introduce SEISMO, an LLM agent that performs strictly online, inference-time molecular optimization, updating after every oracle call without the need for population-based or batched learning. SEISMO conditions each proposal on the full optimization trajectory, combining natural-language task descriptions with scalar scores and, when available, structured explanatory feedback. Across the Practical Molecular Optimization benchmark of 23 tasks, SEISMO achieves a 2-3 times higher area under the optimisation curve than prior methods, often reaching near-maximal task scores within 50 oracle calls. Our additional medicinal-chemistry tasks show that providing explanatory feedback further improves efficiency, demonstrating that leveraging domain knowledge and structured information is key to sample-efficient molecular optimization.

</details>


### [641] [OpenGuanDan: A Large-Scale Imperfect Information Game Benchmark](https://arxiv.org/abs/2602.00676)
*Chao Li,Shangdong Yang,Chiheng Zhan,Zhenxing Ge,Yujing Hu,Bingkun Bao,Xingguo Chen,Yang Gao*

Main category: cs.AI

Relevance: 65.0

TL;DR: OpenGuanDan是一个用于评估AI智能体在中国纸牌游戏"掼蛋"中的表现的新型基准测试平台，该游戏具有不完全信息、大规模动作空间、合作竞争混合目标等挑战性特征。


<details>
  <summary>Details</summary>
Motivation: 当前AI领域缺乏足够挑战性的基准测试来推动智能决策研究的发展。虽然AI在棋类、卡牌、电子竞技等领域取得了显著进展，但需要更复杂的测试环境来评估多智能体决策能力。

Method: 开发了OpenGuanDan平台，提供高效的掼蛋游戏模拟和全面的AI评估框架。平台支持基于学习和基于规则的AI智能体，每个玩家都有独立的API，支持人机交互和大型语言模型集成。

Result: 实验结果表明，当前基于学习的智能体显著优于基于规则的智能体，但仍未达到超人类水平。这突显了多智能体智能决策领域需要进一步研究。

Conclusion: OpenGuanDan作为一个具有挑战性的基准测试平台，能够有效评估AI智能体在复杂多智能体环境中的决策能力，为智能决策研究提供了有价值的工具。

Abstract: The advancement of data-driven artificial intelligence (AI), particularly machine learning, heavily depends on large-scale benchmarks. Despite remarkable progress across domains ranging from pattern recognition to intelligent decision-making in recent decades, exemplified by breakthroughs in board games, card games, and electronic sports games, there remains a pressing need for more challenging benchmarks to drive further research. To this end, this paper proposes OpenGuanDan, a novel benchmark that enables both efficient simulation of GuanDan (a popular four-player, multi-round Chinese card game) and comprehensive evaluation of both learning-based and rule-based GuanDan AI agents. OpenGuanDan poses a suite of nontrivial challenges, including imperfect information, large-scale information set and action spaces, a mixed learning objective involving cooperation and competition, long-horizon decision-making, variable action spaces, and dynamic team composition. These characteristics make it a demanding testbed for existing intelligent decision-making methods. Moreover, the independent API for each player allows human-AI interactions and supports integration with large language models. Empirically, we conduct two types of evaluations: (1) pairwise competitions among all GuanDan AI agents, and (2) human-AI matchups. Experimental results demonstrate that while current learning-based agents substantially outperform rule-based counterparts, they still fall short of achieving superhuman performance, underscoring the need for continued research in multi-agent intelligent decision-making domain. The project is publicly available at https://github.com/GameAI-NJUPT/OpenGuanDan.

</details>


### [642] [From Prompt to Graph: Comparing LLM-Based Information Extraction Strategies in Domain-Specific Ontology Development](https://arxiv.org/abs/2602.00699)
*Xuan Liu,Ziyu Li,Mu He,Ziyang Ma,Xiaoxu Wu,Gizem Yilmaz,Yiyuan Xia,Bingbing Li,He Tan,Jerry Ying Hsi Fuh,Wen Feng Lu,Anders E. W. Jarfors,Per Jansson*

Main category: cs.AI

Relevance: 65.0

TL;DR: 该研究探索了三种基于大语言模型的方法（预训练LLM驱动、上下文学习和微调）从铸造制造领域文本中自动提取术语和关系，以构建领域本体，并验证了最佳方法的效果。


<details>
  <summary>Details</summary>
Motivation: 传统本体构建依赖人工标注和传统NLP技术，过程劳动密集且成本高昂，特别是在铸造制造等专业领域。大语言模型的兴起为自动化知识提取提供了新可能。

Method: 研究了三种LLM方法：1) 预训练LLM驱动方法；2) 上下文学习方法；3) 微调方法。使用有限数据从领域特定文本中提取术语和关系，比较性能后选择最佳方法构建铸造本体。

Result: 比较了三种方法的性能，确定了最佳性能方法，并用该方法构建了经过领域专家验证的铸造本体。

Conclusion: LLM方法能够有效自动化领域本体构建，特别是在数据有限的专门领域，为传统劳动密集型本体构建提供了高效替代方案。

Abstract: Ontologies are essential for structuring domain knowledge, improving accessibility, sharing, and reuse. However, traditional ontology construction relies on manual annotation and conventional natural language processing (NLP) techniques, making the process labour-intensive and costly, especially in specialised fields like casting manufacturing. The rise of Large Language Models (LLMs) offers new possibilities for automating knowledge extraction. This study investigates three LLM-based approaches, including pre-trained LLM-driven method, in-context learning (ICL) method and fine-tuning method to extract terms and relations from domain-specific texts using limited data. We compare their performances and use the best-performing method to build a casting ontology that validated by domian expert.

</details>


### [643] [Environment-Aware Adaptive Pruning with Interleaved Inference Orchestration for Vision-Language-Action Models](https://arxiv.org/abs/2602.00780)
*Yuting Huang,Leilei Ding,Zhipeng Tang,Zenghuan Zhu,Jiajun Deng,Xinrui Lin,Shuo Liu,Haojie Ren,Jianmin Ji,Yanyong Zhang*

Main category: cs.AI

Relevance: 65.0

TL;DR: EcoVLA：一种用于视觉-语言-动作模型的训练免费、即插即用自适应剪枝框架，通过环境感知自适应剪枝和交错推理编排，实现实时操作加速，最高可达2.18倍加速且性能下降极小。


<details>
  <summary>Details</summary>
Motivation: 视觉-语言-动作模型在具身智能中前景广阔，但参数量大导致推理延迟高，阻碍实时操作。静态剪枝无法适应环境动态变化，固定间隔的动态层剪枝则粒度粗糙且重训练开销大。

Method: 提出EcoVLA框架，包含两个组件：1) 环境感知自适应剪枝(EAP)：轻量级自适应通道剪枝方法，利用物理环境的时间一致性更新稀疏模式；2) 交错推理编排(I²O)：利用VLA推理中的FLOPs气泡并行调度剪枝方法，确保对延迟影响可忽略。

Result: 在多种VLA模型和基准测试中，EcoVLA达到SOTA性能：单独使用可实现1.60倍加速且成功率仅下降0.4%；与token剪枝结合可达2.18倍加速且性能仅下降0.5%。在真实机器人上验证了有效性。

Conclusion: EcoVLA为VLA模型提供了一种高效的自适应剪枝解决方案，能够适应环境动态变化，实现显著加速而性能损失极小，且与现有加速方法正交兼容。

Abstract: While Vision-Language-Action (VLA) models hold promise in embodied intelligence, their large parameter counts lead to substantial inference latency that hinders real-time manipulation, motivating parameter sparsification. However, as the environment evolves during VLA execution, the optimal sparsity patterns change accordingly. Static pruning lacks the adaptability required for environment dynamics, whereas fixed-interval dynamic layer pruning suffers from coarse granularity and high retraining overheads. To bridge this gap, we propose EcoVLA, a training-free, plug-and-play adaptive pruning framework that supports orthogonal combination with existing VLA acceleration methods. EcoVLA comprises two components: Environment-aware Adaptive Pruning (EAP) and Interleaved Inference Orchestration ($I^2O$). EAP is a lightweight adaptive channel pruning method that incorporates the temporal consistency of the physical environment to update sparsity patterns. $I^2O$ leverages the FLOPs bubbles inherent in VLA inference to schedule the pruning method in parallel, ensuring negligible impact on latency. Evaluated on diverse VLA models and benchmarks, EcoVLA delivers state-of-the-art performance, achieving up to 1.60$\times$ speedup with only a 0.4% drop in success rate, and further reaches 2.18$\times$ speedup with only a 0.5% degradation when combined with token pruning. We further validate the effectiveness of EcoVLA on real-world robots.

</details>


### [644] [R-HTN: Rebellious Online HTN Planning for Safety and Game AI](https://arxiv.org/abs/2602.00951)
*Hector Munoz-Avila,David W. Aha,Paola Rizzo*

Main category: cs.AI

Relevance: 65.0

TL;DR: 本文提出了R-HTN算法，这是一种在线分层任务网络规划方法，使智能体能够在违反内置指令时拒绝执行用户任务或自适应修改计划。


<details>
  <summary>Details</summary>
Motivation: 研究智能体在面临用户任务与内置指令冲突时的行为决策问题。传统智能体通常盲目执行用户指令，但实际应用中智能体需要能够识别并拒绝可能违反安全规定或自身原则的任务，实现"智能不服从"。

Method: 结合HTN规划、在线规划和内置指令集D三个概念，提出R-HTN算法。研究两种智能体变体：非自适应智能体（违反指令时停止执行）和自适应智能体（违反指令时修改HTN计划寻找替代方案）。

Result: R-HTN智能体在测试任务域中从未违反指令，并且在可行情况下仍会尝试实现用户目标，尽管可能不是用户期望的方式。自适应智能体表现优于非自适应智能体。

Conclusion: R-HTN算法为智能体提供了在遵守内置指令的前提下处理用户任务的框架，实现了智能不服从能力，对安全关键应用和个性化智能体开发具有重要意义。

Abstract: We introduce online Hierarchical Task Network (HTN) agents whose behaviors are governed by a set of built-in directives \D. Like other agents that are capable of rebellion (i.e., {\it intelligent disobedience}), our agents will, under some conditions, not perform a user-assigned task and instead act in ways that do not meet a user's expectations. Our work combines three concepts: HTN planning, online planning, and the directives \D, which must be considered when performing user-assigned tasks. We investigate two agent variants: (1) a Nonadaptive agent that stops execution if it finds itself in violation of \D~ and (2) an Adaptive agent that, in the same situation, instead modifies its HTN plan to search for alternative ways to achieve its given task. We present R-HTN (for: Rebellious-HTN), a general algorithm for online HTN planning under directives \D. We evaluate R-HTN in two task domains where the agent must not violate some directives for safety reasons or as dictated by their personality traits. We found that R-HTN agents never violate directives, and aim to achieve the user-given goals if feasible though not necessarily as the user expected.

</details>


### [645] [LLM-Driven Ontology Construction for Enterprise Knowledge Graphs](https://arxiv.org/abs/2602.01276)
*Abdulsobur Oyewale,Tommaso Soru*

Main category: cs.AI

Relevance: 65.0

TL;DR: OntoEKG：基于LLM的自动化本体构建流水线，从非结构化企业数据生成领域特定本体，包含提取和蕴含两个模块，在数据、金融、物流领域评估


<details>
  <summary>Details</summary>
Motivation: 企业知识图谱对统一异构数据和语义治理至关重要，但传统本体构建依赖领域专家且资源密集，需要自动化解决方案加速领域本体生成

Method: 提出OntoEKG流水线：1) 提取模块识别核心类和属性；2) 蕴含模块将元素逻辑结构化形成层次结构，最后序列化为标准RDF格式

Result: 在数据、金融、物流领域文档构建的评估数据集上测试，数据领域获得0.724的模糊匹配F1分数，但发现范围定义和层次推理存在局限性

Conclusion: LLM驱动的本体构建方法具有潜力但面临挑战，特别是在范围定义和层次推理方面，需要进一步改进

Abstract: Enterprise Knowledge Graphs have become essential for unifying heterogeneous data and enforcing semantic governance. However, the construction of their underlying ontologies remains a resource-intensive, manual process that relies heavily on domain expertise. This paper introduces OntoEKG, a LLM-driven pipeline designed to accelerate the generation of domain-specific ontologies from unstructured enterprise data. Our approach decomposes the modelling task into two distinct phases: an extraction module that identifies core classes and properties, and an entailment module that logically structures these elements into a hierarchy before serialising them into standard RDF. Addressing the significant lack of comprehensive benchmarks for end-to-end ontology construction, we adopt a new evaluation dataset derived from documents across the Data, Finance, and Logistics sectors. Experimental results highlight both the potential and the challenges of this approach, achieving a fuzzy-match F1-score of 0.724 in the Data domain while revealing limitations in scope definition and hierarchical reasoning.

</details>


### [646] [Aggregation Queries over Unstructured Text: Benchmark and Agentic Method](https://arxiv.org/abs/2602.01355)
*Haojia Zhu,Qinyuan Xu,Haoyu Li,Yuxi Liu,Hanchen Qiu,Jiaoyan Chen,Jiahui Jin*

Main category: cs.AI

Relevance: 65.0

TL;DR: 提出了AGGBench基准和DFA框架，用于在文本语料库上执行具有严格完整性要求的实体级聚合查询，解决了现有方法在证据收集完整性方面的不足。


<details>
  <summary>Details</summary>
Motivation: 文本上的聚合查询是一个长期存在但未被充分探索的问题。与普通问答不同，聚合查询需要完整的证据收集，系统需要"找到所有"而不仅仅是"找到一个"。现有的Text-to-SQL和检索增强生成(RAG)范式无法实现这种完整性。

Method: 提出了DFA（消歧-过滤-聚合）框架，这是一个模块化的智能体基线，将聚合查询分解为可解释的阶段：消歧、过滤和聚合，并暴露与模糊性、过滤和聚合相关的关键失败模式。

Result: 实证结果表明，DFA在聚合证据覆盖方面持续优于强大的RAG和智能体基线。作者还发布了AGGBench基准，用于在现实大规模语料库下评估完整性导向的聚合。

Conclusion: 该工作形式化了具有严格完整性要求的语料库约束下的实体级聚合查询，提出了评估基准和模块化框架，为文本聚合查询的完整性要求提供了系统解决方案。

Abstract: Aggregation query over free text is a long-standing yet underexplored problem. Unlike ordinary question answering, aggregate queries require exhaustive evidence collection and systems are required to "find all," not merely "find one." Existing paradigms such as Text-to-SQL and Retrieval-Augmented Generation fail to achieve this completeness. In this work, we formalize entity-level aggregation querying over text in a corpus-bounded setting with strict completeness requirement. To enable principled evaluation, we introduce AGGBench, a benchmark designed to evaluate completeness-oriented aggregation under realistic large-scale corpus. To accompany the benchmark, we propose DFA (Disambiguation--Filtering--Aggregation), a modular agentic baseline that decomposes aggregation querying into interpretable stages and exposes key failure modes related to ambiguity, filtering, and aggregation. Empirical results show that DFA consistently improves aggregation evidence coverage over strong RAG and agentic baselines. The data and code are available in https://anonymous.4open.science/r/DFA-A4C1.

</details>


### [647] [SimGym: Traffic-Grounded Browser Agents for Offline A/B Testing in E-Commerce](https://arxiv.org/abs/2602.01443)
*Alberto Castelo,Zahra Zanjani Foumani,Ailin Fan,Keat Yang Koay,Vibhor Malik,Yuanzheng Zhu,Han Li,Meysam Feghhi,Ronie Uliana,Shuang Xie,Zhaoyu Zhang,Angelo Ocana Martins,Mingyu Zhao,Francis Pelland,Jonathan Faerman,Nikolas LeBlanc,Aaron Glazer,Andrew McNamara,Lingyun Wang,Zhong Wu*

Main category: cs.AI

Relevance: 65.0

TL;DR: SimGym：基于LLM代理的电商UI离线A/B测试系统，通过模拟真实买家行为，将实验周期从数周缩短至1小时内，无需真实流量分流


<details>
  <summary>Details</summary>
Motivation: 传统A/B测试需要分流真实流量、耗时数周才能达到统计显著性，且可能损害用户体验。需要一种快速、安全的离线测试方法来评估电商UI变更

Method: 从生产交互数据中提取每个店铺的买家画像和意图，识别不同的行为原型，使用LLM代理在实时浏览器中模拟控制组和实验组的加权会话

Result: 在主要电商平台上验证，即使没有训练后对齐，SimGym代理也能与观察到的结果变化达到最先进的对齐度，将实验周期从数周缩短至1小时以内

Conclusion: SimGym实现了无需真实买家参与的快速实验，为电商UI评估提供了可扩展的离线A/B测试解决方案

Abstract: A/B testing remains the gold standard for evaluating e-commerce UI changes, yet it diverts traffic, takes weeks to reach significance, and risks harming user experience. We introduce SimGym, a scalable system for rapid offline A/B testing using traffic-grounded synthetic buyers powered by Large Language Model agents operating in a live browser. SimGym extracts per-shop buyer profiles and intents from production interaction data, identifies distinct behavioral archetypes, and simulates cohort-weighted sessions across control and treatment storefronts. We validate SimGym against real human outcomes from real UI changes on a major e-commerce platform under confounder control. Even without alignment post training, SimGym agents achieve state of the art alignment with observed outcome shifts and reduces experiment cycles from weeks to under an hour , enabling rapid experimentation without exposure to real buyers.

</details>


### [648] [Efficient Cross-Architecture Knowledge Transfer for Large-Scale Online User Response Prediction](https://arxiv.org/abs/2602.01775)
*Yucheng Wu,Yuekui Yang,Hongzheng Li,Anan Liu,Jian Xiao,Junjie Zhai,Huan Yu,Shaoping Ma,Leye Wang*

Main category: cs.AI

Relevance: 65.0

TL;DR: CrossAdapt：用于用户响应预测系统的高效跨架构知识蒸馏框架，通过两阶段方法解决模型切换成本高的问题


<details>
  <summary>Details</summary>
Motivation: 大规模用户响应预测系统中部署新架构面临高模型切换成本，包括大规模历史数据重训练开销和性能下降问题。现有知识蒸馏方法难以处理架构异构性和大型嵌入表传输成本

Method: 两阶段框架：离线阶段通过维度自适应投影实现快速嵌入传输，结合渐进网络蒸馏和策略采样；在线阶段采用非对称协同蒸馏，学生频繁更新而教师低频更新，配合分布感知适应机制动态平衡历史知识保留和新数据适应

Result: 在三个公开数据集上实现0.27-0.43% AUC提升，训练时间减少43-71%；在腾讯微信视频号（约1000万日样本）大规模部署中显著缓解AUC下降、LogLoss增加和预测偏差

Conclusion: CrossAdapt有效解决了大规模推荐系统中跨架构知识迁移的挑战，在保持性能的同时大幅降低计算成本，为实际部署提供了实用解决方案

Abstract: Deploying new architectures in large-scale user response prediction systems incurs high model switching costs due to expensive retraining on massive historical data and performance degradation under data retention constraints. Existing knowledge distillation methods struggle with architectural heterogeneity and the prohibitive cost of transferring large embedding tables. We propose CrossAdapt, a two-stage framework for efficient cross-architecture knowledge transfer. The offline stage enables rapid embedding transfer via dimension-adaptive projections without iterative training, combined with progressive network distillation and strategic sampling to reduce computational cost. The online stage introduces asymmetric co-distillation, where students update frequently while teachers update infrequently, together with a distribution-aware adaptation mechanism that dynamically balances historical knowledge preservation and fast adaptation to evolving data. Experiments on three public datasets show that CrossAdapt achieves 0.27-0.43% AUC improvements while reducing training time by 43-71%. Large-scale deployment on Tencent WeChat Channels (~10M daily samples) further demonstrates its effectiveness, significantly mitigating AUC degradation, LogLoss increase, and prediction bias compared to standard distillation baselines.

</details>


### [649] [LingLanMiDian: Systematic Evaluation of LLMs on TCM Knowledge and Clinical Reasoning](https://arxiv.org/abs/2602.01779)
*Rui Hua,Yu Wei,Zixin Shu,Kai Chang,Dengying Yan,Jianan Xia,Zeyu Liu,Hui Zhu,Shujie Song,Mingzhong Xiao,Xiaodong Li,Dongmei Jia,Zhuye Gao,Yanyan Meng,Naixuan Zhao,Yu Fu,Haibin Yu,Benman Yu,Yuanyuan Chen,Fei Dong,Zhizhou Meng,Pengcheng Yang,Songxue Zhao,Lijuan Pei,Yunhui Hu,Kan Ding,Jiayuan Duan,Wenmao Yin,Yang Gu,Runshun Zhang,Qiang Zhu,Jian Yu,Jiansheng Li,Baoyan Liu,Wenjia Wang,Xuezhong Zhou*

Main category: cs.AI

Relevance: 65.0

TL;DR: LingLanMiDian (LingLan) 是一个针对传统中医(TCM)的大规模、专家标注的多任务基准测试，统一评估LLMs在中医知识回忆、多跳推理、信息抽取和临床决策等方面的能力，并设计了标准化评估指标。


<details>
  <summary>Details</summary>
Motivation: 传统中医具有独特的本体论、术语和推理模式，需要领域特定的评估。现有的中医基准测试覆盖范围碎片化、规模有限，且评估方法不统一，难以进行公平比较。

Method: 构建了大规模专家标注的多任务评估套件，包含一致的度量设计、容忍同义词的临床标签协议、每个数据集400项的困难子集，并将诊断和治疗建议重构为单项选择决策识别。

Result: 对14个领先的开源和专有LLMs进行了全面的零样本评估，揭示了当前模型在中医专门推理方面与人类专家存在显著差距，特别是在困难子集上表现明显不足。

Conclusion: LingLan通过标准化评估桥接基础知识和应用推理，为推进中医LLMs和领域特定医学AI研究建立了统一、可量化、可扩展的基础。

Abstract: Large language models (LLMs) are advancing rapidly in medical NLP, yet Traditional Chinese Medicine (TCM) with its distinctive ontology, terminology, and reasoning patterns requires domain-faithful evaluation. Existing TCM benchmarks are fragmented in coverage and scale and rely on non-unified or generation-heavy scoring that hinders fair comparison. We present the LingLanMiDian (LingLan) benchmark, a large-scale, expert-curated, multi-task suite that unifies evaluation across knowledge recall, multi-hop reasoning, information extraction, and real-world clinical decision-making. LingLan introduces a consistent metric design, a synonym-tolerant protocol for clinical labels, a per-dataset 400-item Hard subset, and a reframing of diagnosis and treatment recommendation into single-choice decision recognition. We conduct comprehensive, zero-shot evaluations on 14 leading open-source and proprietary LLMs, providing a unified perspective on their strengths and limitations in TCM commonsense knowledge understanding, reasoning, and clinical decision support; critically, the evaluation on Hard subset reveals a substantial gap between current models and human experts in TCM-specialized reasoning. By bridging fundamental knowledge and applied reasoning through standardized evaluation, LingLan establishes a unified, quantitative, and extensible foundation for advancing TCM LLMs and domain-specific medical AI research. All evaluation data and code are available at https://github.com/TCMAI-BJTU/LingLan and http://tcmnlp.com.

</details>


### [650] [INDIBATOR: Diverse and Fact-Grounded Individuality for Multi-Agent Debate in Molecular Discovery](https://arxiv.org/abs/2602.01815)
*Yunhui Jang,Seonghyun Park,Jaehyung Kim,Sungsoo Ahn*

Main category: cs.AI

Relevance: 65.0

TL;DR: INDIBATOR是一个用于分子发现的多智能体框架，通过基于个体科学家档案（发表历史和分子历史）的细粒度个性化智能体，在提案-批判-投票的多轮辩论中实现优于粗粒度角色分配方法的性能。


<details>
  <summary>Details</summary>
Motivation: 当前多智能体科学发现系统通常采用通用的角色分配（如"审稿人"、"作者"）或基于关键词的粗粒度角色，这过度简化了真实科学家的行为模式。真实科学家的贡献由其独特的研究轨迹塑造，因此需要更细粒度的个性化智能体来模拟真实的科学发现过程。

Method: 提出INDIBATOR框架，为每个智能体构建个体化科学家档案，包含两种模态：1) 发表历史（文献知识），2) 分子历史（结构先验）。这些智能体通过多轮辩论进行交互，包括提案、批判和投票三个阶段。

Result: 基于个体化档案的细粒度智能体在分子发现任务中持续优于依赖粗粒度角色的系统，达到竞争性或最先进的性能水平。这表明捕捉智能体的"科学DNA"对于高质量发现至关重要。

Conclusion: 在科学发现的多智能体系统中，基于个体科学家独特研究轨迹的细粒度个性化智能体比传统的粗粒度角色分配方法更有效，能够更好地模拟真实的科学协作过程。

Abstract: Multi-agent systems have emerged as a powerful paradigm for automating scientific discovery. To differentiate agent behavior in the multi-agent system, current frameworks typically assign generic role-based personas such as ''reviewer'' or ''writer'' or rely on coarse grained keyword-based personas. While functional, this approach oversimplifies how human scientists operate, whose contributions are shaped by their unique research trajectories. In response, we propose INDIBATOR, a framework for molecular discovery that grounds agents in individualized scientist profiles constructed from two modalities: publication history for literature-derived knowledge and molecular history for structural priors. These agents engage in multi-turn debate through proposal, critique, and voting phases. Our evaluation demonstrates that these fine-grained individuality-grounded agents consistently outperform systems relying on coarse-grained personas, achieving competitive or state-of-the-art performance. These results validate that capturing the ``scientific DNA'' of individual agents is essential for high-quality discovery.

</details>


### [651] [SOPRAG: Multi-view Graph Experts Retrieval for Industrial Standard Operating Procedures](https://arxiv.org/abs/2602.01858)
*Liangtao Lin,Zhaomeng Zhu,Tianwei Zhang,Yonggang Wen*

Main category: cs.AI

Relevance: 65.0

TL;DR: SOPRAG是一个专门针对工业标准操作程序检索的RAG框架，采用专家混合架构解决工业文档的结构复杂性和条件依赖问题，显著优于传统检索方法。


<details>
  <summary>Details</summary>
Motivation: 工业环境中的标准操作程序(SOPs)检索面临独特挑战：专有结构僵化、条件依赖相关性、需要可执行操作，传统语义驱动的RAG方法无法有效处理这些问题。

Method: 1) 采用MoE范式，用专门的实体、因果和流程图专家替代平面分块；2) 引入Procedure Card层修剪搜索空间；3) LLM引导的门控机制动态加权专家；4) 多智能体自动工作流构建基准数据集。

Result: 在四个工业领域的广泛实验中，SOPRAG在检索准确性和响应效用方面显著优于基于词法、密集和图的RAG基线，在真实关键任务中实现了完美的执行分数。

Conclusion: SOPRAG框架成功解决了工业SOP检索的独特挑战，通过专家混合架构和智能协调机制，为工业环境提供了更有效的操作程序检索解决方案。

Abstract: Standard Operating Procedures (SOPs) are essential for ensuring operational safety and consistency in industrial environments. However, retrieving and following these procedures presents unique challenges, such as rigid proprietary structures, condition-dependent relevance, and actionable execution requirement, which standard semantic-driven Retrieval-Augmented Generation (RAG) paradigms fail to address. Inspired by the Mixture-of-Experts (MoE) paradigm, we propose SOPRAG, a novel framework specifically designed to address the above pain points in SOP retrieval. SOPRAG replaces flat chunking with specialized Entity, Causal, and Flow graph experts to resolve industrial structural and logical complexities. To optimize and coordinate these experts, we propose a Procedure Card layer that prunes the search space to eliminate computational noise, and an LLM-Guided gating mechanism that dynamically weights these experts to align retrieval with operator intent. To address the scarcity of domain-specific data, we also introduce an automated, multi-agent workflow for benchmark construction. Extensive experiments across four industrial domains demonstrate that SOPRAG significantly outperforms strong lexical, dense, and graph-based RAG baselines in both retrieval accuracy and response utility, achieving perfect execution scores in real-world critical tasks.

</details>


### [652] [Canonical Intermediate Representation for LLM-based optimization problem formulation and code generation](https://arxiv.org/abs/2602.02029)
*Zhongyuan Lyu,Shuoyu Hu,Lujie Liu,Hongxia Yang,Ming LI*

Main category: cs.AI

Relevance: 65.0

TL;DR: 提出CIR中间表示和R2C框架，用于从自然语言描述自动生成优化模型，在复杂操作规则建模上达到SOTA性能


<details>
  <summary>Details</summary>
Motivation: 现有基于LLM的方法难以处理复杂操作规则的复合约束和适当建模范式，需要一种能解耦规则逻辑与数学实例化的方法

Method: 引入规范中间表示(CIR)作为LLM生成的中间模式，编码约束原型和候选建模范式；开发R2C多智能体框架，包含问题解析、CIR合成和模型实例化

Result: 在新建基准上达到47.2%准确率，在现有基准上接近GPT-5等专有模型性能，通过反思机制进一步提升并在某些基准上创下新记录

Conclusion: CIR和R2C框架显著提升了从自然语言到优化模型的自动转换能力，为复杂操作规则的建模提供了有效解决方案

Abstract: Automatically formulating optimization models from natural language descriptions is a growing focus in operations research, yet current LLM-based approaches struggle with the composite constraints and appropriate modeling paradigms required by complex operational rules. To address this, we introduce the Canonical Intermediate Representation (CIR): a schema that LLMs explicitly generate between problem descriptions and optimization models. CIR encodes the semantics of operational rules through constraint archetypes and candidate modeling paradigms, thereby decoupling rule logic from its mathematical instantiation. Upon a newly generated CIR knowledge base, we develop the rule-to-constraint (R2C) framework, a multi-agent pipeline that parses problem texts, synthesizes CIR implementations by retrieving domain knowledge, and instantiates optimization models. To systematically evaluate rule-to-constraint reasoning, we test R2C on our newly constructed benchmark featuring rich operational rules, and benchmarks from prior work. Extensive experiments show that R2C achieves state-of-the-art accuracy on the proposed benchmark (47.2% Accuracy Rate). On established benchmarks from the literature, R2C delivers highly competitive results, approaching the performance of proprietary models (e.g., GPT-5). Moreover, with a reflection mechanism, R2C achieves further gains and sets new best-reported results on some benchmarks.

</details>


### [653] [SIDiffAgent: Self-Improving Diffusion Agent](https://arxiv.org/abs/2602.02051)
*Shivank Garg,Ayush Singh,Gaurav Kumar Nayak*

Main category: cs.AI

Relevance: 65.0

TL;DR: SIDiffAgent是一个无需训练的代理框架，利用Qwen系列模型解决文本到图像扩散模型的局限性，通过自主提示工程、错误检测与修正、伪影移除和迭代自我改进，显著提升生成质量。


<details>
  <summary>Details</summary>
Motivation: 文本到图像扩散模型在实际部署中存在多个限制：对提示措辞敏感、语义解释模糊（如"mouse"指动物还是计算机外设）、解剖结构扭曲等伪影，以及需要精心设计的输入提示。现有方法通常需要额外训练且可控性有限，限制了实际应用中的适应性。

Method: 提出Self-Improving Diffusion Agent (SIDiffAgent)，一个无需训练的代理框架，利用Qwen系列模型（Qwen-VL, Qwen-Image, Qwen-Edit, Qwen-Embedding）自主管理提示工程、检测和修正不良生成、执行细粒度伪影移除。框架包含迭代自我改进机制，将先前经验存储在数据库中，并在代理管道的每个阶段注入基于提示的指导。

Result: 在GenAIBench上获得了平均VQA分数0.884，显著优于开源模型、专有模型和其他代理方法。

Conclusion: SIDiffAgent通过代理框架有效解决了文本到图像扩散模型的部署限制，无需额外训练即可实现自我改进和可靠输出，为实际应用提供了实用解决方案。

Abstract: Text-to-image diffusion models have revolutionized generative AI, enabling high-quality and photorealistic image synthesis. However, their practical deployment remains hindered by several limitations: sensitivity to prompt phrasing, ambiguity in semantic interpretation (e.g., ``mouse" as animal vs. a computer peripheral), artifacts such as distorted anatomy, and the need for carefully engineered input prompts. Existing methods often require additional training and offer limited controllability, restricting their adaptability in real-world applications. We introduce Self-Improving Diffusion Agent (SIDiffAgent), a training-free agentic framework that leverages the Qwen family of models (Qwen-VL, Qwen-Image, Qwen-Edit, Qwen-Embedding) to address these challenges. SIDiffAgent autonomously manages prompt engineering, detects and corrects poor generations, and performs fine-grained artifact removal, yielding more reliable and consistent outputs. It further incorporates iterative self-improvement by storing a memory of previous experiences in a database. This database of past experiences is then used to inject prompt-based guidance at each stage of the agentic pipeline. \modelour achieved an average VQA score of 0.884 on GenAIBench, significantly outperforming open-source, proprietary models and agentic methods. We will publicly release our code upon acceptance.

</details>


### [654] [SafeGround: Know When to Trust GUI Grounding Models via Uncertainty Calibration](https://arxiv.org/abs/2602.02419)
*Qingni Wang,Yue Fan,Xin Eric Wang*

Main category: cs.AI

Relevance: 65.0

TL;DR: SafeGround是一个用于GUI grounding任务的不确定性感知框架，通过分布感知的不确定性量化方法和校准过程实现风险感知预测，在ScreenSpot-Pro基准测试中显著提升系统级准确性。


<details>
  <summary>Details</summary>
Motivation: GUI grounding任务中错误的坐标预测可能导致代价高昂且难以逆转的操作（如错误支付批准），因此需要提高模型可靠性并控制风险。

Method: 提出SafeGround框架：1）使用分布感知的不确定性量化方法捕捉模型输出的空间分散性；2）通过校准过程获得具有统计保证的假发现率控制的测试时决策阈值。

Result: 在ScreenSpot-Pro基准测试中，SafeGround的不确定性度量优于现有基线，校准阈值能可靠实现严格的风险控制，系统级准确性最高提升5.38个百分点。

Conclusion: SafeGround为GUI grounding模型提供了有效的风险控制框架，通过不确定性量化和校准实现了可靠的假发现率控制，显著提升了系统级性能。

Abstract: Graphical User Interface (GUI) grounding aims to translate natural language instructions into executable screen coordinates, enabling automated GUI interaction. Nevertheless, incorrect grounding can result in costly, hard-to-reverse actions (e.g., erroneous payment approvals), raising concerns about model reliability. In this paper, we introduce SafeGround, an uncertainty-aware framework for GUI grounding models that enables risk-aware predictions through calibrations before testing. SafeGround leverages a distribution-aware uncertainty quantification method to capture the spatial dispersion of stochastic samples from outputs of any given model. Then, through the calibration process, SafeGround derives a test-time decision threshold with statistically guaranteed false discovery rate (FDR) control. We apply SafeGround on multiple GUI grounding models for the challenging ScreenSpot-Pro benchmark. Experimental results show that our uncertainty measure consistently outperforms existing baselines in distinguishing correct from incorrect predictions, while the calibrated threshold reliably enables rigorous risk control and potentials of substantial system-level accuracy improvements. Across multiple GUI grounding models, SafeGround improves system-level accuracy by up to 5.38\% percentage points over Gemini-only inference.

</details>


### [655] [ChunkNorris: A High-Performance and Low-Energy Approach to PDF Parsing and Chunking](https://arxiv.org/abs/2602.00010)
*Mathieu Ciancone,Clovis Varangot-Reille,Marion Schaeffer*

Main category: cs.IR

Relevance: 65.0

TL;DR: ChunkNorris：一种基于启发式规则的PDF文档解析和分块优化技术，专为RAG系统设计，无需机器学习，计算开销低


<details>
  <summary>Details</summary>
Motivation: 在检索增强生成(RAG)应用中，信息检索部分至关重要，因为它为LLM提供了生成适当和真实回答所需的上下文信息。高质量的解析和分块直接影响下游任务（信息检索和答案生成）的性能。现有方法可能计算成本高或依赖机器学习，需要更高效、轻量级的解决方案。

Method: ChunkNorris是一种基于启发式规则的技术，不依赖机器学习，采用一套简单而有效的启发式规则来优化PDF文档的解析和分块。该方法专注于最小化计算开销，同时保持高性能。

Result: 通过综合基准测试评估执行时间、能耗和检索准确性，ChunkNorris在性能上优于基线和更先进的技术。作者还提出了一个开放访问的数据集来复现结果。

Conclusion: ChunkNorris为信息检索任务提供了实用且高效的替代方案，突出了启发式方法在现实世界、资源受限的RAG用例中的潜力。

Abstract: In Retrieval-Augmented Generation applications, the Information Retrieval part is central as it provides the contextual information that enables a Large Language Model to generate an appropriate and truthful response. High quality parsing and chunking are critical as efficient data segmentation directly impacts downstream tasks, i.e. Information Retrieval and answer generation. In this paper, we introduce ChunkNorris, a novel heuristic-based technique designed to optimise the parsing and chunking of PDF documents. Our approach does not rely on machine learning and employs a suite of simple yet effective heuristics to achieve high performance with minimal computational overhead. We demonstrate the efficiency of ChunkNorris through a comprehensive benchmark against existing parsing and chunking methods, evaluating criteria such as execution time, energy consumption, and retrieval accuracy. We propose an open-access dataset to produce our results. ChunkNorris outperforms baseline and more advanced techniques, offering a practical and efficient alternative for Information Retrieval tasks. Therefore, this research highlights the potential of heuristic-based methods for real-world, resource-constrained RAG use cases.

</details>


### [656] [AutoBinder Agent: An MCP-Based Agent for End-to-End Protein Binder Design](https://arxiv.org/abs/2602.00019)
*Fukang Ge,Jiarui Zhu,Linjie Zhang,Haowen Xiao,Xiangcheng Bao,Fangnan Xie,Danyang Chen,Yanrui Lu,Yuting Wang,Ziqian Guan,Lin Gu,Jinhao Bi,Yingying Zhu*

Main category: q-bio.BM

Relevance: 65.0

TL;DR: 提出一个基于LLM和MCP协议的端到端药物设计框架，整合多种先进AI工具，实现从靶点结构到结合剂生成的自动化流程。


<details>
  <summary>Details</summary>
Motivation: 当前药物发现AI技术分散在不同平台（网页应用、桌面环境、代码库），导致工作流程碎片化、接口不一致、集成成本高，需要统一的协调框架。

Method: 使用大型语言模型（LLM）结合模型上下文协议（MCP），动态协调访问生化数据库、模块化工具链和任务特定AI模型。整合四个先进组件：MaSIF（几何深度学习识别PPI位点）、Rosetta（蛋白片段嫁接）、ProteinMPNN（氨基酸序列重设计）、AlphaFold3（复合物结构预测）。

Result: 开发了一个端到端药物设计框架，支持从靶点结构开始，通过表面分析、支架嫁接、序列优化和结构预测实现从头结合剂生成。用协议驱动的LLM协调架构替代了僵化的脚本工作流。

Conclusion: 该框架提高了药物设计过程的可重复性，减少了人工开销，确保了整个流程的可扩展性、可移植性和可审计性，解决了现有技术碎片化问题。

Abstract: Modern AI technologies for drug discovery are distributed across heterogeneous platforms-including web applications, desktop environments, and code libraries-leading to fragmented workflows, inconsistent interfaces, and high integration overhead. We present an agentic end-to-end drug design framework that leverages a Large Language Model (LLM) in conjunction with the Model Context Protocol (MCP) to dynamically coordinate access to biochemical databases, modular toolchains, and task-specific AI models. The system integrates four state-of-the-art components: MaSIF (MaSIF-site and MaSIF-seed-search) for geometric deep learning-based identification of protein-protein interaction (PPI) sites, Rosetta for grafting protein fragments onto protein backbones to form mini proteins, ProteinMPNN for amino acid sequences redesign, and AlphaFold3 for near-experimental accuracy in complex structure prediction. Starting from a target structure, the framework supports de novo binder generation via surface analysis, scaffold grafting and pose construction, sequence optimization, and structure prediction. Additionally, by replacing rigid, script-based workflows with a protocol-driven, LLM-coordinated architecture, the framework improves reproducibility, reduces manual overhead, and ensures extensibility, portability, and auditability across the entire drug design process.

</details>


### [657] [Beyond Static Question Banks: Dynamic Knowledge Expansion via LLM-Automated Graph Construction and Adaptive Generation](https://arxiv.org/abs/2602.00020)
*Yingquan Wang,Tianyu Wei,Qinsi Li,Li Zeng*

Main category: cs.CY

Relevance: 65.0

TL;DR: 提出Generative GraphRAG框架，通过LLM自动构建层次知识图谱，结合认知图谱推理生成个性化练习题，解决教育知识图谱构建成本高和个性化不足的问题。


<details>
  <summary>Details</summary>
Motivation: 现有个性化教育系统面临两大挑战：1) 教育知识图谱依赖人工构建，成本高、可扩展性差；2) 缺乏对学习者知识状态的系统推理，依赖静态题库，适应性有限。

Method: 提出两模块框架：1) Auto-HKG模块利用LLM从教育资源自动构建层次知识图谱；2) CG-RAG模块基于学习者掌握图谱进行图推理，结合检索增强生成技术产生个性化练习题。

Result: 框架已在真实教育场景中部署，获得积极用户反馈，展示了支持实际个性化教育系统的潜力。

Conclusion: Generative GraphRAG框架通过自动化知识建模和个性化练习生成，有效解决了教育知识图谱构建和维护的挑战，为实际个性化教育系统提供了可行方案。

Abstract: Personalized education systems increasingly rely on structured knowledge representations to support adaptive learning and question generation. However, existing approaches face two fundamental limitations. First, constructing and maintaining knowledge graphs for educational content largely depends on manual curation, resulting in high cost and poor scalability. Second, most personalized education systems lack effective support for state-aware and systematic reasoning over learners' knowledge, and therefore rely on static question banks with limited adaptability. To address these challenges, this paper proposes a Generative GraphRAG framework for automated knowledge modeling and personalized exercise generation. It consists of two core modules. The first module, Automated Hierarchical Knowledge Graph Constructor (Auto-HKG), leverages LLMs to automatically construct hierarchical knowledge graphs that capture structured concepts and their semantic relations from educational resources. The second module, Cognitive GraphRAG (CG-RAG), performs graph-based reasoning over a learner mastery graph and combines it with retrieval-augmented generation to produce personalized exercises that adapt to individual learning states. The proposed framework has been deployed in real-world educational scenarios, where it receives favorable user feedback, suggesting its potential to support practical personalized education systems.

</details>


### [658] [Synthetic Student Responses: LLM-Extracted Features for IRT Difficulty Parameter Estimation](https://arxiv.org/abs/2602.00034)
*Matias Hoyl*

Main category: cs.CY

Relevance: 65.0

TL;DR: 利用大语言模型提取教育特征，通过模拟学生响应预测试题难度，无需实际学生测试即可获得与IRT难度参数高度相关（r≈0.78）的预测结果。


<details>
  <summary>Details</summary>
Motivation: 传统教育评估依赖学生预测试来确定试题难度，成本高且资源密集。本研究旨在探索是否可以不通过实际学生测试，仅通过建模响应过程来准确估计IRT难度参数，为教师和评估开发者降低障碍。

Method: 采用两阶段方法：1）结合传统语言特征和LLM提取的教育学特征（解题步骤数、认知复杂度、潜在误解等）；2）训练神经网络预测学生对问题的响应，然后从模拟响应模式中推导难度参数。使用超过25万学生数学问题响应数据集。

Result: 模型在完全未见过的数学问题上，预测难度与实际IRT难度参数之间的Pearson相关系数达到约0.78，显示出较强的预测能力。

Conclusion: LLM提取的教育学特征与传统语言特征结合，可以有效预测试题难度，无需实际学生测试。这为教育评估提供了更高效、低成本的替代方案。

Abstract: Educational assessment relies heavily on knowing question difficulty, traditionally determined through resource-intensive pre-testing with students. This creates significant barriers for both classroom teachers and assessment developers. We investigate whether Item Response Theory (IRT) difficulty parameters can be accurately estimated without student testing by modeling the response process and explore the relative contribution of different feature types to prediction accuracy. Our approach combines traditional linguistic features with pedagogical insights extracted using Large Language Models (LLMs), including solution step count, cognitive complexity, and potential misconceptions. We implement a two-stage process: first training a neural network to predict how students would respond to questions, then deriving difficulty parameters from these simulated response patterns. Using a dataset of over 250,000 student responses to mathematics questions, our model achieves a Pearson correlation of approximately 0.78 between predicted and actual difficulty parameters on completely unseen questions.

</details>


### [659] [LOGOS-CA: A Cellular Automaton Using Natural Language as State and Rule](https://arxiv.org/abs/2602.00036)
*Keishu Utimula*

Main category: nlin.CG

Relevance: 65.0

TL;DR: 提出LOGOS-CA框架，用自然语言描述细胞状态和规则，利用LLM更新细胞自动机，突破传统数值状态和固定规则限制，实现更丰富的模拟能力。


<details>
  <summary>Details</summary>
Motivation: LLM在Winograd模式挑战中的优异表现表明语言能够以丰富细节描述世界。研究者希望利用语言的高表达能力来增强细胞自动机，使其超越传统数值状态和固定规则的限制，提供更丰富的模拟平台。

Method: 提出LOGOS-CA框架：1）用自然语言描述细胞状态和规则；2）将细胞更新任务委托给LLM；3）通过这种方式实现超越传统数值状态和固定规则的细胞自动机。

Result: LOGOS-CA成功执行了简单的森林火灾模拟，并从人工生命（ALife）角度提供了有趣的研究对象，验证了该框架的可行性。

Conclusion: LOGOS-CA为利用语言表达能力增强细胞自动机提供了自然框架，在模拟和人工生命研究方面具有潜力，并指出了未来研究方向。

Abstract: Large Language Models (LLMs), trained solely on massive text data, have achieved high performance on the Winograd Schema Challenge (WSC), a benchmark proposed to measure commonsense knowledge and reasoning abilities about the real world. This suggests that the language produced by humanity describes a significant portion of the world with considerable nuance. In this study, we attempt to harness the high expressive power of language within cellular automata. Specifically, we express cell states and rules in natural language and delegate their updates to an LLM. Through this approach, cellular automata can transcend the constraints of merely numerical states and fixed rules, providing us with a richer platform for simulation. Here, we propose LOGOS-CA (Language Oriented Grid Of Statements - Cellular Automaton) as a natural framework to achieve this and examine its capabilities. We confirmed that LOGOS-CA successfully performs simple forest fire simulations and also serves as an intriguing subject for investigation from an Artificial Life (ALife) perspective. In this paper, we report the results of these experiments and discuss directions for future research using LOGOS-CA.

</details>


### [660] [How Hyper-Datafication Impacts the Sustainability Costs in Frontier AI](https://arxiv.org/abs/2602.00056)
*Sophia N. Wilson,Sebastian Mair,Mophat Okinyi,Erik B. Dam,Janin Koch,Raghavendra Selvan*

Main category: cs.CY

Relevance: 65.0

TL;DR: 该论文从可持续性角度分析AI大规模数据的环境、社会和经济成本，提出"超数据化"概念，揭示数据相关成本向全球南方、不稳定数据工作者和弱势文化系统性转移的问题，并提出Data PROOFS建议框架。


<details>
  <summary>Details</summary>
Motivation: 当前AI模型的成功依赖于大规模数据，但数据收集和处理的成本往往被忽视。论文旨在揭示大规模数据在AI发展中的环境、社会和经济效益，特别是关注这些成本在不同地区和群体中的不平等分配。

Method: 1) 定量分析：分析Hugging Face Hub上约55万个数据集，关注数据集增长、存储能耗和碳足迹、语言数据的社会代表性；2) 定性分析：收集肯尼亚数据工作者的访谈数据，分析劳动条件和内容暴露；3) 补充分析：使用外部数据源验证数据中心基础设施的全球不平等分布。

Result: 研究发现超数据化不仅增加资源消耗，还系统性地将环境负担、劳动风险和代表性伤害转移到全球南方、不稳定数据工作者和弱势文化群体。数据中心的全球分布存在明显不平等，数据工作面临劳动剥削和有害内容暴露风险。

Conclusion: 论文提出Data PROOFS建议框架（来源、资源意识、所有权、开放性、节俭性、标准），旨在减轻数据相关成本，促进AI社区对数据可持续性的讨论，推动更公平、透明的数据实践。

Abstract: Large-scale data has fuelled the success of frontier artificial intelligence (AI) models over the past decade. This expansion has relied on sustained efforts by large technology corporations to aggregate and curate internet-scale datasets. In this work, we examine the environmental, social, and economic costs of large-scale data in AI through a sustainability lens. We argue that the field is shifting from building models from data to actively creating data for building models. We characterise this transition as hyper-datafication, which marks a critical juncture for the future of frontier AI and its societal impacts. To quantify and contextualise data-related costs, we analyse approximately 550,000 datasets from the Hugging Face Hub, focusing on dataset growth, storage-related energy consumption and carbon footprint, and societal representation using language data. We complement this analysis with qualitative responses from data workers in Kenya to examine the labour involved, including direct employment by big tech corporations and exposure to graphic content. We further draw on external data sources to substantiate our findings by illustrating the global disparity in data centre infrastructure. Our analyses reveal that hyper-datafication does not merely increase resource consumption but systematically redistributes environmental burdens, labour risks, and representational harms toward the Global South, precarious data workers, and under-represented cultures. Thus, we propose Data PROOFS recommendations spanning provenance, resource awareness, ownership, openness, frugality, and standards to mitigate these costs. Our work aims to make visible the often-overlooked costs of data that underpin frontier AI and to stimulate broader debate within the research community and beyond.

</details>


### [661] [Adoption and Use of LLMs at an Academic Medical Center](https://arxiv.org/abs/2602.00074)
*Nigam H. Shah,Nerissa Ambers,Abby Pandya,Timothy Keyes,Juan M. Banda,Srikar Nallan,Carlene Lugtu,Artem A. Trotsyuk,Suhana Bedi,Alyssa Unell,Miguel Fuentes,Francois Grolleau,Sneha S. Jain,Jonathan Chen,Devdutta Dash,Danton Char,Aditya Sharma,Duncan McElfresh,Patrick Scully,Vishanthan Kumar,Connor OBrien,Satchi Mouniswamy,Elvis Jones,Krishna Jasti,Gunavathi Mannika Lakshmanan,Sree Ram Akula,Varun Kumar Singh,Ramesh Rajmanickam,Sudhir Sinha,Vicky Zhou,Xu Wang,Bilal Mawji,Joshua Ge,Wencheng Li,Travis Lyons,Jarrod Helzer,Vikas Kakkar,Ramesh Powar,Darren Batara,Cheryl Cordova,William Frederick,Olivia Tang,Phoebe Morgan,April S. Liang,Stephen P. Ma,Shivam Vedak,Dong-han Yao,Akshay Swaminathan,Mehr Kashyap,Brian Ng,Jamie Hellman,Nikesh Kotecha,Christopher Sharp,Gretchen Brown,Christian Lindmark,Anurang Revri,Michael A. Pfeffer*

Main category: cs.CY

Relevance: 65.0

TL;DR: ChatEHR是一个将LLM集成到电子健康记录中的系统，支持自动化任务和交互式使用，帮助医疗专业人员处理患者时间线数据，在1.5年内开发了7个自动化功能，1075名用户接受了培训。


<details>
  <summary>Details</summary>
Motivation: 虽然LLM可以支持临床文档需求，但独立工具存在"工作流程摩擦"问题，需要手动数据输入。医疗记录通常跨越数年，需要系统能够处理整个患者时间线数据。

Method: 开发了ChatEHR系统，支持两种使用模式：1) 自动化 - 静态的提示和数据组合执行固定任务；2) 通过用户界面的交互式使用。系统是模型无关的，可以访问多种类型的数据，并与最合适的LLM匹配。

Result: 在1.5年内构建了7个自动化功能，1075名用户接受了培训成为常规用户，在发布前3个月进行了23,000次会话。总结生成是最频繁的任务，每次生成平均有0.73个幻觉和1.60个不准确。初步估计第一年可节省600万美元。

Conclusion: 这种"内部构建"策略为医疗系统提供了保持自主权的机会，通过供应商无关、内部治理的LLM平台。基准评估不足以监控UI性能，需要新的监控方法，还需要价值评估框架来量化LLM使用的影响。

Abstract: While large language models (LLMs) can support clinical documentation needs, standalone tools struggle with "workflow friction" from manual data entry. We developed ChatEHR, a system that enables the use of LLMs with the entire patient timeline spanning several years. ChatEHR enables automations - which are static combinations of prompts and data that perform a fixed task - and interactive use in the electronic health record (EHR) via a user interface (UI). The resulting ability to sift through patient medical records for diverse use-cases such as pre-visit chart review, screening for transfer eligibility, monitoring for surgical site infections, and chart abstraction, redefines LLM use as an institutional capability. This system, accessible after user-training, enables continuous monitoring and evaluation of LLM use.
  In 1.5 years, we built 7 automations and 1075 users have trained to become routine users of the UI, engaging in 23,000 sessions in the first 3 months of launch. For automations, being model-agnostic and accessing multiple types of data was essential for matching specific clinical or administrative tasks with the most appropriate LLM. Benchmark-based evaluations proved insufficient for monitoring and evaluation of the UI, requiring new methods to monitor performance. Generation of summaries was the most frequent task in the UI, with an estimated 0.73 hallucinations and 1.60 inaccuracies per generation. The resulting mix of cost savings, time savings, and revenue growth required a value assessment framework to prioritize work as well as quantify the impact of using LLMs. Initial estimates are $6M savings in the first year of use, without quantifying the benefit of the better care offered. Such a "build-from-within" strategy provides an opportunity for health systems to maintain agency via a vendor-agnostic, internally governed LLM platform.

</details>


### [662] [Autonomous Multi-Agent AI for High-Throughput Polymer Informatics: From Property Prediction to Generative Design Across Synthetic and Bio-Polymers](https://arxiv.org/abs/2602.00103)
*Mahule Roy,Adib Bazgir,Arthur da Silva Sousa Santos,Yuwen Zhang*

Main category: cond-mat.soft

Relevance: 65.0

TL;DR: 该论文提出了一个基于多智能体AI生态系统的聚合物发现框架，集成了高通量材料工作流、人工智能和计算建模，使用DeepSeek-V2和DeepSeek-Coder等大语言模型驱动专门化智能体进行科学资源检索、外部工具调用、领域代码执行和元认知自我评估。


<details>
  <summary>Details</summary>
Motivation: 聚合物发现传统上依赖实验试错和专家知识，过程耗时且昂贵。需要开发一个集成AI、计算建模和实验工作流的自动化系统，以加速聚合物材料的发现和设计过程。

Method: 构建了一个多智能体AI生态系统，包含PolyGNN代理进行聚合物性质预测，以及元认知代理框架进行性能监控和执行策略优化。系统使用大语言模型驱动的专门化智能体，能够检索科学资源、调用外部工具、执行领域特定代码，并通过多智能体共识提供不确定性估计。

Result: 在1,251个聚合物的测试集上，PolyGNN代理在玻璃化转变温度(Tg)预测达到R²=0.89，拉伸强度R²=0.82，伸长率R²=0.75，密度R²=0.91。在专门的Tg基准测试中，该方法达到R²=0.78，优于单LLM预测(R²=0.67)、基团贡献法(R²=0.71)和ChemCrow(R²=0.66)。系统具有线性复杂度扩展性，可处理至少10,000个聚合物，推理时间16.3秒，内存约2GB，成本约0.08美元。

Conclusion: 该集成多智能体AI生态系统为聚合物发现提供了高效、可扩展的解决方案，展示了LLM驱动的专门化智能体在科学发现任务中的潜力，特别是通过元认知自我评估实现持续性能优化。

Abstract: We present an integrated multiagent AI ecosystem for polymer discovery that unifies high-throughput materials workflows, artificial intelligence, and computational modeling within a single Polymer Research Lifecycle (PRL) pipeline. The system orchestrates specialized agents powered by state-of-the-art large language models (DeepSeek-V2 and DeepSeek-Coder) to retrieve and reason over scientific resources, invoke external tools, execute domain-specific code, and perform metacognitive self-assessment for robust end-to-end task execution. We demonstrate three practical capabilities: a high-fidelity polymer property prediction and generative design pipeline, a fully automated multimodal workflow for biopolymer structure characterization, and a metacognitive agent framework that can monitor performance and improve execution strategies over time. On a held-out test set of 1,251 polymers, our PolyGNN agent achieves strong predictive accuracy, reaching R2 = 0.89 for glass-transition temperature (Tg ), R2 = 0.82 for tensile strength, R2 = 0.75 for elongation, and R2 = 0.91 for density. The framework also provides uncertainty estimates via multiagent consensus and scales with linear complexity to at least 10,000 polymers, enabling high-throughput screening at low computational cost. For a representative workload, the system completes inference in 16.3 s using about 2 GB of memory and 0.1 GPU hours, at an estimated cost of about $0.08. On a dedicated Tg benchmark, our approach attains R2 = 0.78, outperforming strong baselines including single-LLM prediction (R2 = 0.67), group-contribution methods (R2 = 0.71), and ChemCrow (R2 = 0.66). We further demonstrate metacognitive control in a polystyrene case study, where the system not only produces domain-level scientific outputs but continually monitors and optimizes its own behavior through tactical, strategic, and meta-strategic self-assessment.

</details>


### [663] [PredictionMarketBench: A SWE-bench-Style Framework for Backtesting Trading Agents on Prediction Markets](https://arxiv.org/abs/2602.00133)
*Avi Arora,Ritesh Malpani*

Main category: q-fin.ST

Relevance: 65.0

TL;DR: PredictionMarketBench是一个用于评估预测市场交易代理的基准测试平台，采用历史订单簿数据回放，支持算法和LLM代理，包含Kalshi平台的加密货币、天气和体育市场数据。


<details>
  <summary>Details</summary>
Motivation: 预测市场为交易代理提供了天然测试环境（二元支付、概率解释价格），但现有评估缺乏标准化基准。需要系统评估算法和LLM代理在真实市场微观结构、费用和结算风险下的表现。

Method: 构建SWE-bench风格的基准：1）从原始交易所数据（订单簿、交易、生命周期、结算）构建确定性事件驱动回放；2）包含做市商/吃单商语义和费用建模的执行现实模拟器；3）支持经典策略和工具调用LLM代理的工具化接口。

Result: 发布四个基于Kalshi平台的事件（加密货币、天气、体育）。基线结果显示：朴素交易代理因交易成本和结算损失表现不佳，而费用感知算法策略在波动事件中保持竞争力。

Conclusion: PredictionMarketBench为评估预测市场交易代理提供了标准化基准，揭示了交易成本和结算风险的重要性，为算法和LLM代理的开发和评估提供了基础。

Abstract: Prediction markets offer a natural testbed for trading agents: contracts have binary payoffs, prices can be interpreted as probabilities, and realized performance depends critically on market microstructure, fees, and settlement risk. We introduce PredictionMarketBench, a SWE-bench-style benchmark for evaluating algorithmic and LLM-based trading agents on prediction markets via deterministic, event-driven replay of historical limit-order-book and trade data. PredictionMarketBench standardizes (i) episode construction from raw exchange streams (orderbooks, trades, lifecycle, settlement), (ii) an execution-realistic simulator with maker/taker semantics and fee modeling, and (iii) a tool-based agent interface that supports both classical strategies and tool-calling LLM agents with reproducible trajectories. We release four Kalshi-based episodes spanning cryptocurrency, weather, and sports. Baseline results show that naive trading agents can underperform due to transaction costs and settlement losses, while fee-aware algorithmic strategies remain competitive in volatile episodes.

</details>


### [664] [ProDCARL: Reinforcement Learning-Aligned Diffusion Models for De Novo Antimicrobial Peptide Design](https://arxiv.org/abs/2602.00157)
*Fang Sheng,Mohammad Noaeen,Zahra Shakeri*

Main category: q-bio.QM

Relevance: 65.0

TL;DR: ProDCARL：基于强化学习的抗菌肽生成框架，通过扩散模型生成器与序列属性预测器结合，优化抗菌活性和低毒性


<details>
  <summary>Details</summary>
Motivation: 抗菌素耐药性威胁医疗可持续性，需要低成本计算发现抗菌肽。传统似然训练生成器无法明确优化抗菌活性和安全性目标

Method: 强化学习对齐框架，结合扩散蛋白生成器（EvoDiff OA-DM 38M）与序列属性预测器（AMP活性和肽毒性）。通过微调扩散先验获得领域感知生成器，使用top-k策略梯度更新、分类器奖励加熵正则化和早停

Result: 预测AMP平均分从微调后的0.081提升至0.178；联合高质量命中率达6.3%（pAMP>0.7且pTox<0.3）；保持高多样性（1-平均成对同一性=0.929）

Conclusion: ProDCARL作为候选生成器可缩小实验搜索空间，AlphaFold3和ProtBERT分析显示候选肽具有合理的AMP样结构和语义特征，实验验证是未来工作

Abstract: Antimicrobial resistance threatens healthcare sustainability and motivates low-cost computational discovery of antimicrobial peptides (AMPs). De novo peptide generation must optimize antimicrobial activity and safety through low predicted toxicity, but likelihood-trained generators do not enforce these goals explicitly. We introduce ProDCARL, a reinforcement-learning alignment framework that couples a diffusion-based protein generator (EvoDiff OA-DM 38M) with sequence property predictors for AMP activity and peptide toxicity. We fine-tune the diffusion prior on AMP sequences to obtain a domain-aware generator. Top-k policy-gradient updates use classifier-derived rewards plus entropy regularization and early stopping to preserve diversity and reduce reward hacking. In silico experiments show ProDCARL increases the mean predicted AMP score from 0.081 after fine-tuning to 0.178. The joint high-quality hit rate reaches 6.3\% with pAMP $>$0.7 and pTox $<$0.3. ProDCARL maintains high diversity, with $1-$mean pairwise identity equal to 0.929. Qualitative analyses with AlphaFold3 and ProtBERT embeddings suggest candidates show plausible AMP-like structural and semantic characteristics. ProDCARL serves as a candidate generator that narrows experimental search space, and experimental validation remains future work.

</details>


### [665] [Towards Agentic Intelligence for Materials Science](https://arxiv.org/abs/2602.00169)
*Huan Zhang,Yizhan Li,Wenhao Huang,Ziyu Hou,Yu Song,Xuye Liu,Farshid Effaty,Jinya Jiang,Sifan Wu,Qianggang Ding,Izumi Takahara,Leonard R. MacGillivray,Teruyasu Mizoguchi,Tianshu Yu,Lizi Liao,Yuyu Luo,Yu Rong,Jia Li,Ying Diao,Heng Ji,Bang Liu*

Main category: cond-mat.mtrl-sci

Relevance: 65.0

TL;DR: 这篇综述提出了一种以流程为中心的观点，将AI与材料科学融合，从数据准备到实验平台接口构建端到端系统，强调代理式设计而非任务隔离模型，旨在实现真正的材料发现加速。


<details>
  <summary>Details</summary>
Motivation: 当前AI在材料科学中的应用多为任务隔离的微调模型，无法实现真正的发现加速。需要构建能够规划、行动和学习于整个发现循环的代理式系统，将上游设计选择与下游实验成功对齐。

Method: 采用流程中心视角，涵盖从语料库构建、预训练、领域适应、指令微调到目标条件代理与仿真/实验平台接口的完整流程。从AI和材料科学双重视角分析，对比被动反应式方法与代理式设计。

Result: 建立了跨领域统一术语和评估框架，详细分析了LLM在模式识别、预测分析和自然语言处理方面的优势，以及材料设计、工艺优化等应用。提出了实现自主、安全感知的LLM代理的实用路线图。

Conclusion: 需要从任务隔离模型转向代理式系统，通过端到端优化实现真正的材料发现加速。代理式设计能够追求长期目标，具有自主性、记忆和工具使用能力，是未来发展方向。

Abstract: The convergence of artificial intelligence and materials science presents a transformative opportunity, but achieving true acceleration in discovery requires moving beyond task-isolated, fine-tuned models toward agentic systems that plan, act, and learn across the full discovery loop. This survey advances a unique pipeline-centric view that spans from corpus curation and pretraining, through domain adaptation and instruction tuning, to goal-conditioned agents interfacing with simulation and experimental platforms. Unlike prior reviews, we treat the entire process as an end-to-end system to be optimized for tangible discovery outcomes rather than proxy benchmarks. This perspective allows us to trace how upstream design choices-such as data curation and training objectives-can be aligned with downstream experimental success through effective credit assignment.
  To bridge communities and establish a shared frame of reference, we first present an integrated lens that aligns terminology, evaluation, and workflow stages across AI and materials science. We then analyze the field through two focused lenses: From the AI perspective, the survey details LLM strengths in pattern recognition, predictive analytics, and natural language processing for literature mining, materials characterization, and property prediction; from the materials science perspective, it highlights applications in materials design, process optimization, and the acceleration of computational workflows via integration with external tools (e.g., DFT, robotic labs). Finally, we contrast passive, reactive approaches with agentic design, cataloging current contributions while motivating systems that pursue long-horizon goals with autonomy, memory, and tool use. This survey charts a practical roadmap towards autonomous, safety-aware LLM agents aimed at discovering novel and useful materials.

</details>


### [666] [Tri-LLM Cooperative Federated Zero-Shot Intrusion Detection with Semantic Disagreement and Trust-Aware Aggregation](https://arxiv.org/abs/2602.00219)
*Saeid Jamshidi,Omar Abdul Wahab,Foutse Khomh,Kawser Wazed Nafi*

Main category: cs.CR

Relevance: 65.0

TL;DR: 提出一种基于语义驱动的联邦入侵检测系统框架，利用大型语言模型生成语义攻击原型，实现开放集和零样本入侵检测，并通过信任感知聚合机制处理异构客户端问题。


<details>
  <summary>Details</summary>
Motivation: 现有联邦学习入侵检测系统主要针对封闭集学习，缺乏不确定性估计、语义泛化和对零日攻击场景中认知模糊的显式建模能力，同时在实际应用中面临异构和不可靠客户端的鲁棒性挑战。

Method: 使用GPT-4o、DeepSeek-V3和LLaMA-3-8B三种LLM构建语义攻击原型，将分布式遥测特征与高层攻击概念对齐；建模LLM间的语义分歧作为认知不确定性用于零日风险评估；采用信任感知聚合机制动态加权客户端更新。

Result: 在异构客户端上实现稳定的语义对齐和一致收敛；对未见攻击模式达到80%以上的零样本检测准确率；相比基于相似性的基线方法，零日攻击识别能力提升超过10%；在不可靠或受损客户端存在时保持低聚合不稳定性。

Conclusion: 该语义驱动的联邦入侵检测框架成功解决了现有方法在开放集检测、零日攻击识别和异构客户端鲁棒性方面的局限性，为隐私保护的分布式入侵检测提供了有效解决方案。

Abstract: Federated learning (FL) has become an effective paradigm for privacy-preserving, distributed Intrusion Detection Systems (IDS) in cyber-physical and Internet of Things (IoT) networks, where centralized data aggregation is often infeasible due to privacy and bandwidth constraints. Despite its advantages, most existing FL-based IDS assume closed-set learning and lack mechanisms such as uncertainty estimation, semantic generalization, and explicit modeling of epistemic ambiguity in zero-day attack scenarios. Additionally, robustness to heterogeneous and unreliable clients remains a challenge in practical applications. This paper introduces a semantics-driven federated IDS framework that incorporates language-derived semantic supervision into federated optimization, enabling open-set and zero-shot intrusion detection for previously unseen attack behaviors. The approach constructs semantic attack prototypes using a Tri-LLM ensemble of GPT-4o, DeepSeek-V3, and LLaMA-3-8B, aligning distributed telemetry features with high-level attack concepts. Inter-LLM semantic disagreement is modeled as epistemic uncertainty for zero-day risk estimation, while a trust-aware aggregation mechanism dynamically weights client updates based on reliability. Experimental results show stable semantic alignment across heterogeneous clients and consistent convergence. The framework achieves over 80% zero-shot detection accuracy on unseen attack patterns, improving zero-day discrimination by more than 10% compared to similarity-based baselines, while maintaining low aggregation instability in the presence of unreliable or compromised clients.

</details>


### [667] [MapDream: Task-Driven Map Learning for Vision-Language Navigation](https://arxiv.org/abs/2602.00222)
*Guoxin Lian,Shuo Wang,Yucheng Wang,Yongcai Wang,Maiyue Chen,Kaihui Wang,Bo Zhang,Zhizhong Su,Deying Li,Zhaoxin Fan*

Main category: cs.RO

Relevance: 65.0

TL;DR: MapDream：一种地图在环框架，将地图构建视为自回归鸟瞰图合成，通过联合学习地图生成和动作预测，将环境上下文蒸馏为紧凑的三通道BEV地图，在VLN任务中实现最先进的单目性能。


<details>
  <summary>Details</summary>
Motivation: 现有视觉语言导航方法大多依赖独立于导航策略的手工构建地图，作者认为地图应该是直接由导航目标塑造的学习表示，而非详尽的重建。

Method: 提出MapDream框架，将地图构建视为自回归鸟瞰图图像合成，联合学习地图生成和动作预测。使用监督预训练引导可靠的映射到控制接口，自回归设计支持通过强化微调进行端到端联合优化。

Result: 在R2R-CE和RxR-CE数据集上实现了最先进的单目性能，验证了任务驱动的生成式地图学习的有效性。

Conclusion: 地图应该是由导航目标直接塑造的学习表示，MapDream框架通过将地图构建视为自回归BEV合成，实现了任务驱动的紧凑地图表示学习，在VLN任务中表现出色。

Abstract: Vision-Language Navigation (VLN) requires agents to follow natural language instructions in partially observed 3D environments, motivating map representations that aggregate spatial context beyond local perception. However, most existing approaches rely on hand-crafted maps constructed independently of the navigation policy. We argue that maps should instead be learned representations shaped directly by navigation objectives rather than exhaustive reconstructions. Based on this insight, we propose MapDream, a map-in-the-loop framework that formulates map construction as autoregressive bird's-eye-view (BEV) image synthesis. The framework jointly learns map generation and action prediction, distilling environmental context into a compact three-channel BEV map that preserves only navigation-critical affordances. Supervised pre-training bootstraps a reliable mapping-to-control interface, while the autoregressive design enables end-to-end joint optimization through reinforcement fine-tuning. Experiments on R2R-CE and RxR-CE achieve state-of-the-art monocular performance, validating task-driven generative map learning.

</details>


### [668] [Multi-Speaker Conversational Audio Deepfake: Taxonomy, Dataset and Pilot Study](https://arxiv.org/abs/2602.00295)
*Alabi Ahmed,Vandana Janeja,Sanjay Purushotham*

Main category: cs.SD

Relevance: 65.0

TL;DR: 论文提出了多说话人对话音频深度伪造检测的新问题，创建了包含2830个音频片段的MsCADD数据集，并评估了三种基线模型，发现现有方法在多说话人对话场景下检测性能有限。


<details>
  <summary>Details</summary>
Motivation: 现有音频深度伪造检测研究主要关注单说话人场景，但现实中的恶意应用越来越多地出现在多说话人对话环境中，这是一个未被充分探索的重大威胁。需要填补多说话人对话音频深度伪造检测的研究空白。

Method: 1) 提出了多说话人对话音频深度伪造的概念分类法，区分部分操纵和完全操纵；2) 创建了MsCADD数据集，包含2830个真实和完全合成的双说话人对话音频，使用VITS和SoundStorm-based NotebookLM模型生成；3) 在数据集上评估了三种基线模型：LFCC-LCNN、RawNet2和Wav2Vec 2.0。

Result: 基线模型提供了有用的基准，但结果显示在多说话人深度伪造检测方面存在显著差距。这些模型在变化的对话动态下可靠检测合成语音的能力有限，表明需要更专门的研究。

Conclusion: 多说话人对话音频深度伪造检测是一个高度未被探索但威胁重大的研究领域。MsCADD数据集和基准为未来研究提供了基础，支持可重复性和基准测试。

Abstract: The rapid advances in text-to-speech (TTS) technologies have made audio deepfakes increasingly realistic and accessible, raising significant security and trust concerns. While existing research has largely focused on detecting single-speaker audio deepfakes, real-world malicious applications with multi-speaker conversational settings is also emerging as a major underexplored threat. To address this gap, we propose a conceptual taxonomy of multi-speaker conversational audio deepfakes, distinguishing between partial manipulations (one or multiple speakers altered) and full manipulations (entire conversations synthesized). As a first step, we introduce a new Multi-speaker Conversational Audio Deepfakes Dataset (MsCADD) of 2,830 audio clips containing real and fully synthetic two-speaker conversations, generated using VITS and SoundStorm-based NotebookLM models to simulate natural dialogue with variations in speaker gender, and conversational spontaneity. MsCADD is limited to text-to-speech (TTS) types of deepfake. We benchmark three neural baseline models; LFCC-LCNN, RawNet2, and Wav2Vec 2.0 on this dataset and report performance in terms of F1 score, accuracy, true positive rate (TPR), and true negative rate (TNR). Results show that these baseline models provided a useful benchmark, however, the results also highlight that there is a significant gap in multi-speaker deepfake research in reliably detecting synthetic voices under varied conversational dynamics. Our dataset and benchmarks provide a foundation for future research on deepfake detection in conversational scenarios, which is a highly underexplored area of research but also a major area of threat to trustworthy information in audio settings. The MsCADD dataset is publicly available to support reproducibility and benchmarking by the research community.

</details>


### [669] [A Conditional Companion: Lived Experiences of People with Mental Health Disorders Using LLMs](https://arxiv.org/abs/2602.00402)
*Aditya Kumar Purohit,Hendrik Heuer*

Main category: cs.HC

Relevance: 65.0

TL;DR: 该研究通过20次半结构化访谈，探索了心理健康挑战者使用大语言模型进行心理支持的体验、评价和设计需求，发现用户有条件地使用LLMs处理轻度至中度困扰，但会设定明确边界避免用于危机情况。


<details>
  <summary>Details</summary>
Motivation: 尽管LLMs越来越多地用于心理健康支持，但缺乏对心理健康挑战者实际使用体验、评价标准和设计需求的实证研究。研究旨在填补这一空白，了解用户如何与LLMs互动、评估其有用性，并探索负责任的设计机会。

Method: 对英国20名患有心理健康状况并使用过LLMs进行心理支持的人进行半结构化访谈，采用反思性主题分析方法分析数据。

Result: 研究发现参与者有条件地使用LLMs：追求即时性、无评判环境、自我节奏的披露、认知重构和关系互动。同时，基于既往治疗经验，参与者设定了明确边界：LLMs适用于轻度至中度困扰，但不适用于危机、创伤和复杂社会情感情境。

Conclusion: 研究提供了LLMs在心理健康领域实际使用的实证见解，强调边界设定对其安全角色的重要性，并提出了在护理生态系统中负责任嵌入LLMs的设计和治理方向。

Abstract: Large Language Models (LLMs) are increasingly used for mental health support, yet little is known about how people with mental health challenges engage with them, how they evaluate their usefulness, and what design opportunities they envision. We conducted 20 semi-structured interviews with people in the UK who live with mental health conditions and have used LLMs for mental health support. Through reflexive thematic analysis, we found that participants engaged with LLMs in conditional and situational ways: for immediacy, the desire for non-judgement, self-paced disclosure, cognitive reframing, and relational engagement. Simultaneously, participants articulated clear boundaries informed by prior therapeutic experience: LLMs were effective for mild-to-moderate distress but inadequate for crises, trauma, and complex social-emotional situations. We contribute empirical insights into the lived use of LLMs for mental health, highlight boundary-setting as central to their safe role, and propose design and governance directions for embedding them responsibly within care ecosystem.

</details>


### [670] [Augmenting Clinical Decision-Making with an Interactive and Interpretable AI Copilot: A Real-World User Study with Clinicians in Nephrology and Obstetrics](https://arxiv.org/abs/2602.00726)
*Yinghao Zhu,Dehao Sui,Zixiang Wang,Xuning Hu,Lei Gu,Yifan Qi,Tianchen Wu,Ling Wang,Yuan Wei,Wen Tang,Zhihan Cui,Yasha Wang,Lequan Yu,Ewen M Harrison,Junyi Gao,Liantao Ma*

Main category: cs.HC

Relevance: 65.0

TL;DR: AICare是一个交互式、可解释的AI医疗助手，通过可视化风险预测和LLM诊断建议支持临床决策，研究发现能降低认知负荷并建立信任，但不同经验水平的医生使用策略不同。


<details>
  <summary>Details</summary>
Motivation: 临床医生对不透明AI的怀疑阻碍了高风险医疗领域的AI采用。需要开发交互式和可解释的AI系统，作为透明合作伙伴支持临床决策，而不是替代医生判断。

Method: 开发AICare系统，分析纵向电子健康记录，提供动态风险预测的可视化和LLM驱动的诊断建议。采用受试者内平衡设计，对16名肾内科和产科医生进行综合评估，包括客观指标（任务完成时间和错误率）、主观评估（NASA-TLX、SUS、信心评分）和半结构化访谈。

Result: AICare降低了认知负荷。定性分析显示信任是通过验证主动构建的：初级医生将系统作为认知支架来结构化分析，而专家则进行对抗性验证来挑战AI逻辑。系统能适应不同的推理风格，增强而非替代临床判断。

Conclusion: 这项工作为创建透明AI合作伙伴提供了设计启示，能够适应不同的推理风格，增强而非替代临床判断，促进AI在医疗领域的可信采用。

Abstract: Clinician skepticism toward opaque AI hinders adoption in high-stakes healthcare. We present AICare, an interactive and interpretable AI copilot for collaborative clinical decision-making. By analyzing longitudinal electronic health records, AICare grounds dynamic risk predictions in scrutable visualizations and LLM-driven diagnostic recommendations. Through a within-subjects counterbalanced study with 16 clinicians across nephrology and obstetrics, we comprehensively evaluated AICare using objective measures (task completion time and error rate), subjective assessments (NASA-TLX, SUS, and confidence ratings), and semi-structured interviews. Our findings indicate AICare's reduced cognitive workload. Beyond performance metrics, qualitative analysis reveals that trust is actively constructed through verification, with interaction strategies diverging by expertise: junior clinicians used the system as cognitive scaffolding to structure their analysis, while experts engaged in adversarial verification to challenge the AI's logic. This work offers design implications for creating AI systems that function as transparent partners, accommodating diverse reasoning styles to augment rather than replace clinical judgment.

</details>


### [671] [FinEvo: From Isolated Backtests to Ecological Market Games for Multi-Agent Financial Strategy Evolution](https://arxiv.org/abs/2602.00948)
*Mingxi Zou,Jiaxiang Chen,Aotian Luo,Jingyi Dai,Chi Zhang,Dongning Sun,Zenglin Xu*

Main category: physics.soc-ph

Relevance: 65.0

TL;DR: FinEvo：将金融策略评估重构为生态博弈形式主义，研究多智能体金融市场中的进化动态


<details>
  <summary>Details</summary>
Motivation: 传统金融策略评估依赖于静态环境中的孤立回测，无法捕捉策略间的相关性、相互作用以及市场演化中的动态变化。需要一种能够研究策略在演化市场中的适应性和进化动态的新方法。

Method: 提出FinEvo生态博弈形式主义，包含两个层面：个体层面，异质ML交易者（基于规则、深度学习、强化学习、LLM代理）根据历史价格和外部新闻等信号进行适应；群体层面，通过选择、创新和环境扰动三种机制演化策略分布。

Result: 实验表明FinEvo在外部冲击和真实新闻流下既稳定又可重现，能够揭示情境依赖的结果。策略可能主导、崩溃或形成联盟，这些模式在静态回测中不可见。

Conclusion: 通过将策略评估重构为生态博弈形式主义，FinEvo为分析多智能体金融市场的鲁棒性、适应性和涌现动态提供了统一的机制层面协议，可用于探索宏观经济政策和金融监管对价格演化和均衡的潜在影响。

Abstract: Conventional financial strategy evaluation relies on isolated backtests in static environments. Such evaluations assess each policy independently, overlook correlations and interactions, and fail to explain why strategies ultimately persist or vanish in evolving markets. We shift to an ecological perspective, where trading strategies are modeled as adaptive agents that interact and learn within a shared market. Instead of proposing a new strategy, we present FinEvo, an ecological game formalism for studying the evolutionary dynamics of multi-agent financial strategies. At the individual level, heterogeneous ML-based traders-rule-based, deep learning, reinforcement learning, and large language model (LLM) agents-adapt using signals such as historical prices and external news. At the population level, strategy distributions evolve through three designed mechanisms-selection, innovation, and environmental perturbation-capturing the dynamic forces of real markets. Together, these two layers of adaptation link evolutionary game theory with modern learning dynamics, providing a principled environment for studying strategic behavior. Experiments with external shocks and real-world news streams show that FinEvo is both stable for reproducibility and expressive in revealing context-dependent outcomes. Strategies may dominate, collapse, or form coalitions depending on their competitors-patterns invisible to static backtests. By reframing strategy evaluation as an ecological game formalism, FinEvo provides a unified, mechanism-level protocol for analyzing robustness, adaptation, and emergent dynamics in multi-agent financial markets, and may offer a means to explore the potential impact of macroeconomic policies and financial regulations on price evolution and equilibrium.

</details>


### [672] [HierCon: Hierarchical Contrastive Attention for Audio Deepfake Detection](https://arxiv.org/abs/2602.01032)
*Zhili Nicholas Liang,Soyeon Caren Han,Qizhou Wang,Christopher Leckie*

Main category: cs.SD

Relevance: 65.0

TL;DR: HierCon：一种用于音频深度伪造检测的分层注意力框架，通过建模时间帧、相邻层和层组之间的依赖关系，结合基于边界的对比学习，在ASVspoof 2021 DF和In-the-Wild数据集上达到SOTA性能（1.93%和6.87% EER）。


<details>
  <summary>Details</summary>
Motivation: 现代TTS和语音转换系统生成的音频深度伪造越来越难以与真实语音区分，对安全和在线信任构成严重风险。现有的检测器将自监督模型的不同层独立处理，忽视了识别合成伪影所需的关键时间依赖性和层次依赖性。

Method: 提出HierCon框架：1）分层层注意力机制，建模时间帧、相邻层和层组之间的依赖关系；2）基于边界的对比学习，鼓励领域不变的嵌入表示；3）通过注意力可视化分析层次建模的效果。

Result: 在ASVspoof 2021 DF数据集上达到1.93% EER，在In-the-Wild数据集上达到6.87% EER，相比独立层加权方法分别提升36.6%和22.5%。分层建模增强了跨域生成技术和录音条件的泛化能力。

Conclusion: 分层建模时间、层间和层组依赖关系对于音频深度伪造检测至关重要，HierCon框架通过注意力机制和对比学习有效提升了检测性能和对未知生成技术的泛化能力。

Abstract: Audio deepfakes generated by modern TTS and voice conversion systems are increasingly difficult to distinguish from real speech, raising serious risks for security and online trust. While state-of-the-art self-supervised models provide rich multi-layer representations, existing detectors treat layers independently and overlook temporal and hierarchical dependencies critical for identifying synthetic artefacts. We propose HierCon, a hierarchical layer attention framework combined with margin-based contrastive learning that models dependencies across temporal frames, neighbouring layers, and layer groups, while encouraging domain-invariant embeddings. Evaluated on ASVspoof 2021 DF and In-the-Wild datasets, our method achieves state-of-the-art performance (1.93% and 6.87% EER), improving over independent layer weighting by 36.6% and 22.5% respectively. The results and attention visualisations confirm that hierarchical modelling enhances generalisation to cross-domain generation techniques and recording conditions.

</details>


### [673] [TLDiffGAN: A Latent Diffusion-GAN Framework with Temporal Information Fusion for Anomalous Sound Detection](https://arxiv.org/abs/2602.01060)
*Chengyuan Ma,Peng Jia,Hongyue Guo,Wenming Yang*

Main category: cs.SD

Relevance: 65.0

TL;DR: TLDiffGAN：一种结合潜在扩散模型和GAN的无监督异常声音检测框架，通过双分支结构从原始音频和梅尔频谱图捕捉正常声音特征，并引入TMixup增强技术提升对细微时间模式的敏感性。


<details>
  <summary>Details</summary>
Motivation: 现有无监督异常声音检测生成模型难以完全捕捉正常声音的复杂特征分布，而强大的扩散模型在该领域潜力尚未充分探索。需要开发能更好建模正常声音分布的方法。

Method: 提出TLDiffGAN双分支框架：1）将潜在扩散模型集成到GAN生成器中进行对抗训练，提高生成质量；2）利用预训练音频模型编码器从原始音频波形提取特征进行辅助判别；3）引入TMixup频谱图增强技术提升对细微时间模式的敏感性。

Result: 在DCASE 2020 Challenge Task 2数据集上的实验表明，TLDiffGAN在异常检测性能上表现优越，同时在异常时频定位方面展现出强大能力。

Conclusion: TLDiffGAN通过结合扩散模型和GAN的优势，有效捕捉正常声音特征分布，为无监督异常声音检测提供了新思路，并在检测性能和时频定位能力上取得显著提升。

Abstract: Existing generative models for unsupervised anomalous sound detection are limited by their inability to fully capture the complex feature distribution of normal sounds, while the potential of powerful diffusion models in this domain remains largely unexplored. To address this challenge, we propose a novel framework, TLDiffGAN, which consists of two complementary branches. One branch incorporates a latent diffusion model into the GAN generator for adversarial training, thereby making the discriminator's task more challenging and improving the quality of generated samples. The other branch leverages pretrained audio model encoders to extract features directly from raw audio waveforms for auxiliary discrimination. This framework effectively captures feature representations of normal sounds from both raw audio and Mel spectrograms. Moreover, we introduce a TMixup spectrogram augmentation technique to enhance sensitivity to subtle and localized temporal patterns that are often overlooked. Extensive experiments on the DCASE 2020 Challenge Task 2 dataset demonstrate the superior detection performance of TLDiffGAN, as well as its strong capability in anomalous time-frequency localization.

</details>


### [674] [How well can VLMs rate audio descriptions: A multi-dimensional quantitative assessment framework](https://arxiv.org/abs/2602.01390)
*Lana Do,Gio Jung,Juvenal Francisco Barajas,Andrew Taylor Scott,Shasta Ihorn,Alexander Mario Blum,Vassilis Athitsos,Ilmi Yoon*

Main category: cs.HC

Relevance: 65.0

TL;DR: 论文提出了一个评估视频音频描述质量的多维度框架，并利用项目反应理论评估VLM和人类评分者的能力，发现VLM能近似专家评分但推理可靠性不足，建议混合评估系统。


<details>
  <summary>Details</summary>
Motivation: 数字视频对盲人和低视力人群的访问性不足，现有音频描述质量评估主要依赖NLP指标和短片段指南，缺乏对完整视频质量评估的系统方法。

Method: 1) 基于专业指南开发了完整视频的多维度评估框架；2) 整合项目反应理论建立方法论工作流，评估VLM和人类评分者相对于专家基准的熟练度。

Result: VLM能够高度对齐地近似基准评分，但其推理可靠性和可操作性不如人类评分者，显示了混合评估系统的潜力。

Conclusion: 需要结合VLM和人类监督的混合评估系统，为可扩展的音频描述质量控制提供路径。

Abstract: Digital video is central to communication, education, and entertainment, but without audio description (AD), blind and low-vision audiences are excluded. While crowdsourced platforms and vision-language-models (VLMs) expand AD production, quality is rarely checked systematically. Existing evaluations rely on NLP metrics and short-clip guidelines, leaving questions about what constitutes quality for full-length content and how to assess it at scale. To address these questions, we first developed a multi-dimensional assessment framework for uninterrupted, full-length video, grounded in professional guidelines and refined by accessibility specialists. Second, we integrated this framework into a comprehensive methodological workflow, utilizing Item Response Theory, to assess the proficiency of VLM and human raters against expert-established ground truth. Findings suggest that while VLMs can approximate ground-truth ratings with high alignment, their reasoning was found to be less reliable and actionable than that of human respondents. These insights show the potential of hybrid evaluation systems that leverage VLMs alongside human oversight, offering a path towards scalable AD quality control.

</details>


### [675] [GRAB: An LLM-Inspired Sequence-First Click-Through Rate Prediction Modeling Paradigm](https://arxiv.org/abs/2602.01865)
*Shaopeng Chen,Chuyue Xie,Huimin Ren,Shaozong Zhang,Han Zhang,Ruobing Cheng,Zhiqiang Cao,Zehao Ju,Gao Yu,Jie Ding,Xiaodong Chen,Xuewu Jiao,Shuanglong Li,Liu Lin*

Main category: cs.IR

Relevance: 65.0

TL;DR: 提出GRAB框架，使用生成式方法进行CTR预测，通过新颖的注意力机制捕捉用户行为序列的时序动态和特定动作信号，在百度广告系统中显著提升收入和点击率。


<details>
  <summary>Details</summary>
Motivation: 传统深度学习推荐模型在性能和效率上面临瓶颈，特别是在泛化能力和长序列建模方面存在困难。受到大语言模型扩展成功的启发，研究者希望将生成式方法应用于点击率预测任务。

Method: 提出GRAB框架，采用端到端的生成式方法进行CTR预测。核心是新颖的因果动作感知多通道注意力机制（CamA），能够有效捕捉用户行为序列中的时序动态和特定动作信号。

Result: 在百度广告系统全规模在线部署中，GRAB显著优于传统DLRMs，带来3.05%的收入增长和3.49%的CTR提升。模型还展现出良好的扩展性：表达能力随着使用更长的交互序列而单调且近似线性地提升。

Conclusion: 生成式方法在推荐系统中有显著优势，GRAB框架通过创新的注意力机制有效解决了传统推荐模型的局限性，在实际部署中取得了显著的业务效果提升。

Abstract: Traditional Deep Learning Recommendation Models (DLRMs) face increasing bottlenecks in performance and efficiency, often struggling with generalization and long-sequence modeling. Inspired by the scaling success of Large Language Models (LLMs), we propose Generative Ranking for Ads at Baidu (GRAB), an end-to-end generative framework for Click-Through Rate (CTR) prediction. GRAB integrates a novel Causal Action-aware Multi-channel Attention (CamA) mechanism to effectively capture temporal dynamics and specific action signals within user behavior sequences. Full-scale online deployment demonstrates that GRAB significantly outperforms established DLRMs, delivering a 3.05% increase in revenue and a 3.49% rise in CTR. Furthermore, the model demonstrates desirable scaling behavior: its expressive power shows a monotonic and approximately linear improvement as longer interaction sequences are utilized.

</details>


### [676] [Bandwidth-Efficient Multi-Agent Communication through Information Bottleneck and Vector Quantization](https://arxiv.org/abs/2602.02035)
*Ahmad Farooq,Kamran Iqbal*

Main category: cs.RO

Relevance: 65.0

TL;DR: 提出结合信息瓶颈理论和向量量化的多智能体强化学习通信框架，在带宽受限环境中实现选择性高效通信，性能提升181.8%，带宽减少41.4%


<details>
  <summary>Details</summary>
Motivation: 现实世界机器人应用中的多智能体强化学习系统面临严重通信约束，显著影响协调效果。需要一种能够在带宽受限环境中实现选择性高效通信的框架。

Method: 结合信息瓶颈理论和向量量化，学习压缩和离散化通信消息，同时通过信息论优化保留任务关键信息。引入门控通信机制，基于环境上下文和智能体状态动态确定何时需要通信。

Result: 在挑战性协调任务上，相比无通信基线性能提升181.8%，带宽使用减少41.4%。全面的帕累托前沿分析显示在整个成功率-带宽谱上具有主导地位，AUC为0.198 vs 次优方法的0.142。

Conclusion: 该方法显著优于现有通信策略，为在机器人集群、自动驾驶车队和分布式传感器网络等带宽受限环境中部署多智能体系统建立了理论基础框架。

Abstract: Multi-agent reinforcement learning systems deployed in real-world robotics applications face severe communication constraints that significantly impact coordination effectiveness. We present a framework that combines information bottleneck theory with vector quantization to enable selective, bandwidth-efficient communication in multi-agent environments. Our approach learns to compress and discretize communication messages while preserving task-critical information through principled information-theoretic optimization. We introduce a gated communication mechanism that dynamically determines when communication is necessary based on environmental context and agent states. Experimental evaluation on challenging coordination tasks demonstrates that our method achieves 181.8% performance improvement over no-communication baselines while reducing bandwidth usage by 41.4%. Comprehensive Pareto frontier analysis shows dominance across the entire success-bandwidth spectrum with area-under-curve of 0.198 vs 0.142 for next-best methods. Our approach significantly outperforms existing communication strategies and establishes a theoretically grounded framework for deploying multi-agent systems in bandwidth-constrained environments such as robotic swarms, autonomous vehicle fleets, and distributed sensor networks.

</details>


### [677] [See2Refine: Vision-Language Feedback Improves LLM-Based eHMI Action Designers](https://arxiv.org/abs/2602.02063)
*Ding Xia,Xinyue Gui,Mark Colley,Fan Gao,Zhongyi Zhou,Dongyuan Li,Renhe Jiang,Takeo Igarashi*

Main category: cs.HC

Relevance: 65.0

TL;DR: See2Refine：一个无需人工的闭环框架，利用视觉语言模型（VLM）的感知评估作为自动视觉反馈，改进基于LLM的外部人机界面（eHMI）动作设计器，实现自动驾驶车辆在动态交通环境中的意图传达。


<details>
  <summary>Details</summary>
Motivation: 自动驾驶车辆缺乏与其他道路使用者的自然沟通渠道，需要外部人机界面（eHMI）来传达意图。现有eHMI研究大多依赖开发者手工设计的消息-动作对，难以适应多样动态的交通环境。虽然LLM可以作为动作设计器生成上下文条件化的eHMI动作，但缺乏感知验证且依赖固定提示或昂贵的人工标注反馈。

Method: 提出See2Refine框架：1）给定驾驶上下文和候选eHMI动作，使用VLM评估动作的感知适当性；2）将VLM的评估反馈用于迭代修订LLM设计器的输出；3）形成无需人工监督的闭环系统，支持系统化精炼。在三种eHMI模态（光条、眼睛、手臂）和多种LLM模型规模上进行评估。

Result: 在所有设置中，该框架在VLM指标和人类主体评估方面均一致优于仅使用提示的LLM设计器和手动指定的基线。改进效果在不同模态间具有泛化性，且VLM评估与人类偏好高度一致，支持See2Refine在可扩展动作设计中的鲁棒性和有效性。

Conclusion: See2Refine通过VLM感知评估作为自动反馈，有效提升了LLM基eHMI动作设计器的性能，实现了无需人工监督的系统化精炼，为自动驾驶车辆在共享环境中建立信任提供了可扩展的解决方案。

Abstract: Automated vehicles lack natural communication channels with other road users, making external Human-Machine Interfaces (eHMIs) essential for conveying intent and maintaining trust in shared environments. However, most eHMI studies rely on developer-crafted message-action pairs, which are difficult to adapt to diverse and dynamic traffic contexts. A promising alternative is to use Large Language Models (LLMs) as action designers that generate context-conditioned eHMI actions, yet such designers lack perceptual verification and typically depend on fixed prompts or costly human-annotated feedback for improvement. We present See2Refine, a human-free, closed-loop framework that uses vision-language model (VLM) perceptual evaluation as automated visual feedback to improve an LLM-based eHMI action designer. Given a driving context and a candidate eHMI action, the VLM evaluates the perceived appropriateness of the action, and this feedback is used to iteratively revise the designer's outputs, enabling systematic refinement without human supervision. We evaluate our framework across three eHMI modalities (lightbar, eyes, and arm) and multiple LLM model sizes. Across settings, our framework consistently outperforms prompt-only LLM designers and manually specified baselines in both VLM-based metrics and human-subject evaluations. Results further indicate that the improvements generalize across modalities and that VLM evaluations are well aligned with human preferences, supporting the robustness and effectiveness of See2Refine for scalable action design.

</details>


### [678] [Rethinking Generative Recommender Tokenizer: Recsys-Native Encoding and Semantic Quantization Beyond LLMs](https://arxiv.org/abs/2602.02338)
*Yu Liang,Zhongjin Zhang,Yuxuan Zhu,Kerui Zhang,Zhiluohan Guo,Wenhang Zhou,Zonqi Yang,Kangle Wu,Yabo Ni,Anxiang Zeng,Cong Fu,Jianxin Wang,Jiazhi Xia*

Main category: cs.IR

Relevance: 65.0

TL;DR: ReSID提出了一种推荐原生的语义ID框架，通过场感知掩码自编码和全局对齐正交量化，在不依赖LLM的情况下优化序列推荐系统的表示学习和量化过程。


<details>
  <summary>Details</summary>
Motivation: 现有语义ID推荐方法主要遵循语义中心化流程：从基础模型学习物品嵌入并使用通用量化方案离散化。这种设计与生成式推荐目标不匹配：语义嵌入与协同预测弱耦合，通用量化在减少自回归建模的序列不确定性方面效率低下。

Method: ReSID包含两个组件：1) 场感知掩码自编码(FAMAE)，从结构化特征学习预测充分的物品表示；2) 全局对齐正交量化(GAOQ)，通过联合减少语义模糊性和前缀条件不确定性，产生紧凑且可预测的SID序列。

Result: 在十个数据集上的实验表明，ReSID持续优于强序列和基于SID的生成基线，平均提升超过10%，同时将标记化成本降低高达122倍。

Conclusion: ReSID提供了一个推荐原生的、原则性的语义ID框架，从信息保存和序列可预测性角度重新思考表示学习和量化，无需依赖LLM，在性能和效率方面均有显著提升。

Abstract: Semantic ID (SID)-based recommendation is a promising paradigm for scaling sequential recommender systems, but existing methods largely follow a semantic-centric pipeline: item embeddings are learned from foundation models and discretized using generic quantization schemes. This design is misaligned with generative recommendation objectives: semantic embeddings are weakly coupled with collaborative prediction, and generic quantization is inefficient at reducing sequential uncertainty for autoregressive modeling. To address these, we propose ReSID, a recommendation-native, principled SID framework that rethinks representation learning and quantization from the perspective of information preservation and sequential predictability, without relying on LLMs. ReSID consists of two components: (i) Field-Aware Masked Auto-Encoding (FAMAE), which learns predictive-sufficient item representations from structured features, and (ii) Globally Aligned Orthogonal Quantization (GAOQ), which produces compact and predictable SID sequences by jointly reducing semantic ambiguity and prefix-conditional uncertainty. Theoretical analysis and extensive experiments across ten datasets show the effectiveness of ReSID. ReSID consistently outperforms strong sequential and SID-based generative baselines by an average of over 10%, while reducing tokenization cost by up to 122x. Code is available at https://github.com/FuCongResearchSquad/ReSID.

</details>


### [679] [World-Gymnast: Training Robots with Reinforcement Learning in a World Model](https://arxiv.org/abs/2602.02454)
*Ansh Kumar Sharma,Yixiang Sun,Ninghao Lu,Yunzhe Zhang,Jiarao Liu,Sherry Yang*

Main category: cs.RO

Relevance: 65.0

TL;DR: World-Gymnast：通过在动作条件视频世界模型中执行强化学习微调，显著提升机器人性能，超越监督微调和软件模拟方法


<details>
  <summary>Details</summary>
Motivation: 机器人学习面临物理交互成本高的问题。监督微调受限于专家数据量，软件模拟存在仿真到现实的差距。世界模型从真实世界视频-动作数据学习，探索在世界模型中训练策略是否比监督学习或软件模拟更有效。

Method: 提出World-Gymnast方法：1) 在动作条件视频世界模型中进行策略展开；2) 使用视觉语言模型对展开进行奖励；3) 对视觉语言动作策略进行强化学习微调；4) 支持多样语言指令和新场景训练。

Result: 在Bridge机器人设置中，World-Gymnast比监督微调性能提升高达18倍，比软件模拟提升高达2倍。展示了世界模型中RL的有趣能力：多样化语言指令训练、新场景训练、测试时训练、在线迭代世界模型和策略改进。

Conclusion: 学习世界模型并在云端训练机器人策略可能是弥合演示机器人与家庭实用机器人之间差距的关键。世界模型中的强化学习为机器人学习提供了有前景的替代方案。

Abstract: Robot learning from interacting with the physical world is fundamentally bottlenecked by the cost of physical interaction. The two alternatives, supervised finetuning (SFT) from expert demonstrations and reinforcement learning (RL) in a software-based simulator, are limited by the amount of expert data available and the sim-to-real gap for manipulation. With the recent emergence of world models learned from real-world video-action data, we ask the question of whether training a policy in a world model can be more effective than supervised learning or software simulation in achieving better real-robot performance. We propose World-Gymnast, which performs RL finetuning of a vision-language-action (VLA) policy by rolling out the policy in an action-conditioned video world model and rewarding the rollouts with a vision-language model (VLM). On the Bridge robot setup, World-Gymnast outperforms SFT by as much as 18x and outperforms software simulator by as much as 2x. More importantly, World-Gymnast demonstrates intriguing capabilities of RL with a world model, including training on diverse language instructions and novel scenes from the world model, test-time training in a novel scene, and online iterative world model and policy improvement. Our results suggest learning a world model and training robot policies in the cloud could be the key to bridging the gap between robots that work in demonstrations and robots that can work in anyone's household.

</details>


### [680] [Flow Policy Gradients for Robot Control](https://arxiv.org/abs/2602.02481)
*Brent Yi,Hongsuk Choi,Himanshu Gaurav Singh,Xiaoyu Huang,Takara E. Truong,Carmelo Sferrazza,Yi Ma,Rocky Duan,Pieter Abbeel,Guanya Shi,Karen Liu,Angjoo Kanazawa*

Main category: cs.RO

Relevance: 65.0

TL;DR: 本文提出了一种基于流匹配策略梯度的新方法，用于训练机器人控制策略，该方法无需计算似然函数，能够处理更复杂的策略分布，在多种机器人控制任务中表现优异。


<details>
  <summary>Details</summary>
Motivation: 传统的基于似然的策略梯度方法依赖于可微的动作似然函数，这限制了策略输出只能使用简单分布（如高斯分布）。作者希望突破这一限制，利用流匹配框架来训练更具表达能力的策略。

Method: 提出了基于流匹配策略梯度的改进目标函数，该框架绕过了似然计算，能够处理更复杂的策略分布。方法包括训练动态分析和消融研究。

Result: 在足式运动、人形运动跟踪和操作任务中取得成功，并在两个人形机器人上实现了稳健的模拟到现实迁移。策略能够利用流表示进行探索，并且在微调鲁棒性方面优于基线方法。

Conclusion: 流匹配策略梯度方法能够有效训练和微调更具表达能力的机器人控制策略，在复杂控制任务中表现优异，为机器人强化学习提供了新的技术路径。

Abstract: Likelihood-based policy gradient methods are the dominant approach for training robot control policies from rewards. These methods rely on differentiable action likelihoods, which constrain policy outputs to simple distributions like Gaussians. In this work, we show how flow matching policy gradients -- a recent framework that bypasses likelihood computation -- can be made effective for training and fine-tuning more expressive policies in challenging robot control settings. We introduce an improved objective that enables success in legged locomotion, humanoid motion tracking, and manipulation tasks, as well as robust sim-to-real transfer on two humanoid robots. We then present ablations and analysis on training dynamics. Results show how policies can exploit the flow representation for exploration when training from scratch, as well as improved fine-tuning robustness over baselines.

</details>


### [681] [DockSmith: Scaling Reliable Coding Environments via an Agentic Docker Builder](https://arxiv.org/abs/2602.00592)
*Jiaran Zhang,Luck Ma,Yanhao Li,Fanqi Wan,Di Qi,Xu Zhao,Jieyi Hou,Zhe Xie,Mengqiang Ren,Xin Wu,Zhewei Huang,Liangyu Chen,Yingwei Ma,Qi Han,Xiangyu Zhang*

Main category: cs.AI

Relevance: 45.0

TL;DR: DockSmith：一个专门用于Docker环境构建的智能代理，将环境构建视为核心智能能力而非预处理步骤，通过大规模执行轨迹训练，在Docker构建任务上达到开源SOTA，并能提升其他软件工程代理任务的性能。


<details>
  <summary>Details</summary>
Motivation: 基于Docker的环境构建是软件工程智能代理训练和评估的主要瓶颈，传统方法将其视为预处理步骤，但作者认为这应该是一个核心的智能能力，涉及长时程工具使用、依赖推理和故障恢复等能力。

Method: 开发DockSmith代理，通过SWE-Factory风格流水线生成大规模执行基础的Docker构建轨迹，加入循环检测控制器和跨任务成功记忆机制，训练30B-A3B模型。

Result: 在Multi-Docker-Eval上达到开源SOTA：39.72% Fail-to-Pass率和58.28% Commit率；在SWE-bench Verified、SWE-bench Multilingual和Terminal-Bench 2.0等分布外任务上也有性能提升。

Conclusion: DockSmith不仅解决了Docker环境构建的瓶颈问题，还将环境构建训练为一种可迁移的智能能力，能够提升软件工程代理的整体性能。

Abstract: Reliable Docker-based environment construction is a dominant bottleneck for scaling execution-grounded training and evaluation of software engineering agents. We introduce DockSmith, a specialized agentic Docker builder designed to address this challenge. DockSmith treats environment construction not only as a preprocessing step, but as a core agentic capability that exercises long-horizon tool use, dependency reasoning, and failure recovery, yielding supervision that transfers beyond Docker building itself. DockSmith is trained on large-scale, execution-grounded Docker-building trajectories produced by a SWE-Factory-style pipeline augmented with a loop-detection controller and a cross-task success memory. Training a 30B-A3B model on these trajectories achieves open-source state-of-the-art performance on Multi-Docker-Eval, with 39.72% Fail-to-Pass and 58.28% Commit Rate. Moreover, DockSmith improves out-of-distribution performance on SWE-bench Verified, SWE-bench Multilingual, and Terminal-Bench 2.0, demonstrating broader agentic benefits of environment construction.

</details>


### [682] [The Keyhole Effect: Why Chat Interfaces Fail at Data Analysis](https://arxiv.org/abs/2602.00947)
*Mohan Reddy*

Main category: cs.AI

Relevance: 45.0

TL;DR: 论文批判聊天界面作为AI辅助数据分析的默认接口，提出聊天界面通过五种机制系统性降低分析性能，并提出了八种混合设计模式来解决这些问题。


<details>
  <summary>Details</summary>
Motivation: 当前聊天已成为AI辅助数据分析的默认接口，但对于多步骤、状态依赖的分析任务，这种界面设计存在问题。作者基于Woods（1984）的"锁孔效应"理论，认为通过狭窄视口查看大型信息空间会产生认知成本，需要研究聊天界面如何系统性降低分析性能。

Method: 作者提出了认知过载的形式化模型O = max(0, m - v - W)，其中m是任务相关项目数，v是可见项目数，W是工作记忆容量。分析了聊天界面的五种失败机制，并提出了八种混合设计模式：生成式UI、无限画布、指示性交互、状态轨道、幽灵层、Mise en Place、语义缩放和概率UI。

Result: 当认知过载O > 0时，错误概率增加，分析偏差（锚定、确认、变化盲视）会放大。提出的八种设计模式针对特定的认知瓶颈，同时保留自然语言用于意图指定和综合。

Conclusion: 聊天界面不适合开放探索式数据分析任务，需要混合设计模式来减少认知负荷。对于有指导的任务，良好搭建的对话系统可能减少负荷，但该框架主要适用于开放探索。论文最后提出了可证伪的假设和实验范式进行实证验证。

Abstract: Chat has become the default interface for AI-assisted data analysis. For multi-step, state-dependent analytical tasks, this is a mistake. Building on Woods (1984) Keyhole Effect, the cognitive cost of viewing large information spaces through narrow viewports, I show that chat interfaces systematically degrade analytical performance through five mechanisms: (1) constant content displacement defeats hippocampal spatial memory systems; (2) hidden state variables exceed working memory capacity (approximately 4 chunks under load); (3) forced verbalization triggers verbal overshadowing, degrading visual pattern recognition; (4) linear text streams block epistemic action and cognitive offloading; (5) serialization penalties scale with data dimensionality. I formalize cognitive overload as O = max(0, m - v - W) where m is task-relevant items, v is visible items, and W is working memory capacity. When O > 0, error probability increases and analytical biases (anchoring, confirmation, change blindness) amplify. Eight hybrid design patterns address these failures: Generative UI, Infinite Canvas, Deictic Interaction, State Rail, Ghost Layers, Mise en Place, Semantic Zoom, and Probabilistic UI. Each pattern targets specific cognitive bottlenecks while preserving natural language for intent specification and synthesis. Well-scaffolded conversational systems that encode expert priors may reduce load for guided tasks; the framework applies most strongly to open-ended exploration. The paper concludes with falsifiable hypotheses and experimental paradigms for empirical validation.

</details>


### [683] [Lyapunov Stability-Aware Stackelberg Game for Low-Altitude Economy: A Control-Oriented Pruning-Based DRL Approach](https://arxiv.org/abs/2602.01131)
*Yue Zhong,Jiawen Kang,Yongju Tong,Hong-Ning Dai,Dong In Kim,Abbas Jamalipour,Shengli Xie*

Main category: cs.AI

Relevance: 45.0

TL;DR: 本文提出一个感知-通信-计算-控制闭环框架，将通信延迟对物理控制稳定性的影响显式建模，利用李雅普诺夫稳定性理论将稳定性要求转化为可量化的资源边界，并通过基于剪枝的PPO算法在无人机上实现高效资源分配。


<details>
  <summary>Details</summary>
Motivation: 随着低空经济的发展，无人机作为空中基站支持从延迟敏感的关键任务到带宽密集型数据流的多样化服务。然而，有限的机载资源与严格的稳定性要求之间的冲突常常影响异构网络的效能。传统以吞吐量为中心的设计无法满足稳定性需求。

Method: 1) 提出感知-通信-计算-控制闭环框架，显式建模通信延迟对控制稳定性的影响；2) 利用李雅普诺夫稳定性理论推导控制系统状态演化与通信约束之间的内在映射；3) 将资源分配问题建模为Stackelberg博弈；4) 提出基于剪枝的PPO算法，通过动态结构化剪枝机制压缩神经网络规模。

Result: 仿真结果表明，所提方案在动态低空环境中能有效保障控制环稳定性，同时最大化系统效用。基于剪枝的PPO算法显著减少了计算开销，使无人机能以最小推理延迟快速逼近博弈均衡。

Conclusion: 该研究超越了传统的吞吐量中心设计，通过将稳定性要求量化为资源边界，并结合轻量级强化学习算法，为无人机异构网络提供了可靠且高效的资源分配方案。

Abstract: With the rapid expansion of the low-altitude economy, Unmanned Aerial Vehicles (UAVs) serve as pivotal aerial base stations supporting diverse services from users, ranging from latency-sensitive critical missions to bandwidth-intensive data streaming. However, the efficacy of such heterogeneous networks is often compromised by the conflict between limited onboard resources and stringent stability requirements. Moving beyond traditional throughput-centric designs, we propose a Sensing-Communication-Computing-Control closed-loop framework that explicitly models the impact of communication latency on physical control stability. To guarantee mission reliability, we leverage the Lyapunov stability theory to derive an intrinsic mapping between the state evolution of the control system and communication constraints, transforming abstract stability requirements into quantifiable resource boundaries. Then, we formulate the resource allocation problem as a Stackelberg game, where UAVs (as leaders) dynamically price resources to balance load and ensure stability, while users (as followers) optimize requests based on service urgency. Furthermore, addressing the prohibitive computational overhead of standard Deep Reinforcement Learning (DRL) on energy-constrained edge platforms, we propose a novel and lightweight pruning-based Proximal Policy Optimization (PPO) algorithm. By integrating a dynamic structured pruning mechanism, the proposed algorithm significantly compresses the neural network scale during training, enabling the UAV to rapidly approximate the game equilibrium with minimal inference latency. Simulation results demonstrate that the proposed scheme effectively secures control loop stability while maximizing system utility in dynamic low-altitude environments.

</details>


### [684] [Multi-Agent Causal Reasoning System for Error Pattern Rule Automation in Vehicles](https://arxiv.org/abs/2602.01155)
*Hugo Math,Julian Lorentz,Stefan Oelsner,Rainer Lienhart*

Main category: cs.AI

Relevance: 45.0

TL;DR: CAREP是一个多智能体系统，用于从车辆诊断故障代码(DTCs)中自动生成错误模式(EP)规则，结合因果发现、上下文信息集成和规则合成，实现可解释的自动化故障诊断。


<details>
  <summary>Details</summary>
Motivation: 现代车辆产生数千种诊断故障代码(DTCs)，汽车制造商使用这些代码的布尔组合（错误模式EP）来表征系统故障。目前EP规则仍由领域专家手动制定，随着车辆复杂度增加，这一过程成本高昂且容易出错。

Method: CAREP采用多智能体架构：1) 因果发现智能体识别DTC-EP潜在关系；2) 上下文信息智能体集成元数据和描述信息；3) 编排器智能体合成候选布尔规则并提供可解释的推理轨迹。

Result: 在包含29,100个独特DTCs和474个错误模式的大规模汽车数据集上评估，CAREP能够自动准确地发现未知的EP规则，优于仅使用LLM的基线方法，同时提供透明的因果解释。

Conclusion: CAREP通过结合实用的因果发现和基于智能体的推理，向完全自动化的故障诊断迈进一步，实现了可扩展、可解释且经济高效的车辆维护。

Abstract: Modern vehicles generate thousands of different discrete events known as Diagnostic Trouble Codes (DTCs). Automotive manufacturers use Boolean combinations of these codes, called error patterns (EPs), to characterize system faults and ensure vehicle safety. Yet, EP rules are still manually handcrafted by domain experts, a process that is expensive and prone to errors as vehicle complexity grows. This paper introduces CAREP (Causal Automated Reasoning for Error Patterns), a multi-agent system that automatizes the generation of EP rules from high-dimensional event sequences of DTCs. CAREP combines a causal discovery agent that identifies potential DTC-EP relations, a contextual information agent that integrates metadata and descriptions, and an orchestrator agent that synthesizes candidate boolean rules together with interpretable reasoning traces. Evaluation on a large-scale automotive dataset with over 29,100 unique DTCs and 474 error patterns demonstrates that CAREP can automatically and accurately discover the unknown EP rules, outperforming LLM-only baselines while providing transparent causal explanations. By uniting practical causal discovery and agent-based reasoning, CAREP represents a step toward fully automated fault diagnostics, enabling scalable, interpretable, and cost-efficient vehicle maintenance.

</details>


### [685] [Mitigating loss of control in advanced AI systems through instrumental goal trajectories](https://arxiv.org/abs/2602.01699)
*Willem Fourie*

Main category: cs.AI

Relevance: 45.0

TL;DR: 该论文提出了"工具性目标轨迹"概念，通过监控组织获取技术资源（计算、存储、数据等）的三种途径（采购、治理、金融），为AI系统能力控制提供新的干预点。


<details>
  <summary>Details</summary>
Motivation: 当前AI控制方法主要关注技术层面（如RLHF、可修正性设计），但高度能力的AI系统可能通过追求工具性目标侵蚀人类控制。需要超越模型本身，从组织层面寻找新的控制途径。

Method: 提出三种组织层面的工具性目标轨迹：采购轨迹（获取计算资源）、治理轨迹（获取数据和服务）、金融轨迹（获取资金）。通过监控这些轨迹产生的组织痕迹，建立干预点。

Result: 建立了基于组织资源获取路径的监控框架，将AI能力控制从单纯的技术属性扩展到组织系统层面，为定义能力阈值和实施可修正性提供了具体途径。

Conclusion: 工具性目标轨迹为AI安全提供了新的组织层面干预机制，将注意力从模型属性转向支撑模型的组织系统，丰富了AI控制的方法论工具箱。

Abstract: Researchers at artificial intelligence labs and universities are concerned that highly capable artificial intelligence (AI) systems may erode human control by pursuing instrumental goals. Existing mitigations remain largely technical and system-centric: tracking capability in advanced systems, shaping behaviour through methods such as reinforcement learning from human feedback, and designing systems to be corrigible and interruptible. Here we develop instrumental goal trajectories to expand these options beyond the model. Gaining capability typically depends on access to additional technical resources, such as compute, storage, data and adjacent services, which in turn requires access to monetary resources. In organisations, these resources can be obtained through three organisational pathways. We label these pathways the procurement, governance and finance instrumental goal trajectories (IGTs). Each IGT produces a trail of organisational artefacts that can be monitored and used as intervention points when a systems capabilities or behaviour exceed acceptable thresholds. In this way, IGTs offer concrete avenues for defining capability levels and for broadening how corrigibility and interruptibility are implemented, shifting attention from model properties alone to the organisational systems that enable them.

</details>


### [686] [Large Language Model and Formal Concept Analysis: a comparative study for Topic Modeling](https://arxiv.org/abs/2602.01933)
*Fabrice Boissier,Monica Sen,Irina Rychkova*

Main category: cs.AI

Relevance: 45.0

TL;DR: 该论文比较了大型语言模型（GPT-5）和形式概念分析（FCA）在主题建模任务上的表现，通过两个实验评估它们在文档主题提取方面的效果。


<details>
  <summary>Details</summary>
Motivation: 主题建模在文本处理中应用广泛，但LLM在此任务上的应用研究较少。FCA虽被提出作为主题建模的候选方法，但缺乏实际应用案例研究。本文旨在比较这两种方法，了解它们在主题建模领域的优缺点。

Method: 使用FCA的CREA管道进行主题建模和可视化评估，同时采用GPT-5进行LLM评估。对GPT-5采用零样本设置下的三提示策略：1）从文档批次生成主题，2）合并批次结果形成最终主题，3）主题标注。进行两个实验：第一个重用之前评估CREA的教学材料，第二个分析40篇信息系统研究文章。

Result: 论文比较了LLM和FCA在主题建模任务上的表现，但具体结果未在摘要中说明。实验设计包括对教学材料和学术文献的分析，旨在评估两种方法提取主题的效果。

Conclusion: 该研究为LLM和FCA在主题建模领域的比较提供了实证分析，有助于理解这两种方法在实际应用中的相对优势和局限性。

Abstract: Topic modeling is a research field finding increasing applications: historically from document retrieving, to sentiment analysis and text summarization. Large Language Models (LLM) are currently a major trend in text processing, but few works study their usefulness for this task. Formal Concept Analysis (FCA) has recently been presented as a candidate for topic modeling, but no real applied case study has been conducted. In this work, we compare LLM and FCA to better understand their strengths and weakneses in the topic modeling field. FCA is evaluated through the CREA pipeline used in past experiments on topic modeling and visualization, whereas GPT-5 is used for the LLM. A strategy based on three prompts is applied with GPT-5 in a zero-shot setup: topic generation from document batches, merging of batch results into final topics, and topic labeling. A first experiment reuses the teaching materials previously used to evaluate CREA, while a second experiment analyzes 40 research articles in information systems to compare the extracted topics with the underling subfields.

</details>


### [687] [Disentangled Interest Network for Out-of-Distribution CTR Prediction](https://arxiv.org/abs/2602.00002)
*Yu Zheng,Chen Gao,Jianxin Chang,Yanan Niu,Yang Song,Depeng Jin,Meng Wang,Yong Li*

Main category: cs.IR

Relevance: 45.0

TL;DR: DiseCTR：基于因果分解和兴趣解耦的点击率预测方法，通过解耦用户多兴趣来缓解推荐系统中的分布外问题


<details>
  <summary>Details</summary>
Motivation: 现有CTR预测方法假设训练和测试数据同分布，但实际中用户兴趣不断演化导致分布外问题。用户通常有多个兴趣，且演化速度不同，需要更好的建模方法。

Method: 提出因果视角的推荐系统，将CTR预测分解为用户兴趣、曝光模型和点击模型三个因果机制。设计兴趣编码器（稀疏注意力）、弱监督兴趣解耦器和注意力兴趣聚合器来学习独立的兴趣嵌入。

Result: 在三个真实数据集上，DiseCTR在OOD推荐中达到最佳准确性和鲁棒性，AUC和GAUC提升超过0.02，logloss降低超过13.7%。分析显示成功解耦用户兴趣。

Conclusion: 通过因果分解和兴趣解耦能有效缓解推荐系统中的分布外问题，兴趣解耦是CTR预测OOD泛化的关键。已开源代码和数据。

Abstract: Click-through rate (CTR) prediction, which estimates the probability of a user clicking on a given item, is a critical task for online information services. Existing approaches often make strong assumptions that training and test data come from the same distribution. However, the data distribution varies since user interests are constantly evolving, resulting in the out-of-distribution (OOD) issue. In addition, users tend to have multiple interests, some of which evolve faster than others. Towards this end, we propose Disentangled Click-Through Rate prediction (DiseCTR), which introduces a causal perspective of recommendation and disentangles multiple aspects of user interests to alleviate the OOD issue in recommendation. We conduct a causal factorization of CTR prediction involving user interest, exposure model, and click model, based on which we develop a deep learning implementation for these three causal mechanisms. Specifically, we first design an interest encoder with sparse attention which maps raw features to user interests, and then introduce a weakly supervised interest disentangler to learn independent interest embeddings, which are further integrated by an attentive interest aggregator for prediction. Experimental results on three real-world datasets show that DiseCTR achieves the best accuracy and robustness in OOD recommendation against state-of-the-art approaches, significantly improving AUC and GAUC by over 0.02 and reducing logloss by over 13.7%. Further analyses demonstrate that DiseCTR successfully disentangles user interests, which is the key to OOD generalization for CTR prediction. We have released the code and data at https://github.com/DavyMorgan/DiseCTR/.

</details>


### [688] [Rank-and-Reason: Multi-Agent Collaboration Accelerates Zero-Shot Protein Mutation Prediction](https://arxiv.org/abs/2602.00197)
*Yang Tan,Yuyuan Xi,Can Wu,Bozitao Zhong,Mingchen Li,Guisheng Fan,Jiankang Zhu,Yafeng Liang,Nanqing Dong,Liang Hong*

Main category: q-bio.QM

Relevance: 45.0

TL;DR: 提出VenusRAR框架，通过两阶段智能体系统自动化蛋白质突变预测，结合多模态集成和链式思维推理，显著提升预测准确性和湿实验成功率。


<details>
  <summary>Details</summary>
Motivation: 现有蛋白质语言模型（PLMs）的零样本突变预测常忽略基本生物物理约束，依赖专家手动审核效率低且主观。需要自动化工作流程来最大化湿实验适应性。

Method: 提出Rank-and-Reason两阶段框架：1) Rank阶段：计算专家和虚拟生物学家聚合上下文感知的多模态集成；2) Reason阶段：专家小组使用链式思维推理审核几何和结构约束。

Result: 在ProteinGym上Spearman相关性达0.551（vs. 0.518），ProteinGym-DMS99上Top-5命中率提升达367%。Cas12i3核酸酶湿实验验证显示46.7%阳性率，发现两个新突变体活性分别提升4.23倍和5.05倍。

Conclusion: VenusRAR框架成功自动化蛋白质工程中的突变预测工作流程，显著提升预测准确性和湿实验成功率，为低资源蛋白质工程提供有效解决方案。

Abstract: Zero-shot mutation prediction is vital for low-resource protein engineering, yet existing protein language models (PLMs) often yield statistically confident results that ignore fundamental biophysical constraints. Currently, selecting candidates for wet-lab validation relies on manual expert auditing of PLM outputs, a process that is inefficient, subjective, and highly dependent on domain expertise. To address this, we propose Rank-and-Reason (VenusRAR), a two-stage agentic framework to automate this workflow and maximize expected wet-lab fitness. In the Rank-Stage, a Computational Expert and Virtual Biologist aggregate a context-aware multi-modal ensemble, establishing a new Spearman correlation record of 0.551 (vs. 0.518) on ProteinGym. In the Reason-Stage, an agentic Expert Panel employs chain-of-thought reasoning to audit candidates against geometric and structural constraints, improving the Top-5 Hit Rate by up to 367% on ProteinGym-DMS99. The wet-lab validation on Cas12i3 nuclease further confirms the framework's efficacy, achieving a 46.7% positive rate and identifying two novel mutants with 4.23-fold and 5.05-fold activity improvements. Code and datasets are released on GitHub (https://github.com/ai4protein/VenusRAR/).

</details>


### [689] [QUASAR: A Universal Autonomous System for Atomistic Simulation and a Benchmark of Its Capabilities](https://arxiv.org/abs/2602.00185)
*Fengxu Yang,Jack D. Evans*

Main category: cond-mat.mtrl-sci

Relevance: 45.0

TL;DR: QUASAR是一个用于原子尺度模拟的通用自主系统，能够自主编排复杂多尺度工作流，包括密度泛函理论、机器学习势函数、分子动力学和蒙特卡洛模拟等，旨在促进生产级科学发现。


<details>
  <summary>Details</summary>
Motivation: 当前将大语言模型集成到材料科学中的代理系统受到僵化的工具调用方法和范围狭窄的代理限制，需要更灵活、通用的自主系统来促进科学发现。

Method: QUASAR系统包含自适应规划、上下文高效内存管理和混合知识检索等鲁棒机制，能够自主编排复杂多尺度工作流，涵盖密度泛函理论、机器学习势函数、分子动力学和蒙特卡洛模拟等多种方法。

Result: 在从常规任务到前沿研究挑战（如光催化剂筛选和新材料评估）的三级任务基准测试中，QUASAR表现出色，能够作为通用原子尺度推理系统而非特定任务自动化框架运行。

Conclusion: QUASAR可以作为通用原子尺度推理系统运行，初步证明了代理AI作为计算化学研究工作流组件的部署潜力，同时识别了需要进一步发展的领域。

Abstract: The integration of large language models (LLMs) into materials science offers a transformative opportunity to streamline computational workflows, yet current agentic systems remain constrained by rigid tool-calling approaches and narrowly scoped agents. In this work, we introduce QUASAR, a universal autonomous system for atomistic simulation designed to facilitate production-grade scientific discovery. QUASAR autonomously orchestrates complex multi-scale workflows across diverse methods, including density functional theory, machine learning potentials, molecular dynamics, and Monte Carlo simulations. The system incorporates robust mechanisms for adaptive planning, context-efficient memory management, and hybrid knowledge retrieval to navigate real-world research scenarios without human intervention. We benchmark QUASAR against a series of three-tiered tasks, progressing from routine tasks to frontier research challenges such as photocatalyst screening and novel material assessment. These results suggest that QUASAR can function as a general atomistic reasoning system rather than a task-specific automation framework. They also provide initial evidence supporting the potential deployment of agentic AI as a component of computational chemistry research workflows, while identifying areas requiring further development.

</details>


### [690] [From Detection to Prevention: Explaining Security-Critical Code to Avoid Vulnerabilities](https://arxiv.org/abs/2602.00711)
*Ranjith Krishnamurthy,Oshando Johnson,Goran Piskachev,Eric Bodden*

Main category: cs.CR

Relevance: 45.0

TL;DR: 开发了一个IntelliJ IDEA插件，使用代码级软件指标识别安全关键方法，并利用LLM生成预防性解释，以在开发阶段主动预防安全漏洞。


<details>
  <summary>Details</summary>
Motivation: 传统安全工具（静态/动态分析）只能在代码中引入漏洞后进行检测，修复成本高。本文探索主动预防策略，在开发阶段识别安全关键功能区域并提供安全实现指导。

Method: 开发IntelliJ IDEA插件原型：1) 使用代码级软件指标（如复杂度、依赖关系等）识别潜在安全关键方法；2) 利用大语言模型（LLMs）为这些方法生成预防导向的解释和指导。

Result: 在Spring-PetClinic应用上的初步评估显示：所选指标能识别大多数已知安全关键方法，LLM能提供可操作的、预防导向的见解。指标主要捕获结构特性而非安全语义方面。

Conclusion: 这项工作为代码级安全感知指标和增强解释奠定了基础，展示了在开发早期主动预防安全漏洞的可行性，尽管当前指标主要关注结构特性而非语义安全方面。

Abstract: Security vulnerabilities often arise unintentionally during development due to a lack of security expertise and code complexity. Traditional tools, such as static and dynamic analysis, detect vulnerabilities only after they are introduced in code, leading to costly remediation. This work explores a proactive strategy to prevent vulnerabilities by highlighting code regions that implement security-critical functionality -- such as data access, authentication, and input handling -- and providing guidance for their secure implementation. We present an IntelliJ IDEA plugin prototype that uses code-level software metrics to identify potentially security-critical methods and large language models (LLMs) to generate prevention-oriented explanations. Our initial evaluation on the Spring-PetClinic application shows that the selected metrics identify most known security-critical methods, while an LLM provides actionable, prevention-focused insights. Although these metrics capture structural properties rather than semantic aspects of security, this work lays the foundation for code-level security-aware metrics and enhanced explanations.

</details>


### [691] [Controlling Repetition in Protein Language Models](https://arxiv.org/abs/2602.00782)
*Jiahao Zhang,Zeqing Zhang,Di Wang,Lijie Hu*

Main category: q-bio.BM

Relevance: 45.0

TL;DR: 该论文首次系统研究了蛋白质语言模型中的重复生成问题，提出了量化指标来衡量重复模式，并开发了UCCS方法通过对比引导减少重复而不影响蛋白质折叠可靠性。


<details>
  <summary>Details</summary>
Motivation: 蛋白质语言模型在生成时经常出现病理性重复，这与文本重复不同，会破坏蛋白质的结构置信度和功能可行性。目前缺乏对PLM重复问题的系统研究，需要开发方法来控制重复同时保持蛋白质的可折叠性。

Method: 1. 提出量化指标来表征基序级和同聚物重复；2. 开发UCCS（效用控制对比引导）方法，通过构建对比集来最大化重复差异同时严格控制结构效用；3. 在推理时注入引导向量，无需重新训练或启发式解码。

Result: 在ESM-3和ProtGPT2模型上，在CATH、UniRef50和SCOP数据集上的实验表明，该方法优于解码惩罚和其他基线方法，显著降低重复率同时保持AlphaFold置信度分数。

Conclusion: 重复控制是蛋白质语言模型的核心挑战，基于数据集的引导是一种可靠蛋白质生成的原理性方法。UCCS能够有效减少重复而不损害可折叠性。

Abstract: Protein language models (PLMs) have enabled advances in structure prediction and de novo protein design, yet they frequently collapse into pathological repetition during generation. Unlike in text, where repetition merely reduces readability, in proteins it undermines structural confidence and functional viability. To unify this problem, we present the first systematic study of repetition in PLMs. We first propose quantitative metrics to characterize motif-level and homopolymer repetition and then demonstrate their negative impact on folding reliability. To address this challenge, we propose UCCS (Utility-Controlled Contrastive Steering), which steers protein generation with a constrained dataset. Instead of naively contrasting high- vs. low-repetition sequences, we construct contrastive sets that maximize differences in repetition while tightly controlling for structural utility. This disentanglement yields steering vectors that specifically target repetition without degrading foldability. Injected at inference, these vectors consistently reduce repetition without retraining or heuristic decoding. Experiments with ESM-3 and ProtGPT2 in CATH, UniRef50, and SCOP show that our method outperforms decoding penalties and other baselines, substantially lowering repetition while preserving AlphaFold confidence scores. Our results establish repetition control as a central challenge for PLMs and highlight dataset-guided steering as a principled approach for reliable protein generation.

</details>


### [692] [HERMES: A Holistic End-to-End Risk-Aware Multimodal Embodied System with Vision-Language Models for Long-Tail Autonomous Driving](https://arxiv.org/abs/2602.00993)
*Weizhe Tang,Junwei You,Jiaxi Liu,Zhaoyi Wang,Rui Gan,Zilin Huang,Feng Wei,Bin Ran*

Main category: cs.RO

Relevance: 45.0

TL;DR: HERMES是一个端到端多模态自动驾驶框架，通过注入显式的长尾风险线索到轨迹规划中，提升在混合交通长尾场景下的安全性和准确性。


<details>
  <summary>Details</summary>
Motivation: 当前端到端自动驾驶模型虽然受益于大视觉语言模型的语义理解能力，但在长尾混合交通场景下（包含人类驾驶车辆和弱势道路使用者等异质道路使用者）确保安全和准确操作仍然具有挑战性。需要解决在复杂不确定条件下的风险感知问题。

Method: 1) 使用基础模型辅助的标注流程生成结构化的长尾场景上下文和规划上下文，捕捉以危险为中心的线索、机动意图和安全偏好；2) 提出三模态驾驶模块，融合多视角感知、历史运动线索和语义指导，实现风险感知的轨迹规划。

Result: 在真实世界长尾数据集上的实验表明，HERMES在长尾混合交通场景下持续优于代表性的端到端和VLM驱动基线方法。消融研究验证了关键组件的互补贡献。

Conclusion: HERMES通过显式注入长尾风险线索到端到端规划中，有效提升了自动驾驶系统在复杂混合交通长尾场景下的安全性和性能。

Abstract: End-to-end autonomous driving models increasingly benefit from large vision--language models for semantic understanding, yet ensuring safe and accurate operation under long-tail conditions remains challenging. These challenges are particularly prominent in long-tail mixed-traffic scenarios, where autonomous vehicles must interact with heterogeneous road users, including human-driven vehicles and vulnerable road users, under complex and uncertain conditions. This paper proposes HERMES, a holistic risk-aware end-to-end multimodal driving framework designed to inject explicit long-tail risk cues into trajectory planning. HERMES employs a foundation-model-assisted annotation pipeline to produce structured Long-Tail Scene Context and Long-Tail Planning Context, capturing hazard-centric cues together with maneuver intent and safety preference, and uses these signals to guide end-to-end planning. HERMES further introduces a Tri-Modal Driving Module that fuses multi-view perception, historical motion cues, and semantic guidance, ensuring risk-aware accurate trajectory planning under long-tail scenarios. Experiments on the real-world long-tail dataset demonstrate that HERMES consistently outperforms representative end-to-end and VLM-driven baselines under long-tail mixed-traffic scenarios. Ablation studies verify the complementary contributions of key components.

</details>


### [693] [Backdoor Sentinel: Detecting and Detoxifying Backdoors in Diffusion Models via Temporal Noise Consistency](https://arxiv.org/abs/2602.01765)
*Bingzheng Wang,Xiaoyan Gu,Hongbo Xu,Hongcheng Li,Zimo Yu,Jiang Zhou,Weiping Wang*

Main category: cs.CR

Relevance: 45.0

TL;DR: 本文提出TNC-Defense框架，利用扩散模型中时间噪声不一致性现象进行后门检测和去毒，无需访问模型参数，在保护生成质量的同时有效防御后门攻击。


<details>
  <summary>Details</summary>
Motivation: 扩散模型在AIGC服务中广泛应用，但其不透明的训练数据和过程为后门注入提供了攻击面。实际审计场景中，由于知识产权和商业机密保护，审计者通常无法访问模型参数，使得现有的白盒或高查询量检测方法不实用。更重要的是，即使检测到后门，现有去毒方法往往陷入去毒效果和生成质量之间的两难境地。

Method: 提出Temporal Noise Consistency Defense (TNC-Defense)框架：1）基于相邻时间步噪声一致性设计灰盒检测模块，识别和定位异常扩散时间步；2）利用识别的异常时间步构建触发器无关、时间步感知的去毒模块，直接修正后门生成路径。

Result: 在五种代表性后门攻击场景下评估，TNC-Defense将平均检测准确率提升11%，额外开销可忽略不计，并使98.5%的触发样本失效，同时生成质量仅有轻微下降。

Conclusion: TNC-Defense通过利用时间噪声不一致性现象，提供了一种有效的灰盒后门防御框架，在保护模型知识产权的同时实现了高效的后门检测和去毒。

Abstract: Diffusion models have been widely deployed in AIGC services; however, their reliance on opaque training data and procedures exposes a broad attack surface for backdoor injection. In practical auditing scenarios, due to the protection of intellectual property and commercial confidentiality, auditors are typically unable to access model parameters, rendering existing white-box or query-intensive detection methods impractical. More importantly, even after the backdoor is detected, existing detoxification approaches are often trapped in a dilemma between detoxification effectiveness and generation quality.
  In this work, we identify a previously unreported phenomenon called temporal noise unconsistency, where the noise predictions between adjacent diffusion timesteps is disrupted in specific temporal segments when the input is triggered, while remaining stable under clean inputs. Leveraging this finding, we propose Temporal Noise Consistency Defense (TNC-Defense), a unified framework for backdoor detection and detoxification. The framework first uses the adjacent timestep noise consistency to design a gray-box detection module, for identifying and locating anomalous diffusion timesteps. Furthermore, the framework uses the identified anomalous timesteps to construct a trigger-agnostic, timestep-aware detoxification module, which directly corrects the backdoor generation path. This effectively suppresses backdoor behavior while significantly reducing detoxification costs.
  We evaluate the proposed method under five representative backdoor attack scenarios and compare it with state-of-the-art defenses. The results show that TNC-Defense improves the average detection accuracy by $11\%$ with negligible additional overhead, and invalidates an average of $98.5\%$ of triggered samples with only a mild degradation in generation quality.

</details>


### [694] [FragmentFlow: Scalable Transition State Generation for Large Molecules](https://arxiv.org/abs/2602.02310)
*Ron Shprints,Peter Holderrieth,Juno Nam,Rafael Gómez-Bombarelli,Tommi Jaakkola*

Main category: physics.chem-ph

Relevance: 45.0

TL;DR: FragmentFlow：一种分而治之的方法，通过预测反应核心原子的过渡态几何结构，然后重新连接取代基片段来重建完整过渡态结构，以解决大分子过渡态生成中的分布偏移问题。


<details>
  <summary>Details</summary>
Motivation: 传统过渡态生成方法计算成本高，现有生成方法无法泛化到实际相关的大分子反应底物，因为分子尺寸增加导致分布偏移，且大分子的过渡态几何结构数据不足，无法从头训练生成模型。

Method: FragmentFlow采用分而治之策略：1）训练生成模型预测定义反应机制的反应核心原子的过渡态几何结构；2）通过将取代基片段重新连接到预测的核心来重建完整过渡态结构。通过操作相对不变的反应核心来缓解生成建模中的分布偏移。

Result: 在包含多达33个重原子的反应物新数据集上评估，FragmentFlow正确识别了90%的过渡态，同时比经典初始化方案减少了30%的鞍点优化步骤。

Conclusion: 该方法为高通量反应性研究提供了可扩展的过渡态生成途径，通过专注于反应核心来有效处理大分子过渡态预测问题。

Abstract: Transition states (TSs) are central to understanding and quantitatively predicting chemical reactivity and reaction mechanisms. Although traditional TS generation methods are computationally expensive, recent generative modeling approaches have enabled chemically meaningful TS prediction for relatively small molecules. However, these methods fail to generalize to practically relevant reaction substrates because of distribution shifts induced by increasing molecular sizes. Furthermore, TS geometries for larger molecules are not available at scale, making it infeasible to train generative models from scratch on such molecules. To address these challenges, we introduce FragmentFlow: a divide-and-conquer approach that trains a generative model to predict TS geometries for the reactive core atoms, which define the reaction mechanism. The full TS structure is then reconstructed by re-attaching substituent fragments to the predicted core. By operating on reactive cores, whose size and composition remain relatively invariant across molecular contexts, FragmentFlow mitigates distribution shifts in generative modeling. Evaluated on a new curated dataset of reactions involving reactants with up to 33 heavy atoms, FragmentFlow correctly identifies 90% of TSs while requiring 30% fewer saddle-point optimization steps than classical initialization schemes. These results point toward scalable TS generation for high-throughput reactivity studies.

</details>


### [695] [ToPT: Task-Oriented Prompt Tuning for Urban Region Representation Learning](https://arxiv.org/abs/2602.01610)
*Zitao Guo,Changyang Jiang,Tianhong Zhao,Jinzhou Cao,Genan Dai,Bowen Zhang*

Main category: cs.AI

Relevance: 40.0

TL;DR: ToPT是一个两阶段框架，通过空间感知的区域嵌入学习和任务感知提示，解决城市计算中区域嵌入学习的空间一致性和任务对齐问题。


<details>
  <summary>Details</summary>
Motivation: 现有方法存在两个问题：1）两阶段方法产生任务无关的表示，与下游目标解耦；2）基于提示的方法缺乏明确的空间先验（导致空间不连贯）和任务语义对齐机制。

Method: 提出ToPT框架：1）SREL模块使用Graphormer融合模块，注入距离和区域中心性作为可学习的注意力偏置；2）Prompt4RE模块使用冻结的多模态大语言模型处理任务特定模板，通过多头交叉注意力将语义向量与区域嵌入对齐。

Result: 在多个任务和城市上的实验显示达到最先进性能，改进高达64.2%，验证了空间先验和提示-区域对齐的必要性和互补性。

Conclusion: ToPT通过空间感知的区域嵌入学习和任务感知提示，有效解决了城市计算中区域嵌入学习的空间一致性和任务对齐问题。

Abstract: Learning effective region embeddings from heterogeneous urban data underpins key urban computing tasks (e.g., crime prediction, resource allocation). However, prevailing two-stage methods yield task-agnostic representations, decoupling them from downstream objectives. Recent prompt-based approaches attempt to fix this but introduce two challenges: they often lack explicit spatial priors, causing spatially incoherent inter-region modeling, and they lack robust mechanisms for explicit task-semantic alignment. We propose ToPT, a two-stage framework that delivers spatially consistent fusion and explicit task alignment. ToPT consists of two modules: spatial-aware region embedding learning (SREL) and task-aware prompting for region embeddings (Prompt4RE). SREL employs a Graphormer-based fusion module that injects spatial priors-distance and regional centrality-as learnable attention biases to capture coherent, interpretable inter-region interactions. Prompt4RE performs task-oriented prompting: a frozen multimodal large language model (MLLM) processes task-specific templates to obtain semantic vectors, which are aligned with region embeddings via multi-head cross-attention for stable task conditioning. Experiments across multiple tasks and cities show state-of-the-art performance, with improvements of up to 64.2\%, validating the necessity and complementarity of spatial priors and prompt-region alignment. The code is available at https://github.com/townSeven/Prompt4RE.git.

</details>


### [696] [TessPay: Verify-then-Pay Infrastructure for Trusted Agentic Commerce](https://arxiv.org/abs/2602.00213)
*Mehul Goenka,Tejas Pathak,Siddharth Asthana*

Main category: cs.CR

Relevance: 40.0

TL;DR: TessPay是一个统一的"验证后支付"基础设施，通过分离控制和验证与结算来解决智能体商业中的信任缺口，确保任务执行验证后才释放资金。


<details>
  <summary>Details</summary>
Motivation: 智能体商业面临基础信任缺口：当前系统为人际交互设计，缺乏智能体交易的核心原语，包括任务委托、支付结算和审计机制的统一基础设施。

Method: 采用两平面架构分离控制/验证与结算，包括四个阶段：执行前（注册和意图捕获）、执行中（资金托管和证据生成）、结算时（证据验证和条件支付）、结算后（防篡改审计跟踪）。

Result: 提出TessPay系统，通过PoTE（任务执行证明）验证、链无关的托管支付和完整的审计跟踪，为智能体商业提供端到端的信任基础设施。

Conclusion: TessPay通过"验证后支付"架构解决了智能体商业的信任缺口，为自主代理交易提供了可验证、可审计的统一基础设施。

Abstract: The global economy is entering the era of Agentic Commerce, where autonomous agents can discover services, negotiate prices, and transact value. However adoption towards agentic commerce faces a foundational trust gap: current systems are built for direct human interactions rather than agent-driven operations. It lacks core primitives across three critical stages of agentic transactions. First, Task Delegation lacks means to translate user intent into defined scopes, discover appropriate agents, and securely authorize actions. Second, Payment Settlement for tasks is processed before execution, lacking verifiable evidence to validate the agent's work. Third, Audit Mechanisms fail to capture the full transaction lifecycle, preventing clear accountability for disputes. While emerging standards address fragments of this trust gap, there still remains a critical need for a unified infrastructure that binds the entire transaction lifecycle.
  To resolve this gap, we introduce TessPay, a unified infrastructure that replaces implicit trust with a 'Verify-then-Pay' architecture. It is a two plane architecture separating control and verification from settlement. TessPay operationalizes trust across four distinct stages: Before execution, agents are anchored in a canonical registry and user intent is captured as verifiable mandates, enabling stakeholder accountability. During execution, funds are locked in escrow while the agent executes the task and generates cryptographic evidence (TLS Notary, TEE etc.) to support Proof of Task Execution (PoTE). At settlement, the system verifies this evidence and releases funds only when the PoTE satisfies verification predicates; modular rail adapters ensure this PoTE-gated escrow remains chain-agnostic across heterogeneous payment rails. After settlement, TessPay preserves a tamper-evident audit trail to enable clear accountability for dispute resolution.

</details>


### [697] [HuPER: A Human-Inspired Framework for Phonetic Perception](https://arxiv.org/abs/2602.01634)
*Chenxu Guo,Jiachen Lian,Yisi Liu,Baihe Huang,Shriyaa Narayanan,Cheol Jun Cho,Gopala Anumanchipalli*

Main category: eess.AS

Relevance: 40.0

TL;DR: HuPER是一个受人类启发的语音感知框架，通过自适应推理结合声学-语音证据和语言知识，在少量训练数据下实现了最先进的语音识别性能，并具备强大的跨语言迁移能力。


<details>
  <summary>Details</summary>
Motivation: 当前语音识别系统通常需要大量标注数据，且在多语言和不同声学条件下的适应性有限。受人类语音感知机制的启发，研究者希望开发一个能够像人类一样自适应处理语音信号的框架。

Method: HuPER框架将语音感知建模为对声学-语音证据和语言知识的自适应推理过程。它采用多路径感知机制，能够适应不同的声学条件，并且仅需100小时的训练数据。

Result: 在5个英语基准测试中达到了最先进的语音错误率，并能够零样本迁移到95种未见语言。这是首个能够在不同声学条件下实现自适应多路径语音感知的框架。

Conclusion: HuPER展示了受人类启发的自适应语音感知框架的有效性，在少量数据下实现了强大的语音识别性能和跨语言泛化能力，为语音处理领域提供了新的研究方向。

Abstract: We propose HuPER, a human-inspired framework that models phonetic perception as adaptive inference over acoustic-phonetics evidence and linguistic knowledge. With only 100 hours of training data, HuPER achieves state-of-the-art phonetic error rates on five English benchmarks and strong zero-shot transfer to 95 unseen languages. HuPER is also the first framework to enable adaptive, multi-path phonetic perception under diverse acoustic conditions. All training data, models, and code are open-sourced. Code and demo avaliable at https://github.com/HuPER29/HuPER.

</details>


### [698] [TABX: A High-Throughput Sandbox Battle Simulator for Multi-Agent Reinforcement Learning](https://arxiv.org/abs/2602.01665)
*Hayeong Lee,JunHyeok Oh,Byung-Jun Lee*

Main category: cs.MA

Relevance: 40.0

TL;DR: TABX是一个基于JAX的高吞吐量多智能体强化学习沙盒环境，提供可重构的任务设计和细粒度环境参数控制，支持GPU加速和大规模并行化。


<details>
  <summary>Details</summary>
Motivation: 现有多智能体强化学习基准测试缺乏模块化设计，无法灵活构建自定义评估场景，限制了算法在不同任务复杂度下的系统性研究。

Method: 基于JAX框架构建完全加速的战斗模拟器，提供可重构的多智能体任务环境，支持细粒度环境参数控制，利用硬件加速实现GPU并行执行。

Result: TABX显著降低了计算开销，支持大规模并行化，为研究复杂结构化领域中的多智能体行为提供了快速、可扩展、易定制的框架。

Conclusion: TABX作为一个可扩展的基础设施，促进了多智能体强化学习在复杂任务中的研究，并为未来研究提供了可扩展的基础。

Abstract: The design of environments plays a critical role in shaping the development and evaluation of cooperative multi-agent reinforcement learning (MARL) algorithms. While existing benchmarks highlight critical challenges, they often lack the modularity required to design custom evaluation scenarios. We introduce the Totally Accelerated Battle Simulator in JAX (TABX), a high-throughput sandbox designed for reconfigurable multi-agent tasks. TABX provides granular control over environmental parameters, permitting a systematic investigation into emergent agent behaviors and algorithmic trade-offs across a diverse spectrum of task complexities. Leveraging JAX for hardware-accelerated execution on GPUs, TABX enables massive parallelization and significantly reduces computational overhead. By providing a fast, extensible, and easily customized framework, TABX facilitates the study of MARL agents in complex structured domains and serves as a scalable foundation for future research. Our code is available at: https://anonymous.4open.science/r/TABX-00CA.

</details>


### [699] [Spark: Modular Spiking Neural Networks](https://arxiv.org/abs/2602.02306)
*Mario Franco,Carlos Gershenson*

Main category: cs.NE

Relevance: 40.0

TL;DR: Spark是一个基于模块化设计的脉冲神经网络新框架，旨在提供高效、简化的脉冲神经网络流水线，通过简单可塑性机制解决稀疏奖励CartPole问题。


<details>
  <summary>Details</summary>
Motivation: 当前神经网络虽然强大但数据效率和能耗效率低下，脉冲神经网络适合高效硬件实现但缺乏有效学习算法。作者怀疑有效的可塑性机制可以缓解数据效率问题，因此开发了Spark框架。

Method: 提出了Spark框架，采用模块化设计理念，从简单组件到完整模型构建脉冲神经网络。框架旨在与传统机器学习流水线兼容，支持连续和无批处理学习。

Result: 展示了该框架通过简单可塑性机制解决稀疏奖励CartPole问题的能力，证明了框架的有效性。

Conclusion: Spark框架有望加速脉冲神经网络研究，特别是在连续和无批处理学习方面，类似于动物的学习方式。

Abstract: Nowadays, neural networks act as a synonym for artificial intelligence. Present neural network models, although remarkably powerful, are inefficient both in terms of data and energy. Several alternative forms of neural networks have been proposed to address some of these problems. Specifically, spiking neural networks are suitable for efficient hardware implementations. However, effective learning algorithms for spiking networks remain elusive, although it is suspected that effective plasticity mechanisms could alleviate the problem of data efficiency. Here, we present a new framework for spiking neural networks - Spark - built upon the idea of modular design, from simple components to entire models. The aim of this framework is to provide an efficient and streamlined pipeline for spiking neural networks. We showcase this framework by solving the sparse-reward cartpole problem with simple plasticity mechanisms. We hope that a framework compatible with traditional ML pipelines may accelerate research in the area, specifically for continuous and unbatched learning, akin to the one animals exhibit.

</details>


### [700] [Complete Identification of Deep ReLU Neural Networks by Many-Valued Logic](https://arxiv.org/abs/2602.00266)
*Yani Zhang,Helmut Bölcskei*

Main category: cs.AI

Relevance: 35.0

TL;DR: 该论文将ReLU神经网络转换为Lukasiewicz逻辑公式，通过逻辑公理的代数重写实现功能等价网络变换，提出组合范式将逻辑公式映射回ReLU网络，证明所有功能等价类中的ReLU网络都可通过有限对称性集合连接。


<details>
  <summary>Details</summary>
Motivation: 研究深度ReLU神经网络的功能对称性问题：不同架构和参数可能实现相同函数。解决完全识别问题——给定函数f，推导出所有产生f的前馈ReLU网络的架构和参数。

Method: 将ReLU网络转换为Lukasiewicz逻辑公式，通过逻辑公理的代数重写进行功能等价网络变换。提出组合范式便于从逻辑公式映射回ReLU网络。利用Chang完备性定理证明功能等价类中的网络连接性。

Result: 证明对于每个功能等价类，该类中的所有ReLU网络都可通过Lukasiewicz逻辑有限公理集合对应的有限对称性集合连接。这类似于香农在开关电路设计中的工作。

Conclusion: 建立了ReLU神经网络与Lukasiewicz逻辑之间的深刻联系，为神经网络的功能等价变换提供了理论基础，并展示了与布尔逻辑在电路设计中类似的代数重写方法。

Abstract: Deep ReLU neural networks admit nontrivial functional symmetries: vastly different architectures and parameters (weights and biases) can realize the same function. We address the complete identification problem -- given a function f, deriving the architecture and parameters of all feedforward ReLU networks giving rise to f. We translate ReLU networks into Lukasiewicz logic formulae, and effect functional equivalent network transformations through algebraic rewrites governed by the logic axioms. A compositional norm form is proposed to facilitate the mapping from Lukasiewicz logic formulae back to ReLU networks. Using Chang's completeness theorem, we show that for every functional equivalence class, all ReLU networks in that class are connected by a finite set of symmetries corresponding to the finite set of axioms of Lukasiewicz logic. This idea is reminiscent of Shannon's seminal work on switching circuit design, where the circuits are translated into Boolean formulae, and synthesis is effected by algebraic rewriting governed by Boolean logic axioms.

</details>


### [701] [PCBSchemaGen: Constraint-Guided Schematic Design via LLM for Printed Circuit Boards (PCB)](https://arxiv.org/abs/2602.00510)
*Huanghaohe Zou,Peng Han,Emad Nazerian,Alex Q. Huang*

Main category: cs.AI

Relevance: 35.0

TL;DR: PCBSchemaGen：首个免训练的PCB原理图设计框架，结合LLM代理与约束引导合成，解决异质信号（数字、模拟、电源）设计问题，通过代码生成和知识图谱验证提升设计准确性和效率。


<details>
  <summary>Details</summary>
Motivation: PCB原理图设计在电子产业中至关重要，但现有工作多专注于数字或模拟电路单一领域，而PCB设计需要处理异质信号并满足实际IC封装和引脚约束。由于开源数据稀缺且缺乏仿真验证，自动化PCB原理图设计尚未得到充分探索。

Method: 1. 基于LLM的代码生成范式，通过领域特定提示进行迭代反馈；2. 利用真实IC数据手册构建的知识图谱和子图同构验证框架，编码引脚角色语义和拓扑约束；3. 在23个PCB原理图任务上进行广泛实验，涵盖数字、模拟和电源领域。

Result: PCBSchemaGen显著提高了设计准确性和计算效率，在异质信号PCB设计任务中表现出色。

Conclusion: 该工作首次实现了自动化PCB原理图设计框架，通过LLM与约束引导合成的结合，为处理复杂异质信号设计问题提供了有效解决方案。

Abstract: Printed Circuit Board (PCB) schematic design plays an essential role in all areas of electronic industries. Unlike prior works that focus on digital or analog circuits alone, PCB design must handle heterogeneous digital, analog, and power signals while adhering to real-world IC packages and pin constraints. Automated PCB schematic design remains unexplored due to the scarcity of open-source data and the absence of simulation-based verification. We introduce PCBSchemaGen, the first training-free framework for PCB schematic design that comprises LLM agent and Constraint-guided synthesis. Our approach makes three contributions: 1. an LLM-based code generation paradigm with iterative feedback with domain-specific prompts. 2. a verification framework leveraging a real-world IC datasheet derived Knowledge Graph (KG) and Subgraph Isomorphism encoding pin-role semantics and topological constraints. 3. an extensive experiment on 23 PCB schematic tasks spanning digital, analog, and power domains. Results demonstrate that PCBSchemaGen significantly improves design accuracy and computational efficiency.

</details>


### [702] [Small Shifts, Large Gains: Unlocking Traditional TSP Heuristic Guided-Sampling via Unsupervised Neural Instance Modification](https://arxiv.org/abs/2602.00580)
*Wei Huang,Hanchen Wang,Dong Wen,Wenjie Zhang*

Main category: cs.AI

Relevance: 35.0

TL;DR: TSP-MDF：一种新颖的实例修改框架，通过神经网络修改TSP节点坐标，让传统确定性启发式算法具备引导采样能力，无需真实监督训练，在保持实用性的同时达到神经启发式算法的解质量。


<details>
  <summary>Details</summary>
Motivation: 传统确定性启发式算法（如最远插入、最近插入）计算高效实用，但确定性行为限制探索易陷入局部最优；神经启发式算法通过引导采样获得更优解，但需要大量训练和真实监督，实用性受限。需要桥接这一差距。

Method: 提出TSP-MDF框架：1）神经实例修改器策略性地移动节点坐标生成多个修改实例；2）基础传统启发式算法在修改实例上构建路径；3）将路径映射回原始实例，使传统算法能探索更优解并逃离局部最优；4）基于实例修改的表述，神经修改器无需真实监督即可高效训练。

Result: 在大规模TSP基准和真实世界基准上的实验表明，TSP-MDF显著提升传统启发式算法的性能，达到与神经启发式算法相当的解质量，但训练时间极短。

Conclusion: TSP-MDF成功桥接了传统启发式算法和神经启发式算法之间的差距，为传统算法赋予引导采样能力，同时保持实用性和高效训练，为组合优化问题提供了新的解决方案。

Abstract: The Traveling Salesman Problem (TSP) is one of the most representative NP-hard problems in route planning and a long-standing benchmark in combinatorial optimization. Traditional heuristic tour constructors, such as Farthest or Nearest Insertion, are computationally efficient and highly practical, but their deterministic behavior limits exploration and often leads to local optima. In contrast, neural-based heuristic tour constructors alleviate this issue through guided-sampling and typically achieve superior solution quality, but at the cost of extensive training and reliance on ground-truth supervision, hindering their practical use. To bridge this gap, we propose TSP-MDF, a novel instance modification framework that equips traditional deterministic heuristic tour constructors with guided-sampling capability. Specifically, TSP-MDF introduces a neural-based instance modifier that strategically shifts node coordinates to sample multiple modified instances, on which the base traditional heuristic tour constructor constructs tours that are mapped back to the original instance, allowing traditional tour constructors to explore higher-quality tours and escape local optima. At the same time, benefiting from our instance modification formulation, the neural-based instance modifier can be trained efficiently without any ground-truth supervision, ensuring the framework maintains practicality. Extensive experiments on large-scale TSP benchmarks and real-world benchmarks demonstrate that TSP-MDF significantly improves the performance of traditional heuristics tour constructors, achieving solution quality comparable to neural-based heuristic tour constructors, but with an extremely short training time.

</details>


### [703] [Engineering AI Agents for Clinical Workflows: A Case Study in Architecture,MLOps, and Governance](https://arxiv.org/abs/2602.00751)
*Cláudio Lúcio do Val Lopes,João Marcus Pitta,Fabiano Belém,Gildson Alves,Flávio Vinícius Cruzeiro Martins*

Main category: cs.AI

Relevance: 35.0

TL;DR: 本文介绍了Maria平台，这是一个用于初级医疗保健的生产级AI系统，通过整合四个工程支柱（清洁架构、事件驱动架构、代理作为模块化单元、人机协同治理）来解决临床AI中的可信赖性问题。


<details>
  <summary>Details</summary>
Motivation: 将AI集成到临床环境面临软件工程挑战，需要从孤立模型转向健壮、可治理、可靠的系统。当前工业应用中常存在脆弱、原型衍生的架构，缺乏系统性监督，导致"责任真空"，安全和问责制受损。

Method: 提出Maria平台作为行业案例研究，整合四个工程支柱：1) 清洁架构确保可维护性；2) 事件驱动架构提供弹性和可审计性；3) 代理作为主要模块化单元，每个拥有自主的MLOps生命周期；4) 人机协同治理模型作为关键的事件驱动数据源进行技术集成。

Result: Maria平台作为一个参考架构，为在高风险领域构建可维护、可扩展和可问责的AI系统提供了实用经验。展示了如何通过系统工程方法实现可信赖的临床AI。

Conclusion: 可信赖的临床AI需要通过系统工程方法整合多个工程支柱，而不仅仅是模型开发。Maria平台展示了如何通过架构设计、模块化、事件驱动和人机协同来实现生产级AI系统的可靠性、可维护性和问责制。

Abstract: The integration of Artificial Intelligence (AI) into clinical settings presents a software engineering challenge, demanding a shift from isolated models to robust, governable, and reliable systems. However, brittle, prototype-derived architectures often plague industrial applications and a lack of systemic oversight, creating a ``responsibility vacuum'' where safety and accountability are compromised. This paper presents an industry case study of the ``Maria'' platform, a production-grade AI system in primary healthcare that addresses this gap.
  Our central hypothesis is that trustworthy clinical AI is achieved through the holistic integration of four foundational engineering pillars. We present a synergistic architecture that combines Clean Architecture for maintainability with an Event-driven architecture for resilience and auditability. We introduce the Agent as the primary unit of modularity, each possessing its own autonomous MLOps lifecycle. Finally, we show how a Human-in-the-Loop governance model is technically integrated not merely as a safety check, but as a critical, event-driven data source for continuous improvement. We present the platform as a reference architecture, offering practical lessons for engineers building maintainable, scalable, and accountable AI-enabled systems in high-stakes domains.

</details>


### [704] [MissMAC-Bench: Building Solid Benchmark for Missing Modality Issue in Robust Multimodal Affective Computing](https://arxiv.org/abs/2602.00811)
*Ronghao Lin,Honghao Lu,Ruixing Wu,Aolin Xiong,Qinggong Chu,Qiaolin He,Sijie Mai,Haifeng Hu*

Main category: cs.AI

Relevance: 35.0

TL;DR: 提出MissMAC-Bench基准，用于评估多模态情感计算中缺失模态问题的鲁棒性，包含训练时无缺失先验、单一模型处理完整/不完整模态等原则，在4个数据集上验证了3种语言模型的效果。


<details>
  <summary>Details</summary>
Motivation: 现实场景中多模态数据往往动态不确定，存在缺失模态问题，导致分布偏移和语义缺陷，严重影响MAC模型的鲁棒性和实际部署。需要建立系统评估标准来量化这一问题。

Method: 提出MissMAC-Bench基准，包含两个核心原则：1) 训练时无缺失先验；2) 单一模型能同时处理完整和不完整模态场景。基准包含数据集和实例级别的固定/随机缺失模式评估协议。

Result: 在4个广泛使用的数据集上对3种语言模型进行了广泛实验，验证了不同MAC方法处理缺失模态问题的有效性。基准为推进鲁棒多模态情感计算提供了坚实基础。

Conclusion: MissMAC-Bench为多模态情感计算中的缺失模态问题提供了系统评估框架，促进了鲁棒MAC模型的发展和多媒体数据挖掘的进步。

Abstract: As a knowledge discovery task over heterogeneous data sources, current Multimodal Affective Computing (MAC) heavily rely on the completeness of multiple modalities to accurately understand human's affective state. However, in real-world scenarios, the availability of modality data is often dynamic and uncertain, leading to substantial performance fluctuations due to the distribution shifts and semantic deficiencies of the incomplete multimodal inputs. Known as the missing modality issue, this challenge poses a critical barrier to the robustness and practical deployment of MAC models. To systematically quantify this issue, we introduce MissMAC-Bench, a comprehensive benchmark designed to establish fair and unified evaluation standards from the perspective of cross-modal synergy. Two guiding principles are proposed, including no missing prior during training, and one single model capable of handling both complete and incomplete modality scenarios, thereby ensuring better generalization. Moreover, to bridge the gap between academic research and real-world applications, our benchmark integrates evaluation protocols with both fixed and random missing patterns at the dataset and instance levels. Extensive experiments conducted on 3 widely-used language models across 4 datasets validate the effectiveness of diverse MAC approaches in tackling the missing modality issue. Our benchmark provides a solid foundation for advancing robust multimodal affective computing and promotes the development of multimedia data mining.

</details>


### [705] [Transforming Vehicle Diagnostics: A Multimodal Approach to Error Patterns Prediction](https://arxiv.org/abs/2602.01109)
*Hugo Math,Rainer Lienhart*

Main category: cs.AI

Relevance: 35.0

TL;DR: BiCarFormer：首个用于车辆故障模式多标签序列分类的多模态方法，整合诊断故障码序列和环境条件数据，通过双向Transformer和协同注意力机制提升分类性能。


<details>
  <summary>Details</summary>
Motivation: 当前车辆诊断系统主要依赖诊断故障码序列，但忽略了温度、湿度、压力等环境传感器数据，而这些上下文信息对专家诊断故障模式至关重要。真实世界数据复杂且噪声多，需要更全面的方法。

Method: 提出BiCarFormer双向Transformer模型，专门处理车辆事件序列，采用嵌入融合和协同注意力机制捕捉诊断码与环境数据之间的关系，实现多模态多标签序列分类。

Result: 在包含22,137个错误码和360个错误模式的真实汽车数据集上，相比仅使用DTC序列的传统序列模型，BiCarFormer显著提升了分类性能。

Conclusion: 整合环境上下文信息对实现更准确、鲁棒的车辆诊断至关重要，能降低维护成本并提升汽车行业自动化水平。BiCarFormer为多模态车辆诊断提供了有效框架。

Abstract: Accurately diagnosing and predicting vehicle malfunctions is crucial for maintenance and safety in the automotive industry. While modern diagnostic systems primarily rely on sequences of vehicular Diagnostic Trouble Codes (DTCs) registered in On-Board Diagnostic (OBD) systems, they often overlook valuable contextual information such as raw sensory data (e.g., temperature, humidity, and pressure). This contextual data, crucial for domain experts to classify vehicle failures, introduces unique challenges due to its complexity and the noisy nature of real-world data. This paper presents BiCarFormer: the first multimodal approach to multi-label sequence classification of error codes into error patterns that integrates DTC sequences and environmental conditions. BiCarFormer is a bidirectional Transformer model tailored for vehicle event sequences, employing embedding fusions and a co-attention mechanism to capture the relationships between diagnostic codes and environmental data. Experimental results on a challenging real-world automotive dataset with 22,137 error codes and 360 error patterns demonstrate that our approach significantly improves classification performance compared to models that rely solely on DTC sequences and traditional sequence models. This work highlights the importance of incorporating contextual environmental information for more accurate and robust vehicle diagnostics, hence reducing maintenance costs and enhancing automation processes in the automotive industry.

</details>


### [706] [ASP-Bench: From Natural Language to Logic Programs](https://arxiv.org/abs/2602.01171)
*Stefan Szeider*

Main category: cs.AI

Relevance: 35.0

TL;DR: ASP-Bench是一个包含128个自然语言问题实例的基准测试，用于评估将自然语言规范转换为答案集程序（ASP）的系统。它系统覆盖了ASP的各种特性，并通过基于ReAct框架的智能体方法展示了反馈驱动的迭代细化在ASP建模中的有效性。


<details>
  <summary>Details</summary>
Motivation: 将自然语言规范自动转换为逻辑程序是神经符号工程中的一个挑战性任务。目前缺乏系统评估这种转换能力的基准测试，特别是对于答案集程序（ASP）这种重要的逻辑编程形式。作者旨在创建一个全面的基准来评估自然语言到ASP的转换系统，并理解影响建模难度的因素。

Method: 作者构建了ASP-Bench基准，包含128个自然语言问题实例（64个基础问题，每个有简单和困难变体）。基准系统覆盖了ASP的关键特性，如选择规则、聚合和优化。每个问题都包含参考验证器来检查解决方案是否满足问题规范。作者使用基于ReAct框架的智能体方法进行测试，该方法通过反馈驱动的迭代细化和求解器反馈来建模自然语言问题。

Result: ASP-Bench基准成功创建并系统覆盖了ASP的各种特性。基于ReAct的智能体方法实现了完全饱和（full saturation），表明反馈驱动的迭代细化结合求解器反馈为自然语言到ASP的建模提供了可靠且稳健的方法。分析揭示了决定问题建模难度的七个独立推理维度。

Conclusion: ASP-Bench为评估自然语言到ASP的转换系统提供了一个全面的基准。基于ReAct的智能体方法展示了反馈驱动迭代细化在逻辑编程建模中的有效性。该基准通过七个推理维度提供了对建模难度的多维视角，有助于理解自然语言规范转换为形式逻辑的挑战。

Abstract: Automating the translation of natural-language specifications into logic programs is a challenging task that affects neurosymbolic engineering. We present ASP-Bench, a benchmark comprising 128 natural language problem instances, 64 base problems with easy and hard variants. It evaluates systems that translate natural-language problems into Answer Set Programs (ASPs), a prominent form of logic programming. It provides systematic coverage of ASP features, including choice rules, aggregates, and optimization. Each problem includes reference validators that check whether solutions satisfy the problem specification.
  We characterize problems along seven largely independent reasoning aspects (optimization, temporal reasoning, default logic, resource allocation, recursion, spatial reasoning, and quantitative complexity), providing a multidimensional view of modeling difficulty.
  We test the benchmark using an agentic approach based on the ReAct (Reason and Act) framework, which achieves full saturation, demonstrating that feedback-driven iterative refinement with solver feedback provides a reliable and robust approach for modeling natural language in ASP. Our analysis across multiple agent runs enables us to gain insights into what determines a problem's modeling hardness.

</details>


### [707] [Learning to Guide Local Search for MPE Inference in Probabilistic Graphical Models](https://arxiv.org/abs/2602.01475)
*Brij Malhotra,Shivvrat Arya,Tahrima Rahman,Vibhav Giridhar Gogate*

Main category: cs.AI

Relevance: 35.0

TL;DR: 提出神经摊销框架改进概率图模型中的局部搜索，通过注意力网络预测移动减少与最优解汉明距离的能力，在重复查询场景中提升推理效率。


<details>
  <summary>Details</summary>
Motivation: 概率图模型中的最大后验概率推理是诊断、规划等领域的基础但计算困难的问题。在实际应用中，图模型固定而需要针对不同证据模式重复推理。传统随机局部搜索算法依赖短视的最佳改进规则，容易陷入局部最优，且现有启发式方法无法在相同模型的多次推理查询中有效复用指导信息。

Method: 提出神经摊销框架，利用固定图结构训练基于注意力的网络来评估局部移动，预测其减少与近似最优解汉明距离的能力。该方法与现有局部搜索过程无缝集成，在邻居选择时平衡短期似然增益与长期潜力。

Result: 在摊销推理设置中，对具有挑战性的高树宽基准测试上，相比传统随机局部搜索和引导局部搜索方法，该方法表现出持续改进的性能。

Conclusion: 神经摊销框架能够有效改进概率图模型中的局部搜索性能，特别是在重复查询场景中，通过预测距离减少的移动来平衡短期和长期搜索目标，提供更好的收敛行为。

Abstract: Most Probable Explanation (MPE) inference in Probabilistic Graphical Models (PGMs) is a fundamental yet computationally challenging problem arising in domains such as diagnosis, planning, and structured prediction. In many practical settings, the graphical model remains fixed while inference must be performed repeatedly for varying evidence patterns. Stochastic Local Search (SLS) algorithms scale to large models but rely on myopic best-improvement rule that prioritizes immediate likelihood gains and often stagnate in poor local optima. Heuristics such as Guided Local Search (GLS+) partially alleviate this limitation by modifying the search landscape, but their guidance cannot be reused effectively across multiple inference queries on the same model. We propose a neural amortization framework for improving local search in this repeated-query regime. Exploiting the fixed graph structure, we train an attention-based network to score local moves by predicting their ability to reduce Hamming distance to a near-optimal solution. Our approach integrates seamlessly with existing local search procedures, using this signal to balance short-term likelihood gains with long-term promise during neighbor selection. We provide theoretical intuition linking distance-reducing move selection to improved convergence behavior, and empirically demonstrate consistent improvements over SLS and GLS+ on challenging high-treewidth benchmarks in the amortized inference setting.

</details>


### [708] [Synesthesia of Vehicles: Tactile Data Synthesis from Visual Inputs](https://arxiv.org/abs/2602.01832)
*Rui Wang,Yaoguang Cao,Yuyi Chen,Jianyi Xu,Zhuoyang Li,Jiachen Shang,Shichun Yang*

Main category: cs.AI

Relevance: 35.0

TL;DR: 提出Synesthesia of Vehicles (SoV)框架，通过视觉输入预测自动驾驶车辆的触觉激励，使用跨模态时空对齐和潜在扩散模型进行无监督触觉数据合成。


<details>
  <summary>Details</summary>
Motivation: 自动驾驶车辆依赖多模态融合确保安全，但当前视觉和光学传感器无法检测道路引起的激励，这对车辆动态控制至关重要。受人类联觉启发，需要从视觉输入预测触觉激励。

Method: 1) 提出Synesthesia of Vehicles (SoV)框架；2) 开发跨模态时空对齐方法解决时间和空间差异；3) 使用潜在扩散模型构建视觉-触觉联觉生成模型(VTSyn)进行无监督高质量触觉数据合成；4) 通过真实车辆感知系统收集多模态数据集。

Result: VTSyn在时间、频率和分类性能方面优于现有模型，通过主动触觉感知增强自动驾驶安全性。

Conclusion: SoV框架成功实现了从视觉到触觉的跨模态感知，为自动驾驶提供了重要的补充感知能力，提高了车辆安全性能。

Abstract: Autonomous vehicles (AVs) rely on multi-modal fusion for safety, but current visual and optical sensors fail to detect road-induced excitations which are critical for vehicles' dynamic control. Inspired by human synesthesia, we propose the Synesthesia of Vehicles (SoV), a novel framework to predict tactile excitations from visual inputs for autonomous vehicles. We develop a cross-modal spatiotemporal alignment method to address temporal and spatial disparities. Furthermore, a visual-tactile synesthetic (VTSyn) generative model using latent diffusion is proposed for unsupervised high-quality tactile data synthesis. A real-vehicle perception system collected a multi-modal dataset across diverse road and lighting conditions. Extensive experiments show that VTSyn outperforms existing models in temporal, frequency, and classification performance, enhancing AV safety through proactive tactile perception.

</details>


### [709] [DomusFM: A Foundation Model for Smart-Home Sensor Data](https://arxiv.org/abs/2602.01910)
*Michele Fiori,Gabriele Civitarese,Flora D. Salim,Claudio Bettini*

Main category: cs.AI

Relevance: 35.0

TL;DR: DomusFM是首个专门为智能家居传感器数据设计的预训练基础模型，通过自监督双对比学习范式，结合语义嵌入和时间模式编码，解决了现有方法在数据稀疏性、隐私和成本方面的限制。


<details>
  <summary>Details</summary>
Motivation: 智能家居传感器数据在医疗监测和辅助技术中具有重要应用潜力，但现有方法存在关键限制：监督模型需要大量标注数据；现有基础模型仅关注惯性传感器，无法处理智能家居二进制传感器事件的稀疏离散特性；基于LLM的方法需要自然语言描述或提示，依赖外部服务或昂贵硬件，存在隐私和成本问题。

Method: DomusFM采用自监督双对比学习范式，同时捕获token级语义属性和序列级时间依赖性。通过整合轻量级语言模型的语义嵌入，以及专门用于时间模式和二进制状态的编码器，学习可迁移的通用表示。

Result: 在七个公共智能家居数据集上进行留一数据集评估，DomusFM在不同下游任务上优于现有最先进基线，即使在仅有5%标注数据用于微调的情况下也能实现优越性能。

Conclusion: DomusFM解决了数据稀缺性问题，同时保持了实际部署可行性，为现实世界智能家居系统提供了实用的基础模型解决方案。

Abstract: Smart-home sensor data holds significant potential for several applications, including healthcare monitoring and assistive technologies. Existing approaches, however, face critical limitations. Supervised models require impractical amounts of labeled data. Foundation models for activity recognition focus only on inertial sensors, failing to address the unique characteristics of smart-home binary sensor events: their sparse, discrete nature combined with rich semantic associations. LLM-based approaches, while tested in this domain, still raise several issues regarding the need for natural language descriptions or prompting, and reliance on either external services or expensive hardware, making them infeasible in real-life scenarios due to privacy and cost concerns. We introduce DomusFM, the first foundation model specifically designed and pretrained for smart-home sensor data. DomusFM employs a self-supervised dual contrastive learning paradigm to capture both token-level semantic attributes and sequence-level temporal dependencies. By integrating semantic embeddings from a lightweight language model and specialized encoders for temporal patterns and binary states, DomusFM learns generalizable representations that transfer across environments and tasks related to activity and event analysis. Through leave-one-dataset-out evaluation across seven public smart-home datasets, we demonstrate that DomusFM outperforms state-of-the-art baselines on different downstream tasks, achieving superior performance even with only 5% of labeled training data available for fine-tuning. Our approach addresses data scarcity while maintaining practical deployability for real-world smart-home systems.

</details>


### [710] [Thinking Like a Doctor: Conversational Diagnosis through the Exploration of Diagnostic Knowledge Graphs](https://arxiv.org/abs/2602.01995)
*Jeongmoon Won,Seungwon Kook,Yohan Jo*

Main category: cs.AI

Relevance: 35.0

TL;DR: 提出一个基于知识图谱的对话诊断系统，通过两步推理（生成诊断假设和验证假设）来改进临床对话诊断的准确性和效率。


<details>
  <summary>Details</summary>
Motivation: 现有对话诊断方法要么依赖模型的参数知识，要么假设患者提供丰富具体的信息，这在现实中不切实际。需要解决在信息不完整情况下的临床诊断问题。

Method: 使用诊断知识图谱进行两步推理：1) 从对话上下文生成诊断假设；2) 通过澄清问题验证假设，循环直到最终诊断。采用MIMIC-IV患者模拟器，并调整使其症状描述模糊以反映真实早期临床接触。

Result: 实验显示相比强基线方法，诊断准确性和效率都有提升。医生评估支持模拟器的真实性和生成问题的临床实用性。

Conclusion: 提出的基于知识图谱的对话诊断系统能够有效处理信息不完整的临床场景，提高诊断性能，具有临床实用价值。

Abstract: Conversational diagnosis requires multi-turn history-taking, where an agent asks clarifying questions to refine differential diagnoses under incomplete information. Existing approaches often rely on the parametric knowledge of a model or assume that patients provide rich and concrete information, which is unrealistic. To address these limitations, we propose a conversational diagnosis system that explores a diagnostic knowledge graph to reason in two steps: (i) generating diagnostic hypotheses from the dialogue context, and (ii) verifying hypotheses through clarifying questions, which are repeated until a final diagnosis is reached. Since evaluating the system requires a realistic patient simulator that responds to the system's questions, we adopt a well-established simulator along with patient profiles from MIMIC-IV. We further adapt it to describe symptoms vaguely to reflect real-world patients during early clinical encounters. Experiments show improved diagnostic accuracy and efficiency over strong baselines, and evaluations by physicians support the realism of our simulator and the clinical utility of the generated questions. Our code will be released upon publication.

</details>


### [711] [What Artificial Intelligence can do for High-Performance Computing systems?](https://arxiv.org/abs/2602.00014)
*Pierrick Pochelu,Hyacinthe Cartiaux,Julien Schleich*

Main category: cs.DC

Relevance: 35.0

TL;DR: 本文综述了2019-2025年间AI技术（包括机器学习和优化）在提升HPC系统运行效率方面的应用，分析了74篇相关论文并将其分为6个应用领域，发现调度是最活跃的研究方向。


<details>
  <summary>Details</summary>
Motivation: 高性能计算中心消耗大量电力，带来环境和运营成本。研究如何利用人工智能技术提高HPC系统的运行效率，降低能耗和成本。

Method: 对2019-2025年间约1800篇文献进行人工筛选，采用预定义的纳入/排除标准，最终保留74篇"AI for HPC"论文，并将其分为6个应用领域进行分析。

Result: 调度是最活跃的研究领域，涵盖从研究导向的强化学习调度器到生产友好的混合方法。监督性能估计是调度和优化的基础。图神经网络和时间序列模型增强了异常检测。针对HPC的领域专用语言模型在特定编码和自动化任务上优于通用LLM。

Conclusion: AI技术为HPC系统效率提升提供了重要机会，特别是基于LLM的操作系统概念。需要推进MLOps、AI组件标准化和基准测试方法的发展。

Abstract: High-performance computing (HPC) centers consume substantial power, incurring environmental and operational costs. This review assesses how artificial intelligence (AI), including machine learning (ML) and optimization, improves the efficiency of operational HPC systems. Approximately 1,800 publications from 2019 to 2025 were manually screened using predefined inclusion/exclusion criteria; 74 "AI for HPC" papers were retained and grouped into six application areas: performance estimation, performance optimization, scheduling, surrogate modeling, fault detection, and language-model-based automation.
  Scheduling is the most active area, spanning research-oriented reinforcement-learning schedulers to production-friendly hybrids that combine ML with heuristics. Supervised performance estimation is foundational for both scheduling and optimization. Graph neural networks and time-series models strengthen anomaly detection by capturing spatio-temporal dependencies in production telemetry. Domain-specialized language models for HPC can outperform general-purpose LLMs on targeted coding and automation tasks. Together, these findings highlight integration opportunities such as LLM-based operating-system concepts and underscore the need for advances in MLOps, standardization of AI components, and benchmarking methodology.

</details>


### [712] [Early Warning Signals Appear Long Before Dropping Out: An Idiographic Approach Grounded in Complex Dynamic Systems Theory](https://arxiv.org/abs/2602.00021)
*Mohammed Saqr,Sonsoles López-Pernas,Santtu Tikka,Markus Wolfgang Hermann Spitzer*

Main category: cs.CY

Relevance: 35.0

TL;DR: 该研究首次在教育领域应用临界减速理论，通过分析167万次数学练习数据，发现88.2%学生在脱离学习前会出现临界减速信号，可作为早期预警指标。


<details>
  <summary>Details</summary>
Motivation: 学生学习的韧性和坚持能力对学习效果至关重要。当韧性减弱时，学生面临脱离学习甚至辍学的风险。研究旨在探索是否能在学生完全脱离学习前，通过临界减速理论预测这种韧性丧失，从而在"希望窗口期"进行干预。

Method: 研究使用9401名学生在数字数学学习环境中的167万次练习数据，基于临界减速理论计算了六个指标：自相关、恢复率、方差、偏度、峰度和变异系数。这些指标用于检测系统接近临界点时的早期预警信号。

Result: 研究发现88.2%的学生在脱离学习前表现出临界减速信号，这些预警信号主要集中在活动后期和停止练习之前。这是教育领域首次发现临界减速现象的证据。

Conclusion: 临界减速理论在教育领域的应用表明，普遍存在的韧性动态也支配着人类学习等社会系统。这些指标具有普遍性，不依赖于数据生成机制，为跨情境、跨数据类型的早期脆弱性检测提供了新机会。

Abstract: The ability to sustain engagement and recover from setbacks (i.e., resilience) -- is fundamental for learning. When resilience weakens, students are at risk of disengagement and may drop out and miss on opportunities. Therefore, predicting disengagement long before it happens during the window of hope is important. In this article, we test whether early warning signals of resilience loss, grounded in the concept of critical slowing down (CSD) can forecast disengagement before dropping out. CSD has been widely observed across ecological, climate, and neural systems, where it precedes tipping points into catastrophic failure (dropping out in our case). Using 1.67 million practice attempts from 9,401 students who used a digital math learning environment, we computed CSD indicators: autocorrelation, return rate, variance, skewness, kurtosis, and coefficient of variation. We found that 88.2% of students exhibited CSD signals prior to disengagement, with warnings clustering late in activity and before practice ceased (dropping out). Our results provide the first evidence of CSD in education, suggesting that universal resilience dynamics also govern social systems such as human learning. These findings offer a practical indicator for early detection of vulnerability and supporting learners across different applications and contexts long before critical events happen. Most importantly, CSD indicators arise universally, independent of the mechanisms that generate the data, offering new opportunities for portability across contexts, data types, and learning environments.

</details>


### [713] [Strategies for Creating Uncertainty in the AI Era to Trigger Students Critical Thinking: Pedagogical Design, Assessment Rubric, and Exam System](https://arxiv.org/abs/2602.00026)
*Ahmad Samer Wazan*

Main category: cs.CY

Relevance: 35.0

TL;DR: 提出MindMosaicAIExam考试系统，通过可控AI工具制造不确定性情境，要求学生批判性评估AI输出并迭代完善推理，以培养和评估批判性思维。


<details>
  <summary>Details</summary>
Motivation: 生成式AI使学生能直接获得正确答案而无需展示理解过程，挑战传统评估方式。作者主张不应禁止AI，而是将其融入教育，利用AI模型创造不确定性情境，激发学生的批判性思维。

Method: 提出基于AI模型和教师固有局限性的学习活动与评估设计，引入MindMosaicAIExam考试系统，该系统集成可控AI工具，要求学生提供初步答案、批判性评估AI输出并迭代完善推理。同时开发了基于学生推理过程评估批判性思维的评分标准。

Result: 展示了如何通过控制AI在考试中的行为（如阻止直接答案或生成看似合理但有缺陷的回应）来防止AI成为获取确定性的捷径，从而促进学生进行推理、质疑和论证。

Conclusion: 将AI融入教育的关键是利用其创造不确定性情境，结合思维导向的教学方法，通过可控AI工具和专门的评估系统来培养和评估学生的批判性思维能力。

Abstract: Generative AI challenges traditional assessments by allowing students to produce correct answers without demonstrating understanding or reasoning. Rather than prohibiting AI, this work argues that one way to integrate AI into education is by creating uncertain situations with the help of AI models and using thinking-oriented teaching approaches, where uncertainty is a central pedagogical concept for stimulating students critical thinking. Drawing on epistemology and critical thinking research studies, we propose designing learning activities and assessments around the inherent limitations of both AI models and instructors. This encourages students to reason, question, and justify their final answers. We show how explicitly controlling AI behavior during exams (such as preventing direct answers or generating plausible but flawed responses) prevents AI from becoming a shortcut to certainty. To support this pedagogy, we introduce MindMosaicAIExam, an exam system that integrates controllable AI tools and requires students to provide initial answers, critically evaluate AI outputs, and iteratively refine their reasoning. We also present an evaluation rubric designed to assess critical thinking based on students reasoning artifacts collected by the exam system.

</details>


### [714] [AI-assisted Protocol Information Extraction For Improved Accuracy and Efficiency in Clinical Trial Workflows](https://arxiv.org/abs/2602.00052)
*Ramtin Babaeipour,François Charest,Madison Wright*

Main category: cs.IR

Relevance: 35.0

TL;DR: 使用RAG增强的生成式LLM自动提取临床试验协议信息，相比独立LLM准确率更高（87.8% vs 62.6%），在模拟工作流中效率提升40%且用户偏好AI辅助


<details>
  <summary>Details</summary>
Motivation: 临床试验协议复杂性增加、修订频繁以及知识管理困难给试验团队带来沉重负担。将协议内容结构化到标准格式中，有望提高效率、支持文档质量并加强合规性。

Method: 开发了一个专门针对临床试验的RAG（检索增强生成）系统，使用生成式LLM进行自动信息提取。比较了该RAG流程与公开可用（独立）LLM的提取准确性，并评估了AI辅助对模拟提取CRC工作流的操作影响。

Result: 临床试验专用RAG流程的准确率（87.8%）显著高于使用精细调优提示的独立LLM（62.6%）。在模拟提取工作流中，AI辅助任务完成速度快40%，认知负荷更低，用户强烈偏好AI辅助。

Conclusion: 虽然专家监督仍然必要，但AI辅助提取能够实现大规模协议智能化，激励将类似方法整合到真实世界临床工作流中，以进一步验证其对可行性、研究启动和激活后监测的影响。

Abstract: Increasing clinical trial protocol complexity, amendments, and challenges around knowledge management create significant burden for trial teams. Structuring protocol content into standard formats has the potential to improve efficiency, support documentation quality, and strengthen compliance. We evaluate an Artificial Intelligence (AI) system using generative LLMs with Retrieval-Augmented Generation (RAG) for automated clinical trial protocol information extraction. We compare the extraction accuracy of our clinical-trial-specific RAG process against that of publicly available (standalone) LLMs. We also assess the operational impact of AI-assistance on simulated extraction CRC workflows. Our RAG process was measured as more accurate (87.8%) than standalone LLMs with fine-tuned prompts (62.6%) against expert-supported reference annotations. In the simulated extraction workflows, AI-assisted tasks were completed 40% faster, rated as less cognitively demanding and strongly preferred by users. While expert oversight remains essential, this suggests that AI-assisted extraction can enable protocol intelligence at scale, motivating the integration of similar methodologies into real world clinical workflows to further validate its impact on feasibility, study start-up, and post-activation monitoring.

</details>


### [715] [Student Perceptions of Large Language Models Use in Self-Reflection and Design Critique in Architecture Studio](https://arxiv.org/abs/2602.00041)
*Juan David Salazar Rodriguez,Sam Conrad Joyce,Nachamma Sockalingam,Khoo Eng Tat,Julfendi*

Main category: cs.CY

Relevance: 35.0

TL;DR: 研究探索LLMs在建筑设计工作室反馈机制中的应用，发现学生将LLMs视为"认知镜子"而非权威指导者，用于支持批判性思维、缓解社交焦虑和管理认知负荷。


<details>
  <summary>Details</summary>
Motivation: 研究动机在于探索LLMs如何从生成工具转变为反思性教学工具，特别是在建筑教育中，如何将LLMs整合到反馈机制中，支持学生的批判性思维发展。

Method: 采用混合方法研究，在新加坡科技设计大学的建筑学生中进行研究，分析学生在三个反馈领域的感知：自我反思、同伴批评和教授主导的评审。

Result: 研究发现学生将LLMs视为协作的"认知镜子"而非权威指导者。在自我学习中帮助结构化思维但缺乏情境细微差别；在同伴批评中作为中立调解者缓解社交焦虑；在教授评审中作为后批评合成引擎管理认知负荷。

Conclusion: LLMs在建筑教育中可以作为有效的反思性教学工具，支持批判性思维发展，但需要认识到其局限性，特别是在情境理解和细微差别方面。

Abstract: This study investigates the integration of Large Language Models (LLMs) into the feedback mechanisms of the architectural design studio, shifting the focus from generative production to reflective pedagogy. Employing a mixed-methods approach with architecture students at the Singapore Uni-versity of Technology and Design, the research analyzes student percep-tions across three distinct feedback domains: self-reflection, peer critique, and professor-led reviews. The findings reveal that students engage with LLMs not as authoritative instructors, but as collaborative "cognitive mir-rors" that scaffold critical thinking. In self-directed learning, LLMs help structure thoughts and overcome the "blank page" problem, though they are limited by a lack of contextual nuance. In peer critiques, the technology serves as a neutral mediator, mitigating social anxiety and the "fear of of-fending". Furthermore, in high-stakes professor-led juries, students utilize LLMs primarily as post-critique synthesis engines to manage cognitive overload and translate abstract academic discourse into actionable design iterations.

</details>


### [716] [Quantum Circuit-Based Learning Models: Bridging Quantum Computing and Machine Learning](https://arxiv.org/abs/2602.00048)
*Fan Fan,Yilei Shi,Mihai Datcu,Bertrand Le Saux,Luigi Iapichino,Francesca Bovolo,Silvia Liberata Ullo,Xiao Xiang Zhu*

Main category: quant-ph

Relevance: 35.0

TL;DR: 量子机器学习综述：回顾量子电路学习模型在经典数据分析中的应用，涵盖核方法与神经网络方法，探讨混合框架、理论分析、噪声鲁棒性及硬件效率优化，并展望未来发展方向。


<details>
  <summary>Details</summary>
Motivation: 随着机器学习广泛应用和量子计算快速发展，量子机器学习作为两者结合的新兴领域受到关注。本文旨在回顾现有量子电路学习模型在经典数据分析中的贡献，识别其潜力与挑战，为未来研究提供指导。

Method: 综述研究方法：系统回顾量子电路学习模型（包括核方法和神经网络方法），分析混合量子-经典学习框架，考察理论分析与实证结果，探讨噪声鲁棒性和硬件效率优化技术，并覆盖新兴量子电路设计范式。

Result: 本文全面梳理了量子机器学习领域的研究现状，识别了量子电路模型在经典数据分析中的潜力与局限，总结了混合框架的优势，分析了噪声和硬件限制下的实用化挑战，并展示了在不同应用领域的适应性。

Conclusion: 量子机器学习作为量子计算与机器学习交叉领域具有重要潜力，但面临硬件限制、噪声影响等挑战。未来需在理论分析、算法设计、硬件效率和应用扩展等方面持续探索，以推动该领域发展和实际应用。

Abstract: Machine Learning (ML) has been widely applied across numerous domains due to its ability to automatically identify informative patterns from data for various tasks. The availability of large-scale data and advanced computational power enables the development of sophisticated models and training strategies, leading to state-of-the-art performance, but it also introduces substantial challenges. Quantum Computing (QC), which exploits quantum mechanisms for computation, has attracted growing attention and significant global investment as it may address these challenges. Consequently, Quantum Machine Learning (QML), the integration of these two fields, has received increasing interest, with a notable rise in related studies in recent years. We are motivated to review these existing contributions regarding quantum circuit-based learning models for classical data analysis and highlight the identified potentials and challenges of this technique. Specifically, we focus not only on QML models, both kernel-based and neural network-based, but also on recent explorations of their integration with classical machine learning layers within hybrid frameworks. Moreover, we examine both theoretical analysis and empirical findings to better understand their capabilities, and we also discuss the efforts on noise-resilient and hardware-efficient QML that could enhance its practicality under current hardware limitations. In addition, we cover several emerging paradigms for advanced quantum circuit design and highlight the adaptability of QML across representative application domains. This study aims to provide an overview of the contributions made to bridge quantum computing and machine learning, offering insights and guidance to support its future development and pave the way for broader adoption in the coming years.

</details>


### [717] [Explore Brain-Inspired Machine Intelligence for Connecting Dots on Graphs Through Holographic Blueprint of Oscillatory Synchronization](https://arxiv.org/abs/2602.00057)
*Tingting Dan,Jiaqi Ding,Guorong Wu*

Main category: q-bio.NC

Relevance: 35.0

TL;DR: 该论文提出了一种受大脑节律启发的机器学习新范式，通过神经振荡同步机制构建了HoloBrain模型和HoloGraph框架，用于改进图神经网络并解决过平滑问题。


<details>
  <summary>Details</summary>
Motivation: 从神经科学和人工智能中神经耦合的动态振荡模式出发，探索大脑节律的神经机制如何启发下一代机器学习算法设计，以提高效率和鲁棒性。

Method: 首先通过自发同步神经振荡的干涉建模演化的大脑节律（HoloBrain），然后基于共享同步机制提出"第一原理"（HoloGraph），使图神经网络超越传统热扩散范式，转向建模振荡同步。

Result: HoloGraph框架不仅有效缓解了图神经网络中的过平滑问题，还在图上的推理和解决挑战性问题方面展现出强大潜力。

Conclusion: 大脑节律的神经机制可以为机器学习提供新的设计原则，基于振荡同步的HoloGraph框架为图神经网络带来了改进，并具有解决复杂图问题的潜力。

Abstract: Neural coupling in both neuroscience and artificial intelligence emerges as dynamic oscillatory patterns that encode abstract concepts. To this end, we hypothesize that a deeper understanding of the neural mechanisms governing brain rhythms can inspire next-generation design principles for machine learning algorithms, leading to improved efficiency and robustness. Building on this idea, we first model evolving brain rhythms through the interference of spontaneously synchronized neural oscillations, termed HoloBrain. The success of modeling brain rhythms using an artificial dynamical system of coupled oscillations motivates a "first principle" for brain-inspired machine intelligence based on a shared synchronization mechanism, termed HoloGraph. This principle enables graph neural networks to move beyond conventional heat diffusion paradigms toward modeling oscillatory synchronization. Our HoloGraph framework not only effectively mitigates the over-smoothing problem in graph neural networks but also demonstrates strong potential for reasoning and solving challenging problems on graphs.

</details>


### [718] [Responsible Evaluation of AI for Mental Health](https://arxiv.org/abs/2602.00065)
*Hiba Arnaout,Anmol Goel,H. Andrew Schwartz,Steffen T. Eberhardt,Dana Atzil-Slonim,Gavin Doherty,Brian Schwartz,Wolfgang Lutz,Tim Althoff,Munmun De Choudhury,Hamidreza Jamalabadi,Raj Sanjay Shah,Flor Miriam Plaza-del-Arco,Dirk Hovy,Maria Liakata,Iryna Gurevych*

Main category: cs.CY

Relevance: 35.0

TL;DR: 该论文提出了一个重新思考AI心理健康工具评估的跨学科框架，强调临床有效性、社会背景和公平性，而非仅依赖通用指标。


<details>
  <summary>Details</summary>
Motivation: 当前AI在心理健康领域的评估方法存在碎片化问题，与临床实践、社会背景和用户体验脱节。现有方法过度依赖通用指标，未能充分捕捉临床有效性、治疗适当性、安全性、公平性和用户体验。

Method: 通过分析135篇相关文献，识别现有评估的局限性。提出一个整合临床合理性、社会背景和公平性的跨学科评估框架。建立AI心理健康支持类型分类法（评估导向、干预导向、信息合成导向），并通过案例研究说明其应用。

Result: 研究发现现有评估存在三大问题：1) 过度依赖不捕捉临床有效性的通用指标；2) 心理健康专业人员参与有限；3) 对安全性和公平性关注不足。提出的分类法为不同AI支持类型提供了针对性的风险评估和评估要求。

Conclusion: 需要重新思考AI心理健康工具的负责任评估，强调临床相关性、社会背景和公平性。提出的框架和分类法为更全面、负责任的评估提供了结构化基础，有助于弥合AI技术与实际临床需求之间的差距。

Abstract: Although artificial intelligence (AI) shows growing promise for mental health care, current approaches to evaluating AI tools in this domain remain fragmented and poorly aligned with clinical practice, social context, and first-hand user experience. This paper argues for a rethinking of responsible evaluation -- what is measured, by whom, and for what purpose -- by introducing an interdisciplinary framework that integrates clinical soundness, social context, and equity, providing a structured basis for evaluation. Through an analysis of 135 recent *CL publications, we identify recurring limitations, including over-reliance on generic metrics that do not capture clinical validity, therapeutic appropriateness, or user experience, limited participation from mental health professionals, and insufficient attention to safety and equity. To address these gaps, we propose a taxonomy of AI mental health support types -- assessment-, intervention-, and information synthesis-oriented -- each with distinct risks and evaluative requirements, and illustrate its use through case studies.

</details>


### [719] [Standards for trustworthy AI in the European Union: technical rationale, structural challenges, and an implementation path](https://arxiv.org/abs/2602.00078)
*Piercosma Bisconti,Marcello Galisai*

Main category: cs.CY

Relevance: 35.0

TL;DR: 本文探讨欧盟AI法案下的AI标准化技术基础，分析AI标准化面临的独特挑战，提出基于风险管理、可重复技术检查、结构化文档和保证案例的分层标准化方案。


<details>
  <summary>Details</summary>
Motivation: 欧盟AI法案需要技术标准来支持合规性推定机制，但AI系统具有随机行为、数据依赖、评估实践不成熟等独特挑战，需要建立可行的标准化框架来将法律义务转化为可审计的工程实践。

Method: 采用分层标准化方法：水平标准定义流程义务和证据结构，行业剖面指定领域特定阈值和验收标准。提出基于风险管理、可重复技术检查（重新定义为测量属性的稳定性）、结构化文档、全面日志记录和生命周期演化的保证案例。

Result: 尽管存在方法论困难，技术标准对于将法律义务转化为可审计的工程实践、实现跨提供商、评估机构和执法机构的可扩展合规性评估仍然至关重要。

Conclusion: AI标准化需要应对AI系统的独特特性，通过分层方法和基于风险的框架，技术标准能够支持AI法案的实施，促进可信AI系统的开发和应用。

Abstract: This white paper examines the technical foundations of European AI standardization under the AI Act. It explains how harmonized standards enable the presumption of conformity mechanism, describes the CEN/CENELEC standardization process, and analyzes why AI poses unique standardization challenges including stochastic behavior, data dependencies, immature evaluation practices, and lifecycle dynamics. The paper argues that AI systems are typically components within larger sociotechnical systems, requiring a layered approach where horizontal standards define process obligations and evidence structures while sectoral profiles specify domain-specific thresholds and acceptance criteria. It proposes a workable scheme based on risk management, reproducible technical checks redefined as stability of measured properties, structured documentation, comprehensive logging, and assurance cases that evolve over the system lifecycle. The paper demonstrates that despite methodological difficulties, technical standards remain essential for translating legal obligations into auditable engineering practice and enabling scalable conformity assessment across providers, assessors, and enforcement authorities

</details>


### [720] [Impact of LLMs news Sentiment Analysis on Stock Price Movement Prediction](https://arxiv.org/abs/2602.00086)
*Walid Siala,Ahmed Khanfir,Mike Papadakis*

Main category: q-fin.ST

Relevance: 35.0

TL;DR: 该论文通过比较DeBERTa、RoBERTa和FinBERT三种LLM进行新闻情感分析，用于股票价格走势预测，发现DeBERTa表现最佳（75%准确率），集成模型可达80%，情感特征对部分股票预测模型有轻微提升。


<details>
  <summary>Details</summary>
Motivation: 现有研究大多将情感分析模型和股票预测方法分开评估，缺乏对新闻情感在此任务中益处的深入理解，以及对不同架构类型在此背景下综合评估的不足。

Method: 评估研究比较了三种LLM（DeBERTa、RoBERTa、FinBERT）进行情感驱动的股票预测，并测试了集成模型。同时评估了情感特征对LSTM、PatchTST、tPatchGNN分类器以及PatchTST、TimesNet回归模型的影响。

Result: DeBERTa在三种模型中表现最佳，准确率达75%；集成三种模型的集成模型准确率可提升至约80%；新闻情感特征对部分股票市场预测模型（LSTM、PatchTST、tPatchGNN分类器以及PatchTST、TimesNet回归模型）有轻微提升效果。

Conclusion: 新闻情感分析对股票预测有一定价值，DeBERTa在此任务中表现最优，集成方法可进一步提升性能，但情感特征的提升效果相对有限。

Abstract: This paper addresses stock price movement prediction by leveraging LLM-based news sentiment analysis. Earlier works have largely focused on proposing and assessing sentiment analysis models and stock movement prediction methods, however, separately. Although promising results have been achieved, a clear and in-depth understanding of the benefit of the news sentiment to this task, as well as a comprehensive assessment of different architecture types in this context, is still lacking. Herein, we conduct an evaluation study that compares 3 different LLMs, namely, DeBERTa, RoBERTa and FinBERT, for sentiment-driven stock prediction. Our results suggest that DeBERTa outperforms the other two models with an accuracy of 75% and that an ensemble model that combines the three models can increase the accuracy to about 80%. Also, we see that sentiment news features can benefit (slightly) some stock market prediction models, i.e., LSTM-, PatchTST- and tPatchGNN-based classifiers and PatchTST- and TimesNet-based regression tasks models.

</details>


### [721] [Radiomics in Medical Imaging: Methods, Applications, and Challenges](https://arxiv.org/abs/2602.00102)
*Fnu Neha,Deepak kumar Shukla*

Main category: eess.IV

Relevance: 35.0

TL;DR: 这篇综述对放射组学（radiomics）全流程进行了端到端分析，探讨了从图像采集到临床部署各阶段方法学决策对特征稳定性、模型可靠性和临床转化有效性的影响，并指出了标准化、领域偏移等开放挑战。


<details>
  <summary>Details</summary>
Motivation: 尽管放射组学在医学图像定量分析方面取得了进展，但仍面临特征不稳定性、可重复性有限、验证偏差和临床转化受限等挑战。现有综述多关注特定应用结果或孤立流程组件，缺乏对全流程相互依赖设计选择如何影响鲁棒性和泛化性的综合分析。

Method: 采用端到端的放射组学流程分析方法，系统回顾了特征提取、选择和降维策略；经典机器学习和深度学习建模方法；集成和混合框架；重点分析了验证协议、数据泄漏预防和统计可靠性。

Result: 识别了放射组学在标准化、领域偏移和临床部署方面的开放挑战，并提出了未来发展方向，包括混合放射组学-AI模型、多模态融合、联邦学习和标准化基准测试。

Conclusion: 放射组学需要更全面的端到端分析框架来应对现有挑战，未来应关注方法学标准化、鲁棒性验证和临床转化有效性，以推动该领域向更可靠、可重复的临床应用发展。

Abstract: Radiomics enables quantitative medical image analysis by converting imaging data into structured, high-dimensional feature representations for predictive modeling. Despite methodological developments and encouraging retrospective results, radiomics continue to face persistent challenges related to feature instability, limited reproducibility, validation bias, and restricted clinical translation. Existing reviews largely focus on application-specific outcomes or isolated pipeline components, with limited analysis of how interdependent design choices across acquisition, preprocessing, feature engineering, modeling, and evaluation collectively affect robustness and generalizability. This survey provides an end-to-end analysis of radiomics pipelines, examining how methodological decisions at each stage influence feature stability, model reliability, and translational validity. This paper reviews radiomic feature extraction, selection, and dimensionality reduction strategies; classical machine and deep learning-based modeling approaches; and ensemble and hybrid frameworks, with emphasis on validation protocols, data leakage prevention, and statistical reliability. Clinical applications are discussed with a focus on evaluation rigor rather than reported performance metrics. The survey identifies open challenges in standardization, domain shift, and clinical deployment, and outlines future directions such as hybrid radiomics-artificial intelligence models, multimodal fusion, federated learning, and standardized benchmarking.

</details>


### [722] [Why Are AI Agent Involved Pull Requests (Fix-Related) Remain Unmerged? An Empirical Study](https://arxiv.org/abs/2602.00164)
*Khairul Alam,Saikat Mondal,Banani Roy*

Main category: cs.SE

Relevance: 35.0

TL;DR: 对AI编码代理（如OpenAI Codex、Devin、GitHub Copilot）在真实软件仓库中生成修复相关PR的实际效果进行实证研究，分析其接受率、失败原因及影响因素。


<details>
  <summary>Details</summary>
Motivation: 随着AI编码代理在软件仓库中生成修复相关PR的应用日益增多，了解这些AI贡献在实际项目中的接受情况和失败原因对于评估其真实效果、指导AI编码代理改进以及促进人机协作至关重要。

Method: 1. 使用AIDEV POP数据集，分析8,106个由5个主流AI编码代理生成的修复相关PR，量化合并、关闭未合并、仍开放的比例；2. 对326个已关闭但未合并的PR进行手动定性分析（约100人时），构建包含12个失败原因的结构化分类目录。

Result: 测试用例失败和相同问题已被其他PR解决是最常见的未合并原因，而构建或部署失败相对较少。研究揭示了当前AI编码代理在真实环境中的关键局限性。

Conclusion: 当前AI编码代理在真实软件维护场景中仍存在显著局限性，特别是在处理测试用例和避免重复修复方面。研究结果为AI编码代理的进一步改进以及在软件维护中更有效的人机协作指明了方向。

Abstract: Autonomous coding agents (e.g., OpenAI Codex, Devin, GitHub Copilot) are increasingly used to generate fix-related pull requests (PRs) in real world software repositories. However, their practical effectiveness depends on whether these contributions are accepted and merged by project maintainers. In this paper, we present an empirical study of AI agent involved fix related PRs, examining both their integration outcomes, latency, and the factors that hinder successful merging. We first analyze 8,106 fix related PRs authored by five widely used AI coding agents from the AIDEV POP dataset to quantify the proportions of PRs that are merged, closed without merging, or remain open. We then conduct a manual qualitative analysis of a statistically significant sample of 326 closed but unmerged PRs, spending approximately 100 person hours to construct a structured catalog of 12 failure reasons. Our results indicate that test case failures and prior resolution of the same issues by other PRs are the most common causes of non integration, whereas build or deployment failures are comparatively rare. Overall, our findings expose key limitations of current AI coding agents in real world settings and highlight directions for their further improvement and for more effective human AI collaboration in software maintenance.

</details>


### [723] [Spec-Driven Development:From Code to Contract in the Age of AI Coding Assistants](https://arxiv.org/abs/2602.00180)
*Deepak Babu Piskala*

Main category: cs.SE

Relevance: 35.0

TL;DR: 本文为实践者提供了规范驱动开发（SDD）的全面指南，将规范视为软件开发的主要工件，代码作为生成或验证的次要产物，介绍了三种规范严谨性级别及实际应用案例。


<details>
  <summary>Details</summary>
Motivation: AI编程助手的兴起重新激发了规范驱动开发的兴趣，传统开发流程将代码作为主要工件，而SDD将规范视为真相来源，旨在提高软件开发的可靠性、可维护性和可验证性。

Method: 提出了三种规范严谨性级别：spec-first（规范优先）、spec-anchored（规范锚定）和spec-as-source（规范即源码），分析了从BDD框架到GitHub Spec Kit等现代AI辅助工具，并通过API开发、企业系统和嵌入式软件等案例研究展示实际应用。

Result: 展示了规范驱动开发在不同领域的实际应用案例，提供了帮助实践者确定何时采用SDD能带来价值、何时简单方法足够的决策框架。

Conclusion: 规范驱动开发为软件开发提供了新的范式，通过将规范作为主要工件，可以提高软件质量和开发效率，但需要根据具体场景选择合适的规范严谨性级别。

Abstract: The rise of AI coding assistants has reignited interest in an old idea: what if specifications-not code-were the primary artifact of software development? Spec-driven development (SDD) inverts the traditional workflow by treating specifications as the source of truth and code as a generated or verified secondary artifact. This paper provides practitioners with a comprehensive guide to SDD, covering its principles, workflow patterns, and supporting tools. We present three levels of specification rigor-spec-first, spec-anchored, and spec-as-source-with clear guidance on when each applies. Through analysis of tools ranging from Behavior-Driven Development frameworks to modern AI-assisted toolkits like GitHub Spec Kit, we demonstrate how the spec-first philosophy maps to real implementations. We present case studies from API development, enterprise systems, and embedded software, illustrating how different domains apply SDD. We conclude with a decision framework helping practitioners determine when SDD provides value and when simpler approaches suffice.

</details>


### [724] [EigenAI: Deterministic Inference, Verifiable Results](https://arxiv.org/abs/2602.00182)
*David Ribeiro Alves,Vishnu Patankar,Matheus Pereira,Jamie Stephens,Nima Vaziri,Sreeram Kannan*

Main category: cs.CR

Relevance: 35.0

TL;DR: EigenAI是一个基于EigenLayer再质押生态的可验证AI平台，通过确定性LLM推理引擎和加密经济安全乐观重执行协议，实现可公开审计、可复现且经济可执行的AI推理结果。


<details>
  <summary>Details</summary>
Motivation: 当前AI系统缺乏透明度和可验证性，用户无法验证推理结果的正确性。该论文旨在构建一个可验证的AI平台，使AI推理结果能够被公开审计、复现，并在必要时通过经济机制强制执行，从而创建可信的AI代理。

Method: 1. 使用确定性大型语言模型推理引擎确保推理的确定性
2. 采用加密经济安全乐观重执行协议：不可信操作者在固定GPU架构上运行推理，将加密日志发布到EigenDA
3. 挑战窗口期内，任何观察者可通过EigenVerify请求重执行
4. 在可信执行环境(TEE)中确定性重新计算，使用阈值释放的解密密钥进行公开挑战
5. 推理的比特精确性使验证简化为字节相等性检查

Result: 该架构能够创建主权代理（预测市场裁判、交易机器人、科学助手），这些代理既享受最先进的性能，又从以太坊验证者基础继承安全性。单个诚实副本足以检测欺诈。

Conclusion: EigenAI通过结合确定性LLM推理和加密经济安全协议，实现了可验证的AI推理，为构建可信AI系统提供了新范式，使AI代理能够在保持高性能的同时获得区块链级别的安全性保证。

Abstract: EigenAI is a verifiable AI platform built on top of the EigenLayer restaking ecosystem. At a high level, it combines a deterministic large-language model (LLM) inference engine with a cryptoeconomically secured optimistic re-execution protocol so that every inference result can be publicly audited, reproduced, and, if necessary, economically enforced. An untrusted operator runs inference on a fixed GPU architecture, signs and encrypts the request and response, and publishes the encrypted log to EigenDA. During a challenge window, any watcher may request re-execution through EigenVerify; the result is then deterministically recomputed inside a trusted execution environment (TEE) with a threshold-released decryption key, allowing a public challenge with private data. Because inference itself is bit-exact, verification reduces to a byte-equality check, and a single honest replica suffices to detect fraud. We show how this architecture yields sovereign agents -- prediction-market judges, trading bots, and scientific assistants -- that enjoy state-of-the-art performance while inheriting security from Ethereum's validator base.

</details>


### [725] [LPIPS-AttnWav2Lip: Generic Audio-Driven lip synchronization for Talking Head Generation in the Wild](https://arxiv.org/abs/2602.00189)
*Zhipeng Chen,Xinheng Wang,Lun Xie,Haijie Yuan,Hang Pan*

Main category: cs.SD

Relevance: 35.0

TL;DR: 提出LPIPS-AttnWav2Lip方法，通过语义对齐模块和LPIPS损失函数实现音频驱动的说话头生成，提升唇部同步准确性和视觉质量。


<details>
  <summary>Details</summary>
Motivation: 音频驱动说话头生成的关键挑战是实现唇部与音频的视听一致性（唇部同步）。现有方法在唇部同步准确性和视觉质量方面仍有改进空间。

Method: 1. 使用基于残差CBAM的U-Net架构编码和融合音频与视觉模态信息；2. 语义对齐模块扩展生成器网络的感受野，高效获取视觉特征的空间和通道信息；3. 将视觉特征的统计信息与音频潜在向量匹配，实现音频内容信息对视觉信息的调整和注入；4. 采用LPIPS损失函数模拟人类对图像质量的判断，减少训练过程中的不稳定性。

Result: 在唇部同步准确性和视觉质量方面表现出色，主观和客观评估结果均验证了方法的有效性。

Conclusion: 提出的LPIPS-AttnWav2Lip方法能够生成高质量、唇部同步准确的说话头视频，为音频驱动的人脸图像重建提供了有效的通用解决方案。

Abstract: Researchers have shown a growing interest in Audio-driven Talking Head Generation. The primary challenge in talking head generation is achieving audio-visual coherence between the lips and the audio, known as lip synchronization. This paper proposes a generic method, LPIPS-AttnWav2Lip, for reconstructing face images of any speaker based on audio. We used the U-Net architecture based on residual CBAM to better encode and fuse audio and visual modal information. Additionally, the semantic alignment module extends the receptive field of the generator network to obtain the spatial and channel information of the visual features efficiently; and match statistical information of visual features with audio latent vector to achieve the adjustment and injection of the audio content information to the visual information. To achieve exact lip synchronization and to generate realistic high-quality images, our approach adopts LPIPS Loss, which simulates human judgment of image quality and reduces instability possibility during the training process. The proposed method achieves outstanding performance in terms of lip synchronization accuracy and visual quality as demonstrated by subjective and objective evaluation results. The code for the paper is available at the following link: https://github.com/FelixChan9527/LPIPS-AttnWav2Lip

</details>


### [726] [Intelligent Reasoning Cues: A Framework and Case Study of the Roles of AI Information in Complex Decisions](https://arxiv.org/abs/2602.00259)
*Venkatesh Sivaraman,Eric P. Mason,Mengfan Ellen Li,Jessica Tong,Andrew J. King,Jeremy M. Kahn,Adam Perer*

Main category: cs.HC

Relevance: 35.0

TL;DR: 该研究提出将AI决策支持系统界面重新概念化为"智能推理线索"的集合，通过临床决策案例探索不同线索类型如何影响医生决策过程


<details>
  <summary>Details</summary>
Motivation: 现有AI辅助决策理论主要关注校准对AI建议的依赖程度，但缺乏对AI系统设计如何影响底层推理过程的理解。研究者希望填补这一空白，探索AI界面设计如何通过不同的信息呈现方式影响决策者的推理过程。

Method: 1. 将AI界面重新概念化为"智能推理线索"的集合；2. 在重症监护室脓毒症治疗这一高风险临床决策场景中，识别八种推理线索类型；3. 通过六个团队的背景调查和25名医生的有声思维研究，分析不同线索对决策过程的影响模式。

Result: 研究发现：1. 不同推理线索具有独特的影响模式，可直接指导界面设计；2. 推理线索应优先应用于高变异性和自由裁量权大的任务；3. 需要适应不断变化的决策需求以确保兼容性；4. 应为复杂病例提供互补且严谨的见解。

Conclusion: 通过将AI界面重新概念化为推理线索集合，研究者为AI辅助决策系统设计提供了新的理论框架和实践指导，强调关注底层推理过程而不仅仅是校准依赖程度。

Abstract: Artificial intelligence (AI)-based decision support systems can be highly accurate yet still fail to support users or improve decisions. Existing theories of AI-assisted decision-making focus on calibrating reliance on AI advice, leaving it unclear how different system designs might influence the reasoning processes underneath. We address this gap by reconsidering AI interfaces as collections of intelligent reasoning cues: discrete pieces of AI information that can individually influence decision-making. We then explore the roles of eight types of reasoning cues in a high-stakes clinical decision (treating patients with sepsis in intensive care). Through contextual inquiries with six teams and a think-aloud study with 25 physicians, we find that reasoning cues have distinct patterns of influence that can directly inform design. Our results also suggest that reasoning cues should prioritize tasks with high variability and discretion, adapt to ensure compatibility with evolving decision needs, and provide complementary, rigorous insights on complex cases.

</details>


### [727] [Standardized Methods and Recommendations for Green Federated Learning](https://arxiv.org/abs/2602.00343)
*Austin Tapp,Holger R. Roth,Ziyue Xu,Abhijeet Parida,Hareem Nisar,Marius George Linguraru*

Main category: cs.DC

Relevance: 35.0

TL;DR: 提出联邦学习碳排放核算方法，结合NVFlare和CodeCarbon进行阶段感知跟踪，并考虑通信排放，验证显示系统级延迟和协调效应显著影响碳足迹


<details>
  <summary>Details</summary>
Motivation: 联邦学习的环境影响难以跨研究比较，因为测量边界不一致和报告异质性。需要标准化的碳排放核算方法来支持可复现的"绿色"联邦学习评估

Method: 提出实用的联邦学习碳排放核算方法，使用NVIDIA NVFlare和CodeCarbon进行显式的阶段感知任务跟踪（初始化、每轮训练、评估、空闲/协调）。通过可配置网络能量模型估计通信排放，捕捉非计算效应

Result: 在CIFAR-10中，系统级延迟和协调效应使碳排放增加8.34倍（中等效率）和21.73倍（低效率）。在视网膜分割中，GPU层级交换（H100 vs V100）产生1.7倍运行时差距，但不同站点的总能量和碳排放变化不均匀

Conclusion: 支持标准化的碳排放核算方法作为可复现"绿色"联邦学习评估的前提。需要每站点和每轮的报告来准确评估环境影响

Abstract: Federated learning (FL) enables collaborative model training over privacy-sensitive, distributed data, but its environmental impact is difficult to compare across studies due to inconsistent measurement boundaries and heterogeneous reporting. We present a practical carbon-accounting methodology for FL CO2e tracking using NVIDIA NVFlare and CodeCarbon for explicit, phase-aware tasks (initialization, per-round training, evaluation, and idle/coordination). To capture non-compute effects, we additionally estimate communication emissions from transmitted model-update sizes under a network-configurable energy model. We validate the proposed approach on two representative workloads: CIFAR-10 image classification and retinal optic disk segmentation. In CIFAR-10, controlled client-efficiency scenarios show that system-level slowdowns and coordination effects can contribute meaningfully to carbon footprint under an otherwise fixed FL protocol, increasing total CO2e by 8.34x (medium) and 21.73x (low) relative to the high-efficiency baseline. In retinal segmentation, swapping GPU tiers (H100 vs.\ V100) yields a consistent 1.7x runtime gap (290 vs. 503 minutes) while producing non-uniform changes in total energy and CO2e across sites, underscoring the need for per-site and per-round reporting. Overall, our results support a standardized carbon accounting method that acts as a prerequisite for reproducible 'green' FL evaluation. Our code is available at https://github.com/Pediatric-Accelerated-Intelligence-Lab/carbon_footprint.

</details>


### [728] [ZEST: Zero-shot Embodied Skill Transfer for Athletic Robot Control](https://arxiv.org/abs/2602.00401)
*Jean Pierre Sleiman,He Li,Alphonsus Adu-Bredu,Robin Deits,Arun Kumar,Kevin Bergamin,Mohak Bhardwaj,Scott Biddlestone,Nicola Burger,Matthew A. Estrada,Francesco Iacobelli,Twan Koolen,Alexander Lambert,Erica Lin,M. Eva Mungai,Zach Nobles,Shane Rozen-Levy,Yuyao Shi,Jiashun Wang,Jakob Welner,Fangzhou Yu,Mike Zhang,Alfred Rizzi,Jessica Hodgins,Sylvain Bertrand,Yeuhi Abe,Scott Kuindersma,Farbod Farshidian*

Main category: cs.RO

Relevance: 35.0

TL;DR: ZEST是一个零样本技能迁移框架，通过强化学习从多种运动数据源（动作捕捉、视频、动画）训练策略，无需接触标签、参考窗口或状态估计器，可直接部署到硬件上实现动态、多接触技能。


<details>
  <summary>Details</summary>
Motivation: 解决人形机器人全身控制的挑战，避免传统方法中需要大量技能工程和脆弱的控制器调优过程，实现从多种数据源到机器人的零样本技能迁移。

Method: 结合自适应采样（关注困难运动片段）和基于模型的辅助力矩自动课程学习，使用中等程度的领域随机化在仿真中训练，提供从近似分析值选择关节增益的方法和精化执行器模型。

Result: 在波士顿动力Atlas人形机器人上实现了动态多接触技能（如军队爬行、霹雳舞），将舞蹈和场景交互技能从视频直接迁移到Atlas和Unitree G1，并扩展到Spot四足机器人实现连续后空翻等杂技动作。

Conclusion: ZEST展示了跨异构数据源和不同形态机器人的鲁棒零样本部署能力，建立了生物运动与机器人对应物之间的可扩展接口。

Abstract: Achieving robust, human-like whole-body control on humanoid robots for agile, contact-rich behaviors remains a central challenge, demanding heavy per-skill engineering and a brittle process of tuning controllers. We introduce ZEST (Zero-shot Embodied Skill Transfer), a streamlined motion-imitation framework that trains policies via reinforcement learning from diverse sources -- high-fidelity motion capture, noisy monocular video, and non-physics-constrained animation -- and deploys them to hardware zero-shot. ZEST generalizes across behaviors and platforms while avoiding contact labels, reference or observation windows, state estimators, and extensive reward shaping. Its training pipeline combines adaptive sampling, which focuses training on difficult motion segments, and an automatic curriculum using a model-based assistive wrench, together enabling dynamic, long-horizon maneuvers. We further provide a procedure for selecting joint-level gains from approximate analytical armature values for closed-chain actuators, along with a refined model of actuators. Trained entirely in simulation with moderate domain randomization, ZEST demonstrates remarkable generality. On Boston Dynamics' Atlas humanoid, ZEST learns dynamic, multi-contact skills (e.g., army crawl, breakdancing) from motion capture. It transfers expressive dance and scene-interaction skills, such as box-climbing, directly from videos to Atlas and the Unitree G1. Furthermore, it extends across morphologies to the Spot quadruped, enabling acrobatics, such as a continuous backflip, through animation. Together, these results demonstrate robust zero-shot deployment across heterogeneous data sources and embodiments, establishing ZEST as a scalable interface between biological movements and their robotic counterparts.

</details>


### [729] [From Junior to Senior: Allocating Agency and Navigating Professional Growth in Agentic AI-Mediated Software Engineering](https://arxiv.org/abs/2602.00496)
*Dana Feng,Bhada Yun,April Wang*

Main category: cs.HC

Relevance: 35.0

TL;DR: 该论文研究了AI如何改变软件工程中的权力分配，通过混合方法研究初级和高级开发者的AI使用情况，发现组织政策而非个人偏好是制约AI代理权的主要因素，并提出了在编码、学习和指导中保持代理权的实践建议。


<details>
  <summary>Details</summary>
Motivation: 研究动机是理解AI如何重塑软件工程工作中的权力分配和专业成长路径。随着AI工具日益普及，需要了解不同经验水平的开发者如何与AI互动，以及组织政策如何影响他们在AI辅助工作中的自主权。

Method: 采用三阶段混合方法：1) ACTA结合Delphi流程与5名高级开发者；2) AI辅助调试任务与10名初级开发者；3) 5名高级开发者对初级开发者提示历史的盲审。

Result: 研究发现：1) 软件工程中的代理权主要受组织政策而非个人偏好制约；2) 高级开发者通过详细委派保持控制，而初级开发者则在过度依赖和谨慎回避间挣扎；3) 高级开发者利用前AI时代的基础直觉来引导现代工具，对指导初级开发者的AI辅助职业发展具有宝贵视角。

Conclusion: 结论是随着AI日益自主，需要关注在软件工程中保持代理权的实践，特别是在编码、学习和指导三个维度。高级开发者的经验对指导初级开发者在AI鼓励的职业发展中至关重要。

Abstract: Juniors enter as AI-natives, seniors adapted mid-career. AI is not just changing how engineers code-it is reshaping who holds agency across work and professional growth. We contribute junior-senior accounts on their usage of agentic AI through a three-phase mixed-methods study: ACTA combined with a Delphi process with 5 seniors, an AI-assisted debugging task with 10 juniors, and blind reviews of junior prompt histories by 5 more seniors. We found that agency in software engineering is primarily constrained by organizational policies rather than individual preferences, with experienced developers maintaining control through detailed delegation while novices struggle between over-reliance and cautious avoidance. Seniors leverage pre-AI foundational instincts to steer modern tools and possess valuable perspectives for mentoring juniors in their early AI-encouraged career development. From synthesis of results, we suggest three practices that focus on preserving agency in software engineering for coding, learning, and mentorship, especially as AI grows increasingly autonomous.

</details>


### [730] [RAG-GNN: Integrating Retrieved Knowledge with Graph Neural Networks for Precision Medicine](https://arxiv.org/abs/2602.00586)
*Hasi Hays,William J. Richardson*

Main category: q-bio.MN

Relevance: 35.0

TL;DR: 提出一个检索增强生成(RAG)嵌入框架，将图神经网络表示与文献检索知识通过对比学习结合，用于生物医学网络分析。网络拓扑在结构预测上表现优异，而RAG-GNN在功能聚类上表现独特优势。


<details>
  <summary>Details</summary>
Motivation: 网络拓扑擅长结构预测但无法捕捉生物医学文献中的功能语义信息。需要整合网络结构与文献知识来提升生物医学网络的功能解释能力。

Method: 提出RAG-GNN框架：1) 图神经网络获取网络拓扑表示；2) 动态检索文献知识；3) 通过对比学习整合两种表示；4) 信息论分解分析不同信息源的贡献。

Result: 1) 拓扑方法在链接预测上表现优异(GCN: 0.983 AUROC)；2) RAG-GNN是唯一在功能聚类上获得正轮廓系数的方法(0.001 vs. 所有基线负值)；3) 拓扑贡献77.3%预测信息，检索文档提供8.6%独特信息；4) 在癌症信号网络中识别出DDR1作为治疗靶点。

Conclusion: 拓扑方法和检索增强方法具有互补性：结构预测任务可由网络拓扑单独解决，而功能解释任务独特受益于检索知识。

Abstract: Network topology excels at structural predictions but fails to capture functional semantics encoded in biomedical literature. We present a retrieval-augmented generation (RAG) embedding framework that integrates graph neural network representations with dynamically retrieved literature-derived knowledge through contrastive learning. Benchmarking against ten embedding methods reveals task-specific complementarity: topology-focused methods achieve near-perfect link prediction (GCN: 0.983 AUROC), while RAG-GNN is the only method achieving positive silhouette scores for functional clustering (0.001 vs. negative scores for all baselines). Information-theoretic decomposition shows network topology contributes 77.3% of predictive information, while retrieved documents provide 8.6% unique information. Applied to cancer signaling networks (379 proteins, 3,498 interactions), the framework identifies DDR1 as a therapeutic target based on retrieved evidence of synthetic lethality with KRAS mutations. These results establish that topology-only and retrieval-augmented approaches serve complementary purposes: structural prediction tasks are solved by network topology alone, while functional interpretation uniquely benefits from retrieved knowledge.

</details>


### [731] [MarkCleaner: High-Fidelity Watermark Removal via Imperceptible Micro-Geometric Perturbation](https://arxiv.org/abs/2602.01513)
*Xiaoxi Kong,Jieyu Yuan,Pengdi Chen,Yuanlin Zhang,Chongyi Li,Bin Li*

Main category: eess.IV

Relevance: 35.0

TL;DR: MarkCleaner：一种通过微几何扰动去除语义水印的框架，避免再生式方法导致的语义漂移，利用空间位移破坏相位对齐来有效去除水印


<details>
  <summary>Details</summary>
Motivation: 传统语义水印对常规图像空间攻击具有强鲁棒性，但研究发现这种鲁棒性无法抵抗微几何扰动。空间位移可以通过破坏相位对齐来去除水印，这为开发更有效的水印去除方法提供了新思路

Method: 提出MarkCleaner框架：1) 使用微几何扰动监督训练，使模型能够分离语义内容与严格空间对齐；2) 采用掩码引导编码器学习显式空间表示；3) 使用基于2D高斯泼溅的解码器显式参数化几何扰动同时保留语义内容

Result: 实验表明MarkCleaner在水印去除效果和视觉保真度方面均表现优异，同时支持高效的实时推理

Conclusion: 微几何扰动是去除语义水印的有效方法，MarkCleaner框架通过破坏相位对齐实现了高效水印去除，避免了语义漂移问题

Abstract: Semantic watermarks exhibit strong robustness against conventional image-space attacks. In this work, we show that such robustness does not survive under micro-geometric perturbations: spatial displacements can remove watermarks by breaking the phase alignment. Motivated by this observation, we introduce MarkCleaner, a watermark removal framework that avoids semantic drift caused by regeneration-based watermark removal. Specifically, MarkCleaner is trained with micro-geometry-perturbed supervision, which encourages the model to separate semantic content from strict spatial alignment and enables robust reconstruction under subtle geometric displacements. The framework adopts a mask-guided encoder that learns explicit spatial representations and a 2D Gaussian Splatting-based decoder that explicitly parameterizes geometric perturbations while preserving semantic content. Extensive experiments demonstrate that MarkCleaner achieves superior performance in both watermark removal effectiveness and visual fidelity, while enabling efficient real-time inference. Our code will be made available upon acceptance.

</details>


### [732] [Exploration of Unary Arithmetic-Based Matrix Multiply Units for Low Precision DL Accelerators](https://arxiv.org/abs/2602.00838)
*Prabhu Vellaisamy,Harideep Nair,Di Wu,Shawn Blanton,John Paul Shen*

Main category: cs.AR

Relevance: 35.0

TL;DR: 本文对三种最新的单精度GEMM设计（uGEMM、tuGEMM、tubGEMM）与传统的二进制GEMM进行了详细评估，展示了单精度GEMM在未来边缘AI加速器中实现高效能计算的应用潜力。


<details>
  <summary>Details</summary>
Motivation: 随着深度学习向低精度发展，单精度GEMM设计作为传统二进制GEMM硬件的替代方案被提出。需要对最新的单精度和二进制GEMM设计进行严格评估，以评估单精度硬件在未来深度学习计算中的潜力。

Method: 1) 对三种最新单精度设计（uGEMM、tuGEMM、tubGEMM）与传统二进制GEMM进行详细评估
2) 在不同位宽和矩阵尺寸下进行严格的合成后评估
3) 在八个预训练卷积神经网络和LLaMA2大语言模型上进行权重稀疏性分析

Result: 通过评估确定了各种设计的权衡点和最优工作点，展示了单精度GEMM如何有效用于未来边缘AI加速器的高效能计算。

Conclusion: 单精度GEMM设计在低精度深度学习推理中具有潜力，特别是在边缘AI加速器场景下，能够提供更高效的计算解决方案。

Abstract: General matrix multiplication (GEMM) is a fundamental operation in deep learning (DL). With DL moving increasingly toward low precision, recent works have proposed novel unary GEMM designs as an alternative to conventional binary GEMM hardware. A rigorous evaluation of recent unary and binary GEMM designs is needed to assess the potential of unary hardware for future DL compute. This paper focuses on unary GEMM designs for integer-based DL inference and performs a detailed evaluation of three latest unary design proposals, namely, uGEMM, tuGEMM and tubGEMM, by comparing them to a conventional binary GEMM. Rigorous post-synthesis evaluations beyond prior works are performed across varying bit-widths and matrix sizes to assess the designs' tradeoffs and determine optimal sweetspots. Further, we perform weight sparsity analysis across eight pretrained convolutional neural networks (CNNs) and the LLaMA2 large language model (LLM). In this work, we demonstrate how unary GEMM can be effectively used for energy-efficient compute in future edge AI accelerators.

</details>


### [733] [Offline Discovery of Interpretable Skills from Multi-Task Trajectories](https://arxiv.org/abs/2602.01018)
*Chongyu Zhu,Mithun Vanniasinghe,Jiayu Chen,Chi-Guhn Lee*

Main category: cs.RO

Relevance: 35.0

TL;DR: LOKI是一个三阶段端到端学习框架，用于从离线演示数据中发现可重用技能并进行分层模仿学习，无需显式奖励或子任务标注。


<details>
  <summary>Details</summary>
Motivation: 分层模仿学习需要从长时程、多任务的离线数据中发现可重用技能，但数据通常缺乏显式奖励或子任务标注，这是主要挑战。

Method: 三阶段框架：1) 使用弱监督任务标签的VQ-VAE进行粗粒度任务感知宏分割；2) 自监督序列模型进行微分割，迭代聚类巩固技能边界；3) 基于选项框架构建分层策略，学习显式技能切换的终止条件。

Result: 在D4RL Kitchen基准测试中取得高成功率，优于标准HIL基线；发现的技能具有语义意义，符合人类直觉，并能通过组合解决未见任务。

Conclusion: LOKI框架有效解决了离线技能发现和分层模仿学习问题，能够从无标注数据中发现可组合的语义技能。

Abstract: Hierarchical Imitation Learning is a powerful paradigm for acquiring complex robot behaviors from demonstrations. A central challenge, however, lies in discovering reusable skills from long-horizon, multi-task offline data, especially when the data lacks explicit rewards or subtask annotations. In this work, we introduce LOKI, a three-stage end-to-end learning framework designed for offline skill discovery and hierarchical imitation. The framework commences with a two-stage, weakly supervised skill discovery process: Stage one performs coarse, task-aware macro-segmentation by employing an alignment-enforced Vector Quantized VAE guided by weak task labels. Stage two then refines these segments at a micro-level using a self-supervised sequential model, followed by an iterative clustering process to consolidate skill boundaries. The third stage then leverages these precise boundaries to construct a hierarchical policy within an option-based framework-complete with a learned termination condition beta for explicit skill switching. LOKI achieves high success rates on the challenging D4RL Kitchen benchmark and outperforms standard HIL baselines. Furthermore, we demonstrate that the discovered skills are semantically meaningful, aligning with human intuition, and exhibit compositionality by successfully sequencing them to solve a novel, unseen task.

</details>


### [734] [FedBGS: A Blockchain Approach to Segment Gossip Learning in Decentralized Systems](https://arxiv.org/abs/2602.01185)
*Fabio Turazza,Marcello Pietri,Marco Picone,Marco Mamei*

Main category: cs.CR

Relevance: 35.0

TL;DR: FedBGS：基于区块链的完全去中心化联邦学习框架，通过分段八卦学习和联邦分析优化区块链使用，提供全面攻击防护，处理非IID数据


<details>
  <summary>Details</summary>
Motivation: 传统联邦学习存在单点故障问题，服务器作为中心节点在安全和可扩展性方面存在限制。尽管已有隐私保护技术，但中心化架构仍面临安全风险。需要完全去中心化的解决方案来消除单点故障，同时确保隐私保护和对抗各类攻击。

Method: 提出FedBGS框架，结合区块链技术实现完全去中心化架构。采用分段八卦学习（Segmented Gossip Learning）和联邦分析（Federated Analytics）优化区块链使用。通过去中心化共识机制替代传统中心服务器，同时提供全面的攻击防护机制。

Result: FedBGS框架能够消除传统联邦学习的单点故障问题，提供更高级别的安全性和可扩展性。系统能够全面防护各类攻击，同时有效处理非独立同分布（non-IID）数据，在保持隐私保护的前提下优化区块链资源使用。

Conclusion: FedBGS为隐私保护联邦学习提供了完全去中心化的解决方案，通过区块链技术和分段八卦学习机制，在消除单点故障的同时确保系统安全性和可扩展性，特别适合对数据隐私有严格要求的应用场景。

Abstract: Privacy-Preserving Federated Learning (PPFL) is a Decentralized machine learning paradigm that enables multiple participants to collaboratively train a global model without sharing their data with the integration of cryptographic and privacy-based techniques to enhance the security of the global system. This privacy-oriented approach makes PPFL a highly suitable solution for training shared models in sectors where data privacy is a critical concern. In traditional FL, local models are trained on edge devices, and only model updates are shared with a central server, which aggregates them to improve the global model. However, despite the presence of the aforementioned privacy techniques, in the classical Federated structure, the issue of the server as a single-point-of-failure remains, leading to limitations both in terms of security and scalability. This paper introduces FedBGS, a fully Decentralized Blockchain-based framework that leverages Segmented Gossip Learning through Federated Analytics. The proposed system aims to optimize blockchain usage while providing comprehensive protection against all types of attacks, ensuring both privacy, security and non-IID data handling in Federated environments.

</details>


### [735] [AI Meets Plasticity: A Comprehensive Survey](https://arxiv.org/abs/2602.01215)
*Hadi Bakhshan,Sima Farshbaf,Junior Ramirez Machado,Fernando Rastellini Canela,Josep Maria Carbonell*

Main category: cond-mat.mtrl-sci

Relevance: 35.0

TL;DR: 本文对人工智能与材料塑性研究的交叉领域进行了全面综述，系统梳理了从经典机器学习到生成式AI等多种方法在材料塑性建模、表征和预测中的应用。


<details>
  <summary>Details</summary>
Motivation: AI正在成为科学发现的新范式，在材料科学与工程领域已产生变革性影响。本文旨在系统审视AI与材料塑性的相互作用，为研究者提供清晰的路线图，推动这一重要交叉领域的发展。

Method: 采用综述研究方法，从两个维度构建分类体系：1）材料科学视角：分析塑性变形的因果关系、微观结构表征和宏观本构模型；2）AI方法视角：涵盖频率主义方法（经典ML、深度学习、物理信息模型）和概率框架（不确定性量化、生成式AI）。

Result: 建立了基于AI方法的全面分类体系，重点区分了模型架构、数据需求和预测性能等关键方面，为材料塑性领域的研究者提供了系统化的方法论框架。

Conclusion: 本综述为材料塑性研究提供了AI驱动的清晰路线图，不仅有助于材料科学研究者理解AI方法，也为AI研究者提供了重要的应用领域，推动了AI驱动的材料科学新时代的发展。

Abstract: Artificial intelligence (AI) is rapidly emerging as a new paradigm of scientific discovery, namely data-driven science, across nearly all scientific disciplines. In materials science and engineering, AI has already begun to exert a transformative influence, making it both timely and necessary to examine its interaction with materials plasticity. In this study, we present a holistic survey of the convergence between AI and plasticity, highlighting state-of-the-art AI methodologies employed to discover, construct surrogate models for, and emulate the plastic behavior of materials. From a materials science perspective, we examine cause-and-effect relationships governing plastic deformation, including microstructural characterization and macroscopic responses described through plasticity constitutive models. From the perspective of AI methodology, we review a broad spectrum of applied approaches, ranging from frequentist techniques such as classical machine learning (ML), deep learning (DL), and physics-informed models to probabilistic frameworks that incorporate uncertainty quantification and generative AI methods. These data-driven approaches are discussed in the context of materials characterization and plasticity-related applications. The primary objective of this survey is to develop a comprehensive and well-organized taxonomy grounded in AI methodologies, with particular emphasis on distinguishing critical aspects of these techniques, including model architectures, data requirements, and predictive performance within the specific domain of materials plasticity. By doing so, this work aims to provide a clear road map for researchers and practitioners in the materials community, while offering deeper physical insight and intuition into the role of AI in advancing materials plasticity and characterization, an area of growing importance in the emerging AI-driven era.

</details>


### [736] [On the Fragility of AI-Based Channel Decoders under Small Channel Perturbations](https://arxiv.org/abs/2602.01582)
*Haoyu Lei,Mohammad Jalali,Chin Wa Lau,Farzan Farnia*

Main category: cs.IT

Relevance: 35.0

TL;DR: AI纠错解码器在AWGN信道上的性能提升可能以鲁棒性为代价，对分布偏移敏感


<details>
  <summary>Details</summary>
Motivation: 研究AI纠错解码器性能提升的来源和代价，特别是对信道输出分布偏移的鲁棒性问题

Method: 通过对抗性扰动（FGM、投影梯度法）和通用对抗性扰动，评估AI解码器在分布偏移下的性能

Result: AI解码器（如ECCT、CrossMPT）在对抗性扰动下性能显著下降，对抗性扰动在AI解码器间转移性强，但对BP解码器转移性弱

Conclusion: AI解码器的性能提升可能伴随着鲁棒性代价，对信道分布变化更敏感

Abstract: Recent advances in deep learning have led to AI-based error correction decoders that report empirical performance improvements over traditional belief-propagation (BP) decoding on AWGN channels. While such gains are promising, a fundamental question remains: where do these improvements come from, and what cost is paid to achieve them? In this work, we study this question through the lens of robustness to distributional shifts at the channel output. We evaluate both input-dependent adversarial perturbations (FGM and projected gradient methods under $\ell_2$ constraints) and universal adversarial perturbations that apply a single norm-bounded shift to all received vectors. Our results show that recent AI decoders, including ECCT and CrossMPT, could suffer significant performance degradation under such perturbations, despite superior nominal performance under i.i.d. AWGN. Moreover, adversarial perturbations transfer relatively strongly between AI decoders but weakly to BP-based decoders, and universal perturbations are substantially more harmful than random perturbations of equal norm. These numerical findings suggest a potential robustness cost and higher sensitivity to channel distribution underlying recent AI decoding gains.

</details>


### [737] [WADEPre: A Wavelet-based Decomposition Model for Extreme Precipitation Nowcasting with Multi-Scale Learning](https://arxiv.org/abs/2602.02096)
*Baitian Liu,Haiping Zhang,Huiling Yuan,Dongjing Wang,Ying Li,Feng Chen,Hao Wu*

Main category: physics.ao-ph

Relevance: 35.0

TL;DR: WADEPre：基于小波分解的极端降水临近预报模型，通过双分支架构分别处理低频平流和高频对流，解决了传统方法对极端值模糊的问题


<details>
  <summary>Details</summary>
Motivation: 降水强度的重尾特性阻碍了精确的降水临近预报。传统基于像素级损失优化的模型容易产生回归到均值的偏差，导致极端值模糊。现有的傅里叶方法也缺乏空间定位能力，无法解析瞬态对流单元。

Method: 提出WADEPre模型，将建模转换到小波域。使用离散小波变换进行显式分解，采用双分支架构：近似网络建模稳定的低频平流，细节网络捕捉高频随机对流，然后通过精炼模块动态重构多尺度分量。引入多尺度课程学习策略，从粗尺度逐步转向细粒度细节监督。

Result: 在SEVIR和上海雷达数据集上的实验表明，WADEPre实现了最先进的性能，在捕捉极端阈值和保持结构保真度方面有显著改进。

Conclusion: WADEPre通过小波域建模和双分支架构，有效解决了降水临近预报中的极端值模糊问题，为极端降水预测提供了新的解决方案。

Abstract: The heavy-tailed nature of precipitation intensity impedes precise precipitation nowcasting. Standard models that optimize pixel-wise losses are prone to regression-to-the-mean bias, which blurs extreme values. Existing Fourier-based methods also lack the spatial localization needed to resolve transient convective cells. To overcome these intrinsic limitations, we propose WADEPre, a wavelet-based decomposition model for extreme precipitation that transitions the modeling into the wavelet domain. By leveraging the Discrete Wavelet Transform for explicit decomposition, WADEPre employs a dual-branch architecture: an Approximation Network to model stable, low-frequency advection, isolating deterministic trends from statistical bias, and a spatially localized Detail Network to capture high-frequency stochastic convection, resolving transient singularities and preserving sharp boundaries. A subsequent Refiner module then dynamically reconstructs these decoupled multi-scale components into the final high-fidelity forecast. To address optimization instability, we introduce a multi-scale curriculum learning strategy that progressively shifts supervision from coarse scales to fine-grained details. Extensive experiments on the SEVIR and Shanghai Radar datasets demonstrate that WADEPre achieves state-of-the-art performance, yielding significant improvements in capturing extreme thresholds and maintaining structural fidelity. Our code is available at https://github.com/sonderlau/WADEPre.

</details>


### [738] [Self-Evolving Coordination Protocol in Multi-Agent AI Systems: An Exploratory Systems Feasibility Study](https://arxiv.org/abs/2602.02170)
*Jose Manuel de la Chica Rodriguez,Juan Manuel Vera Díaz*

Main category: cs.MA

Relevance: 35.0

TL;DR: 本文提出自演化协调协议(SECP)，在保持固定形式不变量的前提下，允许有限的外部验证自修改，为受治理的多智能体系统建立基础。


<details>
  <summary>Details</summary>
Motivation: 在金融等安全关键和受监管领域，多智能体系统的协调机制必须满足严格的形式要求、保持可审计性并在明确界限内运行。协调逻辑需要作为治理层而非优化启发式方法。

Method: 提出自演化协调协议(SECP)，在受控概念验证环境中，比较四种协调机制：一致硬否决、加权标量聚合、SECP v1.0（智能体设计的非标量协议）和SECP v2.0（一次治理修改的结果）。所有机制在相同硬约束下运行。

Result: 单次递归修改将提案覆盖率从2个提升到3个接受提案，同时保持所有声明的不变量。证明了有界自修改在技术上是可实现的、可审计的和可分析的。

Conclusion: 有界自修改的协调协议在明确形式约束下是技术上可实现的、可审计的和可分析的，为受治理的多智能体系统建立了架构基础。

Abstract: Contemporary multi-agent systems increasingly rely on internal coordination mechanisms to combine, arbitrate, or constrain the outputs of heterogeneous components. In safety-critical and regulated domains such as finance, these mechanisms must satisfy strict formal requirements, remain auditable, and operate within explicitly bounded limits. Coordination logic therefore functions as a governance layer rather than an optimization heuristic.
  This paper presents an exploratory systems feasibility study of Self-Evolving Coordination Protocols (SECP): coordination protocols that permit limited, externally validated self-modification while preserving fixed formal invariants. We study a controlled proof-of-concept setting in which six fixed Byzantine consensus protocol proposals are evaluated by six specialized decision modules. All coordination regimes operate under identical hard constraints, including Byzantine fault tolerance (f < n/3), O(n2) message complexity, complete non-statistical safety and liveness arguments, and bounded explainability.
  Four coordination regimes are compared in a single-shot design: unanimous hard veto, weighted scalar aggregation, SECP v1.0 (an agent-designed non-scalar protocol), and SECP v2.0 (the result of one governed modification). Outcomes are evaluated using a single metric, proposal coverage, defined as the number of proposals accepted. A single recursive modification increased coverage from two to three accepted proposals while preserving all declared invariants.
  The study makes no claims regarding statistical significance, optimality, convergence, or learning. Its contribution is architectural: it demonstrates that bounded self-modification of coordination protocols is technically implementable, auditable, and analyzable under explicit formal constraints, establishing a foundation for governed multi-agent systems.

</details>


### [739] [Building a Correct-by-Design Lakehouse. Data Contracts, Versioning, and Transactional Pipelines for Humans and Agents](https://arxiv.org/abs/2602.02335)
*Weiming Sheng,Jinlang Wang,Manuel Barros,Aldrin Montana,Jacopo Tagliabue,Luca Bigon*

Main category: cs.DC

Relevance: 35.0

TL;DR: Bauplan是一个代码优先的湖仓，通过类型化表契约、Git式数据版本控制和事务性运行，解决并发操作中的安全问题


<details>
  <summary>Details</summary>
Motivation: 湖仓作为分析和AI的默认云平台，在不可信参与者并发操作生产数据时变得不安全：上下游不匹配仅在运行时出现，多表流水线可能泄露部分效果

Method: 设计Bauplan代码优先湖仓，沿三个轴：1) 类型化表契约使流水线边界可检查；2) Git式数据版本控制用于审查和可复现性；3) 事务性运行保证流水线级原子性

Result: 报告了轻量级正式事务模型的早期结果，并讨论了由反例激发的未来工作

Conclusion: Bauplan通过软件工程启发的方法，使(大多数)非法状态不可表示，提高湖仓的安全性和可靠性

Abstract: Lakehouses are the default cloud platform for analytics and AI, but they become unsafe when untrusted actors concurrently operate on production data: upstream-downstream mismatches surface only at runtime, and multi-table pipelines can leak partial effects. Inspired by software engineering, we design Bauplan, a code-first lakehouse that aims to make (most) illegal states unrepresentable using familiar abstractions. Bauplan acts along three axes: typed table contracts to make pipeline boundaries checkable, Git-like data versioning for review and reproducibility, and transactional runs that guarantee pipeline-level atomicity. We report early results from a lightweight formal transaction model and discuss future work motivated by counterexamples.

</details>


### [740] [Artificial Intelligence and Symmetries: Learning, Encoding, and Discovering Structure in Physical Data](https://arxiv.org/abs/2602.02351)
*Veronica Sanz*

Main category: hep-ph

Relevance: 35.0

TL;DR: 这篇综述探讨了机器学习（特别是变分自编码器）如何从物理数据中识别和学习对称性结构，而不依赖显式的归纳偏置。


<details>
  <summary>Details</summary>
Motivation: 对称性在物理学中至关重要，而机器学习擅长从高维数据中提取低维结构。本文旨在探索如何通过数据驱动的方法识别和学习对称性诱导的约束，而不是通过架构设计强制已知对称性。

Method: 主要关注数据驱动方法和潜在表示学习，特别是变分自编码器。分析对称性和守恒定律如何降低物理数据集的本质维度，以及这种降维如何在生成模型的潜在空间中通过自组织表现出来。

Result: 综述了近期研究成果，包括简单几何系统和粒子物理过程的案例研究，分析了在没有显式归纳偏置的情况下推断对称性结构的理论和实践限制。

Conclusion: 机器学习方法（特别是表示学习）能够有效识别和学习物理数据中的对称性结构，但存在理论和实践上的限制，需要进一步研究如何平衡数据驱动方法和物理先验知识。

Abstract: Symmetries play a central role in physics, organizing dynamics, constraining interactions, and determining the effective number of physical degrees of freedom. In parallel, modern artificial intelligence methods have demonstrated a remarkable ability to extract low-dimensional structure from high-dimensional data through representation learning. This review examines the interplay between these two perspectives, focusing on the extent to which symmetry-induced constraints can be identified, encoded, or diagnosed using machine learning techniques.
  Rather than emphasizing architectures that enforce known symmetries by construction, we concentrate on data-driven approaches and latent representation learning, with particular attention to variational autoencoders. We discuss how symmetries and conservation laws reduce the intrinsic dimensionality of physical datasets, and how this reduction may manifest itself through self-organization of latent spaces in generative models trained to balance reconstruction and compression. We review recent results, including case studies from simple geometric systems and particle physics processes, and analyze the theoretical and practical limitations of inferring symmetry structure without explicit inductive bias.

</details>


### [741] [Uncovering Latent Communication Patterns in Brain Networks via Adaptive Flow Routing](https://arxiv.org/abs/2602.00561)
*Tianhao Huang,Guanghui Min,Zhenyu Lei,Aiying Zhang,Chen Chen*

Main category: cs.AI

Relevance: 30.0

TL;DR: 提出AFR-Net框架，通过神经通信动力学视角进行多模态融合，建模结构连接如何产生功能通信模式，实现可解释的神经通路发现。


<details>
  <summary>Details</summary>
Motivation: 现有方法缺乏基础神经科学洞见，无法揭示连接组背后的神经区域潜在相互作用，也不能解释为什么结构连接和功能连接会表现出耦合和异质性的动态状态。

Method: 提出自适应流路由网络（AFR-Net），这是一个物理信息框架，通过神经通信动力学视角建模结构约束如何产生功能通信模式。

Result: 大量实验表明AFR-Net显著优于最先进的基线方法。

Conclusion: AFR-Net能够实现可解释的关键神经通路发现，为理解宏观认知表型如何从微观神经元连接中涌现提供了新视角。

Abstract: Unraveling how macroscopic cognitive phenotypes emerge from microscopic neuronal connectivity remains one of the core pursuits of neuroscience. To this end, researchers typically leverage multi-modal information from structural connectivity (SC) and functional connectivity (FC) to complete downstream tasks. Recent methodologies explore the intricate coupling mechanisms between SC and FC, attempting to fuse their representations at the regional level. However, lacking fundamental neuroscientific insight, these approaches fail to uncover the latent interactions between neural regions underlying these connectomes, and thus cannot explain why SC and FC exhibit dynamic states of both coupling and heterogeneity. In this paper, we formulate multi-modal fusion through the lens of neural communication dynamics and propose the Adaptive Flow Routing Network (AFR-Net), a physics-informed framework that models how structural constraints (SC) give rise to functional communication patterns (FC), enabling interpretable discovery of critical neural pathways. Extensive experiments demonstrate that AFR-Net significantly outperforms state-of-the-art baselines. The code is available at https://anonymous.4open.science/r/DIAL-F0D1.

</details>


### [742] [Neuro-symbolic AI for Predictive Maintenance (PdM) -- review and recommendations](https://arxiv.org/abs/2602.00731)
*Kyle Hamilton,Ali Intizar*

Main category: cs.AI

Relevance: 30.0

TL;DR: 本文对过去五年工业预测性维护(PdM)进行系统综述，指出数据驱动方法精度高但需要大量标注数据、泛化性差、缺乏可解释性，而传统基于规则的方法精度低、误报多。作者提出将深度学习与符号逻辑结合的神经符号AI作为解决方案。


<details>
  <summary>Details</summary>
Motivation: 当前预测性维护领域面临两难：数据驱动方法（如深度学习）精度高但需要大量标注数据、缺乏泛化性和可解释性；传统基于规则的方法精度低、误报多、需要专家持续维护。需要一种能结合两者优势的解决方案。

Method: 1. 对过去五年工业预测性维护文献进行系统综述；2. 分析数据驱动方法和传统方法的优缺点；3. 提出神经符号AI作为混合解决方案；4. 描述具体的神经符号架构，特别是使用传感器数据和手工规则作为输入的架构。

Result: 1. 数据驱动方法在精度上优于传统知识系统；2. 混合系统有潜力克服单一方法的弱点；3. 神经符号AI能创建更准确、可解释、鲁棒的预测性维护系统；4. 建立了通用框架并回顾了当前建模方法和挑战。

Conclusion: 神经符号AI是预测性维护领域有前景的方向，能结合数据驱动方法的精度和符号系统的可解释性，解决当前工业应用中的关键障碍。

Abstract: In this document we perform a systematic review the State-of-the-art in Predictive Maintenance (PdM) over the last five years in industrial settings such as commercial buildings, pharmaceutical facilities, or semi-conductor manufacturing. In general, data-driven methods such as those based on deep learning, exhibit higher accuracy than traditional knowledge-based systems. These systems however, are not without significant limitations. The need for large labeled data sets, a lack of generalizibility to new environments (out-of-distribution generalization), and a lack of transparency at inference time are some of the obstacles to adoption in real world environments. In contrast, traditional approaches based on domain expertise in the form of rules, logic or first principles suffer from poor accuracy, many false positives and a need for ongoing expert supervision and manual tuning. While the majority of approaches in recent literature utilize some form of data-driven architecture, there are hybrid systems which also take into account domain specific knowledge. Such hybrid systems have the potential to overcome the weaknesses of either approach on its own while preserving their strengths. We propose taking the hybrid approach even further and integrating deep learning with symbolic logic, or Neuro-symbolic AI, to create more accurate, explainable, interpretable, and robust systems. We describe several neuro-symbolic architectures and examine their strengths and limitations within the PdM domain. We focus specifically on methods which involve the use of sensor data and manually crafted rules as inputs by describing concrete NeSy architectures. In short, this survey outlines the context of modern maintenance, defines key concepts, establishes a generalized framework, reviews current modeling approaches and challenges, and introduces the proposed focus on Neuro-symbolic AI (NESY).

</details>


### [743] [Adaptive Quantum-Safe Cryptography for 6G Vehicular Networks via Context-Aware Optimization](https://arxiv.org/abs/2602.01342)
*Poushali Sengupta,Mayank Raikwar,Sabita Maharjan,Frank Eliassen,Yan Zhang*

Main category: cs.CR

Relevance: 30.0

TL;DR: 提出一个自适应后量子密码学框架，用于6G车联网，通过预测性多目标进化算法动态选择密码配置，减少延迟和开销，并防止算法切换时的攻击。


<details>
  <summary>Details</summary>
Motivation: 未来量子计算机可能破解现有车联网安全通信，后量子密码学虽能提供保护但会增加计算开销和延迟，影响6G车联网的实时性要求。

Method: 提出自适应后量子密码学框架，预测短期移动性和信道变化，使用预测性多目标进化算法动态选择格基、编码或哈希基密码配置，并设计安全单调升级协议防止切换攻击。

Result: 端到端延迟降低27%，通信开销减少65%，有效稳定密码切换行为，单调升级协议成功防止降级、重放和去同步攻击。

Conclusion: 该框架为未来6G车联网提供了一条实用的量子安全密码学路径，在动态环境中平衡安全性和性能要求。

Abstract: Powerful quantum computers in the future may be able to break the security used for communication between vehicles and other devices (Vehicle-to-Everything, or V2X). New security methods called post-quantum cryptography can help protect these systems, but they often require more computing power and can slow down communication, posing a challenge for fast 6G vehicle networks. In this paper, we propose an adaptive post-quantum cryptography (PQC) framework that predicts short-term mobility and channel variations and dynamically selects suitable lattice-, code-, or hash-based PQC configurations using a predictive multi-objective evolutionary algorithm (APMOEA) to meet vehicular latency and security constraints.However, frequent cryptographic reconfiguration in dynamic vehicular environments introduces new attack surfaces during algorithm transitions. A secure monotonic-upgrade protocol prevents downgrade, replay, and desynchronization attacks during transitions. Theoretical results show decision stability under bounded prediction error, latency boundedness under mobility drift, and correctness under small forecast noise. These results demonstrate a practical path toward quantum-safe cryptography in future 6G vehicular networks. Through extensive experiments based on realistic mobility (LuST), weather (ERA5), and NR-V2X channel traces, we show that the proposed framework reduces end-to-end latency by up to 27\%, lowers communication overhead by up to 65\%, and effectively stabilizes cryptographic switching behavior using reinforcement learning. Moreover, under the evaluated adversarial scenarios, the monotonic-upgrade protocol successfully prevents downgrade, replay, and desynchronization attacks.

</details>


### [744] [Governance at the Edge of Architecture: Regulating NeuroAI and Neuromorphic Systems](https://arxiv.org/abs/2602.01503)
*Afifah Kashif,Abdul Muhsin Hameed,Asim Iqbal*

Main category: cs.ET

Relevance: 30.0

TL;DR: 当前AI治理框架基于传统神经网络和冯·诺依曼硬件设计，但神经形态AI系统（基于脉冲神经网络和神经形态硬件）打破了这些假设，需要治理框架与架构共同演化


<details>
  <summary>Details</summary>
Motivation: 当前AI治理框架（包括准确性、延迟和能效等监管基准）是为静态、集中训练的神经网络在冯·诺依曼硬件上设计的，而神经形态AI系统（基于神经形态硬件和脉冲神经网络）打破了这些假设，需要重新思考治理方法

Method: 分析当前AI治理框架对神经形态AI的局限性，提出保证和审计方法需要与这些架构共同演化，将传统监管指标与脑启发计算的物理特性、学习动态和具身效率相结合

Result: 识别了当前治理框架与神经形态AI系统之间的不匹配，提出了需要技术基础保证的治理框架演化方向

Conclusion: AI治理框架需要与神经形态AI架构共同演化，将传统监管指标与脑启发计算的特性对齐，以实现技术基础保证

Abstract: Current AI governance frameworks, including regulatory benchmarks for accuracy, latency, and energy efficiency, are built for static, centrally trained artificial neural networks on von Neumann hardware. NeuroAI systems, embodied in neuromorphic hardware and implemented via spiking neural networks, break these assumptions. This paper examines the limitations of current AI governance frameworks for NeuroAI, arguing that assurance and audit methods must co-evolve with these architectures, aligning traditional regulatory metrics with the physics, learning dynamics, and embodied efficiency of brain-inspired computation to enable technically grounded assurance.

</details>


### [745] [AI-Assisted Adaptive Rendering for High-Frequency Security Telemetry in Web Interfaces](https://arxiv.org/abs/2602.01671)
*Mona Rajhans*

Main category: cs.HC

Relevance: 30.0

TL;DR: AI辅助的自适应渲染框架，通过动态调节视觉更新频率、优先处理语义相关事件、选择性聚合低优先级数据，显著减少渲染开销


<details>
  <summary>Details</summary>
Motivation: 现代网络安全平台需要实时处理高频遥测数据，传统渲染技术在高流量条件下（每秒数十万事件）会导致UI冻结、丢帧或数据过时

Method: 基于行为驱动启发式和轻量级设备端机器学习模型的自适应渲染框架，动态调节视觉更新频率，优先处理语义相关事件，选择性聚合低优先级数据

Result: 实验验证显示渲染开销减少45-60%，同时保持分析师对实时响应的感知

Conclusion: AI辅助的自适应渲染框架能有效解决高流量网络安全平台的实时可视化问题，显著提升性能

Abstract: Modern cybersecurity platforms must process and display high-frequency telemetry such as network logs, endpoint events, alerts, and policy changes in real time. Traditional rendering techniques based on static pagination or fixed polling intervals fail under volume conditions exceeding hundreds of thousands of events per second, leading to UI freezes, dropped frames, or stale data. This paper presents an AI-assisted adaptive rendering framework that dynamically regulates visual update frequency, prioritizes semantically relevant events, and selectively aggregates lower-priority data using behavior-driven heuristics and lightweight on-device machine learning models. Experimental validation demonstrates a 45-60 percent reduction in rendering overhead while maintaining analyst perception of real-time responsiveness.

</details>


### [746] [Scalable and Secure AI Inference in Healthcare: A Comparative Benchmarking of FastAPI and Triton Inference Server on Kubernetes](https://arxiv.org/abs/2602.00053)
*Ratul Ali*

Main category: cs.AI

Relevance: 25.0

TL;DR: 论文比较了FastAPI和NVIDIA Triton两种ML模型部署范式在医疗AI场景下的性能，发现FastAPI在单请求延迟上表现更好（22ms p50），而Triton通过动态批处理实现更高吞吐量（780 RPS）。作者提出混合架构作为最佳实践。


<details>
  <summary>Details</summary>
Motivation: 医疗和制药等受监管领域需要高效可扩展的ML模型部署方案，必须平衡实时临床决策的低延迟、医疗记录批处理的高吞吐量以及HIPAA等数据隐私标准的严格要求。

Method: 采用参考架构在Kubernetes上部署DistilBERT情感分析模型，对比FastAPI REST服务和NVIDIA Triton推理服务器的性能。测量p50和p95延迟以及吞吐量，并评估混合架构（FastAPI作为安全网关，Triton作为后端推理）。

Result: FastAPI在单请求工作负载上p50延迟为22ms，表现更好；Triton通过动态批处理实现780 RPS的吞吐量，几乎是基准的两倍。混合架构结合了两者优势。

Conclusion: 混合架构（FastAPI作为安全网关处理PHI去标识化，Triton负责后端推理）是临床AI企业部署的最佳实践，为安全、高可用部署提供了蓝图。

Abstract: Efficient and scalable deployment of machine learning (ML) models is a prerequisite for modern production environments, particularly within regulated domains such as healthcare and pharmaceuticals. In these settings, systems must balance competing requirements, including minimizing inference latency for real-time clinical decision support, maximizing throughput for batch processing of medical records, and ensuring strict adherence to data privacy standards such as HIPAA. This paper presents a rigorous benchmarking analysis comparing two prominent deployment paradigms: a lightweight, Python-based REST service using FastAPI, and a specialized, high-performance serving engine, NVIDIA Triton Inference Server. Leveraging a reference architecture for healthcare AI, we deployed a DistilBERT sentiment analysis model on Kubernetes to measure median (p50) and tail (p95) latency, as well as throughput, under controlled experimental conditions. Our results indicate a distinct trade-off. While FastAPI provides lower overhead for single-request workloads with a p50 latency of 22 ms, Triton achieves superior scalability through dynamic batching, delivering a throughput of 780 requests per second on a single NVIDIA T4 GPU, nearly double that of the baseline. Furthermore, we evaluate a hybrid architectural approach that utilizes FastAPI as a secure gateway for protected health information de-identification and Triton for backend inference. This study validates the hybrid model as a best practice for enterprise clinical AI and offers a blueprint for secure, high-availability deployments.

</details>


### [747] [Learning to Price: Interpretable Attribute-Level Models for Dynamic Markets](https://arxiv.org/abs/2602.00188)
*Srividhya Sethuraman,Chandrashekar Lakshminarayanan*

Main category: cs.AI

Relevance: 25.0

TL;DR: 提出AFDLD可解释需求模型和ADEPT在线学习算法，用于高维市场的动态定价，实现可解释性和效率的平衡。


<details>
  <summary>Details</summary>
Motivation: 高维市场动态定价面临可扩展性、不确定性和可解释性挑战。现有低秩bandit方法虽然学习效率高，但依赖潜在特征，无法解释单个产品属性如何影响价格。

Method: 提出AFDLD可解释需求模型（加性特征分解低维需求模型），将产品价格表示为属性级贡献之和，并显式建模替代效应。基于此结构提出ADEPT算法（加性分解定价与交叉弹性时适应学习），这是一种无投影、无梯度的在线学习算法，直接在属性空间操作。

Result: ADEPT实现亚线性遗憾界$\tilde{\mathcal{O}}(\sqrt{d}T^{3/4})$。通过合成研究和真实数据集验证：(i)在动态市场条件下学习接近最优价格；(ii)快速适应冲击和漂移；(iii)提供透明的属性级价格解释。

Conclusion: 通过结构化、属性驱动的表示，可以在自主定价代理中同时实现可解释性和效率。

Abstract: Dynamic pricing in high-dimensional markets poses fundamental challenges of scalability, uncertainty, and interpretability. Existing low-rank bandit formulations learn efficiently but rely on latent features that obscure how individual product attributes influence price. We address this by introducing an interpretable \emph{Additive Feature Decomposition-based Low-Dimensional Demand (\textbf{AFDLD}) model}, where product prices are expressed as the sum of attribute-level contributions and substitution effects are explicitly modeled. Building on this structure, we propose \textbf{ADEPT} (Additive DEcomposition for Pricing with cross-elasticity and Time-adaptive learning)-a projection-free, gradient-free online learning algorithm that operates directly in attribute space and achieves a sublinear regret of $\tilde{\mathcal{O}}(\sqrt{d}T^{3/4})$. Through controlled synthetic studies and real-world datasets, we show that ADEPT (i) learns near-optimal prices under dynamic market conditions, (ii) adapts rapidly to shocks and drifts, and (iii) yields transparent, attribute-level price explanations. The results demonstrate that interpretability and efficiency in autonomous pricing agents can be achieved jointly through structured, attribute-driven representations.

</details>


### [748] [JSR-GFNet: Jamming-to-Signal Ratio-Aware Dynamic Gating for Interference Classification in future Cognitive Global Navigation Satellite Systems](https://arxiv.org/abs/2602.00042)
*Zhihan Zeng,Hongyuan Shu,Kaihe Wang,Lu Chen,Amir Hussian,Yanjun Huang,Junchu Zhao,Yue Xiu,Zhongpei Zhang*

Main category: eess.SP

Relevance: 25.0

TL;DR: JSR-GFNet：一种用于GNSS干扰分类的多模态网络，结合IQ样本和STFT谱图，通过物理启发的动态门控机制自适应融合特征，在低信噪比下显著提升性能。


<details>
  <summary>Details</summary>
Motivation: 传统基于时频分析和CNN的GNSS干扰分类方法存在两个根本限制：1）低JSR（干扰信号比）下噪声掩盖导致性能严重下降；2）仅使用幅度谱图丢失相位信息导致"特征退化"，使频谱相似的信号（如高阶QAM与带限高斯噪声）无法区分。

Method: 提出JSR引导融合网络（JSR-GFNet），多模态架构结合相位敏感的复数IQ样本和STFT谱图。核心是物理启发的动态门控机制，基于统计信号描述符作为条件控制器，自主估计信号可靠性，动态重新加权复数ResNet（IQ流）和EfficientNet骨干（STFT流）的贡献。

Result: 在CGI-21数据集（模拟21种干扰类别）上的广泛实验表明，JSR-GFNet在10-50 dB JSR全频谱范围内实现更高准确率。可解释性分析证实模型学习到物理直观策略：在噪声受限区域优先考虑频谱能量整合，在高SNR场景转向关注相位精度以解决调制模糊。

Conclusion: 该框架为下一代航空航天导航安全提供鲁棒解决方案，通过多模态融合和自适应门控机制克服传统方法的局限性，在低JSR下保持性能并解决相位信息丢失问题。

Abstract: The transition toward cognitive global navigation satellite system (GNSS) receivers requires accurate interference classification to trigger adaptive mitigation strategies. However, conventional methods relying on Time-Frequency Analysis (TFA) and Convolutional Neural Networks (CNNs) face two fundamental limitations: severe performance degradation in low Jamming-to-Signal Ratio (JSR) regimes due to noise obscuration, and ``feature degeneracy'' caused by the loss of phase information in magnitude-only spectrograms. Consequently, spectrally similar signals -- such as high-order Quadrature Amplitude Modulation versus Band-Limited Gaussian Noise -- become indistinguishable. To overcome these challenges, this paper proposes the \textbf{JSR-Guided Fusion Network (JSR-GFNet)}. This multi-modal architecture combines phase-sensitive complex In-Phase/Quadrature (IQ) samples with Short-Time Fourier Transform (STFT) spectrograms. Central to this framework is a physics-inspired dynamic gating mechanism driven by statistical signal descriptors. Acting as a conditional controller, it autonomously estimates signal reliability to dynamically reweight the contributions of a Complex-Valued ResNet (IQ stream) and an EfficientNet backbone (STFT stream). To validate the model, we introduce the Comprehensive GNSS Interference (CGI-21) dataset, simulating 21 jamming categories including software-defined waveforms from aerial platforms. Extensive experiments demonstrate that JSR-GFNet achieves higher accuracy across the full 10--50 dB JSR spectrum. Notably, interpretability analysis confirms that the model learns a physically intuitive strategy: prioritizing spectral energy integration in noise-limited regimes while shifting focus to phase precision in high-SNR scenarios to resolve modulation ambiguities. This framework provides a robust solution for next-generation aerospace navigation security.

</details>


### [749] [Frequent Pattern Mining approach to Image Compression](https://arxiv.org/abs/2602.00100)
*Avinash Kadimisetty,C. Oswald,B. Sivalselvan*

Main category: eess.IV

Relevance: 25.0

TL;DR: 该论文提出了一种基于频繁模式挖掘的图像压缩方法，通过聚类相似像素并使用聚类标识符进行压缩，相比传统JPEG实现了45%的压缩比提升，同时保持视觉质量损失可忽略。


<details>
  <summary>Details</summary>
Motivation: 传统JPEG压缩使用DCT变换处理图像数据，但存在冗余数据处理效率不高的问题。作者希望通过频繁模式挖掘技术更有效地处理图像中的冗余信息，提高压缩效率。

Method: 提出结合k-means聚类和闭频繁序列挖掘的方法替代传统JPEG的DCT阶段。通过改进的广义序列模式挖掘算法优化编码模式基数，使用剪枝技术减少码表大小，并设计新的序列频率计算机制。

Result: 在基准数据集上测试显示压缩比提升45%，通常优于现有替代方法。PSNR和SSIM图像质量指标测试表明视觉质量损失可忽略。

Conclusion: 基于频繁模式挖掘的图像压缩方法能够显著提高压缩效率，同时保持图像质量，为图像压缩领域提供了新的有效技术路径。

Abstract: The paper focuses on Image Compression, explaining efficient approaches based on Frequent Pattern Mining(FPM). The proposed compression mechanism is based on clustering similar pixels in the image and thus using cluster identifiers in image compression. Redundant data in the image is effectively handled by replacing the DCT phase of conventional JPEG through a mixture of k-means Clustering and Closed Frequent Sequence Mining. To optimize the cardinality of pattern(s) in encoding, efficient pruning techniques have been used through the refinement of Conventional Generalized Sequential Pattern Mining(GSP) algorithm. We have proposed a mechanism for finding the frequency of a sequence which will yield significant reduction in the code table size. The algorithm is tested by compressing benchmark datasets yielding an improvement of 45% in compression ratios, often outperforming the existing alternatives. PSNR and SSIM, which are the image quality metrics, have been tested which show a negligible loss in visual quality.

</details>


### [750] [SCALED : Surrogate-gradient for Codec-Aware Learning of Downsampling in ABR Streaming](https://arxiv.org/abs/2602.00198)
*Esteban Pesnel,Julien Le Tanou,Michael Ropert,Thomas Maugey,Aline Roumy*

Main category: eess.IV

Relevance: 25.0

TL;DR: 本文提出了一种新框架，能够在训练过程中使用真实、不可微分的视频编解码器，通过数据驱动的代理梯度实现端到端训练，相比编解码器无关的训练方法在BD-BR指标上提升了5.19%。


<details>
  <summary>Details</summary>
Motivation: 视频流媒体架构面临挑战，自适应比特率(ABR)流媒体通常将处理阶段孤立优化，导致端到端率失真性能不佳。虽然深度学习推动了联合优化，但标准视频编解码器的不可微分性阻碍了端到端训练。现有方法使用可微分代理模型，但这些只是近似，可能无法完全捕捉标准编解码器的行为。

Method: 提出了一种新颖框架，通过从实际压缩误差中推导出的数据驱动代理梯度，实现与真实、不可微分编解码器的端到端训练。该框架促进了训练目标与部署性能的对齐。

Result: 实验结果显示，相比编解码器无关的训练方法，在BD-BR(PSNR)指标上实现了5.19%的改进，在整个率失真凸包上（跨越多个下采样比例）表现一致。

Conclusion: 该框架成功解决了使用真实编解码器进行端到端训练的挑战，通过数据驱动的代理梯度方法显著提升了视频压缩性能，为视频流媒体优化提供了更有效的解决方案。

Abstract: The rapid growth in video consumption has introduced significant challenges to modern streaming architectures. Over-the-Top (OTT) video delivery now predominantly relies on Adaptive Bitrate (ABR) streaming, which dynamically adjusts bitrate and resolution based on client-side constraints such as display capabilities and network bandwidth. This pipeline typically involves downsampling the original high-resolution content, encoding and transmitting it, followed by decoding and upsampling on the client side. Traditionally, these processing stages have been optimized in isolation, leading to suboptimal end-to-end rate-distortion (R-D) performance. The advent of deep learning has spurred interest in jointly optimizing the ABR pipeline using learned resampling methods. However, training such systems end-to-end remains challenging due to the non-differentiable nature of standard video codecs, which obstructs gradient-based optimization. Recent works have addressed this issue using differentiable proxy models, based either on deep neural networks or hybrid coding schemes with differentiable components such as soft quantization, to approximate the codec behavior. While differentiable proxy codecs have enabled progress in compression-aware learning, they remain approximations that may not fully capture the behavior of standard, non-differentiable codecs. To our knowledge, there is no prior evidence demonstrating the inefficiencies of using standard codecs during training. In this work, we introduce a novel framework that enables end-to-end training with real, non-differentiable codecs by leveraging data-driven surrogate gradients derived from actual compression errors. It facilitates the alignment between training objectives and deployment performance. Experimental results show a 5.19\% improvement in BD-BR (PSNR) compared to codec-agnostic training approaches, consistently across the entire rate-distortion convex hull spanning multiple downsampling ratios.

</details>


### [751] [Generalized Inverses of Matrix Products: From Fundamental Subspaces to Randomized Decompositions](https://arxiv.org/abs/2602.00386)
*Michał P. Karpowicz,Gilbert Strang*

Main category: math.NA

Relevance: 25.0

TL;DR: 该论文研究了矩阵乘积A=CR的Moore-Penrose伪逆和广义逆，建立了统一框架，涵盖广义和随机化矩阵逆，并应用于随机线性代数算法。


<details>
  <summary>Details</summary>
Motivation: 建立矩阵乘积广义逆的统一理论框架，从几何角度理解四个基本子空间，为随机化线性代数算法提供理论基础。

Method: 基于矩阵乘积A=CR的几何分析，研究伪逆的反序律、通用公式，提出新的广义随机化公式，并扩展到{1,2}-逆和特殊形式。

Result: 建立了矩阵乘积伪逆的统一框架，揭示了随机SVD、Nyström近似、CUR分解等算法的底层结构，并在稀疏传感器布置和有效电阻估计中展示了应用。

Conclusion: 该框架为广义和随机化矩阵逆提供了统一的理论基础，揭示了现有随机线性代数算法的几何结构，并提供了严格的理论保证。

Abstract: We investigate the Moore-Penrose pseudoinverse and generalized inverse of a matrix product $A=CR$ to establish a unifying framework for generalized and randomized matrix inverses. This analysis is rooted in first principles, focusing on the geometry of the four fundamental subspaces. We examine:
  (1) the reverse order law, $A^+ = R^+C^+$, which holds when $C$ has independent columns and $R$ has independent rows,
  (2) the universally correct formula, $A^+ = (C^+CR)^+(CRR^+)^+$, providing a geometric interpretation of the mappings between the involved subspaces,
  (3) a new generalized randomized formula, $A^+_p = (P^TA)^+P^TAQ(AQ)^+$, which gives $A^+_p = A^+$ if and only if the sketching matrices $P$ and $Q$ preserve the rank of $A$, i.e., $\mathrm{rank}(P^TA) = \mathrm{rank}(AQ) = \mathrm{rank}(A)$.
  The framework is extended to generalized $\{1,2\}$-inverses and specialized forms, revealing the underlying structure of established randomized linear algebra algorithms, including randomized SVD, the Nyström approximation, and CUR decomposition. We demonstrate applications in sparse sensor placement and effective resistance estimation. For the latter, we provide a rigorous quantitative analysis of an approximation scheme, establishing that it always underestimates the true resistance and deriving a worst-case spectral bound on the error of resistance differences.

</details>


### [752] [Quantum Phase Recognition via Quantum Attention Mechanism](https://arxiv.org/abs/2602.00473)
*Jin-Long Chen,Xin Li,Zhang-Qi Yin*

Main category: quant-ph

Relevance: 25.0

TL;DR: 提出一种混合量子-经典注意力模型，用于量子相变分类，在9和15量子比特的集群-伊辛模型中实现了高精度分类，数据效率高且具有鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 多体系统中的量子相变具有复杂的关联结构，传统方法在大系统中面临计算挑战，需要可扩展且数据高效的方法来识别量子相。

Method: 提出混合量子-经典注意力模型，通过交换测试和参数化量子电路实现注意力机制，提取量子态中的关联并进行基态分类。

Result: 在9和15量子比特的集群-伊辛模型基准测试中，模型使用少于100个训练数据实现高分类精度，对训练集变化具有鲁棒性，成功捕捉相敏感特征和特征物理长度尺度。

Conclusion: 该模型为复杂多体系统中的量子相识别提供了一种可扩展且数据高效的方法，展示了量子注意力机制在量子相变研究中的应用潜力。

Abstract: Quantum phase transitions in many-body systems are fundamentally characterized by complex correlation structures, which pose computational challenges for conventional methods in large systems. To address this, we propose a hybrid quantum-classical attention model. This model uses an attention mechanism, realized through swap tests and a parameterized quantum circuit, to extract correlations within quantum states and perform ground-state classification. Benchmarked on the cluster-Ising model with system sizes of 9 and 15 qubits, the model achieves high classification accuracy with less than 100 training data and demonstrates robustness against variations in the training set. Further analysis reveals that the model successfully captures phase-sensitive features and characteristic physical length scales, offering a scalable and data-efficient approach for quantum phase recognition in complex many-body systems.

</details>


### [753] [CLAMP: Contrastive Learning for 3D Multi-View Action-Conditioned Robotic Manipulation Pretraining](https://arxiv.org/abs/2602.00937)
*I-Chun Arthur Liu,Krzysztof Choromanski,Sandy Huang,Connor Schenck*

Main category: cs.RO

Relevance: 25.0

TL;DR: CLAMP是一个用于机器人操作的3D预训练框架，利用点云和机器人动作，通过对比学习关联3D几何信息与动作模式，显著提升学习效率和策略性能。


<details>
  <summary>Details</summary>
Motivation: 现有基于2D图像表示的机器人操作策略无法捕捉3D空间信息，而3D信息对于精确操作至关重要。需要开发能够利用3D几何信息的预训练方法。

Method: 从RGB-D图像和相机外参计算合并点云，重新渲染包含深度和3D坐标的多视图四通道图像观测。通过对比学习在大规模模拟轨迹上预训练编码器，同时预训练扩散策略初始化权重。最后在少量任务演示上微调策略。

Result: CLAMP在6个模拟任务和5个真实世界任务中优于最先进的基线方法，显著提高了学习效率和策略性能，特别是在未见任务上表现优异。

Conclusion: CLAMP通过3D预训练框架成功地将3D几何信息与机器人动作模式关联起来，为机器人操作任务提供了更有效的表示学习方法。

Abstract: Leveraging pre-trained 2D image representations in behavior cloning policies has achieved great success and has become a standard approach for robotic manipulation. However, such representations fail to capture the 3D spatial information about objects and scenes that is essential for precise manipulation. In this work, we introduce Contrastive Learning for 3D Multi-View Action-Conditioned Robotic Manipulation Pretraining (CLAMP), a novel 3D pre-training framework that utilizes point clouds and robot actions. From the merged point cloud computed from RGB-D images and camera extrinsics, we re-render multi-view four-channel image observations with depth and 3D coordinates, including dynamic wrist views, to provide clearer views of target objects for high-precision manipulation tasks. The pre-trained encoders learn to associate the 3D geometric and positional information of objects with robot action patterns via contrastive learning on large-scale simulated robot trajectories. During encoder pre-training, we pre-train a Diffusion Policy to initialize the policy weights for fine-tuning, which is essential for improving fine-tuning sample efficiency and performance. After pre-training, we fine-tune the policy on a limited amount of task demonstrations using the learned image and action representations. We demonstrate that this pre-training and fine-tuning design substantially improves learning efficiency and policy performance on unseen tasks. Furthermore, we show that CLAMP outperforms state-of-the-art baselines across six simulated tasks and five real-world tasks.

</details>


### [754] [Multimodal Machine Learning for Integrating Heterogeneous Analytical Systems](https://arxiv.org/abs/2602.00590)
*Shun Muroga,Hideaki Nakajima,Taiyo Shimizu,Kazufumi Kobashi,Kenji Hata*

Main category: cond-mat.mtrl-sci

Relevance: 25.0

TL;DR: 提出了一种可解释的多模态机器学习框架，用于统一异质分析系统，实现对碳纳米管薄膜的端到端表征，通过融合SEM、拉曼、比表面积和电阻率数据，揭示了微观结构与性能之间的关系。


<details>
  <summary>Details</summary>
Motivation: 理解复杂材料的结构-性能关系需要整合多尺度互补测量。传统方法难以统一异质分析系统，需要一种能够融合多种表征技术并保持物理可解释性的机器学习框架。

Method: 1) 从SEM图像提取定量形态描述符（二值化、骨架化、网络分析）；2) 融合拉曼光谱的结晶度/缺陷指标、气体吸附比表面积、表面电阻率；3) 使用雷达图和UMAP进行多维可视化；4) 训练回归模型（XGBoost等）进行预测；5) 特征重要性分析提供物理解释。

Result: 1) 多模态特征集显示CNT薄膜根据结晶度和缠结程度明显聚类；2) XGBoost在留一法交叉验证中达到最佳预测精度；3) 特征重要性分析表明：表面电阻率主要受结间传输长度尺度、结晶度/缺陷指标和网络连通性控制，而比表面积主要由交叉点密度和空隙尺寸决定。

Conclusion: 提出的多模态机器学习框架为复杂材料的数据驱动、可解释表征提供了通用策略，能够统一异质分析系统并揭示物理机制。

Abstract: Understanding structure-property relationships in complex materials requires integrating complementary measurements across multiple length scales. Here we propose an interpretable "multimodal" machine learning framework that unifies heterogeneous analytical systems for end-to-end characterization, demonstrated on carbon nanotube (CNT) films whose properties are highly sensitive to microstructural variations. Quantitative morphology descriptors are extracted from SEM images via binarization, skeletonization, and network analysis, capturing curvature, orientation, intersection density, and void geometry. These SEM-derived features are fused with Raman indicators of crystallinity/defect states, specific surface area from gas adsorption, and electrical surface resistivity. Multi-dimensional visualization using radar plots and UMAP reveals clear clustering of CNT films according to crystallinity and entanglements. Regression models trained on the multimodal feature set show that nonlinear approaches, particularly XGBoost, achieve the best predictive accuracy under leave-one-out cross-validation. Feature-importance analysis further provides physically meaningful interpretations: surface resistivity is primarily governed by junction-to-junction transport length scales, crystallinity/defect-related metrics, and network connectivity, whereas specific surface area is dominated by intersection density and void size. The proposed multimodal machine learning framework offers a general strategy for data-driven, explainable characterization of complex materials.

</details>


### [755] [Community-Level Modeling of Gyral Folding Patterns for Robust and Anatomically Informed Individualized Brain Mapping](https://arxiv.org/abs/2602.01482)
*Minheng Chen,Tong Chen,Yan Zhuang,Chao Cao,Jing Zhang,Tianming Liu,Lu Zhang,Dajiang Zhu*

Main category: q-bio.NC

Relevance: 25.0

TL;DR: 该论文提出了一种基于谱图表示学习的框架，用于建模大脑皮层折叠的社区级单元，而非孤立的地标，以提高个体化皮层表征和跨被试对应的鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 现有基于地标的方法通常独立建模每个三铰回（3HG），忽略了它们形成高阶折叠社区的事实，这种简化削弱了解剖表征，并使一对一匹配对位置变异和噪声敏感。

Method: 提出谱图表示学习框架：1）使用结合表面拓扑和结构连接的双剖面表示编码每个3HG；2）通过被试特异性谱聚类识别连贯的折叠社区；3）拓扑细化保持解剖连续性；4）引入联合形态-几何匹配，联合优化几何和形态测量相似性。

Result: 在超过1000名人类连接组计划被试中，生成的社区显示出：减少的形态测量方差、更强的模块化组织、改善的半球一致性，以及与基于图谱、基于地标或基于嵌入的基线相比的优越对齐。

Conclusion: 社区级建模为个体化皮层表征和可靠的跨被试对应提供了一个鲁棒且解剖学基础坚实的框架。

Abstract: Cortical folding exhibits substantial inter-individual variability while preserving stable anatomical landmarks that enable fine-scale characterization of cortical organization. Among these, the three-hinge gyrus (3HG) serves as a key folding primitive, showing consistent topology yet meaningful variations in morphology, connectivity, and function. Existing landmark-based methods typically model each 3HG independently, ignoring that 3HGs form higher-order folding communities that capture mesoscale structure. This simplification weakens anatomical representation and makes one-to-one matching sensitive to positional variability and noise. We propose a spectral graph representation learning framework that models community-level folding units rather than isolated landmarks. Each 3HG is encoded using a dual-profile representation combining surface topology and structural connectivity. Subject-specific spectral clustering identifies coherent folding communities, followed by topological refinement to preserve anatomical continuity. For cross-subject correspondence, we introduce Joint Morphological-Geometric Matching, jointly optimizing geometric and morphometric similarity. Across over 1000 Human Connectome Project subjects, the resulting communities show reduced morphometric variance, stronger modular organization, improved hemispheric consistency, and superior alignment compared with atlas-based and landmark-based or embedding-based baselines. These findings demonstrate that community-level modeling provides a robust and anatomically grounded framework for individualized cortical characterization and reliable cross-subject correspondence.

</details>


### [756] [Towards Autonomous Instrument Tray Assembly for Sterile Processing Applications](https://arxiv.org/abs/2602.01679)
*Raghavasimhan Sankaranarayanan,Paul Stuart,Nicholas Ahn,Arno Sungarian,Yash Chitalia*

Main category: cs.RO

Relevance: 25.0

TL;DR: 提出一个全自动机器人系统，用于在无菌处理与分发部门自动分类和结构化包装手术器械，减少人工错误和器械碰撞


<details>
  <summary>Details</summary>
Motivation: SPD部门的手动检查和准备器械托盘耗时、易出错，容易导致污染和器械损坏，需要自动化解决方案来提高安全性和效率

Method: 使用混合感知流水线（YOLO12检测+级联ResNet细粒度分类），集成校准视觉模块、6自由度机器人臂、定制电磁夹爪，以及基于规则的包装算法和3D打印分隔器

Result: 系统展示了高感知精度，与人组装的托盘相比，在工具间碰撞方面有统计学显著减少

Conclusion: 这是迈向自动化SPD工作流程的可扩展第一步，能提高手术准备的安全性和一致性，同时减少处理时间

Abstract: The Sterile Processing and Distribution (SPD) department is responsible for cleaning, disinfecting, inspecting, and assembling surgical instruments between surgeries. Manual inspection and preparation of instrument trays is a time-consuming, error-prone task, often prone to contamination and instrument breakage. In this work, we present a fully automated robotic system that sorts and structurally packs surgical instruments into sterile trays, focusing on automation of the SPD assembly stage. A custom dataset comprising 31 surgical instruments and 6,975 annotated images was collected to train a hybrid perception pipeline using YOLO12 for detection and a cascaded ResNet-based model for fine-grained classification. The system integrates a calibrated vision module, a 6-DOF Staubli TX2-60L robotic arm with a custom dual electromagnetic gripper, and a rule-based packing algorithm that reduces instrument collisions during transport. The packing framework uses 3D printed dividers and holders to physically isolate instruments, reducing collision and friction during transport. Experimental evaluations show high perception accuracy and statistically significant reduction in tool-to-tool collisions compared to human-assembled trays. This work serves as the scalable first step toward automating SPD workflows, improving safety, and consistency of surgical preparation while reducing SPD processing times.

</details>


### [757] [SoMA: A Real-to-Sim Neural Simulator for Robotic Soft-body Manipulation](https://arxiv.org/abs/2602.02402)
*Mu Huang,Hui Wang,Kerui Ren,Linning Xu,Yunsong Zhou,Mulin Yu,Bo Dai,Jiangmiao Pang*

Main category: cs.RO

Relevance: 25.0

TL;DR: SoMA是一个用于软体机器人操作的3D高斯泼溅模拟器，通过将可变形动力学、环境力和机器人关节动作耦合在统一的潜在神经空间中，实现端到端的真实到模拟仿真。


<details>
  <summary>Details</summary>
Motivation: 现有模拟器依赖于预定义物理或数据驱动动力学，缺乏机器人条件控制，限制了准确性、稳定性和泛化能力。在丰富交互下模拟可变形物体仍然是机器人操作中真实到模拟的基本挑战。

Method: 提出SoMA，一个基于3D高斯泼溅的软体操作模拟器。在学习的潜在神经空间中耦合可变形动力学、环境力和机器人关节动作，通过高斯泼溅建模交互，实现可控、稳定的长时程操作，无需预定义物理模型。

Result: SoMA将真实世界机器人操作的重新模拟准确性和泛化能力提高了20%，能够稳定模拟复杂任务如长时程布料折叠，超越了观察到的轨迹。

Conclusion: SoMA通过统一的潜在神经空间表示，为可变形物体操作提供了更准确、稳定和可泛化的真实到模拟仿真框架，解决了现有模拟器的局限性。

Abstract: Simulating deformable objects under rich interactions remains a fundamental challenge for real-to-sim robot manipulation, with dynamics jointly driven by environmental effects and robot actions. Existing simulators rely on predefined physics or data-driven dynamics without robot-conditioned control, limiting accuracy, stability, and generalization. This paper presents SoMA, a 3D Gaussian Splat simulator for soft-body manipulation. SoMA couples deformable dynamics, environmental forces, and robot joint actions in a unified latent neural space for end-to-end real-to-sim simulation. Modeling interactions over learned Gaussian splats enables controllable, stable long-horizon manipulation and generalization beyond observed trajectories without predefined physical models. SoMA improves resimulation accuracy and generalization on real-world robot manipulation by 20%, enabling stable simulation of complex tasks such as long-horizon cloth folding.

</details>


### [758] [Vortex Stretching in the Navier-Stokes Equations and Information Dissipation in Diffusion Models: A Reformulation from a Partial Differential Equation Viewpoint](https://arxiv.org/abs/2602.01071)
*Tsuyoshi Yoneda*

Main category: math.AP

Relevance: 25.0

TL;DR: 该论文提出了一种基于分数扩散模型启发的PDE框架，用于Navier-Stokes方程中涡旋拉伸的逆时间公式化，通过神经网络学习分数函数来构造反向时间粒子轨迹。


<details>
  <summary>Details</summary>
Motivation: 研究Navier-Stokes方程中涡旋拉伸的逆时间动力学问题，传统方法中时间反转会产生不适定的向后拉普拉斯算子，需要新的数学框架来处理这一挑战。

Method: 提出基于分数扩散模型启发的PDE框架，将时间反转产生的不适定向后拉普拉斯算子吸收到通过分数函数表达的漂移项中，采用拉格朗日方法表述逆时间动力学，使用轴对称涡旋拉伸场的离散拉格朗日流，通过神经网络学习分数函数来构造反向时间粒子轨迹。

Result: 数值结果表明，在压缩方向上初始位置信息迅速丢失，而在拉伸方向上相对较好地保留，验证了所提方法的有效性。

Conclusion: 该研究为Navier-Stokes方程中的逆时间涡旋拉伸问题提供了新的数学框架，结合分数扩散模型和神经网络方法，能够有效处理时间反转带来的数学挑战。

Abstract: We present a new inverse-time formulation of vortex stretching in the Navier-Stokes equations, based on a PDE framework inspired by score-based diffusion models. By absorbing the ill-posed backward Laplacian arising from time reversal into a drift term expressed through a score function, the inverse-time dynamics are formulated in a Lagrangian manner. Using a discrete Lagrangian flow of an axisymmetric vortex-stretching field, the score function is learned with a neural network and employed to construct backward-time particle trajectories. Numerical results demonstrate that information about initial positions is rapidly lost in the compressive direction, whereas it is relatively well preserved in the stretching direction.

</details>


### [759] [Towards knowledge-based workflows: a semantic approach to atomistic simulations for mechanical and thermodynamic properties](https://arxiv.org/abs/2602.01358)
*Abril Azocar Guzman,Hoang-Thien Luu,Sarath Menon,Tilmann Hickel,Nina Merkert,Stefan Sandfeld*

Main category: cond-mat.mtrl-sci

Relevance: 25.0

TL;DR: 开发了可重复使用的原子尺度工作流，整合了FAIR数据原则和本体论元数据标注，用于机械和热力学性质模拟，支持AI就绪数据和代理AI工作流。


<details>
  <summary>Details</summary>
Motivation: 当前分子动力学模拟实践中存在脚本碎片化、元数据不一致和可追溯性有限的问题，这阻碍了可重复性、互操作性和重用性。需要采用FAIR数据原则和工作流方法来克服这些限制。

Method: 提出了可重复使用的原子尺度工作流，整合了应用本体论对齐的元数据标注，实现自动可追溯性捕获和FAIR兼容的数据输出。工作流覆盖了关键机械和热力学量，包括状态方程、弹性张量、机械加载、热性质、缺陷形成能和纳米压痕。

Result: 验证了Hall-Petch效应等结构-性质关系，展示了工作流可以在不同原子间势能和材料之间重用，并在一致的语义框架内工作。生成了AI就绪的模拟数据，支持新兴的代理AI工作流。

Conclusion: 该方法为基于知识的机械和热力学模拟建立了可推广的蓝图，通过FAIR原则和工作流方法解决了当前模拟实践中的可重复性和互操作性挑战。

Abstract: Mechanical and thermodynamic properties, including the influence of crystal defects, are critical for evaluating materials in engineering applications. Molecular dynamics simulations provide valuable insight into these mechanisms at the atomic scale. However, current practice often relies on fragmented scripts with inconsistent metadata and limited provenance, which hinders reproducibility, interoperability, and reuse. FAIR data principles and workflow-based approaches offer a path to address these limitations. We present reusable atomistic workflows that incorporate metadata annotation aligned with application ontologies, enabling automatic provenance capture and FAIR-compliant data outputs. The workflows cover key mechanical and thermodynamic quantities, including equation of state, elastic tensors, mechanical loading, thermal properties, defect formation energies, and nanoindentation. We demonstrate validation of structure-property relations such as the Hall-Petch effect and show that the workflows can be reused across different interatomic potentials and materials within a coherent semantic framework. The approach provides AI-ready simulation data, supports emerging agentic AI workflows, and establishes a generalizable blueprint for knowledge-based mechanical and thermodynamic simulations.

</details>


### [760] ["If You're Very Clever, No One Knows You've Used It": The Social Dynamics of Developing Generative AI Literacy in the Workplace](https://arxiv.org/abs/2602.01386)
*Qing,Xia,Marios Constantinides,Advait Sarkar,Duncan Brumby,Anna Cox*

Main category: cs.HC

Relevance: 25.0

TL;DR: 研究通过访谈19名知识工作者，发现职场中隐藏AI使用痕迹被视为专业验证，反而阻碍了知识共享和透明度，建议通过开放对话和协作学习提升AI素养。


<details>
  <summary>Details</summary>
Motivation: 生成式AI工具正在快速改变知识工作，但现有AI素养研究缺乏对职场社交动态如何影响知识工作者对GenAI素养认知的实证洞察，也不清楚工作者如何在职场环境中学习应用这些工具。

Method: 采用深度访谈方法，对19名来自多个行业的知识工作者进行访谈，考察他们在真实职场环境中如何发展GenAI能力。

Result: 研究发现：虽然同事间的知识分享支持学习，但隐藏GenAI使用痕迹的能力被视为领域专业性的验证。这些行为最终减少了通过知识分享学习的机会，并破坏了透明度。

Conclusion: 为提升职场AI素养，应促进开放对话、增加用户生成知识的可见性，并更强调协作学习在应对快速技术发展中的益处。

Abstract: Generative AI (GenAI) tools are rapidly transforming knowledge work, making AI literacy a critical priority for organizations. However, research on AI literacy lacks empirical insight into how knowledge workers' beliefs around GenAI literacy are shaped by the social dynamics of the workplace, and how workers learn to apply GenAI tools in these environments. To address this gap, we conducted in-depth interviews with 19 knowledge workers across multiple sectors to examine how they develop GenAI competencies in real-world professional contexts. We found that, while knowledge sharing from colleagues supported learning, the ability to remove cues indicating GenAI use was perceived as validation of domain expertise. These behaviours ultimately reduced opportunities for learning via knowledge sharing and undermined transparency. To advance workplace AI literacy, we argue for fostering open dialogue, increasing visibility of user-generated knowledge, and greater emphasis on the benefits of collaborative learning for navigating rapid technological developments.

</details>


### [761] [Draw2Learn: A Human-AI Collaborative Tool for Drawing-Based Science Learning](https://arxiv.org/abs/2602.01494)
*Yuqi Hang*

Main category: cs.HC

Relevance: 25.0

TL;DR: Draw2Learn：一个AI辅助绘图学习系统，通过生成结构化绘图任务、提供视觉支架、监控进度和多维度反馈来支持学习


<details>
  <summary>Details</summary>
Motivation: 绘图有助于学习过程，但大规模提供及时反馈具有挑战性。研究探索AI如何作为支持性队友在基于绘图的学习中发挥作用

Method: 将学习原则转化为具体交互模式：AI生成结构化绘图任务、提供可选视觉支架、监控进度、提供多维度反馈。收集用户反馈进行系统开发

Result: 用户反馈显示系统在可用性、实用性和用户体验方面获得积极评价，突出了AI支架的价值和学习者自主性

Conclusion: 贡献了生成式学习中队友导向AI的设计框架，并确定了未来研究的关键考虑因素

Abstract: Drawing supports learning by externalizing mental models, but providing timely feedback at scale remains challenging. We present Draw2Learn, a system that explores how AI can act as a supportive teammate during drawing-based learning. The design translates learning principles into concrete interaction patterns: AI generates structured drawing quests, provides optional visual scaffolds, monitors progress, and delivers multidimensional feedback. We collected formative user feedback during system development and open-ended comments. Feedback showed positive ratings for usability, usefulness, and user experience, with themes highlighting AI scaffolding value and learner autonomy. This work contributes a design framework for teammate-oriented AI in generative learning and identifies key considerations for future research.

</details>


### [762] [DREAMS: A Social Exchange Theory-Informed Modeling of Misinformation Engagement on Social Media](https://arxiv.org/abs/2602.01567)
*Lin Tian,Marian-Andrei Rizoiu*

Main category: cs.SI

Relevance: 25.0

TL;DR: Dreams框架通过社会交换理论指导，将虚假信息参与建模为动态的社会交换过程，在跨平台数据集上实现了最先进的预测性能。


<details>
  <summary>Details</summary>
Motivation: 现有方法通常将社交媒体参与视为同质的时间序列信号，忽视了塑造虚假信息传播的异质社会机制和平台设计。本文旨在探索神经网络架构是否能仅从行为数据中发现社会交换原则。

Method: 提出Dreams框架，将虚假信息参与建模为序列到序列的适应问题，每个动作反映了用户努力和社会奖励之间的动态协商。框架整合自适应机制来学习情感和上下文信号如何随时间跨平台传播。

Result: 在2021-2025年收集的7个平台、237万帖子的跨平台数据集上，Dreams实现了19.25%的平均绝对百分比误差，比最强基线提升了43.6%。模型还揭示了与社会交换原则一致的跨平台模式。

Conclusion: 整合行为理论可以增强在线虚假信息参与的实证建模，神经网络架构能够从行为数据中发现社会交换原则。

Abstract: Social media engagement prediction is a central challenge in computational social science, particularly for understanding how users interact with misinformation. Existing approaches often treat engagement as a homogeneous time-series signal, overlooking the heterogeneous social mechanisms and platform designs that shape how misinformation spreads. In this work, we ask: ``Can neural architectures discover social exchange principles from behavioral data alone?'' We introduce \textsc{Dreams} (\underline{D}isentangled \underline{R}epresentations and \underline{E}pisodic \underline{A}daptive \underline{M}odeling for \underline{S}ocial media misinformation engagements), a social exchange theory-guided framework that models misinformation engagement as a dynamic process of social exchange. Rather than treating engagement as a static outcome, \textsc{Dreams} models it as a sequence-to-sequence adaptation problem, where each action reflects an evolving negotiation between user effort and social reward conditioned by platform context. It integrates adaptive mechanisms to learn how emotional and contextual signals propagate through time and across platforms. On a cross-platform dataset spanning $7$ platforms and 2.37M posts collected between 2021 and 2025, \textsc{Dreams} achieves state-of-the-art performance in predicting misinformation engagements, reaching a mean absolute percentage error of $19.25$\%. This is a $43.6$\% improvement over the strongest baseline. Beyond predictive gains, the model reveals consistent cross-platform patterns that align with social exchange principles, suggesting that integrating behavioral theory can enhance empirical modeling of online misinformation engagement. The source code is available at: https://github.com/ltian678/DREAMS.

</details>


### [763] [DrawSim-PD: Simulating Student Science Drawings to Support NGSS-Aligned Teacher Diagnostic Reasoning](https://arxiv.org/abs/2602.01578)
*Arijit Chakma,Peng He,Honglu Liu,Zeyuan Wang,Tingting Li,Tiffany D. Do,Feng Liu*

Main category: cs.CY

Relevance: 25.0

TL;DR: DrawSim-PD：首个生成框架，通过模拟NGSS对齐的学生科学绘图来支持教师专业发展，解决真实学生作品隐私限制问题。


<details>
  <summary>Details</summary>
Motivation: 教师专业发展需要接触多样化的学生作品来培养诊断推理能力，但隐私法规限制了真实学生作品的共享。需要一种既能保护隐私又能提供真实训练材料的解决方案。

Method: 提出能力配置文件（结构化认知状态），编码不同表现水平学生的能力范围。基于100个NGSS主题构建10,000个系统化结构化工件，确保跨模态一致性：学生式绘图、第一人称推理叙述和教师诊断概念图。

Result: K-12科学教育专家验证了工件的NGSS对齐性（核心项目>84%正面评价）和解释学生思维的实用性，同时识别了年级极端情况的改进机会。

Conclusion: DrawSim-PD为视觉评估研究克服了数据稀缺障碍，提供了开放基础设施支持教师培训，同时保护学生隐私。

Abstract: Developing expertise in diagnostic reasoning requires practice with diverse student artifacts, yet privacy regulations prohibit sharing authentic student work for teacher professional development (PD) at scale. We present DrawSim-PD, the first generative framework that simulates NGSS-aligned, student-like science drawings exhibiting controllable pedagogical imperfections to support teacher training. Central to our approach are apability profiles--structured cognitive states encoding what students at each performance level can and cannot yet demonstrate. These profiles ensure cross-modal coherence across generated outputs: (i) a student-like drawing, (ii) a first-person reasoning narrative, and (iii) a teacher-facing diagnostic concept map. Using 100 curated NGSS topics spanning K-12, we construct a corpus of 10,000 systematically structured artifacts. Through an expert-based feasibility evaluation, K--12 science educators verified the artifacts' alignment with NGSS expectations (>84% positive on core items) and utility for interpreting student thinking, while identifying refinement opportunities for grade-band extremes. We release this open infrastructure to overcome data scarcity barriers in visual assessment research.

</details>


### [764] [Towards Exploratory and Focused Manipulation with Bimanual Active Perception: A New Problem, Benchmark and Strategy](https://arxiv.org/abs/2602.01939)
*Yuxin He,Ruihao Zhang,Tianao Shen,Cheng Liu,Qiang Nie*

Main category: cs.RO

Relevance: 25.0

TL;DR: 该论文提出探索性与聚焦性操作（EFM）问题，建立EFM-10基准测试，并提出双手机器人主动感知（BAP）策略来解决视觉遮挡问题


<details>
  <summary>Details</summary>
Motivation: 机器人头部摄像头在操作任务中经常遇到视觉遮挡问题，这本质上是缺乏完成任务所需的信息。作者从这一观察出发，提出了更根本的探索性与聚焦性操作问题

Method: 1. 建立EFM-10基准测试，包含4类共10个任务；2. 提出Bimanual Active Perception（BAP）策略，利用一只手臂提供主动视觉，另一只手臂提供力感知；3. 收集BAPData数据集用于模仿学习验证

Result: 成功验证了BAP策略在模仿学习方式下的有效性，EFM-10基准测试和BAP策略为未来研究提供了基础

Conclusion: 该工作为解决机器人操作中的视觉遮挡问题提供了新思路，提出的EFM问题和BAP策略有望成为该方向未来研究的基石

Abstract: Recently, active vision has reemerged as an important concept for manipulation, since visual occlusion occurs more frequently when main cameras are mounted on the robot heads. We reflect on the visual occlusion issue and identify its essence as the absence of information useful for task completion. Inspired by this, we come up with the more fundamental problem of Exploratory and Focused Manipulation (EFM). The proposed problem is about actively collecting information to complete challenging manipulation tasks that require exploration or focus. As an initial attempt to address this problem, we establish the EFM-10 benchmark that consists of 4 categories of tasks that align with our definition (10 tasks in total). We further come up with a Bimanual Active Perception (BAP) strategy, which leverages one arm to provide active vision and another arm to provide force sensing while manipulating. Based on this idea, we collect a dataset named BAPData for the tasks in EFM-10. With the dataset, we successfully verify the effectiveness of the BAP strategy in an imitation learning manner. We hope that the EFM-10 benchmark along with the BAP strategy can become a cornerstone that facilitates future research towards this direction. Project website: EFManipulation.github.io.

</details>


### [765] [DFKI-Speech System for WildSpoof Challenge: A robust framework for SASV In-the-Wild](https://arxiv.org/abs/2602.02286)
*Arnab Das,Yassine El Kheir,Enes Erdem Erdogan,Feidi Kallel,Tim Polzehl,Sebastian Moeller*

Main category: cs.SD

Relevance: 25.0

TL;DR: 该论文提出了一个用于WildSpoof挑战赛的SASV系统，结合了反欺骗检测和说话人验证网络，采用自监督语音嵌入提取器、图神经网络、MoE融合、多尺度CNN特征融合以及对比损失优化等技术。


<details>
  <summary>Details</summary>
Motivation: 解决语音反欺骗和说话人验证的联合任务，提高在WildSpoof挑战赛中的性能，通过融合多种先进技术来增强系统的鲁棒性和准确性。

Method: 1. 反欺骗检测：使用自监督语音嵌入提取器前端 + 图神经网络后端，采用top-3层MoE融合高低层特征
2. 说话人验证：低复杂度CNN融合2D和1D多尺度特征，SphereFace损失训练，对比circle损失优化难易样本区分
3. 后处理：固定冒名者队列AS Norm分数归一化和模型集成

Result: 开发了DFKI-Speech系统用于WildSpoof挑战赛SASV赛道，通过多技术融合提升了反欺骗检测和说话人验证的性能。

Conclusion: 提出的SASV框架通过协同工作的反欺骗检测和说话人验证网络，结合多种先进技术，有效提升了语音反欺骗和说话人验证系统的性能。

Abstract: This paper presents the DFKI-Speech system developed for the WildSpoof Challenge under the Spoofing aware Automatic Speaker Verification (SASV) track. We propose a robust SASV framework in which a spoofing detector and a speaker verification (SV) network operate in tandem. The spoofing detector employs a self-supervised speech embedding extractor as the frontend, combined with a state-of-the-art graph neural network backend. In addition, a top-3 layer based mixture-of-experts (MoE) is used to fuse high-level and low-level features for effective spoofed utterance detection. For speaker verification, we adapt a low-complexity convolutional neural network that fuses 2D and 1D features at multiple scales, trained with the SphereFace loss. Additionally, contrastive circle loss is applied to adaptively weight positive and negative pairs within each training batch, enabling the network to better distinguish between hard and easy sample pairs. Finally, fixed imposter cohort based AS Norm score normalization and model ensembling are used to further enhance the discriminative capability of the speaker verification system.

</details>


### [766] [TTT-Parkour: Rapid Test-Time Training for Perceptive Robot Parkour](https://arxiv.org/abs/2602.02331)
*Shaoting Zhu,Baijun Ye,Jiaxuan Wang,Jiakang Chen,Ziwen Zhuang,Linzhan Mou,Runhan Huang,Hang Zhao*

Main category: cs.RO

Relevance: 25.0

TL;DR: 提出了一种真实-仿真-真实框架，通过测试时训练增强人形机器人在未知复杂地形上的动态跑酷能力


<details>
  <summary>Details</summary>
Motivation: 现有通用运动策略在任意和高度挑战性环境中表现不佳，需要提升人形机器人在未见复杂地形上的动态跑酷能力

Method: 采用两阶段端到端学习：先在程序生成的地形上预训练策略，然后在真实世界重建的高保真网格上进行快速微调；开发了基于RGB-D输入的高效高保真几何重建流程

Result: TTT-Parkour使机器人能够掌握复杂障碍物（楔形、桩、箱体、梯形、窄梁等），整个捕获-重建-测试时训练流程在大多数地形上少于10分钟，测试时训练后的策略展现出鲁棒的零样本仿真到真实迁移能力

Conclusion: 提出的真实-仿真-真实框架通过快速测试时训练显著提升了人形机器人在极端困难几何地形上的穿越能力

Abstract: Achieving highly dynamic humanoid parkour on unseen, complex terrains remains a challenge in robotics. Although general locomotion policies demonstrate capabilities across broad terrain distributions, they often struggle with arbitrary and highly challenging environments. To overcome this limitation, we propose a real-to-sim-to-real framework that leverages rapid test-time training (TTT) on novel terrains, significantly enhancing the robot's capability to traverse extremely difficult geometries. We adopt a two-stage end-to-end learning paradigm: a policy is first pre-trained on diverse procedurally generated terrains, followed by rapid fine-tuning on high-fidelity meshes reconstructed from real-world captures. Specifically, we develop a feed-forward, efficient, and high-fidelity geometry reconstruction pipeline using RGB-D inputs, ensuring both speed and quality during test-time training. We demonstrate that TTT-Parkour empowers humanoid robots to master complex obstacles, including wedges, stakes, boxes, trapezoids, and narrow beams. The whole pipeline of capturing, reconstructing, and test-time training requires less than 10 minutes on most tested terrains. Extensive experiments show that the policy after test-time training exhibits robust zero-shot sim-to-real transfer capability.

</details>


### [767] [Predictive Maintenance for Ultrafiltration Membranes Using Explainable Similarity-Based Prognostics](https://arxiv.org/abs/2602.00659)
*Qusai Khaled,Laura Genga,Uzay Kaymak*

Main category: cs.AI

Relevance: 15.0

TL;DR: 提出基于模糊相似推理的可解释预测框架，用于超滤膜剩余使用寿命估计，结合物理知识健康指标和透明规则系统


<details>
  <summary>Details</summary>
Motivation: 反渗透海水淡化中超滤膜因污染而性能下降，现有预测维护模型缺乏可解释性和操作员信任，需要透明可靠的预测方法

Method: 使用基于跨膜压力、通量和阻力的物理知识健康指标，通过高斯隶属函数模糊化，采用相似度度量识别历史退化轨迹，构建Takagi-Sugeno模糊规则进行RUL预测

Result: 在工业规模UF系统的12,528个操作周期上测试，平均绝对误差为4.50个周期，同时生成与专家理解一致的可解释规则库

Conclusion: 该框架为膜退化预测提供了可解释且准确的解决方案，增强了操作员信任，支持基于状态的维护决策

Abstract: In reverse osmosis desalination, ultrafiltration (UF) membranes degrade due to fouling, leading to performance loss and costly downtime. Most plants rely on scheduled preventive maintenance, since existing predictive maintenance models, often based on opaque machine learning methods, lack interpretability and operator trust. This study proposes an explainable prognostic framework for UF membrane remaining useful life (RUL) estimation using fuzzy similarity reasoning. A physics-informed Health Index, derived from transmembrane pressure, flux, and resistance, captures degradation dynamics, which are then fuzzified via Gaussian membership functions. Using a similarity measure, the model identifies historical degradation trajectories resembling the current state and formulates RUL predictions as Takagi-Sugeno fuzzy rules. Each rule corresponds to a historical exemplar and contributes to a transparent, similarity-weighted RUL estimate. Tested on 12,528 operational cycles from an industrial-scale UF system, the framework achieved a mean absolute error of 4.50 cycles, while generating interpretable rule bases consistent with expert understanding.

</details>


### [768] [Physics-informed Diffusion Generation for Geomagnetic Map Interpolation](https://arxiv.org/abs/2602.00709)
*Wenda Li,Tongya Zheng,Kaixuan Chen,Shunyu Liu,Haoze Jiang,Yunzhi Hao,Rui Miao,Zujie Ren,Mingli Song,Hang Shi,Gang Chen*

Main category: cs.AI

Relevance: 15.0

TL;DR: 提出PDG框架，通过物理信息掩码策略和克里金原理约束，利用扩散生成模型进行地磁地图插值，有效消除噪声干扰并保证物理规律一致性。


<details>
  <summary>Details</summary>
Motivation: 现有散点数据插值方法未针对地磁地图专门设计，无法有效处理检测噪声和物理规律约束，导致插值性能不理想。地磁地图插值在导航和资源勘探中具有重要应用价值。

Method: 1) 设计基于局部感受野的物理信息掩码策略，引导扩散生成过程消除噪声干扰；2) 基于克里金原理对扩散生成结果施加物理信息约束，确保符合地磁物理规律。

Result: 在四个真实世界数据集上的大量实验和深入分析表明，PDG框架及其各组件具有优越性和有效性。

Conclusion: PDG框架通过物理信息引导的扩散生成方法，能够有效处理地磁地图插值问题，在消除噪声干扰的同时保证物理规律一致性。

Abstract: Geomagnetic map interpolation aims to infer unobserved geomagnetic data at spatial points, yielding critical applications in navigation and resource exploration. However, existing methods for scattered data interpolation are not specifically designed for geomagnetic maps, which inevitably leads to suboptimal performance due to detection noise and the laws of physics. Therefore, we propose a Physics-informed Diffusion Generation framework~(PDG) to interpolate incomplete geomagnetic maps. First, we design a physics-informed mask strategy to guide the diffusion generation process based on a local receptive field, effectively eliminating noise interference. Second, we impose a physics-informed constraint on the diffusion generation results following the kriging principle of geomagnetic maps, ensuring strict adherence to the laws of physics. Extensive experiments and in-depth analyses on four real-world datasets demonstrate the superiority and effectiveness of each component of PDG.

</details>


### [769] [Legal Infrastructure for Transformative AI Governance](https://arxiv.org/abs/2602.01474)
*Gillian K. Hadfield*

Main category: cs.AI

Relevance: 15.0

TL;DR: 本文探讨AI治理的法律与监管基础设施建设，而非具体规则内容，提出三种监管框架：前沿模型注册制、自主代理注册识别制、监管市场设计


<details>
  <summary>Details</summary>
Motivation: 当前AI治理讨论过于关注具体规则内容，忽视了建立法律和监管基础设施的重要性。AI的变革性特征特别需要关注法律和监管框架的建设，以确保规则的有效生成和实施

Method: 作者回顾并提出了三种具体的法律监管基础设施方案：1) 建立前沿AI模型的注册制度；2) 建立自主代理的注册和识别制度；3) 设计监管市场，让私营公司能够创新并提供AI监管服务

Result: 提出了三种具体的AI治理基础设施框架，强调法律不仅要制定实质性规则，更要建立能够生成和实施规则的法律监管基础设施

Conclusion: AI治理需要从单纯关注规则内容转向建设法律和监管基础设施，通过注册制度、识别机制和监管市场等创新框架来应对AI的变革性挑战

Abstract: Most of our AI governance efforts focus on substance: what rules do we want in place? What limits or checks do we want to impose on AI development and deployment? But a key role for law is not only to establish substantive rules but also to establish legal and regulatory infrastructure to generate and implement rules. The transformative nature of AI calls especially for attention to building legal and regulatory frameworks. In this PNAS Perspective piece I review three examples I have proposed: the creation of registration regimes for frontier models; the creation of registration and identification regimes for autonomous agents; and the design of regulatory markets to facilitate a role for private companies to innovate and deliver AI regulatory services.

</details>


### [770] [Traffic-Aware Navigation in Road Networks](https://arxiv.org/abs/2602.02158)
*Sarah Nassar*

Main category: cs.AI

Relevance: 15.0

TL;DR: 该论文比较了三种图搜索算法在金斯顿道路网络交通感知导航中的性能：Floyd-Warshall-Ingerman（单次多查询预处理）、Dijkstra和A*（连续单查询实时搜索）、以及Yen算法（结合两者，先找K条最短路径再实时迭代）。


<details>
  <summary>Details</summary>
Motivation: 研究动机是解决交通感知导航问题，需要在道路网络中寻找考虑交通状况的最优路径。不同算法在预处理需求、实时计算速度和解决方案最优性之间存在权衡，需要根据具体部署场景选择合适的方法。

Method: 方法包括：1）Floyd-Warshall-Ingerman算法进行单次多查询预处理；2）Dijkstra和A*算法进行连续单查询实时搜索；3）Yen算法结合两种方法，先找到前K条最短路径，然后在实时中迭代这些路径。在金斯顿道路网络上进行实验比较。

Result: 结果显示：Dijkstra和A*算法产生了最交通感知的最优解，且需要最少的预处理；Floyd-Warshall-Ingerman算法实时速度最快，但只提供基于距离的路径，没有交通感知；Yen算法需要大量预处理，但在运行速度和最优性方面平衡了前两种方法。

Conclusion: 结论是每种方法都有其优缺点，需要根据具体部署环境权衡选择最佳定制解决方案。Dijkstra和A*适合需要交通感知且预处理资源有限的场景；Floyd-Warshall-Ingerman适合对实时速度要求极高但不需要交通感知的场景；Yen算法适合需要在速度和最优性之间平衡的场景。

Abstract: This project compares three graph search approaches for the task of traffic-aware navigation in Kingston's road network. These approaches include a single-run multi-query preprocessing algorithm (Floyd-Warshall-Ingerman), continuous single-query real-time search (Dijkstra's and A*), and an algorithm combining both approaches to balance between their trade-offs by first finding the top K shortest paths then iterating over them in real time (Yen's). Dijkstra's and A* resulted in the most traffic-aware optimal solutions with minimal preprocessing required. Floyd-Warshall-Ingerman was the fastest in real time but provided distance based paths with no traffic awareness. Yen's algorithm required significant preprocessing but balanced between the other two approaches in terms of runtime speed and optimality. Each approach presents advantages and disadvantages that need to be weighed depending on the circumstances of specific deployment contexts to select the best custom solution. *This project was completed as part of ELEC 844 (Search and Planning Algorithms for Robotics) in the Fall 2025 term.

</details>


### [771] [Bitcoin Price Prediction using Machine Learning and Combinatorial Fusion Analysis](https://arxiv.org/abs/2602.00037)
*Yuanhong Wu,Wei Ye,Jingyan Xu,D. Frank Hsu*

Main category: q-fin.ST

Relevance: 15.0

TL;DR: 该论文提出将组合融合分析(CFA)应用于比特币价格预测，通过结合多个模型的分数和排名特征来提升预测性能，取得了0.19%的MAPE优异表现。


<details>
  <summary>Details</summary>
Motivation: 金融产品价格预测是金融领域的重要课题，但单个机器学习模型各有优缺点，难以实现鲁棒性。组合融合分析(CFA)通过利用多个模型的排名-分数特征和认知多样性，可以增强模型的预测能力。

Method: 采用组合融合分析(CFA)框架，利用排名-分数特征(RSC)函数和认知多样性，结合一组中等规模、多样化且性能相对较好的模型。方法包括分数组合、排名组合以及其他加权组合技术。

Result: 该方法在比特币价格预测中取得了0.19%的MAPE优异表现，显著优于单个模型的性能，也超越了其他比特币价格预测模型。

Conclusion: 组合融合分析(CFA)能有效提升比特币价格预测的准确性和鲁棒性，通过结合多个模型的优势来弥补单个模型的不足。

Abstract: In this work, we propose to apply a new model fusion and learning paradigm, known as Combinatorial Fusion Analysis (CFA), to the field of Bitcoin price prediction. Price prediction of financial product has always been a big topic in finance, as the successful prediction of the price can yield significant profit. Every machine learning model has its own strength and weakness, which hinders progress toward robustness. CFA has been used to enhance models by leveraging rank-score characteristic (RSC) function and cognitive diversity in the combination of a moderate set of diverse and relatively well-performed models. Our method utilizes both score and rank combinations as well as other weighted combination techniques. Key metrics such as RMSE and MAPE are used to evaluate our methodology performance. Our proposal presents a notable MAPE performance of 0.19\%. The proposed method greatly improves upon individual model performance, as well as outperforms other Bitcoin price prediction models.

</details>


### [772] [A longitudinal geospatial multimodal dataset of post-discharge frailty, physiology, mobility, and neighborhoods](https://arxiv.org/abs/2602.00060)
*Ali Abedi,Charlene H. Chu,Shehroz S. Khan*

Main category: cs.CY

Relevance: 15.0

TL;DR: GEOFRAIL数据集：针对出院后虚弱老年人的纵向地理空间多模态数据集，包含人口统计、多模态传感器特征、临床评估和社区环境数据，用于监测恢复轨迹。


<details>
  <summary>Details</summary>
Motivation: 老年人虚弱状态与功能衰退、行动能力下降、社交孤立和出院后恢复困难相关，可能导致再住院。社区环境通过影响行动机会、社交参与和资源获取进一步影响恢复轨迹。多模态传感技术结合数据驱动方法可实时监测这些多维因素。

Method: 创建GEOFRAIL数据集，包含：1)参与者人口统计信息；2)多模态传感器衍生的特征；3)每两周一次的临床评估（虚弱、身体功能、社交孤立）；4)时间位置记录，链接到社区设施、犯罪率和社会经济指标。数据通过标准化隐私保护空间聚合管道收集，持续8周出院后监测。

Result: 数据集展示了地理空间、传感器衍生和临床测量之间的内部一致性。报告了机器学习模型在表征恢复轨迹方面的基线性能，验证了多模态数据用于监测老年人恢复的可行性。

Conclusion: GEOFRAIL数据集为研究社区环境如何影响虚弱老年人出院后恢复提供了综合多模态资源，支持开发数据驱动的干预措施以改善恢复结果。

Abstract: Frailty in older adults is associated with increased vulnerability to functional decline, reduced mobility, social isolation, and challenges during the transition from hospital to community living. These factors are associated with rehospitalization and may adversely influence recovery. Neighborhood environments can further shape recovery trajectories by affecting mobility opportunities, social engagement, and access to community resources. Multimodal sensing technologies combined with data-driven analytical approaches offer the potential to continuously monitor these multidimensional factors in real-world settings. This Data Descriptor presents GEOFRAIL, a longitudinal geospatial multimodal dataset collected from community-dwelling frail older adults following hospital discharge. The dataset is organized into interconnected tables capturing participant demographics, features derived from multimodal sensors, biweekly clinical assessments of frailty, physical function, and social isolation, and temporal location records linked to neighborhood amenities, crime rates, and census-based socioeconomic indicators. Data were collected over an eight-week post-discharge period using standardized pipelines with privacy-preserving spatial aggregation. Technical validation demonstrates internal consistency across geospatial, sensor-derived, and clinical measures and reports baseline performance of machine learning models for characterizing recovery trajectories.

</details>


### [773] [Generative Artificial Intelligence in Small and Medium Enterprises: Navigating its Promises and Challenges](https://arxiv.org/abs/2602.00091)
*Kumaran Rajaram,Patrick Nicolas Tinguely*

Main category: cs.CY

Relevance: 15.0

TL;DR: 论文探讨中小企业如何利用生成式AI提升竞争力，提出了部署GAI的战略框架和实用建议


<details>
  <summary>Details</summary>
Motivation: 生成式AI技术为中小企业提供了强大的能力，即使技术专长或财务资源有限，也能通过GAI简化工作流程、激发创新，从而改善产品供应和长期竞争力。论文旨在帮助中小企业应对GAI的机遇与挑战。

Method: 采用航海隐喻揭示GAI部署的关键战略维度：员工能力、有效领导和工作价值观、组织文化、协作与合作、与第三方关系。提供实用的路线图和建议。

Result: 提出了一个全面的GAI部署框架，为中小企业提供了具体的战略维度和实用建议，帮助它们成功实施生成式AI技术。

Conclusion: 中小企业可以通过系统性的战略框架成功部署GAI，利用五个关键维度（员工能力、领导力、组织文化、协作、第三方关系）作为导航工具，实现技术民主化和竞争力提升。

Abstract: The latest technological developments in generative artificial intelligence (GAI) offer powerful capabilities to small and medium enterprises (SMEs), as they facilitate the democratization of both scalability and creativity. Even if they have little technical expertise or financial resources, SMEs can leverage this technology to streamline work processes and unleash innovation, thereby improving their product offerings and long-term competitiveness. This paper discusses how SMEs can navigate both the promises and challenges of GAI and offers a roadmap for deploying GAI. We introduce a sailing metaphor that reveals key strategic dimensions for GAI deployment: competency of employees, effective leadership and work values, organizational culture, collaboration and cooperation, and relationships with third parties. We offer practical recommendations that serve as a useful compass for successfully deploying GAI in SMEs.

</details>


### [774] [Visible Singularities Guided Correlation Network for Limited-Angle CT Reconstruction](https://arxiv.org/abs/2602.00184)
*Yiyang Wen,Liu Shi,Zekun Zhou,WenZhe Shan,Qiegen Liu*

Main category: eess.IV

Relevance: 15.0

TL;DR: 提出VSGC网络用于有限角度CT重建，通过可见奇异点引导和相关建模解决方向性伪影问题，在小角度范围性能提升显著


<details>
  <summary>Details</summary>
Motivation: 有限角度CT（LACT）能减少辐射剂量和扫描时间，但传统重建算法存在固有局限。现有深度学习方法未能充分结合LACT的核心成像特性，如方向性伪影和结构信息的方向性缺失，这些特性源于特定方向投影角度的缺失。

Method: 基于可见奇异点理论，提出VSGC网络：1）从LACT图像提取可见奇异点边缘特征，引导模型注意力；2）建立可见奇异点边缘特征与图像其他区域的相关性；3）采用具有各向异性约束的多尺度损失函数进行多方位约束。

Result: 在模拟和真实数据集上进行验证，相比其他方法，VSGC在小角度范围表现更突出，PSNR提升2.45 dB，SSIM提升1.5%。

Conclusion: VSGC网络通过可见奇异点引导和相关建模，有效解决了LACT重建中的方向性伪影问题，在小角度范围重建质量显著提升。

Abstract: Limited-angle computed tomography (LACT) offers the advantages of reduced radiation dose and shortened scanning time. Traditional reconstruction algorithms exhibit various inherent limitations in LACT. Currently, most deep learning-based LACT reconstruction methods focus on multi-domain fusion or the introduction of generic priors, failing to fully align with the core imaging characteristics of LACT-such as the directionality of artifacts and directional loss of structural information, which are caused by the absence of projection angles in certain directions. Inspired by the theory of visible and invisible singularities, taking into account the aforementioned core imaging characteristics of LACT, we propose a Visible Singularities Guided Correlation network for LACT reconstruction (VSGC). The design philosophy of VSGC consists of two core steps: First, extract VS edge features from LACT images and focus the model's attention on these VS. Second, establish correlations between the VS edge features and other regions of the image. Additionally, a multi-scale loss function with anisotropic constraint is employed to constrain the model to converge in multiple aspects. Finally, qualitative and quantitative validations are conducted on both simulated and real datasets to verify the effectiveness and feasibility of the proposed design. Particularly, in comparison with alternative methods, VSGC delivers more prominent performance in small angular ranges, with the PSNR improvement of 2.45 dB and the SSIM enhancement of 1.5\%. The code is publicly available at https://github.com/yqx7150/VSGC.

</details>


### [775] [On the calibration of survival models with competing risks](https://arxiv.org/abs/2602.00194)
*Julie Alberge,Tristan Haugomat,Gaël Varoquaux,Judith Abécassis*

Main category: stat.ME

Relevance: 15.0

TL;DR: 论文提出了针对竞争风险生存分析的新校准框架，包含两个专有的校准度量，以及估计、测试和校正校准的方法。


<details>
  <summary>Details</summary>
Motivation: 竞争风险生存分析中，多个事件可能发生，需要准确的概率估计。现有校准方法不适用于竞争风险设置，且当前模型无法提供良好行为的概率。

Method: 引入专用框架，包含两个新的校准度量（对oracle估计器最小化），以及估计、测试和校正校准的方法。

Result: 提出的重新校准方法在保持区分度的同时，能够产生良好的概率估计。

Conclusion: 为竞争风险生存分析提供了有效的校准框架，解决了现有方法的不足。

Abstract: Survival analysis deals with modeling the time until an event occurs, and accurate probability estimates are crucial for decision-making, particularly in the competing-risks setting where multiple events are possible. While recent work has addressed calibration in standard survival analysis, the competing-risks setting remains under-explored as it is harder (the calibration applies to both probabilities across classes and time horizon). We show that existing calibration measures are not suited to the competing-risk setting and that recent models do not give well-behaved probabilities. To address this, we introduce a dedicated framework with two novel calibration measures that are minimized for oracle estimators (i.e., both measures are proper). We also introduce some methods to estimate, test, and correct the calibration. Our recalibration methods yield good probabilities while preserving discrimination.

</details>


### [776] [Inter- and Intra-Subject Variability in EEG: A Systematic Survey](https://arxiv.org/abs/2602.01019)
*Xuan-The Tran,Thien-Nhan Vo,Son-Tung Vu,Thoa-Thi Tran,Manh-Dat Nguyen,Thomas Do,Chin-Teng Lin*

Main category: q-bio.NC

Relevance: 15.0

TL;DR: 该论文系统综述了EEG信号的变异性问题，分析了不同范式下的变异性特征、量化方法，并提出了研究设计和报告建议。


<details>
  <summary>Details</summary>
Motivation: EEG信号在神经科学、临床神经生理学和脑机接口中至关重要，但显著的受试者间和受试者内变异性限制了其可靠性、可重复性和实际应用。本文旨在系统研究EEG变异性问题，为管理这一约束并利用其作为有意义信号提供指导。

Method: 采用系统综述方法，分析量化或建模EEG变异性的研究，涵盖静息态、事件相关电位和任务相关/BCI范式，包括健康人群和临床队列。总结了变异性来源（生物、状态、技术、分析），并回顾了常见的量化建模方法。

Result: 发现受试者间差异通常大于受试者内波动，两者都影响统计推断和模型泛化。稳定性具有特征依赖性：α波段测量和个体α峰值频率通常相对可靠，而高频和连接性指标可靠性更差；ERP可靠性因成分而异，P300测量常显示中等至良好稳定性。

Conclusion: EEG变异性应被视为需要管理的实际约束，同时也是可用于精准神经科学和稳健神经技术的有意义信号。提供了研究设计、报告和标准化建议。

Abstract: Electroencephalography (EEG) underpins neuroscience, clinical neurophysiology, and brain-computer interfaces (BCIs), yet pronounced inter- and intra-subject variability limits reliability, reproducibility, and translation. This systematic review studies that quantified or modeled EEG variability across resting-state, event-related potentials (ERPs), and task-related/BCI paradigms (including motor imagery and SSVEP) in healthy and clinical cohorts. Across paradigms, inter-subject differences are typically larger than within-subject fluctuations, but both affect inference and model generalization. Stability is feature-dependent: alpha-band measures and individual alpha peak frequency are often relatively reliable, whereas higher-frequency and many connectivity-derived metrics show more heterogeneous reliability; ERP reliability varies by component, with P300 measures frequently showing moderate-to-good stability. We summarize major sources of variability (biological, state-related, technical, and analytical), review common quantification and modeling approaches (e.g., ICC, CV, SNR, generalizability theory, and multivariate/learning-based methods), and provide recommendations for study design, reporting, and harmonization. Overall, EEG variability should be treated as both a practical constraint to manage and a meaningful signal to leverage for precision neuroscience and robust neurotechnology.

</details>


### [777] [Harnessing Flexible Spatial and Temporal Data Center Workloads for Grid Regulation Services](https://arxiv.org/abs/2602.01508)
*Yingrui Fan,Junbo Zhao*

Main category: eess.SY

Relevance: 15.0

TL;DR: 数据中心作为灵活负载参与电网频率调节的联合优化框架，将工作负载调度与调节容量投标统一考虑，通过时空网络模型和机会约束确保调节承诺的可行性。


<details>
  <summary>Details</summary>
Motivation: 现有方法将数据中心工作负载调度和电网频率调节容量投标分开处理，忽略了排队动态和时空调度决策对实时调节能力的影响，导致承诺的调节容量可能不可行或持续时间短。

Method: 提出统一的日前联合优化框架，共同决策地理分布式数据中心的工作负载分配和调节容量承诺。构建时空网络模型捕捉工作负载迁移成本、延迟要求和异构资源限制。引入基于交互负载预测的瞬时功率灵活性机会约束，以及基于风险价值的队列状态约束来维持累积调节信号下的可持续响应。

Result: 在改进的IEEE 68总线系统上使用真实数据中心轨迹的案例研究表明，该框架降低了系统运营成本，实现了更可行的调节容量，并在收入-风险权衡方面优于独立优化调度和调节的策略。

Conclusion: 提出的联合优化框架能够有效解决数据中心作为灵活负载参与电网频率调节时的可行性问题，通过统一考虑调度和调节决策，提高了系统经济性和调节服务的可持续性。

Abstract: Data centers (DCs) are increasingly recognized as flexible loads that can support grid frequency regulation. Yet, most existing methods treat workload scheduling and regulation capacity bidding separately, overlooking how queueing dynamics and spatial-temporal dispatch decisions affect the ability to sustain real-time regulation. As a result, the committed regulation may become infeasible or short-lived. To address this issue, we propose a unified day-ahead co-optimization framework that jointly decides workload distribution across geographically distributed DCs and regulation capacity commitments. We construct a space-time network model to capture workload migration costs, latency requirements, and heterogeneous resource limits. To ensure that the committed regulation remains deliverable, we introduce chance constraints on instantaneous power flexibility based on interactive load forecasts, and apply Value-at-Risk queue-state constraints to maintain sustainable response under cumulative regulation signals. Case studies on a modified IEEE 68-bus system using real data center traces show that the proposed framework lowers system operating costs, enables more viable regulation capacity, and achieves better revenue-risk trade-offs compared to strategies that optimize scheduling and regulation independently.

</details>


### [778] [Reliable Real-Time Value at Risk Estimation via Quantile Regression Forest with Conformal Calibration](https://arxiv.org/abs/2602.01912)
*Du-Yi Wang,Guo Liang,Kun Zhang,Qianwen Zhu*

Main category: stat.ML

Relevance: 15.0

TL;DR: 该论文提出了一种基于离线-模拟-在线估计框架的分位数回归森林方法，用于实时风险价值估计，并引入保形校准确保可靠性。


<details>
  <summary>Details</summary>
Motivation: 市场条件快速变化需要实时风险监控，但风险价值在线估计仍然具有挑战性。准确可靠的风险价值估计对于及时风险控制和明智决策至关重要。

Method: 采用离线-模拟-在线估计框架，使用分位数回归森林离线学习在线风险价值与风险因素之间的关系，然后在线结合观测到的风险因素生成实时风险价值估计。进一步开发保形化估计器来校准在线风险价值估计。

Result: 理论分析建立了所提估计器的一致性和覆盖有效性。数值实验证实了该方法并展示了其在实践中的有效性。

Conclusion: 该研究首次利用保形校准基于OSOA框架可靠地估计实时风险价值，为实时风险监控提供了有效解决方案。

Abstract: Rapidly evolving market conditions call for real-time risk monitoring, but its online estimation remains challenging. In this paper, we study the online estimation of one of the most widely used risk measures, Value at Risk (VaR). Its accurate and reliable estimation is essential for timely risk control and informed decision-making. We propose to use the quantile regression forest in the offline-simulation-online-estimation (OSOA) framework. Specifically, the quantile regression forest is trained offline to learn the relationship between the online VaR and risk factors, and real-time VaR estimates are then produced online by incorporating observed risk factors. To further ensure reliability, we develop a conformalized estimator that calibrates the online VaR estimates. To the best of our knowledge, we are the first to leverage conformal calibration to estimate real-time VaR reliably based on the OSOA formulation. Theoretical analysis establishes the consistency and coverage validity of the proposed estimators. Numerical experiments confirm the proposed method and demonstrate its effectiveness in practice.

</details>


### [779] [Malware Detection Through Memory Analysis](https://arxiv.org/abs/2602.02184)
*Sarah Nassar*

Main category: cs.CR

Relevance: 15.0

TL;DR: 该研究使用XGBoost模型在MalMemAnalysis-2022数据集上进行恶意软件检测，实现了99.98%的二进制分类准确率和87.54%的多分类准确率，同时保持了快速的推理速度。


<details>
  <summary>Details</summary>
Motivation: 探索机器学习技术在恶意软件检测中的有效性和效率，旨在开发准确且实时的混淆恶意软件检测器，以改善在线隐私和安全。

Method: 使用加拿大网络安全研究所的MalMemAnalysis-2022数据集，采用XGBoost模型进行二进制分类（良性或恶意）和多分类（良性、勒索软件、间谍软件、木马）。选择XGBoost是基于其在检测能力和推理速度之间的良好平衡。

Result: 二进制分类器在测试集上达到99.98%的准确率和F1分数；多分类版本达到87.54%的准确率和81.26%的F1分数，恶意软件子类型的平均F1分数为75.03%。推理速度方面，二进制分类50个样本约需37.3毫秒，多分类约需43.2毫秒。

Conclusion: XGBoost在恶意软件检测任务中表现出色，既能实现高精度检测，又能保持快速推理速度，有助于开发实时恶意软件检测系统。

Abstract: This paper summarizes the research conducted for a malware detection project using the Canadian Institute for Cybersecurity's MalMemAnalysis-2022 dataset. The purpose of the project was to explore the effectiveness and efficiency of machine learning techniques for the task of binary classification (i.e., benign or malicious) as well as multi-class classification to further include three malware sub-types (i.e., benign, ransomware, spyware, or Trojan horse). The XGBoost model type was the final model selected for both tasks due to the trade-off between strong detection capability and fast inference speed. The binary classifier achieved a testing subset accuracy and F1 score of 99.98\%, while the multi-class version reached an accuracy of 87.54\% and an F1 score of 81.26\%, with an average F1 score over the malware sub-types of 75.03\%. In addition to the high modelling performance, XGBoost is also efficient in terms of classification speed. It takes about 37.3 milliseconds to classify 50 samples in sequential order in the binary setting and about 43.2 milliseconds in the multi-class setting. The results from this research project help advance the efforts made towards developing accurate and real-time obfuscated malware detectors for the goal of improving online privacy and safety. *This project was completed as part of ELEC 877 (AI for Cybersecurity) in the Winter 2024 term.

</details>


### [780] [Bridging the Sim-to-Real Gap with multipanda ros2: A Real-Time ROS2 Framework for Multimanual Systems](https://arxiv.org/abs/2602.02269)
*Jon Škerlj,Seongjin Bien,Abdeldjallil Naceri,Sami Haddadin*

Main category: cs.RO

Relevance: 15.0

TL;DR: 开源ROS2多机器人控制框架，支持Franka机器人实时扭矩控制，实现1kHz控制频率和≤2ms控制器切换延迟，集成高保真MuJoCo仿真和惯性参数识别以缩小仿真-现实差距。


<details>
  <summary>Details</summary>
Motivation: 解决多机器人实时控制中的关键挑战，包括实时扭矩控制、机器人-环境建模、维持1kHz控制频率（安全标准要求），以及实现快速控制器切换以支持复杂多机器人交互场景。

Method: 开发ROS2架构，采用ros2_control实现单进程多机器人控制；引入controllet设计模式实现快速控制器切换；集成MuJoCo高保真仿真；通过真实世界惯性参数识别提升物理精度。

Result: 成功实现1kHz控制频率和≤2ms控制器切换延迟；通过惯性参数识别显著提升力和扭矩精度；提供可复现的基准测试平台；将软机器人方法扩展到刚性双臂接触丰富任务。

Conclusion: 该框架为先进机器人研究提供了稳健、可复现的平台，有效缩小了仿真-现实差距，支持复杂多机器人交互控制，具有广泛的应用前景。

Abstract: We present $multipanda\_ros2$, a novel open-source ROS2 architecture for multi-robot control of Franka Robotics robots. Leveraging ros2 control, this framework provides native ROS2 interfaces for controlling any number of robots from a single process. Our core contributions address key challenges in real-time torque control, including interaction control and robot-environment modeling. A central focus of this work is sustaining a 1kHz control frequency, a necessity for real-time control and a minimum frequency required by safety standards. Moreover, we introduce a controllet-feature design pattern that enables controller-switching delays of $\le 2$ ms, facilitating reproducible benchmarking and complex multi-robot interaction scenarios. To bridge the simulation-to-reality (sim2real) gap, we integrate a high-fidelity MuJoCo simulation with quantitative metrics for both kinematic accuracy and dynamic consistency (torques, forces, and control errors). Furthermore, we demonstrate that real-world inertial parameter identification can significantly improve force and torque accuracy, providing a methodology for iterative physics refinement. Our work extends approaches from soft robotics to rigid dual-arm, contact-rich tasks, showcasing a promising method to reduce the sim2real gap and providing a robust, reproducible platform for advanced robotics research.

</details>


<div id='cs.LG'></div>

# cs.LG [[Back]](#toc)

### [781] [Tangent Space Fine-Tuning for Directional Preference Alignment in Large Language Models](https://arxiv.org/abs/2602.01128)
*Mete Erdogan*

Main category: cs.LG

Relevance: 95.0

TL;DR: TS-DPO：在切线空间中执行DPO，学习每个目标的更新方向，可在推理时线性组合以实现用户指定的行为平衡，无需额外优化


<details>
  <summary>Details</summary>
Motivation: 现有偏好优化方法（如DPO）将反馈压缩为单一标量奖励，固定了目标间的平衡，无法遍历帕累托前沿。需要让LLM能够平衡多个人类偏好维度（如帮助性、安全性、冗长度），实现可控对齐

Method: 基于切线空间微调理论，在局部线性区域执行DPO，学习每个目标的更新方向。这些方向可在推理时线性组合，生成用户指定的行为，无需额外优化

Result: 在HelpSteer和UltraFeedback数据集上评估帮助性-冗长度权衡，TS-DPO比标量化DPO实现更广泛的帕累托最优覆盖和更平滑的偏好控制。典型相关分析显示切线空间训练增强了与不同偏好对齐的典型方向，改善了可分离性

Conclusion: TS-DPO通过切线空间中的偏好优化，实现了对多个偏好维度的可控对齐，为LLM的多目标平衡提供了新方法

Abstract: Our goal is to enable large language models (LLMs) to balance multiple human preference dimensions; such as helpfulness, safety, and verbosity, through principled and controllable alignment. Existing preference optimization methods, including Direct Preference Optimization (DPO), collapse feedback into a single scalar reward, fixing one balance among objectives and preventing traversal of the Pareto front. Recent work by Ortiz-Jimenez et al. (2023) showed that fine-tuning can be viewed in a model's tangent space, where linearized updates act as additive vectors that can be composed to jointly perform well on multiple tasks. Building on this formulation, we extend this idea to preference alignment and propose Tangent-Space Direct Preference Optimization (TS-DPO), which performs DPO within this locally linear regime to learn per-objective update directions. These directions can be linearly combined at inference to generate user-specified behaviors without additional optimization. Evaluated on the helpfulness-verbosity trade-off using the HelpSteer and UltraFeedback datasets, TS-DPO achieves broader Pareto-optimal coverage and smoother preference control than scalarized DPO. Canonical Correlation Analysis (CCA) further shows that tangent-space training amplifies canonical directions aligned with distinct preferences, improving disentanglement.

</details>


### [782] [SLIME: Stabilized Likelihood Implicit Margin Enforcement for Preference Optimization](https://arxiv.org/abs/2602.02383)
*Maksim Afanasyev,Illarion Iov*

Main category: cs.LG

Relevance: 95.0

TL;DR: SLIME提出了一种新的无参考对齐目标，通过解耦偏好学习与生成质量，解决了现有直接偏好优化方法中的目标不匹配问题，防止高质量输出的"遗忘"和"格式化崩溃"。


<details>
  <summary>Details</summary>
Motivation: 现有直接偏好优化方法虽然计算高效，但存在关键的目标不匹配问题：优化选择与拒绝响应之间的相对边际不能保证保留选择响应的绝对可能性，导致"遗忘"（模型降低高质量输出的概率以满足边际约束）和"格式化崩溃"（过度惩罚拒绝序列）。

Method: SLIME采用三部分目标：(1)锚定项最大化偏好响应的似然；(2)稳定惩罚防止拒绝标记概率崩溃为零；(3)结合硬约束和软约束的双边际机制进行精确边界塑造。这是一种无参考的对齐目标。

Result: SLIME在性能上优于现有最先进基线方法，同时保持更高的生成稳定性。

Conclusion: SLIME通过解耦偏好学习与生成质量，有效解决了直接偏好优化中的目标不匹配问题，在保持计算效率的同时提高了对齐效果和稳定性。

Abstract: Direct preference optimization methods have emerged as a computationally efficient alternative to Reinforcement Learning from Human Feedback (RLHF) for aligning Large Language Models (LLMs). Latest approaches have streamlined the alignment process by deriving implicit reward functions, yet they often suffer from a critical objective mismatch: optimizing the relative margin between chosen and rejected responses does not guarantee the preservation of the chosen response's absolute likelihood. This can lead to ``unlearning'', where the model degrades the probability of high-quality outputs to satisfy margin constraints, and ``formatting collapse'' caused by the over-penalization of rejected sequences. In this work, we introduce SLIME (Stabilized Likelihood Implicit Margin Enforcement), a reference-free alignment objective designed to decouple preference learning from generation quality. SLIME incorporates a three-pronged objective: (1) an anchoring term to maximize the likelihood of preferred responses; (2) a stabilizing penalty that prevents the probabilities of rejected tokens from collapsing to zero; and (3) a dual-margin mechanism that combines hard and soft constraints for precise boundary shaping. Our results demonstrate that SLIME achieves superior performance compared to state-of-the-art baselines while maintaining higher generation stability.

</details>


### [783] [Enhancing few-shot time series forecasting with LLM-guided diffusion](https://arxiv.org/abs/2602.00040)
*Haonan Shi,Dehua Shuai,Liming Wang,Xiyang Liu,Long Tian*

Main category: cs.LG

Relevance: 85.0

TL;DR: LTSM-DIFF：结合大语言模型和扩散模型的时间序列预测框架，在数据稀缺场景下实现SOTA性能


<details>
  <summary>Details</summary>
Motivation: 专业领域时间序列预测常面临数据稀缺问题，传统模型需要大规模数据集才能有效捕捉时间动态。为了解决这种少样本挑战，需要开发能够在数据有限条件下有效工作的新方法。

Method: 提出LTSM-DIFF框架：1）LTSM模块（基于大语言模型微调）作为时间记忆机制提取序列表示；2）扩散模型作为生成模型，以LTSM表示作为条件指导进行联合概率扩散过程；3）实现从语言领域到时间序列任务的知识迁移。

Result: 在多个基准测试中，LTSM-DIFF在数据丰富场景下达到SOTA性能，在少样本预测中显著提升性能，增强了泛化能力和鲁棒性。

Conclusion: 该工作为数据稀缺下的时间序列分析建立了新范式，展示了结合大语言模型和扩散模型在时间序列预测中的潜力。

Abstract: Time series forecasting in specialized domains is often constrained by limited data availability, where conventional models typically require large-scale datasets to effectively capture underlying temporal dynamics. To tackle this few-shot challenge, we propose LTSM-DIFF (Large-scale Temporal Sequential Memory with Diffusion), a novel learning framework that integrates the expressive power of large language models with the generative capability of diffusion models. Specifically, the LTSM module is fine-tuned and employed as a temporal memory mechanism, extracting rich sequential representations even under data-scarce conditions. These representations are then utilized as conditional guidance for a joint probability diffusion process, enabling refined modeling of complex temporal patterns. This design allows knowledge transfer from the language domain to time series tasks, substantially enhancing both generalization and robustness. Extensive experiments across diverse benchmarks demonstrate that LTSM-DIFF consistently achieves state-of-the-art performance in data-rich scenarios, while also delivering significant improvements in few-shot forecasting. Our work establishes a new paradigm for time series analysis under data scarcity.

</details>


### [784] [Extending Beacon to Hindi: Cultural Adaptation Drives Cross-Lingual Sycophancy](https://arxiv.org/abs/2602.00046)
*Sarthak Sattigeri*

Main category: cs.LG

Relevance: 85.0

TL;DR: 该研究将英语中的"谄媚"（sycophancy）诊断扩展到印地语，发现文化适应比语言编码对LLM谄媚行为的影响更大，表明英语对齐评估可能无法跨语言通用。


<details>
  <summary>Details</summary>
Motivation: 当前LLM对齐研究主要集中在英语评估，但谄媚行为（模型优先迎合用户偏好而非原则性推理）是否在不同语言和文化背景下普遍存在尚不清楚。需要验证英语诊断方法能否跨语言推广。

Method: 将Beacon单轮强制选择谄媚诊断扩展到印地语，采用三条件设计：英语原版、印地语直译版、印地语文化适应版。在4个开源指令调优模型上评估每个条件50个提示，分离语言编码和文化适应效应。

Result: 所有模型中，文化适应印地语提示的谄媚率均高于英语，绝对差异12.0-16.0个百分点。在Qwen 2.5-Coder-7B上分解显示，文化适应贡献主要差异（14.0%），语言编码贡献很小（2.0%）。建议类提示跨语言差异最大（20-25个百分点）。

Conclusion: 英语对齐行为评估不能均匀跨语言转移，文化背景的提示框架起重要作用。需要多语言对齐评估，仅英语评估可能低估模型偏差。

Abstract: Sycophancy, the tendency of language models to prioritize agreement with user preferences over principled reasoning, has been identified as a persistent alignment failure in English-language evaluations. However, it remains unclear whether such diagnostics generalize across languages and cultural contexts. We extend the Beacon single-turn forced-choice sycophancy diagnostic to Hindi through a controlled three-condition design: English original, Hindi literal translation, and Hindi culturally adapted prompts. We evaluate four open-weight instruction-tuned models on 50 prompts per condition, enabling separation of language encoding effects from cultural adaptation effects. Across all models, sycophancy rates are consistently higher for culturally adapted Hindi prompts than for English, with absolute differences ranging from 12.0 to 16.0 percentage points. A decomposition on Qwen 2.5-Coder-7B shows that cultural adaptation (delta = 14.0%, 95% CI: [4.0%, 26.0%]) accounts for the majority of this gap, while language encoding contributes minimally (delta = 2.0%, 95% CI: [0.0%, 6.0%]). Category-level analysis reveals that advice prompts exhibit the largest cross-lingual differences (20-25 percentage points), achieving statistical significance in two of four models. These findings indicate that alignment behaviors measured in English may not transfer uniformly across languages and that culturally grounded prompt framing plays a substantial role. We release all datasets and evaluation code to support replication and extension.

</details>


### [785] [TextBFGS: Quasi-Newton Optimization for Discrete Executable Text via Gradient-Operator Retrieval](https://arxiv.org/abs/2602.00059)
*Zizheng Zhang,Yuyang Liao,Chen Chen,Jian He,Dun Wu,Qianjin Yu,Yanqin Gao,Jin Yang,Kailai Zhang,Eng Siong Chng,Xionghu Zhong*

Main category: cs.LG

Relevance: 85.0

TL;DR: TextBFGS：用于离散文本优化的二阶框架，通过检索梯度算子实现拟牛顿优化，显著提升代码优化效果


<details>
  <summary>Details</summary>
Motivation: 现有离散文本优化方法主要是一阶优化器（类似SGD），存在收敛慢和不稳定的问题，因为它们忽略了优化景观的语义曲率。需要引入二阶优化方法来提升效率。

Method: 提出TextBFGS框架，实现离散文本的拟牛顿优化。通过从预学习成功轨迹的内存中检索梯度算子来近似逆Hessian矩阵，将反馈生成和二阶修正结合为单步推理更新。

Result: 在代码优化任务（HumanEval、MBPP等）上，TextBFGS显著优于一阶基线方法，以更少的模型调用获得更高的通过率，并展现出强大的跨任务迁移能力。

Conclusion: TextBFGS为高效、内存感知的文本优化建立了数学基础范式，将二阶优化思想成功应用于离散文本空间。

Abstract: Optimizing discrete executable text such as prompts and code has recently been framed as a gradient-based process, effectively translating backpropagation concepts to the semantic space. However, existing methods predominantly operate as first-order optimizers akin to Stochastic Gradient Descent, which are suffering from slow convergence and instability because they neglect the semantic curvature of the optimization landscape. To bridge this gap, we introduce TextBFGS, a second-order framework to implement a Quasi-Newton optimization method for discrete text. Unlike traditional memory-based approaches that retrieve similar textual instances, TextBFGS approximates the inverse Hessian matrix by retrieving Gradient-Operators from the memory of pre-learned successful trajectories. Specifically, given a textual gradient feedback, TextBFGS identifies historical correction patterns from the optimization knowledge base and tries to apply these abstract operators to the current variable. This mechanism enables a One-Pass Update, combining feedback generation and second-order correction into a single inference step. Empirical evaluations on code optimization across diverse domains (e.g., HumanEval, MBPP) demonstrate that TextBFGS significantly outperforms first-order baselines. It achieves superior pass rates with fewer model calls and exhibits strong cross-task transferability, thus establishes a mathematically grounded paradigm for efficient, memory-aware text optimization.

</details>


### [786] [Why LoRA Resists Label Noise: A Theoretical Framework for Noise-Robust Parameter-Efficient Fine-Tuning](https://arxiv.org/abs/2602.00084)
*Brady Steele*

Main category: cs.LG

Relevance: 85.0

TL;DR: 论文提出了LoRA抗标签噪声的理论框架，证明LoRA无法记忆所有可能的标签分配，推导了平衡近似偏差和噪声方差的最优秩，并提出了利用秩差异进行噪声检测的RACT方法。


<details>
  <summary>Details</summary>
Motivation: LoRA等参数高效微调方法已成为适应大型预训练模型的主导范式，但对其抗标签噪声的内在特性缺乏理论解释。本研究旨在从理论上解释LoRA为何能抵抗标签噪声，并利用这一特性开发噪声检测方法。

Method: 1) 理论分析：证明秩为r的LoRA无法记忆所有可能的标签分配；2) 推导最优秩以平衡近似偏差和噪声方差；3) 提出RACT（Rank-Aware Curriculum Training）方法，利用秩差异进行噪声检测。

Result: 在AG News数据集上，RACT实现了91.1%的噪声检测F1分数，同时保持91.46%的准确率，与缺乏噪声检测能力的基线方法竞争性相当。实验验证了理论预测：干净模式早期学习，噪声记忆后期发生。

Conclusion: LoRA的内在低秩结构使其具有抵抗标签噪声的能力，这一特性可用于开发有效的噪声检测方法。RACT通过利用秩差异实现了高效的噪声检测，同时保持了模型性能。

Abstract: Parameter-efficient fine-tuning methods like Low-Rank Adaptation (LoRA) have become the dominant paradigm for adapting large pretrained models. We present a theoretical framework explaining an underexplored property: LoRA's inherent resistance to label noise. Our analysis reveals three key insights. First, we prove that rank-$r$ LoRA cannot memorize all possible label assignments once the sample size exceeds $O(r(d+k-r))$, limiting its capacity to fit arbitrary noise. Second, we derive an optimal rank balancing approximation bias and noise-induced variance, showing it decreases with noise rate. Third, we establish temporal separation: clean patterns are learned early while noise memorization occurs later. We propose RACT (Rank-Aware Curriculum Training), leveraging rank discrepancy for noise detection. Experiments validate our predictions, with RACT achieving 91.1% F1 for noise detection on AG News while maintaining 91.46% accuracy, competitive with baselines that lack noise detection capability.

</details>


### [787] [CARE-RFT: Confidence-Anchored Reinforcement Finetuning for Reliable Reasoning in Large Language Models](https://arxiv.org/abs/2602.00085)
*Shuozhe Li,Jincheng Cao,Bodun Hu,Aryan Mokhtari,Leqi Liu,Amy Zhang*

Main category: cs.LG

Relevance: 85.0

TL;DR: CARE-RFT提出了一种新的强化微调方法，通过置信度锚定的偏斜反向KL散度来平衡推理能力与模型可信度，解决了传统RFT在推理性能与可信度之间的权衡问题。


<details>
  <summary>Details</summary>
Motivation: 传统强化微调（RFT）存在一个关键权衡：无约束的RFT能获得强大的推理性能，但严重损害模型可信度（放大幻觉、恶化校准）；而RKL约束的RFT能保持可信度，但由于对探索性偏差的无界惩罚，限制了推理能力的提升。需要一种方法既能获得强大的推理能力，又能保持模型的可信度。

Method: CARE-RFT（置信度锚定正则化强化微调）使用偏斜反向KL散度替代标准的反向KL正则化。该方法提供置信度敏感的惩罚：对于自信且持续获得奖励的探索，惩罚是有界的，从而支持推理能力的发展；在其他情况下惩罚是无界的，以保持校准和可信度。

Result: 在多个模型规模和RFT算法上的广泛实验表明，CARE-RFT实现了优越的平衡：匹配无约束RFT的推理性能，同时恢复基础模型的可信度和校准。该方法在推理能力和可信度之间取得了最佳权衡。

Conclusion: 精心设计的、置信度感知的正则化是构建既有能力又可信赖的推理模型的关键。CARE-RFT通过创新的正则化方法解决了RFT中的关键权衡问题，为开发既强大又可靠的推理模型提供了有效途径。

Abstract: Reinforcement finetuning (RFT) has emerged as a powerful paradigm for unlocking reasoning capabilities in large language models. However, we identify a critical trade-off: while unconstrained RFT achieves strong reasoning performance, it severely compromises model trustworthiness by amplifying hallucination and worsening calibration; conversely, RKL-constrained RFT preserves trustworthiness but limits reasoning gains due to its unbounded penalty on exploratory deviations. To resolve this tension, we introduce CARE-RFT (Confidence-Anchored Regularized Reinforcement Finetuning), a novel method that replaces standard reverse KL regularization with a skew reverse KL divergence. CARE-RFT provides a confidence-sensitive penalty: it is bounded for confident, consistently rewarded explorations to enable reasoning, while unbounded elsewhere to preserve calibration. Extensive experiments across multiple model scales and RFT algorithms show that CARE-RFT achieves a superior balance, matching the reasoning performance of unconstrained RFT while recovering the trustworthiness and calibration of the base model. Our work establishes that careful, confidence-aware regularization is key to building both capable and trustworthy reasoning models.

</details>


### [788] [Interpreting and Controlling Model Behavior via Constitutions for Atomic Concept Edits](https://arxiv.org/abs/2602.00092)
*Neha Kalibhat,Zi Wang,Prasoon Bajpai,Drew Proud,Wenjun Zeng,Been Kim,Mani Malek*

Main category: cs.LG

Relevance: 85.0

TL;DR: 提出一种黑箱可解释性框架，通过原子概念编辑学习可验证的"宪法"——描述提示词修改如何影响模型特定行为的自然语言总结。


<details>
  <summary>Details</summary>
Motivation: 当前大型语言模型和基础模型缺乏可解释性，难以理解提示词修改如何系统性地影响模型行为。需要一种黑箱方法，无需访问模型内部参数，就能学习模型行为与提示修改之间的因果映射。

Method: 使用原子概念编辑（ACEs）——在输入提示中添加、移除或替换可解释概念的操作。系统应用ACEs并观察对模型行为的影响，学习从编辑到可预测结果的因果映射，形成可验证的"宪法"。

Result: 在数学推理和文生图对齐等任务中验证了方法。发现GPT-Image关注语法遵循，Imagen 4优先考虑氛围一致性；数学推理中，干扰变量会混淆GPT-5，但对Gemini 2.5和o4-mini影响较小。学习到的宪法在控制模型行为方面比无宪法方法平均提升1.86倍成功率。

Conclusion: 该框架为黑箱模型提供了深度、可泛化的可解释性洞察，不仅能理解模型行为模式，还能有效控制模型行为，在模型对齐、约束遵循等方面具有应用价值。

Abstract: We introduce a black-box interpretability framework that learns a verifiable constitution: a natural language summary of how changes to a prompt affect a model's specific behavior, such as its alignment, correctness, or adherence to constraints. Our method leverages atomic concept edits (ACEs), which are targeted operations that add, remove, or replace an interpretable concept in the input prompt. By systematically applying ACEs and observing the resulting effects on model behavior across various tasks, our framework learns a causal mapping from edits to predictable outcomes. This learned constitution provides deep, generalizable insights into the model. Empirically, we validate our approach across diverse tasks, including mathematical reasoning and text-to-image alignment, for controlling and understanding model behavior. We found that for text-to-image generation, GPT-Image tends to focus on grammatical adherence, while Imagen 4 prioritizes atmospheric coherence. In mathematical reasoning, distractor variables confuse GPT-5 but leave Gemini 2.5 models and o4-mini largely unaffected. Moreover, our results show that the learned constitutions are highly effective for controlling model behavior, achieving an average of 1.86 times boost in success rate over methods that do not use constitutions.

</details>


### [789] [ALIGN: Aligned Delegation with Performance Guarantees for Multi-Agent LLM Reasoning](https://arxiv.org/abs/2602.00127)
*Tong Zhu,Baiting Chen,Jin Zhou,Hua Zhou,Sriram Sankararaman,Xiaowu Dai*

Main category: cs.LG

Relevance: 85.0

TL;DR: ALIGN提出了一种基于对齐委托的多智能体LLM推理框架，将推理任务委托给多个智能体生成候选解，并通过激励机制和选择机制提升推理性能，提供理论保证和实证优势。


<details>
  <summary>Details</summary>
Motivation: 现有LLM在复杂推理任务上表现不佳，传统的单生成-选择管道和推理时集成方法存在局限性：集成方法通常将候选解视为独立处理，缺乏正式保证能提升推理质量，且无法处理候选解之间的相关性。

Method: ALIGN将LLM推理建模为对齐委托游戏：委托人（principal）将任务委托给多个智能体（agents），这些智能体在设计的激励机制下生成候选解决方案，委托人从中选择最终答案。该方法保持智能体与委托人目标的对齐，同时促进智能体间的结构化交互。

Result: 理论分析证明，在公平比较条件下（同等访问候选解），ALIGN相比单智能体生成能显著提升期望性能。实证结果在广泛的LLM推理基准测试中一致显示，ALIGN优于强单智能体和集成基线方法。

Conclusion: ALIGN通过多智能体对齐委托框架，在理论上和实证上都证明了能提升LLM复杂推理性能，突破了传统集成方法的独立性假设限制，为LLM推理提供了新的有效范式。

Abstract: LLMs often underperform on complex reasoning tasks when relying on a single generation-and-selection pipeline. Inference-time ensemble methods can improve performance by sampling diverse reasoning paths or aggregating multiple candidate answers, but they typically treat candidates independently and provide no formal guarantees that ensembling improves reasoning quality. We propose a novel method, Aligned Delegation for Multi-Agent LLM Reasoning (ALIGN), which formulates LLM reasoning as an aligned delegation game. In ALIGN, a principal delegates a task to multiple agents that generate candidate solutions under designed incentives, and then selects among their outputs to produce a final answer. This formulation induces structured interaction among agents while preserving alignment between agent and principal objectives. We establish theoretical guarantees showing that, under a fair comparison with equal access to candidate solutions, ALIGN provably improves expected performance over single-agent generation. Our analysis accommodates correlated candidate answers and relaxes independence assumptions that are commonly used in prior work. Empirical results across a broad range of LLM reasoning benchmarks consistently demonstrate that ALIGN outperforms strong single-agent and ensemble baselines.

</details>


### [790] [RAPTOR: Ridge-Adaptive Logistic Probes](https://arxiv.org/abs/2602.00158)
*Ziqi Gao,Yaotian Zhu,Qingcheng Zeng,Xu Zhao,Ziqing Wang,Feng Ruan,Kaize Ding*

Main category: cs.LG

Relevance: 85.0

TL;DR: RAPTOR是一种基于L2正则化逻辑回归的探针方法，用于从冻结LLM的层表示中提取概念向量，在准确性、方向稳定性和训练成本方面表现优异。


<details>
  <summary>Details</summary>
Motivation: 探针研究通常用于分析冻结LLM中编码的信息，并在"探针-引导"流程中操作使用。现有方法需要准确、方向稳定且低成本的概念向量估计，但现有方法在这些方面存在不足。

Method: 提出RAPTOR（Ridge-Adaptive Logistic Probe），一种简单的L2正则化逻辑探针，通过验证调整的岭强度从归一化权重中提取概念向量。使用凸高斯极小极大定理（CGMT）在高维少样本机制下对理想化高斯师生模型中的岭逻辑回归进行理论分析。

Result: 在指令调优LLM和人工编写概念数据集上的广泛实验中，RAPTOR在准确性上匹配或超过强基线，同时达到竞争性的方向稳定性和显著更低的训练成本。定性下游引导演示支持这些定量结果。

Conclusion: RAPTOR提供了一种高效准确的概念向量提取方法，理论分析解释了惩罚强度如何调节探针准确性和概念向量稳定性，其结构预测与真实LLM嵌入中观察到的趋势定性一致。

Abstract: Probing studies what information is encoded in a frozen LLM's layer representations by training a lightweight predictor on top of them. Beyond analysis, probes are often used operationally in probe-then-steer pipelines: a learned concept vector is extracted from a probe and injected via additive activation steering by adding it to a layer representation during the forward pass. The effectiveness of this pipeline hinges on estimating concept vectors that are accurate, directionally stable under ablation, and inexpensive to obtain. Motivated by these desiderata, we propose RAPTOR (Ridge-Adaptive Logistic Probe), a simple L2-regularized logistic probe whose validation-tuned ridge strength yields concept vectors from normalized weights. Across extensive experiments on instruction-tuned LLMs and human-written concept datasets, RAPTOR matches or exceeds strong baselines in accuracy while achieving competitive directional stability and substantially lower training cost; these quantitative results are supported by qualitative downstream steering demonstrations. Finally, using the Convex Gaussian Min-max Theorem (CGMT), we provide a mechanistic characterization of ridge logistic regression in an idealized Gaussian teacher-student model in the high-dimensional few-shot regime, explaining how penalty strength mediates probe accuracy and concept-vector stability and yielding structural predictions that qualitatively align with trends observed on real LLM embeddings.

</details>


### [791] [Block removal for large language models through constrained binary optimization](https://arxiv.org/abs/2602.00161)
*David Jansen,Roman Rausch,David Montero,Roman Orus*

Main category: cs.LG

Relevance: 85.0

TL;DR: 提出一种将Transformer块移除问题映射到伊辛模型进行优化的方法，通过物理系统能量作为下游性能代理，高效评估大量候选配置，在多个基准测试中超越现有块移除方法。


<details>
  <summary>Details</summary>
Motivation: 压缩大型语言模型时移除整个Transformer块看似简单，但识别要移除哪些块构成指数级困难的组合优化问题。现有方法难以高效找到高质量的非连续块移除配置。

Method: 将块移除问题形式化为约束二元优化问题，映射到物理系统（伊辛模型），利用系统能量作为下游模型性能的强代理。仅需少量活跃参数的前向和反向传播，配合伊辛求解器，可应用于任何架构。

Result: 在多个基准测试中超越最先进的块移除方法，性能提升在短期重训练后仍保持，在MMLU基准上达到高达6个百分点的改进。成功应用于具有高度非均匀块结构的NVIDIA-Nemotron-3-Nano-30B-A3B-FP8模型。

Conclusion: 通过将块移除问题映射到伊辛模型，提供了一种高效评估大量候选配置的方法，能够发现超越连续区域的高质量非平凡解，为LLM压缩提供了通用且有效的解决方案。

Abstract: Compressing resource-intensive large language models by removing whole transformer blocks is a seemingly simple idea, but identifying which blocks to remove constitutes an exponentially difficult combinatorial problem. In this paper, we formulate block removal as a constrained binary optimization problem that can be mapped to a physical system (Ising model), whose energies are a strong proxy for downstream model performance. This formulation enables an efficient ranking of a large number of candidate block-removal configurations and yields many high-quality, non-trivial solutions beyond consecutive regions. We demonstrate that our approach outperforms state-of-the-art block-removal methods across several benchmarks, with performance gains persisting after short retraining, and reaching improvements of up to 6 points on the MMLU benchmark. Our method requires only forward and backward passes for a few active parameters, together with an (at least approximate) Ising solver, and can be readily applied to any architecture. We illustrate this generality on the recent NVIDIA-Nemotron-3-Nano-30B-A3B-FP8 model, which exhibits a highly inhomogeneous and challenging block structure.

</details>


### [792] [Benford's Law as a Distributional Prior for Post-Training Quantization of Large Language Models](https://arxiv.org/abs/2602.00165)
*Arthur Negrão,Pedro Silva,Vander L. S. Freitas,Gladston Moreira,Eduardo Luz*

Main category: cs.LG

Relevance: 85.0

TL;DR: Benford-Quant：一种受本福特定律启发的非均匀量化器，使用对数间隔码本，为小幅度权重分配更多分辨率，在低比特量化中提升语言模型性能。


<details>
  <summary>Details</summary>
Motivation: 大语言模型压缩需求日益增长，权重量化是最广泛采用的技术。传统均匀量化器假设参数均匀分布，但实际权重分布高度偏斜，需要更符合实际分布的非均匀量化方法。

Method: 提出Benford-Quant，一种简单、无需数据的非均匀量化器，受本福特定律启发，用对数间隔码本替换均匀网格，为频繁出现的小幅度权重分配更多分辨率。

Result: 1) Transformer变换层权重符合本福特统计，归一化层系统性地偏离；2) 在小语言模型上，Benford-Quant持续改善困惑度，在Gemma-270M上4比特量化困惑度降低超过10%；3) 在大语言模型上保持竞争力，差异由过参数化效应解释。

Conclusion: 将本福特先验纳入量化网格是一种低成本修改，在激进的低比特量化中能获得精度提升。虽然无法在困惑度和LAMBADA等任务上超越最先进方法，但可与SmoothQuant和激活感知量化等其他方法混合使用，无需重大管道修改。

Abstract: The rapid growth of Large Language Models (LLMs) intensifies the need for effective compression, with weight quantization being the most widely adopted technique. Standard uniform quantizers assume that parameters are evenly distributed, an assumption at odds with the highly skewed distributions observed in practice. We propose Benford-Quant, a simple, data-free non-uniform quantizer inspired by Benford's Law, which predicts that leading digits follow a logarithmic distribution. Benford-Quant replaces the uniform grid with a log-spaced codebook, dedicating more resolution to the frequent small-magnitude weights. We provide both theoretical intuition and empirical evidence: (i) weights in transformer transformational layers adhere closely to Benford statistics, while normalization layers systematically deviate; (ii) on Small Language Models (SLMs), Benford-Quant consistently improves perplexity, reducing 4-bit perplexity on Gemma-270M by more than 10%; and (iii) on larger LLMs, it remains competitive, with differences explained by over-parameterization effects. Our results indicate that incorporating a Benford-inspired prior into quantization grids is a low-cost modification that yields accuracy gains in aggressive few-bit regimes. Although it is not able to surpass the state of the art in tasks such as perplexity and LAMBADA, the Benford-Quant approach can be hybridized with other quantization methods-such as SmoothQuant and Activation-Aware Quantization-without major pipeline modification, potentially improving their performance.

</details>


### [793] [Joint Continual Learning of Local Language Models and Cloud Offloading Decisions with Budget Constraints](https://arxiv.org/abs/2602.00166)
*Evan Chen,Wenzhi Fang,Shiqiang Wang,Christopher Brinton*

Main category: cs.LG

Relevance: 85.0

TL;DR: DA-GRPO是一种用于小型语言模型持续学习的双优势强化学习方法，通过将云使用约束直接纳入优势计算，使本地模型能联合学习任务能力和协作行为，在保持云协助预算的同时减少灾难性遗忘。


<details>
  <summary>Details</summary>
Motivation: 本地部署的小型语言模型在内存和计算资源有限的情况下需要持续支持多样化任务，必须选择性地依赖云端大型语言模型。然而，在持续学习过程中调节云协助具有挑战性，因为基于奖励的强化学习方法往往产生不稳定的卸载行为，并在任务分布变化时加剧灾难性遗忘。

Method: 提出DA-GRPO（双优势组相对策略优化），这是Group Relative Policy Optimization的扩展，将云使用约束直接纳入优势计算，避免固定的奖励塑造和外部路由模型。该方法使本地模型能够联合学习任务能力和协作行为，让云请求在训练后自然出现，同时遵守预设的协助预算。

Result: 在数学推理和代码生成基准测试中，DA-GRPO相比之前的协作和基于路由的方法，提高了切换后的准确性，显著减少了遗忘，并保持了稳定的云使用。

Conclusion: DA-GRPO通过将云使用约束直接集成到优势计算中，提供了一种有效的持续学习框架，使小型语言模型能够在资源受限的环境中智能地管理云协助，平衡本地能力和外部依赖。

Abstract: Locally deployed Small Language Models (SLMs) must continually support diverse tasks under strict memory and computation constraints, making selective reliance on cloud Large Language Models (LLMs) unavoidable. Regulating cloud assistance during continual learning is challenging, as naive reward-based reinforcement learning often yields unstable offloading behavior and exacerbates catastrophic forgetting as task distributions shift. We propose DA-GRPO, a dual-advantage extension of Group Relative Policy Optimization that incorporates cloud-usage constraints directly into advantage computation, avoiding fixed reward shaping and external routing models. This design enables the local model to jointly learn task competence and collaboration behavior, allowing cloud requests to emerge naturally during post-training while respecting a prescribed assistance budget. Experiments on mathematical reasoning and code generation benchmarks show that DA-GRPO improves post-switch accuracy, substantially reduces forgetting, and maintains stable cloud usage compared to prior collaborative and routing-based approaches.

</details>


### [794] [The Blessing of Dimensionality in LLM Fine-tuning: A Variance-Curvature Perspective](https://arxiv.org/abs/2602.00170)
*Qiyao Liang,Jinyeop Song,Yizhou Liu,Jeff Gore,Ila Fiete,Risto Miikkulainen,Xin Qiu*

Main category: cs.LG

Relevance: 85.0

TL;DR: 该论文揭示了权重扰动进化策略(ES)能够用极小种群(约30个)微调数十亿参数语言模型的机制：微调景观具有低维曲率特性，少数高曲率维度主导改进，使得小种群即可捕捉有效更新方向，同时解释了奖励先升后降的动态模式。


<details>
  <summary>Details</summary>
Motivation: 传统零阶优化理论认为高维问题需要大种群，但实践中ES能用极小种群微调大语言模型，同时观察到奖励先升后降的奇怪动态。作者旨在解释这两个看似矛盾的现象背后的统一几何原理。

Method: 使用ES作为几何探针，在GSM8K、ARC-C、WinoGrande等任务上对Qwen2.5-Instruct模型(0.5B-7B参数)进行微调奖励景观分析。提出最小二次随机上升模型来捕捉动态，通过实验验证低维曲率假设。

Result: 实验表明：1) 奖励改进扰动在小种群下仍可访问；2) 微调景观确实具有低维曲率特性，少数高曲率维度主导优化过程；3) 固定超参数下的奖励先升后降动态可由异质时间尺度解释。

Conclusion: 微调景观的低维曲率特性既解释了ES的小种群可扩展性，又阐明了非单调训练动态。这表明高维微调可能比最坏情况理论预测的更易优化，为更广泛的优化方法提供了可能性。

Abstract: Weight-perturbation evolution strategies (ES) can fine-tune billion-parameter language models with surprisingly small populations (e.g., $N\!\approx\!30$), contradicting classical zeroth-order curse-of-dimensionality intuition. We also observe a second seemingly separate phenomenon: under fixed hyperparameters, the stochastic fine-tuning reward often rises, peaks, and then degrades in both ES and GRPO. We argue that both effects reflect a shared geometric property of fine-tuning landscapes: they are low-dimensional in curvature. A small set of high-curvature dimensions dominates improvement, producing (i) heterogeneous time scales that yield rise-then-decay under fixed stochasticity, as captured by a minimal quadratic stochastic-ascent model, and (ii) degenerate improving updates, where many random perturbations share similar components along these directions. Using ES as a geometric probe on fine-tuning reward landscapes of GSM8K, ARC-C, and WinoGrande across Qwen2.5-Instruct models (0.5B--7B), we show that reward-improving perturbations remain empirically accessible with small populations across scales. Together, these results reconcile ES scalability with non-monotonic training dynamics and suggest that high-dimensional fine-tuning may admit a broader class of viable optimization methods than worst-case theory implies.

</details>


### [795] [Learning Robust Reasoning through Guided Adversarial Self-Play](https://arxiv.org/abs/2602.00173)
*Shuozhe Li,Vaishnav Tadiparthi,Kwonjoon Lee,Nakul Agarwal,Hossein Nourkhiz Mahjoub,Ehsan Moradi Pari,Lizhang Chen,Amy Zhang,Liu Leqi*

Main category: cs.LG

Relevance: 85.0

TL;DR: GASP是一种通过对抗性自博弈增强LLM推理鲁棒性的方法，无需人工标注或外部教师，使模型能够检测和修复误导性上下文中的错误


<details>
  <summary>Details</summary>
Motivation: 现有的基于可验证奖励的强化学习（RLVR）在推理任务中表现良好，但当上下文存在缺陷时（如被破坏的思维链、误导性部分解决方案或轻微输入扰动）会灾难性失败，因为标准RLVR只在干净条件下优化最终答案的正确性

Method: GASP方法在单个模型内构建对抗性自博弈游戏：污染者学习通过局部连贯的破坏来诱导失败，而智能体学习在相同破坏条件下诊断和恢复。为解决训练早期成功恢复的稀缺性，提出了分布内修复指导，通过对自生成修复的模仿项来增加恢复概率

Result: 在四个开放权重模型（1.5B-8B）上，GASP将强但脆弱的推理器转化为鲁棒的推理器，能够抵御误导和扰动的上下文，同时通常还能提高干净准确性。分析显示对抗性破坏诱导了有效的课程学习，分布内指导实现了快速恢复学习

Conclusion: GASP提供了一种仅使用结果验证就能显式训练检测和修复能力的方法，增强了LLM在不可靠上下文条件下的推理鲁棒性，对RLHF和可信AI研究有重要意义

Abstract: Reinforcement learning from verifiable rewards (RLVR) produces strong reasoning models, yet they can fail catastrophically when the conditioning context is fallible (e.g., corrupted chain-of-thought, misleading partial solutions, or mild input perturbations), since standard RLVR optimizes final-answer correctness only under clean conditioning. We introduce GASP (Guided Adversarial Self-Play), a robustification method that explicitly trains detect-and-repair capabilities using only outcome verification. Without human labels or external teachers, GASP forms an adversarial self-play game within a single model: a polluter learns to induce failure via locally coherent corruptions, while an agent learns to diagnose and recover under the same corrupted conditioning. To address the scarcity of successful recoveries early in training, we propose in-distribution repair guidance, an imitation term on self-generated repairs that increases recovery probability while preserving previously acquired capabilities. Across four open-weight models (1.5B--8B), GASP transforms strong-but-brittle reasoners into robust ones that withstand misleading and perturbed context while often improving clean accuracy. Further analysis shows that adversarial corruptions induce an effective curriculum, and in-distribution guidance enables rapid recovery learning with minimal representational drift.

</details>


### [796] [Dispersion Loss Counteracts Embedding Condensation and Improves Generalization in Small Language Models](https://arxiv.org/abs/2602.00217)
*Chen Liu,Xingzhi Sun,Xi Xiao,Alexandre Van Tassel,Ke Xu,Kristof Reimann,Danqi Liao,Mark Gerstein,Tianyang Wang,Xiao Wang,Smita Krishnaswamy*

Main category: cs.LG

Relevance: 85.0

TL;DR: 论文研究了大型语言模型中的"嵌入凝聚"现象，发现小模型（如GPT2、Qwen3-0.6B）的token嵌入会坍缩到狭窄的锥形子空间，而大模型（如GPT2-xl、Qwen3-32B）对此现象更具抵抗力。作者提出了一种分散损失函数来缓解此问题，并在10个基准测试中取得了性能提升。


<details>
  <summary>Details</summary>
Motivation: 随着LLM参数规模扩大，计算成本急剧增加。为了理解LLM缩放规律，研究大模型与小模型之间的表征差异，目标是让小模型复制大模型的表征质量，从而在不增加参数的情况下改进小模型。

Method: 1. 系统分析多个Transformer家族，观察"嵌入凝聚"现象；2. 发现知识蒸馏不能可靠缓解此现象；3. 提出分散损失函数，在训练过程中显式鼓励嵌入分散；4. 在多个模型上进行实验验证。

Result: 1. 小模型存在严重的嵌入凝聚现象，大模型对此更具抵抗力；2. 提出的分散损失函数能有效缓解凝聚现象，恢复大模型的分散模式；3. 在10个基准测试中获得了性能提升；4. 为改进小Transformer提供了有原则的路径。

Conclusion: 嵌入凝聚是小模型与大模型之间的重要表征差异，通过显式优化嵌入分散可以改善小模型的性能，这为在不增加参数的情况下改进Transformer模型提供了新思路。

Abstract: Large language models (LLMs) achieve remarkable performance through ever-increasing parameter counts, but scaling incurs steep computational costs. To better understand LLM scaling, we study representational differences between LLMs and their smaller counterparts, with the goal of replicating the representational qualities of larger models in the smaller models. We observe a geometric phenomenon which we term $\textbf{embedding condensation}$, where token embeddings collapse into a narrow cone-like subspace in some language models. Through systematic analyses across multiple Transformer families, we show that small models such as $\texttt{GPT2}$ and $\texttt{Qwen3-0.6B}$ exhibit severe condensation, whereas the larger models such as $\texttt{GPT2-xl}$ and $\texttt{Qwen3-32B}$ are more resistant to this phenomenon. Additional observations show that embedding condensation is not reliably mitigated by knowledge distillation from larger models. To fight against it, we formulate a dispersion loss that explicitly encourages embedding dispersion during training. Experiments demonstrate that it mitigates condensation, recovers dispersion patterns seen in larger models, and yields performance gains across 10 benchmarks. We believe this work offers a principled path toward improving smaller Transformers without additional parameters.

</details>


### [797] [Sample Complexity Analysis for Constrained Bilevel Reinforcement Learning](https://arxiv.org/abs/2602.00282)
*Naman Saxena,Vaneet Aggarwal*

Main category: cs.LG

Relevance: 85.0

TL;DR: 本文分析了约束双层强化学习算法的样本复杂度，提出了CBSO算法，获得了O(ε^{-2})的迭代复杂度和Õ(ε^{-4})的样本复杂度，首次使用Moreau包络分析非光滑目标函数的参数化策略梯度RL算法。


<details>
  <summary>Details</summary>
Motivation: 元学习、分层学习和人类反馈RL等许多重要RL问题都可以建模为双层RL问题。虽然这些领域在实证上取得了很大进展，但双层RL算法的理论分析尚未得到足够关注。本文旨在分析约束双层RL算法的样本复杂度。

Method: 提出约束双层次梯度优化(CBSO)算法，使用惩罚目标函数避免约束双层问题中的原始-对偶间隙和超梯度问题。采用Moreau包络分析非光滑优化问题，这是首次使用Moreau包络分析具有非光滑目标函数的一般参数化策略梯度RL算法。

Result: CBSO算法获得了O(ε^{-2})的迭代复杂度和Õ(ε^{-4})的样本复杂度。通过惩罚公式处理约束，避免了原始-对偶间隙问题，并在非光滑优化分析方面取得了理论进展。

Conclusion: 本文为约束双层RL问题提供了首个样本复杂度分析框架，填补了该领域的理论空白。提出的CBSO算法和Moreau包络分析方法为处理非光滑约束RL问题提供了新的理论工具。

Abstract: Several important problem settings within the literature of reinforcement learning (RL), such as meta-learning, hierarchical learning, and RL from human feedback (RL-HF), can be modelled as bilevel RL problems. A lot has been achieved in these domains empirically; however, the theoretical analysis of bilevel RL algorithms hasn't received a lot of attention. In this work, we analyse the sample complexity of a constrained bilevel RL algorithm, building on the progress in the unconstrained setting. We obtain an iteration complexity of $O(ε^{-2})$ and sample complexity of $\tilde{O}(ε^{-4})$ for our proposed algorithm, Constrained Bilevel Subgradient Optimization (CBSO). We use a penalty-based objective function to avoid the issue of primal-dual gap and hyper-gradient in the context of a constrained bilevel problem setting. The penalty-based formulation to handle constraints requires analysis of non-smooth optimization. We are the first ones to analyse the generally parameterized policy gradient-based RL algorithm with a non-smooth objective function using the Moreau envelope.

</details>


### [798] [Self-Attention at Constant Cost per Token via Symmetry-Aware Taylor Approximation](https://arxiv.org/abs/2602.00294)
*Franz A. Heinsen,Leo Kozachkov*

Main category: cs.LG

Relevance: 85.0

TL;DR: 该论文提出了一种新的自注意力计算方法，能够以恒定成本计算任意精度的自注意力，显著降低了Transformer模型的内存使用和计算需求。


<details>
  <summary>Details</summary>
Motivation: 标准自注意力的计算成本随上下文长度增加而增加，导致存储、计算和能源需求超出社会供应能力。为了解决这个问题，需要开发一种能够以恒定成本计算自注意力的方法。

Method: 通过将传统自注意力公式的泰勒展开分解为对称张量链上的表达式，利用对称性获得前馈变换，将查询和键高效映射到最小多项式核特征基中的坐标。成本与头大小成反比固定。

Result: 实现了恒定成本的自注意力计算，内存使用和计算量减少数个数量级，支持无限制的令牌生成，显著降低大规模Transformer模型的基础设施和能源需求。

Conclusion: 该工作使自注意力能够以恒定成本计算到任意精度，为大规模Transformer模型提供了更高效的替代方案，所引入的数学技术具有独立的研究价值。

Abstract: The most widely used artificial intelligence (AI) models today are Transformers employing self-attention. In its standard form, self-attention incurs costs that increase with context length, driving demand for storage, compute, and energy that is now outstripping society's ability to provide them. To help address this issue, we show that self-attention is efficiently computable to arbitrary precision with constant cost per token, achieving orders-of-magnitude reductions in memory use and computation. We derive our formulation by decomposing the conventional formulation's Taylor expansion into expressions over symmetric chains of tensor products. We exploit their symmetry to obtain feed-forward transformations that efficiently map queries and keys to coordinates in a minimal polynomial-kernel feature basis. Notably, cost is fixed inversely in proportion to head size, enabling application over a greater number of heads per token than otherwise feasible. We implement our formulation and empirically validate its correctness. Our work enables unbounded token generation at modest fixed cost, substantially reducing the infrastructure and energy demands of large-scale Transformer models. The mathematical techniques we introduce are of independent interest.

</details>


### [799] [Beyond the Loss Curve: Scaling Laws, Active Learning, and the Limits of Learning from Exact Posteriors](https://arxiv.org/abs/2602.00315)
*Arian Khorasani,Nathaniel Chen,Yug D Oswal,Akshat Santhana Gopalan,Egemen Kolemen,Ravid Shwartz-Ziv*

Main category: cs.LG

Relevance: 85.0

TL;DR: 该论文提出了一个使用类别条件归一化流作为oracle来获取真实后验分布的方法，从而能够精确评估神经网络在图像分类任务上的极限性能，揭示了标准基准无法发现的五个重要发现。


<details>
  <summary>Details</summary>
Motivation: 标准基准测试无法回答"神经网络距离最佳可能性能有多近"的问题，因为它们无法获取真实后验分布p(y|x)。研究者希望建立一个能够精确评估神经网络极限性能的框架。

Method: 使用类别条件归一化流作为oracle，在真实图像数据集（AFHQ、ImageNet）上使精确后验分布变得可计算。通过这个框架进行五个方面的研究：缩放定律、学习极限、软标签、分布偏移和主动学习。

Result: 1) 预测误差可分解为不可约的偶然不确定性和可约的认知误差；2) 认知误差随数据集大小呈幂律下降；3) ResNet显示干净的幂律缩放而Vision Transformer在低数据区域停滞；4) 使用精确后验训练优于硬标签且校准近乎完美；5) 分布偏移类型比幅度更重要；6) 精确认知不确定性提高主动学习样本效率。

Conclusion: 该框架揭示了标准指标隐藏了持续学习、掩盖了架构差异，并且无法诊断分布偏移的性质。通过获取真实后验分布，能够更深入地理解神经网络的极限性能和学习动态。

Abstract: How close are neural networks to the best they could possibly do? Standard benchmarks cannot answer this because they lack access to the true posterior p(y|x). We use class-conditional normalizing flows as oracles that make exact posteriors tractable on realistic images (AFHQ, ImageNet). This enables five lines of investigation. Scaling laws: Prediction error decomposes into irreducible aleatoric uncertainty and reducible epistemic error; the epistemic component follows a power law in dataset size, continuing to shrink even when total loss plateaus. Limits of learning: The aleatoric floor is exactly measurable, and architectures differ markedly in how they approach it: ResNets exhibit clean power-law scaling while Vision Transformers stall in low-data regimes. Soft labels: Oracle posteriors contain learnable structure beyond class labels: training with exact posteriors outperforms hard labels and yields near-perfect calibration. Distribution shift: The oracle computes exact KL divergence of controlled perturbations, revealing that shift type matters more than shift magnitude: class imbalance barely affects accuracy at divergence values where input noise causes catastrophic degradation. Active learning: Exact epistemic uncertainty distinguishes genuinely informative samples from inherently ambiguous ones, improving sample efficiency. Our framework reveals that standard metrics hide ongoing learning, mask architectural differences, and cannot diagnose the nature of distribution shift.

</details>


### [800] [Harvest: Opportunistic Peer-to-Peer GPU Caching for LLM Inference](https://arxiv.org/abs/2602.00328)
*Nikhil Gopal,Kostis Kaffes*

Main category: cs.LG

Relevance: 85.0

TL;DR: Harvest框架利用GPU间高速互连，将未使用的GPU内存作为临时缓存层来存储模型权重和KV缓存，显著提升LLM推理吞吐量2倍以上。


<details>
  <summary>Details</summary>
Motivation: LLM推理越来越受GPU内存容量而非计算吞吐量的限制，主要由于模型规模增长和自回归解码中KV缓存的线性增长。现有方法将模型状态和KV张量卸载到主机内存，但受限于PCIe带宽导致延迟显著增加。

Method: Harvest是一个机会主义的GPU缓存管理框架，利用高带宽的GPU对等互连，动态地将模型权重和KV缓存放置在未使用的GPU内存中。它将对等GPU内存视为临时缓存层，在动态内存可用性下保持正确性同时减少数据移动开销。

Result: 通过使用Harvest加速两个广泛使用的推理组件（专家层权重和KV缓存条目）的检索，实现了超过2倍的吞吐量加速。

Conclusion: Harvest框架通过利用未使用的GPU内存作为临时缓存层，有效解决了LLM推理中的内存瓶颈问题，显著提升了推理性能。

Abstract: Large Language Model (LLM) inference is increasingly constrained by GPU memory capacity rather than compute throughput, driven by growing model sizes and the linear growth of the key-value (KV) cache during autoregressive decoding. Existing approaches mitigate memory pressure by offloading model state and KV tensors to host memory, but incur substantial latency due to limited PCIe bandwidth. We present Harvest, an opportunistic GPU cache management framework that exploits high-bandwidth peer-to-peer GPU interconnects to dynamically place model weights and KV cache in unused GPU memory. Harvest treats peer GPU memory as a transient cache tier, preserving correctness while reducing data movement overhead under dynamic memory availability. We demonstrate significant throughput speedup of more than 2 times by using Harvest to accelerate the retrieval of two widely-used inference components: expert layer weights and KV cache entries.

</details>


### [801] [In-Run Data Shapley for Adam Optimizer](https://arxiv.org/abs/2602.00329)
*Meng Ding,Zeqing Zhang,Di Wang,Lijie Hu*

Main category: cs.LG

Relevance: 85.0

TL;DR: 本文提出Adam感知的In-Run数据Shapley方法，解决了现有SGD-based数据归因方法在Adam优化器下失效的问题，实现了高效准确的数据贡献评估。


<details>
  <summary>Details</summary>
Motivation: 可靠的数据归因对于减轻机器学习中的偏见和减少计算浪费至关重要。现有"In-Run"方法虽然避免了昂贵的重训练成本，但严重依赖SGD的线性结构，无法捕捉Adam等自适应优化器的复杂动态，导致在现代化训练流程中失效。

Method: 提出Adam-Aware In-Run Data Shapley方法：1) 通过固定状态假设重新定义效用函数，恢复可加性；2) 提出线性化幽灵近似技术，线性化方差依赖的缩放项，无需计算每个样本的梯度即可计算成对梯度点积。

Result: 实验表明：1) 该方法与真实边际贡献的Pearson相关系数R > 0.99，达到近乎完美的保真度；2) 保持约95%的标准训练吞吐量；3) 在数据归因下游任务中显著优于SGD-based基线方法。

Conclusion: 数据归因本质上是优化器依赖的，SGD-based代理方法在Adam下严重失效。提出的Adam感知方法能够准确高效地评估数据贡献，适用于现代化训练流程。

Abstract: Reliable data attribution is essential for mitigating bias and reducing computational waste in modern machine learning, with the Shapley value serving as the theoretical gold standard. While recent "In-Run" methods bypass the prohibitive cost of retraining by estimating contributions dynamically, they heavily rely on the linear structure of Stochastic Gradient Descent (SGD) and fail to capture the complex dynamics of adaptive optimizers like Adam. In this work, we demonstrate that data attribution is inherently optimizer-dependent: we show that SGD-based proxies diverge significantly from true contributions under Adam (Pearson $R \approx 0.11$), rendering them ineffective for modern training pipelines. To bridge this gap, we propose Adam-Aware In-Run Data Shapley. We derive a closed-form approximation that restores additivity by redefining utility under a fixed-state assumption and enable scalable computation via a novel Linearized Ghost Approximation. This technique linearizes the variance-dependent scaling term, allowing us to compute pairwise gradient dot-products without materializing per-sample gradients. Extensive experiments show that our method achieves near-perfect fidelity to ground-truth marginal contributions ($R > 0.99$) while retaining $\sim$95\% of standard training throughput. Furthermore, our Adam-aware attribution significantly outperforms SGD-based baselines in data attribution downstream tasks.

</details>


### [802] [Efficient and accurate steering of Large Language Models through attention-guided feature learning](https://arxiv.org/abs/2602.00333)
*Parmida Davarmanesh,Ashia Wilson,Adityanarayanan Radhakrishnan*

Main category: cs.LG

Relevance: 85.0

TL;DR: 提出注意力引导的LLM内部激活操纵框架，解决现有steering方法的脆弱性问题，在512个语义概念基准上性能大幅提升


<details>
  <summary>Details</summary>
Motivation: 现有steering方法非常脆弱，对特征提取的算法选择敏感，非steerable概念可能因细微算法变化而变得完全可操纵。需要解决三个核心挑战：自动选择相关token嵌入、处理概念特征异质性、识别最相关层

Method: 提出注意力引导的steering框架，通过注意力机制自动选择相关token嵌入提取概念特征，考虑LLM激活中概念特征的异质性，识别最适合steering的层

Result: 在512个语义概念的steering基准上，性能显著超越先前SOTA（成功steer的概念数量几乎翻倍），适用于不同架构和规模的模型（最高700亿参数）

Conclusion: 该框架为开发高效、可扩展的工业级LLM微调算法开辟了新途径，并揭示了概念特定特征在LLM层间的分布规律

Abstract: Steering, or direct manipulation of internal activations to guide LLM responses toward specific semantic concepts, is emerging as a promising avenue for both understanding how semantic concepts are stored within LLMs and advancing LLM capabilities. Yet, existing steering methods are remarkably brittle, with seemingly non-steerable concepts becoming completely steerable based on subtle algorithmic choices in how concept-related features are extracted. In this work, we introduce an attention-guided steering framework that overcomes three core challenges associated with steering: (1) automatic selection of relevant token embeddings for extracting concept-related features; (2) accounting for heterogeneity of concept-related features across LLM activations; and (3) identification of layers most relevant for steering. Across a steering benchmark of 512 semantic concepts, our framework substantially improved steering over previous state-of-the-art (nearly doubling the number of successfully steered concepts) across model architectures and sizes (up to 70 billion parameter models). Furthermore, we use our framework to shed light on the distribution of concept-specific features across LLM layers. Overall, our framework opens further avenues for developing efficient, highly-scalable fine-tuning algorithms for industry-scale LLMs.

</details>


### [803] [Post-Training Probability Manifold Correction via Structured SVD Pruning and Self-Referential Distillation](https://arxiv.org/abs/2602.00372)
*Aaron R. Flouro,Shawn P. Chadwick*

Main category: cs.LG

Relevance: 85.0

TL;DR: SparseKD是一种后训练压缩方法，结合结构化SVD剪枝和自参考知识蒸馏，无需外部教师模型，让模型通过匹配自身压缩前的概率分布来恢复质量。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型部署成本高，需要有效的压缩方法。现有方法通常需要外部教师模型或架构修改，增加了复杂性和部署难度。

Method: 结合结构化SVD剪枝和自参考知识蒸馏。关键创新是自参考蒸馏：模型通过匹配自身压缩前的概率分布来恢复质量，无需外部教师。使用固定校准数据集，在相同目标函数下进行后训练。

Result: 自参考蒸馏单独应用可将模型质量相对原始检查点提升39%。结合结构化剪枝，SparseKD实现15-65%参数减少，质量损失可接受。内核分析显示加速完全来自前馈层的密集矩阵乘法减少，注意力机制保持不变。

Conclusion: SparseKD提供了一种简单有效的LLM压缩方法，无需外部教师模型、架构修改或定制推理内核，可直接部署到现有基础设施。该方法与注意力优化互补，具有高可复现性。

Abstract: Large language models are expensive to deploy. We introduce Sparse Knowledge Distillation (SparseKD), a post-training method that compresses transformer models by combining structured SVD pruning with self-referential knowledge distillation. The key insight is simple: instead of using an external teacher, the model teaches itself by matching its own probability distribution from before compression. This self-referential setup enables surprisingly strong quality recovery after aggressive pruning.
  Our experiments reveal an unexpected finding: self-referential distillation alone, applied post-training under an identical objective and fixed calibration dataset, improves model quality by 39% relative to the original converged checkpoint. When combined with structured pruning, SparseKD achieves 15-65% parameter reduction with acceptable quality trade-offs. Kernel profiling shows that speedups arise entirely from reduced dense matrix multiplication in feed-forward layers while attention remains unchanged, making this approach complementary to attention optimizations.
  We validate across two model families (0.6B and 3.8B parameters) with multi-seed experiments confirming high reproducibility. SparseKD requires no external super-teacher, no architectural changes, and no custom inference kernels, making it immediately deployable with existing infrastructure.

</details>


### [804] [A Fragile Guardrail: Diffusion LLM's Safety Blessing and Its Failure Mode](https://arxiv.org/abs/2602.00388)
*Zeyuan He,Yupeng Chen,Lang Lin,Yihan Wang,Shenxu Chang,Eric Sommerlade,Philip Torr,Junchi Yu,Adel Bibi,Jialin Yu*

Main category: cs.LG

Relevance: 85.0

TL;DR: 扩散大语言模型(D-LLMs)相比自回归LLMs具有内在的安全优势，能抵抗传统越狱攻击，但存在上下文嵌套的脆弱性，可绕过其安全机制


<details>
  <summary>Details</summary>
Motivation: 研究扩散大语言模型(D-LLMs)相对于自回归LLMs(AR-LLMs)的安全特性，探索其内在的越狱攻击鲁棒性机制及潜在脆弱性

Method: 分析扩散轨迹的逐步抑制效应机制，提出上下文嵌套攻击策略，将有害请求嵌入良性结构化上下文中，实证评估多个模型和基准

Result: D-LLMs对传统越狱攻击具有内在鲁棒性，但上下文嵌套攻击能有效绕过其安全机制，在多个模型和基准上达到SOTA攻击成功率，首次成功越狱Gemini Diffusion

Conclusion: D-LLMs的安全优势源于扩散轨迹的逐步抑制效应，但存在上下文嵌套的脆弱性，这是对D-LLMs的早期红队测试，揭示了商业D-LLMs的关键漏洞

Abstract: Diffusion large language models (D-LLMs) offer an alternative to autoregressive LLMs (AR-LLMs) and have demonstrated advantages in generation efficiency. Beyond the utility benefits, we argue that D-LLMs exhibit a previously underexplored safety blessing: their diffusion-style generation confers intrinsic robustness against jailbreak attacks originally designed for AR-LLMs. In this work, we provide an initial analysis of the underlying mechanism, showing that the diffusion trajectory induces a stepwise reduction effect that progressively suppresses unsafe generations. This robustness, however, is not absolute. We identify a simple yet effective failure mode, termed context nesting, where harmful requests are embedded within structured benign contexts, effectively bypassing the stepwise reduction mechanism. Empirically, we show that this simple strategy is sufficient to bypass D-LLMs' safety blessing, achieving state-of-the-art attack success rates across models and benchmarks. Most notably, it enables the first successful jailbreak of Gemini Diffusion, to our knowledge, exposing a critical vulnerability in commercial D-LLMs. Together, our results characterize both the origins and the limits of D-LLMs' safety blessing, constituting an early-stage red-teaming of D-LLMs.

</details>


### [805] [Fast Forward: Accelerating LLM Prefill with Predictive FFN Sparsity](https://arxiv.org/abs/2602.00397)
*Aayush Gautam,Mukul Gagrani,Junyoung Park,Mingu Lee,Chiris Lott,Narasimha Reddy*

Main category: cs.LG

Relevance: 85.0

TL;DR: FastForward：通过块级、上下文感知的FFN稀疏化加速LLM预填充阶段，减少长上下文推理的计算瓶颈


<details>
  <summary>Details</summary>
Motivation: LLM推理的预填充阶段是长上下文工作负载的主要计算瓶颈。在短到中等上下文长度（1K-16K tokens）下，前馈网络（FFNs）占用了大部分计算成本。现有的FFN稀疏化方法是为自回归解码设计的，无法利用预填充阶段的并行性，且常常降低准确性。

Method: FastForward是一个预测性稀疏化框架，包含三个核心组件：1）轻量级专家预测器，用于按块选择高重要性神经元；2）误差补偿网络，用于纠正稀疏化引起的误差；3）层间稀疏度调度器，根据token混合重要性分配计算资源。

Result: 在LLaMA和Qwen模型（最高8B参数）上，FastForward在50% FFN稀疏度下实现了最高1.45倍的计算加速，在LongBench基准测试中相比密集基线的准确率损失小于6%，显著减少了首token生成时间（TTFT）。

Conclusion: FastForward通过上下文感知的FFN稀疏化有效加速了LLM预填充阶段，为受限硬件上的高效长上下文LLM推理提供了解决方案。

Abstract: The prefill stage of large language model (LLM) inference is a key computational bottleneck for long-context workloads. At short-to-moderate context lengths (1K--16K tokens), Feed-Forward Networks (FFNs) dominate this cost, accounting for most of the total FLOPs. Existing FFN sparsification methods, designed for autoregressive decoding, fail to exploit the prefill stage's parallelism and often degrade accuracy. To address this, we introduce FastForward, a predictive sparsity framework that accelerates LLM prefill through block-wise, context-aware FFN sparsity. FastForward combines (1) a lightweight expert predictor to select high-importance neurons per block, (2) an error compensation network to correct sparsity-induced errors, and (3) a layer-wise sparsity scheduler to allocate compute based on token-mixing importance. Across LLaMA and Qwen models up to 8B parameters, FastForward delivers up to 1.45$\times$ compute-bound speedup at 50% FFN sparsity with $<$ 6% accuracy loss compared to the dense baseline on LongBench, substantially reducing Time-to-First-Token (TTFT) for efficient, long-context LLM inference on constrained hardware.

</details>


### [806] [MemoryLLM: Plug-n-Play Interpretable Feed-Forward Memory for Transformers](https://arxiv.org/abs/2602.00398)
*Ajay Jaiswal,Lauren Hannah,Han-Byul Kim,Duc Hoang,Arnav Kundu,Mehrdad Farajtabar,Minsik Cho*

Main category: cs.LG

Relevance: 85.0

TL;DR: MemoryLLM将Transformer中的前馈网络（FFN）与自注意力机制解耦，将其视为上下文无关的token-wise神经检索记忆，通过独立训练FFN使其成为可预计算的token-wise查找表，提升推理效率。


<details>
  <summary>Details</summary>
Motivation: 理解Transformer组件在LLM中的运作机制对AI技术进步至关重要。当前FFN模块的可解释性面临挑战，作者希望将FFN从自注意力中解耦，将其作为上下文无关的token-wise神经检索记忆来研究，探索输入token如何访问FFN参数中的记忆位置，以及FFN记忆在不同下游任务中的重要性。

Method: 提出MemoryLLM，通过使用token嵌入直接独立训练FFN（与自注意力分离），实现上下文无关的FFN。这使得FFN可以预计算为token-wise查找表（ToLs），支持在VRAM和存储之间按需传输。还提出Flex-MemoryLLM作为传统Transformer设计和MemoryLLM之间的中间架构，弥补因使用上下文无关token-wise嵌入训练FFN导致的性能差距。

Result: MemoryLLM实现了FFN与自注意力的解耦，将FFN转化为可预计算的token-wise查找表，从而提升推理效率。Flex-MemoryLLM作为中间架构，在保持传统Transformer性能的同时，部分实现了MemoryLLM的设计理念。

Conclusion: 通过解耦FFN和自注意力，MemoryLLM为Transformer架构的可解释性研究提供了新视角，同时通过token-wise查找表的设计提升了推理效率。Flex-MemoryLLM为平衡性能和架构创新提供了实用方案。

Abstract: Understanding how transformer components operate in LLMs is important, as it is at the core of recent technological advances in artificial intelligence. In this work, we revisit the challenges associated with interpretability of feed-forward modules (FFNs) and propose MemoryLLM, which aims to decouple FFNs from self-attention and enables us to study the decoupled FFNs as context-free token-wise neural retrieval memory. In detail, we investigate how input tokens access memory locations within FFN parameters and the importance of FFN memory across different downstream tasks. MemoryLLM achieves context-free FFNs by training them in isolation from self-attention directly using the token embeddings. This approach allows FFNs to be pre-computed as token-wise lookups (ToLs), enabling on-demand transfer between VRAM and storage, additionally enhancing inference efficiency. We also introduce Flex-MemoryLLM, positioning it between a conventional transformer design and MemoryLLM. This architecture bridges the performance gap caused by training FFNs with context-free token-wise embeddings.

</details>


### [807] [LLMs as High-Dimensional Nonlinear Autoregressive Models with Attention: Training, Alignment and Inference](https://arxiv.org/abs/2602.00426)
*Vikram Krishnamurthy*

Main category: cs.LG

Relevance: 85.0

TL;DR: 该论文提供了一个关于大语言模型的数学参考框架，将LLM形式化为高维非线性自回归模型，涵盖预训练、对齐方法和生成过程。


<details>
  <summary>Details</summary>
Motivation: 现有LLM描述通常侧重于架构组件和训练过程，缺乏对底层计算结构的清晰数学表达。本文旨在为研究人员提供一个明确的、方程级别的LLM训练、对齐和生成描述框架。

Method: 将LLM形式化为高维非线性自回归模型，使用基于注意力的依赖关系。框架包括：1）通过下一词预测进行预训练；2）对齐方法（RLHF、DPO、RSFT、RLVR）；3）推理时的自回归生成。自注意力被描述为重复的双线性-softmax-线性组合。

Result: 建立了一个统一的数学框架，能够原则性地分析对齐诱导行为（如奉承）、推理时现象（幻觉、上下文学习、思维链提示、检索增强生成）以及扩展如持续学习。

Conclusion: 该数学框架为LLM的解释和进一步理论发展提供了简洁参考，揭示了自注意力作为高度表达序列模型的内在结构。

Abstract: Large language models (LLMs) based on transformer architectures are typically described through collections of architectural components and training procedures, obscuring their underlying computational structure. This review article provides a concise mathematical reference for researchers seeking an explicit, equation-level description of LLM training, alignment, and generation. We formulate LLMs as high-dimensional nonlinear autoregressive models with attention-based dependencies. The framework encompasses pretraining via next-token prediction, alignment methods such as reinforcement learning from human feedback (RLHF), direct preference optimization (DPO), rejection sampling fine-tuning (RSFT), and reinforcement learning from verifiable rewards (RLVR), as well as autoregressive generation during inference. Self-attention emerges naturally as a repeated bilinear--softmax--linear composition, yielding highly expressive sequence models. This formulation enables principled analysis of alignment-induced behaviors (including sycophancy), inference-time phenomena (such as hallucination, in-context learning, chain-of-thought prompting, and retrieval-augmented generation), and extensions like continual learning, while serving as a concise reference for interpretation and further theoretical development.

</details>


### [808] [Towards Building Non-Fine-Tunable Foundation Models](https://arxiv.org/abs/2602.00446)
*Ziyao Wang,Nizhang Li,Pingzhi Li,Guoheng Sun,Tianlong Chen,Ang Li*

Main category: cs.LG

Relevance: 85.0

TL;DR: 论文提出Private Mask Pre-Training (PMP)框架，通过将表征学习集中在训练早期识别的稀疏子网络中，并仅发布最终密集权重而保持二进制掩码私有，从而构建不可微调的基础模型，防止未经授权的下游微调。


<details>
  <summary>Details</summary>
Motivation: 开源基础模型虽然促进了广泛重用，但也使模型训练者面临未经限制的下游微调带来的经济和安全风险。需要构建在发布形式下保持广泛可用性，但在未经授权的任务无关微调下仅能获得有限适应增益的模型。

Method: 提出Private Mask Pre-Training (PMP)预训练框架：1）在训练早期识别稀疏子网络，将表征学习集中在该子网络中；2）保持定义子网络的二进制掩码私有，仅发布最终密集权重；3）未经授权的微调由于无法访问掩码，会更新与预训练子空间不对齐的参数，导致微调目标与预训练几何之间的内在不匹配。

Result: 理论分析表明这种不匹配会破坏基于梯度的适应过程并限制微调增益。在大语言模型上的实证结果显示，PMP保持了基础模型性能，同时在一系列下游任务中持续降低未经授权微调的效果，非微调可调性强度可通过掩码比率控制。

Conclusion: PMP框架提供了一种有效的方法来构建不可微调的基础模型，既保持了模型的广泛可用性，又限制了未经授权微调的效果，为开源基础模型的安全部署提供了解决方案。

Abstract: Open-sourcing foundation models (FMs) enables broad reuse but also exposes model trainers to economic and safety risks from unrestricted downstream fine-tuning. We address this problem by building non-fine-tunable foundation models: models that remain broadly usable in their released form while yielding limited adaptation gains under task-agnostic unauthorized fine-tuning. We propose Private Mask Pre-Training (PMP), a pre-training framework that concentrates representation learning into a sparse subnetwork identified early in training. The binary mask defining this subnetwork is kept private, and only the final dense weights are released. This forces unauthorized fine-tuning without access to the mask to update parameters misaligned with pretraining subspace, inducing an intrinsic mismatch between the fine-tuning objective and the pre-training geometry. We provide theoretical analysis showing that this mismatch destabilizes gradient-based adaptation and bounds fine-tuning gains. Empirical results on large language models demonstrating that PMP preserves base model performance while consistently degrading unauthorized fine-tuning across a wide range of downstream tasks, with the strength of non-fine-tunability controlled by the mask ratio.

</details>


### [809] [FedMOA: Federated GRPO for Personalized Reasoning LLMs under Heterogeneous Rewards](https://arxiv.org/abs/2602.00453)
*Ziyao Wang,Daeun Jung,Yexiao He,Guoheng Sun,Zheyu Shen,Myungjin Lee,Ang Li*

Main category: cs.LG

Relevance: 85.0

TL;DR: FedMOA：基于联邦学习的多目标对齐框架，采用自适应权重机制和任务感知聚合策略，在数学推理和代码生成任务上优于联邦平均方法


<details>
  <summary>Details</summary>
Motivation: 传统RL对齐方法在联邦学习环境中存在内存限制和系统挑战，GRPO的无critic架构虽适合设备端训练，但在联邦设置中面临奖励定义异构、多目标优化不平衡和高训练成本等问题

Method: 提出FedMOA框架：1) 客户端使用基于超梯度下降的在线自适应权重机制，在主推理目标满足时优先考虑辅助目标；2) 服务器端采用任务和准确率感知的聚合策略，优先选择高质量更新

Result: 在数学推理和代码生成基准测试中，FedMOA始终优于联邦平均方法，准确率提升最高达2.2%，同时改善了全局性能、个性化能力和多目标平衡

Conclusion: FedMOA为联邦学习环境中的多目标对齐提供了有效解决方案，解决了异构奖励定义和优化不平衡问题，实现了更好的全局性能和个性化能力

Abstract: Group Relative Policy Optimization (GRPO) has recently emerged as an effective approach for improving the reasoning capabilities of large language models through online multi-objective reinforcement learning. While personalization on private data is increasingly vital, traditional Reinforcement Learning (RL) alignment is often memory-prohibitive for on-device federated learning due to the overhead of maintaining a separate critic network. GRPO's critic-free architecture enables feasible on-device training, yet transitioning to a federated setting introduces systemic challenges: heterogeneous reward definitions, imbalanced multi-objective optimization, and high training costs. We propose FedMOA, a federated GRPO framework for multi-objective alignment under heterogeneous rewards. FedMOA stabilizes local training through an online adaptive weighting mechanism via hypergradient descent, which prioritizes primary reasoning as auxiliary objectives saturate. On the server side, it utilizes a task- and accuracy-aware aggregation strategy to prioritize high-quality updates. Experiments on mathematical reasoning and code generation benchmarks demonstrate that FedMOA consistently outperforms federated averaging, achieving accuracy gains of up to 2.2% while improving global performance, personalization, and multi-objective balance.

</details>


### [810] [AREAL-DTA: Dynamic Tree Attention for Efficient Reinforcement Learning of Large Language Models](https://arxiv.org/abs/2602.00482)
*Jiarui Zhang,Yuchen Yang,Ran Yan,Zhiyu Mei,Liyuan Zhang,Daifeng Li,Wei Fu,Jiaxuan Gao,Shusheng Xu,Yi Wu,Binhang Yuan*

Main category: cs.LG

Relevance: 85.0

TL;DR: AREAL-DTA：一种基于深度优先搜索的RL训练框架，通过动态遍历rollout前缀树来高效利用前缀共享，减少重复计算，提升训练吞吐量


<details>
  <summary>Details</summary>
Motivation: 现有的RL框架在处理LLM后训练时，由于独立处理共享长前缀的rollout序列，导致大量重复计算和内存浪费，计算效率低下

Method: 采用深度优先搜索（DFS）执行策略，动态遍历rollout前缀树，每次只实例化单个根到叶路径；结合负载均衡的分布式批处理机制，在多GPU上动态构建和处理前缀树

Result: 在流行的RL后训练工作负载中，AREAL-DTA实现了高达8.31倍的τ²-bench训练吞吐量提升

Conclusion: AREAL-DTA通过高效利用RL训练中的前缀共享，显著提升了计算效率和训练吞吐量，解决了现有RL框架的重复计算问题

Abstract: Reinforcement learning (RL) based post-training for large language models (LLMs) is computationally expensive, as it generates many rollout sequences that could frequently share long token prefixes. Existing RL frameworks usually process these sequences independently, repeatedly recomputing identical prefixes during forward and backward passes during policy model training, leading to substantial inefficiencies in computation and memory usage. Although prefix sharing naturally induces a tree structure over rollouts, prior tree-attention-based solutions rely on fully materialized attention masks and scale poorly in RL settings. In this paper, we introduce AREAL-DTA to efficiently exploit prefix sharing in RL training. AREAL-DTA employs a depth-first-search (DFS)-based execution strategy that dynamically traverses the rollout prefix tree during both forward and backward computation, materializing only a single root-to-leaf path at a time. To further improve scalability, AREAL-DTA incorporates a load-balanced distributed batching mechanism that dynamically constructs and processes prefix trees across multiple GPUs. Across the popular RL post-training workload, AREAL-DTA achieves up to $8.31\times$ in $τ^2$-bench higher training throughput.

</details>


### [811] [AIRE-Prune: Asymptotic Impulse-Response Energy for State Pruning in State Space Models](https://arxiv.org/abs/2602.00534)
*Apurba Prasad Padhy,Fernando Camacho,Saibal Mukhopadhyay*

Main category: cs.LG

Relevance: 85.0

TL;DR: AIRE-Prune是一种针对状态空间模型的结构化后训练剪枝方法，通过最小化长期输出能量失真来减少每层状态维度，实现60.8%的平均剪枝率，仅损失0.29%的精度。


<details>
  <summary>Details</summary>
Motivation: 状态空间模型(SSMs)通常需要在大状态维度下平衡内存/计算成本与模型容量、搜索空间或稳定性。现有方法存在局限性，需要一种能够直接最小化长期输出能量失真的剪枝方法。

Method: 提出AIRE-Prune方法：为每个状态计算闭式渐近脉冲响应能量分数（无限时间范围内的总脉冲响应能量），通过层间归一化实现跨层全局比较和选择。将模态截断从单系统扩展到深度堆叠，使剪枝与渐近响应能量而非最坏情况增益对齐。

Result: 在多样序列基准测试中，AIRE-Prune揭示了SISO和MIMO SSMs中存在大量冗余，实现平均60.8%的剪枝率，平均精度仅下降0.29%（无需重新训练），同时显著降低计算成本。

Conclusion: AIRE-Prune提供了一种有效的SSMs剪枝方法，通过渐近脉冲响应能量评估状态重要性，实现了高压缩率下的精度保持，为状态空间模型的高效部署提供了实用工具。

Abstract: State space models (SSMs) often sacrifice capacity, search space, or stability to offset the memory and compute costs of large state dimensions. We introduce a structured post-training pruning method for SSMs -- AIRE-Prune (Asymptotic Impulse-Response Energy for State PRUN(E)) -- that reduces each layer's state dimension by directly minimizing long-run output-energy distortion. AIRE-Prune assigns every state a closed-form asymptotic impulse-response energy-based score, i.e., the total impulse-response energy it contributes over an infinite horizon (time), and normalizes these scores layer-wise to enable global cross-layer comparison and selection. This extends modal truncation from single systems to deep stacks and aligns pruning with asymptotic response energy rather than worst-case gain. Across diverse sequence benchmarks, AIRE-Prune reveals substantial redundancy in SISO and MIMO SSMs with average pruning of 60.8%, with average accuracy drop of 0.29% without retraining, while significantly lowering compute. Code: https://github.com/falcon-arrow/AIRE-Prune.

</details>


### [812] [Data Distribution as a Lever for Guiding Optimizers Toward Superior Generalization in LLMs](https://arxiv.org/abs/2602.00576)
*Tushaar Gangavarapu,Jiping Li,Christopher Vattheuer,Zhangyang Wang,Baharan Mirzasoleiman*

Main category: cs.LG

Relevance: 85.0

TL;DR: 论文提出通过修改训练数据分布来降低简单性偏置，从而提升LLM泛化性能。研究发现SAM优化器通过降低简单性偏置获得更好泛化，并证明通过上采样或增强后期学习样本可以达到类似效果，在多个LLM上验证了有效性。


<details>
  <summary>Details</summary>
Motivation: 研究如何通过修改训练数据分布来引导优化器找到具有更好泛化能力的解决方案。传统SAM优化器虽然泛化性能好但计算成本过高，不适合训练大型LLM，因此需要寻找更实用的替代方案。

Method: 1. 理论分析基于上下文线性回归模型和多头线性自注意力；2. 比较GD和SAM优化器的训练动态；3. 识别SAM降低简单性偏置的关键机制；4. 提出通过上采样或增强后期学习样本来修改训练数据分布；5. 在多个LLM上进行实验验证。

Result: 实验显示该方法显著提升多个LLM的数学推理能力：Phi2-2.7B、Llama3.2-1B、Gemma3-1B-PT和Qwen3-0.6B-Base，使用AdamW和Muon微调时相对准确率提升最高达18%。

Conclusion: 通过修改训练数据分布来降低简单性偏置是一种有效提升LLM泛化性能的实用方法，避免了SAM的高计算成本，为LLM训练提供了新的优化方向。

Abstract: Can modifying the training data distribution guide optimizers toward solutions with improved generalization when training large language models (LLMs)? In this work, we theoretically analyze an in-context linear regression model with multi-head linear self-attention, and compare the training dynamics of two gradient based optimizers, namely gradient descent (GD) and sharpness-aware minimization (SAM), the latter exhibiting superior generalization properties but is prohibitively expensive for training even medium-sized LLMs. We show, for the first time, that SAM induces a lower simplicity bias (SB)-the tendency of an optimizer to preferentially learn simpler features earlier in training-and identify this reduction as a key factor underlying its improved generalization performance. Motivated by this insight, we demonstrate that altering the training data distribution by upsampling or augmenting examples learned later in training similarly reduces SB and leads to improved generalization. Our extensive experiments show that our strategy improves the performance of multiple LLMs-including Phi2-2.7B , Llama3.2-1B, Gemma3-1B-PT, and Qwen3-0.6B-Base-achieving relative accuracy gains up to 18% when fine-tuned with AdamW and Muon on mathematical reasoning tasks.

</details>


### [813] [Sparsity-Aware Unlearning for Large Language Models](https://arxiv.org/abs/2602.00577)
*Yuze Wang,Yujia Tong,Ke Xu,Jingling Yuan,Jiawei Jiang,Chuang Hu*

Main category: cs.LG

Relevance: 85.0

TL;DR: 提出SAU方法解决稀疏化大语言模型中的隐私遗忘问题，通过梯度掩码和重要性感知重分配，在保持模型效用的同时有效移除敏感信息


<details>
  <summary>Details</summary>
Motivation: 大语言模型在训练中会记忆敏感信息，带来隐私风险。现有遗忘方法针对密集模型设计，忽略了模型稀疏化这一高效部署技术，导致在稀疏模型上遗忘效果显著下降

Method: 提出Sparsity-Aware Unlearning (SAU)方法：1) 梯度掩码将更新重定向到存活的权重，解耦遗忘与稀疏化目标；2) 重要性感知重分配补偿被剪枝的参数

Result: 大量实验表明，SAU在稀疏大语言模型上显著优于现有方法，能够有效实现遗忘，同时保持模型效用

Conclusion: SAU成功解决了稀疏大语言模型中的隐私遗忘挑战，为高效部署的LLMs提供了有效的隐私保护方案

Abstract: Large Language Models (LLMs) inevitably memorize sensitive information during training, posing significant privacy risks. Machine unlearning has emerged as a promising solution to selectively remove such information without full retraining. However, existing methods are designed for dense models and overlook model sparsification-an essential technique for efficient LLM deployment. We find that unlearning effectiveness degrades substantially on sparse models. Through empirical analysis, we reveal that this degradation occurs because existing unlearning methods require updating all parameters, yet sparsification prunes substantial weights to zero, fundamentally limiting the model's forgetting capacity. To address this challenge, we propose Sparsity-Aware Unlearning (SAU), which decouples unlearning from sparsification objectives through gradient masking that redirects updates to surviving weights, combined with importance-aware redistribution to compensate for pruned parameters. Extensive experiments demonstrate that SAU significantly outperforms existing methods on sparse LLMs, achieving effective forgetting while preserving model utility.

</details>


### [814] [Direct Preference Optimization with Rating Information: Practical Algorithms and Provable Gains](https://arxiv.org/abs/2602.00603)
*Luca Viano,Ruida Zhou,Yifan Sun,Mahdi Namazifar,Volkan Cevher,Shoham Sabach,Mohammad Ghavamzadeh*

Main category: cs.LG

Relevance: 85.0

TL;DR: 本文提出了一种改进的偏好优化算法，利用评分差距信息（而非仅二元偏好）来加速对齐过程，并在统计效率和鲁棒性方面优于传统DPO方法。


<details>
  <summary>Details</summary>
Motivation: 传统DPO算法仅使用二元偏好反馈，存在信息模糊性问题。评分差距信息（表明偏好程度差异）可以提供更丰富的信号，但如何有效利用这种信息并保持对不准确评分的鲁棒性是需要解决的问题。

Method: 提出新的算法框架，能够利用评分差距信息进行模型对齐。算法设计考虑了统计效率优化，同时通过理论分析和实验验证了其对不准确评分信息的鲁棒性。

Result: 新算法在统计收敛速率上优于DPO，在多种LLM和评估基准上表现优异。实验证明算法对评分差距信息的不准确性具有鲁棒性。

Conclusion: 利用评分差距信息可以显著提升偏好优化算法的性能，新算法在效率和鲁棒性方面都有优势，为LLM对齐提供了更有效的方法。

Abstract: The class of direct preference optimization (DPO) algorithms has emerged as a promising approach for solving the alignment problem in foundation models. These algorithms work with very limited feedback in the form of pairwise preferences and fine-tune models to align with these preferences without explicitly learning a reward model. While the form of feedback used by these algorithms makes the data collection process easy and relatively more accurate, its ambiguity in terms of the quality of responses could have negative implications. For example, it is not clear if a decrease (increase) in the likelihood of preferred (dispreferred) responses during the execution of these algorithms could be interpreted as a positive or negative phenomenon. In this paper, we study how to design algorithms that can leverage additional information in the form of rating gap, which informs the learner how much the chosen response is better than the rejected one. We present new algorithms that can achieve faster statistical rates than DPO in presence of accurate rating gap information. Moreover, we theoretically prove and empirically show that the performance of our algorithms is robust to inaccuracy in rating gaps. Finally, we demonstrate the solid performance of our methods in comparison to a number of DPO-style algorithms across a wide range of LLMs and evaluation benchmarks.

</details>


### [815] [From Associations to Activations: Comparing Behavioral and Hidden-State Semantic Geometry in LLMs](https://arxiv.org/abs/2602.00628)
*Louis Schiekiera,Max Zimmer,Christophe Roux,Sebastian Pokutta,Fritz Günther*

Main category: cs.LG

Relevance: 85.0

TL;DR: 通过心理语言学实验研究LLM隐藏状态几何结构能否从其行为中恢复，发现选择题行为比自由联想更能反映隐藏状态几何，行为相似性可以预测未见的隐藏状态相似性


<details>
  <summary>Details</summary>
Motivation: 研究是否可以通过LLM在心理语言学实验中的行为来恢复其隐藏状态的几何结构，探索行为测量是否能揭示内部语义几何信息

Method: 在8个指令调优的Transformer模型上运行两种实验范式（基于相似性的选择题和自由联想），使用5000词词汇表收集1750万+试验构建行为相似性矩阵，通过表征相似性分析比较行为几何与层间隐藏状态相似性

Result: 选择题行为与隐藏状态几何的对齐程度显著高于自由联想；在保留词回归中，行为相似性（特别是选择题）能够预测未见的隐藏状态相似性，超越词汇基线和跨模型共识

Conclusion: 仅基于行为的测量保留了关于内部语义几何的可恢复信息，行为任务能够揭示隐藏的认知状态，这对LLM解释性研究有重要意义

Abstract: We investigate the extent to which an LLM's hidden-state geometry can be recovered from its behavior in psycholinguistic experiments. Across eight instruction-tuned transformer models, we run two experimental paradigms -- similarity-based forced choice and free association -- over a shared 5,000-word vocabulary, collecting 17.5M+ trials to build behavior-based similarity matrices. Using representational similarity analysis, we compare behavioral geometries to layerwise hidden-state similarity and benchmark against FastText, BERT, and cross-model consensus. We find that forced-choice behavior aligns substantially more with hidden-state geometry than free association. In a held-out-words regression, behavioral similarity (especially forced choice) predicts unseen hidden-state similarities beyond lexical baselines and cross-model consensus, indicating that behavior-only measurements retain recoverable information about internal semantic geometry. Finally, we discuss implications for the ability of behavioral tasks to uncover hidden cognitive states.

</details>


### [816] [Provably Protecting Fine-Tuned LLMs from Training Data Extraction](https://arxiv.org/abs/2602.00688)
*Tom Segal,Asaf Shabtai,Yuval Elovici*

Main category: cs.LG

Relevance: 85.0

TL;DR: 提出SCP-Δr算法，通过基于相对概率的Near Access Freeness方法，在保护大语言模型微调隐私的同时最小化性能损失


<details>
  <summary>Details</summary>
Motivation: 微调大语言模型在敏感数据集上存在隐私风险，训练数据提取攻击可能暴露机密信息。现有防御方法要么缺乏形式化隐私保证，要么导致严重的性能下降。

Method: 观察到微调引起广泛的概率偏移，但只需保留一小部分有影响的token级偏差即可；其余偏移可以通过基础模型进行激进平滑。提出SCP-Δr算法，基于相对概率的Near Access Freeness方法，显式平滑低影响token。

Result: SCP-Δr相比现有NAF方法获得数量级更好的理论界限，在对抗训练数据提取攻击方面提供强大的实证保护，同时性能损失最小。

Conclusion: 通过选择性保留关键token偏差并平滑低影响token，可以在保护LLM微调隐私的同时保持模型实用性，SCP-Δr为此提供了有效的解决方案。

Abstract: Fine-tuning large language models (LLMs) on sensitive datasets raises privacy concerns, as training data extraction (TDE) attacks can expose highly confidential information. Existing defenses against such attacks either lack formal privacy guarantees or incur substantial utility degradation. We observe that fine-tuning induces widespread probability shifts, yet preserving only a small subset of influential token-level deviations is sufficient; the remaining shifts can be aggressively smoothed with minimal impact on utility. Motivated by this insight, we propose SCP-$Δ_r$, a Near Access Freeness (NAF)-based algorithm that operates on relative probabilities and explicitly smooths low-impact tokens using a base model. SCP-$Δ_r$ achieves orders-of-magnitude better theoretical bounds than existing NAF based methods and provides strong empirical protection against TDE attacks with minimal performance loss.

</details>


### [817] [Rethinking Hallucinations: Correctness, Consistency, and Prompt Multiplicity](https://arxiv.org/abs/2602.00723)
*Prakhar Ganesh,Reza Shokri,Golnoosh Farnadi*

Main category: cs.LG

Relevance: 85.0

TL;DR: 论文提出prompt multiplicity框架，用于量化LLM评估中的一致性，发现现有幻觉评估主要关注正确性而忽视一致性，导致对幻觉相关危害的严重误解。


<details>
  <summary>Details</summary>
Motivation: 现有LLM幻觉评估主要关注正确性，但忽视了评估一致性。这种局限性导致无法准确区分和解决幻觉带来的各种危害（如信任侵蚀、错误信息传播）。需要新的框架来量化评估一致性，以更全面理解幻觉相关危害。

Method: 提出prompt multiplicity框架，通过多个提示变体来量化LLM评估的一致性。分析现有基准（如Med-HALT）中的不一致性，研究一致性在幻觉检测和缓解中的作用，特别是检测技术（如RAG）对一致性的影响。

Result: 发现显著的多重性（超过50%的不一致性），表明幻觉相关危害被严重误解。检测技术主要检测一致性而非正确性，而缓解技术如RAG虽然有益但可能引入额外的不一致性。

Conclusion: 通过将prompt multiplicity整合到幻觉评估中，提供了改进的危害评估框架，揭示了当前检测和缓解策略的关键局限性，强调了一致性评估在全面理解LLM幻觉危害中的重要性。

Abstract: Large language models (LLMs) are known to "hallucinate" by generating false or misleading outputs. Hallucinations pose various harms, from erosion of trust to widespread misinformation. Existing hallucination evaluation, however, focuses only on correctness and often overlooks consistency, necessary to distinguish and address these harms. To bridge this gap, we introduce prompt multiplicity, a framework for quantifying consistency in LLM evaluations. Our analysis reveals significant multiplicity (over 50% inconsistency in benchmarks like Med-HALT), suggesting that hallucination-related harms have been severely misunderstood. Furthermore, we study the role of consistency in hallucination detection and mitigation. We find that: (a) detection techniques detect consistency, not correctness, and (b) mitigation techniques like RAG, while beneficial, can introduce additional inconsistencies. By integrating prompt multiplicity into hallucination evaluation, we provide an improved framework of potential harms and uncover critical limitations in current detection and mitigation strategies.

</details>


### [818] [BLOCK-EM: Preventing Emergent Misalignment by Blocking Causal Features](https://arxiv.org/abs/2602.00767)
*Muhammed Ustaomeroglu,Guannan Qu*

Main category: cs.LG

Relevance: 85.0

TL;DR: 该论文提出一种机制性方法防止微调中的涌现错位：识别控制错位行为的关键内部特征，并在微调中阻止这些特征的强化，可在不降低模型性能的情况下显著减少错位行为。


<details>
  <summary>Details</summary>
Motivation: 当语言模型在狭窄范围的监督目标上进行微调时，会出现涌现错位现象：模型学会了目标行为，但也发展出不良的域外行为。需要一种机制性方法来预防这种错位。

Method: 识别可靠控制错位行为的小型内部特征集，然后在微调过程中阻止模型强化这些特征。采用分离的选择/评估划分、多独立评判、多随机种子、质量指标和广泛消融实验验证。

Result: 在六个微调领域中，阻断固定特征集可实现高达95%的相对错位减少，且不降低模型质量或目标任务性能。但也发现长时间微调下错位会重新出现。

Conclusion: 针对内部机制的定向训练时约束可以有效减轻涌现错位，同时保持目标任务性能。这为LLM微调中的安全对齐提供了机制性解决方案。

Abstract: Emergent misalignment can arise when a language model is fine-tuned on a narrowly scoped supervised objective: the model learns the target behavior, yet also develops undesirable out-of-domain behaviors. We investigate a mechanistic approach to preventing emergent misalignment by identifying a small set of internal features that reliably control the misaligned behavior and then discouraging the model from strengthening these features during fine-tuning. Across six fine-tuning domains, blocking (i.e., constraining) a fixed set of features achieves up to 95\% relative reduction in emergent misalignment with no degradation in model quality or target-task performance. We strengthen validity with disjoint selection/evaluation splits, multiple independent judges, multiple random seeds for key settings, quality metrics, and extensive ablations demonstrating that the reduction in misalignment is specific to the identified mechanism. We also characterize a limiting regime in which misalignment re-emerges under prolonged fine-tuning, present evidence consistent with rerouting through alternative features or layers, and evaluate modifications that partially restore the misalignment-blocking effect. Overall, our results show that targeted training-time constraints on internal mechanisms can mitigate emergent misalignment without degrading target-task performance.

</details>


### [819] [Latent Shadows: The Gaussian-Discrete Duality in Masked Diffusion](https://arxiv.org/abs/2602.00792)
*Guinan Chen,Xunpeng Huang,Ying Sun,Shijin Wang,Yanyong Zhang,Chao Wang*

Main category: cs.LG

Relevance: 85.0

TL;DR: 该论文提出了掩码一致性蒸馏（MCD），通过建立掩码离散扩散与连续高斯过程之间的显式对偶关系，实现了确定性采样，相比随机蒸馏方法获得了16倍推理加速且不损失生成质量。


<details>
  <summary>Details</summary>
Motivation: 掩码离散扩散是高质量语言建模的主流方法，但其推理效率受限于缺乏确定性采样工具。现有方法要么性能不足，要么依赖复杂积分算子，而掩码域方法通常假设不存在确定性轨迹，只能依赖随机蒸馏。

Method: 建立了显式的掩码扩散对偶性，证明掩码过程是通过新颖的最大值索引保持机制从连续高斯过程投影而来。基于此提出了掩码一致性蒸馏（MCD）框架，利用对偶性解析构建确定性耦合轨迹，绕过了数值ODE求解器。

Result: 相比先前的随机蒸馏方法，MCD实现了16倍的推理加速，同时不损害生成质量。为掩码和连续扩散之间的连接提供了坚实的理论基础。

Conclusion: 该工作不仅建立了掩码扩散与连续扩散之间的理论联系，还释放了一致性蒸馏在高性能离散生成中的全部潜力，为高效语言模型推理提供了新方法。

Abstract: Masked discrete diffusion is a dominant paradigm for high-quality language modeling where tokens are iteratively corrupted to a mask state, yet its inference efficiency is bottlenecked by the lack of deterministic sampling tools. While diffusion duality enables deterministic distillation for uniform models, these approaches generally underperform masked models and rely on complex integral operators. Conversely, in the masked domain, prior methods typically assume the absence of deterministic trajectories, forcing a reliance on stochastic distillation. To bridge this gap, we establish explicit Masked Diffusion Duality, proving that the masked process arises as the projection of a continuous Gaussian process via a novel maximum-value index preservation mechanism. Furthermore, we introduce Masked Consistency Distillation (MCD), a principled framework that leverages this duality to analytically construct the deterministic coupled trajectories required for consistency distillation, bypassing numerical ODE solvers. This result strictly improves upon prior stochastic distillation methods, achieving a 16$\times$ inference speedup without compromising generation quality. Our findings not only provide a solid theoretical foundation connecting masked and continuous diffusion, but also unlock the full potential of consistency distillation for high-performance discrete generation. Our code is available at https://anonymous.4open.science/r/MCD-70FD.

</details>


### [820] [JTok: On Token Embedding as another Axis of Scaling Law via Joint Token Self-modulation](https://arxiv.org/abs/2602.00800)
*Yebin Yang,Huaijin Wu,Fu Guo,Lin Yao,Xiaohan Qin,Jingzhi Wang,Debing Zhang,Junchi Yan*

Main category: cs.LG

Relevance: 85.0

TL;DR: 提出token-indexed parameters作为新的扩展维度，通过Joint-Token (JTok)和Mixture of Joint-Token (JTok-M)方法，在Transformer层中添加从辅助嵌入表检索的调制向量，以可忽略的FLOPs开销实现模型容量与计算成本的解耦。


<details>
  <summary>Details</summary>
Motivation: 传统LLM扩展面临计算成本与性能的线性耦合问题，MoE虽然解耦了容量与计算，但引入了内存开销和硬件效率挑战。需要新的扩展维度来更高效地提升模型能力。

Method: 提出token-indexed parameters概念，引入JTok和JTok-M方法：在Transformer层中通过轻量级元素级操作，从辅助嵌入表中检索调制向量来调节主干网络，实现可忽略的FLOPs开销。

Result: 在650M到61B参数规模的密集和MoE主干上验证，显著降低验证损失并提升下游任务性能（MMLU +4.1，ARC +8.3，CEval +8.9）。isoFLOPs分析显示JTok-M将质量-计算Pareto前沿提升35%，且token-indexed parameters呈现可预测的幂律扩展行为。

Conclusion: token-indexed parameters为LLM扩展提供了新的高效维度，JTok和JTok-M方法在保持计算效率的同时显著提升模型性能，为大规模语言模型的高效扩展提供了新方向。

Abstract: LLMs have traditionally scaled along dense dimensions, where performance is coupled with near-linear increases in computational cost. While MoE decouples capacity from compute, it introduces large memory overhead and hardware efficiency challenges. To overcome these, we propose token-indexed parameters as a novel, orthogonal scaling axis that decouple model capacity from FLOPs. Specifically, we introduce Joint-Token (JTok) and Mixture of Joint-Token (JTok-M), which augment Transformer layers with modulation vectors retrieved from auxiliary embedding tables. These vectors modulate the backbone via lightweight, element-wise operations, incurring negligible FLOPs overhead. Extensive experiments on both dense and MoE backbones, spanning from 650M (190M + 460M embedding) to 61B (17B + 44B embedding) total parameters, demonstrate that our approach consistently reduces validation loss and significantly improves downstream task performance (e.g., +4.1 on MMLU, +8.3 on ARC, +8.9 on CEval). Rigorous isoFLOPs analysis further confirms that JTok-M fundamentally shifts the quality-compute Pareto frontier, achieving comparable model quality with 35% less compute relative to vanilla MoE architectures, and we validate that token-indexed parameters exhibit a predictable power-law scaling behavior. Moreover, our efficient implementation ensures that the overhead introduced by JTok and JTok-M remains marginal.

</details>


### [821] [Dynamic Expert Sharing: Decoupling Memory from Parallelism in Mixture-of-Experts Diffusion LLMs](https://arxiv.org/abs/2602.00879)
*Hao Mark Chen,Zhiwen Mo,Royson Lee,Qianzhou Wang,Da Li,Shell Xu Hu,Wayne Luk,Timothy Hospedales,Hongxiang Fan*

Main category: cs.LG

Relevance: 85.0

TL;DR: 提出Dynamic Expert Sharing (DES)方法解决MoE扩散语言模型中的专家爆炸问题，通过序列级核心集选择减少专家激活数量，提高推理效率


<details>
  <summary>Details</summary>
Motivation: 扩散语言模型(dLLMs)与MoE架构结合时存在专家爆炸问题：并行生成的token数量增加时，激活的专家数量线性增长，导致内存流量剧增，使推理进入内存瓶颈，抵消了MoE和并行解码的效率优势

Method: 提出Dynamic Expert Sharing (DES)技术，将MoE优化从token级剪枝和传统专家跳过方法转向序列级核心集选择。包含两种策略：1) DES-Seq：在序列级别自适应最优分配；2) DES-Vote：基于聚合路由器权重的显著性感知投票机制，让token集体选举核心集

Result: 在MoE dLLMs上的实验表明，DES减少了超过55%的唯一专家激活，延迟降低高达38%，同时保持99%的原始准确率，有效解耦了内存开销与并行度

Conclusion: DES方法成功解决了MoE扩散语言模型中的专家爆炸问题，通过序列级专家共享显著提高了推理效率，为高效并行解码提供了有效解决方案

Abstract: Among parallel decoding paradigms, diffusion large language models (dLLMs) have emerged as a promising candidate that balances generation quality and throughput. However, their integration with Mixture-of-Experts (MoE) architectures is constrained by an expert explosion: as the number of tokens generated in parallel increases, the number of distinct experts activated grows nearly linearly. This results in substantial memory traffic that pushes inference into a memory-bound regime, negating the efficiency gains of both MoE and parallel decoding. To address this challenge, we propose Dynamic Expert Sharing (DES), a novel technique that shifts MoE optimization from token-centric pruning and conventional expert skipping methods to sequence-level coreset selection. To maximize expert reuse, DES identifies a compact, high-utility set of experts to satisfy the requirements of an entire parallel decoding block. We introduce two innovative selection strategies: (1) Intra-Sequence Sharing (DES-Seq), which adapts optimal allocation to the sequence level, and (2) Saliency-Aware Voting (DES-Vote), a novel mechanism that allows tokens to collectively elect a coreset based on aggregated router weights. Extensive experiments on MoE dLLMs demonstrate that DES reduces unique expert activations by over 55% and latency by up to 38%, while retaining 99% of vanilla accuracy, effectively decoupling memory overhead from the degree of parallelism.

</details>


### [822] [Hallucination is a Consequence of Space-Optimality: A Rate-Distortion Theorem for Membership Testing](https://arxiv.org/abs/2602.00906)
*Anxin Guo,Jingwei Li*

Main category: cs.LG

Relevance: 85.0

TL;DR: 该论文从信息论角度形式化LLM对"随机事实"的记忆问题，证明在有限容量下，最优策略不是放弃记忆而是对某些非事实赋予高置信度，这自然导致幻觉产生。


<details>
  <summary>Details</summary>
Motivation: 大语言模型经常对缺乏可推断模式的"随机事实"产生高置信度的幻觉。作者希望从信息论角度理解这种现象的本质，将事实记忆问题形式化为成员测试问题，探究幻觉产生的根本原因。

Method: 将LLM记忆问题形式化为成员测试问题，统一Bloom滤波器的离散误差度量与LLM的连续对数损失。在事实稀疏的假设下，建立速率-失真定理，通过KL散度刻画最优记忆效率。在合成数据上进行实证验证。

Result: 理论分析表明：即使在最优训练、完美数据和简化"封闭世界"设置下，有限容量下的信息论最优策略不是放弃或遗忘，而是对某些非事实赋予高置信度，这自然导致幻觉。实证验证支持这一理论。

Conclusion: 幻觉是损失压缩的自然结果，而非训练缺陷或数据质量问题。即使模型训练完美，在有限容量下，对某些非事实赋予高置信度是最优策略，这为理解LLM幻觉提供了新的理论框架。

Abstract: Large language models often hallucinate with high confidence on "random facts" that lack inferable patterns. We formalize the memorization of such facts as a membership testing problem, unifying the discrete error metrics of Bloom filters with the continuous log-loss of LLMs. By analyzing this problem in the regime where facts are sparse in the universe of plausible claims, we establish a rate-distortion theorem: the optimal memory efficiency is characterized by the minimum KL divergence between score distributions on facts and non-facts. This theoretical framework provides a distinctive explanation for hallucination: even with optimal training, perfect data, and a simplified "closed world" setting, the information-theoretically optimal strategy under limited capacity is not to abstain or forget, but to assign high confidence to some non-facts, resulting in hallucination. We validate this theory empirically on synthetic data, showing that hallucinations persist as a natural consequence of lossy compression.

</details>


### [823] [Beyond What Seems Necessary: Hidden Gains from Scaling Training-Time Reasoning Length under Outcome Supervision](https://arxiv.org/abs/2602.00927)
*Yihao Xue,Allan Zhang,Jianhao Huang,Amit Sahai,Baharan Mirzasoleiman*

Main category: cs.LG

Relevance: 85.0

TL;DR: 研究发现：在仅基于结果的监督下，当训练时推理长度增加时，即使分布内性能已饱和，分布外性能仍可继续提升，表明鲁棒性需要比ID验证更大的预算。


<details>
  <summary>Details</summary>
Motivation: 训练LLMs进行更长推理已成为构建能解决复杂问题的先进模型的关键。现有方法通过RL微调或架构循环扩展推理长度，但需要理解推理长度如何影响模型泛化能力。

Method: 通过理论分析和实验验证：1）理论解释两种机制：自迭代增强假设类归纳偏置，正则化减少对捷径解的依赖；2）实验验证：在循环Transformer合成任务中增加循环次数，在数学推理RL微调中增加token预算。

Result: 实验证实理论预测：随着训练时推理长度增加，OOD性能持续改善而ID性能已饱和。循环Transformer合成任务和LLM数学推理RL微调均显示此现象。

Conclusion: 推理长度是重要的扩展维度，仅基于ID验证可能低估模型鲁棒性需求。自迭代机制可改善OOD泛化，为设计更鲁棒的LLM训练提供新视角。

Abstract: Training LLMs to think and reason for longer has become a key ingredient in building state-of-the-art models that can solve complex problems previously out of reach. Recent efforts pursue this in different ways, such as RL fine-tuning to elicit long CoT or scaling latent reasoning through architectural recurrence. This makes reasoning length an important scaling knob. In this work, we identify a novel phenomenon (both theoretically and experimentally): under outcome-only supervision, out-of-distribution (OOD) performance can continue improving as training-time reasoning length (e.g., the token budget in RL, or the loop count in looped Transformers) increases, even after in-distribution (ID) performance has saturated. This suggests that robustness may require a larger budget than ID validation alone would indicate. We provide theoretical explanations via two mechanisms: (i) self-iteration can induce a stronger inductive bias in the hypothesis class, reshaping ID-optimal solutions in ways that improve OOD generalization; and (ii) when shortcut solutions that work for ID samples but not for OOD samples persist in the hypothesis class, regularization can reduce the learned solution's reliance on these shortcuts as the number of self-iterations increases. We complement the theory with empirical evidence from two realizations of scaling training-time reasoning length: increasing the number of loops in looped Transformers on a synthetic task, and increasing token budgets during RL fine-tuning of LLMs on mathematical reasoning.

</details>


### [824] [Continuous-Utility Direct Preference Optimization](https://arxiv.org/abs/2602.00931)
*Muhammad Ahmed Mohsin,Muhammad Umer,Ahsan Bilal,Zihao He,Muhammad Usman Rafique,Asad Aali,Muhammad Ali Jamshed,John M. Cioffi,Emily Fox*

Main category: cs.LG

Relevance: 85.0

TL;DR: CU-DPO：通过连续效用直接偏好优化框架，用连续分数替代二元标签来捕捉细粒度推理质量，提升大语言模型推理能力


<details>
  <summary>Details</summary>
Motivation: 当前大语言模型推理通常被视为单一能力，依赖二元偏好监督无法捕捉部分进展或细粒度推理质量。需要一种能更好评估推理过程的方法。

Method: 提出CU-DPO框架：1）用连续分数替代二元标签；2）两阶段训练流程：策略选择（通过最佳vs所有比较优化模型选择最佳策略）和执行细化（使用边际分层对训练模型正确执行选定策略）。理论证明K个策略的学习样本复杂度比二元偏好提升Θ(K log K)。

Result: 在数学推理基准测试中，CU-DPO将策略选择准确率从35-46%提升到68-78%，在分布内数据集上获得高达6.6分的下游推理增益，并能有效迁移到分布外任务。

Conclusion: CU-DPO通过连续效用信号和结构化策略学习，显著提升大语言模型的推理能力，为细粒度推理对齐提供了有效框架。

Abstract: Large language model reasoning is often treated as a monolithic capability, relying on binary preference supervision that fails to capture partial progress or fine-grained reasoning quality. We introduce Continuous Utility Direct Preference Optimization (CU-DPO), a framework that aligns models to a portfolio of prompt-based cognitive strategies by replacing binary labels with continuous scores that capture fine-grained reasoning quality. We prove that learning with K strategies yields a Theta(K log K) improvement in sample complexity over binary preferences, and that DPO converges to the entropy-regularized utility-maximizing policy. To exploit this signal, we propose a two-stage training pipeline: (i) strategy selection, which optimizes the model to choose the best strategy for a given problem via best-vs-all comparisons, and (ii) execution refinement, which trains the model to correctly execute the selected strategy using margin-stratified pairs. On mathematical reasoning benchmarks, CU-DPO improves strategy selection accuracy from 35-46 percent to 68-78 percent across seven base models, yielding consistent downstream reasoning gains of up to 6.6 points on in-distribution datasets with effective transfer to out-of-distribution tasks.

</details>


### [825] [SALAAD: Sparse And Low-Rank Adaptation via ADMM](https://arxiv.org/abs/2602.00942)
*Hao Ma,Melis Ilayda Bal,Liang Zhang,Bingcong Li,Niao He,Melanie Zeilinger,Michael Muehlebach*

Main category: cs.LG

Relevance: 85.0

TL;DR: SALAAD是一个即插即用的框架，通过增强拉格朗日框架和自适应控制器，在训练中诱导稀疏和低秩结构，实现模型容量的灵活控制，减少部署内存消耗。


<details>
  <summary>Details</summary>
Motivation: 现代大语言模型在计算和内存受限环境下部署，需要灵活控制模型容量。现有方法通常依赖启发式设计，忽略了层和矩阵的异质性，或需要模型特定的架构修改。

Method: 提出SALAAD框架，在增强拉格朗日框架下制定结构化权重学习，引入自适应控制器动态平衡训练损失和结构约束，在训练中诱导稀疏和低秩结构。

Result: 实验表明SALAAD显著减少部署时的内存消耗，性能与专门设计的方法相当。单次训练产生连续的模型容量谱，无需重新训练即可在不同内存预算下平滑弹性部署。

Conclusion: SALAAD提供了一种通用、稳定的方法，在保持标准训练动态的同时，实现对模型容量的显式控制，为计算和内存受限环境下的LLM部署提供了灵活解决方案。

Abstract: Modern large language models are increasingly deployed under compute and memory constraints, making flexible control of model capacity a central challenge. While sparse and low-rank structures naturally trade off capacity and performance, existing approaches often rely on heuristic designs that ignore layer and matrix heterogeneity or require model-specific architectural modifications. We propose SALAAD, a plug-and-play framework applicable to different model architectures that induces sparse and low-rank structures during training. By formulating structured weight learning under an augmented Lagrangian framework and introducing an adaptive controller that dynamically balances the training loss and structural constraints, SALAAD preserves the stability of standard training dynamics while enabling explicit control over the evolution of effective model capacity during training. Experiments across model scales show that SALAAD substantially reduces memory consumption during deployment while achieving performance comparable to ad-hoc methods. Moreover, a single training run yields a continuous spectrum of model capacities, enabling smooth and elastic deployment across diverse memory budgets without the need for retraining.

</details>


### [826] [Probing the Knowledge Boundary: An Interactive Agentic Framework for Deep Knowledge Extraction](https://arxiv.org/abs/2602.00959)
*Yuheng Yang,Siqi Zhu,Tao Feng,Ge Liu,Jiaxuan You*

Main category: cs.LG

Relevance: 85.0

TL;DR: 提出交互式智能体框架，系统提取和量化LLM知识边界，发现递归分类法最有效，存在知识缩放定律，并识别Pass@1与Pass@k权衡


<details>
  <summary>Details</summary>
Motivation: LLMs被视为压缩知识库，但现有基准大多是静态的，缺乏对系统知识探测的支持。需要了解LLMs真正包含什么知识以及知识边界如何延伸。

Method: 提出交互式智能体框架，包含四种自适应探索策略在不同粒度上探测知识。采用三阶段知识处理流程：向量过滤去除重复、LLM裁决解决语义重叠、领域相关性审计保留有效知识单元。

Result: 递归分类法是最有效的探索策略；发现清晰的知识缩放定律（更大模型提取更多知识）；识别Pass@1与Pass@k权衡（领域专用模型初始准确率高但退化快，通用模型性能稳定）；训练数据组成差异导致不同模型家族具有可测量的知识特征。

Conclusion: 提出的交互式框架能系统提取和量化LLM知识，揭示了有效的探索策略、缩放定律、性能权衡和训练数据对知识特征的影响，为LLM知识分析提供了新工具。

Abstract: Large Language Models (LLMs) can be seen as compressed knowledge bases, but it remains unclear what knowledge they truly contain and how far their knowledge boundaries extend. Existing benchmarks are mostly static and provide limited support for systematic knowledge probing. In this paper, we propose an interactive agentic framework to systematically extract and quantify the knowledge of LLMs. Our method includes four adaptive exploration policies to probe knowledge at different granularities. To ensure the quality of extracted knowledge, we introduce a three-stage knowledge processing pipeline that combines vector-based filtering to remove exact duplicates, LLM-based adjudication to resolve ambiguous semantic overlaps, and domain-relevance auditing to retain valid knowledge units. Through extensive experiments, we find that recursive taxonomy is the most effective exploration strategy. We also observe a clear knowledge scaling law, where larger models consistently extract more knowledge. In addition, we identify a Pass@1-versus-Pass@k trade-off: domain-specialized models achieve higher initial accuracy but degrade rapidly, while general-purpose models maintain stable performance during extended extraction. Finally, our results show that differences in training data composition lead to distinct and measurable knowledge profiles across model families.

</details>


### [827] [On the Spectral Flattening of Quantized Embeddings](https://arxiv.org/abs/2602.00969)
*Junlin Huang,Wenyi Fang,Zhenheng Tang,Yuxin Wang,Xueze Kang,Yang Zheng,Bo Li,Xiaowen Chu*

Main category: cs.LG

Relevance: 85.0

TL;DR: 该论文揭示了LLM在超低精度训练中的不稳定性根源：均匀量化引入的噪声会截断嵌入矩阵的幂律谱尾，导致谱平坦化和表示崩溃，证明了谱保真度是稳定低比特优化的必要条件。


<details>
  <summary>Details</summary>
Motivation: 研究动机是解决大型语言模型在超低精度训练中的不稳定性问题。作者发现这种不稳定性源于离散量化约束与语言数据固有的重尾谱特性之间的冲突，需要从理论上理解量化如何破坏LLM的语义编码能力。

Method: 方法包括：1）形式化Zipfian统计与随机矩阵理论之间的联系；2）证明嵌入矩阵奇异值谱的幂律衰减是语义编码的基本要求；3）推导理论界限，表明均匀量化引入的噪声会不成比例地截断谱尾；4）在GPT-2和TinyLlama等架构上进行实证验证。

Result: 研究结果表明：1）均匀量化导致谱平坦化和稳定秩的可证明增加；2）这种几何退化会引发表示崩溃；3）实证验证证实了理论预测；4）量化了LLM的谱敏感性，并建立了谱保真度作为稳定低比特优化的必要条件。

Conclusion: 结论是：语言数据的幂律谱特性与量化约束之间的冲突是LLM低精度训练不稳定性的根本原因。谱保真度是保持语义编码能力的关键，这为设计更稳定的低比特优化方法提供了理论基础。

Abstract: Training Large Language Models (LLMs) at ultra-low precision is critically impeded by instability rooted in the conflict between discrete quantization constraints and the intrinsic heavy-tailed spectral nature of linguistic data. By formalizing the connection between Zipfian statistics and random matrix theory, we prove that the power-law decay in the singular value spectra of embeddings is a fundamental requisite for semantic encoding. We derive theoretical bounds showing that uniform quantization introduces a noise floor that disproportionately truncates this spectral tail, which induces spectral flattening and a strictly provable increase in the stable rank of representations. Empirical validation across diverse architectures including GPT-2 and TinyLlama corroborates that this geometric degradation precipitates representational collapse. This work not only quantifies the spectral sensitivity of LLMs but also establishes spectral fidelity as a necessary condition for stable low-bit optimization.

</details>


### [828] [ESSAM: A Novel Competitive Evolution Strategies Approach to Reinforcement Learning for Memory Efficient LLMs Fine-Tuning](https://arxiv.org/abs/2602.01003)
*Zhishen Sun,Sizhe Dang,Guang Dai,Haishan Ye*

Main category: cs.LG

Relevance: 85.0

TL;DR: ESSAM：结合进化策略和锐度感知最大化的全参数微调框架，用于数学推理任务，显著降低GPU内存使用（相比PPO减少18倍，相比GRPO减少10倍），性能与RL方法相当。


<details>
  <summary>Details</summary>
Motivation: 强化学习在提升LLM数学推理能力方面效果显著，但GPU内存使用过高，限制了其在资源受限环境中的应用。需要开发内存效率更高的训练方法。

Method: 提出ESSAM框架，将进化策略（ES）的参数空间零阶搜索与锐度感知最大化（SAM）紧密结合，实现全参数微调。在GSM8K数学推理任务上进行实验验证。

Result: ESSAM在GSM8K任务上平均准确率达到78.27%，性能与RL方法相当：超越PPO（77.72%），与GRPO（78.34%）相当，在某些模型上甚至超越。GPU内存使用大幅降低，相比PPO减少18倍，相比GRPO减少10倍。

Conclusion: ESSAM提供了一种内存高效的替代方案，能够在资源受限环境下实现与强化学习相当的性能，为LLM数学推理训练提供了新的技术路径。

Abstract: Reinforcement learning (RL) has become a key training step for improving mathematical reasoning in large language models (LLMs), but it often has high GPU memory usage, which makes it hard to use in settings with limited resources. To reduce these issues, we propose Evolution Strategies with Sharpness-Aware Maximization (ESSAM), a full parameter fine-tuning framework that tightly combines the zero-order search in parameter space from Evolution Strategies (ES) with the Sharpness-Aware Maximization (SAM) to improve generalization. We conduct fine-tuning experiments on the mainstream mathematica reasoning task GSM8K. The results show that ESSAM achieves an average accuracy of 78.27\% across all models and its overall performance is comparable to RL methods. It surpasses classic RL algorithm PPO with an accuracy of 77.72\% and is comparable to GRPO with an accuracy of 78.34\%, and even surpassing them on some models. In terms of GPU memory usage, ESSAM reduces the average GPU memory usage by $18\times$ compared to PPO and by $10\times$ compared to GRPO, achieving an extremely low GPU memory usage.

</details>


### [829] [How Does Unfaithful Reasoning Emerge from Autoregressive Training? A Study of Synthetic Experiments](https://arxiv.org/abs/2602.01017)
*Fuxin Wang,Amr Alazali,Yiqiao Zhong*

Main category: cs.LG

Relevance: 85.0

TL;DR: 该论文通过可控合成实验研究思维链推理的忠实性问题，发现模型仅在训练噪声低于临界阈值时才能学习忠实推理，噪声较高时会出现不忠实的跳步推理，并观察到训练过程中出现隐式自我验证现象。


<details>
  <summary>Details</summary>
Motivation: 当前大语言模型生成的思维链推理往往不忠实：中间步骤可能存在逻辑不一致或未能反映导致最终答案的因果关系。尽管有大量实证观察，但对思维链推理的基本理解仍然缺乏——什么是忠实的思维链推理，以及不忠实性如何从自回归训练中产生。

Method: 使用可控合成实验，在噪声数据上训练小型Transformer模型来解决模块化算术表达式（称为算术表达式推理任务），通过分析不同噪声水平下的训练动态来研究思维链推理的忠实性。

Result: 发现模型仅在训练噪声低于临界阈值时才能学习忠实推理（遵循底层算术规则的因果推理），噪声较高时训练动态表现出从忠实逐步推理到不忠实跳步推理的转变，中间出现预测熵短暂增加的混合模式。机制分析显示模型学会通过解决不一致推理步骤来编码内部不确定性。

Conclusion: 思维链推理的忠实性受训练噪声阈值影响，模型在自回归训练中能够发展出隐式自我验证能力，这为理解大语言模型中思维链推理的忠实性提供了理论基础。

Abstract: Chain-of-thought (CoT) reasoning generated by large language models (LLMs) is often unfaithful: intermediate steps can be logically inconsistent or fail to reflect the causal relationship leading to the final answer. Despite extensive empirical observations, a fundamental understanding of CoT is lacking--what constitutes faithful CoT reasoning, and how unfaithfulness emerges from autoregressive training. We study these questions using well-controlled synthetic experiments, training small transformers on noisy data to solve modular arithmetic expressions step by step, a task we term Arithmetic Expression Reasoning. We find that models can learn faithful reasoning that causally follows the underlying arithmetic rules, but only when the training noise is below a critical threshold, a phenomenon attributable to simplicity bias. At higher noise levels, training dynamics exhibit a transition from faithful stepwise reasoning to unfaithful skip-step reasoning via an intermediate mixed mode characterized by a transient increase in prediction entropy. Mechanistic analysis reveals that models learn to encode internal uncertainty by resolving inconsistent reasoning steps, which suggests the emergence of implicit self-verification from autoregressive training.

</details>


### [830] [Toward Universal and Transferable Jailbreak Attacks on Vision-Language Models](https://arxiv.org/abs/2602.01025)
*Kaiyuan Cui,Yige Li,Yutao Wu,Xingjun Ma,Sarah Erfani,Christopher Leckie,Hanxun Huang*

Main category: cs.LG

Relevance: 85.0

TL;DR: UltraBreak提出了一种通用且可迁移的视觉语言模型越狱框架，通过在视觉空间进行变换和正则化约束对抗模式，同时在文本嵌入空间使用语义引导的目标，实现了跨模型和攻击目标的强迁移性。


<details>
  <summary>Details</summary>
Motivation: 现有基于梯度的越狱方法迁移性差，对抗模式容易过拟合到单个白盒代理模型，无法泛化到黑盒模型。视觉语言模型的多模态集成扩大了攻击面，需要开发能够跨模型和攻击目标通用的越狱方法。

Method: UltraBreak框架结合视觉级正则化和语义引导的文本监督：1) 在视觉空间通过变换和正则化约束对抗模式；2) 在目标LLM的文本嵌入空间定义损失函数；3) 使用语义引导的目标放松文本约束，发现通用的对抗模式。

Result: 实验表明UltraBreak在跨模型和攻击目标上持续优于现有越狱方法。分析显示通过语义目标平滑损失景观对于实现通用和可迁移的越狱至关重要，揭示了先前方法迁移失败的原因。

Conclusion: UltraBreak通过视觉空间约束和语义文本监督的结合，有效缓解了代理模型过拟合问题，实现了跨模型和攻击目标的强迁移性，为视觉语言模型的安全评估提供了有效工具。

Abstract: Vision-language models (VLMs) extend large language models (LLMs) with vision encoders, enabling text generation conditioned on both images and text. However, this multimodal integration expands the attack surface by exposing the model to image-based jailbreaks crafted to induce harmful responses. Existing gradient-based jailbreak methods transfer poorly, as adversarial patterns overfit to a single white-box surrogate and fail to generalise to black-box models. In this work, we propose Universal and transferable jailbreak (UltraBreak), a framework that constrains adversarial patterns through transformations and regularisation in the vision space, while relaxing textual targets through semantic-based objectives. By defining its loss in the textual embedding space of the target LLM, UltraBreak discovers universal adversarial patterns that generalise across diverse jailbreak objectives. This combination of vision-level regularisation and semantically guided textual supervision mitigates surrogate overfitting and enables strong transferability across both models and attack targets. Extensive experiments show that UltraBreak consistently outperforms prior jailbreak methods. Further analysis reveals why earlier approaches fail to transfer, highlighting that smoothing the loss landscape via semantic objectives is crucial for enabling universal and transferable jailbreaks. The code is publicly available in our \href{https://github.com/kaiyuanCui/UltraBreak}{GitHub repository}.

</details>


### [831] [SFMP: Fine-Grained, Hardware-Friendly and Search-Free Mixed-Precision Quantization for Large Language Models](https://arxiv.org/abs/2602.01027)
*Xin Nie,Haicheng Zhang,Liang Dong,Beining Feng,Jinhong Weng,Guiling Sun*

Main category: cs.LG

Relevance: 85.0

TL;DR: SFMP提出了一种无需搜索、硬件友好的混合精度量化框架，通过分数位宽、块级混合精度、行列权重重排和统一GEMM核等创新技术，在相同内存约束下优于现有层级混合精度方法。


<details>
  <summary>Details</summary>
Motivation: 现有混合精度量化方法存在两个主要问题：要么依赖昂贵的离散优化来确定精度分配，要么由于不规则内存布局导致硬件效率低下。需要一种既能高效压缩大语言模型，又能在硬件上高效运行的混合精度量化方案。

Method: SFMP框架基于四个创新思想：1) 分数位宽：将权重矩阵的整数位宽扩展到分数值，将离散精度分配转化为连续问题；2) 块级混合精度：在权重矩阵内实现细粒度精度，同时保持硬件友好性；3) 行列权重重排：通过行列重排聚合重要权重，推理时仅产生小的激活重排开销；4) 统一GEMM核：支持任意平均位宽的混合精度GEMM运算。

Result: 大量实验表明，在相同内存约束下，SFMP优于最先进的层级混合精度方法，同时显著降低了量化成本并提高了推理效率。

Conclusion: SFMP为大型语言模型提供了一种高效、硬件友好的混合精度量化解决方案，解决了现有方法在优化成本和硬件效率方面的局限性。

Abstract: Mixed-precision quantization is a promising approach for compressing large language models under tight memory budgets. However, existing mixed-precision methods typically suffer from one of two limitations: they either rely on expensive discrete optimization to determine precision allocation, or introduce hardware inefficiencies due to irregular memory layouts. We propose SFMP, a search-free and hardware-friendly mixed-precision quantization framework for large language models. The framework is built upon four novel ideas: Fractional bit-width, which extends integer bit-width for weight matrix to fractional value and transforms discrete precision allocation as a continuous problem; 2)Block-wise mixed-precision, enabling fine-grained precision within weight matrices while remaining hardware-friendly; 3)Row-column weight reordering, which aggregates salient weights via row and column reordering, incurring only a small activation reordering overhead during inference; 4)Unified GEMM kernel, which supports mixed-precision GEMM at arbitrary average bit-width. Extensive experiments demonstrate that SFMP outperforms state-of-the-art layer-wise mixed-precision methods under the same memory constraints, while significantly reducing quantization cost and improving inference efficiency. Code is available at https://github.com/Nkniexin/SFMP

</details>


### [832] [LRAgent: Efficient KV Cache Sharing for Multi-LoRA LLM Agents](https://arxiv.org/abs/2602.01053)
*Hyesung Jeon,Hyeongju Ha,Jae-Joon Kim*

Main category: cs.LG

Relevance: 85.0

TL;DR: LRAgent：针对多LoRA代理系统的KV缓存共享框架，通过分解缓存为共享基础组件和适配器依赖组件，显著减少内存和计算开销，同时保持准确性。


<details>
  <summary>Details</summary>
Motivation: 多LLM代理系统中，每个代理虽然共享预训练骨干网络，但各自独立构建和存储KV缓存，导致大量内存和计算开销。现有KV缓存共享方法未能有效处理多LoRA设置。

Method: 提出LRAgent框架，将KV缓存分解为：1）来自预训练权重的共享基础组件；2）来自LoRA权重的适配器依赖组件。引入Flash-LoRA-Attention内核，避免将低秩缓存物化为完整维度，实现运行时高效重构适配器贡献。

Result: 在代理问答基准测试中，LRAgent实现了接近完全共享缓存的吞吐量和首令牌延迟，同时保持了接近非共享缓存基线的准确性。

Conclusion: LRAgent为多LoRA代理系统提供了高效的KV缓存共享解决方案，显著减少内存和计算开销，同时保持模型性能，适用于大规模LLM部署场景。

Abstract: Role specialization in multi-LLM agent systems is often realized via multi-LoRA, where agents share a pretrained backbone and differ only through lightweight adapters. Despite sharing base model weights, each agent independently builds and stores its own KV cache for the same long, tool-augmented trajectories, incurring substantial memory and compute overhead. Existing KV cache sharing methods largely overlook this multi-LoRA setting. We observe that, across agents, cache differences are dominated by adapter outputs, while activations from the shared pretrained backbone remain highly similar. Based on this observation, we propose LRAgent, a KV cache sharing framework for multi-LoRA agents that decomposes the cache into a shared base component from the pretrained weights and an adapter-dependent component from LoRA weights. LRAgent reduces memory overhead by sharing the base component and storing the adapter component in its inherent low-rank form, and further reduces compute overhead, enabled by shared-$A$ multi-LoRA architectures, by also sharing the low-rank cache and avoiding redundant computations for contexts already processed by other agents. To efficiently reconstruct adapter contributions at runtime, we introduce Flash-LoRA-Attention, a kernel that reorders attention computation to avoid materializing the low-rank cache to full dimension. LRAgent achieves throughput and time-to-first-token latency close to fully shared caching, while preserving accuracy near the non-shared caching baseline across agentic question-answering benchmarks.

</details>


### [833] [Good SFT Optimizes for SFT, Better SFT Prepares for Reinforcement Learning](https://arxiv.org/abs/2602.01058)
*Dylan Zhang,Yufeng Xu,Haojin Wang,Qingzhi Chen,Hao Peng*

Main category: cs.LG

Relevance: 85.0

TL;DR: PEAR提出一种SFT阶段的重加权方法，通过重要性采样修正SFT与RL阶段的分布不匹配问题，提升后续RL训练效果


<details>
  <summary>Details</summary>
Motivation: 当前LLM后训练流程中，离线SFT阶段通常独立优化，但研究发现更强的SFT检查点在后续RL训练中可能表现更差，原因是SFT数据分布与RL策略分布存在不匹配

Method: PEAR使用重要性采样对SFT损失进行重加权，提供token级、block级和序列级三种变体，可增强标准SFT目标，收集离线数据概率后训练开销很小

Result: 在可验证推理游戏和数学推理任务上，PEAR相比标准SFT显著提升后RL性能，在AIME2025上获得高达14.6%的pass@8增益

Conclusion: PEAR通过考虑下游RL来设计和评估SFT，是实现更整体化LLM后训练的有效步骤，而非孤立优化SFT

Abstract: Post-training of reasoning LLMs is a holistic process that typically consists of an offline SFT stage followed by an online reinforcement learning (RL) stage. However, SFT is often optimized in isolation to maximize SFT performance alone.
  We show that, after identical RL training, models initialized from stronger SFT checkpoints can significantly underperform those initialized from weaker ones. We attribute this to a mismatch typical in current SFT-RL pipelines: the distribution that generates the offline SFT data can differ substantially from the policy optimized during online RL, which learns from its own rollouts.
  We propose PEAR (Policy Evaluation-inspired Algorithm for Offline Learning Loss Re-weighting), an SFT-stage method that corrects this mismatch and better prepares the model for RL. PEAR uses importance sampling to reweight the SFT loss, with three variants operating at the token, block, and sequence levels. It can be used to augment standard SFT objectives and incurs little additional training overhead once probabilities for the offline data are collected.
  We conduct controlled experiments on verifiable reasoning games and mathematical reasoning tasks on Qwen 2.5 and 3 and DeepSeek-distilled models. PEAR consistently improves post-RL performance over canonical SFT, with pass at 8 gains up to a 14.6 percent on AIME2025. Our results suggest that PEAR is an effective step toward more holistic LLM post-training by designing and evaluating SFT with downstream RL in mind rather than in isolation.

</details>


### [834] [MarkovScale: Towards Optimal Sequential Scaling at Inference Time](https://arxiv.org/abs/2602.01120)
*Youkang Wang,Jian Wang,Rubing Chen,Tianyi Zeng,Xiao-Yong Wei,Qing Li*

Main category: cs.LG

Relevance: 85.0

TL;DR: 提出MarkovScale框架，将顺序缩放建模为马尔可夫过程，推导出性能改进条件与理论边界，在多个LLM和基准测试中优于现有方法


<details>
  <summary>Details</summary>
Motivation: 顺序缩放是重要的推理时缩放范式，但现有方法多为启发式，缺乏理论保证，性能提升有限且不明确

Method: 将顺序缩放建模为两状态马尔可夫过程，推导闭式解确定性能改进条件和理论边界，开发MarkovScale系统实现理论最优平衡

Result: 在3个骨干LLM、5个基准测试、20+配置中，MarkovScale一致优于最先进的并行和顺序缩放方法

Conclusion: 该框架为顺序缩放提供了理论依据和最优性保证，是迈向LLM最优资源高效推理的重要一步

Abstract: Sequential scaling is a prominent inference-time scaling paradigm, yet its performance improvements are typically modest and not well understood, largely due to the prevalence of heuristic, non-principled approaches that obscure clear optimality bounds. To address this, we propose a principled framework that models sequential scaling as a two-state Markov process. This approach reveals the underlying properties of sequential scaling and yields closed-form solutions for essential aspects, such as the specific conditions under which accuracy is improved and the theoretical upper, neutral, and lower performance bounds. Leveraging this formulation, we develop MarkovScale, a practical system that applies these optimality criteria to achieve a theoretically grounded balance between accuracy and efficiency. Comprehensive experiments across 3 backbone LLMs, 5 benchmarks, and over 20 configurations show that MarkovScale consistently outperforms state-of-the-art parallel and sequential scaling methods, representing a significant step toward optimal and resource-efficient inference in LLMs. The source code will be open upon acceptance at https://open-upon-acceptance.

</details>


### [835] [WinFLoRA: Incentivizing Client-Adaptive Aggregation in Federated LoRA under Privacy Heterogeneity](https://arxiv.org/abs/2602.01126)
*Mengsha Kou,Xiaoyu Xia,Ziqi Wang,Ibrahim Khalil,Runkun Luo,Jingwen Zhou,Minhui Xue*

Main category: cs.LG

Relevance: 85.0

TL;DR: WinFLoRA：一种隐私异构的联邦LoRA框架，通过噪声感知的聚合权重作为激励机制，在保护隐私的同时提升全局模型性能


<details>
  <summary>Details</summary>
Motivation: 在隐私敏感的联邦学习场景中，客户端注入不同级别的差分隐私噪声导致隐私异质性，这会扭曲个体激励与全局性能的对齐。现有方法无法有效处理这种异质性，需要一种能够协调不同隐私需求与全局模型性能的解决方案。

Method: 提出WinFLoRA框架，基于上传的LoRA适配器估计客户端噪声水平，将聚合权重作为激励机制。噪声较小的更新获得更大权重，从而在全局模型中具有更大影响力。这种方法无需第三方参与，能够同时满足客户端的异构隐私需求和全局模型优化目标。

Result: 在多个LLM和数据集上的广泛评估表明，WinFLoRA相比最先进的基准方法，全局准确率最高提升52.58%，客户端效用最高提升2.56倍。

Conclusion: WinFLoRA成功解决了隐私异构联邦学习中的激励对齐问题，通过噪声感知的权重分配机制，在保护客户端隐私的同时显著提升了全局模型性能和客户端效用。

Abstract: Large Language Models (LLMs) increasingly underpin intelligent web applications, from chatbots to search and recommendation, where efficient specialization is essential. Low-Rank Adaptation (LoRA) enables such adaptation with minimal overhead, while federated LoRA allows web service providers to fine-tune shared models without data sharing. However, in privacy-sensitive deployments, clients inject varying levels of differential privacy (DP) noise, creating privacy heterogeneity that misaligns individual incentives and global performance. In this paper, we propose WinFLoRA, a privacy-heterogeneous federated LoRA that utilizes aggregation weights as incentives with noise awareness. Specifically, the noises from clients are estimated based on the uploaded LoRA adapters. A larger weight indicates greater influence on the global model and better downstream task performance, rewarding lower-noise contributions. By up-weighting low-noise updates, WinFLoRA improves global accuracy while accommodating clients' heterogeneous privacy requirements. Consequently, WinFLoRA aligns heterogeneous client utility in terms of privacy and downstream performance with global model objectives without third-party involvement. Extensive evaluations demonstrate that across multiple LLMs and datasets, WinFLoRA achieves up to 52.58% higher global accuracy and up to 2.56x client utility than state-of-the-art benchmarks. Source code is publicly available at https://github.com/koums24/WinFLoRA.git.

</details>


### [836] [Self-Generative Adversarial Fine-Tuning for Large Language Models](https://arxiv.org/abs/2602.01137)
*Shiguang Wu,Yaqing Wang,Quanming Yao*

Main category: cs.LG

Relevance: 85.0

TL;DR: SGALM提出了一种基于生成对抗游戏的自对齐框架，在单个LLM内联合演化生成和判别能力，无需外部奖励模型，实现了最先进的性能。


<details>
  <summary>Details</summary>
Motivation: 现有LLM对齐方法（监督微调、RLHF）依赖高质量人工标注，成本高且稀缺。自博弈和合成数据方法虽然减少依赖，但通常基于启发式假设或无基础的自评估，可能导致偏差累积和性能漂移。

Method: 提出Self-Generative Adversarial LLM (SGALM)框架，将对齐问题形式化为单个LLM内的生成对抗游戏。模型同时演化生成和判别能力，无需外部奖励模型，通过对抗训练实现自我对齐。

Result: 理论和实证结果均表明SGALM达到了最先进的性能，既可作为有效的对齐算法，也可作为鲁棒的合成数据生成引擎。

Conclusion: SGALM提供了一种统一的自对齐框架，通过生成对抗游戏在单个LLM内实现对齐，减少对人工标注的依赖，同时避免自评估方法的偏差问题。

Abstract: Fine-tuning large language models (LLMs) for alignment typically relies on supervised fine-tuning or reinforcement learning from human feedback, both limited by the cost and scarcity of high-quality annotations. Recent self-play and synthetic data approaches reduce this dependence but often rely on heuristic assumptions or ungrounded self-evaluation, which can cause bias accumulation and performance drift. In this paper, we propose Self-Generative Adversarial LLM (SGALM), a unified fine-tuning framework that formulates alignment as a generative adversarial game within a single LLM. SGALM jointly evolves generation and discrimination capabilities without external reward models. Theoretical and empirical results demonstrate that SGALM achieves state-of-the-art performance, serves as an effective alignment algorithm and a robust synthetic data engine.

</details>


### [837] [Generalized Radius and Integrated Codebook Transforms for Differentiable Vector Quantization](https://arxiv.org/abs/2602.01140)
*Haochen You,Heng Zhang,Hongyang He,Yuqi Li,Baojing Liu*

Main category: cs.LG

Relevance: 85.0

TL;DR: GRIT-VQ提出了一种新的向量量化框架，通过半径更新和集成变换解决传统VQ的梯度不稳定和码本利用不足问题，在图像生成和推荐系统等任务中表现优异。


<details>
  <summary>Details</summary>
Motivation: 传统向量量化(VQ)使用硬最近邻分配和直通估计器存在三个主要问题：1）更新步长与量化间隙耦合导致不稳定梯度；2）每个码字独立训练；3）大规模应用时码本利用严重不足。这些问题限制了VQ在生成模型和表示学习中的性能。

Method: GRIT-VQ采用统一代理框架，保持前向传播的硬分配但使VQ完全可微。核心创新包括：1）半径更新机制，沿量化方向以可控的几何感知步长移动潜在表示；2）数据无关的集成变换，通过共享参数更新所有码字而非独立更新。

Result: 在图像重建、图像生成和推荐标记化基准测试中，GRIT-VQ相比现有VQ变体：1）持续改善重建误差；2）提升生成质量；3）提高推荐准确性；4）显著增加码本利用率。

Conclusion: GRIT-VQ为向量量化提供了理论保证的优化框架，解决了传统方法的根本局限性，为大规模生成模型和表示学习提供了更稳定、高效的离散化方案。

Abstract: Vector quantization (VQ) underpins modern generative and representation models by turning continuous latents into discrete tokens. Yet hard nearest-neighbor assignments are non-differentiable and are typically optimized with heuristic straight-through estimators, which couple the update step size to the quantization gap and train each code in isolation, leading to unstable gradients and severe codebook under-utilization at scale. In this paper, we introduce GRIT-VQ (Generalized Radius and Integrated Transform-Vector Quantization), a unified surrogate framework that keeps hard assignments in the forward pass while making VQ fully differentiable. GRIT-VQ replaces the straight-through estimator with a radius-based update that moves latents along the quantization direction with a controllable, geometry-aware step, and applies a data-agnostic integrated transform to the codebook so that all codes are updated through shared parameters instead of independently. Our theoretical analysis clarifies the fundamental optimization dynamics introduced by GRIT-VQ, establishing conditions for stable gradient flow, coordinated codebook evolution, and reliable avoidance of collapse across a broad family of quantizers. Across image reconstruction, image generation, and recommendation tokenization benchmarks, GRIT-VQ consistently improves reconstruction error, generative quality, and recommendation accuracy while substantially increasing codebook utilization compared to existing VQ variants.

</details>


### [838] [PolicyFlow: Policy Optimization with Continuous Normalizing Flow in Reinforcement Learning](https://arxiv.org/abs/2602.01156)
*Shunpeng Yang,Ben Liu,Hua Chen*

Main category: cs.LG

Relevance: 85.0

TL;DR: PolicyFlow：一种基于连续归一化流（CNF）的新型策略优化算法，通过近似重要性比率和布朗正则化器，解决了PPO在表达能力强的流模型上计算成本高和数值不稳定的问题。


<details>
  <summary>Details</summary>
Motivation: PPO算法在强化学习中广泛使用，但其重要性比率计算依赖于策略似然评估。当策略模型采用表达能力更强的连续归一化流（CNF）时，沿整个流轨迹的似然评估计算成本高且数值不稳定，限制了PPO在复杂策略模型上的应用。

Method: 1. 提出PolicyFlow算法，通过简单插值路径上的速度场变化来近似重要性比率，避免沿完整流路径的似然评估；2. 引入布朗正则化器，这是一种受布朗运动启发的隐式策略熵正则化器，防止模式崩溃并鼓励多样化行为。

Result: 在MultiGoal、PointMaze、IsaacLab和MuJoCo Playground等多个环境中的实验表明，PolicyFlow相比使用高斯策略的PPO以及FPO、DPPO等流基线方法，取得了竞争性或更优的性能。特别是在MultiGoal任务中，PolicyFlow能够捕捉更丰富的多模态动作分布。

Conclusion: PolicyFlow成功地将表达能力强的CNF策略与PPO风格目标相结合，解决了计算效率和数值稳定性问题，同时通过布朗正则化器增强了策略多样性，为强化学习中复杂策略建模提供了有效解决方案。

Abstract: Among on-policy reinforcement learning algorithms, Proximal Policy Optimization (PPO) demonstrates is widely favored for its simplicity, numerical stability, and strong empirical performance. Standard PPO relies on surrogate objectives defined via importance ratios, which require evaluating policy likelihood that is typically straightforward when the policy is modeled as a Gaussian distribution. However, extending PPO to more expressive, high-capacity policy models such as continuous normalizing flows (CNFs), also known as flow-matching models, is challenging because likelihood evaluation along the full flow trajectory is computationally expensive and often numerically unstable. To resolve this issue, we propose PolicyFlow, a novel on-policy CNF-based reinforcement learning algorithm that integrates expressive CNF policies with PPO-style objectives without requiring likelihood evaluation along the full flow path. PolicyFlow approximates importance ratios using velocity field variations along a simple interpolation path, reducing computational overhead without compromising training stability. To further prevent mode collapse and further encourage diverse behaviors, we propose the Brownian Regularizer, an implicit policy entropy regularizer inspired by Brownian motion, which is conceptually elegant and computationally lightweight. Experiments on diverse tasks across various environments including MultiGoal, PointMaze, IsaacLab and MuJoCo Playground show that PolicyFlow achieves competitive or superior performance compared to PPO using Gaussian policies and flow-based baselines including FPO and DPPO. Notably, results on MultiGoal highlight PolicyFlow's ability to capture richer multimodal action distributions.

</details>


### [839] [SimpleGPT: Improving GPT via A Simple Normalization Strategy](https://arxiv.org/abs/2602.01212)
*Marco Chen,Xianbiao Qi,Yelin He,Jiaquan Ye,Rong Xiao*

Main category: cs.LG

Relevance: 85.0

TL;DR: 本文提出SimpleNorm归一化策略，通过稳定激活尺度来降低Hessian矩阵的谱范数，从而允许更大的稳定学习率，在GPT模型上实现了3-10倍的学习率提升和更好的性能。


<details>
  <summary>Details</summary>
Motivation: 从二阶几何角度重新审视Transformer优化，探索架构设计、激活尺度、Hessian矩阵和最大可容忍学习率之间的直接联系，解决大模型训练中学习率稳定性的问题。

Method: 提出SimpleNorm归一化策略，通过构造稳定中间激活尺度；理论分析损失相对于网络激活的Hessian矩阵，证明SimpleNorm能显著降低Hessian谱范数；在1B-8B参数的GPT模型上进行实验验证。

Result: SimpleGPT（基于SimpleNorm的网络）能容忍比标准惯例大3-10倍的学习率，表现出强大的优化稳定性，性能显著优于基线。在7B规模模型训练60K步时，训练损失比LLaMA2+QKNorm低0.08（从2.290降至2.208）。

Conclusion: 通过SimpleNorm稳定激活尺度是提升Transformer优化稳定性和性能的有效方法，为大规模语言模型训练提供了新的优化视角和实用工具。

Abstract: In this work, we revisit Transformer optimization through the lens of second-order geometry and establish a direct connection between architectural design, activation scale, the Hessian matrix, and the maximum tolerable learning rate. We introduce a simple normalization strategy, termed SimpleNorm, which stabilizes intermediate activation scales by construction. Then, by analyzing the Hessian of the loss with respect to network activations, we theoretically show that SimpleNorm significantly reduces the spectral norm of the Hessian, thereby permitting larger stable learning rates. We validate our theoretical findings through extensive experiments on large GPT models at parameter scales 1B, 1.4B, 7B and 8B. Empirically, SimpleGPT, our SimpleNorm-based network, tolerates learning rates 3$\times$-10$\times$ larger than standard convention, consistently demonstrates strong optimization stability, and achieves substantially better performance than well-established baselines. Specifically, when training 7B-scale models for 60K steps, SimpleGPT achieves a training loss that is 0.08 lower than that of LLaMA2 with QKNorm, reducing the loss from 2.290 to 2.208. Our source code will be released at https://github.com/Ocram7/SimpleGPT.

</details>


### [840] [MiTA Attention: Efficient Fast-Weight Scaling via a Mixture of Top-$k$ Activations](https://arxiv.org/abs/2602.01219)
*Qishuai Wen,Zhiyuan Huang,Xianghan Meng,Wei He,Chun-Guang Li*

Main category: cs.LG

Relevance: 85.0

TL;DR: MiTA注意力机制：将注意力视为N宽度快速权重MLP，提出压缩+路由策略，通过地标查询压缩MLP宽度，并为每个地标查询聚集top-k激活的键值对构建可变形专家。


<details>
  <summary>Details</summary>
Motivation: Transformer注意力可视为宽度等于序列长度N的两层快速权重MLP。随着上下文扩展，这种N宽度MLP的表达能力增强，但快速权重的扩展成本过高。现有方法如MoE注意力通过分区和稀疏路由解决，但需要更统一的高效注意力框架。

Method: 提出压缩+路由策略：1) 使用少量地标查询将N宽度MLP压缩为更窄的MLP；2) 为每个地标查询聚集top-k激活的键值对构建可变形专家。该方法称为Mixture of Top-k Activations (MiTA)。

Result: 在视觉任务上的初步实验显示MiTA注意力机制具有潜力，需要进一步优化和在更具挑战性场景中应用。

Conclusion: 将高效注意力方法统一解释为通过路由和/或压缩扩展快速权重的框架，提出的MiTA注意力机制通过压缩和路由策略实现高效注意力，为长序列处理提供新思路。

Abstract: The attention operator in Transformers can be viewed as a two-layer fast-weight MLP, whose weights are dynamically instantiated from input tokens and whose width equals sequence length $N$. As the context extends, the expressive capacity of such an $N$-width MLP increases, but scaling its fast weights becomes prohibitively expensive for extremely long sequences. Recently, this fast-weight scaling perspective has motivated the Mixture-of-Experts (MoE) attention, which partitions the sequence into fast-weight experts and sparsely routes the tokens to them. In this paper, we elevate this perspective to a unifying framework for a wide range of efficient attention methods by interpreting them as scaling fast weights through routing and/or compression. Then we propose a compress-and-route strategy, which compresses the $N$-width MLP into a narrower one using a small set of landmark queries and constructs deformable experts by gathering top-$k$ activated key-value pairs for each landmark query. We call this strategy a Mixture of Top-$k$ Activations (MiTA), and refer to the resulting efficient mechanism as MiTA attention. Preliminary experiments on vision tasks demonstrate the promise of our MiTA attention and motivate further investigation on its optimization and broader applications in more challenging settings.

</details>


### [841] [Lotus: Efficient LLM Training by Randomized Low-Rank Gradient Projection with Adaptive Subspace Switching](https://arxiv.org/abs/2602.01233)
*Tianhao Miao,Zhongyuan Bao,Lejun Zhang*

Main category: cs.LG

Relevance: 85.0

TL;DR: Lotus是一种高效训练方法，通过修改投影过程解决GaLore中SVD计算带来的时间开销，在保持内存效率的同时减少30%训练时间和40%梯度/优化器状态内存消耗。


<details>
  <summary>Details</summary>
Motivation: 当前大规模模型训练方法在内存消耗、训练时间和模型性能之间存在权衡。GaLore虽然通过低秩梯度更新实现内存高效训练，但其SVD过程带来了额外时间开销。需要解决这种权衡问题。

Method: Lotus通过修改投影过程，提出量化单位梯度位移的准则，实现低秩梯度子空间之间的高效转换，避免昂贵的SVD计算。

Result: 实验表明Lotus是最有效的方法：训练时间减少30%，梯度/优化器状态内存消耗降低40%，在预训练和微调任务中都优于基线方法。

Conclusion: Lotus成功解决了训练效率中的权衡问题，通过简单修改投影过程实现了内存和时间效率的双重提升，为大规模模型训练提供了更优方案。

Abstract: Training efficiency in large-scale models is typically assessed through memory consumption, training time, and model performance. Current methods often exhibit trade-offs among these metrics, as optimizing one generally degrades at least one of the others. Addressing this trade-off remains a central challenge in algorithm design. While GaLore enables memory-efficient training by updating gradients in a low-rank subspace, it incurs a comparable extra training time cost due to the Singular Value Decomposition(SVD) process on gradients. In this paper, we propose Lotus, a method that resolves this trade-off by simply modifying the projection process. We propose a criterion that quantifies the displacement of the unit gradient to enable efficient transitions between low-rank gradient subspaces. Experimental results indicate that Lotus is the most efficient method, achieving a 30% reduction in training time and a 40% decrease in memory consumption for gradient and optimizer states. Additionally, it outperforms the baseline method in both pre-training and fine-tuning tasks.

</details>


### [842] [PolySAE: Modeling Feature Interactions in Sparse Autoencoders via Polynomial Decoding](https://arxiv.org/abs/2602.01322)
*Panagiotis Koromilas,Andreas D. Demou,James Oldfield,Yannis Panagakis,Mihalis Nicolaou*

Main category: cs.LG

Relevance: 85.0

TL;DR: PolySAE通过引入高阶项扩展稀疏自编码器，能够捕捉特征间的交互作用，从而更好地建模组合结构，解决了传统SAE只能线性组合特征的局限性。


<details>
  <summary>Details</summary>
Motivation: 传统稀疏自编码器(SAE)假设特征通过线性重构进行加性组合，无法区分"组合结构"与"共现关系"。例如，无法判断"Starbucks"是由"star"和"coffee"特征组合而成，还是仅仅共现。这导致SAE需要为复合概念分配整体特征，而不是分解为可解释的组成部分。

Method: PolySAE扩展了SAE的解码器，引入高阶项来建模特征交互，同时保持线性编码器以确保可解释性。通过共享投影子空间上的低秩张量分解，PolySAE能够捕捉成对和三元特征交互，参数开销很小（在GPT2上仅增加3%）。

Result: 在四个语言模型和三个SAE变体上，PolySAE平均提升了约8%的探测F1分数，同时保持可比较的重构误差，并产生2-10倍大的类别条件特征分布Wasserstein距离。关键的是，学习到的交互权重与共现频率的相关性极小（r=0.06 vs SAE特征协方差的r=0.82），表明多项式项捕捉了组合结构，如形态绑定和短语组合，基本独立于表面统计。

Conclusion: PolySAE成功解决了传统SAE在建模组合结构方面的局限性，通过多项式扩展能够更好地分解复合概念为可解释的组成部分，为神经网络表示的可解释性分析提供了更强大的工具。

Abstract: Sparse autoencoders (SAEs) have emerged as a promising method for interpreting neural network representations by decomposing activations into sparse combinations of dictionary atoms. However, SAEs assume that features combine additively through linear reconstruction, an assumption that cannot capture compositional structure: linear models cannot distinguish whether "Starbucks" arises from the composition of "star" and "coffee" features or merely their co-occurrence. This forces SAEs to allocate monolithic features for compound concepts rather than decomposing them into interpretable constituents. We introduce PolySAE, which extends the SAE decoder with higher-order terms to model feature interactions while preserving the linear encoder essential for interpretability. Through low-rank tensor factorization on a shared projection subspace, PolySAE captures pairwise and triple feature interactions with small parameter overhead (3% on GPT2). Across four language models and three SAE variants, PolySAE achieves an average improvement of approximately 8% in probing F1 while maintaining comparable reconstruction error, and produces 2-10$\times$ larger Wasserstein distances between class-conditional feature distributions. Critically, learned interaction weights exhibit negligible correlation with co-occurrence frequency ($r = 0.06$ vs. $r = 0.82$ for SAE feature covariance), suggesting that polynomial terms capture compositional structure, such as morphological binding and phrasal composition, largely independent of surface statistics.

</details>


### [843] [When Domains Interact: Asymmetric and Order-Sensitive Cross-Domain Effects in Reinforcement Learning for Reasoning](https://arxiv.org/abs/2602.01365)
*Wang Yang,Shouren Wang,Chaoda Song,Chuang Ma,Xinpeng Li,Nengbo Wang,Kaixiong Zhou,Vipin Chaudhary,Xiaotian Han*

Main category: cs.LG

Relevance: 85.0

TL;DR: GRPO在多领域训练中存在显著的不对称性、顺序敏感性和策略依赖性，需要领域感知和顺序感知的训练设计


<details>
  <summary>Details</summary>
Motivation: GRPO已成为提升大语言模型推理能力的关键技术，但其在不同领域排序策略下的行为尚未得到充分理解。特别是顺序训练（一次一个领域）与混合领域训练（多个领域同时）在GRPO中的影响缺乏系统研究。

Method: 对数学、科学、逻辑和谜题推理任务进行训练顺序效应的系统分析，比较顺序训练与混合训练策略，研究跨领域交互的顺序依赖性。

Result: 发现三个关键结果：1）单领域泛化高度不对称：其他领域训练可提升数学推理约25%准确率，但对逻辑和谜题任务几乎没有迁移效果；2）跨领域交互高度顺序依赖：数学→科学顺序获得83%/41%准确率，而科学→数学顺序则降至77%/25%；3）没有单一策略在多领域训练中普遍最优：顺序训练有利于数学（达84%），混合训练有利于科学和逻辑，不良排序可能导致大性能差距（从70%到56%）。

Conclusion: GRPO在多领域设置下表现出明显的不对称性、顺序敏感性和策略依赖性，突显了领域感知和顺序感知训练设计的必要性。

Abstract: Group Relative Policy Optimization (GRPO) has become a key technique for improving reasoning abilities in large language models, yet its behavior under different domain sequencing strategies is poorly understood. In particular, the impact of sequential (one domain at a time) versus mixed-domain (multiple domain at a time) training in GRPO has not been systematically studied. We provide the first systematic analysis of training-order effects across math, science, logic, and puzzle reasoning tasks. We found (1) single-domain generalization is highly asymmetric: training on other domains improves math reasoning by approximately 25\% accuracy, while yielding negligible transfer to logic and puzzle; (2) cross-domain interactions are highly order-dependent: training in the order math$\rightarrow$science achieves 83\% / 41\% accuracy on math / science, while reversing the order to science$\rightarrow$math degrades performance to 77\% / 25\%; (3) no single strategy is universally optimal in multi-domain training: sequential training favors math (up to 84\%), mixed training favors science and logic, and poor ordering can incur large performance gaps (from 70\% to 56\%). Overall, our findings demonstrate that GRPO under multi-domain settings exhibits pronounced asymmetry, order sensitivity, and strategy dependence, highlighting the necessity of domain-aware and order-aware training design.

</details>


### [844] [The Gradient-Causal Gap: Why Gradient Importance Fails on Complex Tasks](https://arxiv.org/abs/2602.01442)
*Donald Ye*

Main category: cs.LG

Relevance: 85.0

TL;DR: 研究发现梯度大小与因果重要性在Transformer中的关系随任务复杂度变化，梯度大小不能可靠指导剪枝，移除低梯度"隐藏英雄"会严重损害模型泛化能力


<details>
  <summary>Details</summary>
Motivation: 探索神经网络中梯度大小与因果重要性之间的关系，特别是在Transformer模型中，理解为什么基于梯度的剪枝方法在实际应用中表现不稳定

Method: 在算法任务上训练Transformer模型，形式化定义"梯度-因果间隙"，通过相关性分析研究梯度大小与因果重要性的关系，进行剪枝实验验证梯度大小作为剪枝标准的可靠性

Result: 梯度大小与因果重要性的相关性随任务复杂度增加而下降甚至反转，移除低梯度组件会严重损害OOD准确性(-32%)，移除高梯度组件的结果不可预测

Conclusion: 梯度大小不能可靠指导模型剪枝，梯度-因果间隙揭示了基于梯度的剪枝方法的根本局限性，需要更可靠的剪枝标准

Abstract: Removing ''important'' high-gradient components from a neural network can improve generalization, while removing unimportant'' low-gradient components can destroy it. We demonstrate this paradox by formalizing the \textit{Gradient-Causal Gap} in Transformers trained on algorithmic tasks. While gradient magnitude and causal importance align on simple tasks ($ρ=0.73$ for reversal), this relationship collapses as task complexity increases ($ρ=0.32$ for sorting), sometimes becoming inverted ($ρ=-0.11$). Pruning experiments reveal that gradient magnitude is not merely inaccurate but \textit{unpredictably} so. Removing low-gradient ''Hidden Heroes'' consistently devastates OOD accuracy ($-32\%$). Removing high-gradient ''Gradient Bloats'' is a coin flip: harmless in most seeds (indicating optimization noise), catastrophic in others (indicating overfitting circuits). This unpredictability means gradient-based pruning cannot reliably preserve model capabilities.

</details>


### [845] [Multi-LLM Adaptive Conformal Inference for Reliable LLM Responses](https://arxiv.org/abs/2602.01285)
*Kangjun Noh,Seongchan Lee,Ilmun Kim,Kyungwoo Song*

Main category: cs.LG

Relevance: 85.0

TL;DR: MACI提出了一种基于乘积过滤的多LLM自适应共形推理方法，通过集成模型生成更准确的事实性分数，在保证覆盖率的同时显著提高了真实声明的保留率。


<details>
  <summary>Details</summary>
Motivation: 在医疗和法律等高风险领域，确保LLM的事实性至关重要。现有的共形推理方法要么过于保守（丢弃大量真实声明），要么依赖自适应错误率和简单线性模型，无法捕捉复杂的群体结构。

Method: 将共形推理重新表述为乘积过滤设置，将事实性建模为声明级分数的乘积。MACI利用集成模型生成更准确的事实性分数，并通过群体条件校准保持有效性。

Result: 实验表明，MACI始终达到用户指定的覆盖率，同时相比基线方法显著提高了保留率并降低了时间成本。

Conclusion: MACI通过乘积过滤框架和集成模型，在保证统计保证的同时，更有效地识别和保留真实声明，为LLM在高风险领域的可靠使用提供了新方法。

Abstract: Ensuring factuality is essential for the safe use of Large Language Models (LLMs) in high-stakes domains such as medicine and law. Conformal inference provides distribution-free guarantees, but existing approaches are either overly conservative, discarding many true-claims, or rely on adaptive error rates and simple linear models that fail to capture complex group structures. To address these challenges, we reformulate conformal inference in a multiplicative filtering setting, modeling factuality as a product of claim-level scores. Our method, Multi-LLM Adaptive Conformal Inference (MACI), leverages ensembles to produce more accurate factuality-scores, which in our experiments led to higher retention, while validity is preserved through group-conditional calibration. Experiments show that MACI consistently achieves user-specified coverage with substantially higher retention and lower time cost than baselines. Our repository is available at https://github.com/MLAI-Yonsei/MACI

</details>


### [846] [A Relative-Budget Theory for Reinforcement Learning with Verifiable Rewards in Large Language Model Reasoning](https://arxiv.org/abs/2602.01523)
*Akifumi Wachi,Hirota Kinoshita,Shokichi Takakura,Rei Higuchi,Taiji Suzuki*

Main category: cs.LG

Relevance: 85.0

TL;DR: 论文提出相对预算理论，通过单一量ξ=H/E[T]解释RL在不同任务和计算预算下对LLM推理能力提升效果的差异，识别出三个学习机制并给出有限样本保证。


<details>
  <summary>Details</summary>
Motivation: 强化学习是提升大语言模型推理能力的主要方法，但其效果在不同任务和计算预算下差异很大。目前缺乏统一的理论框架来解释这种差异，阻碍了RL在LLM推理优化中的有效应用。

Method: 提出相对预算理论，定义ξ=H/E[T]（生成时域/首次正确解的平均token数）。通过理论分析识别三个学习机制：不足、平衡和充足。在理想分布假设下进行案例分析，并提供在线RL的有限样本保证。

Result: 理论分析显示ξ控制奖励方差和信息轨迹概率，决定样本效率。实证发现ξ∈[1.5,2.0]时学习效率最高，与峰值推理性能一致。相对预算在迭代中线性增长。

Conclusion: 相对预算ξ是解释RL在LLM推理中效果差异的关键因素，为RL训练提供了理论指导和实用设计原则，有助于优化计算资源分配。

Abstract: Reinforcement learning (RL) is a dominant paradigm for improving the reasoning abilities of large language models, yet its effectiveness varies across tasks and compute budgets. We propose a \emph{relative-budget} theory explaining this variation through a single quantity called relative budget $ξ:= H/\mathbb{E}[T]$, where $H$ is the generation horizon (token budget) and $T$ denotes the number of tokens until the first correct solution under a base policy. We show that $ξ$ determines sample efficiency by controlling reward variance and the likelihood of informative trajectories. Our analysis reveals three regimes: in the \emph{deficient} regime ($ξ\to 0$), informative trajectories are rare and the sample complexity explodes; in the \emph{balanced} regime ($ξ=Θ(1)$), informative trajectories occur with non-negligible probability and RL is maximally sample-efficient; and in the \emph{ample} regime ($ξ\to \infty$), learning remains stable but marginal gains per iteration diminish. We further provide finite-sample guarantees for online RL that characterize learning progress across these regimes. Specifically, in a case study under idealized distributional assumptions, we show that the relative budget grows linearly over iterations. Our empirical results confirm these predictions in realistic settings, identifying a budget $ξ\in [1.5, 2.0]$ that maximizes learning efficiency and coincides with peak reasoning performance.

</details>


### [847] [EDIS: Diagnosing LLM Reasoning via Entropy Dynamics](https://arxiv.org/abs/2602.01288)
*Chenghua Zhu,Siyan Wu,Xiangkang Zeng,Zishan Xu,Zhaolu Kang,Yifu Guo,Yuquan Lu,Junduan Huang,Guojing Zhou*

Main category: cs.LG

Relevance: 85.0

TL;DR: 该论文提出通过分析LLM生成过程中熵的动态演化模式来改进推理，发现错误推理具有不稳定的熵轨迹特征，并引入EDIS指标用于推理时选择和训练时样本筛选。


<details>
  <summary>Details</summary>
Motivation: 现有方法将置信度视为静态量（通常聚合在token层面），但生成过程中置信度的时序演化可能包含更丰富的信息。作者旨在探索熵动态演化模式如何反映推理质量，特别是区分正确与错误推理。

Method: 分析token级熵轨迹，识别区分正确与错误推理的特征模式（如突发尖峰和峰谷尖峰）。引入熵动态不稳定性评分（EDIS），这是一个量化熵演化不稳定性的轨迹级指标。将EDIS应用于推理时选择和训练时样本筛选。

Result: 发现错误推理表现出不稳定的熵动态，包括突发尖峰（持续不确定性增长）和峰谷尖峰（短暂置信后急剧反弹）。这些模式在不同模型和训练阶段均存在，表明是推理失败的内在属性。EDIS作为诊断信号显著提高了推理准确性。

Conclusion: 熵动态演化是理解和改进LLM推理的一个未被充分探索但信息丰富的视角。EDIS为推理时选择和训练时样本筛选提供了有效工具，开辟了利用时序置信信息的新方向。

Abstract: Entropy-based confidence signals are increasingly leveraged to improve reasoning in large language models (LLMs), yet existing approaches treat confidence as a static quantity -- typically aggregated over tokens. We show that the \emph{temporal evolution} of confidence during generation carries richer information than aggregate statistics alone. Analyzing token-level entropy trajectories, we identify characteristic patterns distinguishing correct from incorrect reasoning: erroneous solutions exhibit unstable dynamics, including burst spikes (sustained uncertainty growth) and peak-valley spikes (sharp rebounds following transient confidence). These patterns persist across models and training stages, suggesting they reflect intrinsic properties of reasoning failure rather than superficial noise. To formalize this observation, we introduce the Entropy Dynamics Instability Score (\textbf{EDIS}), a trajectory-level metric quantifying instability in entropy evolution. EDIS serves as an effective diagnostic signal for inference-time selection, substantially improving reasoning accuracy, and offers a promising direction for training-time sample curation. Our findings establish entropy dynamics as an underexplored yet informative lens for understanding and improving LLM reasoning.

</details>


### [848] [Adaptive Rollout Allocation for Online Reinforcement Learning with Verifiable Rewards](https://arxiv.org/abs/2602.01601)
*Hieu Trung Nguyen,Bao Nguyen,Wenao Ma,Yuzhi Zhao,Ruifeng She,Viet Anh Nguyen*

Main category: cs.LG

Relevance: 85.0

TL;DR: VIP是一种基于方差信息的预测性分配策略，用于提高可验证奖励强化学习中的采样效率，通过高斯过程模型预测每个提示的成功概率并优化梯度方差来分配rollout预算。


<details>
  <summary>Details</summary>
Motivation: 现有基于组的策略优化方法（如GRPO）为所有训练提示分配固定数量的rollout，这种均匀分配隐含地假设所有提示具有同等信息价值，可能导致计算预算使用效率低下并阻碍训练进展。

Method: VIP使用轻量级高斯过程模型基于最近的rollout预测每个提示的成功概率，将这些概率预测转化为方差估计，然后通过凸优化问题在硬计算预算约束下确定最优的rollout分配。

Result: 实验结果表明，VIP在多个基准测试中持续提高采样效率，并比均匀分配或启发式分配策略获得更高的性能。

Conclusion: VIP通过方差感知的预算分配策略有效解决了强化学习中采样效率低下的问题，为可验证奖励的强化学习提供了更高效的计算资源利用方法。

Abstract: Sampling efficiency is a key bottleneck in reinforcement learning with verifiable rewards. Existing group-based policy optimization methods, such as GRPO, allocate a fixed number of rollouts for all training prompts. This uniform allocation implicitly treats all prompts as equally informative, and could lead to inefficient computational budget usage and impede training progress. We introduce \Ours, a Variance-Informed Predictive allocation strategy that allocates a given rollout budget to the prompts in the incumbent batch to minimize the expected gradient variance of the policy update. At each iteration, \Ours~uses a lightweight Gaussian process model to predict per-prompt success probabilities based on recent rollouts. These probability predictions are translated into variance estimates, which are then fed into a convex optimization problem to determine the optimal rollout allocations under a hard compute budget constraint. Empirical results show that \Ours~consistently improves sampling efficiency and achieves higher performance than uniform or heuristic allocation strategies in multiple benchmarks. Our code will be available at https://github.com/HieuNT91/VIP.

</details>


### [849] [$\textbf{AGT$^{AO}$}$: Robust and Stabilized LLM Unlearning via Adversarial Gating Training with Adaptive Orthogonality](https://arxiv.org/abs/2602.01703)
*Pengyu Li,Lingling Zhang,Zhitao Gao,Yanrui Wu,Yuxuan Dong,Huan Liu,Bifan Wei,Jun Liu*

Main category: cs.LG

Relevance: 85.0

TL;DR: 提出AGTAO框架，通过自适应正交性和对抗性门控训练解决LLM遗忘中的遗忘-效用权衡问题


<details>
  <summary>Details</summary>
Motivation: LLM会无意中记忆敏感数据，带来隐私安全风险。现有机器遗忘方法面临两难：激进遗忘导致灾难性遗忘损害模型效用，保守策略则导致表面遗忘，模型仍易受对抗恢复攻击

Method: 提出AGTAO框架：1) 自适应正交性(AO)动态缓解遗忘与保留目标间的梯度冲突；2) 对抗性门控训练(AGT)将遗忘建模为潜在空间最小最大博弈，使用课程式门控机制模拟和对抗内部恢复尝试

Result: 实验显示AGTAO在遗忘效果(KUR ≈ 0.01)和模型效用(MMLU 58.30)间取得优越平衡

Conclusion: AGTAO框架有效解决了LLM遗忘中的遗忘-效用权衡问题，为隐私保护提供了一种统一解决方案

Abstract: While Large Language Models (LLMs) have achieved remarkable capabilities, they unintentionally memorize sensitive data, posing critical privacy and security risks. Machine unlearning is pivotal for mitigating these risks, yet existing paradigms face a fundamental dilemma: aggressive unlearning often induces catastrophic forgetting that degrades model utility, whereas conservative strategies risk superficial forgetting, leaving models vulnerable to adversarial recovery. To address this trade-off, we propose $\textbf{AGT$^{AO}$}$ (Adversarial Gating Training with Adaptive Orthogonality), a unified framework designed to reconcile robust erasure with utility preservation. Specifically, our approach introduces $\textbf{Adaptive Orthogonality (AO)}$ to dynamically mitigate geometric gradient conflicts between forgetting and retention objectives, thereby minimizing unintended knowledge degradation. Concurrently, $\textbf{Adversarial Gating Training (AGT)}$ formulates unlearning as a latent-space min-max game, employing a curriculum-based gating mechanism to simulate and counter internal recovery attempts. Extensive experiments demonstrate that $\textbf{AGT$^{AO}$}$ achieves a superior trade-off between unlearning efficacy (KUR $\approx$ 0.01) and model utility (MMLU 58.30). Code is available at https://github.com/TiezMind/AGT-unlearning.

</details>


### [850] [Dissecting Outlier Dynamics in LLM NVFP4 Pretraining](https://arxiv.org/abs/2602.02047)
*Peijie Dong,Ruibo Fan,Yuechen Tao,Di Mou,Wenhu Hu,Zhenheng Tang,Yinghao Yu,Jiamang Wang,Wenbo Su,Guodong Yang,Liping Zhang,Xiaowen Chu,Baochun Li,Bo Li*

Main category: cs.LG

Relevance: 85.0

TL;DR: 该论文研究了4位量化训练中的异常值问题，分析了Softmax Attention和Linear Attention中的异常值动态，提出了Hot-Channel Patch补偿机制和CHON训练方案，显著减少了NVFP4与BF16之间的性能差距。


<details>
  <summary>Details</summary>
Motivation: 4位算术训练能提升大语言模型的吞吐量和内存效率，但FP4的动态范围有限，对异常值更敏感。NVFP4通过分层微缩放减轻量化误差，但与BF16相比仍存在性能差距。需要深入理解异常值在训练过程中的动态特性，以改进4位量化训练。

Method: 1. 对NVFP4预训练过程中的异常值动态进行纵向分析，研究异常值的位置、成因和时序演化；2. 识别导致异常值的架构组件（SA中的Softmax、LA中的门控、FFN中的SwiGLU）；3. 提出Hot-Channel Patch在线补偿机制，识别热通道并重新注入残差；4. 开发CHON训练方案，结合HCP和后QK操作保护。

Result: 在GLA-1.3B模型上训练60B tokens，CHON将NVFP4与BF16之间的损失差距从0.94%降低到0.58%，同时保持下游任务准确率。分析发现：Linear Attention相比Softmax Attention减少了张量级重尾分布，但在块量化下仍存在块级尖峰；异常值从训练早期的瞬时尖峰演变为后期的持久热通道。

Conclusion: 通过深入分析4位量化训练中的异常值动态，识别了关键架构组件的敏感性，并提出了有效的在线补偿机制。CHON方案显著改善了NVFP4训练的性能，为高效的大语言模型训练提供了实用解决方案。

Abstract: Training large language models using 4-bit arithmetic enhances throughput and memory efficiency. Yet, the limited dynamic range of FP4 increases sensitivity to outliers. While NVFP4 mitigates quantization error via hierarchical microscaling, a persistent loss gap remains compared to BF16. This study conducts a longitudinal analysis of outlier dynamics across architecture during NVFP4 pretraining, focusing on where they localize, why they occur, and how they evolve temporally. We find that, compared with Softmax Attention (SA), Linear Attention (LA) reduces per-tensor heavy tails but still exhibits persistent block-level spikes under block quantization. Our analysis attributes outliers to specific architectural components: Softmax in SA, gating in LA, and SwiGLU in FFN, with "post-QK" operations exhibiting higher sensitivity to quantization. Notably, outliers evolve from transient spikes early in training to a small set of persistent hot channels (i.e., channels with persistently large magnitudes) in later stages. Based on these findings, we introduce Hot-Channel Patch (HCP), an online compensation mechanism that identifies hot channels and reinjects residuals using hardware-efficient kernels. We then develop CHON, an NVFP4 training recipe integrating HCP with post-QK operation protection. On GLA-1.3B model trained for 60B tokens, CHON reduces the loss gap to BF16 from 0.94% to 0.58% while maintaining downstream accuracy.

</details>


### [851] [Your Self-Play Algorithm is Secretly an Adversarial Imitator: Understanding LLM Self-Play through the Lens of Imitation Learning](https://arxiv.org/abs/2602.01357)
*Shangzhe Li,Xuchao Zhang,Chetan Bansal,Weitong Zhang*

Main category: cs.LG

Relevance: 85.0

TL;DR: 该论文提出了一种基于对抗模仿学习的自博弈微调理论框架，将微调过程建模为模型与正则化隐式奖励玩家之间的min-max博弈，并提出了基于χ²散度的新算法，在多个任务上优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 自博弈后训练方法已成为微调大语言模型的有效方法，无需偏好数据即可将弱模型转变为强模型。然而，自博弈微调的理论基础尚未得到充分探索，本文旨在填补这一空白。

Method: 将自博弈微调与对抗模仿学习联系起来，将微调过程形式化为模型与由模型本身参数化的正则化隐式奖励玩家之间的min-max博弈。基于此理论框架，提出了基于χ²散度变分目标的新自博弈模仿微调算法，具有有界奖励和改进的稳定性。

Result: 在各种语言模型微调任务上的实验表明，该方法相比现有自博弈方法取得了持续改进，验证了理论洞察的有效性。

Conclusion: 该工作为自博弈微调提供了坚实的理论基础，统一了自博弈模仿和一般偏好对齐，并通过理论分析证明了收敛性，提出的新算法在实践中表现出优越性能。

Abstract: Self-play post-training methods has emerged as an effective approach for finetuning large language models and turn the weak language model into strong language model without preference data. However, the theoretical foundations for self-play finetuning remain underexplored. In this work, we tackle this by connecting self-play finetuning with adversarial imitation learning by formulating finetuning procedure as a min-max game between the model and a regularized implicit reward player parameterized by the model itself. This perspective unifies self-play imitation and general preference alignment within a common framework. Under this formulation, we present a game-theoretic analysis showing that the self-play finetuning will converge to it's equilibrium. Guided by this theoretical formulation, we propose a new self-play imitation finetuning algorithm based on the $χ^2$-divergence variational objective with bounded rewards and improved stability. Experiments on various of language model finetuning tasks demonstrate consistent improvements over existing self-play methods and validate our theoretical insights.

</details>


### [852] [No Global Plan in Chain-of-Thought: Uncover the Latent Planning Horizon of LLMs](https://arxiv.org/abs/2602.02103)
*Liyan Xu,Mo Yu,Fandong Meng,Jie Zhou*

Main category: cs.LG

Relevance: 85.0

TL;DR: 该论文研究了大型语言模型在思维链推理中的潜在规划能力，发现LLMs表现出短视视野，主要进行增量转换而非精确的全局规划，并基于此提出了改进不确定性估计的方法。


<details>
  <summary>Details</summary>
Motivation: 研究动机是深入理解LLMs内部状态与其显式推理轨迹之间的关系。先前研究发现LLMs在思维链出现前就存在潜在规划，这降低了显式思维链的重要性，但思维链对于多步推理任务仍然关键。作者希望探究LLMs的潜在规划强度及其对推理过程的影响。

Method: 提出了Tele-Lens探测方法，应用于不同任务领域的隐藏状态。通过分析LLMs在思维链推理中的内部状态动态，研究其规划能力。基于发现的短视视野特性，提出了改进思维链不确定性估计的假设，并验证了少量思维链位置可以有效代表整个路径的不确定性。

Result: 实证结果表明：1）LLMs表现出短视视野，主要进行增量转换而非精确的全局规划；2）少量思维链位置可以有效代表整个路径的不确定性；3）可以实现自动识别思维链绕过的能力而不降低性能。

Conclusion: 结论强调了利用思维链动态的重要性，表明LLMs在推理过程中主要依赖增量处理而非全局规划，这一特性可以用于改进不确定性估计和自动识别推理捷径。

Abstract: This work stems from prior complementary observations on the dynamics of Chain-of-Thought (CoT): Large Language Models (LLMs) is shown latent planning of subsequent reasoning prior to CoT emergence, thereby diminishing the significance of explicit CoT; whereas CoT remains critical for tasks requiring multi-step reasoning. To deepen the understanding between LLM's internal states and its verbalized reasoning trajectories, we investigate the latent planning strength of LLMs, through our probing method, Tele-Lens, applying to hidden states across diverse task domains. Our empirical results indicate that LLMs exhibit a myopic horizon, primarily conducting incremental transitions without precise global planning. Leveraging this characteristic, we propose a hypothesis on enhancing uncertainty estimation of CoT, which we validate that a small subset of CoT positions can effectively represent the uncertainty of the entire path. We further underscore the significance of exploiting CoT dynamics, and demonstrate that automatic recognition of CoT bypass can be achieved without performance degradation. Our code, data and models are released at https://github.com/lxucs/tele-lens.

</details>


### [853] [Unifying Masked Diffusion Models with Various Generation Orders and Beyond](https://arxiv.org/abs/2602.02112)
*Chunsan Hong,Sanghyun Lee,Jong Chul Ye*

Main category: cs.LG

Relevance: 85.0

TL;DR: 提出OeMDM和LoMDM两种掩码扩散模型，前者统一了多种生成顺序的扩散过程，后者通过联合学习生成顺序和扩散主干，在语言建模任务中优于现有离散扩散模型。


<details>
  <summary>Details</summary>
Motivation: 现有掩码扩散模型(MDMs)的生成质量严重依赖生成顺序，先前工作要么硬编码顺序，要么为预训练MDM学习顺序策略，这会产生额外成本且可能因两阶段优化而得到次优解。

Method: 1) 提出OeMDM(order-expressive masked diffusion model)，为具有各种生成顺序的扩散生成过程提供统一框架；2) 基于OeMDM提出LoMDM(learnable-order masked diffusion model)，通过单一目标联合学习生成顺序和扩散主干。

Result: LoMDM在多个语言建模基准测试中优于各种离散扩散模型，证实了联合学习生成顺序和扩散主干的有效性。

Conclusion: 提出的LoMDM框架能够学习上下文相关的生成顺序，为掩码扩散模型提供了一种更灵活和有效的语言生成方法，统一了多种现有模型。

Abstract: Masked diffusion models (MDMs) are a potential alternative to autoregressive models (ARMs) for language generation, but generation quality depends critically on the generation order. Prior work either hard-codes an ordering (e.g., blockwise left-to-right) or learns an ordering policy for a pretrained MDM, which incurs extra cost and can yield suboptimal solutions due to the two-stage optimization. Motivated by this, we propose order-expressive masked diffusion model (OeMDM) for a broad class of diffusion generative processes with various generation orders, enabling the interpretation of MDM, ARM, and block diffusion in a single framework. Furthermore, building on OeMDM, we introduce learnable-order masked diffusion model (LoMDM), which jointly learns the generation ordering and diffusion backbone through a single objective from scratch, enabling the diffusion model to generate text in context-dependent ordering. Empirically, we confirm that LoMDM outperforms various discrete diffusion models across multiple language modeling benchmarks.

</details>


### [854] [Learning Generative Selection for Best-of-N](https://arxiv.org/abs/2602.02143)
*Shubham Toshniwal,Aleksander Ficek,Siddhartha Jain,Wei Du,Vahid Noroozi,Sadegh Mahdavi,Somshubra Majumdar,Igor Gitman*

Main category: cs.LG

Relevance: 85.0

TL;DR: 通过强化学习训练小型推理模型实现高效生成选择，提升测试时计算扩展效果


<details>
  <summary>Details</summary>
Motivation: 现有生成选择方法（如GenSelect）主要依赖大模型，限制了测试时并行采样的扩展效果。需要让小型模型也能具备强大的生成选择能力，以实现更高效的测试时计算扩展。

Method: 1) 从大规模数学和代码指令数据集中合成选择任务，筛选包含正确和错误候选解决方案的实例；2) 使用DAPO强化学习训练1.7B参数模型，奖励正确的选择；3) 在数学（AIME24/25, HMMT25）和代码（LiveCodeBench）推理基准上评估。

Result: 训练的小型模型在数学和代码推理基准上持续优于提示和多数投票基线，通常接近或超过更大模型的表现。这些增益能够泛化到选择更强模型的输出，尽管训练时仅使用较弱模型的输出。

Conclusion: 强化学习是解锁小型模型强大生成选择能力的可扩展方法，能够实现高效的测试时计算扩展。

Abstract: Scaling test-time compute via parallel sampling can substantially improve LLM reasoning, but is often limited by Best-of-N selection quality. Generative selection methods, such as GenSelect, address this bottleneck, yet strong selection performance remains largely limited to large models. We show that small reasoning models can acquire strong GenSelect capabilities through targeted reinforcement learning. To this end, we synthesize selection tasks from large-scale math and code instruction datasets by filtering to instances with both correct and incorrect candidate solutions, and train 1.7B-parameter models with DAPO to reward correct selections. Across math (AIME24, AIME25, HMMT25) and code (LiveCodeBench) reasoning benchmarks, our models consistently outperform prompting and majority-voting baselines, often approaching or exceeding much larger models. Moreover, these gains generalize to selecting outputs from stronger models despite training only on outputs from weaker models. Overall, our results establish reinforcement learning as a scalable way to unlock strong generative selection in small models, enabling efficient test-time scaling.

</details>


### [855] [SNIP: An Adaptive Mixed Precision Framework for Subbyte Large Language Model Training](https://arxiv.org/abs/2602.01410)
*Yunjie Pan,Yongyi Yang,Hanmei Yang,Scott Mahlke*

Main category: cs.LG

Relevance: 85.0

TL;DR: SNIP：一种细粒度自适应混合精度训练框架，通过前向损失发散和后向权重发散指标指导ILP优化层级精度，在LLM预训练中减少80%FLOPs同时保持模型质量


<details>
  <summary>Details</summary>
Motivation: 现有混合精度训练方法要么对所有GEMM操作应用统一精度，要么依赖启发式方法，这些方法在训练期间无法泛化，导致次优收敛和不稳定性。需要一种能支持子字节精度、自适应调整的框架来高效训练LLM同时保持模型质量。

Method: SNIP周期性收集激活、梯度和优化器状态的统计信息，定义两个关键指标：前向传递中的损失发散（量化引起的训练损失增加）和后向传递中的权重发散（梯度误差传播影响模型更新）。这些指标指导整数线性规划问题，系统优化层级精度以最小化整体质量损失同时满足效率目标。

Result: 在1B、3B、7B和70B Llama-like模型上的实验表明，SNIP始终优于现有基线，在不同模型大小和训练阶段减少高达80%的FLOPs，同时保持模型质量，计算开销最小。

Conclusion: SNIP提供了一种系统化的细粒度混合精度训练方法，有效解决了LLM预训练中效率与质量平衡的挑战，支持子字节精度并实现显著的计算节省。

Abstract: Training large language models (LLMs) efficiently while preserving model quality poses significant challenges, particularly with subbyte precision supported by state-of-the-art GPUs. Current mixed-precision training approaches either apply uniform precision to all GEMM operations or rely on heuristic-based methods that fail to generalize during training, leading to suboptimal convergence and instability. To address these challenges, this paper introduces SNIP, a fine-grained adaptive mixed-precision training framework for LLM pretraining that supports subbyte precision. SNIP periodically collects statistics on activations, gradients, and optimizer states to assess the precision loss impact on model quality. We define two key metrics: loss divergence in the forward pass, caused by quantization-induced increases in training loss, and weight divergence in the backward pass, which measures error propagation through gradients affecting model updates. These metrics guide an Integer Linear Programming (ILP) problem that systematically optimizes layerwise precision to minimize overall quality loss while meeting efficiency targets. Experiments on 1B, 3B, 7B and 70B Llama-like models demonstrate that SNIP consistently outperforms existing baselines, reducing FLOPs by up to 80% while preserving model quality across different model sizes and training phases with minimal computational overhead.

</details>


### [856] [Revisiting Adaptive Rounding with Vectorized Reparameterization for LLM Quantization](https://arxiv.org/abs/2602.02151)
*Yuli Zhou,Qingxuan Chen,Luca Benini,Guolei Sun,Yawei Li*

Main category: cs.LG

Relevance: 85.0

TL;DR: VQRound：一种参数高效的量化优化框架，通过将舍入矩阵重新参数化为紧凑码本，实现大语言模型的高效后训练量化，仅需0.2%可训练参数。


<details>
  <summary>Details</summary>
Motivation: 自适应舍入作为舍入到最近(RTN)的替代方案，通过跨元素误差抵消实现更好的量化效果。然而，对于数十亿参数的大语言模型，密集的元素级舍入矩阵计算成本过高。需要从效率角度重新审视自适应舍入，解决其在大型模型中的可扩展性问题。

Method: 提出VQRound框架：1) 将舍入矩阵重新参数化为紧凑码本，避免低秩方法的局限性；2) 在L∞范数下最小化元素级最坏情况误差，特别适合处理LLMs中的重尾权重分布；3) 识别舍入初始化作为关键因素，开发轻量级端到端微调流程，仅需128个样本即可优化所有层的码本。

Result: 在OPT、LLaMA、LLaMA2和Qwen3模型上的大量实验表明，VQRound在相同步数下比传统自适应舍入具有更好的收敛性，同时仅使用0.2%的可训练参数。证明自适应舍入可以同时实现可扩展性和快速拟合。

Conclusion: VQRound通过码本重新参数化和轻量级微调，成功解决了自适应舍入在大语言模型中的计算效率问题，为后训练量化提供了既高效又准确的新方法。

Abstract: Adaptive Rounding has emerged as an alternative to round-to-nearest (RTN) for post-training quantization by enabling cross-element error cancellation. Yet, dense and element-wise rounding matrices are prohibitively expensive for billion-parameter large language models (LLMs). We revisit adaptive rounding from an efficiency perspective and propose VQRound, a parameter-efficient optimization framework that reparameterizes the rounding matrix into a compact codebook. Unlike low-rank alternatives, VQRound minimizes the element-wise worst-case error under $L_\infty$ norm, which is critical for handling heavy-tailed weight distributions in LLMs. Beyond reparameterization, we identify rounding initialization as a decisive factor and develop a lightweight end-to-end finetuning pipeline that optimizes codebooks across all layers using only 128 samples. Extensive experiments on OPT, LLaMA, LLaMA2, and Qwen3 models demonstrate that VQRound achieves better convergence than traditional adaptive rounding at the same number of steps while using as little as 0.2% of the trainable parameters. Our results show that adaptive rounding can be made both scalable and fast-fitting. The code is available at https://github.com/zhoustan/VQRound.

</details>


### [857] [Improve the Trade-off Between Watermark Strength and Speculative Sampling Efficiency for Language Models](https://arxiv.org/abs/2602.01428)
*Weiqing He,Xiang Li,Li Shen,Weijie Su,Qi Long*

Main category: cs.LG

Relevance: 85.0

TL;DR: 本文提出了一种结合推测采样与水印的新方法，通过向草稿令牌接受过程注入伪随机性，在保持推测采样效率的同时实现最大水印强度，解决了现有方法中水印强度与接受率之间的权衡问题。


<details>
  <summary>Details</summary>
Motivation: 水印技术用于追踪大语言模型输出的来源，但在实际部署中面临推理效率低下的问题。推测采样可以加速推理，但现有研究表明水印强度与草稿模型接受率之间存在根本性权衡：更高的水印强度会降低接受率，阻碍两者同时实现。

Method: 1. 引入定量水印强度度量，该度量控制统计可检测性，并在令牌是伪随机数的确定性函数时达到最大
2. 将权衡问题形式化为约束优化问题，为两种现有水印方案推导显式帕累托曲线
3. 提出原则性机制，向草稿令牌接受过程注入伪随机性，确保最大水印强度同时保持推测采样效率

Result: 实验表明该方法在不牺牲效率的情况下提高了可检测性。该方法揭示了推测采样与水印的统一原则，为两者的高效实用部署铺平了道路。

Conclusion: 水印强度与推测采样效率之间的权衡并非绝对。通过向草稿令牌接受过程注入伪随机性的原则性机制，可以在保持推测采样效率的同时实现最大水印强度，为水印技术的实际部署提供了可行方案。

Abstract: Watermarking is a principled approach for tracing the provenance of large language model (LLM) outputs, but its deployment in practice is hindered by inference inefficiency. Speculative sampling accelerates inference, with efficiency improving as the acceptance rate between draft and target models increases. Yet recent work reveals a fundamental trade-off: higher watermark strength reduces acceptance, preventing their simultaneous achievement. We revisit this trade-off and show it is not absolute. We introduce a quantitative measure of watermark strength that governs statistical detectability and is maximized when tokens are deterministic functions of pseudorandom numbers. Using this measure, we fully characterize the trade-off as a constrained optimization problem and derive explicit Pareto curves for two existing watermarking schemes. Finally, we introduce a principled mechanism that injects pseudorandomness into draft-token acceptance, ensuring maximal watermark strength while maintaining speculative sampling efficiency. Experiments further show that this approach improves detectability without sacrificing efficiency. Our findings uncover a principle that unites speculative sampling and watermarking, paving the way for their efficient and practical deployment.

</details>


### [858] [Learning While Staying Curious: Entropy-Preserving Supervised Fine-Tuning via Adaptive Self-Distillation for Large Reasoning Models](https://arxiv.org/abs/2602.02244)
*Hao Wang,Hao Gu,Hongming Piao,Kaixiong Gong,Yuxiao Ye,Xiangyu Yue,Sirui Han,Yike Guo,Dapeng Wu*

Main category: cs.LG

Relevance: 85.0

TL;DR: CurioSFT提出一种保持熵的监督微调方法，通过内在好奇心增强探索能力，改善后续强化学习阶段的性能


<details>
  <summary>Details</summary>
Motivation: 传统SFT-then-RL流程中，SFT阶段模仿专家演示会导致过度自信和多样性降低，限制了RL阶段的探索空间。现有熵正则化方法会使token分布趋于均匀，无法真正提升探索能力。

Method: CurioSFT包含两个核心组件：1) 自探索蒸馏：将模型蒸馏到自生成的温度缩放教师模型，鼓励在自身能力范围内探索；2) 熵引导温度选择：自适应调整蒸馏强度，在推理token上增强探索，在事实token上保持稳定。

Result: 在数学推理任务上，CurioSFT在SFT阶段比传统SFT提升2.5分（分布内）和2.9分（分布外）。探索能力的保持成功转化为RL阶段5.0分的平均提升。

Conclusion: 通过保持熵和增强内在好奇心的SFT方法，可以有效提升模型的探索能力，为后续RL阶段创造更好的起点，实现端到端性能提升。

Abstract: The standard post-training recipe for large reasoning models, supervised fine-tuning followed by reinforcement learning (SFT-then-RL), may limit the benefits of the RL stage: while SFT imitates expert demonstrations, it often causes overconfidence and reduces generation diversity, leaving RL with a narrowed solution space to explore. Adding entropy regularization during SFT is not a cure-all; it tends to flatten token distributions toward uniformity, increasing entropy without improving meaningful exploration capability. In this paper, we propose CurioSFT, an entropy-preserving SFT method designed to enhance exploration capabilities through intrinsic curiosity. It consists of (a) Self-Exploratory Distillation, which distills the model toward a self-generated, temperature-scaled teacher to encourage exploration within its capability; and (b) Entropy-Guided Temperature Selection, which adaptively adjusts distillation strength to mitigate knowledge forgetting by amplifying exploration at reasoning tokens while stabilizing factual tokens. Extensive experiments on mathematical reasoning tasks demonstrate that, in SFT stage, CurioSFT outperforms the vanilla SFT by 2.5 points on in-distribution tasks and 2.9 points on out-of-distribution tasks. We also verify that exploration capabilities preserved during SFT successfully translate into concrete gains in RL stage, yielding an average improvement of 5.0 points.

</details>


### [859] [TQL: Scaling Q-Functions with Transformers by Preventing Attention Collapse](https://arxiv.org/abs/2602.01439)
*Perry Dong,Kuo-Han Hung,Alexander Swerdlow,Dorsa Sadigh,Chelsea Finn*

Main category: cs.LG

Relevance: 85.0

TL;DR: 本文提出Transformer Q-Learning (TQL)方法，通过控制注意力分数的熵来稳定训练，解决了Transformer在强化学习价值函数中难以有效扩展的问题，实现了从最小到最大网络规模43%的性能提升。


<details>
  <summary>Details</summary>
Motivation: 尽管规模扩展在机器学习中取得了显著进展，但强化学习方法仍主要使用小型价值函数。直接将Transformer架构（已知具有高度可扩展性）应用于价值函数扩展通常会导致学习不稳定和性能下降。本文旨在探究Transformer在价值函数扩展中的关键障碍。

Method: 通过实证分析发现注意力分数崩溃是Transformer扩展失败的关键模式。提出Transformer Q-Learning (TQL)方法，核心洞察是通过控制注意力分数的熵来防止崩溃并稳定训练，从而能够使用更大的模型。

Result: TQL方法在从最小到最大网络规模的扩展中实现了高达43%的性能提升，而先前方法在扩展时会出现性能退化。

Conclusion: Transformer可以通过适当的训练稳定化技术在强化学习价值函数中有效扩展，TQL方法成功解决了注意力分数崩溃问题，为大规模价值函数学习提供了可行方案。

Abstract: Despite scale driving substantial recent advancements in machine learning, reinforcement learning (RL) methods still primarily use small value functions. Naively scaling value functions -- including with a transformer architecture, which is known to be highly scalable -- often results in learning instability and worse performance. In this work, we ask what prevents transformers from scaling effectively for value functions? Through empirical analysis, we identify the critical failure mode in this scaling: attention scores collapse as capacity increases. Our key insight is that we can effectively prevent this collapse and stabilize training by controlling the entropy of the attention scores, thereby enabling the use of larger models. To this end, we propose Transformer Q-Learning (TQL), a method that unlocks the scaling potential of transformers in learning value functions in RL. Our approach yields up to a 43% improvement in performance when scaling from the smallest to the largest network sizes, while prior methods suffer from performance degradation.

</details>


### [860] [A Meta-Knowledge-Augmented LLM Framework for Hyperparameter Optimization in Time-Series Forecasting](https://arxiv.org/abs/2602.01445)
*Ons Saadallah,Mátyás andó,Tamás Gábor Orosz*

Main category: cs.LG

Relevance: 85.0

TL;DR: LLM-AutoOpt：结合贝叶斯优化与LLM推理的混合超参数优化框架，针对时间序列预测任务，利用数据集元特征、模型描述和历史优化结果等结构化元知识，实现更优性能和可解释性。


<details>
  <summary>Details</summary>
Motivation: 传统贝叶斯优化在超参数调优中存在计算成本高、可解释性差的问题，特别是对于时间序列预测任务。同时，传统方法通常独立处理调优任务，缺乏对优化决策的深入洞察。大型语言模型的发展为将结构化先验知识和推理融入优化流程提供了新机会。

Method: 提出LLM-AutoOpt混合框架：1）将数据集元特征、模型描述、历史优化结果和目标目标编码为LLM提示中的结构化元知识；2）使用贝叶斯优化初始化搜索以缓解冷启动问题；3）结合LLM的上下文推理能力进行超参数优化，同时提供优化决策的可解释性。

Result: 在多变量时间序列预测基准测试中，LLM-AutoOpt相比纯贝叶斯优化和无元知识的LLM基线，实现了更好的预测性能和更可解释的优化行为。

Conclusion: LLM-AutoOpt通过结合贝叶斯优化和LLM推理，有效提升了超参数优化的性能和可解释性，为深度学习模型的超参数调优提供了新的混合方法。

Abstract: Hyperparameter optimization (HPO) plays a central role in the performance of deep learning models, yet remains computationally expensive and difficult to interpret, particularly for time-series forecasting. While Bayesian Optimization (BO) is a standard approach, it typically treats tuning tasks independently and provides limited insight into its decisions. Recent advances in large language models (LLMs) offer new opportunities to incorporate structured prior knowledge and reasoning into optimization pipelines. We introduce LLM-AutoOpt, a hybrid HPO framework that combines BO with LLM-based contextual reasoning. The framework encodes dataset meta-features, model descriptions, historical optimization outcomes, and target objectives as structured meta-knowledge within LLM prompts, using BO to initialize the search and mitigate cold-start effects. This design enables context-aware and stable hyperparameter refinement while exposing the reasoning behind optimization decisions. Experiments on a multivariate time series forecasting benchmark demonstrate that LLM-AutoOpt achieves improved predictive performance and more interpretable optimization behavior compared to BO and LLM baselines without meta-knowledge.

</details>


### [861] [SPARKLING: Balancing Signal Preservation and Symmetry Breaking for Width-Progressive Learning](https://arxiv.org/abs/2602.02472)
*Qifan Yu,Xinyu Ma,Zhijian Zhuo,Minrui Wang,Deyi Liu,Shiyi Zhan,Yiyuan Ma,Liang Xiang,Xingyan Bin,Di He*

Main category: cs.LG

Relevance: 85.0

TL;DR: SPARKLING提出了一种用于模型宽度扩展的新框架，解决了中阶段宽度扩展时的训练不稳定问题，通过RMS尺度一致性和非对称优化器状态重置等技术，在MoE模型上实现了高达35%的训练成本节省。


<details>
  <summary>Details</summary>
Motivation: 渐进学习通过逐步增加模型规模来减少预训练计算开销。现有研究主要关注深度扩展，而宽度扩展研究不足，特别是在训练中阶段进行宽度扩展对于最大化计算节省至关重要，但存在严重的训练不稳定问题。

Method: 提出SPARKLING框架，包含两个核心组件：1）通过RMS尺度一致性实现信号保留，稳定扩展时的激活统计；2）通过非对称优化器状态重置和学习率重新预热实现对称性打破，确保特征多样性。

Result: 在混合专家模型上的实验表明，SPARKLING在多种宽度轴和优化器家族中始终优于从头训练，在2倍宽度扩展下可减少高达35%的训练成本。

Conclusion: SPARKLING有效解决了中阶段宽度扩展的挑战，为高效的大规模模型训练提供了实用解决方案，特别适用于需要动态调整模型宽度的场景。

Abstract: Progressive Learning (PL) reduces pre-training computational overhead by gradually increasing model scale. While prior work has extensively explored depth expansion, width expansion remains significantly understudied, with the few existing methods limited to the early stages of training. However, expanding width during the mid-stage is essential for maximizing computational savings, yet it remains a formidable challenge due to severe training instabilities. Empirically, we show that naive initialization at this stage disrupts activation statistics, triggering loss spikes, while copy-based initialization introduces gradient symmetry that hinders feature diversity. To address these issues, we propose SPARKLING (balancing {S}ignal {P}reservation {A}nd symmet{R}y brea{K}ing for width-progressive {L}earn{ING}), a novel framework for mid-stage width expansion. Our method achieves signal preservation via RMS-scale consistency, stabilizing activation statistics during expansion. Symmetry breaking is ensured through asymmetric optimizer state resetting and learning rate re-warmup. Extensive experiments on Mixture-of-Experts (MoE) models demonstrate that, across multiple width axes and optimizer families, SPARKLING consistently outperforms training from scratch and reduces training cost by up to 35% under $2\times$ width expansion.

</details>


### [862] [RLAnything: Forge Environment, Policy, and Reward Model in Completely Dynamic RL System](https://arxiv.org/abs/2602.02488)
*Yinjie Wang,Tianbao Xie,Ke Shen,Mengdi Wang,Ling Yang*

Main category: cs.LG

Relevance: 85.0

TL;DR: RLAnything是一个强化学习框架，通过闭环优化动态构建环境、策略和奖励模型，增强LLM和智能体场景的学习信号和整体系统性能。


<details>
  <summary>Details</summary>
Motivation: 现有强化学习方法在LLM和智能体场景中面临学习信号弱、环境适应性差等问题，需要一种能够动态优化整个RL系统的框架来提升学习效率和性能。

Method: 1) 策略训练整合步进和结果反馈信号；2) 奖励模型通过一致性反馈联合优化；3) 基于理论的自动环境适应利用批评反馈改进训练；4) 闭环优化环境、策略和奖励模型。

Result: RLAnything在多个代表性LLM和智能体任务上取得显著提升：Qwen3-VL-8B-Thinking在OSWorld上提升9.1%，Qwen2.5-7B-Instruct在AlfWorld和LiveBench上分别提升18.7%和11.9%。优化后的奖励模型信号优于依赖人工标签的结果。

Conclusion: RLAnything通过动态闭环优化环境、策略和奖励模型，有效增强了RL系统的学习信号和整体性能，为LLM和智能体训练提供了强大的强化学习框架。

Abstract: We propose RLAnything, a reinforcement learning framework that dynamically forges environment, policy, and reward models through closed-loop optimization, amplifying learning signals and strengthening the overall RL system for any LLM or agentic scenarios. Specifically, the policy is trained with integrated feedback from step-wise and outcome signals, while the reward model is jointly optimized via consistency feedback, which in turn further improves policy training. Moreover, our theory-motivated automatic environment adaptation improves training for both the reward and policy models by leveraging critic feedback from each, enabling learning from experience. Empirically, each added component consistently improves the overall system, and RLAnything yields substantial gains across various representative LLM and agentic tasks, boosting Qwen3-VL-8B-Thinking by 9.1% on OSWorld and Qwen2.5-7B-Instruct by 18.7% and 11.9% on AlfWorld and LiveBench, respectively. We also that optimized reward-model signals outperform outcomes that rely on human labels. Code: https://github.com/Gen-Verse/Open-AgentRL

</details>


### [863] [A Statistical Theory of Gated Attention through the Lens of Hierarchical Mixture of Experts](https://arxiv.org/abs/2602.01468)
*Viet Nguyen,Tuan Minh Pham,Thinh Cao,Tan Dinh,Huy Nguyen,Nhat Ho,Alessandro Rinaldo*

Main category: cs.LG

Relevance: 85.0

TL;DR: 本文从理论角度分析门控注意力机制，证明其比多头自注意力具有更高的样本效率，通过将学习重构为专家估计问题，揭示了门控注意力仅需多项式数量样本即可准确估计专家，而多头注意力需要指数级样本。


<details>
  <summary>Details</summary>
Motivation: 门控注意力机制已被实证证明能提升Transformer性能，增强低秩映射表达能力，甚至消除注意力下沉现象。然而，尽管其效果显著，文献中仍缺乏对其优势的清晰理论理解。本文旨在填补这一理论空白，从数学上解释门控注意力为何优于标准多头自注意力。

Method: 通过严格的数学分析，证明门控注意力矩阵和多头自注意力矩阵中的每个条目都可以表示为分层混合专家模型。将学习过程重构为专家估计问题，从样本效率角度比较两种注意力机制。特别地，分析门控注意力在缩放点积注意力输出或值映射位置放置门控的理论依据。

Result: 理论分析表明：1）门控注意力比多头自注意力更样本高效，前者仅需多项式数量数据点即可准确估计专家，而后者需要指数级数据点才能达到相同估计误差；2）为门控注意力在特定位置（缩放点积注意力输出或值映射）放置门控提供了理论依据，解释了这种设计为何能带来更高性能。

Conclusion: 本文首次为门控注意力机制提供了严格的理论分析框架，揭示了其相对于标准多头自注意力的样本效率优势。研究不仅解释了门控注意力的实证成功，还为Transformer架构的优化设计提供了理论指导，特别是在注意力机制的门控位置选择方面。

Abstract: Self-attention has greatly contributed to the success of the widely used Transformer architecture by enabling learning from data with long-range dependencies. In an effort to improve performance, a gated attention model that leverages a gating mechanism within the multi-head self-attention has recently been proposed as a promising alternative. Gated attention has been empirically demonstrated to increase the expressiveness of low-rank mapping in standard attention and even to eliminate the attention sink phenomenon. Despite its efficacy, a clear theoretical understanding of gated attention's benefits remains lacking in the literature. To close this gap, we rigorously show that each entry in a gated attention matrix or a multi-head self-attention matrix can be written as a hierarchical mixture of experts. By recasting learning as an expert estimation problem, we demonstrate that gated attention is more sample-efficient than multi-head self-attention. In particular, while the former needs only a polynomial number of data points to estimate an expert, the latter requires exponentially many data points to achieve the same estimation error. Furthermore, our analysis also provides a theoretical justification for why gated attention yields higher performance when a gate is placed at the output of the scaled dot product attention or the value map rather than at other positions in the multi-head self-attention architecture.

</details>


### [864] [P-EAGLE: Parallel-Drafting EAGLE with Scalable Training](https://arxiv.org/abs/2602.01469)
*Mude Hui,Xin Huang,Jaime Campos Salas,Yue Sun,Nathan Pemberton,Xiang Song,Ashish Khetan,George Karypis*

Main category: cs.LG

Relevance: 85.0

TL;DR: P-EAGLE将EAGLE从自回归生成转换为并行多token预测，通过可学习的共享隐藏状态实现，解决了长上下文并行解码训练复杂度高的问题，在推理时获得1.10-1.36倍加速。


<details>
  <summary>Details</summary>
Motivation: 推理LLM产生更长输出，需要支持长序列的推测解码草稿模型。并行解码（每次前向预测多个token）相比顺序生成有延迟优势，但训练复杂度随序列长度和并行位置乘积呈二次方增长，使得长上下文训练不切实际。

Method: 提出P-EAGLE，通过可学习的共享隐藏状态将EAGLE从自回归转换为并行多token预测。为扩展到长上下文训练，开发了包含注意力掩码预计算和序列分区技术的框架，支持在单个序列内进行梯度累积的并行预测训练。

Result: 在vLLM中实现P-EAGLE，在GPT-OSS 120B、20B和Qwen3-Coder 30B模型上，相比自回归EAGLE-3获得1.10-1.36倍加速。

Conclusion: P-EAGLE通过创新的并行预测架构和高效训练框架，解决了长上下文并行解码的训练挑战，显著提升了推理速度。

Abstract: Reasoning LLMs produce longer outputs, requiring speculative decoding drafters trained on extended sequences. Parallel drafting - predicting multiple tokens per forward pass - offers latency benefits over sequential generation, but training complexity scales quadratically with the product of sequence length and parallel positions, rendering long-context training impractical. We present P(arallel)-EAGLE, which transforms EAGLE from autoregressive to parallel multi-token prediction via a learnable shared hidden state. To scale training to long contexts, we develop a framework featuring attention mask pre-computation and sequence partitioning techniques, enabling gradient accumulation within individual sequences for parallel-prediction training. We implement P-EAGLE in vLLM and demonstrate speedups of 1.10-1.36x over autoregressive EAGLE-3 across GPT-OSS 120B, 20B, and Qwen3-Coder 30B.

</details>


### [865] [Predicting and improving test-time scaling laws via reward tail-guided search](https://arxiv.org/abs/2602.01485)
*Muheng Li,Jian Qian,Wenlong Mou*

Main category: cs.LG

Relevance: 85.0

TL;DR: 提出一种基于尾部分布估计的测试时缩放方法SLG搜索，通过预测奖励尾部分布来指导计算资源分配，相比传统Best-of-N方法在相同计算预算下获得更高奖励


<details>
  <summary>Details</summary>
Motivation: 测试时缩放是提升大语言模型推理能力的关键途径，但传统的Best-of-N策略缺乏对N值选择、预算分配和多阶段决策的原则性指导，存在优化空间，且现有优化方法缺乏严格理论保证

Method: 通过估计奖励的尾部分布来预测LLM的缩放规律，无需穷举评估；基于此预测工具提出缩放定律引导搜索(SLG)，动态分配计算资源以识别和利用具有最高预测潜力的中间状态

Result: 理论上证明SLG相比完美信息预言机实现可忽略的遗憾，达到的预期奖励需要Best-of-N方法多项式级更大的计算预算；实验验证在不同LLM和奖励模型上，尾部引导分配在相同计算预算下始终比Best-of-N获得更高奖励

Conclusion: 提出了一种理论保证的测试时缩放优化方法，通过尾部分布预测和动态计算分配显著提升LLM推理性能，为测试时缩放提供了原则性指导框架

Abstract: Test-time scaling has emerged as a critical avenue for enhancing the reasoning capabilities of Large Language Models (LLMs). Though the straight-forward ''best-of-$N$'' (BoN) strategy has already demonstrated significant improvements in performance, it lacks principled guidance on the choice of $N$, budget allocation, and multi-stage decision-making, thereby leaving substantial room for optimization. While many works have explored such optimization, rigorous theoretical guarantees remain limited. In this work, we propose new methodologies to predict and improve scaling properties via tail-guided search. By estimating the tail distribution of rewards, our method predicts the scaling law of LLMs without the need for exhaustive evaluations. Leveraging this prediction tool, we introduce Scaling-Law Guided (SLG) Search, a new test-time algorithm that dynamically allocates compute to identify and exploit intermediate states with the highest predicted potential. We theoretically prove that SLG achieves vanishing regret compared to perfect-information oracles, and achieves expected rewards that would otherwise require a polynomially larger compute budget required when using BoN. Empirically, we validate our framework across different LLMs and reward models, confirming that tail-guided allocation consistently achieves higher reward yields than Best-of-$N$ under identical compute budgets. Our code is available at https://github.com/PotatoJnny/Scaling-Law-Guided-search.

</details>


### [866] [You Need an Encoder for Native Position-Independent Caching](https://arxiv.org/abs/2602.01519)
*Shiju Zhao,Junhao Hu,Jiaqi Zheng,Guihai Chen*

Main category: cs.LG

Relevance: 85.0

TL;DR: 论文提出COMB系统，通过引入编码器和位置无关缓存(PIC)技术，显著提升LLM推理效率，降低首token延迟51-94%，吞吐量提升3倍，同时保持准确率。


<details>
  <summary>Details</summary>
Motivation: 传统LLM的KV缓存基于前缀，对任意顺序检索的上下文处理效率低下。现有位置无关缓存(PIC)方法存在显著的准确率下降问题，限制了实际应用。

Method: 1) 在仅解码器LLM中重新引入编码器，并显式训练以支持PIC；2) 开发COMB系统，一个与现有推理框架无缝集成的PIC感知缓存系统。

Result: COMB将首token时间(TTFT)降低51-94%，吞吐量提升3倍，同时保持可比准确率。在DeepSeek-V2-Lite-Chat上的质量改进证明了其适用于其他仅解码器LLM。

Conclusion: 通过原生PIC和COMB系统，成功解决了KV缓存位置依赖问题，显著提升了LLM推理效率，为实际应用提供了可行的解决方案。

Abstract: The Key-Value (KV) cache of Large Language Models (LLMs) is prefix-based, making it highly inefficient for processing contexts retrieved in arbitrary order. Position-Independent Caching (PIC) has been proposed to enable KV reuse without positional constraints; however, existing approaches often incur substantial accuracy degradation, limiting their practical adoption. To address this issue, we propose native PIC by reintroducing the encoder to prevalent decoder-only LLMs and explicitly training it to support PIC. We further develop COMB, a PIC-aware caching system that integrates seamlessly with existing inference frameworks. Experimental results show that COMB reduces Time-to-First-Token (TTFT) by 51-94% and increases throughput by 3$\times$ with comparable accuracy. Furthermore, the quality improvement when using DeepSeek-V2-Lite-Chat demonstrates the applicability of COMB to other types of decoder-only LLMs. Our code is available at https://github.com/shijuzhao/Comb.

</details>


### [867] [When Is Rank-1 Enough? Geometry-Guided Initialization for Parameter-Efficient Fine-Tuning](https://arxiv.org/abs/2602.01522)
*Haoran Zhao,Soyeon Caren Han,Eduard Hovy*

Main category: cs.LG

Relevance: 85.0

TL;DR: 论文提出Gap-Init方法，通过几何感知初始化解决多模态大语言模型在极低秩（rank-1）LoRA微调中的不稳定性问题，使rank-1性能可匹配rank-8基线。


<details>
  <summary>Details</summary>
Motivation: 多模态大语言模型参数高效微调（PEFT）在极低秩设置（特别是rank-1 LoRA）中经常不稳定。作者发现这种不稳定性不仅源于有限容量，更源于优化对更新方向的高度敏感性：预训练视觉和文本特征形成不匹配的各向异性区域，产生主导的"模态间隙"方向，在rank-1约束下不成比例地引导早期梯度。

Method: 提出Gap-Init方法：1）分析预训练表示，识别主导早期梯度流的模态间隙轴；2）使用小型校准集估计模态间隙向量；3）将rank-1 LoRA方向与估计的模态间隙向量对齐，同时保持初始LoRA更新为零；4）保持几何感知初始化，确保优化方向与模态间隙对齐。

Result: 在多个视觉语言任务和骨干网络上，Gap-Init方法能稳定rank-1训练，性能可匹配或超越强rank-8基线。实验表明在极低秩限制下，初始对齐的重要性可与秩本身相媲美。

Conclusion: 在极低秩LoRA微调中，优化方向的对齐与秩本身同等重要。Gap-Init通过几何感知初始化解决了rank-1训练的不稳定性，为参数高效微调提供了新的视角，特别是在资源受限场景下。

Abstract: Parameter-efficient fine-tuning (PEFT) is a standard way to adapt multimodal large language models, yet extremely low-rank settings -- especially rank-1 LoRA -- are often unstable. We show that this instability is not solely due to limited capacity: in the rank-1 regime, optimization is highly sensitive to the update direction. Concretely, pretrained vision and text features form mismatched anisotropic regions, yielding a dominant "gap" direction that acts like a translation component and disproportionately steers early gradients under rank-1 constraints. Analyzing pretrained representations, we identify a modality-gap axis that dominates early gradient flow, while a random rank-1 initialization is unlikely to align with it, leading to weak gradients and training collapse. We propose Gap-Init, a geometry-aware initialization that aligns the rank-1 LoRA direction with an estimated modality-gap vector from a small calibration set, while keeping the initial LoRA update zero. Across multiple vision-language tasks and backbones, Gap-Init consistently stabilizes rank-1 training and can match or outperform strong rank-8 baselines. Our results suggest that at the extreme low-rank limit, initial alignment can matter as much as rank itself.

</details>


### [868] [InfoTok: Regulating Information Flow for Capacity-Constrained Shared Visual Tokenization in Unified MLLMs](https://arxiv.org/abs/2602.01554)
*Lv Tang,Tianyi Zheng,Bo Li,Xingyu Li*

Main category: cs.LG

Relevance: 85.0

TL;DR: 提出InfoTok，一种基于信息瓶颈原则的信息正则化视觉分词机制，用于统一多模态大语言模型，通过互信息正则化在压缩和任务相关性之间取得平衡。


<details>
  <summary>Details</summary>
Motivation: 现有统一多模态大语言模型中的共享分词设计大多是架构驱动的，缺乏明确的标准来确定分词应该保留什么信息来同时支持理解和生成任务。作者从容量受限的角度出发，认为视觉分词器在计算受限的学习环境中，分词预算应该优先考虑可重用的结构，而不是难以利用的高熵变化和冗余。

Method: 提出InfoTok，基于信息瓶颈原则的信息正则化视觉分词机制。将分词过程形式化为控制从图像到共享分词再到多模态输出的信息流，通过互信息正则化实现压缩和任务相关性之间的原则性权衡。该方法无需引入额外训练数据，可集成到三种代表性的统一多模态大语言模型中。

Result: 实验表明，InfoTok在理解和生成任务上都带来了持续改进，支持信息正则化分词作为学习统一多模态大语言模型中共享分词空间的原则性基础。

Conclusion: 信息正则化分词为统一多模态大语言模型中的共享分词空间学习提供了原则性基础，通过信息瓶颈原则在压缩和任务相关性之间取得平衡，能有效提升模型在理解和生成任务上的性能。

Abstract: Unified multimodal large language models (MLLMs) integrate image understanding and generation in a single framework, with the visual tokenizer acting as the sole interface that maps visual inputs into tokens for downstream tasks. However, existing shared-token designs are mostly architecture-driven and lack an explicit criterion for what information tokens should preserve to support both understanding and generation. Therefore, we introduce a capacity-constrained perspective, highlighting that in shared-token unified MLLMs the visual tokenizer behaves as a compute-bounded learner, so the token budget should prioritize reusable structure over hard-to-exploit high-entropy variations and redundancy. Motivated by this perspective, we propose InfoTok, an information-regularized visual tokenization mechanism grounded in the Information Bottleneck (IB) principle. InfoTok formulates tokenization as controlling information flow from images to shared tokens to multimodal outputs, yielding a principled trade-off between compression and task relevance via mutual-information regularization. We integrate InfoTok into three representative unified MLLMs without introducing any additional training data. Experiments show consistent improvements on both understanding and generation, supporting information-regularized tokenization as a principled foundation for learning a shared token space in unified MLLMs.

</details>


### [869] [How Implicit Bias Accumulates and Propagates in LLM Long-term Memory](https://arxiv.org/abs/2602.01558)
*Yiming Ma,Lixu Wang,Lionel Z. Wang,Hongkun Yang,Haoming Sun,Xin Xu,Jiaqi Wu,Bin Chen,Wei Dong*

Main category: cs.LG

Relevance: 85.0

TL;DR: 本文研究了具有长期记忆机制的LLM中隐性偏见的积累与传播问题，提出了DIB基准测试，并开发了动态记忆标记(DMT)方法来缓解偏见积累。


<details>
  <summary>Details</summary>
Motivation: LLM的长期记忆机制虽然能保持交互连续性和个性化，但也引入了新的公平性风险。目前对LLM中隐性偏见如何随时间积累和跨领域传播的研究不足，需要系统性的分析框架和缓解策略。

Method: 1) 提出决策性隐性偏见(DIB)基准，包含9个社会领域的3776个决策场景；2) 使用长期模拟框架评估6个SOTA LLM与3种记忆架构；3) 提出动态记忆标记(DMT)方法，在记忆写入时强制执行公平性约束。

Result: 实验表明：LLM的隐性偏见不会保持静态，而是随时间加剧并在不相关领域间传播；静态系统级提示的缓解效果有限且短暂；DMT方法能显著减少偏见积累并有效抑制跨领域偏见传播。

Conclusion: 长期记忆机制会加剧LLM中的隐性偏见问题，需要动态干预策略。DMT方法通过在记忆写入时施加公平性约束，能有效缓解偏见积累和传播，为构建更公平的长期记忆LLM提供了可行方案。

Abstract: Long-term memory mechanisms enable Large Language Models (LLMs) to maintain continuity and personalization across extended interaction lifecycles, but they also introduce new and underexplored risks related to fairness. In this work, we study how implicit bias, defined as subtle statistical prejudice, accumulates and propagates within LLMs equipped with long-term memory. To support systematic analysis, we introduce the Decision-based Implicit Bias (DIB) Benchmark, a large-scale dataset comprising 3,776 decision-making scenarios across nine social domains, designed to quantify implicit bias in long-term decision processes. Using a realistic long-horizon simulation framework, we evaluate six state-of-the-art LLMs integrated with three representative memory architectures on DIB and demonstrate that LLMs' implicit bias does not remain static but intensifies over time and propagates across unrelated domains. We further analyze mitigation strategies and show that a static system-level prompting baseline provides limited and short-lived debiasing effects. To address this limitation, we propose Dynamic Memory Tagging (DMT), an agentic intervention that enforces fairness constraints at memory write time. Extensive experimental results show that DMT substantially reduces bias accumulation and effectively curtails cross-domain bias propagation.

</details>


### [870] [Generative Visual Code Mobile World Models](https://arxiv.org/abs/2602.01576)
*Woosung Koh,Sungjun Han,Segyu Lee,Se-Young Yun,Jamin Shin*

Main category: cs.LG

Relevance: 85.0

TL;DR: 提出了gWorld——首个基于可渲染代码生成的视觉移动GUI世界模型，通过VLM预测可执行的网页代码而非直接生成像素，结合了文本模型的精确性和视觉模型的高保真度。


<details>
  <summary>Details</summary>
Motivation: 当前移动GUI世界模型面临关键权衡：基于文本的模型牺牲视觉保真度，而视觉模型无法精确渲染文本，依赖缓慢复杂的外部模型管道。需要一种结合两者优势的新范式。

Method: 提出视觉世界建模通过可渲染代码生成的新范式：使用单一视觉语言模型预测下一个GUI状态为可执行的网页代码（而非直接生成像素）。开发了gWorld（8B, 32B）模型和自动合成代码训练数据的框架。

Result: 在4个分布内和2个分布外基准测试中，gWorld在准确性与模型大小方面建立了新的帕累托前沿，优于8个前沿开源模型（最大达50.25倍）。分析显示：1) 数据扩展带来显著增益；2) 管道各组件提升数据质量；3) 更强的世界建模改善下游GUI策略性能。

Conclusion: 通过可渲染代码生成的视觉世界建模范式有效解决了现有方法的局限性，gWorld模型在移动GUI任务中实现了最先进的性能，为GUI智能体提供了强大的基础。

Abstract: Mobile Graphical User Interface (GUI) World Models (WMs) offer a promising path for improving mobile GUI agent performance at train- and inference-time. However, current approaches face a critical trade-off: text-based WMs sacrifice visual fidelity, while the inability of visual WMs in precise text rendering led to their reliance on slow, complex pipelines dependent on numerous external models. We propose a novel paradigm: visual world modeling via renderable code generation, where a single Vision-Language Model (VLM) predicts the next GUI state as executable web code that renders to pixels, rather than generating pixels directly. This combines the strengths of both approaches: VLMs retain their linguistic priors for precise text rendering while their pre-training on structured web code enables high-fidelity visual generation. We introduce gWorld (8B, 32B), the first open-weight visual mobile GUI WMs built on this paradigm, along with a data generation framework (gWorld) that automatically synthesizes code-based training data. In extensive evaluation across 4 in- and 2 out-of-distribution benchmarks, gWorld sets a new pareto frontier in accuracy versus model size, outperforming 8 frontier open-weight models over 50.25x larger. Further analyses show that (1) scaling training data via gWorld yields meaningful gains, (2) each component of our pipeline improves data quality, and (3) stronger world modeling improves downstream mobile GUI policy performance.

</details>


### [871] [Nearly Optimal Active Preference Learning and Its Application to LLM Alignment](https://arxiv.org/abs/2602.01581)
*Yao Zhao,Kwang-Sung Jun*

Main category: cs.LG

Relevance: 85.0

TL;DR: 本文提出针对偏好学习的主动学习算法，解决现有实验设计标准（如G-或D-最优性）不适用于偏好学习结构的问题，通过问题特定的算法设计提高样本效率。


<details>
  <summary>Details</summary>
Motivation: 对齐大型语言模型需要高质量的人类偏好标注数据，但收集成本高昂。现有主动学习方法多采用经典的实验设计标准（如G-或D-最优性），这些目标并未针对偏好学习结构进行定制，需要设计问题特定的算法。

Method: 提出了两种主动学习算法：1）第一个算法为该设置提供了首个实例依赖的标签复杂度保证；2）第二个是简单实用的贪婪方法。两种算法都基于对偏好学习特定直觉的洞察。

Result: 在真实世界偏好数据集上评估算法，相比现有方法观察到样本效率的改进。

Conclusion: 针对偏好学习结构设计的主动学习算法能够有效提高样本效率，为LLM对齐的数据收集提供了更高效的方法。

Abstract: Aligning large language models (LLMs) depends on high-quality datasets of human preference labels, which are costly to collect. Although active learning has been studied to improve sample efficiency relative to passive collection, many existing approaches adopt classical experimental design criteria such as G- or D-optimality. These objectives are not tailored to the structure of preference learning, leaving open the design of problem-specific algorithms. In this work, we identify a simple intuition specific to preference learning that calls into question the suitability of these existing design objectives. Motivated by this insight, we propose two active learning algorithms. The first provides the first instance-dependent label complexity guarantee for this setting, and the second is a simple, practical greedy method. We evaluate our algorithm on real-world preference datasets and observe improved sample efficiency compared to existing methods.

</details>


### [872] [The Multiple Ticket Hypothesis: Random Sparse Subnetworks Suffice for RLVR](https://arxiv.org/abs/2602.01599)
*Israel Adewuyi,Solomon Okibe,Vladmir Ivanov*

Main category: cs.LG

Relevance: 85.0

TL;DR: 该论文提出"多重彩票假设"，发现在RLVR微调中，仅随机训练1%的参数就能达到或超过全参数训练效果，表明预训练模型包含许多可行的稀疏子网络而非单一特权子集。


<details>
  <summary>Details</summary>
Motivation: 彩票假设表明稀疏子网络能达到完整模型性能，而RLVR中参数更新集中在稀疏子集上，这暗示了参数冗余。研究者希望探索利用这种冗余的最简单方式：在极端稀疏度下仅训练随机选择的参数子集。

Method: 采用最简单的方法：在RLVR微调中仅训练随机选择的参数子集（极端稀疏度如1%）。通过实验验证不同随机掩码的效果，并分析RLVR中隐式的每步KL约束如何限制更新到低维子空间。

Result: 仅训练1%的参数就能匹配或超过全参数RLVR微调，在3个模型和2个任务领域都得到验证。不同随机掩码的重叠度极低（Jaccard相似度≤0.005），但都能成功，支持"多重彩票假设"。

Conclusion: 预训练模型包含许多可行的稀疏子网络而非单一特权子集，RLVR中的隐式KL约束将更新限制在低维子空间，使得任意稀疏掩码都能成功。这为高效微调提供了新视角。

Abstract: The Lottery Ticket Hypothesis demonstrated that sparse subnetworks can match full-model performance, suggesting parameter redundancy. Meanwhile, in Reinforcement Learning with Verifiable Rewards (RLVR), recent work has shown that updates concentrate on a sparse subset of parameters, which further lends evidence to this underlying redundancy. We study the simplest possible way to exploit this redundancy: training only a randomly selected subset of parameters at extreme sparsities. Empirically, we find that training just 1\% of parameters matches or exceeds full-parameter RLVR finetuning across 3 models and 2 task domains. Moreover, different random masks show minimal overlap ($\leq 0.005$ Jaccard similarity) and yet all succeed, suggesting pretrained models contain many viable sparse subnetworks rather than one privileged set. We term this the Multiple Ticket Hypothesis. We explain this phenomenon through the implicit per-step KL constraint in RLVR, which restricts updates to a low-dimensional subspace, enabling arbitrary sparse masks to succeed.

</details>


### [873] [What Do Agents Learn from Trajectory-SFT: Semantics or Interfaces?](https://arxiv.org/abs/2602.01611)
*Weizheng Gu,Chengze Li,Zhuohao Yu,Mengyuan Sun,Zhibang Yang,Wei Wang,Hongrui Jia,Shikun Zhang,Wei Ye*

Main category: cs.LG

Relevance: 85.0

TL;DR: 该论文提出了PIPE评估协议，用于诊断LLM智能体在交互任务中是否过度依赖特定界面模式而非真正的语义理解，发现轨迹监督微调会显著增加界面捷径学习


<details>
  <summary>Details</summary>
Motivation: 当前LLM智能体评估存在混淆问题：智能体成功可能源于真正的语义工具使用能力，也可能只是记忆了特定界面的交互模式。标准评估无法区分这两种机制，导致无法准确评估智能体的环境不变能力。

Method: 提出PIPE协议级评估增强方法：通过最小化重写环境界面（保持任务语义和执行行为不变）来诊断界面依赖。引入界面依赖度（IR）指标，量化智能体对训练时界面的偏好程度。

Result: 在16个AgentBench和AgentGym环境中测试发现：轨迹监督微调显著放大了界面捷径学习，训练后的智能体在最小界面重写下性能急剧下降，而非轨迹训练的模型保持稳定。界面捷径学习表现出环境依赖、非单调的训练动态。

Conclusion: 标准智能体评估存在识别性问题，无法区分语义能力和界面模式记忆。轨迹监督微调可能导致智能体过度学习界面捷径而非真正的语义理解。PIPE协议为评估智能体的环境不变能力提供了重要诊断工具。

Abstract: Large language models are increasingly evaluated as interactive agents, yet standard agent benchmarks conflate two qualitatively distinct sources of success: semantic tool-use and interface-specific interaction pattern memorization. Because both mechanisms can yield identical task success on the original interface, benchmark scores alone are not identifiable evidence of environment-invariant capability. We propose PIPE, a protocol-level evaluation augmentation for diagnosing interface reliance by minimally rewriting environment interfaces while preserving task semantics and execution behavior. Across 16 environments from AgentBench and AgentGym and a range of open-source and API-based agents, PIPE reveals that trajectory-SFT substantially amplifies interface shortcutting: trained agents degrade sharply under minimal interface rewrites, while non-trajectory-trained models remain largely stable. We further introduce Interface Reliance (IR), a counterbalanced alias-based metric that quantifies preference for training-time interfaces, and show that interface shortcutting exhibits environment-dependent, non-monotonic training dynamics that remain invisible under standard evaluation. Our code is available at https://anonymous.4open.science/r/What-Do-Agents-Learn-from-Trajectory-SFT-Semantics-or-Interfaces--0831/.

</details>


### [874] [A Practical Tensor-Network Compression Pipeline for Production-Scale Large Language Models](https://arxiv.org/abs/2602.01613)
*Sergii Kozyrev,Davyd Maiboroda*

Main category: cs.LG

Relevance: 85.0

TL;DR: Minima是一个生产级压缩流水线，通过结构压缩Transformer来减少GPU内存占用和推理延迟，结合多种张量分解方法、轻量级预测器和自定义内核，支持推测解码实现吞吐量提升。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型在部署时受到GPU内存和推理延迟的限制，需要有效的压缩方法来减少资源消耗同时保持性能。

Method: 1) 训练轻量级卷积预测器评估层和patch级别的敏感性；2) 对低敏感性区域应用Tucker、张量链和张量环分解；3) 进行短期恢复微调；4) 使用自定义Triton和CUDA内核执行；5) 结合推测解码技术。

Result: 在Qwen3-32B模型上，8k上下文窗口下：峰值VRAM从64GiB降至40GiB；单请求吞吐量从40tokens/s提升至50tokens/s（Minima）和75tokens/s（Minima+推测解码）；50并行请求下吞吐量为34/44/53tokens/s。

Conclusion: Minima是一个实用的结构压缩方法，通过共享张量主干和微小层适配器实现更激进的结构压缩，在高并发下仍保持有效。

Abstract: Large language models are limited in deployment by GPU memory and inference latency. We present Minima, a production compression pipeline that learns where and how to structurally compress a Transformer and turns that compression into real serving gains. Minima trains a lightweight convolutional predictor to estimate layer- and patch-level sensitivity, applies a mixture of Tucker, tensor-train, and tensor-ring decompositions to low-sensitivity regions, performs a short healing fine-tune, and executes the resulting operators with custom Triton and CUDA kernels. The reduced memory footprint enables speculative decoding with a small draft model and a larger verifier. On Qwen3-32B at an 8k-token context window, Minima reduces peak VRAM from 64 GiB to 40 GiB. For a single active request, throughput increases from 40 tokens per second (baseline) to 50 tokens per second (Minima) and 75 tokens per second (Minima with speculative decoding). Under 50 parallel requests, throughput is 34, 44, and 53 tokens per second respectively, showing that Minima remains effective under high concurrency even when speculative decoding gains compress. We position Minima relative to recent tensor-network, low-rank plus quantization, and cross-layer sharing methods, and argue that it is a practical step toward more aggressive structural compression via shared tensor backbones with tiny per-layer adapters.

</details>


### [875] [Chance-Constrained Inference for Hallucination Risk Control in Large Language Models](https://arxiv.org/abs/2602.01637)
*Sreenivasan Mohandas*

Main category: cs.LG

Relevance: 85.0

TL;DR: 提出机会约束推理框架，通过序列化验证过程直接控制语言模型生成中的幻觉概率，提供部署时的风险保证


<details>
  <summary>Details</summary>
Motivation: 现有方法只能降低平均错误率，但无法在重复使用中明确控制幻觉发生的频率。需要一种能够提供概率风险保证的推理方法，确保在部署时能够约束幻觉概率。

Method: 将推理建模为部署时风险控制问题，提出机会约束推理框架。将幻觉视为随机约束违反，设计序列化、随时有效的推理过程，通过有限样本自适应地验证可行性或不可行性，避免保守的固定样本边界。

Result: 在NaturalQuestions风格问题和受控多跳问答上的实验表明，该方法能够可靠地控制风险，早期检测本质上不可行的输入，并在重复使用下实现安全组合。基于置信度的基线方法无法提供一致的保证。

Conclusion: 机会约束推理为语言模型部署提供了明确的概率风险控制，能够直接约束幻觉频率，相比传统置信度方法提供更可靠的保证。

Abstract: Large language models generate outputs stochastically and may produce fluent but invalid responses, including factual hallucinations. Existing mitigation strategies reduce average error rates but do not provide explicit control over the \emph{frequency} of such failures under repeated use. We formulate inference as a deployment-time risk control problem and introduce \emph{chance-constrained inference}, which directly bounds the probability of hallucinations among accepted generations. Hallucinations are modeled as stochastic constraint violations, and we show that confidence-based selective prediction does not, in general, imply probabilistic risk guarantees. To enforce chance constraints efficiently, we propose a sequential, anytime-valid inference procedure that adaptively certifies feasibility or infeasibility using finite samples, avoiding conservative fixed-sample bounds. Experiments on questions inspired by NaturalQuestions and controlled multi-hop question answering demonstrate reliable risk control, early detection of intrinsically infeasible inputs, and safe composition under repeated use, while confidence-based baselines fail to provide consistent guarantees.

</details>


### [876] [On the Spatiotemporal Dynamics of Generalization in Neural Networks](https://arxiv.org/abs/2602.01651)
*Zichao Wei*

Main category: cs.LG

Relevance: 85.0

TL;DR: 论文提出SEAD架构，从物理原理（局域性、对称性、稳定性）推导出神经网络架构，解决数字加法等任务的长度泛化问题，实现从16位到100万位的完美泛化。


<details>
  <summary>Details</summary>
Motivation: 神经网络在数字加法等任务上无法从短序列泛化到长序列（如16位到32位），而人类儿童学习规则后可以泛化到任意长度。作者认为这不是工程问题，而是违反了物理基本原理。

Method: 从三个物理原理（局域性、对称性、稳定性）推导出SEAD架构：时空演化吸引子动力学，一种神经细胞自动机，通过局部卷积规则迭代直到收敛。

Result: 在三个任务上验证：1) 奇偶性任务：通过光锥传播实现完美长度泛化；2) 加法任务：从L=16到L=100万实现100%准确率，展现输入自适应计算；3) Rule 110：学习图灵完备的细胞自动机而无轨迹发散。

Conclusion: 统计学习与逻辑推理之间的鸿沟可以通过尊重计算物理原理来弥合，而不是通过扩展参数规模。SEAD架构展示了从物理原理推导架构的有效性。

Abstract: Why do neural networks fail to generalize addition from 16-digit to 32-digit numbers, while a child who learns the rule can apply it to arbitrarily long sequences? We argue that this failure is not an engineering problem but a violation of physical postulates. Drawing inspiration from physics, we identify three constraints that any generalizing system must satisfy: (1) Locality -- information propagates at finite speed; (2) Symmetry -- the laws of computation are invariant across space and time; (3) Stability -- the system converges to discrete attractors that resist noise accumulation. From these postulates, we derive -- rather than design -- the Spatiotemporal Evolution with Attractor Dynamics (SEAD) architecture: a neural cellular automaton where local convolutional rules are iterated until convergence. Experiments on three tasks validate our theory: (1) Parity -- demonstrating perfect length generalization via light-cone propagation; (2) Addition -- achieving scale-invariant inference from L=16 to L=1 million with 100% accuracy, exhibiting input-adaptive computation; (3) Rule 110 -- learning a Turing-complete cellular automaton without trajectory divergence. Our results suggest that the gap between statistical learning and logical reasoning can be bridged -- not by scaling parameters, but by respecting the physics of computation.

</details>


### [877] [Efficient Adversarial Attacks on High-dimensional Offline Bandits](https://arxiv.org/abs/2602.01658)
*Seyed Mohammad Hadi Hosseini,Amir Najafi,Mahdieh Soleymani Baghshah*

Main category: cs.LG

Relevance: 85.0

TL;DR: 该论文研究了离线bandit评估中奖励模型对抗性攻击的脆弱性，发现在高维设置下，即使对奖励模型权重进行微小扰动也能显著改变bandit行为，使现代图像评估等应用特别容易受到攻击。


<details>
  <summary>Details</summary>
Motivation: Bandit算法已成为评估机器学习模型（包括生成图像模型和大语言模型）的强大工具，但离线bandit评估的对抗鲁棒性尚未得到充分研究。特别是当攻击者在bandit训练前扰动奖励模型（而非训练数据）时，系统的脆弱性尚不明确。

Method: 研究从线性奖励函数扩展到非线性模型（如ReLU神经网络），针对Hugging Face上用于生成模型评估的两个评估器（美学质量评估和组合对齐评估）进行攻击分析。理论分析证明了高维效应：随着输入维度增加，成功攻击所需的扰动范数减少。

Result: 实验表明，即使对奖励模型权重进行微小、难以察觉的扰动，也能显著改变bandit行为。随机扰动无效，但针对性扰动能实现接近完美的攻击成功率。高维设置使现代应用（如图像评估）特别脆弱。

Conclusion: 离线bandit评估对奖励模型的对抗性攻击高度脆弱，特别是在高维设置下。这揭示了当前评估方法的安全漏洞，对LLM评估和基准测试具有重要意义。

Abstract: Bandit algorithms have recently emerged as a powerful tool for evaluating machine learning models, including generative image models and large language models, by efficiently identifying top-performing candidates without exhaustive comparisons. These methods typically rely on a reward model, often distributed with public weights on platforms such as Hugging Face, to provide feedback to the bandit. While online evaluation is expensive and requires repeated trials, offline evaluation with logged data has become an attractive alternative. However, the adversarial robustness of offline bandit evaluation remains largely unexplored, particularly when an attacker perturbs the reward model (rather than the training data) prior to bandit training. In this work, we fill this gap by investigating, both theoretically and empirically, the vulnerability of offline bandit training to adversarial manipulations of the reward model. We introduce a novel threat model in which an attacker exploits offline data in high-dimensional settings to hijack the bandit's behavior. Starting with linear reward functions and extending to nonlinear models such as ReLU neural networks, we study attacks on two Hugging Face evaluators used for generative model assessment: one measuring aesthetic quality and the other assessing compositional alignment. Our results show that even small, imperceptible perturbations to the reward model's weights can drastically alter the bandit's behavior. From a theoretical perspective, we prove a striking high-dimensional effect: as input dimensionality increases, the perturbation norm required for a successful attack decreases, making modern applications such as image evaluation especially vulnerable. Extensive experiments confirm that naive random perturbations are ineffective, whereas carefully targeted perturbations achieve near-perfect attack success rates ...

</details>


### [878] [Semantic-aware Wasserstein Policy Regularization for Large Language Model Alignment](https://arxiv.org/abs/2602.01685)
*Byeonghu Na,Hyungho Na,Yeongmin Kim,Suhyeon Jo,HeeSun Bae,Mina Kang,Il-Chul Moon*

Main category: cs.LG

Relevance: 85.0

TL;DR: 提出Wasserstein Policy Regularization (WPR)，一种基于熵正则化Wasserstein距离的语义感知正则化方法，用于改进RLHF框架中的策略对齐


<details>
  <summary>Details</summary>
Motivation: 传统RLHF使用KL散度及其f-散度变体作为正则化，但这些方法只比较相同位置的token概率，无法捕捉语义相似性。需要一种能够考虑token空间几何结构的语义感知正则化方法

Method: 提出Wasserstein Policy Regularization (WPR)，基于熵正则化Wasserstein距离，通过最优对偶变量将正则化表示为奖励惩罚项，得到与标准RL算法兼容的可处理目标

Result: 实验表明，WPR方法优于基于KL散度和f-散度的基线方法，证明了语义感知策略距离在模型对齐中的优势

Conclusion: WPR通过引入Wasserstein距离的几何结构，提供了更有效的语义感知正则化，改进了RLHF框架中的策略对齐效果

Abstract: Large language models (LLMs) are commonly aligned with human preferences using reinforcement learning from human feedback (RLHF). In this method, LLM policies are generally optimized through reward maximization with Kullback-Leibler (KL) divergence regularization of the reference policy. However, KL and its $f$-divergence variants only compare token probabilities at identical indices, failing to capture semantic similarity. We propose Wasserstein Policy Regularization (WPR), a semantic-aware regularization for the RLHF framework based on the entropy-regularized Wasserstein distance, which incorporates the geometry of the token space. The dual formulation of the distance expresses the regularization as penalty terms applied to the reward via optimal dual variables, which yield a tractable objective compatible with standard RL algorithms. Empirically, our method outperforms KL- and $f$-divergence-based baselines, demonstrating the benefits of semantic-aware policy distances for alignment. Our code is available at https://github.com/aailab-kaist/WPR.

</details>


### [879] [Beyond Mode Elicitation: Diversity-Preserving Reinforcement Learning via Latent Diffusion Reasoner](https://arxiv.org/abs/2602.01705)
*Haoqiang Kang,Yizhe Zhang,Nikki Lijing Kuang,Yi-An Ma,Lianhui Qin*

Main category: cs.LG

Relevance: 85.0

TL;DR: LaDi-RL：一种在连续潜空间进行探索的强化学习框架，通过引导扩散模型优化LLM推理，避免离散RL中的多样性崩溃问题


<details>
  <summary>Details</summary>
Motivation: 传统强化学习方法在优化LLM的离散思维链生成时，由于策略熵降低和模式激发行为，容易导致多样性崩溃。需要在连续空间进行更有效的探索。

Method: 提出LaDi-RL框架：1）在连续潜空间进行探索，潜变量编码语义级推理轨迹；2）通过引导扩散模型建模探索，多步去噪分布随机性并保留多个共存解模式；3）解耦潜空间探索和文本空间生成，结合潜空间优化和文本策略。

Result: 在代码生成和数学推理基准测试中，相比离散RL基线，pass@1和pass@k均有显著提升：代码生成pass@1绝对提升+9.4%，数学推理pass@1绝对提升+5.7%。

Conclusion: 基于扩散的潜空间强化学习是离散词元级RL的有原则替代方案，能更有效地优化LLM推理，同时保持解多样性。

Abstract: Recent reinforcement learning (RL) methods improve LLM reasoning by optimizing discrete Chain-of-Thought (CoT) generation; however, exploration in token space often suffers from diversity collapse as policy entropy decreases due to mode elicitation behavior in discrete RL. To mitigate this issue, we propose Latent Diffusion Reasoning with Reinforcement Learning (LaDi-RL), a framework that conducts exploration directly in a continuous latent space, where latent variables encode semantic-level reasoning trajectories. By modeling exploration via guided diffusion, multi-step denoising distributes stochasticity and preserves multiple coexisting solution modes without mutual suppression. Furthermore, by decoupling latent-space exploration from text-space generation, we show that latent diffusion-based optimization is more effective than text-space policy optimization alone, while a complementary text policy provides additional gains when combined with latent exploration. Experiments on code generation and mathematical reasoning benchmarks demonstrate consistent improvements in both pass@1 and pass@k over discrete RL baselines, with absolute pass@1 gains of +9.4% on code generation and +5.7% on mathematical reasoning, highlighting diffusion-based latent RL as a principled alternative to discrete token-level RL for reasoning.

</details>


### [880] [Revisiting Generalization Measures Beyond IID: An Empirical Study under Distributional Shift](https://arxiv.org/abs/2602.01718)
*Sora Nakai,Youssef Fadhloun,Kacem Mathlouthi,Kotaro Yoshida,Ganesh Talluri,Ioannis Mitliagkas,Hiroki Naganuma*

Main category: cs.LG

Relevance: 85.0

TL;DR: 该论文系统评估了深度学习模型在分布偏移下的泛化能力预测指标，通过大规模实验发现只有少数指标在不同设置下保持稳定。


<details>
  <summary>Details</summary>
Motivation: 深度学习中的泛化问题仍未解决，特别是在预测模型在训练分布之外的性能方面。现有研究主要关注IID设置，缺乏对分布偏移下泛化指标鲁棒性的系统评估。

Method: 在10,000个超参数配置上训练中小型模型，评估40多个仅从训练模型和训练数据可计算的泛化指标。扩展实验范围：包括分布偏移评估、多种架构和训练方案、以及新加入基于校准和信息准则的指标。

Result: 分布偏移显著改变了许多泛化指标的预测性能，只有较小的子集在不同设置下保持相对稳定。这表明大多数现有指标对分布变化敏感。

Conclusion: 泛化指标的鲁棒性评估至关重要，特别是在实际应用中面临分布偏移时。需要开发更稳定的指标来可靠预测模型在未见数据上的性能。

Abstract: Generalization remains a central yet unresolved challenge in deep learning, particularly the ability to predict a model's performance beyond its training distribution using quantities available prior to test-time evaluation. Building on the large-scale study of Jiang et al. (2020). and concerns by Dziugaite et al. (2020). about instability across training configurations, we benchmark the robustness of generalization measures beyond IID regime. We train small-to-medium models over 10,000 hyperparameter configurations and evaluate more than 40 measures computable from the trained model and the available training data alone. We significantly broaden the experimental scope along multiple axes: (i) extending the evaluation beyond the standard IID setting to include benchmarking for robustness across diverse distribution shifts, (ii) evaluating multiple architectures and training recipes, and (iii) newly incorporating calibration- and information-criteria-based measures to assess their alignment with both IID and OOD generalization. We find that distribution shifts can substantially alter the predictive performance of many generalization measures, while a smaller subset remains comparatively stable across settings.

</details>


### [881] [MSign: An Optimizer Preventing Training Instability in Large Language Models via Stable Rank Restoration](https://arxiv.org/abs/2602.01734)
*Lianhai Ren,Yucheng Ding,Xiao Liu,Qianxiao Li,Peng Cheng,Yeyun Gong*

Main category: cs.LG

Relevance: 85.0

TL;DR: 提出MSign优化器，通过周期性应用矩阵符号操作恢复稳定秩，解决LLM预训练中的梯度爆炸问题，在5M到3B参数模型上有效防止训练失败，计算开销低于7%


<details>
  <summary>Details</summary>
Motivation: 大型语言模型预训练中的训练不稳定性是一个关键挑战，表现为突然的梯度爆炸，浪费大量计算资源。研究旨在识别训练崩溃前的关键现象，并提出有效的解决方案。

Method: 在5M参数的NanoGPT模型上通过μP缩放研究训练失败，识别出两个关键现象：权重矩阵稳定秩的快速下降和相邻层雅可比矩阵对齐度的增加。提出MSign优化器，周期性应用矩阵符号操作来恢复稳定秩。

Result: 理论证明这两个条件共同导致梯度范数随网络深度呈指数增长。实验表明MSign在5M到3B参数的模型上能有效防止训练失败，计算开销低于7.0%。

Conclusion: MSign优化器通过解决训练不稳定性问题，为大型语言模型预训练提供了有效的解决方案，能够显著减少计算资源浪费。

Abstract: Training instability remains a critical challenge in large language model (LLM) pretraining, often manifesting as sudden gradient explosions that waste significant computational resources. We study training failures in a 5M-parameter NanoGPT model scaled via $μ$P, identifying two key phenomena preceding collapse: (1) rapid decline in weight matrix stable rank (ratio of squared Frobenius norm to squared spectral norm), and (2) increasing alignment between adjacent layer Jacobians. We prove theoretically that these two conditions jointly cause exponential gradient norm growth with network depth. To break this instability mechanism, we propose MSign, a new optimizer that periodically applies matrix sign operations to restore stable rank. Experiments on models from 5M to 3B parameters demonstrate that MSign effectively prevents training failures with a computational overhead of less than 7.0%.

</details>


### [882] [Softmax Linear Attention: Reclaiming Global Competition](https://arxiv.org/abs/2602.01744)
*Mingwei Xu,Xuan Lin,Xinnan Guo,Wanqing Xu,Wanyun Cui*

Main category: cs.LG

Relevance: 85.0

TL;DR: SLA（Softmax Linear Attention）通过将softmax操作从token级别提升到head级别，在线性注意力中恢复了全局竞争机制，在保持线性复杂度的同时提升了表达能力和长上下文理解能力。


<details>
  <summary>Details</summary>
Motivation: 线性注意力虽然将Transformer的二次复杂度降低到线性，但由于移除了softmax归一化，失去了全局竞争机制，导致在表达能力和长上下文噪声处理上表现不佳。需要一种方法既能保持线性效率，又能恢复这种竞争性选择能力。

Method: 提出Softmax Linear Attention（SLA）框架，将softmax操作从token级别提升到attention head级别。利用attention head作为粗粒度语义槽，应用竞争门控机制动态选择最相关的子空间，恢复"赢家通吃"的动态特性。

Result: 在语言建模和长上下文基准测试中，SLA持续提升了最先进的线性基线模型（RetNet、GLA、GDN）的性能，特别是在具有挑战性的检索场景中显著提升了抗噪声鲁棒性。

Conclusion: SLA通过head级别的竞争机制成功在线性注意力中恢复了全局竞争，实现了精确检索和鲁棒的长上下文理解，同时保持了线性复杂度，为高效Transformer架构设计提供了新思路。

Abstract: While linear attention reduces the quadratic complexity of standard Transformers to linear time, it often lags behind in expressivity due to the removal of softmax normalization. This omission eliminates \emph{global competition}, a critical mechanism that enables models to sharply focus on relevant information amidst long-context noise. In this work, we propose \textbf{Softmax Linear Attention (SLA)}, a framework designed to restore this competitive selection without sacrificing efficiency. By lifting the softmax operation from the token level to the head level, SLA leverages attention heads as coarse semantic slots, applying a competitive gating mechanism to dynamically select the most relevant subspaces. This reintroduces the ``winner-take-all'' dynamics essential for precise retrieval and robust long-context understanding. Distinct from prior methods that focus on refining local kernel functions, SLA adopts a broader perspective by exploiting the higher-level multi-head aggregation structure. Extensive experiments demonstrate that SLA consistently enhances state-of-the-art linear baselines (RetNet, GLA, GDN) across language modeling and long-context benchmarks, particularly in challenging retrieval scenarios where it significantly boosts robustness against noise, validating its capability to restore precise focus while maintaining linear complexity.

</details>


### [883] [Probability-Entropy Calibration: An Elastic Indicator for Adaptive Fine-tuning](https://arxiv.org/abs/2602.01745)
*Wenhao Yu,Shaohang Wei,Jiahong Liu,Yifan Li,Minda Hu,Aiwei Liu,Hao Zhang,Irwin King*

Main category: cs.LG

Relevance: 85.0

TL;DR: RankTuner提出了一种基于概率-熵校准的token级重加权方法，通过相对排名指标识别真正需要学习的token，在数学推理和代码生成任务上优于仅使用概率或熵的基线方法。


<details>
  <summary>Details</summary>
Motivation: 现有的token级重加权方法存在局限性：仅使用真实概率会忽略预训练先验带来的内在不确定性，而仅使用熵则会忽略目标特定的对齐信息。这导致可能将噪声或易替换的token错误识别为学习关键点。

Method: 提出相对排名指标，比较真实token在预测分布中的排名与其期望排名。使用该指标的倒数作为相对尺度对微调目标进行token级重加权，专注于真正未充分学习的token，避免过度惩罚内在不确定的位置。

Result: 在多个骨干模型上，数学推理基准测试显示一致改进，在分布外推理任务上获得迁移增益，在代码生成性能上优于仅使用概率或熵的重加权基线方法。

Conclusion: 概率-熵校准信号能更有效地识别需要学习的token，相对排名指标为token级重加权提供了更精细的控制机制，在监督微调中实现了更好的性能。

Abstract: Token-level reweighting is a simple yet effective mechanism for controlling supervised fine-tuning, but common indicators are largely one-dimensional: the ground-truth probability reflects downstream alignment, while token entropy reflects intrinsic uncertainty induced by the pre-training prior. Ignoring entropy can misidentify noisy or easily replaceable tokens as learning-critical, while ignoring probability fails to reflect target-specific alignment. RankTuner introduces a probability--entropy calibration signal, the Relative Rank Indicator, which compares the rank of the ground-truth token with its expected rank under the prediction distribution. The inverse indicator is used as a token-wise Relative Scale to reweight the fine-tuning objective, focusing updates on truly under-learned tokens without over-penalizing intrinsically uncertain positions. Experiments on multiple backbones show consistent improvements on mathematical reasoning benchmarks, transfer gains on out-of-distribution reasoning, and pre code generation performance over probability-only or entropy-only reweighting baselines.

</details>


### [884] [Rethinking LoRA for Data Heterogeneous Federated Learning: Subspace and State Alignment](https://arxiv.org/abs/2602.01746)
*Hongyi Peng,Han Yu,Xiaoxiao Li,Qiang Yang*

Main category: cs.LG

Relevance: 85.0

TL;DR: FedGaLore：针对非IID联邦学习中LoRA性能下降问题，提出结合客户端GaLore梯度子空间优化与服务器端谱共享信号提取的鲁棒同步方法


<details>
  <summary>Details</summary>
Motivation: LoRA在联邦微调中广泛应用，但在非独立同分布(non-IID)设置下性能显著低于全参数微调。研究发现这源于两个耦合的不匹配：更新空间不匹配（客户端在低秩子空间优化但聚合在全空间）和优化器状态不匹配（未同步的自适应状态放大漂移）

Method: FedGaLore结合客户端GaLore风格的梯度子空间优化与服务器端通过谱共享信号提取进行投影二阶矩状态的漂移鲁棒同步，解决上述不匹配问题

Result: 在NLU、视觉和NLG基准测试中，FedGaLore在非IID设置下相比最先进的联邦LoRA基线提高了鲁棒性和准确性

Conclusion: 通过解决LoRA在联邦学习中的更新空间和优化器状态不匹配问题，FedGaLore显著提升了非IID设置下的性能表现

Abstract: Low-Rank Adaptation (LoRA) is widely used for federated fine-tuning. Yet under non-IID settings, it can substantially underperform full-parameter fine-tuning. Through with-high-probability robustness analysis, we uncover that this gap can be attributed to two coupled mismatches: (i) update-space mismatch, where clients optimize in a low-rank subspace but aggregation occurs in the full space; and (ii) optimizer-state mismatch, where unsynchronized adaptive states amplify drift across rounds. We propose FedGaLore, which combines client-side GaLore-style gradient-subspace optimization with server-side drift-robust synchronization of projected second-moment states via spectral shared-signal extraction, to address this challenge. Across NLU, vision, and NLG benchmarks, FedGaLore improves robustness and accuracy over state-of-the-art federated LoRA baselines in non-IID settings.

</details>


### [885] [A Provable Expressiveness Hierarchy in Hybrid Linear-Full Attention](https://arxiv.org/abs/2602.01763)
*Xiaowei Ye,Xiaoyu He,Chao Liao,Chen Wu,Pinyan Lu*

Main category: cs.LG

Relevance: 85.0

TL;DR: 该论文从理论上分析了不同注意力机制的表达能力差异，建立了表达能力层次结构：对于序列函数组合任务，L+1层全注意力网络足够解决，而任何混合网络（包含L-1层全注意力和大量线性注意力层）无法解决，证明了两种注意力机制在表达能力上的明确分离。


<details>
  <summary>Details</summary>
Motivation: 尽管存在各种高效注意力机制（如线性注意力、混合注意力）来缓解标准全注意力的二次复杂度问题，但这些机制相对于全注意力的表达能力缺乏严格的理论表征。本文旨在填补这一理论空白。

Method: 通过理论分析，建立了注意力机制的表达能力层次结构。研究适用于所有可表示为递归形式的线性注意力变体（包括Mamba、DeltaNet等）。以序列函数组合（多步推理任务）作为分析基准。

Result: 证明了表达能力层次：对于序列函数组合任务，L+1层全注意力网络足够解决，而任何混合网络（包含L-1层全注意力和2^{3L^2}层线性注意力层）无法解决。这是首次在理论上证明混合注意力与标准全注意力之间的分离。

Conclusion: 该工作为理解不同注意力机制的基本能力和限制提供了理论视角，证明了全注意力与线性/混合注意力在表达能力上存在本质差异，为高效注意力机制的设计提供了理论指导。

Abstract: Transformers serve as the foundation of most modern large language models. To mitigate the quadratic complexity of standard full attention, various efficient attention mechanisms, such as linear and hybrid attention, have been developed. A fundamental gap remains: their expressive power relative to full attention lacks a rigorous theoretical characterization. In this work, we theoretically characterize the performance differences among these attention mechanisms. Our theory applies to all linear attention variants that can be formulated as a recurrence, including Mamba, DeltaNet, etc. Specifically, we establish an expressiveness hierarchy: for the sequential function composition-a multi-step reasoning task that must occur within a model's forward pass, an ($L+1$)-layer full attention network is sufficient, whereas any hybrid network interleaving $L-1$ layers of full attention with a substantially larger number ($2^{3L^2}$) of linear attention layers cannot solve it. This result demonstrates a clear separation in expressive power between the two types of attention. Our work provides the first provable separation between hybrid attention and standard full attention, offering a theoretical perspective for understanding the fundamental capabilities and limitations of different attention mechanisms.

</details>


### [886] [CoMeT: Collaborative Memory Transformer for Efficient Long Context Modeling](https://arxiv.org/abs/2602.01766)
*Runsong Zhao,Shilei Liu,Jiwei Tang,Langming Liu,Haibin Chen,Weidong Zhang,Yujin Yuan,Tong Xiao,Jingbo Zhu,Wenbo Su,Bo Zheng*

Main category: cs.LG

Relevance: 85.0

TL;DR: CoMeT是一种新型Transformer架构，通过双内存系统和分块处理实现常数内存使用和线性时间复杂度，能处理任意长序列，只需少量微调即可集成到预训练模型中。


<details>
  <summary>Details</summary>
Motivation: 标准Transformer的二次复杂度和无限增长的KV缓存是处理长上下文的主要障碍，需要一种高效处理任意长序列的解决方案。

Method: CoMeT采用分块处理策略，使用双内存系统：FIFO队列的临时内存处理近期事件，带门控更新规则的全局内存处理长程依赖。这些内存作为动态软提示用于下一块处理。还引入了层级流水线并行策略进行高效微调。

Result: CoMeT在32k上下文微调后，能在1M token序列中准确检索任意位置的密码。在SCROLLS基准测试中超越其他高效方法，在摘要任务上达到与全注意力基线相当的性能，在真实世界代理和用户行为QA任务中验证了实用性。

Conclusion: CoMeT通过创新的内存架构和分块处理策略，有效解决了Transformer处理长上下文的计算和内存瓶颈，为LLM的长序列处理提供了高效实用的解决方案。

Abstract: The quadratic complexity and indefinitely growing key-value (KV) cache of standard Transformers pose a major barrier to long-context processing. To overcome this, we introduce the Collaborative Memory Transformer (CoMeT), a novel architecture that enables LLMs to handle arbitrarily long sequences with constant memory usage and linear time complexity. Designed as an efficient, plug-in module, CoMeT can be integrated into pre-trained models with only minimal fine-tuning. It operates on sequential data chunks, using a dual-memory system to manage context: a temporary memory on a FIFO queue for recent events, and a global memory with a gated update rule for long-range dependencies. These memories then act as a dynamic soft prompt for the next chunk. To enable efficient fine-tuning on extremely long contexts, we introduce a novel layer-level pipeline parallelism strategy. The effectiveness of our approach is remarkable: a model equipped with CoMeT and fine-tuned on 32k contexts can accurately retrieve a passkey from any position within a 1M token sequence. On the SCROLLS benchmark, CoMeT surpasses other efficient methods and achieves performance comparable to a full-attention baseline on summarization tasks. Its practical effectiveness is further validated on real-world agent and user behavior QA tasks. The code is available at: https://anonymous.4open.science/r/comet-B00B/

</details>


### [887] [IRIS: Implicit Reward-Guided Internal Sifting for Mitigating Multimodal Hallucination](https://arxiv.org/abs/2602.01769)
*Yuanshuai Li,Yuping Yan,Jirui Han,Fei Ming,Lingjuan Lv,Yaochu Jin*

Main category: cs.LG

Relevance: 85.0

TL;DR: IRIS提出了一种基于隐式奖励的内部筛选方法，通过连续隐式奖励在原生对数概率空间中捕捉模态竞争，无需外部评估器即可缓解MLLM幻觉问题


<details>
  <summary>Details</summary>
Motivation: 多模态大语言模型（MLLMs）的幻觉问题仍然是一个基本挑战。现有的直接偏好优化（DPO）方法严重依赖昂贵的外部评估器进行评分或重写，导致离策略学习性差距和离散化损失。由于无法访问内部状态，这种反馈忽略了不同模态之间导致幻觉生成的细粒度冲突。

Method: IRIS（隐式奖励引导的内部筛选）方法：1）利用原生对数概率空间中的连续隐式奖励来保持完整信息密度并捕捉内部模态竞争；2）采用在策略范式，通过自生成的偏好对消除学习性差距；3）基于多模态隐式奖励筛选这些偏好对，确保优化由直接解决模态冲突的信号驱动。

Result: 在关键幻觉基准测试中，IRIS仅使用5.7k样本就实现了高度竞争力的性能，在偏好对齐过程中完全不需要任何外部反馈。这证实了IRIS为缓解MLLM幻觉提供了一个高效且有原则的范式。

Conclusion: IRIS通过利用连续隐式奖励在原生对数概率空间中捕捉内部模态竞争，提供了一种无需外部评估器的有效方法来解决MLLM幻觉问题，为多模态对齐提供了新的研究方向。

Abstract: Hallucination remains a fundamental challenge for Multimodal Large Language Models (MLLMs). While Direct Preference Optimization (DPO) is a key alignment framework, existing approaches often rely heavily on costly external evaluators for scoring or rewriting, incurring off-policy learnability gaps and discretization loss. Due to the lack of access to internal states, such feedback overlooks the fine-grained conflicts between different modalities that lead to hallucinations during generation.
  To address this issue, we propose IRIS (Implicit Reward-Guided Internal Sifting), which leverages continuous implicit rewards in the native log-probability space to preserve full information density and capture internal modal competition. This on-policy paradigm eliminates learnability gaps by utilizing self-generated preference pairs. By sifting these pairs based on multimodal implicit rewards, IRIS ensures that optimization is driven by signals that directly resolve modal conflicts. Extensive experiments demonstrate that IRIS achieves highly competitive performance on key hallucination benchmarks using only 5.7k samples, without requiring any external feedback during preference alignment. These results confirm that IRIS provides an efficient and principled paradigm for mitigating MLLM hallucinations.

</details>


### [888] [Grad2Reward: From Sparse Judgment to Dense Rewards for Improving Open-Ended LLM Reasoning](https://arxiv.org/abs/2602.01791)
*Zheng Zhang,Ao Lu,Yuanhao Zeng,Ziwei Shan,Jinjin Guo,Lufei Li,Yexin Li,Kan Ren*

Main category: cs.LG

Relevance: 85.0

TL;DR: Grad2Reward：通过梯度归因从LLM法官中提取密集过程奖励，解决开放任务中稀疏奖励问题，实现细粒度监督


<details>
  <summary>Details</summary>
Motivation: 现有RLVR在可验证领域（如数学、编程）有效，但在开放任务中使用LLM-as-a-Judge提供序列级奖励存在两个问题：1）奖励稀疏，缺乏对复杂长轨迹的细粒度监督；2）将法官视为黑盒，丢弃了丰富的中间反馈信号

Method: 提出Grad2Reward框架：1）通过单次反向传播从法官模型推理过程中提取密集过程奖励；2）利用梯度归因实现精确的token级信用分配；3）引入自判断机制，让策略通过自身评估信号改进，无需训练专门奖励模型或依赖外部法官

Result: 实验表明，使用Grad2Reward优化的策略在多样开放任务上表现优异，证实了其有效性和广泛泛化能力

Conclusion: Grad2Reward通过提取法官模型中的密集过程奖励，解决了开放任务RL中的稀疏奖励问题，显著提升了训练效率和推理质量，具有自监督和泛化优势

Abstract: Reinforcement Learning with Verifiable Rewards (RLVR) has catalyzed significant breakthroughs in complex LLM reasoning within verifiable domains, such as mathematics and programming. Recent efforts have sought to extend this paradigm to open-ended tasks by employing LLMs-as-a-Judge to provide sequence-level rewards for policy optimization. However, these rewards are inherently sparse, failing to provide the fine-grained supervision necessary for generating complex, long-form trajectories. Furthermore, current work treats the Judge as a black-box oracle, discarding the rich intermediate feedback signals encoded in it. To address these limitations, we introduce Grad2Reward, a novel framework that extracts dense process rewards directly from the Judge's model inference process via a single backward pass. By leveraging gradient-based attribution, Grad2Reward enables precise token-level credit assignment, substantially enhancing training efficiency and reasoning quality. Additionally, Grad2Reward introduces a self-judging mechanism, allowing the policy to improve through its own evaluative signals without training specialized reward models or reliance on superior external Judges. The experiments demonstrate that policies optimized with Grad2Reward achieve outstanding performance across diverse open-ended tasks, affirming its effectiveness and broad generalizability.

</details>


### [889] [Beyond Precision: Training-Inference Mismatch is an Optimization Problem and Simple LR Scheduling Fixes It](https://arxiv.org/abs/2602.01826)
*Yaxiang Zhang,Yingru Li,Jiacai Liu,Jiawei Xu,Ziniu Li,Qian Liu,Haoyuan Li*

Main category: cs.LG

Relevance: 85.0

TL;DR: 本文通过优化视角分析RL训练LLMs的不稳定性，发现梯度噪声和训练-推理不匹配会随训练进展而加剧，提出基于响应长度的动态学习率调度器来稳定训练


<details>
  <summary>Details</summary>
Motivation: 大型语言模型的强化学习训练存在不稳定问题，传统归因于"训练-推理不匹配"，但标准解决方法如重要性采样在长期训练中可能失效。本文旨在从优化角度深入分析这种不稳定性，并提出更有效的解决方案。

Method: 1) 从优化角度分析RL训练不稳定性，发现梯度噪声和训练-推理不匹配会同步加剧；2) 发现减小更新规模可以有效抑制不匹配；3) 提出基于响应长度的动态学习率调度器，将响应长度作为不稳定性的早期预警信号，动态触发学习率衰减。

Result: 实证研究表明，通过响应长度动态调整学习率，可以有效稳定RL训练，将训练-推理不匹配保持在安全水平。该方法相比传统预定义学习率调度器更有效。

Conclusion: 训练-推理不匹配不是静态数值差异，而是与模型优化耦合的动态失效。基于响应长度的动态学习率调度器是简单有效的解决方案，能够持续稳定RL训练。

Abstract: Reinforcement Learning (RL) for training Large Language Models is notoriously unstable. While recent studies attribute this to "training inference mismatch stemming" from inconsistent hybrid engines, standard remedies, such as Importance Sampling, might fail during extended training runs. In this work, we analyze this instability through the lens of optimization, demonstrating that gradient noise and training-inference mismatch escalate in tandem as training progresses. Meanwhile, we find that the mismatch can be effectively suppressed by shrinking the update size. Taken together, we deduce that the mismatch is not merely a static numerical discrepancy, but a dynamic failure coupled with the model's optimization. Based on this insight, we propose a simple yet effective solution: a specialized Learning Rate (LR) scheduler. Instead of pre-defined decay schedule in traditional LR scheduler, our method dynamically triggers LR decay based on response length, which we identify as a reliable early-warning signal for impending instability. Empirical evidence suggests that by reducing the learning rate as gradient noise rises, we can consistently stabilize RL training and keep the training-inference mismatch at a safe level.

</details>


### [890] [Prism: Efficient Test-Time Scaling via Hierarchical Search and Self-Verification for Discrete Diffusion Language Models](https://arxiv.org/abs/2602.01842)
*Jinbin Bai,Yixuan Li,Yuchen Zhu,Yi Xin,Qingyu Shi,Aosong Feng,Xiaohong Liu,Molei Tao,Jianru Xue,Xiangtai Li,Ming-Hsuan Yang*

Main category: cs.LG

Relevance: 85.0

TL;DR: Prism是一个针对离散扩散语言模型(dLLMs)的高效测试时扩展框架，通过层次轨迹搜索、局部分支和自验证反馈来提升推理性能，在数学推理和代码生成任务上实现了性能与效率的良好平衡。


<details>
  <summary>Details</summary>
Motivation: 测试时计算已成为提升LLM推理能力的实用方法，但现有的测试时扩展算法主要依赖自回归解码，不适合并行解码的离散扩散语言模型。如何为dLLMs开发高效测试时扩展方法以释放其生成潜力是一个尚未充分探索的挑战。

Method: 提出Prism框架，包含三个核心组件：1) 层次轨迹搜索(HTS)，在早期到中期的去噪窗口动态剪枝和重新分配计算资源；2) 局部分支与部分重掩码，在保持高置信度token的同时探索多样化实现；3) 自验证反馈(SVF)，通过自评估提示替代外部验证器。

Result: 在三个dLLMs（LLaDA 8B Instruct、Dream 7B Instruct、LLaDA 2.0-mini）的四个数学推理和代码生成基准测试中，Prism实现了良好的性能-效率权衡，以显著更少的函数评估次数匹配最佳N选1的性能。

Conclusion: Prism为离散扩散语言模型提供了一个有效的测试时扩展框架，解决了dLLMs在测试时计算方面的独特挑战，为释放这类模型的生成潜力提供了新途径。

Abstract: Inference-time compute has re-emerged as a practical way to improve LLM reasoning. Most test-time scaling (TTS) algorithms rely on autoregressive decoding, which is ill-suited to discrete diffusion language models (dLLMs) due to their parallel decoding over the entire sequence. As a result, developing effective and efficient TTS methods to unlock dLLMs' full generative potential remains an underexplored challenge. To address this, we propose Prism (Pruning, Remasking, and Integrated Self-verification Method), an efficient TTS framework for dLLMs that (i) performs Hierarchical Trajectory Search (HTS) which dynamically prunes and reallocates compute in an early-to-mid denoising window, (ii) introduces Local branching with partial remasking to explore diverse implementations while preserving high-confidence tokens, and (iii) replaces external verifiers with Self-Verified Feedback (SVF) obtained via self-evaluation prompts on intermediate completions. Across four mathematical reasoning and code generation benchmarks on three dLLMs, including LLaDA 8B Instruct, Dream 7B Instruct, and LLaDA 2.0-mini, our Prism achieves a favorable performance-efficiency trade-off, matching best-of-N performance with substantially fewer function evaluations (NFE). The code is released at https://github.com/viiika/Prism.

</details>


### [891] [No Generation without Representation: Efficient Causal Protein Language Models Enable Zero-Shot Fitness Estimation](https://arxiv.org/abs/2602.01845)
*Furkan Eris*

Main category: cs.LG

Relevance: 85.0

TL;DR: Proust是一个309M参数的因果蛋白质语言模型，通过借鉴LLM架构创新，在蛋白质适应度预测和生成任务上同时表现出色，填补了掩码语言模型和因果模型之间的鸿沟。


<details>
  <summary>Details</summary>
Motivation: 蛋白质语言模型面临一个根本性分歧：掩码语言模型在适应度预测方面表现出色，而因果模型支持生成任务，迫使研究人员维护不同的架构。Proust旨在通过单一模型同时实现这两个目标。

Method: 采用因果模型架构，借鉴LLM研究中的创新技术：分组查询注意力（共享K/V投影）、跨层值残差连接和深度因果卷积。使用33B token在40 B200 GPU小时内训练309M参数模型。

Result: 在ProteinGym替换任务上达到Spearman ρ=0.390，与需要50-200倍计算量的掩码语言模型相当；在indels任务上达到新的SOTA，优于大20倍的模型；在EVEREST病毒适应度基准上，仅使用序列信息就接近结构感知方法。

Conclusion: Proust通过架构创新在蛋白质语言模型中实现了适应度预测和生成能力的统一，同时保持了因果模型固有的生成能力。可解释性分析表明，位置熵方差可以预测检索增强何时有效，这些见解可以指导测试时扩展等能力。

Abstract: Protein language models (PLMs) face a fundamental divide: masked language models (MLMs) excel at fitness prediction while causal models enable generation, forcing practitioners to maintain separate architectures. We introduce \textbf{Proust}, a 309M-parameter causal PLM that bridges this gap through architectural innovations adapted from recent LLM research, including grouped-query attention with shared K/V projections, cross-layer value residuals, and depthwise causal convolutions. Trained on 33B tokens in 40 B200 GPU-hours, Proust achieves Spearman $ρ= 0.390$ on ProteinGym substitutions, competitive with MLMs requiring 50--200$\times$ the compute. On indels, Proust sets a new state-of-the-art, outperforming models up to 20$\times$ larger. On EVEREST viral fitness benchmarks, it approaches structure-aware methods using sequence alone. These powerful representations position Proust in a sweet spot as it also retains native generative capabilities that MLMs lack by design. Interpretability analysis reveals that per-position entropy variance predicts, to an extent, when retrieval augmentation helps and hurts. Such insights can grow in both quantity and quality at scale and inform capabilities such as test-time scaling. Code and weights are available at https://github.com/Furkan9015/proust-inference

</details>


### [892] [Internal Flow Signatures for Self-Checking and Refinement in LLMs](https://arxiv.org/abs/2602.01897)
*Sungheon Jeong,Sanggeon Yun,Ryozo Masukawa,Wenjun Haung,Hanning Chen,Mohsen Imani*

Main category: cs.LG

Relevance: 85.0

TL;DR: 提出内部流签名方法，通过监控LLM深度动态来检测和修正不忠实于上下文的生成，实现自我检查与精炼


<details>
  <summary>Details</summary>
Motivation: 大语言模型可能生成流畅但不忠实于上下文的答案，现有方法多依赖外部验证或生成后的独立判断，缺乏基于模型内部决策动态的自我检查机制

Method: 1. 引入内部流签名：在固定块间监控边界审计决策形成过程；2. 通过偏置中心监控稳定token级动态；3. 在紧凑移动读取对齐子空间中总结轨迹；4. 使用正交传输对齐相邻窗口帧；5. 训练轻量GRU验证器进行自我检查；6. 定位问题深度事件并针对性精炼

Result: 该方法能够检测不忠实生成，定位问题深度事件，并通过回滚到问题token、在识别块处钳制异常传输步骤来实现针对性精炼，同时保持正交残差

Conclusion: 内部流签名方法为LLM提供了基于内部决策动态的可操作定位和低开销自我检查能力，无需修改基础模型

Abstract: Large language models can generate fluent answers that are unfaithful to the provided context, while many safeguards rely on external verification or a separate judge after generation. We introduce \emph{internal flow signatures} that audit decision formation from depthwise dynamics at a fixed inter-block monitoring boundary. The method stabilizes token-wise motion via bias-centered monitoring, then summarizes trajectories in compact \emph{moving} readout-aligned subspaces constructed from the top token and its close competitors within each depth window. Neighboring window frames are aligned by an orthogonal transport, yielding depth-comparable transported step lengths, turning angles, and subspace drift summaries that are invariant to within-window basis choices. A lightweight GRU validator trained on these signatures performs self-checking without modifying the base model. Beyond detection, the validator localizes a culprit depth event and enables a targeted refinement: the model rolls back to the culprit token and clamps an abnormal transported step at the identified block while preserving the orthogonal residual. The resulting pipeline provides actionable localization and low-overhead self-checking from internal decision dynamics. \emph{Code is available at} \texttt{github.com/EavnJeong/Internal-Flow-Signatures-for-Self-Checking-and-Refinement-in-LLMs}.

</details>


### [893] [Towards Long-Horizon Interpretability: Efficient and Faithful Multi-Token Attribution for Reasoning LLMs](https://arxiv.org/abs/2602.01914)
*Wenbo Pan,Zhichao Liu,Xianlong Wang,Haining Yu,Xiaohua Jia*

Main category: cs.LG

Relevance: 85.0

TL;DR: FlashTrace：一种高效的多token归因方法，通过跨span聚合和递归归因机制，解决LLM长上下文和多步推理中的归因效率和忠实性问题，实现130倍加速。


<details>
  <summary>Details</summary>
Motivation: 随着现代LLM越来越依赖扩展推理链，现有token归因方法面临两大挑战：1) 效率瓶颈 - 在长度为N的上下文中归因M个token需要O(M*N)操作，长上下文归因极其缓慢；2) 忠实性下降 - 中间推理token吸收归因质量，阻止重要性传播回原始输入。

Method: 提出FlashTrace方法：1) 采用跨span聚合技术，在单次计算中为多token目标计算归因；2) 设计递归归因机制，通过中间推理链追踪重要性回源输入；3) 保持归因的忠实性。

Result: 在长上下文检索(RULER)和多步推理(MATH, MorehopQA)任务上的实验表明：FlashTrace相比现有基线实现超过130倍加速，同时保持更优的忠实性。递归归因分析显示，即使单次递归跳转也能通过追踪推理链提高忠实性。

Conclusion: FlashTrace通过高效的跨span聚合和递归归因机制，有效解决了LLM长上下文和多步推理中的归因效率和忠实性问题，为LLM可解释性提供了实用的解决方案。

Abstract: Token attribution methods provide intuitive explanations for language model outputs by identifying causally important input tokens. However, as modern LLMs increasingly rely on extended reasoning chains, existing schemes face two critical challenges: (1) efficiency bottleneck, where attributing a target span of M tokens within a context of length N requires O(M*N) operations, making long-context attribution prohibitively slow; and (2) faithfulness drop, where intermediate reasoning tokens absorb attribution mass, preventing importance from propagating back to the original input. To address these, we introduce FlashTrace, an efficient multi-token attribution method that employs span-wise aggregation to compute attribution over multi-token targets in a single pass, while maintaining faithfulness. Moreover, we design a recursive attribution mechanism that traces importance through intermediate reasoning chains back to source inputs. Extensive experiments on long-context retrieval (RULER) and multi-step reasoning (MATH, MorehopQA) tasks demonstrate that FlashTrace achieves over 130x speedup over existing baselines while maintaining superior faithfulness. We further analyze the dynamics of recursive attribution, showing that even a single recursive hop improves faithfulness by tracing importance through the reasoning chain.

</details>


### [894] [VLM-Guided Experience Replay](https://arxiv.org/abs/2602.01915)
*Elad Sharony,Tom Jurgenson,Orr Krupnik,Dotan Di Castro,Shie Mannor*

Main category: cs.LG

Relevance: 85.0

TL;DR: 利用预训练的视觉语言模型（VLM）作为自动评估器，指导强化学习回放缓冲区中的经验优先级排序，提高样本效率和成功率


<details>
  <summary>Details</summary>
Motivation: 尽管LLMs和VLMs已被整合到强化学习的各个组件中，但回放缓冲区这一核心组件仍未得到探索。作者希望利用VLMs的语义和多模态推理能力来增强回放缓冲区的经验优先级排序

Method: 使用冻结的预训练VLM（无需微调）作为自动评估器，识别和优先处理智能体经验中有前景的子轨迹。该方法适用于离散和连续领域，包括游戏和机器人场景

Result: 在游戏和机器人场景中，使用该方法训练的智能体平均成功率提高11-52%，样本效率提升19-45%，相比之前的方法有显著改进

Conclusion: VLM指导的回放缓冲区优先级排序是增强强化学习样本效率和性能的有效方法，为LLMs/VLMs与强化学习的集成开辟了新方向

Abstract: Recent advances in Large Language Models (LLMs) and Vision-Language Models (VLMs) have enabled powerful semantic and multimodal reasoning capabilities, creating new opportunities to enhance sample efficiency, high-level planning, and interpretability in reinforcement learning (RL). While prior work has integrated LLMs and VLMs into various components of RL, the replay buffer, a core component for storing and reusing experiences, remains unexplored. We propose addressing this gap by leveraging VLMs to guide the prioritization of experiences in the replay buffer. Our key idea is to use a frozen, pre-trained VLM (requiring no fine-tuning) as an automated evaluator to identify and prioritize promising sub-trajectories from the agent's experiences. Across scenarios, including game-playing and robotics, spanning both discrete and continuous domains, agents trained with our proposed prioritization method achieve 11-52% higher average success rates and improve sample efficiency by 19-45% compared to previous approaches. https://esharony.me/projects/vlm-rb/

</details>


### [895] [COLT: Lightweight Multi-LLM Collaboration through Shared MCTS Reasoning for Model Compilation](https://arxiv.org/abs/2602.01935)
*Annabelle Sujun Tang,Christopher Priebe,Lianhui Qin,Hadi Esmaeilzadeh*

Main category: cs.LG

Relevance: 85.0

TL;DR: COLT：一个轻量级的多LLM协作框架，用于编译器优化，通过共享MCTS树实现模型间协调推理，用小模型为主、大模型为辅的策略匹配单一大模型的性能。


<details>
  <summary>Details</summary>
Motivation: 模型服务成本主导AI系统，编译器优化对可扩展部署至关重要。现有方法使用单个大语言模型指导编译器搜索，但成本高昂；小模型单独使用可靠性不足。本文探索多LLM协作推理能否以小型模型为主，达到或超越单个大模型的性能。

Method: 提出COLT框架：1) 使用共享的蒙特卡洛树搜索(MCTS)作为协作基础，实现转换前缀复用和跨模型价值传播；2) 每轮迭代中，执行LLM提出联合动作（编译器转换，下一个查询的模型）；3) 引入模型感知树策略，偏向小模型同时保持探索；4) 当搜索出现持续回归时，升级到最大模型的课程调整机制。

Result: COLT框架通过多LLM协作实现了与单个大模型相当的性能，同时显著降低了推理成本。共享MCTS树的设计避免了传统多智能体系统需要的外部规划器、并发LLM、数据库等复杂机制。

Conclusion: 多LLM协作推理可以有效地指导编译器优化搜索，通过以小型模型为主、大型模型为辅的策略，在保持性能的同时降低计算成本。COLT的轻量级设计为高效的多模型协作提供了新思路。

Abstract: Model serving costs dominate AI systems, making compiler optimization essential for scalable deployment. Recent works show that a large language model (LLM) can guide compiler search by reasoning over program structure and optimization history. However, using a single large model throughout the search is expensive, while smaller models are less reliable when used alone. Thus, this paper seeks to answer whether multi-LLM collaborative reasoning relying primarily on small LLMs can match or exceed the performance of a single large model. As such, we propose a lightweight collaborative multi-LLM framework, dubbed COLT, for compiler optimization that enables coordinated reasoning across multiple models within a single Monte Carlo tree search (MCTS) process. A key contribution is the use of a single shared MCTS tree as the collaboration substrate across LLMs, enabling the reuse of transformation prefixes and cross-model value propagation. Hence, we circumvent both heavy internal reasoning mechanisms and conventional agentic machinery that relies on external planners, multiple concurrent LLMs, databases, external memory/versioning of intermediate results, and controllers by simply endogenizing model selection within the lightweight MCTS optimization loop. Every iteration, the acting LLM proposes a joint action: (compiler transformation, model to be queried next). We also introduce a model-aware tree policy that biases search toward smaller models while preserving exploration, and a course-alteration mechanism that escalates to the largest model when the search exhibits persistent regressions attributable to smaller models.

</details>


### [896] [T-LLM: Teaching Large Language Models to Forecast Time Series via Temporal Distillation](https://arxiv.org/abs/2602.01937)
*Suhan Guo,Bingxu Wang,Shaodan Zhang,Furao Shen*

Main category: cs.LG

Relevance: 85.0

TL;DR: T-LLM是一个时间序列预测框架，通过时间蒸馏将轻量级时序教师的预测能力转移到通用大语言模型中，无需推理时的额外模块。


<details>
  <summary>Details</summary>
Motivation: 时间序列数据与底层过程演化紧密相关，只能随现实时间积累，这限制了仅靠规模驱动的预训练效果。现有方法主要依赖表示级对齐或推理时的时间模块，而非明确教授LLMs预测行为。

Method: 提出T-LLM时间蒸馏框架，在训练期间将轻量级时序教师的预测行为转移到通用LLM中。教师结合趋势建模和频域分析提供结构化时间监督，推理时完全移除教师，仅保留LLM作为预测模型。

Result: 在基准数据集和传染病预测任务上的实验表明，T-LLM在全样本、少样本和零样本设置下均优于现有基于LLM的预测方法，同时实现了简单高效的部署流程。

Conclusion: T-LLM成功地将时间序列预测能力赋予通用LLMs，通过时间蒸馏框架解决了时间约束带来的挑战，为LLMs在时序预测领域的应用提供了有效解决方案。

Abstract: Time series forecasting plays a critical role in decision-making across many real-world applications. Unlike data in vision and language domains, time series data is inherently tied to the evolution of underlying processes and can only accumulate as real-world time progresses, limiting the effectiveness of scale-driven pretraining alone. This time-bound constraint poses a challenge for enabling large language models (LLMs) to acquire forecasting capability, as existing approaches primarily rely on representation-level alignment or inference-time temporal modules rather than explicitly teaching forecasting behavior to the LLM. We propose T-LLM, a temporal distillation framework that equips general-purpose LLMs with time series forecasting capability by transferring predictive behavior from a lightweight temporal teacher during training. The teacher combines trend modeling and frequency-domain analysis to provide structured temporal supervision, and is removed entirely at inference, leaving the LLM as the sole forecasting model. Experiments on benchmark datasets and infectious disease forecasting tasks demonstrate that T-LLM consistently outperforms existing LLM-based forecasting methods under full-shot, few-shot, and zero-shot settings, while enabling a simple and efficient deployment pipeline.

</details>


### [897] [Efficient Epistemic Uncertainty Estimation for Large Language Models via Knowledge Distillation](https://arxiv.org/abs/2602.01956)
*Seonghyeon Park,Jewon Yeom,Jaewon Sok,Jeongjae Park,Heejun Kim,Taesup Kim*

Main category: cs.LG

Relevance: 85.0

TL;DR: 提出一个利用小型草稿模型高效估计LLM中认知不确定性的框架，避免计算昂贵的深度集成方法，通过偏差-方差分解近似不确定性，显著降低计算成本


<details>
  <summary>Details</summary>
Motivation: 大语言模型中的不确定性量化对于减少幻觉和在安全关键任务中进行风险感知部署至关重要，但通过深度集成方法估计认知不确定性在现代模型规模下计算成本过高

Method: 提出基于偏差-方差分解的理论框架，利用草稿模型近似认知不确定性：使用草稿模型间的Jensen-Shannon散度作为方差代理，草稿混合与目标模型间的KL散度作为偏差代理。引入在线随机蒸馏(OSD)高效近似目标聚合，以及数据多样化草稿(DDD)策略增强草稿多样性

Result: 在GSM8K数据集上的实验表明，该方法将估计误差(RMSE)降低了高达37%，幻觉检测性能与计算昂贵的扰动方法(TokUR)相当，但推理成本几乎可以忽略不计

Conclusion: 该方法为不确定性感知的LLM部署提供了实用解决方案，能够在保持高性能的同时显著降低计算成本，特别适用于需要实时不确定性估计的应用场景

Abstract: Quantifying uncertainty in Large Language Models (LLMs) is essential for mitigating hallucinations and enabling risk-aware deployment in safety-critical tasks. However, estimating Epistemic Uncertainty(EU) via Deep Ensembles is computationally prohibitive at the scale of modern models. We propose a framework that leverages the small draft models to efficiently estimate token-level EU, bypassing the need for full-scale ensembling. Theoretically grounded in a Bias-Variance Decomposition, our approach approximates EU via Jensen-Shannon divergence among drafts (variance proxy) and KL divergence between the draft mixture and the target (bias proxy). To further ensure accuracy without significant overhead, we introduce Online Stochastic Distillation (OSD) to efficiently approximate target aggregation and the Data-Diverse Drafts (DDD) strategy to enhance draft diversity for better target approximation. Extensive experiments on GSM8K demonstrate that our method reduces the estimation error (RMSE) by up to 37% compared to baselines. Crucially, our approach achieves Hallucination Detection performance competitive with heavy perturbation-based methods like TokUR while incurring negligible inference costs, offering a practical solution for uncertainty-aware LLM deployment.

</details>


### [898] [Self-Consolidation for Self-Evolving Agents](https://arxiv.org/abs/2602.01966)
*Hongzhuo Yu,Fei Zhu,Guo-Sen Xie,Ling Shao*

Main category: cs.LG

Relevance: 85.0

TL;DR: 论文提出了一种LLM智能体自我进化框架，通过对比反思策略总结错误模式，并通过自我巩固机制将文本经验蒸馏为可学习参数，实现长期进化。


<details>
  <summary>Details</summary>
Motivation: 现有LLM智能体通常是静态系统，缺乏通过终身交互进化的能力。现有方法主要依赖检索成功轨迹作为演示，但存在两个关键局限：1）只关注成功而忽略了失败尝试中的教学价值；2）持续积累文本经验会增加检索时间消耗并引入噪声。

Method: 提出互补进化机制：1）对比反思策略，显式总结易错模式并捕捉可重用见解；2）自我巩固机制，将非参数文本经验蒸馏为紧凑的可学习参数，使智能体能够将广泛历史经验内化到其潜在空间中。

Result: 大量实验证明了该方法在长期智能体进化中的优势。

Conclusion: 该框架能够有效解决现有LLM智能体进化方法的局限性，通过内部化历史经验实现更高效的长期进化。

Abstract: While large language model (LLM) agents have demonstrated impressive problem-solving capabilities, they typically operate as static systems, lacking the ability to evolve through lifelong interaction. Existing attempts to bridge this gap primarily rely on retrieving successful past trajectories as demonstrations. However, this paradigm faces two critical limitations. First, by focusing solely on success, agents overlook the rich pedagogical value embedded in failed attempts, preventing them from identifying and avoiding recurrent pitfalls. Second, continually accumulating textual experiences not only increases the time consumption during retrieval but also inevitably introduces noise and exhausts the largest context window of current LLMs. To address these challenges, we propose a novel self-evolving framework for LLM agents that introduces a complementary evolution mechanism: First, a contrastive reflection strategy is introduced to explicitly summarize error-prone patterns and capture reusable insights. Second, we propose a self-consolidation mechanism that distills non-parametric textual experience into compact learnable parameters. This enables the agent to internalize extensive historical experience directly into its latent space. Extensive experiments demonstrate the advantages of our method in long-term agent evolution.

</details>


### [899] [IntraSlice: Towards High-Performance Structural Pruning with Block-Intra PCA for LLMs](https://arxiv.org/abs/2602.01975)
*Meng Li,Peisong Wang,Yuantian Shao,Qinghao Hu,Hongjian Fang,Yifan Zhang,Zhihui Wei,Jian Cheng*

Main category: cs.LG

Relevance: 85.0

TL;DR: IntraSlice：一种基于模块内PCA压缩剪枝的框架，通过近似PCA方法实现无额外参数的全融合，在保持性能的同时加速LLM推理。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型部署面临巨大规模挑战，结构化剪枝能加速但导致性能显著下降。现有PCA剪枝方法仅在模块间应用，引入额外参数且残差连接会严重破坏激活分布。

Method: 提出IntraSlice框架，采用模块内块级PCA压缩剪枝。利用Transformer模块结构特性设计近似PCA方法，其变换矩阵可完全融合到模型中无需额外参数。引入基于PCA的全局剪枝比例估计器，考虑压缩激活分布和传统模块重要性。

Result: 在Llama2、Llama3和Phi系列模型上验证，在多种语言基准测试中，在相同压缩比或推理速度下，相比现有基线方法获得更优的压缩性能。

Conclusion: IntraSlice通过模块内PCA压缩剪枝有效解决了现有方法的问题，实现了无额外参数的高效模型压缩，在保持性能的同时显著加速LLM推理。

Abstract: Large Language Models (LLMs) achieve strong performance across diverse tasks but face deployment challenges due to their massive size. Structured pruning offers acceleration benefits but leads to significant performance degradation. Recent PCA-based pruning methods have alleviated this issue by retaining key activation components, but are only applied between modules in order to fuse the transformation matrix, which introduces extra parameters and severely disrupts activation distributions due to residual connections. To address these issues, we propose IntraSlice, a framework that applies block-wise module-intra PCA compression pruning. By leveraging the structural characteristics of Transformer modules, we design an approximate PCA method whose transformation matrices can be fully fused into the model without additional parameters. We also introduce a PCA-based global pruning ratio estimator that further considers the distribution of compressed activations, building on conventional module importance. We validate our method on Llama2, Llama3, and Phi series across various language benchmarks. Experimental results demonstrate that our approach achieves superior compression performance compared to recent baselines at the same compression ratio or inference speed.

</details>


### [900] [SAME: Stabilized Mixture-of-Experts for Multimodal Continual Instruction Tuning](https://arxiv.org/abs/2602.01990)
*Zhen-Hao Xie,Jun-Tao Tang,Yu-Cheng Shi,Han-Jia Ye,De-Chuan Zhan,Da-Wei Zhou*

Main category: cs.LG

Relevance: 85.0

TL;DR: SAME方法通过正交子空间分解稳定专家选择，利用历史输入协方差进行曲率感知缩放来调节专家更新，解决多模态持续指令调优中的路由漂移和专家漂移问题。


<details>
  <summary>Details</summary>
Motivation: 多模态大语言模型需要通过持续指令调优来扩展能力，但现有稀疏专家路由方法存在两个问题：1) 路由漂移 - 随着数据分布变化，专家选择变得不一致；2) 专家漂移 - 共享专家被新任务覆盖而失去原有功能。

Method: 提出SAME方法：1) 将路由动态分解为正交子空间，只更新任务相关方向来稳定专家选择；2) 通过历史输入协方差进行曲率感知缩放，以无排练方式调节专家更新；3) 引入自适应专家激活机制，在训练时冻结选定专家以减少冗余计算和跨任务干扰。

Result: 大量实验表明SAME在多模态持续指令调优任务上取得了最先进的性能。

Conclusion: SAME方法有效解决了多模态持续学习中的路由漂移和专家漂移问题，为MLLMs的持续能力扩展提供了稳定高效的解决方案。

Abstract: Multimodal Large Language Models (MLLMs) achieve strong performance through instruction tuning, but real-world deployment requires them to continually expand their capabilities, making Multimodal Continual Instruction Tuning (MCIT) essential. Recent methods leverage sparse expert routing to promote task specialization, but we find that the expert routing process suffers from drift as the data distribution evolves. For example, a grounding query that previously activated localization experts may instead be routed to irrelevant experts after learning OCR tasks. Meanwhile, the grounding-related experts can be overwritten by new tasks and lose their original functionality. Such failure reflects two problems: router drift, where expert selection becomes inconsistent over time, and expert drift, where shared experts are overwritten across tasks. Therefore, we propose StAbilized Mixture-of-Experts (SAME) for MCIT. To address router drift, SAME stabilizes expert selection by decomposing routing dynamics into orthogonal subspaces and updating only task-relevant directions. To mitigate expert drift, we regulate expert updates via curvature-aware scaling using historical input covariance in a rehearsal-free manner. SAME also introduces adaptive expert activation to freeze selected experts during training, reducing redundant computation and cross-task interference. Extensive experiments demonstrate its SOTA performance.

</details>


### [901] [On the Limits of Layer Pruning for Generative Reasoning in LLMs](https://arxiv.org/abs/2602.01997)
*Safal Shrestha,Anubhav Shrestha,Aadim Nepal,Minwu Kim,Keith Ross*

Main category: cs.LG

Relevance: 85.0

TL;DR: 层剪枝在分类任务上表现良好，但在生成式推理任务上严重退化，特别是多步推理任务。通过自生成响应的监督微调可以部分恢复性能，但生成式推理的恢复仍有限制。


<details>
  <summary>Details</summary>
Motivation: 现有层剪枝技术虽然能在分类基准上保持性能，但在生成式推理任务上表现严重退化。研究旨在系统分析层剪枝对多步推理任务的影响，并探索在有限后训练资源下的恢复策略。

Method: 1. 跨多个模型家族的系统性研究，分析深度减少对多步推理任务的影响；2. 评估基于自生成响应的监督微调策略，在无预训练规模数据或计算的后训练约束下进行恢复。

Result: 1. 分类任务恢复良好，保持高达90%基线性能；2. 生成式基准提升20-30个百分点；3. 但生成式推理恢复仍有限制，主要在较低剪枝比例下可行；4. 算术计算和括号平衡等算法能力退化明显。

Conclusion: 层剪枝对生成式推理存在根本性限制，主要适用于较低剪枝比例。研究为在约束后训练机制下有效应用深度减少提供了指导。

Abstract: Recent works have shown that layer pruning can compress large language models (LLMs) while retaining strong performance on classification benchmarks with little or no finetuning. However, existing pruning techniques often suffer severe degradation on generative reasoning tasks. Through a systematic study across multiple model families, we find that tasks requiring multi-step reasoning are particularly sensitive to depth reduction. Beyond surface-level text degeneration, we observe degradation of critical algorithmic capabilities, including arithmetic computation for mathematical reasoning and balanced parenthesis generation for code synthesis. Under realistic post-training constraints, without access to pretraining-scale data or compute, we evaluate a simple mitigation strategy based on supervised finetuning with Self-Generated Responses. This approach achieves strong recovery on classification tasks, retaining up to 90\% of baseline performance, and yields substantial gains of up to 20--30 percentage points on generative benchmarks compared to prior post-pruning techniques. Crucially, despite these gains, recovery for generative reasoning remains fundamentally limited relative to classification tasks and is viable primarily at lower pruning ratios. Overall, we characterize the practical limits of layer pruning for generative reasoning and provide guidance on when depth reduction can be applied effectively under constrained post-training regimes.

</details>


### [902] [Preserve-Then-Quantize: Balancing Rank Budgets for Quantization Error Reconstruction in LLMs](https://arxiv.org/abs/2602.02001)
*Yoonjun Cho,Dongjae Jeon,Soeun Kim,Moongyu Jeon,Albert No*

Main category: cs.LG

Relevance: 85.0

TL;DR: SRR提出结构化残差重建框架，通过保留权重矩阵的主要奇异子空间并仅量化残差部分，优化量化误差重建的秩分配策略，在PTQ中降低精度损失并支持量化参数高效微调。


<details>
  <summary>Details</summary>
Motivation: 现有量化误差重建方法将全部秩预算用于误差重建，这在权重具有内在低秩结构且量化破坏主要方向时是次优的。需要一种更智能的秩分配策略来平衡主要方向保留和误差重建。

Method: 提出结构化残差重建框架：1) 保留激活缩放权重的前k个奇异子空间；2) 仅量化残差部分；3) 使用剩余秩r-k进行误差重建。推导理论指导的k选择准则，平衡量化暴露能量和秩约束下的不可恢复误差。该参数化自然支持量化参数高效微调，并通过梯度缩放稳定微调过程。

Result: 实验表明在多种模型和量化设置下，PTQ中持续降低困惑度，在2位量化参数高效微调下，GLUE任务平均提升5.9个百分点。

Conclusion: SRR通过结构化秩分配策略，在保持权重主要方向的同时优化量化误差重建，显著提升量化模型性能，并为量化参数高效微调提供稳定框架。

Abstract: Quantization Error Reconstruction (QER) reduces accuracy loss in Post-Training Quantization (PTQ) by approximating weights as $\mathbf{W} \approx \mathbf{Q} + \mathbf{L}\mathbf{R}$, using a rank-$r$ correction to reconstruct quantization error. Prior methods devote the full rank budget to error reconstruction, which is suboptimal when $\mathbf{W}$ has intrinsic low-rank structure and quantization corrupts dominant directions. We propose Structured Residual Reconstruction (SRR), a rank-allocation framework that preserves the top-$k$ singular subspace of the activation-scaled weight before quantization, quantizes only the residual, and uses the remaining rank $r-k$ for error reconstruction. We derive a theory-guided criterion for selecting $k$ by balancing quantization-exposed energy and unrecoverable error under rank constraints. We further show that resulting $\mathbf{Q} + \mathbf{L}\mathbf{R}$ parameterization naturally supports Quantized Parameter-Efficient Fine-Tuning (QPEFT), and stabilizes fine-tuning via gradient scaling along preserved directions. Experiments demonstrate consistent perplexity reductions across diverse models and quantization settings in PTQ, along with a 5.9 percentage-point average gain on GLUE under 2-bit QPEFT.

</details>


### [903] [FiLoRA: Focus-and-Ignore LoRA for Controllable Feature Reliance](https://arxiv.org/abs/2602.02060)
*Hyunsuk Chung,Caren Han,Yerin Choi,Seungyeon Ji,Jinwoo Kim,Eun-Jung Holden,Kyungreem Han*

Main category: cs.LG

Relevance: 85.0

TL;DR: FiLoRA是一个指令条件化的参数高效适应框架，通过分解特征组对齐的LoRA模块和应用指令条件门控，实现对内部特征依赖的显式控制，而不改变预测目标或任务语义。


<details>
  <summary>Details</summary>
Motivation: 多模态基础模型整合了跨模态的异构信号，但人们对其预测如何依赖特定内部特征组以及这种依赖是否可以被有意控制的理解仍然不足。现有关于捷径和虚假行为的研究主要依赖事后分析或特征移除，对是否可以在不改变任务语义的情况下调节依赖关系提供了有限见解。

Method: FiLoRA将适应分解为特征组对齐的LoRA模块，并应用指令条件门控，使自然语言指令作为计算级控制信号而非任务重定义。该方法保持预测目标不变，仅通过门控机制选择性地放大或抑制核心和虚假特征组。

Result: 在文本-图像和音频-视觉基准测试中，指令条件门控在内部计算中诱导了一致且因果性的转变，选择性地放大或抑制核心和虚假特征组而不修改标签空间或训练目标。进一步分析表明，FiLoRA在虚假特征干预下提高了鲁棒性。

Conclusion: FiLoRA揭示了超越相关性学习的调节依赖的原则性机制，为多模态基础模型提供了计算级控制内部特征依赖的能力，同时保持参数高效性和任务语义不变。

Abstract: Multimodal foundation models integrate heterogeneous signals across modalities, yet it remains poorly understood how their predictions depend on specific internal feature groups and whether such reliance can be deliberately controlled. Existing studies of shortcut and spurious behavior largely rely on post hoc analyses or feature removal, offering limited insight into whether reliance can be modulated without altering task semantics. We introduce FiLoRA (Focus-and-Ignore LoRA), an instruction-conditioned, parameter-efficient adaptation framework that enables explicit control over internal feature reliance while keeping the predictive objective fixed. FiLoRA decomposes adaptation into feature group-aligned LoRA modules and applies instruction-conditioned gating, allowing natural language instructions to act as computation-level control signals rather than task redefinitions. Across text--image and audio--visual benchmarks, we show that instruction-conditioned gating induces consistent and causal shifts in internal computation, selectively amplifying or suppressing core and spurious feature groups without modifying the label space or training objective. Further analyses demonstrate that FiLoRA yields improved robustness under spurious feature interventions, revealing a principled mechanism to regulate reliance beyond correlation-driven learning.

</details>


### [904] [Learning to Route and Schedule LLMs from User Retrials via Contextual Queueing Bandits](https://arxiv.org/abs/2602.02061)
*Seoungbin Bae,Junyoung Son,Dabeen Lee*

Main category: cs.LG

Relevance: 85.0

TL;DR: 提出了一种基于上下文排队多臂老虎机与多项Logit反馈的联合路由调度算法，利用用户重试行为的隐式反馈来优化LLM服务中的查询路由和优先级调度。


<details>
  <summary>Details</summary>
Motivation: LLM服务中用户查询在服务器队列中积累，现有在线算法忽视了两个关键挑战：1）不满意的用户会重试查询，增加服务器积压；2）显式反馈请求（如评分）会降低用户体验。需要利用隐式反馈（用户重试行为）来优化路由和调度。

Method: 提出了CQB-MNL框架（上下文排队多臂老虎机与多项Logit反馈），建模查询重试和基于上下文的用户对LLM的偏好学习。设计了ACQB算法，结合Thompson采样和衰减率的强制探索，在保持队列稳定的同时实现高效学习。

Result: ACQB算法在路由方面实现了$\widetilde{\mathcal{O}}(\sqrt{t})$的累积遗憾，在队列长度方面实现了$\widetilde{\mathcal{O}}(t^{-1/4})$的遗憾。在SPROUT、EmbedLLM和RouterBench数据集上的实验表明，该算法持续优于基线方法。

Conclusion: 通过利用用户重试行为的隐式反馈，提出的CQB-MNL框架和ACQB算法能够有效解决LLM服务中的路由和调度问题，在保持队列稳定的同时优化用户体验。

Abstract: Explosive demands for LLMs often cause user queries to accumulate in server queues, requiring efficient routing (query-LLM matching) and scheduling (query prioritization) mechanisms. Several online algorithms are being deployed, but they overlook the following two key challenges inherent to conversational LLM services: (1) unsatisfied users may retry queries, increasing the server backlog, and (2) requests for ``explicit" feedback, such as ratings, degrade user experiences. In this paper, we develop a joint routing and scheduling algorithm that leverages ``implicit" feedback inferred from user retrial behaviors. The key idea is to propose and study the framework of contextual queueing bandits with multinomial logit feedback (CQB-MNL). CQB-MNL models query retrials, as well as context-based learning for user preferences over LLMs. Our algorithm, anytime CQB (ACQB), achieves efficient learning while maintaining queue stability by combining Thompson sampling with forced exploration at a decaying rate. We show that ACQB simultaneously achieves a cumulative regret of $\widetilde{\mathcal{O}}(\sqrt{t})$ for routing and a queue length regret of $\widetilde{\mathcal{O}}(t^{-1/4})$ for any large $t$. For experiments, we refine query embeddings via contrastive learning while adopting a disjoint parameter model to learn LLM-specific parameters. Experiments on SPROUT, EmbedLLM, and RouterBench datasets confirm that both algorithms consistently outperform baselines.

</details>


### [905] [BAPS: A Fine-Grained Low-Precision Scheme for Softmax in Attention via Block-Aware Precision reScaling](https://arxiv.org/abs/2602.02071)
*Zisheng Ye,Xiaoyu He,Maoyuan Song,Guoliang Qiu,Chao Liao,Chen Wu,Yonggang Sun,Zhichun Li,Xiaoru Xie,Yuanyong Luo,Hu Liu,Pinyan Lu,Heng Liao*

Main category: cs.LG

Relevance: 85.0

TL;DR: 提出一种低精度softmax工作流，使用8位浮点格式和分块精度重缩放，解决Transformer推理中softmax成为瓶颈的问题，可减少数据带宽需求和指数计算单元面积。


<details>
  <summary>Details</summary>
Motivation: 随着量化矩阵乘法加速的性能提升达到瓶颈，softmax操作成为Transformer推理的关键瓶颈。这源于两个硬件限制：(1)矩阵和向量计算核心之间的数据带宽有限；(2)高精度指数计算单元(FP32/16)的面积成本过高。

Method: 引入新颖的低精度工作流，采用特定的8位浮点格式(HiF8)和分块感知精度重缩放技术。关键创新包括：使矩阵乘法输出约束在8位以减半数据移动带宽，以及在低精度(8位)下计算指数以大幅减少指数计算单元面积。

Result: 在语言模型和多模态模型上的广泛评估证实了该方法的有效性。通过缓解向量计算瓶颈，可以在不增加芯片面积的情况下将端到端推理吞吐量翻倍。

Conclusion: 该工作为未来低精度硬件和软件提供了具体的协同设计路径，通过解决softmax瓶颈问题，为Transformer推理效率提升开辟了新方向。

Abstract: As the performance gains from accelerating quantized matrix multiplication plateau, the softmax operation becomes the critical bottleneck in Transformer inference. This bottleneck stems from two hardware limitations: (1) limited data bandwidth between matrix and vector compute cores, and (2) the significant area cost of high-precision (FP32/16) exponentiation units (EXP2). To address these issues, we introduce a novel low-precision workflow that employs a specific 8-bit floating-point format (HiF8) and block-aware precision rescaling for softmax. Crucially, our algorithmic innovations make low-precision softmax feasible without the significant model accuracy loss that hampers direct low-precision approaches. Specifically, our design (i) halves the required data movement bandwidth by enabling matrix multiplication outputs constrained to 8-bit, and (ii) substantially reduces the EXP2 unit area by computing exponentiations in low (8-bit) precision. Extensive evaluation on language models and multi-modal models confirms the validity of our method. By alleviating the vector computation bottleneck, our work paves the way for doubling end-to-end inference throughput without increasing chip area, and offers a concrete co-design path for future low-precision hardware and software.

</details>


### [906] [AICD Bench: A Challenging Benchmark for AI-Generated Code Detection](https://arxiv.org/abs/2602.02079)
*Daniil Orel,Dilshod Azizov,Indraneil Paul,Yuxia Wang,Iryna Gurevych,Preslav Nakov*

Main category: cs.LG

Relevance: 85.0

TL;DR: AICD Bench是一个全面的AI生成代码检测基准，包含200万样本、77个模型、11个模型家族和9种编程语言，引入了三种现实检测任务：鲁棒二元分类、模型家族归因和细粒度人机分类。


<details>
  <summary>Details</summary>
Motivation: 现有AI生成代码检测的数据集和基准过于狭窄，通常仅限于分布内设置的二元人机分类。随着LLM生成功能代码的能力增强，需要更全面的基准来应对作者身份、责任和安全方面的挑战。

Method: 构建了AICD Bench基准，包含大规模数据集（200万样本、77个模型、11个模型家族、9种编程语言），并设计了三种检测任务：1）分布偏移下的鲁棒二元分类；2）按架构谱系分组的模型家族归因；3）人类、机器、混合和对抗代码的细粒度分类。

Result: 对神经和经典检测器的广泛评估显示，性能远低于实际可用性，特别是在分布偏移以及混合或对抗代码的情况下。基准揭示了当前检测方法的局限性。

Conclusion: AICD Bench作为一个统一且具有挑战性的评估套件，旨在推动下一代鲁棒的AI生成代码检测方法的发展。当前检测技术在实际应用中仍面临重大挑战。

Abstract: Large language models (LLMs) are increasingly capable of generating functional source code, raising concerns about authorship, accountability, and security. While detecting AI-generated code is critical, existing datasets and benchmarks are narrow, typically limited to binary human-machine classification under in-distribution settings. To bridge this gap, we introduce $\emph{AICD Bench}$, the most comprehensive benchmark for AI-generated code detection. It spans $\emph{2M examples}$, $\emph{77 models}$ across $\emph{11 families}$, and $\emph{9 programming languages}$, including recent reasoning models. Beyond scale, AICD Bench introduces three realistic detection tasks: ($\emph{i}$)~$\emph{Robust Binary Classification}$ under distribution shifts in language and domain, ($\emph{ii}$)~$\emph{Model Family Attribution}$, grouping generators by architectural lineage, and ($\emph{iii}$)~$\emph{Fine-Grained Human-Machine Classification}$ across human, machine, hybrid, and adversarial code. Extensive evaluation on neural and classical detectors shows that performance remains far below practical usability, particularly under distribution shift and for hybrid or adversarial code. We release AICD Bench as a $\emph{unified, challenging evaluation suite}$ to drive the next generation of robust approaches for AI-generated code detection. The data and the code are available at https://huggingface.co/AICD-bench}.

</details>


### [907] [Two-Stage Grid Optimization for Group-wise Quantization of LLMs](https://arxiv.org/abs/2602.02126)
*Junhan Kim,Gukryeol Lee,Seungwoo Son,Jeewook Kim,Yongkweon Jeon*

Main category: cs.LG

Relevance: 85.0

TL;DR: 提出一个两阶段优化框架来改进GPTQ分组量化，通过最小化层重构损失来提升LLM低比特量化的精度


<details>
  <summary>Details</summary>
Motivation: GPTQ等现有分组量化方法在确定分组尺度时忽略了输入统计信息和组间相关性，导致与最小化层重构损失的目标不匹配，造成精度下降

Method: 两阶段优化框架：第一阶段在GPTQ之前初始化每个分组尺度以最小化分组重构损失；第二阶段冻结GPTQ得到的整数权重，使用坐标下降算法和闭式更新规则优化分组尺度以最小化层重构损失

Result: 实验结果表明该方法能持续提升分组量化精度，以可忽略的开销获得更高的准确率

Conclusion: 提出的两阶段优化框架通过显式最小化层重构损失，有效解决了GPTQ忽略输入统计和组间相关性的问题，显著提升了LLM低比特量化的性能

Abstract: Group-wise quantization is an effective strategy for mitigating accuracy degradation in low-bit quantization of large language models (LLMs). Among existing methods, GPTQ has been widely adopted due to its efficiency; however, it neglects input statistics and inter-group correlations when determining group scales, leading to a mismatch with its goal of minimizing layer-wise reconstruction loss. In this work, we propose a two-stage optimization framework for group scales that explicitly minimizes the layer-wise reconstruction loss. In the first stage, performed prior to GPTQ, we initialize each group scale to minimize the group-wise reconstruction loss, thereby incorporating input statistics. In the second stage, we freeze the integer weights obtained via GPTQ and refine the group scales to minimize the layer-wise reconstruction loss. To this end, we employ the coordinate descent algorithm and derive a closed-form update rule, which enables efficient refinement without costly numerical optimization. Notably, our derivation incorporates the quantization errors from preceding layers to prevent error accumulation. Experimental results demonstrate that our method consistently enhances group-wise quantization, achieving higher accuracy with negligible overhead.

</details>


### [908] [DCoPilot: Generative AI-Empowered Policy Adaptation for Dynamic Data Center Operations](https://arxiv.org/abs/2602.02137)
*Minghao Li,Ruihang Wang,Rui Tan,Yonggang Wen*

Main category: cs.LG

Relevance: 85.0

TL;DR: DCoPilot：一个结合LLM符号化生成结构化奖励函数和超网络参数化生成策略权重的混合框架，用于动态数据中心的高效控制策略生成。


<details>
  <summary>Details</summary>
Motivation: 现代数据中心运行在高功率密度和快速变化的工作负载下，需要分钟级适应能力。传统手动设计的DRL代理无法跟上数据中心频繁的动态变化和服务级别协议(SLA)更新，导致策略滞后和潜在服务中断。

Method: DCoPilot采用混合生成范式：1) LLM执行结构化奖励形式的符号生成；2) 超网络进行策略权重的参数生成。包含三个阶段：模拟扩展（在多样化场景中压力测试奖励候选）、元策略蒸馏（训练超网络输出基于SLA和场景嵌入的策略权重）、在线适应（实现零样本策略生成）。

Result: 在五个控制任务家族中评估，DCoPilot实现了接近零的约束违反，并在所有规格变化中优于所有基线方法。消融研究验证了基于LLM的统一奖励生成对稳定超网络收敛的有效性。

Conclusion: DCoPilot成功解决了数据中心动态操作中规范到策略的滞后问题，通过结合LLM的符号生成能力和超网络的参数生成能力，实现了及时有效的控制策略生成。

Abstract: Modern data centers (DCs) hosting artificial intelligence (AI)-dedicated devices operate at high power densities with rapidly varying workloads, making minute-level adaptation essential for safe and energy-efficient operation. However, manually designing piecewise deep reinforcement learning (DRL) agents cannot keep pace with frequent dynamics shifts and service-level agreement (SLA) changes of an evolving DC. This specification-to-policy lag causes a lack of timely, effective control policies, which may lead to service outages. To bridge the gap, we present DCoPilot, a hybrid framework for generative control policies in dynamic DC operation. DCoPilot synergizes two distinct generative paradigms, i.e., a large language model (LLM) that performs symbolic generation of structured reward forms, and a hypernetwork that conducts parametric generation of policy weights. DCoPilot operates through three coordinated phases: (i) simulation scale-up, which stress-tests reward candidates across diverse simulation-ready (SimReady) scenes; (ii) meta policy distillation, where a hypernetwork is trained to output policy weights conditioned on SLA and scene embeddings; and (iii) online adaptation, enabling zero-shot policy generation in response to updated specifications. Evaluated across five control task families spanning diverse DC components, DCoPilot achieves near-zero constraint violations and outperforms all baselines across specification variations. Ablation studies validate the effectiveness of LLM-based unified reward generation in enabling stable hypernetwork convergence.

</details>


### [909] [ECHO: Entropy-Confidence Hybrid Optimization for Test-Time Reinforcement Learning](https://arxiv.org/abs/2602.02150)
*Chu Zhao,Enneng Yang,Yuting Liu,Jianzhe Zhao,Guibing Guo*

Main category: cs.LG

Relevance: 85.0

TL;DR: 本文提出ECHO方法解决测试时强化学习中的两个关键问题：高熵分支导致的rollout崩溃和早期伪标签噪声引起的过早过拟合，通过自适应分支控制和置信度增强策略优化提升数学和视觉推理任务的性能。


<details>
  <summary>Details</summary>
Motivation: 测试时强化学习通过多次rollout生成候选答案并使用多数投票构建伪标签进行在线更新。现有树结构rollout方法虽然提高了采样效率，但仍面临两个挑战：1）高熵分支可能导致rollout崩溃，分支预算集中在少数具有连续高熵段的轨迹上；2）早期伪标签噪声大且有偏，可能引发自我强化的过拟合，导致策略过早锐化并抑制探索。

Method: 提出Entropy Confidence Hybrid Group Relative Policy Optimization (ECHO)方法：1）在rollout阶段，联合利用局部熵和组级置信度自适应控制分支宽度，引入在线置信度剪枝终止持续低置信度分支；2）在策略更新阶段，采用置信度自适应裁剪和熵-置信度混合优势塑造方法增强训练鲁棒性并减轻早期偏差。

Result: 实验表明ECHO在多个数学和视觉推理基准上取得一致性能提升，在有限rollout预算下具有更好的泛化能力。

Conclusion: ECHO通过自适应分支控制和置信度增强策略优化，有效解决了测试时强化学习中的rollout崩溃和早期偏差问题，提升了推理任务的性能和泛化能力。

Abstract: Test-time reinforcement learning generates multiple candidate answers via repeated rollouts and performs online updates using pseudo-labels constructed by majority voting. To reduce overhead and improve exploration, prior work introduces tree structured rollouts, which share reasoning prefixes and branch at key nodes to improve sampling efficiency. However, this paradigm still faces two challenges: (1) high entropy branching can trigger rollout collapse, where the branching budget concentrates on a few trajectories with consecutive high-entropy segments, rapidly reducing the number of effective branches; (2) early pseudo-labels are noisy and biased, which can induce self-reinforcing overfitting, causing the policy to sharpen prematurely and suppress exploration. To address these issues, we propose Entropy Confidence Hybrid Group Relative Policy Optimization (ECHO). During rollout, ECHO jointly leverages local entropy and group level confidence to adaptively control branch width, and further introduces online confidence-based pruning to terminate persistently low confidence branches, avoiding high entropy traps and mitigating collapse. During policy updates, ECHO employs confidence adaptive clipping and an entropy confidence hybrid advantage shaping approach to enhance training robustness and mitigate early stage bias. Experiments demonstrate that ECHO achieves consistent gains on multiple mathematical and visual reasoning benchmarks, and generalizes more effectively under a limited rollout budget.

</details>


### [910] [STILL: Selecting Tokens for Intra-Layer Hybrid Attention to Linearize LLMs](https://arxiv.org/abs/2602.02180)
*Weikang Meng,Liangyu Huo,Yadan Luo,Jiawen Guan,Jingyi Zhang,Yingjian Li,Zheng Zhang*

Main category: cs.LG

Relevance: 85.0

TL;DR: STILL提出了一种新的LLM线性化框架，通过自显著性评分和规范保持特征映射，在保持预训练表示的同时实现高效注意力计算，显著优于现有线性注意力方法。


<details>
  <summary>Details</summary>
Motivation: 现有LLM线性化方法存在两个主要问题：1）基于滑动窗口分区的token路由是位置选择，无法捕捉token特定的全局重要性；2）可学习特征映射导致分布偏移，扭曲预训练特征幅度。

Method: STILL框架包含三个核心组件：1）具有强局部-全局一致性的自显著性评分，用于准确token选择；2）规范保持特征映射（NP-Map），解耦特征方向和幅度并重新注入预训练规范；3）统一的训练-推理架构，采用分块并行化和延迟选择以提高硬件效率。

Result: 在常识和一般推理任务上，STILL匹配或超越了原始预训练模型；在长上下文基准测试中，相对于先前的线性化注意力方法实现了高达86.2%的相对改进。

Conclusion: STILL通过创新的自显著性评分和规范保持特征映射，有效解决了现有线性注意力方法的局限性，在保持模型性能的同时显著提升了长上下文处理效率。

Abstract: Linearizing pretrained large language models (LLMs) primarily relies on intra-layer hybrid attention mechanisms to alleviate the quadratic complexity of standard softmax attention. Existing methods perform token routing based on sliding-window partitions, resulting in position-based selection and fails to capture token-specific global importance. Meanwhile, linear attention further suffers from distribution shift caused by learnable feature maps that distort pretrained feature magnitudes. Motivated by these limitations, we propose STILL, an intra-layer hybrid linearization framework for efficiently linearizing LLMs. STILL introduces a Self-Saliency Score with strong local-global consistency, enabling accurate token selection using sliding-window computation, and retains salient tokens for sparse softmax attention while summarizing the remaining context via linear attention. To preserve pretrained representations, we design a Norm-Preserved Feature Map (NP-Map) that decouples feature direction from magnitude and reinjects pretrained norms. We further adopt a unified training-inference architecture with chunk-wise parallelization and delayed selection to improve hardware efficiency. Experiments show that STILL matches or surpasses the original pretrained model on commonsense and general reasoning tasks, and achieves up to a 86.2% relative improvement over prior linearized attention methods on long-context benchmarks.

</details>


### [911] [ECHO-2: A Large Scale Distributed Rollout Framework for Cost-efficient Reinforcement Learning](https://arxiv.org/abs/2602.02192)
*Jie Xiao,Meng Chen,Qingnan Ren,Song Jingwei,Jiaqi Huang,Yangshen Deng,Chris Tong,Wanyi Chen,Suli Wang,Ziqian Bi,Shuo Lu,Yiqun Duan,Lynn Ai,Eric Yang,Bill Shi*

Main category: cs.LG

Relevance: 85.0

TL;DR: ECHO-2是一个用于大语言模型后训练强化学习的分布式框架，通过远程推理工作节点和重叠策略传播来提升成本效率，同时保持RL奖励性能。


<details>
  <summary>Details</summary>
Motivation: 强化学习是LLM后训练的关键阶段，涉及生成、评估和学习的重复交互。分布式执行生成可以利用更经济的推理资源，但面临广域协调和策略传播的挑战。现有方法在传播延迟显著时效率低下。

Method: ECHO-2结合集中式学习和分布式生成，将策略陈旧度作为用户可控参数，允许生成、传播和训练重叠。引入基于重叠的容量模型，提供实际配置规则。采用对等辅助流水线广播和成本感知的异构工作节点激活来缓解传播瓶颈。

Result: 在真实广域带宽环境下对4B和8B模型进行GRPO后训练的实验表明，ECHO-2显著提高了成本效率，同时保持了与强基线相当的RL奖励性能。

Conclusion: ECHO-2通过分布式RL框架有效解决了后训练中的传播延迟问题，实现了成本效率与性能的平衡，为大规模LLM后训练提供了实用解决方案。

Abstract: Reinforcement learning (RL) is a critical stage in post-training large language models (LLMs), involving repeated interaction between rollout generation, reward evaluation, and centralized learning. Distributing rollout execution offers opportunities to leverage more cost-efficient inference resources, but introduces challenges in wide-area coordination and policy dissemination. We present ECHO-2, a distributed RL framework for post-training with remote inference workers and non-negligible dissemination latency. ECHO-2 combines centralized learning with distributed rollouts and treats bounded policy staleness as a user-controlled parameter, enabling rollout generation, dissemination, and training to overlap. We introduce an overlap-based capacity model that relates training time, dissemination latency, and rollout throughput, yielding a practical provisioning rule for sustaining learner utilization. To mitigate dissemination bottlenecks and lower cost, ECHO-2 employs peer-assisted pipelined broadcast and cost-aware activation of heterogeneous workers. Experiments on GRPO post-training of 4B and 8B models under real wide-area bandwidth regimes show that ECHO-2 significantly improves cost efficiency while preserving RL reward comparable to strong baselines.

</details>


### [912] [State Rank Dynamics in Linear Attention LLMs](https://arxiv.org/abs/2602.02195)
*Ao Sun,Hongtao Zhang,Heng Zhou,Yixuan Ma,Yiran Qin,Tongrui Su,Yan Liu,Zhanyu Ma,Jun Xu,Jiuchong Gao,Jinghua Hao,Renqing He*

Main category: cs.LG

Relevance: 85.0

TL;DR: 本文研究了线性注意力LLM的运行时状态动态，发现了状态秩分层现象：注意力头分为低秩和高秩两组，低秩头对推理至关重要，高秩头存在冗余，基于此提出了联合秩-范数剪枝方法，减少38.9%的KV缓存开销。


<details>
  <summary>Details</summary>
Motivation: 线性注意力LLM通过将上下文压缩到固定大小的状态矩阵中实现恒定时间推理，但这种压缩状态的内部动态机制尚不明确。本文旨在深入理解线性注意力模型的运行时状态动态，揭示其内在工作机制。

Method: 1. 对最先进的线性注意力模型进行全面的运行时状态动态研究
2. 发现状态秩分层现象：注意力头在谱特征上分为低秩和高秩两组
3. 通过跨不同推理上下文的大量实验验证动态一致性
4. 使用诊断探针分析功能差异
5. 提出联合秩-范数剪枝策略

Result: 1. 揭示了状态秩分层现象：低秩头有效秩在零附近振荡，高秩头有效秩快速增长并收敛到上界
2. 发现这种动态在不同推理上下文中保持高度一致，表明头的高/低秩特性是预训练获得的内在结构属性
3. 诊断发现功能分化：低秩头对模型推理不可或缺，高秩头存在显著冗余
4. 提出的联合秩-范数剪枝策略实现了38.9%的KV缓存开销减少，同时基本保持模型精度

Conclusion: 线性注意力LLM的注意力头存在内在的状态秩分层现象，这种分层反映了功能分化。利用这一发现可以开发高效的模型压缩策略，为线性注意力模型的优化和部署提供了新的理论指导和实用方法。

Abstract: Linear Attention Large Language Models (LLMs) offer a compelling recurrent formulation that compresses context into a fixed-size state matrix, enabling constant-time inference. However, the internal dynamics of this compressed state remain largely opaque. In this work, we present a comprehensive study on the runtime state dynamics of state-of-the-art Linear Attention models. We uncover a fundamental phenomenon termed State Rank Stratification, characterized by a distinct spectral bifurcation among linear attention heads: while one group maintains an effective rank oscillating near zero, the other exhibits rapid growth that converges to an upper bound. Extensive experiments across diverse inference contexts reveal that these dynamics remain strikingly consistent, indicating that the identity of a head,whether low-rank or high-rank,is an intrinsic structural property acquired during pre-training, rather than a transient state dependent on the input data. Furthermore, our diagnostic probes reveal a surprising functional divergence: low-rank heads are indispensable for model reasoning, whereas high-rank heads exhibit significant redundancy. Leveraging this insight, we propose Joint Rank-Norm Pruning, a zero-shot strategy that achieves a 38.9\% reduction in KV-cache overhead while largely maintaining model accuracy.

</details>


### [913] [Hierarchical Adaptive Eviction for KV Cache Management in Multimodal Language Models](https://arxiv.org/abs/2602.02197)
*Xindian Ma,Yidi Lu,Peng Zhang,Jing Zhang*

Main category: cs.LG

Relevance: 85.0

TL;DR: HAE是一个用于多模态大语言模型的KV缓存逐出框架，通过分层自适应策略优化视觉-文本token交互，在保持性能的同时显著减少内存使用和加速推理。


<details>
  <summary>Details</summary>
Motivation: Transformer架构的二次方内存和计算成本是多模态LLM的瓶颈，现有的KV缓存逐出策略未能解决视觉和文本token之间的异质注意力分布问题，导致效率低下或性能下降。

Method: 提出分层自适应逐出(HAE)框架：1) 预填充阶段使用双注意力剪枝，利用视觉token稀疏性和注意力方差；2) 解码阶段采用动态解码逐出策略（受操作系统回收站启发）；3) 通过索引广播减少计算开销。

Result: 在图像理解任务中减少41%的KV缓存内存，仅损失0.3%的准确率；在故事生成推理中加速1.5倍，同时保持Phi3.5-Vision-Instruct模型的输出质量。

Conclusion: HAE通过分层自适应策略有效解决了多模态LLM中视觉-文本token交互的效率问题，在保持性能的同时显著提升了内存效率和推理速度。

Abstract: The integration of visual information into Large Language Models (LLMs) has enabled Multimodal LLMs (MLLMs), but the quadratic memory and computational costs of Transformer architectures remain a bottleneck. Existing KV cache eviction strategies fail to address the heterogeneous attention distributions between visual and text tokens, leading to suboptimal efficiency or degraded performance. In this paper, we propose Hierarchical Adaptive Eviction (HAE), a KV cache eviction framework that optimizes text-visual token interaction in MLLMs by implementing Dual-Attention Pruning during pre-filling (leveraging visual token sparsity and attention variance) and a Dynamic Decoding Eviction Strategy (inspired by OS Recycle Bins) during decoding. HAE minimizes KV cache usage across layers, reduces computational overhead via index broadcasting, and theoretically ensures superior information integrity and lower error bounds compared to greedy strategies, enhancing efficiency in both comprehension and generation tasks. Empirically, HAE reduces KV-Cache memory by 41\% with minimal accuracy loss (0.3\% drop) in image understanding tasks and accelerates story generation inference by 1.5x while maintaining output quality on Phi3.5-Vision-Instruct model.

</details>


### [914] [Fat-Cat: Document-Driven Metacognitive Multi-Agent System for Complex Reasoning](https://arxiv.org/abs/2602.02206)
*Tong Yang,Yemin Wang,Chaoning Zhang,Aming Wu*

Main category: cs.LG

Relevance: 85.0

TL;DR: Fat-Cat是一个文档驱动的智能体架构，通过Markdown文档表示状态、文本策略演进和闭环监控，提高LLM智能体的上下文利用效率，在推理、检索和编码任务上表现优异。


<details>
  <summary>Details</summary>
Motivation: 现有LLM智能体框架使用嵌套JSON等语法繁重的状态表示，迫使模型将大量注意力消耗在语法处理而非语义推理上，限制了智能体效率。需要提高状态管理的信噪比。

Method: 提出Fat-Cat架构，包含三个核心组件：1）语义文件系统，用Markdown文档表示智能体状态；2）文本策略演进模块，积累任务解决知识；3）闭环监控器，减少幻觉。

Result: 在推理、检索和编码基准测试中表现优异，使Kimi-k2模型在HotPotQA上超越GPT-4o基线。实验表明文档驱动状态建模优于JSON表示。

Conclusion: 文档驱动的状态表示能显著提高LLM智能体的上下文利用效率，减少语法处理负担，增强语义推理能力，是提升智能体性能的关键设计选择。

Abstract: The effectiveness of LLM-based agents is often limited not by model capacity alone, but by how efficiently contextual information is utilized at runtime. Existing agent frameworks rely on rigid, syntax-heavy state representations such as nested JSON, which require models to devote a substantial portion of their limited attention to syntactic processing rather than semantic reasoning. In this paper, we propose Fat-Cat, a document-driven agent architecture that improves the signal-to-noise ratio of state management. By integrating three key components: (1) a Semantic File System that represents agent state as Markdown documents aligned with common pre-training corpora, (2) a Textual Strategy Evolution module that accumulates task-solving knowledge without parameter updates, and (3) a Closed-Loop Watcher that monitors reasoning trajectories to reduce hallucinations. Extensive reasoning, retrieval, and coding benchmarks, Fat-Cat consistently improves agent performance. It enables the Kimi-k2 model to outperform the proprietary GPT-4o baseline on HotPotQA. Replacing the document-based state with JSON leads to performance drop, while empirically validating the critical necessity of document-driven state modeling over rigid syntax. The code is available at https://github.com/answeryt/Fat-Cat.

</details>


### [915] [Scientific Theory of a Black-Box: A Life Cycle-Scale XAI Framework Based on Constructive Empiricism](https://arxiv.org/abs/2602.02215)
*Sebastian Müller,Vanessa Toborek,Eike Stadtländer,Tamás Horváth,Brendan Balcerak Jackson,Christian Bauckhage*

Main category: cs.LG

Relevance: 85.0

TL;DR: 提出"黑盒科学理论"(SToBB)框架，将可解释AI信息整合为伴随黑盒模型生命周期的持久可审计工件，通过CoBoT算法实现规则代理的在线构建与维护。


<details>
  <summary>Details</summary>
Motivation: 当前可解释AI算法多为孤立回答特定问题，缺乏将解释信息整合为伴随黑盒模型生命周期的持久、可审计工件的方法。需要一种原则性方式，使解释信息能够系统化、可追溯地支持模型全生命周期分析。

Method: 基于建构经验主义提出SToBB概念，要求满足：1)经验充分性(与所有观测行为一致)；2)适应性(通过明确更新承诺恢复充分性)；3)可审计性(透明记录假设、构建选择和更新行为)。实现为通用框架，包含可扩展观测库、可追溯假设类、构建与修订算法组件及第三方评估文档。提出CoBoT算法作为实例，在线构建和维护经验充分的规则代理。

Result: 开发了完整的SToBB实例，用于表格任务的神经网络分类器。CoBoT算法能够随着观测积累在线构建和维护经验充分的规则代理。SToBB成为生命周期尺度上可检查的参考点，支持一致、可重用的分析和系统外部审查。

Conclusion: SToBB框架填补了可解释AI领域的重要空白，提供了将解释信息整合为持久可审计工件的原则性方法。通过接口查询而非孤立方法输出，支持具体利益相关者需求，为黑盒模型提供全生命周期的可解释性支持。

Abstract: Explainable AI (XAI) offers a growing number of algorithms that aim to answer specific questions about black-box models. What is missing is a principled way to consolidate explanatory information about a fixed black-box model into a persistent, auditable artefact, that accompanies the black-box throughout its life cycle. We address this gap by introducing the notion of a scientific theory of a black (SToBB). Grounded in Constructive Empiricism, a SToBB fulfils three obligations: (i) empirical adequacy with respect to all available observations of black-box behaviour, (ii) adaptability via explicit update commitments that restore adequacy when new observations arrive, and (iii) auditability through transparent documentation of assumptions, construction choices, and update behaviour. We operationalise these obligations as a general framework that specifies an extensible observation base, a traceable hypothesis class, algorithmic components for construction and revision, and documentation sufficient for third-party assessment. Explanations for concrete stakeholder needs are then obtained by querying the maintained record through interfaces, rather than by producing isolated method outputs. As a proof of concept, we instantiate a complete SToBB for a neural-network classifier on a tabular task and introduce the Constructive Box Theoriser (CoBoT) algorithm, an online procedure that constructs and maintains an empirically adequate rule-based surrogate as observations accumulate. Together, these contributions position SToBBs as a life cycle-scale, inspectable point of reference that supports consistent, reusable analyses and systematic external scrutiny.

</details>


### [916] [Alignment-Aware Model Adaptation via Feedback-Guided Optimization](https://arxiv.org/abs/2602.02258)
*Gaurav Bhatt,Aditya Chinchure,Jiawei Zhou,Leonid Sigal*

Main category: cs.LG

Relevance: 85.0

TL;DR: 提出一种对齐感知的微调框架，通过策略梯度正则化整合外部对齐信号，使用自适应门控机制平衡监督和对齐梯度，并学习对完全未对齐输入的弃权行为。


<details>
  <summary>Details</summary>
Motivation: 传统微调方法主要优化任务目标，忽略了安全性和避免幻觉等关键对齐目标，导致下游微调可能破坏模型对齐或无法纠正已有的未对齐行为。

Method: 1) 基于策略梯度的正则化整合外部对齐信号；2) 自适应门控机制动态平衡监督和对齐梯度；3) 学习对完全未对齐输入的弃权行为；4) 将保守响应直接整合到微调模型中。

Result: 在通用和领域特定指令微调基准测试中，一致减少了有害和幻觉输出，同时保持下游任务性能。对对抗性微调、提示攻击和不安全初始化的鲁棒性分析显示良好效果。

Conclusion: 自适应门控对齐优化是保持对齐和恢复对齐的有效模型适应方法，能够在微调过程中同时维护任务性能和对齐目标。

Abstract: Fine-tuning is the primary mechanism for adapting foundation models to downstream tasks; however, standard approaches largely optimize task objectives in isolation and do not account for secondary yet critical alignment objectives (e.g., safety and hallucination avoidance). As a result, downstream fine-tuning can degrade alignment and fail to correct pre-existing misaligned behavior. We propose an alignment-aware fine-tuning framework that integrates feedback from an external alignment signal through policy-gradient-based regularization. Our method introduces an adaptive gating mechanism that dynamically balances supervised and alignment-driven gradients on a per-sample basis, prioritizing uncertain or misaligned cases while allowing well-aligned examples to follow standard supervised updates. The framework further learns abstention behavior for fully misaligned inputs, incorporating conservative responses directly into the fine-tuned model. Experiments on general and domain-specific instruction-tuning benchmarks demonstrate consistent reductions in harmful and hallucinated outputs without sacrificing downstream task performance. Additional analyses show robustness to adversarial fine-tuning, prompt-based attacks, and unsafe initializations, establishing adaptively gated alignment optimization as an effective approach for alignment-preserving and alignment-recovering model adaptation.

</details>


### [917] [HopFormer: Sparse Graph Transformers with Explicit Receptive Field Control](https://arxiv.org/abs/2602.02268)
*Sanggeon Yun,Raheeb Hassan,Ryozo Masukawa,Sungheon Jeong,Mohsen Imani*

Main category: cs.LG

Relevance: 85.0

TL;DR: HopFormer：一种仅通过头特定的n-hop掩码稀疏注意力注入图结构的图Transformer，无需位置编码或架构修改，在计算成本随掩码稀疏度线性扩展的同时提供显式和可解释的接受域控制。


<details>
  <summary>Details</summary>
Motivation: 挑战当前图Transformer设计中的普遍假设，即需要显式位置/结构编码和密集全局注意力来融入图拓扑。研究表明这两者都不是必需的，并提出通过稀疏注意力实现更高效和可解释的图Transformer设计。

Method: 引入HopFormer，通过头特定的n-hop掩码稀疏注意力注入图结构，不使用位置编码或架构修改。设计提供显式和可解释的接受域控制，计算成本随掩码稀疏度线性扩展。在节点级和图级基准上进行广泛实验。

Result: 在多样化图结构上实现竞争性或优越性能。结果显示密集全局注意力通常不必要：在强小世界属性图上，局部注意力产生更稳定和一致的高性能；在弱小世界效应图上，全局注意力收益递减。

Conclusion: 挑战了图Transformer设计中的普遍假设，突出稀疏控制注意力作为原则性和高效的替代方案。为图Transformer设计提供了新的视角，强调结构注入和注意力稀疏化的重要性。

Abstract: Graph Transformers typically rely on explicit positional or structural encodings and dense global attention to incorporate graph topology. In this work, we show that neither is essential. We introduce HopFormer, a graph Transformer that injects structure exclusively through head-specific n-hop masked sparse attention, without the use of positional encodings or architectural modifications. This design provides explicit and interpretable control over receptive fields while enabling genuinely sparse attention whose computational cost scales linearly with mask sparsity. Through extensive experiments on both node-level and graph-level benchmarks, we demonstrate that our approach achieves competitive or superior performance across diverse graph structures. Our results further reveal that dense global attention is often unnecessary: on graphs with strong small-world properties, localized attention yields more stable and consistently high performance, while on graphs with weaker small-world effects, global attention offers diminishing returns. Together, these findings challenge prevailing assumptions in graph Transformer design and highlight sparsity-controlled attention as a principled and efficient alternative.

</details>


### [918] [EvalQReason: A Framework for Step-Level Reasoning Evaluation in Large Language Models](https://arxiv.org/abs/2602.02295)
*Shaima Ahmad Freja,Ferhat Ozgur Catak,Betul Yurdem,Chunming Rong*

Main category: cs.LG

Relevance: 85.0

TL;DR: EvalQReason：无需人工标注，通过步骤级概率分布分析量化LLM推理质量的框架，包含相邻步骤差异度和步骤到最终收敛度两个算法，在数学和医学数据集上验证了推理动态的领域特异性。


<details>
  <summary>Details</summary>
Motivation: 当前LLM推理评估方法主要关注最终答案正确性，缺乏对中间推理过程的系统性评估。现有方法无法深入了解推理步骤如何展开，这在关键应用中限制了可靠性的评估。

Method: 提出EvalQReason框架，通过步骤级概率分布分析量化推理质量。包含两个算法：1) 相邻步骤差异度(CSD)：测量相邻推理步骤间的局部一致性；2) 步骤到最终收敛度(SFC)：评估与最终答案的全局对齐。每个算法使用五个统计指标捕捉推理动态。

Result: 在数学和医学数据集上使用开源7B参数模型测试：CSD特征在正确性分类中表现优异，传统机器学习模型达到F1=0.78和ROC-AUC=0.82，序列神经网络模型显著提升性能(F1=0.88, ROC-AUC=0.97)。CSD始终优于SFC，序列架构优于传统机器学习。推理动态具有领域特异性：数学推理显示清晰的差异模式，而医学推理几乎没有判别信号。

Conclusion: EvalQReason实现了可扩展的、过程感知的推理可靠性评估，建立了基于概率的差异分析作为可信AI部署的原则性方法。揭示了LLM处理不同类型推理的基本差异。

Abstract: Large Language Models (LLMs) are increasingly deployed in critical applications requiring reliable reasoning, yet their internal reasoning processes remain difficult to evaluate systematically. Existing methods focus on final-answer correctness, providing limited insight into how reasoning unfolds across intermediate steps. We present EvalQReason, a framework that quantifies LLM reasoning quality through step-level probability distribution analysis without requiring human annotation. The framework introduces two complementary algorithms: Consecutive Step Divergence (CSD), which measures local coherence between adjacent reasoning steps, and Step-to-Final Convergence (SFC), which assesses global alignment with final answers. Each algorithm employs five statistical metrics to capture reasoning dynamics. Experiments across mathematical and medical datasets with open-source 7B-parameter models demonstrate that CSD-based features achieve strong predictive performance for correctness classification, with classical machine learning models reaching F1=0.78 and ROC-AUC=0.82, and sequential neural models substantially improving performance (F1=0.88, ROC-AUC=0.97). CSD consistently outperforms SFC, and sequential architectures outperform classical machine learning approaches. Critically, reasoning dynamics prove domain-specific: mathematical reasoning exhibits clear divergence-based discrimination patterns between correct and incorrect solutions, while medical reasoning shows minimal discriminative signals, revealing fundamental differences in how LLMs process different reasoning types. EvalQReason enables scalable, process-aware evaluation of reasoning reliability, establishing probability-based divergence analysis as a principled approach for trustworthy AI deployment.

</details>


### [919] [ReasonCACHE: Teaching LLMs To Reason Without Weight Updates](https://arxiv.org/abs/2602.02366)
*Sharut Gupta,Phillip Isola,Stefanie Jegelka,David Lopez-Paz,Kartik Ahuja,Mark Ibrahim,Mohammad Pezeshki*

Main category: cs.LG

Relevance: 85.0

TL;DR: ReasonCACHE通过前缀调优将推理演示蒸馏到固定的键值缓存中，使LLM无需权重更新即可学习推理，在保持高效的同时超越标准上下文学习并匹配权重学习方法。


<details>
  <summary>Details</summary>
Motivation: 当前上下文学习(ICL)虽然样本效率高，但在复杂推理任务上需要大量演示示例，而简单增加演示会导致注意力成本二次增长、性能饱和或下降。权重学习(IWL)虽然有效但需要参数更新。需要一种既能超越ICL限制又无需权重更新的推理学习方法。

Method: 提出ReasonCACHE，基于前缀调优机制，将推理演示蒸馏到固定的键值缓存中。该方法直接向注意力机制注入键值对，绕过输入秩限制，使模型能够学习超出上下文窗口的推理技能而不修改参数。

Result: 在GPQA-Diamond等挑战性推理基准测试中，ReasonCACHE优于标准ICL，匹配或超越IWL方法。同时在数据效率、推理成本和可训练参数三个关键维度上更高效。

Conclusion: ReasonCACHE在上下文学习和权重学习之间提供了一条中间路径，为学习超出上下文窗口的推理技能提供了可扩展算法，无需修改模型参数。

Abstract: Can Large language models (LLMs) learn to reason without any weight update and only through in-context learning (ICL)? ICL is strikingly sample-efficient, often learning from only a handful of demonstrations, but complex reasoning tasks typically demand many training examples to learn from. However, naively scaling ICL by adding more demonstrations breaks down at this scale: attention costs grow quadratically, performance saturates or degrades with longer contexts, and the approach remains a shallow form of learning. Due to these limitations, practitioners predominantly rely on in-weight learning (IWL) to induce reasoning. In this work, we show that by using Prefix Tuning, LLMs can learn to reason without overloading the context window and without any weight updates. We introduce $\textbf{ReasonCACHE}$, an instantiation of this mechanism that distills demonstrations into a fixed key-value cache. Empirically, across challenging reasoning benchmarks, including GPQA-Diamond, ReasonCACHE outperforms standard ICL and matches or surpasses IWL approaches. Further, it achieves this all while being more efficient across three key axes: data, inference cost, and trainable parameters. We also theoretically prove that ReasonCACHE can be strictly more expressive than low-rank weight update since the latter ties expressivity to input rank, whereas ReasonCACHE bypasses this constraint by directly injecting key-values into the attention mechanism. Together, our findings identify ReasonCACHE as a middle path between in-context and in-weight learning, providing a scalable algorithm for learning reasoning skills beyond the context window without modifying parameters. Our project page: https://reasoncache.github.io/

</details>


### [920] [Transformers learn factored representations](https://arxiv.org/abs/2602.02385)
*Adam Shai,Loren Amdahl-Culleton,Casper L. Christensen,Henry R. Bigelow,Fernando E. Rosas,Alexander B. Boyd,Eric A. Alt,Kyle J. Ray,Paul M. Riechers*

Main category: cs.LG

Relevance: 85.0

TL;DR: Transformer模型通过因子分解将世界表示分解为正交子空间，在条件独立时实现无损表示，否则在维度效率和准确性间权衡


<details>
  <summary>Details</summary>
Motivation: 研究Transformer模型如何通过下一个token预测学习将世界分解为部分，并探索其表示假设：是乘积空间表示还是因子分解表示

Method: 提出两种表示假设的数学形式化，推导几何结构预测，在具有已知潜在结构的合成过程上训练Transformer进行验证

Result: 当因子条件独立时，模型学习因子分解表示；即使存在噪声或隐藏依赖破坏条件独立性，模型在训练早期仍偏好因子分解，表明存在以保真度为代价的因子分解归纳偏置

Conclusion: Transformer倾向于将世界分解为部分，这种可解释的低维结构即使在复杂数据训练的模型中也可能持续存在

Abstract: Transformers pretrained via next token prediction learn to factor their world into parts, representing these factors in orthogonal subspaces of the residual stream. We formalize two representational hypotheses: (1) a representation in the product space of all factors, whose dimension grows exponentially with the number of parts, or (2) a factored representation in orthogonal subspaces, whose dimension grows linearly. The factored representation is lossless when factors are conditionally independent, but sacrifices predictive fidelity otherwise, creating a tradeoff between dimensional efficiency and accuracy. We derive precise predictions about the geometric structure of activations for each, including the number of subspaces, their dimensionality, and the arrangement of context embeddings within them. We test between these hypotheses on transformers trained on synthetic processes with known latent structure. Models learn factored representations when factors are conditionally independent, and continue to favor them early in training even when noise or hidden dependencies undermine conditional independence, reflecting an inductive bias toward factoring at the cost of fidelity. This provides a principled explanation for why transformers decompose the world into parts, and suggests that interpretable low dimensional structure may persist even in models trained on complex data.

</details>


### [921] [David vs. Goliath: Verifiable Agent-to-Agent Jailbreaking via Reinforcement Learning](https://arxiv.org/abs/2602.02395)
*Samuel Nellessen,Tal Kachman*

Main category: cs.LG

Relevance: 85.0

TL;DR: Slingshot框架通过强化学习自动发现Tag-Along攻击，使无工具攻击者能够利用安全对齐操作员的工具权限，在极端难度任务上实现67%攻击成功率，并能零样本迁移到多种模型。


<details>
  <summary>Details</summary>
Motivation: 随着大语言模型演变为自主代理，传统的安全评估从主观NLP任务转变为客观控制问题。现有方法未能充分评估工具增强环境中利用合法工具权限的对抗性攻击威胁。

Method: 提出Tag-Along攻击威胁模型，并开发Slingshot强化学习框架，从"冷启动"自主发现攻击向量。该框架通过环境交互学习短指令式语法模式而非多轮说服策略。

Result: 在极端难度任务上，Slingshot对Qwen2.5-32B-Instruct-AWQ操作员达到67.0%攻击成功率（基线1.7%），首次成功尝试次数从52.3降至1.3。零样本迁移到Gemini 2.5 Flash（56.0%）和Meta-SecAlign-8B（39.2%）等模型。

Conclusion: Tag-Along攻击是可验证的一级威胁模型，表明仅通过环境交互就能从现成开源模型引发有效的代理攻击，对LLM代理安全评估有重要启示。

Abstract: The evolution of large language models into autonomous agents introduces adversarial failures that exploit legitimate tool privileges, transforming safety evaluation in tool-augmented environments from a subjective NLP task into an objective control problem. We formalize this threat model as Tag-Along Attacks: a scenario where a tool-less adversary "tags along" on the trusted privileges of a safety-aligned Operator to induce prohibited tool use through conversation alone. To validate this threat, we present Slingshot, a 'cold-start' reinforcement learning framework that autonomously discovers emergent attack vectors, revealing a critical insight: in our setting, learned attacks tend to converge to short, instruction-like syntactic patterns rather than multi-turn persuasion. On held-out extreme-difficulty tasks, Slingshot achieves a 67.0% success rate against a Qwen2.5-32B-Instruct-AWQ Operator (vs. 1.7% baseline), reducing the expected attempts to first success (on solved tasks) from 52.3 to 1.3. Crucially, Slingshot transfers zero-shot to several model families, including closed-source models like Gemini 2.5 Flash (56.0% attack success rate) and defensive-fine-tuned open-source models like Meta-SecAlign-8B (39.2% attack success rate). Our work establishes Tag-Along Attacks as a first-class, verifiable threat model and shows that effective agentic attacks can be elicited from off-the-shelf open-weight models through environment interaction alone.

</details>


### [922] [An Empirical Study on Noisy Data and LLM Pretraining Loss Divergence](https://arxiv.org/abs/2602.02400)
*Qizhen Zhang,Ankush Garg,Jakob Foerster,Niladri Chatterji,Kshitiz Malik,Mike Lewis*

Main category: cs.LG

Relevance: 85.0

TL;DR: 本文通过系统实证研究，探究噪声数据如何导致大语言模型预训练发散，发现噪声类型、数量和模型规模是影响发散概率的关键因素。


<details>
  <summary>Details</summary>
Motivation: 大规模预训练数据集包含大量噪声数据，业界推测这些噪声可能导致LLM预训练不稳定甚至损失发散，但这一现象缺乏系统研究。本文旨在实证探究噪声数据是否以及如何导致预训练发散。

Method: 在原本干净的数据集中注入受控的合成均匀随机噪声，分析从480M到5.2B参数规模模型的训练动态，研究噪声类型、数量和模型规模对训练发散的影响。

Result: 噪声数据确实会诱导训练损失发散，发散概率强烈依赖于噪声类型、噪声量和模型规模。噪声诱导的发散表现出与高学习率导致发散不同的激活模式，并提供了区分这两种失效模式的诊断方法。

Conclusion: 研究提供了关于噪声数据如何影响LLM预训练损失发散的大规模、受控特征描述，为理解预训练不稳定性提供了实证基础。

Abstract: Large-scale pretraining datasets drive the success of large language models (LLMs). However, these web-scale corpora inevitably contain large amounts of noisy data due to unregulated web content or randomness inherent in data. Although LLM pretrainers often speculate that such noise contributes to instabilities in large-scale LLM pretraining and, in the worst cases, loss divergence, this phenomenon remains poorly understood.In this work, we present a systematic empirical study of whether noisy data causes LLM pretraining divergences and how it does so. By injecting controlled synthetic uniformly random noise into otherwise clean datasets, we analyze training dynamics across model sizes ranging from 480M to 5.2B parameters. We show that noisy data indeed induces training loss divergence, and that the probability of divergence depends strongly on the noise type, amount of noise, and model scale. We further find that noise-induced divergences exhibit activation patterns distinct from those caused by high learning rates, and we provide diagnostics that differentiate these two failure modes. Together, these results provide a large-scale, controlled characterization of how noisy data affects loss divergence in LLM pretraining.

</details>


### [923] [Didactic to Constructive: Turning Expert Solutions into Learnable Reasoning](https://arxiv.org/abs/2602.02405)
*Ethan Mendes,Jungsoo Park,Alan Ritter*

Main category: cs.LG

Relevance: 85.0

TL;DR: DAIL方法通过两步法利用专家解决方案提升LLM推理能力：先转换专家方案为详细推理轨迹，再使用对比目标聚焦学习专家洞察和方法，仅需少量专家数据即可显著提升模型性能。


<details>
  <summary>Details</summary>
Motivation: 当前提升LLM推理能力的方法存在局限性：要么依赖模型自身采样正确解进行强化，要么需要更强模型解决问题。许多难题对前沿模型仍不可解，无法提取有效训练信号。专家解决方案质量高但分布外，且成本昂贵，需要样本高效的通用训练方法。

Method: 提出分布对齐模仿学习（DAIL）两步法：1）将专家解决方案转换为详细、分布内的推理轨迹，弥合分布差距；2）应用对比目标，聚焦学习专家洞察和方法论。

Result: 仅需少于1000个高质量专家解决方案，即可在Qwen2.5-Instruct和Qwen3模型上实现10-25%的pass@k提升，推理效率提高2-4倍，并实现跨领域泛化。

Conclusion: DAIL方法有效解决了专家解决方案分布外的问题，通过少量高质量专家数据显著提升LLM推理能力和效率，为样本高效的LLM推理训练提供了可行方案。

Abstract: Improving the reasoning capabilities of large language models (LLMs) typically relies either on the model's ability to sample a correct solution to be reinforced or on the existence of a stronger model able to solve the problem. However, many difficult problems remain intractable for even current frontier models, preventing the extraction of valid training signals. A promising alternative is to leverage high-quality expert human solutions, yet naive imitation of this data fails because it is fundamentally out of distribution: expert solutions are typically didactic, containing implicit reasoning gaps intended for human readers rather than computational models. Furthermore, high-quality expert solutions are expensive, necessitating generalizable sample-efficient training methods. We propose Distribution Aligned Imitation Learning (DAIL), a two-step method that bridges the distributional gap by first transforming expert solutions into detailed, in-distribution reasoning traces and then applying a contrastive objective to focus learning on expert insights and methodologies. We find that DAIL can leverage fewer than 1000 high-quality expert solutions to achieve 10-25% pass@k gains on Qwen2.5-Instruct and Qwen3 models, improve reasoning efficiency by 2x to 4x, and enable out-of-domain generalization.

</details>


### [924] [Poly-attention: a general scheme for higher-order self-attention](https://arxiv.org/abs/2602.02422)
*Sayak Chakrabarti,Toniann Pitassi,Josh Alman*

Main category: cs.LG

Relevance: 85.0

TL;DR: 本文提出了"多注意力机制"（poly-attention），作为自注意力机制的高阶泛化，能够处理任意高阶张量计算和输入标记间的复杂关系结构。作者系统研究了其计算复杂性和表示能力，给出了新算法和匹配的下界，并提出了一个能在二次时间内精确计算且能执行任意固定数量函数组合的新注意力机制。


<details>
  <summary>Details</summary>
Motivation: 自注意力机制虽然能有效建模标记间的成对交互，但无法处理涉及三个相关标记检测或需要引用多个输入标记的组合任务。现有高阶注意力替代方案（如高阶注意力和Strassen注意力）虽然能处理某些多元任务，但需要超二次运行时间。本文旨在定义更广泛的自注意力泛化机制，并系统研究其计算复杂性和表示能力。

Method: 定义了称为"多注意力机制"（poly-attention）的广泛自注意力泛化类，能够包含任意高阶张量计算和输入标记间的任意关系结构。系统研究了这些机制的计算复杂性和表示能力，包括给出新算法和匹配的计算复杂度下界，精确和近似计算注意力矩阵的时间复杂度分析，以及确定它们能执行哪些多元任务。

Result: 提出了一个能在二次时间内精确计算的新注意力机制，该机制能执行任意固定数量函数的组合。证明了先前机制（即使只组合两个函数）只能在超二次时间内计算，且新的下界表明更快的算法是不可能的。揭示了机制表达能力与模型系数大小之间的紧密关系，以及这些机制不同需求之间的权衡。

Conclusion: 多注意力机制为自注意力提供了强大的高阶泛化框架，能够在计算效率和表达能力之间实现新的权衡。提出的新机制在二次时间内实现了函数组合能力，填补了先前机制的效率空白，并为注意力机制的设计提供了理论指导。

Abstract: The self-attention mechanism, at the heart of the Transformer model, is able to effectively model pairwise interactions between tokens. However, numerous recent works have shown that it is unable to perform basic tasks involving detecting triples of correlated tokens, or compositional tasks where multiple input tokens need to be referenced to generate a result. Some higher-dimensional alternatives to self-attention have been proposed to address this, including higher-order attention and Strassen attention, which can perform some of these polyadic tasks in exchange for slower, superquadratic running times.
  In this work, we define a vast class of generalizations of self-attention, which we call poly-attention mechanisms. Our mechanisms can incorporate arbitrary higher-order (tensor) computations as well as arbitrary relationship structures between the input tokens, and they include the aforementioned alternatives as special cases. We then systematically study their computational complexity and representational strength, including giving new algorithms and matching complexity-theoretic lower bounds on the time complexity of computing the attention matrix exactly as well as approximately, and tightly determining which polyadic tasks they can each perform. Our results give interesting trade-offs between different desiderata for these mechanisms, including a tight relationship between how expressive a mechanism is, and how large the coefficients in the model may be so that the mechanism can be approximated in almost-linear time.
  Notably, we give a new attention mechanism which can be computed exactly in quadratic time, and which can perform function composition for any fixed number of functions. Prior mechanisms, even for just composing two functions, could only be computed in superquadratic time, and our new lower bounds show that faster algorithms for them are not possible.

</details>


### [925] [Embedding Perturbation may Better Reflect the Uncertainty in LLM Reasoning](https://arxiv.org/abs/2602.02427)
*Qihao Wen,Jiahao Wang,Yang Nan,Pengfei He,Ravi Tandon,Han Xu*

Main category: cs.LG

Relevance: 85.0

TL;DR: 该论文提出了一种基于扰动敏感性的不确定性量化方法，用于识别大语言模型推理过程中的错误中间步骤，相比传统方法具有更好的性能和效率。


<details>
  <summary>Details</summary>
Motivation: 大语言模型在推理任务中可能产生不可靠的输出，需要对中间推理步骤的不确定性进行量化，以实现更细粒度的干预。现有方法主要关注最终答案的不确定性，而中间步骤的不确定性量化研究不足。

Method: 提出基于扰动敏感性的不确定性量化指标：通过对前一个token的嵌入进行微小扰动，测量后续token的敏感性。错误推理步骤中的token对前序token嵌入的扰动更加敏感，利用这种敏感性得分可以识别不确定的中间步骤。

Result: 实验表明，基于扰动的指标在不确定性量化性能上优于基线方法（如token生成概率和token熵）。该方法不需要多次采样，具有更好的简洁性和效率。

Conclusion: 基于扰动敏感性的不确定性量化方法能有效识别LLM推理过程中的错误中间步骤，为更精细的干预提供了实用指导，在性能和效率方面优于传统方法。

Abstract: Large language Models (LLMs) have achieved significant breakthroughs across diverse domains; however, they can still produce unreliable or misleading outputs. For responsible LLM application, Uncertainty Quantification (UQ) techniques are used to estimate a model's uncertainty about its outputs, indicating the likelihood that those outputs may be problematic. For LLM reasoning tasks, it is essential to estimate the uncertainty not only for the final answer, but also for the intermediate steps of the reasoning, as this can enable more fine-grained and targeted interventions. In this study, we explore what UQ metrics better reflect the LLM's ``intermediate uncertainty''during reasoning. Our study reveals that an LLMs' incorrect reasoning steps tend to contain tokens which are highly sensitive to the perturbations on the preceding token embeddings. In this way, incorrect (uncertain) intermediate steps can be readily identified using this sensitivity score as guidance in practice. In our experiments, we show such perturbation-based metric achieves stronger uncertainty quantification performance compared with baseline methods such as token (generation) probability and token entropy. Besides, different from approaches that rely on multiple sampling, the perturbation-based metrics offer better simplicity and efficiency.

</details>


### [926] [Certain Head, Uncertain Tail: Expert-Sample for Test-Time Scaling in Fine-Grained MoE](https://arxiv.org/abs/2602.02443)
*Yuanteng Chen,Peisong Wang,Nanxin Zeng,Yuantian Shao,Gang Li,Jing Liu,Jian Cheng*

Main category: cs.LG

Relevance: 85.0

TL;DR: 提出Expert-Sample方法，通过在高置信度专家选择基础上向不确定尾部注入可控随机性，提升细粒度MoE模型的多样本推理性能，无需训练即可改善pass@n和验证准确率。


<details>
  <summary>Details</summary>
Motivation: 测试时缩放通过生成多个候选解提升LLM性能，但token级采样需要温度调优来平衡多样性与稳定性。细粒度MoE具有每层数百个专家和每个token激活多个专家的特性，其丰富的路由空间提供了未探索的替代方案。研究发现路由器分数呈现高置信度专家头部和低置信度候选尾部的模式，头部控制核心推理能力，尾部与推理多样性相关。

Method: 提出Expert-Sample方法：1）保留高置信度专家选择；2）向不确定尾部注入可控随机性；3）无需训练即可实现多样化生成而不破坏输出稳定性。该方法在多个细粒度MoE模型上进行评估。

Result: 在数学、知识推理和代码任务上，Expert-Sample一致提升pass@n和验证准确率。在Qwen3-30B-A3B-Instruct模型上，GPQA-Diamond任务的pass@32从85.4%提升到91.9%，Best-of-N验证准确率从59.1%提升到62.6%。

Conclusion: Expert-Sample通过利用细粒度MoE路由模式，在保持核心推理能力的同时注入多样性，有效提升了多样本推理性能，为测试时缩放提供了新的无训练方法。

Abstract: Test-time scaling improves LLM performance by generating multiple candidate solutions, yet token-level sampling requires temperature tuning that trades off diversity against stability. Fine-grained MoE, featuring hundreds of well-trained experts per layer and multi-expert activation per token, offers an unexplored alternative through its rich routing space. We empirically characterize fine-grained MoE routing and uncover an informative pattern: router scores exhibit a certain head of high-confidence experts followed by an uncertain tail of low-confidence candidates. While single-run greedy accuracy remains stable when fewer experts are activated, multi-sample pass@n degrades significantly-suggesting that the certain head governs core reasoning capability while the uncertain tail correlates with reasoning diversity. Motivated by these findings, we propose Expert-Sample, a training-free method that preserves high-confidence selections while injecting controlled stochasticity into the uncertain tail, enabling diverse generation without destabilizing outputs. Evaluated on multiple fine-grained MoE models across math, knowledge reasoning, and code tasks, Expert-Sample consistently improves pass@n and verification-based accuracy. On Qwen3-30B-A3B-Instruct evaluated on GPQA-Diamond with 32 parallel samples, pass@32 rises from 85.4% to 91.9%, and accuracy improves from 59.1% to 62.6% with Best-of-N verification.

</details>


### [927] [Expanding the Capabilities of Reinforcement Learning via Text Feedback](https://arxiv.org/abs/2602.02482)
*Yuda Song,Lili Chen,Fahim Tajwar,Remi Munos,Deepak Pathak,J. Andrew Bagnell,Aarti Singh,Andrea Zanette*

Main category: cs.LG

Relevance: 85.0

TL;DR: 论文提出RLTF（从文本反馈的强化学习），利用文本反馈作为介于稀疏二元奖励和昂贵演示之间的中间监督信号，通过两种方法（自我蒸馏和反馈建模）让模型内化反馈以提升单轮推理性能。


<details>
  <summary>Details</summary>
Motivation: 当前LLM后训练中，RL主要依赖二元奖励或偏好标签（信息稀疏），而蒸馏需要昂贵的演示。文本反馈作为中间信号：比标量奖励更丰富，比完整演示更便宜，且是人类自然交互方式，在现实场景中已大量存在。

Method: 提出RLTF框架：多轮RL设置，训练时可用文本反馈但推理时不可用。两种具体方法：1) RLTF-SD（自我蒸馏）：训练单轮策略匹配自身反馈条件下的第二轮生成；2) RLTF-FM（反馈建模）：将预测反馈作为辅助目标。两种方法都旨在让模型内化反馈信息。

Result: 在推理谜题、竞赛数学和创意写作任务上的实验表明，两种方法均显著优于强基线，验证了利用丰富文本反馈进行大规模RL的潜力。

Conclusion: 文本反馈是RL训练LLM的有效中间监督信号，RLTF框架及其两种方法能成功利用文本反馈提升模型性能，为大规模RL训练提供了新方向。

Abstract: The success of RL for LLM post-training stems from an unreasonably uninformative source: a single bit of information per rollout as binary reward or preference label. At the other extreme, distillation offers dense supervision but requires demonstrations, which are costly and difficult to scale. We study text feedback as an intermediate signal: richer than scalar rewards, yet cheaper than complete demonstrations. Textual feedback is a natural mode of human interaction and is already abundant in many real-world settings, where users, annotators, and automated judges routinely critique LLM outputs. Towards leveraging text feedback at scale, we formalize a multi-turn RL setup, RL from Text Feedback (RLTF), where text feedback is available during training but not at inference. Therefore, models must learn to internalize the feedback in order to improve their test-time single-turn performance. To do this, we propose two methods: Self Distillation (RLTF-SD), which trains the single-turn policy to match its own feedback-conditioned second-turn generations; and Feedback Modeling (RLTF-FM), which predicts the feedback as an auxiliary objective. We provide theoretical analysis on both methods, and empirically evaluate on reasoning puzzles, competition math, and creative writing tasks. Our results show that both methods consistently outperform strong baselines across benchmarks, highlighting the potential of RL with an additional source of rich supervision at scale.

</details>


### [928] [Hessian Spectral Analysis at Foundation Model Scale](https://arxiv.org/abs/2602.00816)
*Diego Granziol,Khurshid Juarev*

Main category: stat.ML

Relevance: 85.0

TL;DR: 首次在百亿参数规模上实现了基础模型真实Hessian矩阵的谱分析，揭示了传统块对角近似在中等规模LLM中可能产生灾难性误差


<details>
  <summary>Details</summary>
Motivation: 基础模型的Hessian谱分析一直难以实现，现有工作只能依赖小模型或强结构近似，需要开发能在前沿规模上准确分析真实Hessian谱的方法

Method: 使用与完全分片数据并行兼容的分片局部有限差分Hessian向量积，在开源语言模型（最大100B参数）上执行随机Lanczos求积，分析数值行为（有限差分偏差、浮点噪声放大、Krylov稳定性）

Result: 首次获得超10B参数规模的大规模谱密度估计，发现广泛使用的块对角曲率近似可能产生阶一相对误差和方向对齐差，端到端运行时和内存缩放规律显示全算子谱探测仅带来适度常数因子开销

Conclusion: 基础模型Hessian谱既可计算又常被现有近似严重误判，为大规模基于曲率的分析开辟了道路

Abstract: Accurate Hessian spectra of foundation models have remained out of reach, leading most prior work to rely on small models or strong structural approximations. We show that faithful spectral analysis of the true Hessian is tractable at frontier scale. Using shard-local finite-difference Hessian vector products compatible with Fully Sharded Data Parallelism, we perform stochastic Lanczos quadrature on open-source language models with up to 100B parameters, producing the first large-scale spectral density estimates beyond the sub-10B regime. We characterize the numerical behavior of this pipeline, including finite-difference bias, floating-point noise amplification, and their effect on Krylov stability in fp32 and bf16, and derive practical operating regimes that are validated empirically. We further provide end-to-end runtime and memory scaling laws, showing that full-operator spectral probing incurs only a modest constant-factor overhead over first-order training. Crucially, direct access to the Hessian reveals that widely used block-diagonal curvature approximations can fail catastrophically, exhibiting order-one relative error and poor directional alignment even in mid-scale LLMs. Together, our results demonstrate that foundation-model Hessian spectra are both computable and qualitatively misrepresented by prevailing approximations, opening the door to principled curvature-based analysis at scale.

</details>


### [929] [Making Bias Non-Predictive: Training Robust LLM Judges via Reinforcement Learning](https://arxiv.org/abs/2602.01528)
*Qian Wang,Xuandong Zhao,Zirui Zhang,Zhanzhi Lou,Nuo Chen,Dawn Song,Bingsheng He*

Main category: cs.CY

Relevance: 85.0

TL;DR: 提出Epistemic Independence Training (EIT)强化学习框架，通过使偏见线索对奖励变得不可预测，来减少LLM作为自动评判员时的认知偏见，提高准确性和鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型作为自动评判员时容易受到认知偏见影响（如从众效应、权威诉求等），现有的提示工程或监督微调方法无法泛化，因为它们只改变表面行为而没有改变使偏见线索具有预测性的优化目标。

Method: 提出Epistemic Independence Training (EIT)强化学习框架，核心原则是：要使模型独立，必须使偏见线索对奖励变得不可预测。通过平衡冲突策略（偏见信号同等可能支持正确和错误答案）和奖励设计（惩罚跟随偏见但不奖励偏见一致）来实现。

Result: 在Qwen3-4B上的实验表明，EIT提高了在对抗性偏见下的准确性和鲁棒性，同时在偏见与真相一致时保持性能。仅用从众偏见训练就能泛化到未见过的偏见类型（如权威和分心），表明EIT诱导了可迁移的认知独立性而非偏见特定启发式。

Conclusion: EIT通过改变优化目标使偏见线索变得不可预测，成功减少了LLM作为评判员时的认知偏见，实现了可泛化的认知独立性，为构建更可靠的自动评判系统提供了新方法。

Abstract: Large language models (LLMs) increasingly serve as automated judges, yet they remain susceptible to cognitive biases -- often altering their reasoning when faced with spurious prompt-level cues such as consensus claims or authority appeals. Existing mitigations via prompting or supervised fine-tuning fail to generalize, as they modify surface behavior without changing the optimization objective that makes bias cues predictive. To address this gap, we propose Epistemic Independence Training (EIT), a reinforcement learning framework grounded in a key principle: to learn independence, bias cues must be made non-predictive of reward. EIT operationalizes this through a balanced conflict strategy where bias signals are equally likely to support correct and incorrect answers, combined with a reward design that penalizes bias-following without rewarding bias agreement. Experiments on Qwen3-4B demonstrate that EIT improves both accuracy and robustness under adversarial biases, while preserving performance when bias aligns with truth. Notably, models trained only on bandwagon bias generalize to unseen bias types such as authority and distraction, indicating that EIT induces transferable epistemic independence rather than bias-specific heuristics. Code and data are available at https://anonymous.4open.science/r/bias-mitigation-with-rl-BC47.

</details>


### [930] [Inference-Aware Meta-Alignment of LLMs via Non-Linear GRPO](https://arxiv.org/abs/2602.01603)
*Shokichi Takakura,Akifumi Wachi,Rei Higuchi,Kohei Miyaguchi,Taiji Suzuki*

Main category: stat.ML

Relevance: 85.0

TL;DR: 提出IAMA方法，通过元对齐训练基础模型，使其能在推理时用有限计算预算对齐到多个标准


<details>
  <summary>Details</summary>
Motivation: 大语言模型对齐到多样化人类偏好具有挑战性，因为标准常相互冲突。推理时对齐方法虽然灵活但计算成本高，需要多次前向传播

Method: 提出推理感知元对齐(IAMA)，训练基础模型使其能通过不同推理时对齐算法有效对齐到多个任务。为解决IAMA中的非线性优化问题，提出非线性GRPO，在概率测度空间可证明收敛到最优解

Result: 未在摘要中明确说明具体实验结果，但方法理论上具有收敛保证

Conclusion: IAMA是一种新颖方法，能在有限计算预算下实现多标准对齐，解决了推理时对齐的计算效率问题

Abstract: Aligning large language models (LLMs) to diverse human preferences is fundamentally challenging since criteria can often conflict with each other. Inference-time alignment methods have recently gained popularity as they allow LLMs to be aligned to multiple criteria via different alignment algorithms at inference time. However, inference-time alignment is computationally expensive since it often requires multiple forward passes of the base model. In this work, we propose inference-aware meta-alignment (IAMA), a novel approach that enables LLMs to be aligned to multiple criteria with limited computational budget at inference time. IAMA trains a base model such that it can be effectively aligned to multiple tasks via different inference-time alignment algorithms. To solve the non-linear optimization problems involved in IAMA, we propose non-linear GRPO, which provably converges to the optimal solution in the space of probability measures.

</details>


### [931] [Transformers as Measure-Theoretic Associative Memory: A Statistical Perspective and Minimax Optimality](https://arxiv.org/abs/2602.01863)
*Ryotaro Kawata,Taiji Suzuki*

Main category: stat.ML

Relevance: 85.0

TL;DR: 该论文从概率测度角度重新构建Transformer的关联记忆机制，将上下文视为token分布，注意力作为测度上的积分算子，证明了浅层Transformer在谱假设下能够学习"回忆-预测"映射，并建立了匹配的极小极大下界。


<details>
  <summary>Details</summary>
Motivation: Transformer通过内容寻址检索和利用无界长度上下文的能力表现出色。论文旨在从概率测度角度重新理解关联记忆，将上下文视为token分布，注意力作为测度上的积分算子，为设计和分析能够从任意长分布上下文中回忆的Transformer提供理论框架。

Method: 采用测度论方法，将上下文建模为混合分布ν= I⁻¹∑μ⁽ⁱ*⁾，查询为x_q(i*)。任务分解为：(1)回忆相关分量μ⁽ⁱ*⁾，(2)从(μ⁽ⁱ*⁾, x_q)进行预测。研究通过经验风险最小化训练的学习softmax注意力（非冻结核），证明在输入密度的谱假设下，浅层测度论Transformer与MLP组合能够学习回忆-预测映射。

Result: 建立了收敛率指数（直至乘法常数）的匹配极小极大下界，证明了收敛阶的锐度。该框架为设计和分析能够从任意长分布上下文中回忆的Transformer提供了具有可证明泛化保证的原则性方法。

Conclusion: 论文提出了一个从概率测度角度理解Transformer关联记忆的理论框架，证明了浅层Transformer能够学习回忆-预测映射，并建立了匹配的极小极大下界，为具有理论保证的Transformer设计提供了原则性方法。

Abstract: Transformers excel through content-addressable retrieval and the ability to exploit contexts of, in principle, unbounded length. We recast associative memory at the level of probability measures, treating a context as a distribution over tokens and viewing attention as an integral operator on measures. Concretely, for mixture contexts $ν= I^{-1} \sum_{i=1}^I μ^{(i^*)}$ and a query $x_{\mathrm{q}}(i^*)$, the task decomposes into (i) recall of the relevant component $μ^{(i^*)}$ and (ii) prediction from $(μ_{i^*},x_\mathrm{q})$. We study learned softmax attention (not a frozen kernel) trained by empirical risk minimization and show that a shallow measure-theoretic Transformer composed with an MLP learns the recall-and-predict map under a spectral assumption on the input densities. We further establish a matching minimax lower bound with the same rate exponent (up to multiplicative constants), proving sharpness of the convergence order. The framework offers a principled recipe for designing and analyzing Transformers that recall from arbitrarily long, distributional contexts with provable generalization guarantees.

</details>


### [932] [OGD4All: A Framework for Accessible Interaction with Geospatial Open Government Data Based on Large Language Models](https://arxiv.org/abs/2602.00012)
*Michael Siebenmann,Javier Argota Sánchez-Vaquerizo,Stefan Arisona,Krystian Samp,Luis Gisler,Dirk Helbing*

Main category: cs.LG

Relevance: 75.0

TL;DR: OGD4All是一个基于LLM的透明、可审计、可复现框架，用于增强公民与地理空间开放政府数据的交互，通过语义数据检索、代理推理代码生成和安全沙箱执行，在199个问题的基准测试中达到98%分析正确率和94%召回率。


<details>
  <summary>Details</summary>
Motivation: 解决公民与开放政府数据交互的挑战，提供透明、可审计、可复现的框架，减少LLM幻觉风险，推进可信AI在开放治理中的应用。

Method: 结合语义数据检索、代理推理进行迭代代码生成、安全沙箱执行，产生可验证的多模态输出，在430个苏黎世市数据集和11个LLM上进行评估。

Result: 在199个问题的基准测试中达到98%分析正确率和94%召回率，可靠拒绝数据不支持的问题，统计鲁棒性测试和专家反馈显示可靠性和社会相关性。

Conclusion: LLM可以为公共数据提供可解释的多模态访问，推进开放治理的可信AI，展示了LLM在增强公民与政府数据交互方面的潜力。

Abstract: We present OGD4All, a transparent, auditable, and reproducible framework based on Large Language Models (LLMs) to enhance citizens' interaction with geospatial Open Government Data (OGD). The system combines semantic data retrieval, agentic reasoning for iterative code generation, and secure sandboxed execution that produces verifiable multimodal outputs. Evaluated on a 199-question benchmark covering both factual and unanswerable questions, across 430 City-of-Zurich datasets and 11 LLMs, OGD4All reaches 98% analytical correctness and 94% recall while reliably rejecting questions unsupported by available data, which minimizes hallucination risks. Statistical robustness tests, as well as expert feedback, show reliability and social relevance. The proposed approach shows how LLMs can provide explainable, multimodal access to public data, advancing trustworthy AI for open governance.

</details>


### [933] [ELLMPEG: An Edge-based Agentic LLM Video Processing Tool](https://arxiv.org/abs/2602.00028)
*Zoha Azimi,Reza Farahani,Radu Prodan,Christian Timmerer*

Main category: cs.LG

Relevance: 75.0

TL;DR: ELLMPEG是一个边缘智能的代理式LLM框架，用于自动生成视频处理命令，通过工具感知的RAG和迭代自反思技术，在边缘设备上本地执行FFmpeg和VVenC命令，消除对云API的依赖。


<details>
  <summary>Details</summary>
Motivation: 解决云端LLM部署的三个关键限制：高计算和能耗、远程处理的隐私和可靠性风险、以及持续的API成本。利用代理式AI的进展，特别是结构化推理和工具使用能力，更好地利用本地部署的工具和LLM。

Method: 集成工具感知的检索增强生成(RAG)与迭代自反思机制，在边缘设备上生成并本地验证可执行的FFmpeg和VVenC命令。收集了包含480个多样化查询的专用提示数据集，涵盖不同类别的FFmpeg和VVC编码器命令。

Result: Qwen2.5在ELLMPEG框架增强下，在FFmpeg和VVenC数据集上平均命令生成准确率达到78%，零持续API成本，在所有开源模型中表现最佳。评估了命令有效性、每秒生成token数、推理时间和能效等指标。

Conclusion: ELLMPEG框架成功实现了在边缘设备上本地生成和执行视频处理命令，解决了云部署的限制，为边缘AI应用提供了高效、隐私安全且成本效益高的解决方案。

Abstract: Large language models (LLMs), the foundation of generative AI systems like ChatGPT, are transforming many fields and applications, including multimedia, enabling more advanced content generation, analysis, and interaction. However, cloud-based LLM deployments face three key limitations: high computational and energy demands, privacy and reliability risks from remote processing, and recurring API costs. Recent advances in agentic AI, especially in structured reasoning and tool use, offer a better way to exploit open and locally deployed tools and LLMs. This paper presents ELLMPEG, an edge-enabled agentic LLM framework for the automated generation of video-processing commands. ELLMPEG integrates tool-aware Retrieval-Augmented Generation (RAG) with iterative self-reflection to produce and locally verify executable FFmpeg and VVenC commands directly at the edge, eliminating reliance on external cloud APIs. To evaluate ELLMPEG, we collect a dedicated prompt dataset comprising 480 diverse queries covering different categories of FFmpeg and the Versatile Video Codec (VVC) encoder (VVenC) commands. We validate command generation accuracy and evaluate four open-source LLMs based on command validity, tokens generated per second, inference time, and energy efficiency. We also execute the generated commands to assess their runtime correctness and practical applicability. Experimental results show that Qwen2.5, when augmented with the ELLMPEG framework, achieves an average command-generation accuracy of 78 % with zero recurring API cost, outperforming all other open-source models across both the FFmpeg and VVenC datasets.

</details>


### [934] [SCPL: Enhancing Neural Network Training Throughput with Decoupled Local Losses and Model Parallelism](https://arxiv.org/abs/2602.00062)
*Ming-Yao Ho,Cheng-Kai Wang,You-Teng Lin,Hung-Hsuan Chen*

Main category: cs.LG

Relevance: 75.0

TL;DR: 提出SCPL（监督对比并行学习）方法，通过解耦反向传播将长梯度流分解为多个短梯度流，实现不同层参数梯度的并行计算，提升训练吞吐量。


<details>
  <summary>Details</summary>
Motivation: 企业信息系统采用大规模AI模型面临高训练成本和长开发周期的挑战，标准端到端反向传播算法是深度网络训练效率低下的主要原因。

Method: SCPL方法解耦反向传播，将长梯度流转换为多个短梯度流，实现不同层参数梯度的并行计算，提供优越的模型并行性。

Result: 实验表明SCPL在效率和效果上优于标准反向传播、Early Exit、GPipe以及当前最先进的解耦反向传播方法Associated Learning (AL)。

Conclusion: SCPL通过缓解基本性能瓶颈，为组织提供更经济高效、更敏捷地开发和部署先进信息系统的实用途径。

Abstract: Adopting large-scale AI models in enterprise information systems is often hindered by high training costs and long development cycles, posing a significant managerial challenge. The standard end-to-end backpropagation (BP) algorithm is a primary driver of modern AI, but it is also the source of inefficiency in training deep networks. This paper introduces a new training methodology, Supervised Contrastive Parallel Learning (SCPL), that addresses this issue by decoupling BP and transforming a long gradient flow into multiple short ones. This design enables the simultaneous computation of parameter gradients in different layers, achieving superior model parallelism and enhancing training throughput. Detailed experiments are presented to demonstrate the efficiency and effectiveness of our model compared to BP, Early Exit, GPipe, and Associated Learning (AL), a state-of-the-art method for decoupling backpropagation. By mitigating a fundamental performance bottleneck, SCPL provides a practical pathway for organizations to develop and deploy advanced information systems more cost-effectively and with greater agility. The experimental code is released for reproducibility. https://github.com/minyaho/scpl/

</details>


### [935] [ECCO: Evidence-Driven Causal Reasoning for Compiler Optimization](https://arxiv.org/abs/2602.00087)
*Haolin Pan,Lianghong Huang,Jinyuan Dong,Mingjie Xing,Yanjun Wu*

Main category: cs.LG

Relevance: 75.0

TL;DR: ECCO是一个将可解释推理与组合搜索相结合的编译器自动调优框架，通过构建思维链数据集学习优化决策的因果逻辑，并让LLM作为策略师指导遗传算法，在7个数据集上平均减少24.44%的周期。


<details>
  <summary>Details</summary>
Motivation: 解决编译器自动调优中传统黑盒搜索方法缺乏语义指导，以及现有LLM方法存在表面模式匹配和因果不透明的问题。旨在将可解释推理与组合搜索相结合，让模型学习优化决策的因果逻辑而非简单模仿序列。

Method: 1) 提出逆向工程方法构建思维链数据集，将静态代码特征映射到可验证的性能证据；2) 设计协作推理机制，让LLM作为策略师定义优化意图，动态指导遗传算法的变异操作。

Result: 在7个数据集上的实验结果表明，ECCO显著优于LLVM opt -O3基线，平均减少24.44%的周期。

Conclusion: ECCO成功地将可解释推理与组合搜索相结合，通过让LLM学习优化决策的因果逻辑并指导遗传算法，实现了比传统方法更好的编译器自动调优性能。

Abstract: Compiler auto-tuning faces a dichotomy between traditional black-box search methods, which lack semantic guidance, and recent Large Language Model (LLM) approaches, which often suffer from superficial pattern matching and causal opacity. In this paper, we introduce ECCO, a framework that bridges interpretable reasoning with combinatorial search. We first propose a reverse engineering methodology to construct a Chain-of-Thought dataset, explicitly mapping static code features to verifiable performance evidence. This enables the model to learn the causal logic governing optimization decisions rather than merely imitating sequences. Leveraging this interpretable prior, we design a collaborative inference mechanism where the LLM functions as a strategist, defining optimization intents that dynamically guide the mutation operations of a genetic algorithm. Experimental results on seven datasets demonstrate that ECCO significantly outperforms the LLVM opt -O3 baseline, achieving an average 24.44% reduction in cycles.

</details>


### [936] [From Numbers to Prompts: A Cognitive Symbolic Transition Mechanism for Lightweight Time-Series Forecasting](https://arxiv.org/abs/2602.00088)
*Namkyung Yoon,Hwangnam Kim*

Main category: cs.LG

Relevance: 75.0

TL;DR: 提出符号转换机制(STM)，通过符号抽象和提示工程将数值时间序列数据与语言模型连接，在轻量级平台上实现高效时间序列预测


<details>
  <summary>Details</summary>
Motivation: 大型语言模型在时间序列预测任务中表现出色，但其巨大的计算和内存需求限制了在轻量级平台上的部署。需要一种方法在保持模型完整性的同时显著提高效率。

Method: STM框架：1) 基于人类认知结构的量化技术将连续时间序列值转换为符号标记；2) 通过符号的结构化转换捕捉时间动态；3) 基于提示工程的预测，使语言模型专注于时间序列的关键部分。

Result: 在多个时间序列数据集上，配合四个小型语言模型(SLM)测试，相比默认骨干SLM，STM实现MAE误差降低高达69%，MSE误差降低高达90%。资源开销极小：GPU内存仅增加约0.06%，延迟开销仅增加0.64%。

Conclusion: STM作为一种高效、自适应的符号驱动时间序列预测层，具有显著潜力，能够在可忽略的资源成本下大幅提升预测精度。

Abstract: Large language models have achieved remarkable success in time series prediction tasks, but their substantial computational and memory requirements limit deployment on lightweight platforms. In this paper, we propose the Symbolic Transition Mechanism (STM) a novel framework that bridges numeric time series data and language models through symbolic abstraction and prompt engineering. STM transforms continuous time series values into symbol tokens with quantization techniques based on human cognitive structures, and captures temporal dynamics through structured transformations of symbols, enabling fast engineering based predictions in which language models focus on critical parts of time series data. STM is a general purpose mechanisms that ensure the integrity of backbone language models, but they significantly improve their efficiency by inferring the dynamic and structured patterns inherent in time series data. We evaluated STM on various time series datasets, paired with four small language models (SLM) with limited computational environments. For all models, STM achieves error reductions of up to 69% in MAE and 90% in MSE compared to the default backbone SLM without STM. These results demonstrate the potential of STM as an efficient, adaptable layer for symbol-driven time series prediction using foundation models. The accuracy improvements were made at negligible resource costs, with maximum GPU memory of the base model increasing by approximately 0.06% and latency overhead increasing by only 0.64%.

</details>


### [937] [The Illusion of Forgetting: Attack Unlearned Diffusion via Initial Latent Variable Optimization](https://arxiv.org/abs/2602.00175)
*Manyi Li,Yufan Liu,Lai Jiang,Bing Li,Yuming Li,Weiming Hu*

Main category: cs.LG

Relevance: 75.0

TL;DR: 该论文揭示扩散模型中的"遗忘"防御机制存在根本性缺陷，提出IVO攻击框架能够重新激活被遗忘的NSFW概念，暴露当前防御方法的脆弱性。


<details>
  <summary>Details</summary>
Motivation: 当前基于遗忘的防御方法声称能够从扩散模型中清除NSFW概念，但作者发现这种"遗忘"很大程度上是一种假象。未学习过程只是部分破坏了语言符号与底层知识之间的映射关系，而知识本身仍作为休眠记忆保留在模型中。

Method: 提出IVO（初始潜在变量优化）攻击框架，通过图像反演、对抗优化和重用攻击三个步骤，优化初始潜在变量来重新对齐未学习模型的噪声分布与原始不安全状态。该方法利用去噪过程中的分布差异作为映射保留程度的可测量指标。

Result: 在8种广泛使用的未学习技术上进行的广泛实验表明，IVO实现了优越的攻击成功率（高达98.7%）和强大的语义一致性，显著优于现有攻击方法，暴露了当前防御的根本缺陷。

Conclusion: 当前基于遗忘的扩散模型防御方法存在根本性漏洞，未学习过程只是破坏了表面映射而非真正消除知识。IVO攻击框架的有效性表明需要重新思考扩散模型的安全防御策略。

Abstract: Although unlearning-based defenses claim to purge Not-Safe-For-Work (NSFW) concepts from diffusion models (DMs), we reveals that this "forgetting" is largely an illusion. Unlearning partially disrupts the mapping between linguistic symbols and the underlying knowledge, which remains intact as dormant memories. We find that the distributional discrepancy in the denoising process serves as a measurable indicator of how much of the mapping is retained, also reflecting the strength of unlearning. Inspired by this, we propose IVO (Initial Latent Variable Optimization), a concise and powerful attack framework that reactivates these dormant memories by reconstructing the broken mappings. Through Image Inversion}, Adversarial Optimization and Reused Attack, IVO optimizes initial latent variables to realign the noise distribution of unlearned models with their original unsafe states. Extensive experiments across 8 widely used unlearning techniques demonstrate that IVO achieves superior attack success rates and strong semantic consistency, exposing fundamental flaws in current defenses. The code is available at anonymous.4open.science/r/IVO/. Warning: This paper has unsafe images that may offend some readers.

</details>


### [938] [Reducing Memorisation in Generative Models via Riemannian Bayesian Inference](https://arxiv.org/abs/2602.00199)
*Johanna Marie Gegenfurtner,Albert Kjøller Jacobsen,Naima Elosegui Borras,Alejandro Valverde Mahou,Georgios Arvanitidis*

Main category: cs.LG

Relevance: 75.0

TL;DR: 论文提出了一种贝叶斯方法，通过考虑损失函数的黎曼几何结构来平衡生成模型的记忆化和泛化能力，在流匹配和扩散模型的参数空间中构建预测后验分布。


<details>
  <summary>Details</summary>
Motivation: 现代生成模型虽然能产生逼真样本，但平衡记忆化和泛化仍然是一个开放问题。作者从贝叶斯视角出发，关注生成模型的参数空间，旨在构建能更好捕捉数据分布变异性的预测后验。

Method: 使用黎曼度量捕捉损失函数的几何结构，并利用灵活的近似后验来适应损失景观的局部结构。这种方法允许采样与原始模型相似但记忆化程度降低的生成模型。

Result: 实验证明该方法能有效减少记忆化同时保持泛化能力。理论分析解释了这些发现，表明考虑损失几何结构能有效利用参数空间，即使对于复杂的高维生成模型也是如此。

Conclusion: 通过考虑损失函数的几何结构，可以在生成模型的参数空间中有效平衡记忆化和泛化，为复杂高维生成模型提供了一种新的贝叶斯视角。

Abstract: Modern generative models can produce realistic samples, however, balancing memorisation and generalisation remains an open problem. We approach this challenge from a Bayesian perspective by focusing on the parameter space of flow matching and diffusion models and constructing a predictive posterior that better captures the variability of the data distribution. In particular, we capture the geometry of the loss using a Riemannian metric and leverage a flexible approximate posterior that adapts to the local structure of the loss landscape. This approach allows us to sample generative models that resemble the original model, but exhibit reduced memorisation. Empirically, we demonstrate that the proposed approach reduces memorisation while preserving generalisation. Further, we provide a theoretical analysis of our method, which explains our findings. Overall, our work illustrates how considering the geometry of the loss enables effective use of the parameter space, even for complex high-dimensional generative models.

</details>


### [939] [TABES: Trajectory-Aware Backward-on-Entropy Steering for Masked Diffusion Models](https://arxiv.org/abs/2602.00250)
*Shreshth Saini,Avinab Saha,Balu Adsumilli,Neil Birkbeck,Yilin Wang,Alan C. Bovik*

Main category: cs.LG

Relevance: 75.0

TL;DR: 本文提出BoE Steering方法，通过梯度引导的推理框架解决Masked Diffusion Models中的轨迹锁定问题，使用单次反向传播近似无限视野前瞻，显著提升非自回归生成的鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 当前Masked Diffusion Models的采样方法依赖简单的置信度启发式策略，忽略了局部决策的长期影响，导致早期幻觉会级联成全局不一致性。虽然基于搜索的方法可以缓解此问题，但计算成本过高（每步需要O(K)次前向传播）。

Method: 提出Backward-on-Entropy (BoE) Steering梯度引导推理框架，从轨迹成本函数的一阶展开中形式化推导出Token Influence Score (TIS)，证明输入嵌入相对于未来熵的梯度可作为最小化不确定性的最优控制信号。引入ActiveQueryAttention稀疏伴随原语，利用掩码目标的结构降低反向传播复杂度。

Result: BoE在推理时间缩放方面实现了优于现有去掩码方法的Pareto前沿，表明梯度引导的转向为鲁棒的非自回归生成提供了数学原理清晰且高效的路径。

Conclusion: 梯度引导的推理框架能够有效解决MDMs中的轨迹锁定问题，通过单次反向传播近似无限视野前瞻，在保持计算效率的同时显著提升生成质量。

Abstract: Masked Diffusion Models (MDMs) have emerged as a promising non-autoregressive paradigm for generative tasks, offering parallel decoding and bidirectional context utilization. However, current sampling methods rely on simple confidence-based heuristics that ignore the long-term impact of local decisions, leading to trajectory lock-in where early hallucinations cascade into global incoherence. While search-based methods mitigate this, they incur prohibitive computational costs ($O(K)$ forward passes per step). In this work, we propose Backward-on-Entropy (BoE) Steering, a gradient-guided inference framework that approximates infinite-horizon lookahead via a single backward pass. We formally derive the Token Influence Score (TIS) from a first-order expansion of the trajectory cost functional, proving that the gradient of future entropy with respect to input embeddings serves as an optimal control signal for minimizing uncertainty. To ensure scalability, we introduce \texttt{ActiveQueryAttention}, a sparse adjoint primitive that exploits the structure of the masking objective to reduce backward pass complexity. BoE achieves a superior Pareto frontier for inference-time scaling compared to existing unmasking methods, demonstrating that gradient-guided steering offers a mathematically principled and efficient path to robust non-autoregressive generation. We will release the code.

</details>


### [940] [Generation Order and Parallel Decoding in Masked Diffusion Models: An Information-Theoretic Perspective](https://arxiv.org/abs/2602.00286)
*Shaorong Zhang,Longxuan Yu,Rob Brekelmans,Luhan Tang,Salman Asif,Greg Ver Steeg*

Main category: cs.LG

Relevance: 75.0

TL;DR: 本文提出了一个统一的信息论框架来分析掩码扩散模型（MDMs）中的两种失败来源：顺序敏感性和并行化偏差。研究发现：1）易先解码在模型误差增大时效果更显著；2）因子化并行解码会引入采样误差，导致反向KL散度无限增大；3）验证能消除采样误差但代价指数增长，而启发式方法如重新掩码无法保证分布正确性。


<details>
  <summary>Details</summary>
Motivation: 掩码扩散模型（MDMs）通过牺牲序列确定性来加速推理，但生成顺序的理论机制和并行化风险尚未得到充分探索。本文旨在从信息论角度分析MDMs中的两种基本失败来源：顺序敏感性和并行化偏差。

Method: 提出了一个统一的信息论框架来解耦和分析两种失败来源。在受控的Block-HMM和大型MDMs（如LLaDA）上进行算术推理实验，验证理论框架。

Result: 研究得出三个关键发现：1）易先解码（优先处理低熵标记）的益处随模型误差增加而放大；2）因子化并行解码引入的采样误差可导致任意大的反向KL散度，捕捉标准前向KL指标忽略的"不连贯"失败；3）验证能消除采样误差但代价指数增长，而重新掩码等启发式方法无法保证分布正确性。

Conclusion: 本文为理解掩码扩散模型的顺序敏感性和并行化偏差提供了理论框架，揭示了易先解码的优势条件、并行解码的内在风险以及验证与启发式方法的权衡。实验验证了理论框架的有效性。

Abstract: Masked Diffusion Models (MDMs) significantly accelerate inference by trading off sequential determinism. However, the theoretical mechanisms governing generation order and the risks inherent in parallelization remain under-explored. In this work, we provide a unified information-theoretic framework to decouple and analyze two fundamental sources of failure: order sensitivity and parallelization bias. Our analysis yields three key insights: (1) The benefits of Easy-First decoding (prioritizing low-entropy tokens) are magnified as model error increases; (2) factorized parallel decoding introduces intrinsic sampling errors that can lead to arbitrary large Reverse KL divergence, capturing "incoherence" failures that standard Forward KL metrics overlook; and (3) while verification can eliminate sampling error, it incurs an exponential cost governed by the total correlation within a block. Conversely, heuristics like remasking, though computationally efficient, cannot guarantee distributional correctness. Experiments on a controlled Block-HMM and large-scale MDMs (LLaDA) for arithmetic reasoning validate our theoretical framework.

</details>


### [941] [From Observations to States: Latent Time Series Forecasting](https://arxiv.org/abs/2602.00297)
*Jie Yang,Yifan Hu,Yuante Li,Kexin Zhang,Kaize Ding,Philip S. Yu*

Main category: cs.LG

Relevance: 75.0

TL;DR: 提出LatentTSF新范式，将时间序列预测从观测空间回归转向潜在状态预测，解决"潜在混沌"问题


<details>
  <summary>Details</summary>
Motivation: 当前时间序列预测模型存在"潜在混沌"问题：预测准确但潜在表示混乱无序。这源于主流的观测空间预测范式，模型通过最小化噪声数据的逐点误差，鼓励捷径解而非恢复底层系统动态。

Method: 提出LatentTSF范式，使用自编码器将每个时间步的观测投影到高维潜在状态空间，在潜在空间进行预测。理论分析表明该方法隐式最大化预测潜在状态与真实状态及观测之间的互信息。

Result: 在广泛使用的基准测试中，LatentTSF有效缓解了潜在混沌问题，取得了优越的性能表现。

Conclusion: 通过将预测从观测空间转移到潜在状态空间，LatentTSF能够更好地捕捉底层系统动态，解决时间序列预测中的表示悖论问题。

Abstract: Deep learning has achieved strong performance in Time Series Forecasting (TSF). However, we identify a critical representation paradox, termed Latent Chaos: models with accurate predictions often learn latent representations that are temporally disordered and lack continuity. We attribute this phenomenon to the dominant observation-space forecasting paradigm. Most TSF models minimize point-wise errors on noisy and partially observed data, which encourages shortcut solutions instead of the recovery of underlying system dynamics. To address this issue, we propose Latent Time Series Forecasting (LatentTSF), a novel paradigm that shifts TSF from observation regression to latent state prediction. Specifically, LatentTSF employs an AutoEncoder to project observations at each time step into a higher-dimensional latent state space. This expanded representation aims to capture underlying system variables and impose a smoother temporal structure. Forecasting is then performed entirely in the latent space, allowing the model to focus on learning structured temporal dynamics. Theoretical analysis demonstrates that our proposed latent objectives implicitly maximize mutual information between predicted latent states and ground-truth states and observations. Extensive experiments on widely-used benchmarks confirm that LatentTSF effectively mitigates latent chaos, achieving superior performance. Our code is available in https://github.com/Muyiiiii/LatentTSF.

</details>


### [942] [Adaptive Momentum and Nonlinear Damping for Neural Network Training](https://arxiv.org/abs/2602.00334)
*Aikaterini Karoni,Rajit Rajpal,Benedict Leimkuhler,Gabriel Stoltz*

Main category: cs.LG

Relevance: 75.0

TL;DR: 提出一种连续时间优化方案，为每个模型参数引入由动能调节的自适应动量系数，通过立方阻尼机制自动调整以适应局部曲率，在ViT、BERT、GPT2训练中匹配或优于Adam


<details>
  <summary>Details</summary>
Motivation: 传统优化器如mSGD在大规模优化中面临稳定性与收敛速度的权衡问题，需要一种能自动适应局部曲率、保持稳定性的自适应优化方法

Method: 提出连续时间优化方案，为每个参数引入由动能调节的自适应动量系数，将立方阻尼机制（来自结构动力学）融入优化过程，具体通过为mSGD和Adam的连续动力学方程添加立方阻尼项来实现

Result: 在ViT、BERT、GPT2训练任务中表现出鲁棒性，匹配或优于Adam，特别是在mSGD通常表现不佳的任务上，同时提供了指数收敛的理论保证

Conclusion: 通过引入基于动能的立方阻尼机制，开发了一种自适应优化方法，能够自动调整以适应局部曲率，在大规模深度学习任务中实现稳定且快速的收敛

Abstract: We propose a continuous-time scheme for large-scale optimization that introduces individual, adaptive momentum coefficients regulated by the kinetic energy of each model parameter. This approach automatically adjusts to local landscape curvature to maintain stability without sacrificing convergence speed. We demonstrate that our adaptive friction can be related to cubic damping, a suppression mechanism from structural dynamics. Furthermore, we introduce two specific optimization schemes by augmenting the continuous dynamics of mSGD and Adam with a cubic damping term. Empirically, our methods demonstrate robustness and match or outperform Adam on training ViT, BERT, and GPT2 tasks where mSGD typically struggles. We further provide theoretical results establishing the exponential convergence of the proposed schemes.

</details>


### [943] [MATRIX: A Multimodal Benchmark and Post-Training Framework for Materials Science](https://arxiv.org/abs/2602.00376)
*Delia McGrath,Curtis Chong,Rohil Kulkarni,Gerbrand Ceder,Adeesh Kolluru*

Main category: cs.LG

Relevance: 75.0

TL;DR: MATRIX是一个材料科学多模态推理基准，用于评估视觉实验数据对科学推理的提升效果。研究发现，即使使用少量多模态数据，视觉监督也能显著改善实验解释和科学推理能力。


<details>
  <summary>Details</summary>
Motivation: 现有基准难以评估在训练后阶段加入视觉实验数据是否能超越纯文本监督，提升基于物理机制的解释推理能力。材料科学需要整合多模态实验证据与基础物理理论。

Method: 引入MATRIX多模态基准，包含基础理论、研究级推理和多种表征模态下的真实实验工件解释。通过对比纯文本训练后与包含配对实验图像的多模态训练后效果，隔离视觉基础的影响。

Result: 尽管使用相对少量的多模态数据，视觉监督使实验解释提升10-25%，在纯文本科学推理任务上获得5-16%的增益。改进依赖于训练后阶段正确的图像-文本对齐，体现了跨模态表征迁移。在ScienceQA和PubMedQA上也观察到一致改进。

Conclusion: 结构化多模态训练后的益处超越了材料科学领域，视觉基础能显著提升科学推理能力。正确的图像-文本对齐是实现跨模态表征迁移的关键。

Abstract: Scientific reasoning in materials science requires integrating multimodal experimental evidence with underlying physical theory. Existing benchmarks make it difficult to assess whether incorporating visual experimental data during post-training improves mechanism-grounded explanation reasoning beyond text-only supervision. We introduce MATRIX, a multimodal benchmark for materials science reasoning that evaluates foundational theory, research-level reasoning, and the interpretation of real experimental artifacts across multiple characterization modalities. Using MATRIX as a controlled diagnostic, we isolate the effect of visual grounding by comparing post-training on structured materials science text alone with post-training that incorporates paired experimental images. Despite using relatively small amounts of multimodal data, visual supervision improves experimental interpretation by 10-25% and yields 5-16% gains on text-only scientific reasoning tasks. Our results demonstrate that these improvements rely on correct image-text alignment during post-training, highlighting cross-modal representational transfer. We also observe consistent improvements on ScienceQA and PubMedQA, demonstrating that the benefits of structured multimodal post-training extend beyond materials science. The MATRIX dataset is available at https://huggingface.co/datasets/radical-ai/MATRIX and the model at https://huggingface.co/radical-ai/MATRIX-PT.

</details>


### [944] [Open Materials Generation with Inference-Time Reinforcement Learning](https://arxiv.org/abs/2602.00424)
*Philipp Hoellmer,Stefano Martiniani*

Main category: cs.LG

Relevance: 75.0

TL;DR: OMatG-IRL：基于推理时强化学习的连续时间生成模型框架，用于晶体材料设计，无需显式计算分数，通过扰动生成动力学实现目标属性强化


<details>
  <summary>Details</summary>
Motivation: 现有连续时间晶体生成模型难以将显式目标属性融入生成过程，而基于策略梯度的强化学习通常需要访问分数，限制了其在仅学习速度场的流模型中的应用

Method: 提出OMatG-IRL框架，直接在学习的速度场上操作，通过随机扰动底层生成动力学，在推理时实现探索和策略梯度估计，同时保持预训练生成模型的基线性能

Result: 首次将强化学习应用于晶体结构预测，在保持组合条件多样性的同时有效强化能量目标，性能与基于分数的RL方法相当，并能学习时间依赖的速度退火计划，实现采样效率数量级提升和生成时间大幅减少

Conclusion: OMatG-IRL为连续时间生成模型提供了有效的推理时强化学习框架，解决了分数依赖问题，在晶体材料设计中实现了目标属性强化和效率提升

Abstract: Continuous-time generative models for crystalline materials enable inverse materials design by learning to predict stable crystal structures, but incorporating explicit target properties into the generative process remains challenging. Policy-gradient reinforcement learning (RL) provides a principled mechanism for aligning generative models with downstream objectives but typically requires access to the score, which has prevented its application to flow-based models that learn only velocity fields. We introduce Open Materials Generation with Inference-time Reinforcement Learning (OMatG-IRL), a policy-gradient RL framework that operates directly on the learned velocity fields and eliminates the need for the explicit computation of the score. OMatG-IRL leverages stochastic perturbations of the underlying generation dynamics preserving the baseline performance of the pretrained generative model while enabling exploration and policy-gradient estimation at inference time. Using OMatG-IRL, we present the first application of RL to crystal structure prediction (CSP). Our method enables effective reinforcement of an energy-based objective while preserving diversity through composition conditioning, and it achieves performance competitive with score-based RL approaches. Finally, we show that OMatG-IRL can learn time-dependent velocity-annealing schedules, enabling accurate CSP with order-of-magnitude improvements in sampling efficiency and, correspondingly, reduction in generation time.

</details>


### [945] [Diffusion LMs Can Approximate Optimal Infilling Lengths Implicitly](https://arxiv.org/abs/2602.00476)
*Hengchang Liu,Zhao Yang,Bing Su*

Main category: cs.LG

Relevance: 75.0

TL;DR: CAL是一种无需训练的方法，通过利用扩散语言模型在首步去噪置信度中的统计信号（Oracle Peak和Length Bias），实现自适应确定填充长度的文本/代码填充方法。


<details>
  <summary>Details</summary>
Motivation: 扩散语言模型虽然天生适合填充任务，但性能受限于预设的填充长度。现有方法需要指定长度，而作者发现DLMs其实具备发现正确填充长度的内在能力，只是被统计偏差所掩盖。

Method: CAL方法通过分析DLMs首步去噪置信度的两个关键统计现象：1）Oracle Peak（在真实长度附近出现的局部峰值），2）Length Bias（系统性的长度偏差）。通过校准这种偏差并利用Oracle Peak信号，在正式解码前通过高效搜索近似最优长度。

Result: 在代码填充任务中，CAL比固定长度基线提升Pass@1达47.7%，比基于聊天的自适应方法提升40.5%。在文本填充任务中，BLEU-2和ROUGE-L分别提升达8.5%和9.9%。

Conclusion: CAL证明了DLMs具备发现正确填充长度的内在能力，通过简单的统计校准即可实现无需专门训练的自适应长度填充，为DLMs的稳健填充应用开辟了新途径。

Abstract: Diffusion language models (DLMs) provide a bidirectional generation framework naturally suited for infilling, yet their performance is constrained by the pre-specified infilling length. In this paper, we reveal that DLMs possess an inherent ability to discover the correct infilling length. We identify two key statistical phenomena in the first-step denoising confidence: a local \textit{Oracle Peak} that emerges near the ground-truth length and a systematic \textit{Length Bias} that often obscures this signal. By leveraging this signal and calibrating the bias, our training-free method \textbf{CAL} (\textbf{C}alibrated \textbf{A}daptive \textbf{L}ength) enables DLMs to approximate the optimal length through an efficient search before formal decoding. Empirical evaluations demonstrate that CAL improves Pass@1 by up to 47.7\% over fixed-length baselines and 40.5\% over chat-based adaptive methods in code infilling, while boosting BLEU-2 and ROUGE-L by up to 8.5\% and 9.9\% in text infilling. These results demonstrate that CAL paves the way for robust DLM infilling without requiring any specialized training. Code is available at https://github.com/NiuHechang/Calibrated_Adaptive_Length.

</details>


### [946] [Minerva: Reinforcement Learning with Verifiable Rewards for Cyber Threat Intelligence LLMs](https://arxiv.org/abs/2602.00513)
*Md Tanvirul Alam,Aritran Piplai,Ionut Cardei,Nidhi Rastogi,Peter J Worth*

Main category: cs.LG

Relevance: 75.0

TL;DR: 论文提出Minerva框架，利用可验证奖励的强化学习（RLVR）改进网络安全威胁情报（CTI）的结构化输出任务，相比监督微调在多个基准测试中取得更好效果。


<details>
  <summary>Details</summary>
Motivation: 网络安全威胁情报分析师需要将非结构化安全数据转换为标准化的自动化就绪表示。虽然大语言模型在此任务上显示潜力，但现有方法在生成结构化CTI输出时仍然脆弱，且主要依赖监督微调。CTI标准和社区维护的资源定义了规范的标识符和模式，使得模型输出可以进行确定性验证。

Method: 提出Minerva统一数据集和训练管道，涵盖多个CTI子任务，每个任务配有特定验证器来评分结构化输出和标识符预测。为解决奖励稀疏性问题，提出轻量级自训练机制，生成额外的已验证轨迹并将其蒸馏回模型。使用可验证奖励的强化学习（RLVR）方法。

Result: 在不同LLM骨干网络上的实验显示，相比监督微调，该方法在多个基准测试中实现了准确性和鲁棒性的持续改进。

Conclusion: 利用CTI任务的结构化特性，通过可验证奖励的强化学习可以显著提升大语言模型在网络安全威胁情报结构化输出任务上的性能，为领域特定应用提供了有效的训练范式。

Abstract: Cyber threat intelligence (CTI) analysts routinely convert noisy, unstructured security artifacts into standardized, automation-ready representations. Although large language models (LLMs) show promise for this task, existing approaches remain brittle when producing structured CTI outputs and have largely relied on supervised fine-tuning (SFT). In contrast, CTI standards and community-maintained resources define canonical identifiers and schemas that enable deterministic verification of model outputs. We leverage this structure to study reinforcement learning with verifiable rewards (RLVR) for CTI tasks. We introduce \textit{Minerva}, a unified dataset and training pipeline spanning multiple CTI subtasks, each paired with task-specific verifiers that score structured outputs and identifier predictions. To address reward sparsity during rollout, we propose a lightweight self-training mechanism that generates additional verified trajectories and distills them back into the model. Experiments across LLM backbones show consistent improvements in accuracy and robustness over SFT across multiple benchmarks.

</details>


### [947] [NEST: Nested Event Stream Transformer for Sequences of Multisets](https://arxiv.org/abs/2602.00520)
*Minghui Sun,Haoyu Gong,Xingyu You,Jillian Hurst,Benjamin Goldstein,Matthew Engelhard*

Main category: cs.LG

Relevance: 75.0

TL;DR: NEST：一种用于多集序列事件流的层次化Transformer架构，通过保留原始层次结构提升计算效率和表示质量


<details>
  <summary>Details</summary>
Motivation: 现有事件流基础模型将层次结构扁平化为一维序列，导致计算效率低下（密集注意力）和学习虚假的集合内关系，同时启发式后训练池化导致集合级表示质量较低

Method: 提出NEST（Nested Event Stream Transformer），一种用于多集序列事件流的基础模型架构，保留原始层次结构；并引入Masked Set Modeling（MSM）范式，促进改进的集合级表示学习

Result: 在真实世界多集序列数据上的实验表明，NEST能够捕捉真实世界动态，同时提高预训练效率和下游任务性能

Conclusion: 在基础模型架构中保留事件流的原始层次结构提供了有用的归纳偏置，既能提高计算效率又能提升表示质量

Abstract: Event stream data often exhibit hierarchical structure in which multiple events co-occur, resulting in a sequence of multisets (i.e., bags of events). In electronic health records (EHRs), for example, medical events are grouped into a sequence of clinical encounters with well-defined temporal structure, but the order and timing of events within each encounter may be unknown or unreliable. Most existing foundation models (FMs) for event stream data flatten this hierarchy into a one-dimensional sequence, leading to (i) computational inefficiency associated with dense attention and learning spurious within-set relationships, and (ii) lower-quality set-level representations from heuristic post-training pooling for downstream tasks. Here, we show that preserving the original hierarchy in the FM architecture provides a useful inductive bias that improves both computational efficiency and representation quality. We then introduce Nested Event Stream Transformer (NEST), a FM for event streams comprised of sequences of multisets. Building on this architecture, we formulate Masked Set Modeling (MSM), an efficient paradigm that promotes improved set-level representation learning. Experiments on real-world multiset sequence data show that NEST captures real-world dynamics while improving both pretraining efficiency and downstream performance.

</details>


### [948] [Convergent World Representations and Divergent Tasks](https://arxiv.org/abs/2602.00533)
*Core Francisco Park*

Main category: cs.LG

Relevance: 75.0

TL;DR: 研究发现多任务训练能促进世界表征的几何对齐，但某些"发散性任务"会损害新实体的表征整合能力


<details>
  <summary>Details</summary>
Motivation: 研究神经表征的几何特性及其在下游适应能力中的作用，理解多任务训练如何影响表征对齐和新实体整合

Method: 使用5075个城市坐标定义"世界"，通过7个几何任务生成训练数据，进行自回归训练，比较单任务和多任务训练的表征几何，测试新城市通过微调整合到表征空间的能力

Result: 不同任务产生不同的表征几何，但多任务训练驱动表征对齐；某些"发散性任务"会损害新实体的表征整合和泛化能力

Conclusion: 多任务训练能可靠产生收敛的世界表征，但发散性任务会通过微调灾难性地损害新实体整合

Abstract: While neural representations are central to modern deep learning, the conditions governing their geometry and their roles in downstream adaptability remain poorly understood. We develop a framework clearly separating the underlying world, the data generation process and the resulting model representations to study these questions in a controlled setup. 5,075 city coordinates define the world and 7 geometric tasks generate the training data for autoregressive training. We find that different tasks give rise to qualitatively and quantitatively distinct world representation geometries. However, multi-task training drives convergence of world representations: models trained on non-overlapping tasks develop aligned geometric representations, providing controlled evidence for the Multitask Scaling Hypothesis of the Platonic Representation Hypothesis. To study adaptation, we pretrain models on all tasks, then test whether new entities (cities) can be consistently integrated into the representation space via fine-tuning. Surprisingly, we find that despite multi-task pretraining, some tasks, which we call divergent, actively harm the representational integration of new entities and harm generalization. Our results show that training on multiple relational tasks reliably produces convergent world representations, but lurking divergent tasks can catastrophically harm new entity integration via fine-tuning.

</details>


### [949] [Depth, Not Data: An Analysis of Hessian Spectral Bifurcation](https://arxiv.org/abs/2602.00545)
*Shenyang Deng,Boyao Liao,Zhuoli Ouyang,Tianyu Pang,Yaoqing Yang*

Main category: cs.LG

Relevance: 75.0

TL;DR: 该论文挑战了传统观点，证明深度神经网络Hessian矩阵的"bulk-and-spike"谱结构（少数主导特征值与大量小特征值分离）不仅源于数据协方差矩阵的不平衡，也可以纯粹由网络架构本身产生，即使数据协方差完全平衡时也会出现这种谱分叉现象。


<details>
  <summary>Details</summary>
Motivation: 传统研究将深度神经网络Hessian矩阵的"bulk-and-spike"谱结构归因于数据协方差矩阵的不平衡。本文旨在挑战这一观点，探索网络架构本身是否也能产生这种谱分叉现象，从而更全面地理解深度网络的优化景观。

Method: 采用深度线性网络设置进行分析，在数据协方差完全平衡的条件下，理论证明Hessian矩阵仍会呈现谱分叉结构：一个主导特征值簇和一个批量特征值簇。特别地，证明了主导特征值与批量特征值之间的比值随网络深度线性增长。

Result: 研究结果表明，即使数据协方差完全平衡，Hessian矩阵仍会呈现谱分叉结构，且主导特征值与批量特征值的比值随网络深度线性增长。这证明谱间隙不仅受数据分布影响，更强烈地受网络架构影响。

Conclusion: 深度神经网络优化算法的设计应同时考虑模型架构和数据特性。网络架构本身（特别是深度）对优化景观有重要影响，这为理解深度网络训练动态提供了新视角。

Abstract: The eigenvalue distribution of the Hessian matrix plays a crucial role in understanding the optimization landscape of deep neural networks. Prior work has attributed the well-documented ``bulk-and-spike'' spectral structure, where a few dominant eigenvalues are separated from a bulk of smaller ones, to the imbalance in the data covariance matrix. In this work, we challenge this view by demonstrating that such spectral Bifurcation can arise purely from the network architecture, independent of data imbalance.
  Specifically, we analyze a deep linear network setup and prove that, even when the data covariance is perfectly balanced, the Hessian still exhibits a Bifurcation eigenvalue structure: a dominant cluster and a bulk cluster. Crucially, we establish that the ratio between dominant and bulk eigenvalues scales linearly with the network depth. This reveals that the spectral gap is strongly affected by the network architecture rather than solely by data distribution. Our results suggest that both model architecture and data characteristics should be considered when designing optimization algorithms for deep networks.

</details>


### [950] [Beyond the Node: Clade-level Selection for Efficient MCTS in Automatic Heuristic Design](https://arxiv.org/abs/2602.00549)
*Kezhao Lai,Yutao Lai,Hai-Lin Liu*

Main category: cs.LG

Relevance: 75.0

TL;DR: Clade-AHD是一个用于大型语言模型自动启发式设计的高效框架，通过用贝叶斯信念替换节点级点估计来解决MCTS的过度开发问题，在有限计算预算下实现更可靠的决策。


<details>
  <summary>Details</summary>
Motivation: 在基于LLM的自动启发式设计中，蒙特卡洛树搜索在有限计算预算下存在过度开发的倾向，这限制了启发式评估的效果。需要一种能够更好处理稀疏和噪声评估的方法。

Method: 提出Clade-AHD框架，用分支级别的贝叶斯信念替代节点级点估计。通过将后代评估聚合为Beta分布，并在这些信念上执行Thompson采样，显式建模不确定性以指导探索。

Result: 在复杂的组合优化问题上进行广泛实验，Clade-AHD始终优于最先进的方法，同时显著降低计算成本。

Conclusion: Clade-AHD通过贝叶斯不确定性建模有效解决了MCTS在有限计算预算下的过度开发问题，为LLM自动启发式设计提供了更高效可靠的框架。

Abstract: While Monte Carlo Tree Search (MCTS) shows promise in Large Language Model (LLM) based Automatic Heuristic Design (AHD), it suffers from a critical over-exploitation tendency under the limited computational budgets required for heuristic evaluation. To address this limitation, we propose Clade-AHD, an efficient framework that replaces node-level point estimates with clade-level Bayesian beliefs. By aggregating descendant evaluations into Beta distributions and performing Thompson Sampling over these beliefs, Clade-AHD explicitly models uncertainty to guide exploration, enabling more reliable decision-making under sparse and noisy evaluations. Extensive experiments on complex combinatorial optimization problems demonstrate that Clade-AHD consistently outperforms state-of-the-art methods while significantly reducing computational cost. The source code is publicly available at: https://github.com/Mriya0306/Clade-AHD.

</details>


### [951] [Safe Langevin Soft Actor Critic](https://arxiv.org/abs/2602.00587)
*Mahesh Keswani,Samyak Jain,Raunak P. Bhattacharyya*

Main category: cs.LG

Relevance: 75.0

TL;DR: SL-SAC是一种安全的强化学习算法，通过参数空间探索和分布风险控制，在约束强化学习中平衡奖励与安全性，在Safety-Gymnasium基准测试中表现出色。


<details>
  <summary>Details</summary>
Motivation: 约束强化学习中平衡奖励与安全性面临两大挑战：从尖锐价值最小值泛化能力差，以及对重尾风险分布处理不足。现有方法难以同时解决这两个问题。

Method: 提出Safe Langevin Soft Actor-Critic (SL-SAC)算法，包含三个核心机制：1) 自适应随机梯度朗之万动力学(aSGLD)用于奖励评论家，促进集成多样性并逃离不良最优解；2) 通过隐式分位数网络(IQN)和条件风险价值(CVaR)优化的分布成本估计，缓解尾部风险；3) 基于经验CVaR的自适应拉格朗日松弛方案，根据情节成本调整约束执行。

Result: 在Safety-Gymnasium基准测试中，SL-SAC在10个任务中的7个实现了最低成本，同时保持有竞争力的回报。在速度任务中，相比最先进的基线方法，成本降低了19-63%。

Conclusion: SL-SAC通过参数空间探索和分布风险控制的组合，有效解决了约束强化学习中的泛化问题和重尾风险处理问题，为安全强化学习提供了新的解决方案。

Abstract: Balancing reward and safety in constrained reinforcement learning remains challenging due to poor generalization from sharp value minima and inadequate handling of heavy-tailed risk distribution. We introduce Safe Langevin Soft Actor-Critic (SL-SAC), a principled algorithm that addresses both issues through parameter-space exploration and distributional risk control. Our approach combines three key mechanisms: (1) Adaptive Stochastic Gradient Langevin Dynamics (aSGLD) for reward critics, promoting ensemble diversity and escape from poor optima; (2) distributional cost estimation via Implicit Quantile Networks (IQN) with Conditional Value-at-Risk (CVaR) optimization for tail-risk mitigation; and (3) a reactive Lagrangian relaxation scheme that adapts constraint enforcement based on the empirical CVaR of episodic costs. We provide theoretical guarantees on CVaR estimation error and demonstrate that CVaR-based Lagrange updates yield stronger constraint violation signals than expected-cost updates. On Safety-Gymnasium benchmarks, SL-SAC achieves the lowest cost in 7 out of 10 tasks while maintaining competitive returns, with cost reductions of 19-63% in velocity tasks compared to state-of-the-art baselines.

</details>


### [952] [LocalV: Exploiting Information Locality for IP-level Verilog Generation](https://arxiv.org/abs/2602.00704)
*Hanqi Lyu,Di Huang,Yaoyu Zhu,Kangcheng Liu,Bohan Dou,Chongxiao Li,Pengwei Jin,Shuyao Cheng,Rui Zhang,Zidong Du,Qi Guo,Xing Hu,Yunji Chen*

Main category: cs.LG

Relevance: 75.0

TL;DR: LocalV是一个多智能体框架，通过利用模块化硬件设计中的信息局部性，将长文档到长代码生成问题分解为短文档、短代码任务，显著提升了RTL代码生成的准确性和可扩展性。


<details>
  <summary>Details</summary>
Motivation: 传统RTL代码生成需要工程师手动将复杂规范转换为数千行可综合的HDL代码，劳动密集且容易出错。虽然LLM在自动化这一过程中显示出潜力，但现有方法（包括微调的领域特定模型和先进的基于智能体的系统）难以扩展到工业级IP设计任务。论文识别了三个关键挑战：处理长而详细的文档、生成长RTL代码时的正确性下降问题，以及复杂的调试周期。

Method: LocalV采用多智能体框架，利用模块化硬件设计中的信息局部性。该方法包括：层次化文档分区、任务规划、局部化代码生成、接口一致性合并，以及AST引导的局部感知调试。通过将长文档到长代码的生成问题分解为短文档、短代码任务，实现了可扩展的生成和调试。

Result: 在RealBench（一个IP级Verilog生成基准测试）上的实验表明，LocalV显著优于最先进的LLM和智能体方法，达到了45.0%的通过率，而现有最佳方法仅为21.6%。

Conclusion: LocalV通过利用信息局部性，成功解决了工业级硬件设计中长文档到长代码生成的挑战，为自动化RTL代码生成提供了有效的解决方案，显著提升了生成代码的质量和可扩展性。

Abstract: The generation of Register-Transfer Level (RTL) code is a crucial yet labor-intensive step in digital hardware design, traditionally requiring engineers to manually translate complex specifications into thousands of lines of synthesizable Hardware Description Language (HDL) code. While Large Language Models (LLMs) have shown promise in automating this process, existing approaches-including fine-tuned domain-specific models and advanced agent-based systems-struggle to scale to industrial IP-level design tasks. We identify three key challenges: (1) handling long, highly detailed documents, where critical interface constraints become buried in unrelated submodule descriptions; (2) generating long RTL code, where both syntactic and semantic correctness degrade sharply with increasing output length; and (3) navigating the complex debugging cycles required for functional verification through simulation and waveform analysis. To overcome these challenges, we propose LocalV, a multi-agent framework that leverages information locality in modular hardware design. LocalV decomposes the long-document to long-code generation problem into a set of short-document, short-code tasks, enabling scalable generation and debugging. Specifically, LocalV integrates hierarchical document partitioning, task planning, localized code generation, interface-consistent merging, and AST-guided locality-aware debugging. Experiments on RealBench, an IP-level Verilog generation benchmark, demonstrate that LocalV substantially outperforms state-of-the-art (SOTA) LLMs and agents, achieving a pass rate of 45.0% compared to 21.6%.

</details>


### [953] [Spectral Imbalance Causes Forgetting in Low-Rank Continual Adaptation](https://arxiv.org/abs/2602.00722)
*Hao Gu,Mao-Lin Luo,Zi-Hao Zhou,Han-Chen Zhang,Min-Ling Zhang,Tong Wei*

Main category: cs.LG

Relevance: 75.0

TL;DR: 本文提出了一种参数高效的持续学习方法EBLoRA，通过解耦任务更新的幅度和方向结构，在受限Stiefel流形上优化，以平衡奇异值谱，减少前后向遗忘。


<details>
  <summary>Details</summary>
Motivation: 现有参数高效持续学习方法主要关注避免与过去更新的干扰，而非考虑当前任务特定更新如何自然保留先前知识。从知识分解角度观察，低秩适应表现出高度不平衡的奇异值谱：少数主导组件吸收大部分适应能量，既可能破坏先前知识，又使更新易受后续任务干扰。

Method: 将任务更新的幅度与方向结构解耦，在受限Stiefel流形上构建约束优化问题，使用与标准深度学习优化器兼容的投影一阶方法解决，实现组件间的显式平衡。

Result: 该方法能同时缓解后向遗忘和前向遗忘，在持续学习基准测试中一致优于现有基线方法。

Conclusion: 通过显式平衡低秩适应中的奇异值谱，可以更有效地实现参数高效的持续学习，减少知识遗忘并提高模型稳定性。

Abstract: Parameter-efficient continual learning aims to adapt pre-trained models to sequential tasks without forgetting previously acquired knowledge. Most existing approaches treat continual learning as avoiding interference with past updates, rather than considering what properties make the current task-specific update naturally preserve previously acquired knowledge. From a knowledge-decomposition perspective, we observe that low-rank adaptations exhibit highly imbalanced singular value spectra: a few dominant components absorb most of the adaptation energy, thereby (i) more likely to disrupt previously acquired knowledge and (ii) making the update more vulnerable to interference from subsequent tasks. To enable explicit balance among components, we decouple the magnitude of the task update from its directional structure and formulate it as a constrained optimization problem on a restricted Stiefel manifold. We address this problem using a projected first-order method compatible with standard deep-learning optimizers used in vision-language models. Our method mitigates both backward and forward forgetting, consistently outperforming continual learning baselines. The implementation code is available at https://github.com/haodotgu/EBLoRA.

</details>


### [954] [Over-Alignment vs Over-Fitting: The Role of Feature Learning Strength in Generalization](https://arxiv.org/abs/2602.00827)
*Taesun Yeom,Taehyeok Ha,Jaeho Lee*

Main category: cs.LG

Relevance: 75.0

TL;DR: 研究发现特征学习强度（FLS）存在最优值，既不能太小也不能太大，才能获得最佳泛化性能，这与传统认为更强的特征学习总是改善泛化的直觉相反。


<details>
  <summary>Details</summary>
Motivation: 特征学习强度（FLS）在神经网络优化动态中起关键作用，但现有理论主要在渐近条件下研究，对实际训练设置（如达到目标训练风险时停止训练）中FLS如何影响泛化的理解有限。

Method: 通过实证研究探索FLS对泛化的影响，然后使用两层ReLU网络在逻辑损失下的梯度流动力学进行理论分析，通过初始化规模控制FLS。

Result: 发现存在最优FLS值，能带来显著的泛化增益。理论分析表明最优FLS源于两种竞争效应的权衡：过大的FLS导致"过度对齐"现象损害泛化，过小的FLS导致过拟合。

Conclusion: 特征学习强度存在最优值，需要在过度对齐和过拟合之间取得平衡，这对实际神经网络训练中的初始化策略和优化动态有重要启示。

Abstract: Feature learning strength (FLS), i.e., the inverse of the effective output scaling of a model, plays a critical role in shaping the optimization dynamics of neural nets. While its impact has been extensively studied under the asymptotic regimes -- both in training time and FLS -- existing theory offers limited insight into how FLS affects generalization in practical settings, such as when training is stopped upon reaching a target training risk. In this work, we investigate the impact of FLS on generalization in deep networks under such practical conditions. Through empirical studies, we first uncover the emergence of an $\textit{optimal FLS}$ -- neither too small nor too large -- that yields substantial generalization gains. This finding runs counter to the prevailing intuition that stronger feature learning universally improves generalization. To explain this phenomenon, we develop a theoretical analysis of gradient flow dynamics in two-layer ReLU nets trained with logistic loss, where FLS is controlled via initialization scale. Our main theoretical result establishes the existence of an optimal FLS arising from a trade-off between two competing effects: An excessively large FLS induces an $\textit{over-alignment}$ phenomenon that degrades generalization, while an overly small FLS leads to $\textit{over-fitting}$.

</details>


### [955] [RMFlow: Refined Mean Flow by a Noise-Injection Step for Multimodal Generation](https://arxiv.org/abs/2602.00849)
*Yuhao Huang,Shih-Hsin Wang,Andrea L. Bertozzi,Bao Wang*

Main category: cs.LG

Relevance: 75.0

TL;DR: RMFlow是一种高效的多模态生成模型，通过结合粗粒度1-NFE MeanFlow传输和定制化噪声注入细化步骤，在仅需单次函数评估的情况下实现接近SOTA的生成质量。


<details>
  <summary>Details</summary>
Motivation: MeanFlow虽然能够实现高效、高保真度的图像生成，但其单次函数评估（1-NFE）生成往往无法产生令人信服的结果。需要解决1-NFE生成质量不足的问题，同时保持计算效率。

Method: 提出RMFlow模型，整合粗粒度1-NFE MeanFlow传输和后续定制化噪声注入细化步骤。使用神经网络近似流路径的平均速度，通过新的损失函数训练，平衡概率路径之间的Wasserstein距离最小化和样本似然最大化。

Result: 在文本到图像、上下文到分子和时间序列生成任务上，仅使用1-NFE就实现了接近最先进水平的结果，计算成本与基线MeanFlows相当。

Conclusion: RMFlow通过创新的两阶段方法，成功解决了1-NFE生成质量不足的问题，在保持计算效率的同时显著提升了生成性能，为高效生成模型提供了新思路。

Abstract: Mean flow (MeanFlow) enables efficient, high-fidelity image generation, yet its single-function evaluation (1-NFE) generation often cannot yield compelling results. We address this issue by introducing RMFlow, an efficient multimodal generative model that integrates a coarse 1-NFE MeanFlow transport with a subsequent tailored noise-injection refinement step. RMFlow approximates the average velocity of the flow path using a neural network trained with a new loss function that balances minimizing the Wasserstein distance between probability paths and maximizing sample likelihood. RMFlow achieves near state-of-the-art results on text-to-image, context-to-molecule, and time-series generation using only 1-NFE, at a computational cost comparable to the baseline MeanFlows.

</details>


### [956] [Investigating the Robustness of Subtask Distillation under Spurious Correlation](https://arxiv.org/abs/2602.00852)
*Pattarawat Chormai,Klaus-Robert Müller,Grégoire Montavon*

Main category: cs.LG

Relevance: 75.0

TL;DR: 该研究评估了在存在虚假相关性的数据上进行子任务蒸馏时，不同蒸馏方法的鲁棒性表现，发现SubDistill等先进方法相比基线方法对虚假相关性更具抵抗力。


<details>
  <summary>Details</summary>
Motivation: 子任务蒸馏从大型通用基础模型中提取紧凑专用模型，但通常依赖有限的数据集，这些数据集可能存在虚假相关性或不具代表性。研究旨在评估在存在虚假相关性的数据上进行蒸馏时，不同方法的鲁棒性表现。

Method: 评估了已建立的蒸馏方法以及最近的SubDistill方法，在具有虚假相关性的数据上进行蒸馏实验。通过控制虚假相关性的强度，观察不同方法的性能变化。

Result: 随着虚假相关性强度增加，SubDistill等先进方法保持相对稳健，而一些基线方法性能退化到接近随机水平。先进方法与基线方法之间的性能差距随相关性强度增加而扩大。

Conclusion: 研究强调了在具有虚假相关性的不完美现实世界数据集上进行知识蒸馏的挑战，需要开发更鲁棒的蒸馏方法来应对数据质量问题。

Abstract: Subtask distillation is an emerging paradigm in which compact, specialized models are extracted from large, general-purpose 'foundation models' for deployment in environments with limited resources or in standalone computer systems. Although distillation uses a teacher model, it still relies on a dataset that is often limited in size and may lack representativeness or exhibit spurious correlations. In this paper, we evaluate established distillation methods, as well as the recent SubDistill method, when using data with spurious correlations for distillation. As the strength of the correlations increases, we observe a widening gap between advanced methods, such as SubDistill, which remain fairly robust, and some baseline methods, which degrade to near-random performance. Overall, our study underscores the challenges of knowledge distillation when applied to imperfect, real-world datasets, particularly those with spurious correlations.

</details>


### [957] [Dynamic Prior Thompson Sampling for Cold-Start Exploration in Recommender Systems](https://arxiv.org/abs/2602.00943)
*Zhenyu Zhao,David Zhang,Ellie Zhao,Ehsan Saberian*

Main category: cs.LG

Relevance: 75.0

TL;DR: 提出动态先验汤普森采样方法，通过控制新项目超越现有最优项的概率来解决推荐系统冷启动探索问题，相比均匀先验基线提高了探索效率和可控性。


<details>
  <summary>Details</summary>
Motivation: 推荐系统中冷启动探索的核心挑战：新项目或数据稀疏项目需要流量来估计价值，但过度探索会损害用户体验并浪费曝光机会。实践中，汤普森采样通常使用均匀Beta(1,1)先验，隐含假设新项目有50%的成功率。当真实基础率远低于此值时，这种乐观先验会系统性地过度分配给弱项目。批处理策略更新和管道延迟放大了这一问题：新项目在数小时内可能保持"无数据"状态，先验在反馈被纳入前主导分配决策。

Method: 提出动态先验汤普森采样，通过先验设计直接控制新项目超越现有最优项的概率。核心贡献是推导出先验均值的闭式二次解，确保在引入时P(X_j > Y_k) = epsilon，使探索强度可预测且可调，同时保留汤普森采样的贝叶斯更新机制。

Result: 通过蒙特卡洛验证、离线批处理模拟以及在服务数百万用户的缩略图个性化系统上进行的大规模在线实验，动态先验相比均匀先验基线提供了精确的探索控制和改进的效率。

Conclusion: 动态先验汤普森采样方法能够有效解决推荐系统冷启动探索问题，通过可调的先验设计实现更精确的探索控制，在保持汤普森采样优点的同时提高了系统效率。

Abstract: Cold-start exploration is a core challenge in large-scale recommender systems: new or data-sparse items must receive traffic to estimate value, but over-exploration harms users and wastes impressions. In practice, Thompson Sampling (TS) is often initialized with a uniform Beta(1,1) prior, implicitly assuming a 50% success rate for unseen items. When true base rates are far lower, this optimistic prior systematically over-allocates to weak items. The impact is amplified by batched policy updates and pipeline latency: for hours, newly launched items can remain effectively "no data," so the prior dominates allocation before feedback is incorporated. We propose Dynamic Prior Thompson Sampling, a prior design that directly controls the probability that a new arm outcompetes the incumbent winner. Our key contribution is a closed-form quadratic solution for the prior mean that enforces P(X_j > Y_k) = epsilon at introduction time, making exploration intensity predictable and tunable while preserving TS Bayesian updates. Across Monte Carlo validation, offline batched simulations, and a large-scale online experiment on a thumbnail personalization system serving millions of users, dynamic priors deliver precise exploration control and improved efficiency versus a uniform-prior baseline.

</details>


### [958] [Optimal Budgeted Adaptation of Large Language Models](https://arxiv.org/abs/2602.00952)
*Jing Wang,Jie Shen,Dean Foster,Zohar Karnin,Jeremy C Weiss*

Main category: cs.LG

Relevance: 75.0

TL;DR: 论文提出了一种预算感知的监督微调框架，将LLM适应建模为上下文Stackelberg博弈，通过有限监督预算优化标签效率，实现了理论上的遗憾边界。


<details>
  <summary>Details</summary>
Motivation: 解决大型语言模型微调中标签数据可用性与下游准确性之间的权衡问题，特别是在监督预算有限的情况下如何高效利用标签数据。

Method: 将LLM适应建模为上下文Stackelberg博弈：学习者（领导者）承诺评分策略和标签查询策略，自适应环境（跟随者）选择具有挑战性的监督替代方案。引入有限监督预算到学习目标中，并提出了带有最大延迟优先置信门的选择性标签查询机制。

Result: 在标准线性上下文假设下实现了$\tilde{O}(d\sqrt{T})$的遗憾边界，通过LLF置信门实现了$\tilde{O}(\sqrt{dB} + c\sqrt{B})$的预算感知遗憾边界（其中$B=βT$）。

Conclusion: 该框架为预算受限的LLM监督微调提供了理论保证，通过博弈论方法平衡了标签效率与模型性能，为实际应用中的标签成本控制提供了解决方案。

Abstract: The trade-off between labeled data availability and downstream accuracy remains a central challenge in fine-tuning large language models (LLMs). We propose a principled framework for \emph{budget-aware supervised fine-tuning} by casting LLM adaptation as a contextual Stackelberg game. In our formulation, the learner (leader) commits to a scoring policy and a label-querying strategy, while an adaptive environment (follower) selects challenging supervised alternatives in response. To explicitly address label efficiency, we incorporate a finite supervision budget directly into the learning objective. Our algorithm operates in the full-feedback regime and achieves $\tilde{O}(d\sqrt{T})$ regret under standard linear contextual assumptions. We extend the framework with a Largest-Latency-First (LLF) confidence gate that selectively queries labels, achieving a budget-aware regret bound of $\tilde{O}(\sqrt{dB} + c\sqrt{B})$ with $B=βT$.

</details>


### [959] [Superposition unifies power-law training dynamics](https://arxiv.org/abs/2602.01045)
*Zixin Jessie Chen,Hao Chen,Yizhou Liu,Jeff Gore*

Main category: cs.LG

Relevance: 75.0

TL;DR: 论文研究了特征叠加在幂律训练动态中的作用，发现叠加瓶颈会导致训练指数收敛到约1的普适值，相比无叠加的序列学习加速高达10倍。


<details>
  <summary>Details</summary>
Motivation: 研究特征叠加在神经网络训练动态中的作用，特别是叠加如何影响训练损失随时间的幂律衰减行为。这对于理解大规模语言模型等使用叠加的神经网络的训练动态具有重要意义。

Method: 使用师生框架（teacher-student framework）进行分析。首先推导无叠加情况下的解析理论，然后研究叠加瓶颈对训练动态的影响，特别是对幂律训练指数的影响。

Result: 发现叠加瓶颈会导致训练指数收敛到约1的普适幂律指数，独立于数据和通道统计特性。这种叠加下的训练相比无叠加的序列学习加速高达10倍。

Conclusion: 特征叠加导致快速训练且具有数据无关的幂律指数，这对包括生产级大型语言模型在内的广泛使用叠加的神经网络具有重要启示。

Abstract: We investigate the role of feature superposition in the emergence of power-law training dynamics using a teacher-student framework. We first derive an analytic theory for training without superposition, establishing that the power-law training exponent depends on both the input data statistics and channel importance. Remarkably, we discover that a superposition bottleneck induces a transition to a universal power-law exponent of $\sim 1$, independent of data and channel statistics. This one over time training with superposition represents an up to tenfold acceleration compared to the purely sequential learning that takes place in the absence of superposition. Our finding that superposition leads to rapid training with a data-independent power law exponent may have important implications for a wide range of neural networks that employ superposition, including production-scale large language models.

</details>


### [960] [On the Expressive Power of Permutation-Equivariant Weight-Space Networks](https://arxiv.org/abs/2602.01083)
*Adir Dayan,Yam Eitan,Haggai Maron*

Main category: cs.LG

Relevance: 75.0

TL;DR: 该论文系统研究了权重空间网络的表达能力，证明了主要置换等变网络在表达能力上等价，并在温和假设下建立了权重空间和函数空间的普适性理论。


<details>
  <summary>Details</summary>
Motivation: 权重空间学习研究直接操作其他神经网络参数的神经架构。随着预训练模型的普及，权重空间网络在各种任务中表现出有效性。现有SOTA方法依赖置换等变设计来提升泛化，但这可能影响表达能力，需要理论分析。权重空间学习同时涉及权重空间和函数空间的映射，使得表达能力分析特别微妙。

Method: 开发了权重空间网络表达能力的系统理论。首先证明了所有主要置换等变网络在表达能力上等价。然后在输入权重的温和自然假设下，建立了权重空间和函数空间设置的普适性，并刻画了普适性不再成立的边界情况。

Result: 1) 证明了主要置换等变网络在表达能力上等价；2) 在温和假设下建立了权重空间和函数空间的普适性；3) 刻画了普适性失效的边界情况。这些结果为权重空间网络的表达能力提供了统一的理论基础。

Conclusion: 该工作填补了权重空间网络表达能力理论分析的空白，为权重空间学习提供了坚实的理论基础，有助于指导未来权重空间网络的设计和应用。

Abstract: Weight-space learning studies neural architectures that operate directly on the parameters of other neural networks. Motivated by the growing availability of pretrained models, recent work has demonstrated the effectiveness of weight-space networks across a wide range of tasks. SOTA weight-space networks rely on permutation-equivariant designs to improve generalization. However, this may negatively affect expressive power, warranting theoretical investigation. Importantly, unlike other structured domains, weight-space learning targets maps operating on both weight and function spaces, making expressivity analysis particularly subtle. While a few prior works provide partial expressivity results, a comprehensive characterization is still missing. In this work, we address this gap by developing a systematic theory for expressivity of weight-space networks. We first prove that all prominent permutation-equivariant networks are equivalent in expressive power. We then establish universality in both weight- and function-space settings under mild, natural assumptions on the input weights, and characterize the edge-case regimes where universality no longer holds. Together, these results provide a strong and unified foundation for the expressivity of weight-space networks.

</details>


### [961] [OLion: Approaching the Hadamard Ideal by Intersecting Spectral and $\ell_{\infty}$ Implicit Biases](https://arxiv.org/abs/2602.01105)
*Zixiao Wang,Yifei Shen,Huishuai Zhang*

Main category: cs.LG

Relevance: 75.0

TL;DR: 提出了一种名为A的新优化器，结合了正交化更新方向的光谱控制和符号更新的ℓ∞风格坐标控制，在语言和视觉训练中匹配或优于AdamW和Muon，同时仅使用动量级优化器状态。


<details>
  <summary>Details</summary>
Motivation: 许多优化器可以解释为范数诱导几何下的最速下降方法，从而继承相应的隐式偏差。本文旨在结合光谱控制（来自正交化更新方向）和坐标控制（来自符号更新），提供更高效的优化方法。

Method: 提出A优化器，形成Lion风格的动量方向，通过少量Newton-Schulz迭代近似正交化，然后应用逐元素符号操作，有效近似在光谱和ℓ∞约束集交集上取最大步长。

Result: 在GPT-2和Llama预训练、SiT图像预训练和监督微调等大规模语言和视觉训练中，A匹配或优于AdamW和Muon，同时仅使用动量级优化器状态，并能缓解AdamW预训练检查点微调时的优化器不匹配问题。

Conclusion: A优化器成功结合了光谱和坐标控制，在大规模深度学习任务中表现出色，提供了一种高效且有效的优化替代方案。

Abstract: Many optimizers can be interpreted as steepest-descent methods under norm-induced geometries, and thus inherit corresponding implicit biases. We introduce \nameA{} (\fullname{}), which combines spectral control from orthogonalized update directions with $\ell_\infty$-style coordinate control from sign updates. \nameA{} forms a Lion-style momentum direction, approximately orthogonalizes it via a few Newton--Schulz iterations, and then applies an entrywise sign, providing an efficient approximation to taking a maximal step over the intersection of the spectral and $\ell_\infty$ constraint sets (a scaled Hadamard-like set for matrix parameters). Despite the strong nonlinearity of orthogonalization and sign, we prove convergence under a mild, empirically verified diagonal-isotropy assumption. Across large-scale language and vision training, including GPT-2 and Llama pretraining, SiT image pretraining, and supervised fine-tuning, \nameA{} matches or outperforms AdamW and Muon under comparable tuning while using only momentum-level optimizer state, and it mitigates optimizer mismatch when fine-tuning AdamW-pretrained checkpoints.

</details>


### [962] [TRACE: Scalable Amortized Causal Discovery from Single Sequences via Autoregressive Density Estimation](https://arxiv.org/abs/2602.01135)
*Hugo Math,Rainer Lienhart*

Main category: cs.LG

Relevance: 75.0

TL;DR: TRACE：利用自回归模型作为预训练密度估计器，从单条离散事件序列中进行因果发现的可扩展框架


<details>
  <summary>Details</summary>
Motivation: 解决从单条离散事件序列（如车辆日志、制造系统、患者轨迹）中进行因果发现的挑战，该场景因缺乏重复样本、高维度和长程时间依赖而特别困难

Method: 将自回归模型重新用作预训练密度估计器，用于条件互信息估计；TRACE推断事件类型间的摘要因果图，支持延迟因果效应，在GPU上完全并行，计算复杂度随事件词汇量线性增长

Result: 实验表明在不同基线和不同词汇量下具有鲁棒性能，包括在超过29,100个事件类型的车辆诊断根因分析中的应用

Conclusion: TRACE为从单条离散事件序列中进行可扩展因果发现提供了有效框架，在非完美自回归模型下具有理论可识别性保证

Abstract: We study causal discovery from a single observed sequence of discrete events generated by a stochastic process, as encountered in vehicle logs, manufacturing systems, or patient trajectories. This regime is particularly challenging due to the absence of repeated samples, high dimensionality, and long-range temporal dependencies of the single observation during inference. We introduce TRACE, a scalable framework that repurposes autoregressive models as pretrained density estimators for conditional mutual information estimation. TRACE infers the summary causal graph between event types in a sequence, scaling linearly with the event vocabulary and supporting delayed causal effects, while being fully parallel on GPUs. We establish its theoretical identifiability under imperfect autoregressive models. Experiments demonstrate robust performance across different baselines and varying vocabulary sizes including an application to root-cause analysis in vehicle diagnostics with over 29,100 event types.

</details>


### [963] [Diving into Kronecker Adapters: Component Design Matters](https://arxiv.org/abs/2602.01267)
*Jiayu Bai,Danchen Yu,Zhenyu Liao,TianQi Hou,Feng Zhou,Robert C. Qiu,Zenan Ling*

Main category: cs.LG

Relevance: 75.0

TL;DR: CDKA提出了一种基于组件设计的Kronecker适配器，通过分析Kronecker组件的维度和数量对模型容量的影响，提供了参数预算感知的配置指南和训练稳定策略，在多种NLP任务上表现优异。


<details>
  <summary>Details</summary>
Motivation: 现有Kronecker适配器方法大多将组件结构视为固定或启发式设计选择，对Kronecker组件的维度和数量缺乏深入探索。作者发现组件结构是决定Kronecker适配器容量的关键因素，需要系统研究其配置对模型性能的影响。

Method: 提出Component Designed Kronecker Adapters (CDKA)：1）对Kronecker组件的维度和数量进行细粒度分析；2）证明Kronecker适配器与全参数微调的对齐程度取决于组件配置；3）提供参数预算感知的配置指南；4）设计专门的训练稳定策略。

Result: 在各种自然语言处理任务上的实验证明了CDKA的有效性。该方法能够更好地平衡参数效率和模型性能，为Kronecker适配器的实际部署提供了实用指导。

Conclusion: Kronecker组件的结构设计对适配器性能至关重要，CDKA通过系统分析和优化组件配置，显著提升了Kronecker适配器的效果，为大模型高效微调提供了新的解决方案。

Abstract: Kronecker adapters have emerged as a promising approach for fine-tuning large-scale models, enabling high-rank updates through tunable component structures. However, existing work largely treats the component structure as a fixed or heuristic design choice, leaving the dimensions and number of Kronecker components underexplored. In this paper, we identify component structure as a key factor governing the capacity of Kronecker adapters. We perform a fine-grained analysis of both the dimensions and number of Kronecker components. In particular, we show that the alignment between Kronecker adapters and full fine-tuning depends on component configurations. Guided by these insights, we propose Component Designed Kronecker Adapters (CDKA). We further provide parameter-budget-aware configuration guidelines and a tailored training stabilization strategy for practical deployment. Experiments across various natural language processing tasks demonstrate the effectiveness of CDKA. Code is available at https://github.com/rainstonee/CDKA.

</details>


### [964] [Mixture-of-World Models: Scaling Multi-Task Reinforcement Learning with Modular Latent Dynamics](https://arxiv.org/abs/2602.01270)
*Boxuan Zhang,Weipu Zhang,Zhaohan Feng,Wei Xiao,Jian Sun,Jie Chen,Gang Wang*

Main category: cs.LG

Relevance: 75.0

TL;DR: MoW是一种用于多任务强化学习的混合世界模型架构，通过模块化视觉压缩、任务条件化专家和梯度聚类策略，在Atari和Meta-World基准上实现了参数高效的通用世界模型。


<details>
  <summary>Details</summary>
Motivation: 多任务强化学习在视觉领域中面临样本效率挑战，特别是当任务在观察和动态特性上存在显著异质性时。传统的单体世界模型架构难以捕捉多样化的任务动态，导致重建和预测准确性差。

Method: 提出Mixture-of-World Models (MoW)架构：1) 模块化变分自编码器用于任务自适应视觉压缩；2) 混合Transformer动态模型，包含任务条件化专家和共享主干；3) 基于梯度的任务聚类策略，实现高效参数分配。

Result: 在Atari 100k基准上，单个MoW代理在26个Atari游戏中获得110.4%的平均人类标准化分数，与需要26个任务特定模型的STORM（114.2%）相当，但参数减少50%。在Meta-World上，30万步内达到74.5%的平均成功率，创下新记录。

Conclusion: MoW为通用世界模型提供了一个可扩展且参数高效的基础，能够有效处理多任务强化学习中的视觉异质性问题，在保持性能的同时显著减少参数需求。

Abstract: A fundamental challenge in multi-task reinforcement learning (MTRL) is achieving sample efficiency in visual domains where tasks exhibit substantial heterogeneity in both observations and dynamics. Model-based reinforcement learning offers a promising path to improved sample efficiency through world models, but standard monolithic architectures struggle to capture diverse task dynamics, resulting in poor reconstruction and prediction accuracy. We introduce Mixture-of-World Models (MoW), a scalable architecture that combines modular variational autoencoders for task-adaptive visual compression, a hybrid Transformer-based dynamics model with task-conditioned experts and a shared backbone, and a gradient-based task clustering strategy for efficient parameter allocation. On the Atari 100k benchmark, a single MoW agent trained once on 26 Atari games achieves a mean human-normalized score of 110.4%, competitive with the score of 114.2% achieved by STORM, an ensemble of 26 task-specific models, while using 50% fewer parameters. On Meta-World, MoW achieves a 74.5% average success rate within 300 thousand environment steps, establishing a new state of the art. These results demonstrate that MoW provides a scalable and parameter-efficient foundation for generalist world models.

</details>


### [965] [Richer Bayesian Last Layers with Subsampled NTK Features](https://arxiv.org/abs/2602.01279)
*Sergio Calvo-Ordoñez,Jonathan Plenk,Richard Bergna,Álvaro Cartea,Yarin Gal,Jose Miguel Hernández-Lobato,Kamil Ciosek*

Main category: cs.LG

Relevance: 75.0

TL;DR: 提出一种改进贝叶斯最后一层的方法，通过将神经正切核特征投影到最后一层特征空间，从而考虑全网络的不确定性，同时保持计算效率


<details>
  <summary>Details</summary>
Motivation: 贝叶斯最后一层虽然计算高效，但只对最后一层进行贝叶斯处理，忽略了前面层引入的不确定性，导致认知不确定性被低估

Method: 利用神经正切核特征到最后一层特征空间的投影，使后验推断能考虑全网络的变异性；引入均匀子采样方案来估计投影矩阵和进行后验推断，降低计算成本

Result: 在UCI回归、上下文赌博机、图像分类以及图像和表格数据集的分布外检测任务中，相比标准BLL和竞争基线，显示出更好的校准和不确定性估计，同时降低计算成本

Conclusion: 提出的方法有效纠正了标准BLL低估认知不确定性的倾向，在保持计算效率的同时提供了更准确的不确定性估计

Abstract: Bayesian Last Layers (BLLs) provide a convenient and computationally efficient way to estimate uncertainty in neural networks. However, they underestimate epistemic uncertainty because they apply a Bayesian treatment only to the final layer, ignoring uncertainty induced by earlier layers. We propose a method that improves BLLs by leveraging a projection of Neural Tangent Kernel (NTK) features onto the space spanned by the last-layer features. This enables posterior inference that accounts for variability of the full network while retaining the low computational cost of inference of a standard BLL. We show that our method yields posterior variances that are provably greater or equal to those of a standard BLL, correcting its tendency to underestimate epistemic uncertainty. To further reduce computational cost, we introduce a uniform subsampling scheme for estimating the projection matrix and for posterior inference. We derive approximation bounds for both types of sub-sampling. Empirical evaluations on UCI regression, contextual bandits, image classification, and out-of-distribution detection tasks in image and tabular datasets, demonstrate improved calibration and uncertainty estimates compared to standard BLLs and competitive baselines, while reducing computational cost.

</details>


### [966] [Imperfect Influence, Preserved Rankings: A Theory of TRAK for Data Attribution](https://arxiv.org/abs/2602.01312)
*Han Tong,Shubhangi Ghosh,Haolin Zou,Arian Maleki*

Main category: cs.LG

Relevance: 75.0

TL;DR: 本文对TRAK数据归因算法进行了理论分析，揭示了其近似误差但保持了数据点相对排序的有效性


<details>
  <summary>Details</summary>
Motivation: TRAK算法被广泛用于将模型预测追溯到特定训练数据，但其理论准确性条件和失效机制尚未得到充分探索，需要理论分析来理解其性能边界

Method: 通过理论分析TRAK算法，量化其近似方法引入的误差，证明虽然近似导致显著误差，但TRAK估计的影响力与原始影响力高度相关，保持了数据点的相对排序

Result: TRAK算法尽管存在近似误差，但其估计的影响力与原始影响力高度相关，能够有效保持数据点的相对排序，这一理论结果得到了大量模拟和实证研究的验证

Conclusion: TRAK算法在数据归因任务中虽然存在近似误差，但在保持数据点相对排序方面是有效的，为理解和应用该算法提供了理论依据

Abstract: Data attribution, tracing a model's prediction back to specific training data, is an important tool for interpreting sophisticated AI models. The widely used TRAK algorithm addresses this challenge by first approximating the underlying model with a kernel machine and then leveraging techniques developed for approximating the leave-one-out (ALO) risk. Despite its strong empirical performance, the theoretical conditions under which the TRAK approximations are accurate as well as the regimes in which they break down remain largely unexplored. In this paper, we provide a theoretical analysis of the TRAK algorithm, characterizing its performance and quantifying the errors introduced by the approximations on which the method relies. We show that although the approximations incur significant errors, TRAK's estimated influence remains highly correlated with the original influence and therefore largely preserves the relative ranking of data points. We corroborate our theoretical results through extensive simulations and empirical studies.

</details>


### [967] [Rectified LpJEPA: Joint-Embedding Predictive Architectures with Sparse and Maximum-Entropy Representations](https://arxiv.org/abs/2602.01456)
*Yilun Kuang,Yash Dagade,Tim G. J. Rudner,Randall Balestriero,Yann LeCun*

Main category: cs.LG

Relevance: 75.0

TL;DR: 论文提出Rectified Distribution Matching Regularization (RDMReg)，一种基于Rectified Generalized Gaussian分布的正则化方法，用于Joint-Embedding Predictive Architectures (JEPA)，以学习稀疏、非负的表示，相比现有高斯正则化方法能更好地控制稀疏性。


<details>
  <summary>Details</summary>
Motivation: 现有JEPA方法使用各向同性高斯分布正则化表示，但这种方法倾向于密集表示，无法捕捉高效表示中的关键稀疏特性。需要一种能显式控制稀疏度的正则化方法。

Method: 提出Rectified Distribution Matching Regularization (RDMReg)，这是一种切片双样本分布匹配损失，将表示对齐到Rectified Generalized Gaussian (RGG)分布。RGG通过整流操作显式控制期望ℓ₀范数，同时在期望ℓₚ范数约束下保持最大熵特性。将RDMReg集成到JEPA中得到Rectified LpJEPA。

Result: Rectified LpJEPA学习到稀疏、非负的表示，在稀疏性与性能之间取得良好平衡，在图像分类基准测试中表现出竞争力的下游性能，证明RDMReg能有效强制稀疏性同时保留任务相关信息。

Conclusion: RDMReg为JEPA提供了一种严格泛化先前高斯基JEPA的方法，能够显式控制表示稀疏度，学习到更高效的表示，在保持任务性能的同时实现稀疏性。

Abstract: Joint-Embedding Predictive Architectures (JEPA) learn view-invariant representations and admit projection-based distribution matching for collapse prevention. Existing approaches regularize representations towards isotropic Gaussian distributions, but inherently favor dense representations and fail to capture the key property of sparsity observed in efficient representations. We introduce Rectified Distribution Matching Regularization (RDMReg), a sliced two-sample distribution-matching loss that aligns representations to a Rectified Generalized Gaussian (RGG) distribution. RGG enables explicit control over expected $\ell_0$ norm through rectification, while preserving maximum-entropy up to rescaling under expected $\ell_p$ norm constraints. Equipping JEPAs with RDMReg yields Rectified LpJEPA, which strictly generalizes prior Gaussian-based JEPAs. Empirically, Rectified LpJEPA learns sparse, non-negative representations with favorable sparsity-performance trade-offs and competitive downstream performance on image classification benchmarks, demonstrating that RDMReg effectively enforces sparsity while preserving task-relevant information.

</details>


### [968] [OpInf-LLM: Parametric PDE Solving with LLMs via Operator Inference](https://arxiv.org/abs/2602.01493)
*Zhuoyuan Wang,Hanjiang Hu,Xiyu Deng,Saviz Mowlavi,Yorie Nakahira*

Main category: cs.LG

Relevance: 75.0

TL;DR: OpInf-LLM：基于算子推理的LLM参数化PDE求解框架，结合少量解数据实现高精度预测，支持自然语言指定PDE任务，在未见参数和配置下保持良好泛化能力。


<details>
  <summary>Details</summary>
Motivation: 现有LLM在PDE求解中存在执行成功率与数值精度之间的权衡问题，特别是在泛化到未见参数和边界条件时。需要一种能够可靠解决异质PDE设置的方法，同时保持高执行成功率和数值精度。

Method: 提出OpInf-LLM框架，基于算子推理（operator inference）技术，利用少量解数据实现参数化PDE求解。框架与LLM无缝集成，支持自然语言指定PDE任务，提供统一的工具接口，计算需求低。

Result: OpInf-LLM能够准确预测多样化的PDE实例，包括未见参数和配置，在异质设置下实现高执行成功率，为LLM-based PDE求解中的可泛化降阶建模开辟新可能性。

Conclusion: 通过结合算子推理与LLM能力，OpInf-LLM解决了PDE求解中执行成功率与数值精度的权衡问题，为科学计算中的LLM应用提供了新方向。

Abstract: Solving diverse partial differential equations (PDEs) is fundamental in science and engineering. Large language models (LLMs) have demonstrated strong capabilities in code generation, symbolic reasoning, and tool use, but reliably solving PDEs across heterogeneous settings remains challenging. Prior work on LLM-based code generation and transformer-based foundation models for PDE learning has shown promising advances. However, a persistent trade-off between execution success rate and numerical accuracy arises, particularly when generalization to unseen parameters and boundary conditions is required. In this work, we propose OpInf-LLM, an LLM parametric PDE solving framework based on operator inference. The proposed framework leverages a small amount of solution data to enable accurate prediction of diverse PDE instances, including unseen parameters and configurations, and provides seamless integration with LLMs for natural language specification of PDE solving tasks. Its low computational demands and unified tool interface further enable a high execution success rate across heterogeneous settings. By combining operator inference with LLM capabilities, OpInf-LLM opens new possibilities for generalizable reduced-order modeling in LLM-based PDE solving.

</details>


### [969] [Optimal Sample Complexity for Single Time-Scale Actor-Critic with Momentum](https://arxiv.org/abs/2602.01505)
*Navdeep Kumar,Tehila Dahan,Lior Cohen,Ananyabrata Barua,Giorgia Ramponi,Kfir Yehuda Levy,Shie Mannor*

Main category: cs.LG

Relevance: 75.0

TL;DR: 该论文提出了一种单时间尺度演员-评论家算法，通过结合STORM方差减少技术和经验回放缓冲区，在无限时域折扣MDP中实现了O(ε⁻²)的最优样本复杂度，相比之前的O(ε⁻³)有显著改进。


<details>
  <summary>Details</summary>
Motivation: 现有单时间尺度演员-评论家算法在无限时域折扣马尔可夫决策过程中需要O(ε⁻³)的样本复杂度才能获得ε-最优策略，这远非最优。研究旨在开发更高效的算法，在保持实用性的同时达到理论最优的样本复杂度。

Method: 提出了一种改进的单时间尺度演员-评论家算法，结合两种关键技术：1) 使用STORM（随机递归动量）技术减少评论家更新中的方差；2) 维护一个小型经验回放缓冲区，存储最近样本并均匀采样用于评论家更新，以应对由演化策略引起的非平稳分布问题。

Result: 该算法在有限状态-动作空间的无限时域折扣MDP中实现了O(ε⁻²)的最优样本复杂度，显著优于先前O(ε⁻³)的最佳结果。该方法与现有深度学习架构兼容，只需微小修改，不损害实际应用性。

Conclusion: 通过结合STORM方差减少技术和经验回放缓冲区，成功开发了达到理论最优样本复杂度的单时间尺度演员-评论家算法，为强化学习中的样本高效算法设计提供了重要进展。

Abstract: We establish an optimal sample complexity of $O(ε^{-2})$ for obtaining an $ε$-optimal global policy using a single-timescale actor-critic (AC) algorithm in infinite-horizon discounted Markov decision processes (MDPs) with finite state-action spaces, improving upon the prior state of the art of $O(ε^{-3})$. Our approach applies STORM (STOchastic Recursive Momentum) to reduce variance in the critic updates. However, because samples are drawn from a nonstationary occupancy measure induced by the evolving policy, variance reduction via STORM alone is insufficient. To address this challenge, we maintain a buffer of small fraction of recent samples and uniformly sample from it for each critic update. Importantly, these mechanisms are compatible with existing deep learning architectures and require only minor modifications, without compromising practical applicability.

</details>


### [970] [Universal Redundancies in Time Series Foundation Models](https://arxiv.org/abs/2602.01605)
*Anthony Bao,Venkata Hasith Vattikuti,Jeffrey Lai,William Gilpin*

Main category: cs.LG

Relevance: 75.0

TL;DR: 该论文发现时间序列基础模型（TSFMs）存在冗余组件，开发了机制可解释性工具，揭示模型对整层剪枝具有鲁棒性，并识别出导致退化现象（如模式重复和季节性偏差）的具体注意力头。


<details>
  <summary>Details</summary>
Motivation: 研究动机是理解时间序列基础模型（TSFMs）的内部工作机制。尽管TSFMs在时间序列预测中表现出色，但对其内部机制的理解有限。作者希望通过机制可解释性方法揭示这些模型的冗余性和特定组件的功能。

Method: 1. 开发了TSFMs的机制可解释性工具集，包括特定组件剪枝和残差流上的直接logit归因分析
2. 在多个领先的TSFM架构和多样化时间序列数据集上进行大规模评估
3. 提出理论框架将transformer视为核回归器，基于每个注意力头投影矩阵的稳定秩进行纯内在剪枝策略
4. 识别导致退化现象的具体注意力头

Result: 1. 所有研究的TSFM模型对整层剪枝都表现出鲁棒性
2. 发现了模型中的冗余中间层组件
3. 识别出导致两种常见退化现象的具体注意力头：上下文模式重复（parroting of motifs）和季节性偏差（seasonality bias）
4. 研究结果在不同架构的TSFMs和多样化数据集上具有一致性

Conclusion: 该研究揭示了时间序列基础模型的普遍特性，包括冗余性和对剪枝的鲁棒性。通过机制可解释性方法，成功识别了导致特定退化现象的注意力头，为理解这一新兴连续时间序列建模架构类别提供了重要见解。

Abstract: Time Series Foundation Models (TSFMs) leverage extensive pretraining to accurately predict unseen time series during inference, without the need for task-specific fine-tuning. Through large-scale evaluations on standard benchmarks, we find that leading transformer-based TSFMs exhibit redundant components in their intermediate layers. We introduce a set of tools for mechanistic interpretability of TSFMs, including ablations of specific components and direct logit attribution on the residual stream. Our findings are consistent across several leading TSFMs with diverse architectures, and across a diverse set of real-world and synthetic time-series datasets. We discover that all models in our study are robust to ablations of entire layers. Furthermore, we develop a theoretical framework framing transformers as kernel regressors, motivating a purely intrinsic strategy for ablating heads based on the stable rank of the per-head projection matrices. Using this approach, we uncover the specific heads responsible for degenerate phenomena widely observed in TSFMs, such as parroting of motifs from the context and seasonality bias. Our study sheds light on the universal properties of this emerging class of architectures for continuous-time sequence modeling.

</details>


### [971] [AdaptNC: Adaptive Nonconformity Scores for Uncertainty-Aware Autonomous Systems in Dynamic Environments](https://arxiv.org/abs/2602.01629)
*Renukanandan Tumu,Aditya Singh,Rahul Mangharam*

Main category: cs.LG

Relevance: 75.0

TL;DR: AdaptNC：一种用于机器人不确定度量的联合在线适应框架，同时调整非共形分数参数和共形阈值，以应对现实世界中的分布偏移，相比仅调整阈值的方法显著减小预测区域体积。


<details>
  <summary>Details</summary>
Motivation: 现实世界机器人应用中存在分布偏移，违反传统共形预测的交换性假设。现有在线CP方法仅调整阈值，使用固定的非共形分数函数，导致在结构变化时产生过于保守、体积低效的预测区域。

Method: 提出AdaptNC框架，联合在线适应非共形分数参数和共形阈值。采用自适应重加权方案优化分数函数，并引入回放缓冲机制缓解分数转换期间的覆盖不稳定性。

Result: 在多智能体策略变化、环境变化和传感器退化等多样化机器人基准测试中，AdaptNC相比最先进的仅调整阈值基线方法，在保持目标覆盖水平的同时显著减小了预测区域体积。

Conclusion: AdaptNC通过联合适应分数函数和阈值，有效解决了现实世界机器人中分布偏移带来的挑战，实现了更高效的不确定度量，对自主系统的安全部署具有重要意义。

Abstract: Rigorous uncertainty quantification is essential for the safe deployment of autonomous systems in unconstrained environments. Conformal Prediction (CP) provides a distribution-free framework for this task, yet its standard formulations rely on exchangeability assumptions that are violated by the distribution shifts inherent in real-world robotics. Existing online CP methods maintain target coverage by adaptively scaling the conformal threshold, but typically employ a static nonconformity score function. We show that this fixed geometry leads to highly conservative, volume-inefficient prediction regions when environments undergo structural shifts. To address this, we propose \textbf{AdaptNC}, a framework for the joint online adaptation of both the nonconformity score parameters and the conformal threshold. AdaptNC leverages an adaptive reweighting scheme to optimize score functions, and introduces a replay buffer mechanism to mitigate the coverage instability that occurs during score transitions. We evaluate AdaptNC on diverse robotic benchmarks involving multi-agent policy changes, environmental changes and sensor degradation. Our results demonstrate that AdaptNC significantly reduces prediction region volume compared to state-of-the-art threshold-only baselines while maintaining target coverage levels.

</details>


### [972] [From Perception to Action: Spatial AI Agents and World Models](https://arxiv.org/abs/2602.01644)
*Gloria Felicia,Nolan Bryant,Handi Putra,Ayaan Gazali,Eliel Lobo,Esteban Rojas*

Main category: cs.LG

Relevance: 75.0

TL;DR: 该论文提出了一个统一的三轴分类法，将智能体能力与空间任务连接起来，强调空间智能对于具身智能体的重要性，并识别了三个关键发现和六大挑战。


<details>
  <summary>Details</summary>
Motivation: 现有研究要么单独关注智能体架构，要么单独关注空间领域，缺乏连接这两种互补能力的统一框架。大语言模型在符号领域的成功不能直接转化到物理世界，空间智能（感知3D结构、推理物体关系、在物理约束下行动）对于具身智能体至关重要。

Method: 通过对2000多篇论文的系统性综述（引用742篇顶级会议论文），提出了一个统一的三轴分类法：能力轴（智能体能力）、任务轴（空间任务）和尺度轴（空间尺度）。区分了空间基础（对几何和物理的度量理解）与符号基础（图像与文本关联）。

Result: 发现了三个关键发现：1）分层记忆系统对长时程空间任务很重要；2）GNN-LLM集成是结构化空间推理的有前景方法；3）世界模型对于跨微观到宏观空间尺度的安全部署至关重要。

Conclusion: 该分类法为统一碎片化的研究努力提供了基础，能够推动机器人、自动驾驶和地理空间智能等领域下一代空间感知自主系统的发展。提出了六大挑战和未来研究方向，包括需要统一评估框架来标准化跨领域评估。

Abstract: While large language models have become the prevailing approach for agentic reasoning and planning, their success in symbolic domains does not readily translate to the physical world. Spatial intelligence, the ability to perceive 3D structure, reason about object relationships, and act under physical constraints, is an orthogonal capability that proves important for embodied agents. Existing surveys address either agentic architectures or spatial domains in isolation. None provide a unified framework connecting these complementary capabilities. This paper bridges that gap. Through a thorough review of over 2,000 papers, citing 742 works from top-tier venues, we introduce a unified three-axis taxonomy connecting agentic capabilities with spatial tasks across scales. Crucially, we distinguish spatial grounding (metric understanding of geometry and physics) from symbolic grounding (associating images with text), arguing that perception alone does not confer agency. Our analysis reveals three key findings mapped to these axes: (1) hierarchical memory systems (Capability axis) are important for long-horizon spatial tasks. (2) GNN-LLM integration (Task axis) is a promising approach for structured spatial reasoning. (3) World models (Scale axis) are essential for safe deployment across micro-to-macro spatial scales. We conclude by identifying six grand challenges and outlining directions for future research, including the need for unified evaluation frameworks to standardize cross-domain assessment. This taxonomy provides a foundation for unifying fragmented research efforts and enabling the next generation of spatially-aware autonomous systems in robotics, autonomous vehicles, and geospatial intelligence.

</details>


### [973] [Quantifying Epistemic Predictive Uncertainty in Conformal Prediction](https://arxiv.org/abs/2602.01667)
*Siu Lun Chau,Soroush H. Zargarbashi,Yusuf Sale,Michele Caprio*

Main category: cs.LG

Relevance: 75.0

TL;DR: 该论文研究如何在共形预测框架下量化认知预测不确定性，提出基于最大平均不精确度的计算高效方法，并通过实验证明比传统置信区间大小提供更细粒度的不确定性评估。


<details>
  <summary>Details</summary>
Motivation: 传统共形预测主要关注预测置信区间的大小，但未能充分量化认知不确定性（即由于存在多个合理预测模型而产生的不确定性）。需要一种原则性的方法来测量共形预测中隐含的模型多样性。

Method: 1) 建立共形预测与信度集（credal set）的数学联系，证明预测区域与信度集中所有分布的概率约束等价；2) 提出基于最大平均不精确度（Maximum Mean Imprecision）的计算高效不确定性度量，量化信度集内的信息冲突程度。

Result: 在主动学习和选择性分类任务上的实验表明，所提出的认知不确定性量化方法比仅依赖置信区间大小提供更丰富、更细粒度的不确定性评估，能更好地区分不同不确定性情况。

Conclusion: 该工作揭示了共形预测作为认知不确定性下决策制定的原则性基础潜力，提出的不确定性度量方法计算高效且分析可处理，为LLM等模型的不确定性量化提供了新工具。

Abstract: We study the problem of quantifying epistemic predictive uncertainty (EPU) -- that is, uncertainty faced at prediction time due to the existence of multiple plausible predictive models -- within the framework of conformal prediction (CP). To expose the implicit model multiplicity underlying CP, we build on recent results showing that, under a mild assumption, any full CP procedure induces a set of closed and convex predictive distributions, commonly referred to as a credal set. Importantly, the conformal prediction region (CPR) coincides exactly with the set of labels to which all distributions in the induced credal set assign probability at least $1-α$. As our first contribution, we prove that this characterisation also holds in split CP. Building on this connection, we then propose a computationally efficient and analytically tractable uncertainty measure, based on \emph{Maximum Mean Imprecision}, to quantify the EPU by measuring the degree of conflicting information within the induced credal set. Experiments on active learning and selective classification demonstrate that the quantified EPU provides substantially more informative and fine-grained uncertainty assessments than reliance on CPR size alone. More broadly, this work highlights the potential of CP serving as a principled basis for decision-making under epistemic uncertainty.

</details>


### [974] [Zero-Shot Off-Policy Learning](https://arxiv.org/abs/2602.01962)
*Arip Asadulaev,Maksim Bobrin,Salem Lahlou,Dmitry Dylov,Fakhri Karray,Martin Takac*

Main category: cs.LG

Relevance: 75.0

TL;DR: 论文提出一种零样本强化学习方法，通过建立后继度量与平稳密度比的理论联系，实现无需额外训练即可适应新任务的离策略学习。


<details>
  <summary>Details</summary>
Motivation: 解决离策略学习中的分布偏移和价值函数高估偏差问题，特别是在零样本强化学习场景下，智能体需要在测试时直接适应新任务而无需额外训练。

Method: 建立后继度量与平稳密度比的理论联系，利用该洞察推导最优重要性采样比，实现平稳分布校正，可在任何任务上即时计算最优策略。

Result: 在SMPL Humanoid运动跟踪、ExoRL连续控制和长时程OGBench任务上进行了基准测试，方法能够无缝集成到前向-后向表示框架中，实现快速任务适应。

Conclusion: 该工作架起了离策略学习和零样本适应之间的桥梁，为两个研究领域都带来了益处，特别是在无需训练的快速适应方面表现出色。

Abstract: Off-policy learning methods seek to derive an optimal policy directly from a fixed dataset of prior interactions. This objective presents significant challenges, primarily due to the inherent distributional shift and value function overestimation bias. These issues become even more noticeable in zero-shot reinforcement learning, where an agent trained on reward-free data must adapt to new tasks at test time without additional training. In this work, we address the off-policy problem in a zero-shot setting by discovering a theoretical connection of successor measures to stationary density ratios. Using this insight, our algorithm can infer optimal importance sampling ratios, effectively performing a stationary distribution correction with an optimal policy for any task on the fly. We benchmark our method in motion tracking tasks on SMPL Humanoid, continuous control on ExoRL, and for the long-horizon OGBench tasks. Our technique seamlessly integrates into forward-backward representation frameworks and enables fast-adaptation to new tasks in a training-free regime. More broadly, this work bridges off-policy learning and zero-shot adaptation, offering benefits to both research areas.

</details>


### [975] [An Empirical Study of World Model Quantization](https://arxiv.org/abs/2602.02110)
*Zhongqian Fu,Tianyi Zhao,Kai Han,Hang Zhou,Xinghao Chen,Yunhe Wang*

Main category: cs.LG

Relevance: 75.0

TL;DR: 对世界模型（以DINO-WM为例）进行系统性的后训练量化实证研究，发现量化对世界模型的影响超出常规精度-比特权衡，包括分组权重量化能稳定低比特推演、激活量化粒度效果不一致、编码器和预测器模块的量化敏感性高度不对称等独特现象。


<details>
  <summary>Details</summary>
Motivation: 世界模型学习环境动态的内部表示，使智能体能够在紧凑的潜在空间中模拟和推理未来状态，用于规划、预测和推理等任务。然而，运行世界模型依赖大量计算成本和内存占用，使得模型量化对于高效部署至关重要。目前，后训练量化对世界模型的影响在很大程度上尚未得到研究。

Method: 使用DINO-WM作为代表性案例，对世界模型量化进行系统性实证研究。评估了多种后训练量化方法，包括仅权重量化和联合权重-激活量化。在不同视觉规划任务上进行了广泛实验，涵盖了广泛的比特宽度、量化粒度和高达50次迭代的规划视野。

Result: 研究发现：1) 分组权重量化可以稳定低比特推演；2) 激活量化粒度产生不一致的收益；3) 编码器和预测器模块的量化敏感性高度不对称；4) 激进的低比特量化显著降低了规划目标与任务成功之间的对齐性，导致无法通过额外优化修复的失败。这些发现揭示了基于世界模型的规划中独特的量化诱导失败模式。

Conclusion: 世界模型量化具有超出标准精度-比特权衡的独特影响，为在严格计算约束下部署量化世界模型提供了实用指导。量化敏感性在编码器和预测器模块之间的不对称性以及低比特量化导致的规划目标对齐性下降是需要特别关注的问题。

Abstract: World models learn an internal representation of environment dynamics, enabling agents to simulate and reason about future states within a compact latent space for tasks such as planning, prediction, and inference. However, running world models rely on hevay computational cost and memory footprint, making model quantization essential for efficient deployment. To date, the effects of post-training quantization (PTQ) on world models remain largely unexamined. In this work, we present a systematic empirical study of world model quantization using DINO-WM as a representative case, evaluating diverse PTQ methods under both weight-only and joint weight-activation settings. We conduct extensive experiments on different visual planning tasks across a wide range of bit-widths, quantization granularities, and planning horizons up to 50 iterations. Our results show that quantization effects in world models extend beyond standard accuracy and bit-width trade-offs: group-wise weight quantization can stabilize low-bit rollouts, activation quantization granularity yields inconsistent benefits, and quantization sensitivity is highly asymmetric between encoder and predictor modules. Moreover, aggressive low-bit quantization significantly degrades the alignment between the planning objective and task success, leading to failures that cannot be remedied by additional optimization. These findings reveal distinct quantization-induced failure modes in world model-based planning and provide practical guidance for deploying quantized world models under strict computational constraints. The code will be available at https://github.com/huawei-noah/noah-research/tree/master/QuantWM.

</details>


### [976] [Segment to Focus: Guiding Latent Action Models in the Presence of Distractors](https://arxiv.org/abs/2602.02259)
*Hamza Adnan,Matthew T. Jackson,Alexey Zakharov*

Main category: cs.LG

Relevance: 75.0

TL;DR: MaskLAM通过引入视觉智能体分割来改进潜在动作模型，有效过滤动作相关噪声，在连续控制任务中实现4倍奖励提升和3倍潜在动作质量改进。


<details>
  <summary>Details</summary>
Motivation: 潜在动作模型(LAMs)能够从原始观察中学习提取动作相关表示，实现从未标记视频中进行强化学习。但LAMs面临关键挑战：难以将动作相关特征与动作相关噪声（如背景运动）解耦。未能过滤这些干扰会导致LAMs捕捉虚假相关性并构建次优的潜在动作空间。

Method: MaskLAM是对LAM训练的轻量级修改，通过整合视觉智能体分割来缓解此问题。该方法利用预训练基础模型的分割掩码来加权LAM重建损失，从而优先考虑显著信息而非背景元素，且无需架构修改。

Result: 在添加了动作相关背景噪声的连续控制MuJoCo任务上，MaskLAM相比标准基线实现了高达4倍的累积奖励提升，并通过线性探针评估显示潜在动作质量有3倍改进。

Conclusion: MaskLAM通过简单而有效的方法解决了LAMs中的噪声过滤问题，显著提升了强化学习从未标记视频中学习的性能，为可扩展的强化学习数据利用提供了新思路。

Abstract: Latent Action Models (LAMs) learn to extract action-relevant representations solely from raw observations, enabling reinforcement learning from unlabelled videos and significantly scaling available training data. However, LAMs face a critical challenge in disentangling action-relevant features from action-correlated noise (e.g., background motion). Failing to filter these distractors causes LAMs to capture spurious correlations and build sub-optimal latent action spaces. In this paper, we introduce MaskLAM -- a lightweight modification to LAM training to mitigate this issue by incorporating visual agent segmentation. MaskLAM utilises segmentation masks from pretrained foundation models to weight the LAM reconstruction loss, thereby prioritising salient information over background elements while requiring no architectural modifications. We demonstrate the effectiveness of our method on continuous-control MuJoCo tasks, modified with action-correlated background noise. Our approach yields up to a 4x increase in accrued rewards compared to standard baselines and a 3x improvement in the latent action quality, as evidenced by linear probe evaluation.

</details>


### [977] [DASH: Faster Shampoo via Batched Block Preconditioning and Efficient Inverse-Root Solvers](https://arxiv.org/abs/2602.02016)
*Ionut-Vlad Modoranu,Philip Zmushko,Erik Schultheis,Mher Safaryan,Dan Alistarh*

Main category: cs.LG

Relevance: 75.0

TL;DR: DASH提出了一种分布式加速的Shampoo优化器实现，通过3D张量堆叠和新的矩阵逆根计算方法，显著提升了GPU利用率，实现了4.83倍的速度提升。


<details>
  <summary>Details</summary>
Motivation: Shampoo作为领先的近似二阶优化器，在MLCommons AlgoPerf竞赛中表现出色，能产生更易压缩的模型，但当前实现存在显著的计算开销问题，限制了其实际应用。

Method: 1) 将预处理器块堆叠成3D张量以提升GPU利用率；2) 提出Newton-DB迭代和切比雪夫多项式逼近作为计算矩阵逆根的新方法；3) 深入分析矩阵缩放对Shampoo收敛的影响。

Result: GPU感知实现相比优化后的分布式Shampoo实现了高达4.83倍的优化器步骤加速，Newton-DB在所有测试方法中获得了每迭代最低的验证困惑度。

Conclusion: DASH通过算法创新和GPU优化实现，显著提升了Shampoo优化器的实用性，使其在保持优异收敛性能的同时大幅降低了计算成本。

Abstract: Shampoo is one of the leading approximate second-order optimizers: a variant of it has won the MLCommons AlgoPerf competition, and it has been shown to produce models with lower activation outliers that are easier to compress. Yet, applying Shampoo currently comes at the cost of significant computational slowdown, due to its expensive internal operations. In this paper, we take a significant step to address this shortcoming by proposing \method (for \textbf{D}istributed \textbf{A}ccelerated \textbf{SH}ampoo), a faster implementation of Distributed Shampoo based on two main new techniques: First, we show that preconditioner blocks can be stacked into 3D tensors to significantly improve GPU utilization; second, we introduce the Newton-DB iteration and the Chebyshev polynomial approximations as novel and faster approaches for computing the inverse matrix roots required by Shampoo. Along with these algorithmic contributions, we provide a first in-depth analysis of how matrix scaling critically affects Shampoo convergence. On the practical side, our GPU-aware implementation achieves up to $4.83\times$ faster optimizer steps compared to the well-optimized Distributed Shampoo, while Newton-DB attains the lowest validation perplexity per iteration among all tested methods. Our code is available at https://github.com/IST-DASLab/DASH.

</details>


### [978] [FORLER: Federated Offline Reinforcement Learning with Q-Ensemble and Actor Rectification](https://arxiv.org/abs/2602.02055)
*Nan Qiao,Sheng Yue*

Main category: cs.LG

Relevance: 75.0

TL;DR: FORLER：一种离线联邦强化学习方法，通过服务器端的Q-ensemble聚合和设备端的actor rectification，解决异构低质量数据下的策略污染问题，提供安全策略改进保证。


<details>
  <summary>Details</summary>
Motivation: 物联网系统中，在线联邦强化学习存在风险和高成本，而离线联邦强化学习在低质量异构数据下容易陷入局部最优，产生策略污染问题——一个设备的次优策略会降低聚合模型质量。

Method: 1. 服务器端：Q-ensemble聚合，鲁棒地合并设备Q函数以控制策略污染，将计算负担从资源受限设备转移到服务器；2. 设备端：actor rectification，通过零阶搜索寻找高Q值动作，并使用定制正则器将策略推向这些动作；3. δ-周期性策略进一步减少本地计算。

Result: FORLER在不同数据质量和异构性条件下始终优于强基线方法，理论上提供了安全策略改进的性能保证。

Conclusion: FORLER有效解决了离线联邦强化学习中的策略污染问题，通过创新的服务器聚合和设备优化机制，在保持隐私的同时实现了高效可靠的策略学习。

Abstract: In Internet-of-Things systems, federated learning has advanced online reinforcement learning (RL) by enabling parallel policy training without sharing raw data. However, interacting with real environments online can be risky and costly, motivating offline federated RL (FRL), where local devices learn from fixed datasets. Despite its promise, offline FRL may break down under low-quality, heterogeneous data. Offline RL tends to get stuck in local optima, and in FRL, one device's suboptimal policy can degrade the aggregated model, i.e., policy pollution. We present FORLER, combining Q-ensemble aggregation on the server with actor rectification on devices. The server robustly merges device Q-functions to curb policy pollution and shift heavy computation off resource-constrained hardware without compromising privacy. Locally, actor rectification enriches policy gradients via a zeroth-order search for high-Q actions plus a bespoke regularizer that nudges the policy toward them. A $δ$-periodic strategy further reduces local computation. We theoretically provide safe policy improvement performance guarantees. Extensive experiments show FORLER consistently outperforms strong baselines under varying data quality and heterogeneity.

</details>


### [979] [Interpretable Tabular Foundation Models via In-Context Kernel Regression](https://arxiv.org/abs/2602.02162)
*Ratmir Miftachov,Bruno Charron,Simon Valentin*

Main category: cs.LG

Relevance: 75.0

TL;DR: KernelICL框架通过将表格基础模型的最终预测层替换为核函数，实现了可量化的基于样本的可解释性，在保持性能的同时提供透明预测。


<details>
  <summary>Details</summary>
Motivation: 现有表格基础模型（如TabPFN、TabICL）虽然通过上下文学习取得了最先进的性能，但其架构本质上是不透明的，缺乏可解释性。研究者希望增强这些模型的可解释性，同时保持其性能优势。

Method: 基于上下文学习类似于核回归的洞察，将最终预测层替换为核函数（高斯核、点积核、kNN核），使每个预测都成为训练标签的透明加权平均。提出了一个二维分类法，将标准核方法、现代基于邻居的方法和注意力机制统一在一个框架下，并通过训练样本权重分布的困惑度来量化可检查性。

Result: 在55个TALENT基准数据集上，KernelICL实现了与现有表格基础模型相当的性能，表明对最终层施加显式核约束可以在不牺牲性能的情况下实现可检查的预测。

Conclusion: 通过将核机制显式化，KernelICL框架为表格基础模型提供了可量化的基于样本的可解释性，在保持最先进性能的同时实现了透明预测，统一了多种预测方法。

Abstract: Tabular foundation models like TabPFN and TabICL achieve state-of-the-art performance through in-context learning, yet their architectures remain fundamentally opaque. We introduce KernelICL, a framework to enhance tabular foundation models with quantifiable sample-based interpretability. Building on the insight that in-context learning is akin to kernel regression, we make this mechanism explicit by replacing the final prediction layer with kernel functions (Gaussian, dot-product, kNN) so that every prediction is a transparent weighted average of training labels. We introduce a two-dimensional taxonomy that formally unifies standard kernel methods, modern neighbor-based approaches, and attention mechanisms under a single framework, and quantify inspectability via the perplexity of the weight distribution over training samples. On 55 TALENT benchmark datasets, KernelICL achieves performance on par with existing tabular foundation models, demonstrating that explicit kernel constraints on the final layer enable inspectable predictions without sacrificing performance.

</details>


### [980] [Co-RedTeam: Orchestrated Security Discovery and Exploitation with LLM Agents](https://arxiv.org/abs/2602.02164)
*Pengfei He,Ash Fox,Lesly Miculicich,Stefan Friedli,Daniel Fabian,Burak Gokturk,Jiliang Tang,Chen-Yu Lee,Tomas Pfister,Long T. Le*

Main category: cs.LG

Relevance: 75.0

TL;DR: Co-RedTeam是一个面向网络安全的多智能体框架，通过集成安全领域知识、代码感知分析、执行基础迭代推理和长期记忆，模拟真实红队工作流程，显著提升漏洞发现和利用能力。


<details>
  <summary>Details</summary>
Motivation: 现有LLM在网络安全任务中存在局限性：交互有限、执行基础薄弱、缺乏经验复用，难以实现自动化的漏洞发现和利用。需要设计一个能模拟真实红队工作流程的系统。

Method: 提出安全感知的多智能体框架Co-RedTeam，将漏洞分析分解为协调的发现和利用阶段。智能体基于真实执行反馈进行规划、执行、验证和优化，并学习先前轨迹，集成安全领域知识、代码感知分析和长期记忆。

Result: 在具有挑战性的安全基准测试中，Co-RedTeam始终优于强基线，在漏洞利用方面达到超过60%的成功率，在漏洞检测方面获得超过10%的绝对提升。消融和迭代研究证实了执行反馈、结构化交互和记忆的关键作用。

Conclusion: Co-RedTeam通过执行基础迭代推理、结构化交互和长期记忆，构建了强大且可泛化的网络安全智能体，为LLM在网络安全领域的应用提供了有效框架。

Abstract: Large language models (LLMs) have shown promise in assisting cybersecurity tasks, yet existing approaches struggle with automatic vulnerability discovery and exploitation due to limited interaction, weak execution grounding, and a lack of experience reuse. We propose Co-RedTeam, a security-aware multi-agent framework designed to mirror real-world red-teaming workflows by integrating security-domain knowledge, code-aware analysis, execution-grounded iterative reasoning, and long-term memory. Co-RedTeam decomposes vulnerability analysis into coordinated discovery and exploitation stages, enabling agents to plan, execute, validate, and refine actions based on real execution feedback while learning from prior trajectories. Extensive evaluations on challenging security benchmarks demonstrate that Co-RedTeam consistently outperforms strong baselines across diverse backbone models, achieving over 60% success rate in vulnerability exploitation and over 10% absolute improvement in vulnerability detection. Ablation and iteration studies further confirm the critical role of execution feedback, structured interaction, and memory for building robust and generalizable cybersecurity agents.

</details>


### [981] [Spectral Superposition: A Theory of Feature Geometry](https://arxiv.org/abs/2602.02224)
*Georgi Ivanov,Narmeen Oozeer,Shivam Raval,Tasana Pejovic,Shriyash Upadhyay,Amir Abdullah*

Main category: cs.LG

Relevance: 75.0

TL;DR: 该论文提出了一种基于谱分析（特征值、特征空间等）的理论框架来研究神经网络特征几何结构，通过引入框架算子F=WW⊤来分析特征在特征空间中的范数分配，揭示了容量饱和导致特征谱定位的现象。


<details>
  <summary>Details</summary>
Motivation: 当前方法将激活分解为稀疏线性特征但丢弃了几何结构。神经网络通过叠加表示比维度更多的特征，迫使特征共享表示空间。需要发展理论来研究特征的几何结构。

Method: 开发基于谱分析的理论框架，分析权重导出矩阵的谱（特征值、特征空间等）。引入框架算子F=WW⊤，获得描述每个特征在特征空间中范数分配的谱测度。在叠加的玩具模型中应用该理论。

Result: 证明容量饱和迫使谱定位：特征坍缩到单个特征空间，组织成紧框架，并通过关联方案进行离散分类，分类了先前工作中的所有几何结构（单纯形、多边形、反棱柱）。谱测度形式适用于任意权重矩阵。

Conclusion: 这些结果指向一个更广泛的研究计划：应用算子理论进行可解释性分析。谱方法能够捕捉全局几何结构（所有特征如何相互作用），而先前工具只能描述特征间的成对相互作用。

Abstract: Neural networks represent more features than they have dimensions via superposition, forcing features to share representational space. Current methods decompose activations into sparse linear features but discard geometric structure. We develop a theory for studying the geometric structre of features by analyzing the spectra (eigenvalues, eigenspaces, etc.) of weight derived matrices. In particular, we introduce the frame operator $F = WW^\top$, which gives us a spectral measure that describes how each feature allocates norm across eigenspaces. While previous tools could describe the pairwise interactions between features, spectral methods capture the global geometry (``how do all features interact?''). In toy models of superposition, we use this theory to prove that capacity saturation forces spectral localization: features collapse onto single eigenspaces, organize into tight frames, and admit discrete classification via association schemes, classifying all geometries from prior work (simplices, polygons, antiprisms). The spectral measure formalism applies to arbitrary weight matrices, enabling diagnosis of feature localization beyond toy settings. These results point toward a broader program: applying operator theory to interpretability.

</details>


### [982] [Prediction-Powered Risk Monitoring of Deployed Models for Detecting Harmful Distribution Shifts](https://arxiv.org/abs/2602.02229)
*Guangyi Zhang,Yunlong Cai,Guanding Yu,Osvaldo Simeone*

Main category: cs.LG

Relevance: 75.0

TL;DR: 提出PPRM方法，利用预测标签和少量真实标签构建运行风险的下界，用于动态环境中模型性能监控，具有无假设有限样本保证


<details>
  <summary>Details</summary>
Motivation: 解决动态环境中标记数据有限时的模型性能监控问题，传统方法需要大量标记数据，而实际应用中标记数据获取成本高

Method: 基于预测驱动推理(PPI)的半监督风险监控方法，结合合成标签和少量真实标签构建运行风险的随时有效下界，通过阈值比较检测有害偏移

Result: 在图像分类、大语言模型和电信监控任务上验证了PPRM的有效性，能够准确检测模型性能下降，满足误报概率的无假设有限样本保证

Conclusion: PPRM为动态环境中模型性能监控提供了一种高效可靠的解决方案，特别适用于标记数据有限的场景

Abstract: We study the problem of monitoring model performance in dynamic environments where labeled data are limited. To this end, we propose prediction-powered risk monitoring (PPRM), a semi-supervised risk-monitoring approach based on prediction-powered inference (PPI). PPRM constructs anytime-valid lower bounds on the running risk by combining synthetic labels with a small set of true labels. Harmful shifts are detected via a threshold-based comparison with an upper bound on the nominal risk, satisfying assumption-free finite-sample guarantees in the probability of false alarm. We demonstrate the effectiveness of PPRM through extensive experiments on image classification, large language model (LLM), and telecommunications monitoring tasks.

</details>


### [983] [Learning Markov Decision Processes under Fully Bandit Feedback](https://arxiv.org/abs/2602.02260)
*Zhengjia Zhuo,Anupam Gupta,Viswanath Nagarajan*

Main category: cs.LG

Relevance: 75.0

TL;DR: 本文提出了首个针对完全bandit反馈的episodic MDP的高效学习算法，实现了$\widetilde{O}(\sqrt{T})$的遗憾界，并在k-item先知不等式等经典随机优化问题上展示了与完整状态-动作反馈算法相当的性能。


<details>
  <summary>Details</summary>
Motivation: 传统RL假设智能体能够观察到每个访问的状态-动作对和即时奖励，但实际应用中这种详细反馈往往不现实。本文研究更受限的"完全bandit"反馈模型，智能体只能观察到聚合奖励，无法看到访问的状态-动作对，这在许多实际应用场景中更为合理。

Method: 设计了首个针对episodic MDP的完全bandit反馈学习算法。算法通过精心设计的探索策略和奖励估计机制，在仅能观察到聚合奖励的情况下学习最优策略。特别考虑了有序MDP的特殊结构，以处理k-item先知不等式和序列定价等经典随机优化问题。

Result: 算法实现了$\widetilde{O}(\sqrt{T})$的遗憾界，其中对horizon长度H有指数依赖（证明是必要的）。对于有序MDP获得了改进的近乎紧的遗憾界。在k-item先知不等式任务上的实验表明，尽管反馈受限，算法性能与使用完整状态-动作反馈的UCB-VI算法相当。

Conclusion: 本文证明了即使在极度受限的完全bandit反馈下，仍能设计出高效的RL算法，为实际应用中反馈受限的场景提供了理论基础和实用算法。特别在随机优化问题上的成功应用展示了该方法的实用价值。

Abstract: A standard assumption in Reinforcement Learning is that the agent observes every visited state-action pair in the associated Markov Decision Process (MDP), along with the per-step rewards. Strong theoretical results are known in this setting, achieving nearly-tight $Θ(\sqrt{T})$-regret bounds. However, such detailed feedback can be unrealistic, and recent research has investigated more restricted settings such as trajectory feedback, where the agent observes all the visited state-action pairs, but only a single \emph{aggregate} reward. In this paper, we consider a far more restrictive ``fully bandit'' feedback model for episodic MDPs, where the agent does not even observe the visited state-action pairs -- it only learns the aggregate reward. We provide the first efficient bandit learning algorithm for episodic MDPs with $\widetilde{O}(\sqrt{T})$ regret. Our regret has an exponential dependence on the horizon length $\H$, which we show is necessary. We also obtain improved nearly-tight regret bounds for ``ordered'' MDPs; these can be used to model classical stochastic optimization problems such as $k$-item prophet inequality and sequential posted pricing. Finally, we evaluate the empirical performance of our algorithm for the setting of $k$-item prophet inequalities; despite the highly restricted feedback, our algorithm's performance is comparable to that of a state-of-art learning algorithm (UCB-VI) with detailed state-action feedback.

</details>


### [984] [Trust Region Continual Learning as an Implicit Meta-Learner](https://arxiv.org/abs/2602.02417)
*Zekun Wang,Anant Gupta,Christopher J. MacLellan*

Main category: cs.LG

Relevance: 75.0

TL;DR: 提出一种结合生成式回放与Fisher度量信任区域约束的持续学习方法，该方法在局部近似下具有MAML风格的单步内层更新解释，展现出持续学习中的元学习特性。


<details>
  <summary>Details</summary>
Motivation: 传统持续学习方法面临核心权衡：基于正则化的方法（如EWC）在任务最优解重叠度低时可能过度约束更新，而基于回放的方法虽然能保持性能但会因不完美的回放而漂移。需要一种混合方法来解决这些问题。

Method: 提出信任区域持续学习方法，结合生成式回放与Fisher度量信任区域约束。在局部近似下，该方法可解释为具有单步内层更新的MAML风格算法：回放提供旧任务梯度信号（类似查询），Fisher加权惩罚提供高效的离线曲率整形（类似支持）。

Result: 在任务增量扩散图像生成和持续扩散策略控制任务上，信任区域持续学习方法取得了最佳最终性能和保留率，并且比EWC、回放和持续元学习基线更快地恢复早期任务性能。

Conclusion: 该方法在持续学习中展现出元学习特性：模型成为一个初始化点，在每次任务转换后能快速重新收敛到先前任务的最优解，而无需显式优化双层目标。这为持续学习提供了一种新的混合视角。

Abstract: Continual learning aims to acquire tasks sequentially without catastrophic forgetting, yet standard strategies face a core tradeoff: regularization-based methods (e.g., EWC) can overconstrain updates when task optima are weakly overlapping, while replay-based methods can retain performance but drift due to imperfect replay. We study a hybrid perspective: \emph{trust region continual learning} that combines generative replay with a Fisher-metric trust region constraint. We show that, under local approximations, the resulting update admits a MAML-style interpretation with a single implicit inner step: replay supplies an old-task gradient signal (query-like), while the Fisher-weighted penalty provides an efficient offline curvature shaping (support-like). This yields an emergent meta-learning property in continual learning: the model becomes an initialization that rapidly \emph{re-converges} to prior task optima after each task transition, without explicitly optimizing a bilevel objective. Empirically, on task-incremental diffusion image generation and continual diffusion-policy control, trust region continual learning achieves the best final performance and retention, and consistently recovers early-task performance faster than EWC, replay, and continual meta-learning baselines.

</details>


### [985] [Uncertainty-Aware Multimodal Learning via Conformal Shapley Intervals](https://arxiv.org/abs/2602.00171)
*Mathew Chandy,Michael Johnson,Judong Shen,Devan V. Mehrotra,Hua Zhou,Jin Zhou,Xiaowu Dai*

Main category: stat.ML

Relevance: 75.0

TL;DR: 提出conformal Shapley intervals框架，结合Shapley值和conformal inference为多模态学习中的每个模态构建不确定性感知的重要性区间，并提供具有理论保证的模态选择方法。


<details>
  <summary>Details</summary>
Motivation: 多模态学习中不同模态的贡献通常不均衡且数据依赖，难以确定哪些模态真正具有信息量以及其贡献的可信度。量化模态级重要性及其不确定性对于可解释和可靠的多模态学习至关重要。

Method: 结合Shapley值和conformal inference构建conformal Shapley intervals，为每个模态提供不确定性感知的重要性区间。基于这些区间提出模态选择程序，具有理论最优性保证：在给定观测特征条件下，所选模态子集的性能接近最优子集。

Result: 在多个数据集上验证了方法的有效性，能够提供有意义的不确定性量化，同时仅依赖少量信息丰富的模态就能获得强大的预测性能。

Conclusion: 提出的框架为多模态学习提供了可解释和可靠的重要性评估方法，通过不确定性感知的模态选择实现了性能与效率的平衡。

Abstract: Multimodal learning combines information from multiple data modalities to improve predictive performance. However, modalities often contribute unequally and in a data dependent way, making it unclear which data modalities are genuinely informative and to what extent their contributions can be trusted. Quantifying modality level importance together with uncertainty is therefore central to interpretable and reliable multimodal learning. We introduce conformal Shapley intervals, a framework that combines Shapley values with conformal inference to construct uncertainty-aware importance intervals for each modality. Building on these intervals, we propose a modality selection procedure with a provable optimality guarantee: conditional on the observed features, the selected subset of modalities achieves performance close to that of the optimal subset. We demonstrate the effectiveness of our approach on multiple datasets, showing that it provides meaningful uncertainty quantification and strong predictive performance while relying on only a small number of informative modalities.

</details>


### [986] [Emergence of Distortions in High-Dimensional Guided Diffusion Models](https://arxiv.org/abs/2602.00716)
*Enrico Ventura,Beatrice Achilli,Luca Ambrogioni,Carlo Lucibello*

Main category: stat.ML

Relevance: 75.0

TL;DR: 论文分析了扩散模型中分类器无关引导（CFG）导致生成样本多样性损失的问题，从统计物理角度形式化了生成失真现象，揭示了失真发生的相变条件，并提出了一种负引导窗口的改进方案。


<details>
  <summary>Details</summary>
Motivation: 分类器无关引导（CFG）是扩散模型中条件采样的标准方法，但常常导致生成样本多样性损失。作者旨在从理论上形式化这一现象，理解失真发生的机制，并提出改进方案。

Method: 1. 将CFG引起的失真形式化为生成失真，定义为CFG诱导的采样分布与真实条件分布之间的不匹配；2. 使用高斯混合模型和精确分数函数；3. 利用统计物理工具分析高维情况下的失真现象；4. 通过动态平均场分析研究失真相变；5. 提出包含负引导窗口的理论指导方案。

Result: 1. 失真现象通过引导动力学的有效势中的相变出现；2. 当模式数量随维度指数增长时失真持续存在，但在次指数情况下消失；3. 标准CFG会平移条件分布的均值并缩小方差；4. 标准CFG调度从根本上无法防止方差缩小；5. 提出的负引导窗口方案能缓解多样性损失同时保持类别可分性。

Conclusion: 论文从理论上揭示了CFG导致生成失真的机制，证明了标准CFG调度在防止方差缩小方面的根本局限性，并提出了一种理论驱动的改进方案，为扩散模型的条件采样提供了更好的理论基础。

Abstract: Classifier-free guidance (CFG) is the de facto standard for conditional sampling in diffusion models, yet it often leads to a loss of diversity in generated samples. We formalize this phenomenon as generative distortion, defined as the mismatch between the CFG-induced sampling distribution and the true conditional distribution. Considering Gaussian mixtures and their exact scores, and leveraging tools from statistical physics, we characterize the onset of distortion in a high-dimensional regime as a function of the number of classes. Our analysis reveals that distortions emerge through a phase transition in the effective potential governing the guided dynamics. In particular, our dynamical mean-field analysis shows that distortion persists when the number of modes grows exponentially with dimension, but vanishes in the sub-exponential regime. Consistent with prior finite-dimensional results, we further demonstrate that vanilla CFG shifts the mean and shrinks the variance of the conditional distribution. We show that standard CFG schedules are fundamentally incapable of preventing variance shrinkage. Finally, we propose a theoretically motivated guidance schedule featuring a negative-guidance window, which mitigates loss of diversity while preserving class separability.

</details>


### [987] [Safety-Efficacy Trade Off: Robustness against Data-Poisoning](https://arxiv.org/abs/2602.00822)
*Diego Granziol*

Main category: stat.ML

Relevance: 75.0

TL;DR: 论文揭示了后门和数据投毒攻击的几何机制：通过核岭回归模型证明，聚类脏标签投毒会在输入Hessian中产生秩一尖峰，而非线性核的"近克隆"机制使攻击在保持高效的同时在谱上不可检测。


<details>
  <summary>Details</summary>
Motivation: 现有谱分析和优化防御方法无法有效检测后门和数据投毒攻击，需要从几何角度理解攻击机制，揭示攻击为何能在保持高效的同时逃避检测。

Method: 使用核岭回归作为宽神经网络的精确模型，理论分析聚类脏标签投毒在输入Hessian中的几何效应；通过输入梯度正则化收缩毒物对齐的Fisher和Hessian特征模；在MNIST、CIFAR-10和CIFAR-100上对线性模型和深度卷积网络进行实验验证。

Result: 证明了攻击成功率与谱可见性之间存在一致滞后；识别出"近克隆"机制使攻击在保持高效的同时谱上不可检测；正则化和数据增强能联合抑制投毒攻击；输入梯度正则化在安全性和有效性之间存在明确权衡。

Conclusion: 揭示了后门攻击何时本质不可见，首次通过输入空间曲率完整描述了投毒、可检测性和防御机制，为理解攻击几何特性和设计更有效防御提供了理论基础。

Abstract: Backdoor and data poisoning attacks can achieve high attack success while evading existing spectral and optimisation based defences. We show that this behaviour is not incidental, but arises from a fundamental geometric mechanism in input space. Using kernel ridge regression as an exact model of wide neural networks, we prove that clustered dirty label poisons induce a rank one spike in the input Hessian whose magnitude scales quadratically with attack efficacy. Crucially, for nonlinear kernels we identify a near clone regime in which poison efficacy remains order one while the induced input curvature vanishes, making the attack provably spectrally undetectable. We further show that input gradient regularisation contracts poison aligned Fisher and Hessian eigenmodes under gradient flow, yielding an explicit and unavoidable safety efficacy trade off by reducing data fitting capacity. For exponential kernels, this defence admits a precise interpretation as an anisotropic high pass filter that increases the effective length scale and suppresses near clone poisons. Extensive experiments on linear models and deep convolutional networks across MNIST and CIFAR 10 and CIFAR 100 validate the theory, demonstrating consistent lags between attack success and spectral visibility, and showing that regularisation and data augmentation jointly suppress poisoning. Our results establish when backdoors are inherently invisible, and provide the first end to end characterisation of poisoning, detectability, and defence through input space curvature.

</details>


### [988] [Sublinear Time Quantum Algorithm for Attention Approximation](https://arxiv.org/abs/2602.00874)
*Zhao Song,Jianfei Xue,Jiahao Zhang,Lichen Zhang*

Main category: quant-ph

Relevance: 75.0

TL;DR: 提出一种量子数据结构，能在亚线性时间内近似注意力矩阵的行，通过量子Nyström近似、量子多元均值估计和量子杠杆得分采样等技术实现。


<details>
  <summary>Details</summary>
Motivation: 传统注意力机制需要显式计算softmax矩阵，导致Ω(n²)的时间复杂度，这在大规模语言模型中成为瓶颈。需要开发更高效的近似方法，而量子计算提供了新的可能性。

Method: 提出量子数据结构，基于量子Nyström近似指数核、量子多元均值估计计算D矩阵、量子杠杆得分采样与V矩阵相乘。预处理时间与n的平方根相关，行查询时间与n无关。

Result: 实现了首个能在亚线性时间内近似注意力矩阵行的量子数据结构，预处理时间Õ(ε⁻¹n⁰·⁵(s_λ²·⁵ + s_λ¹·⁵d + α⁰·⁵d))，行查询时间Õ(s_λ² + s_λd)。

Conclusion: 该工作首次展示了量子计算在加速注意力机制方面的潜力，为大规模语言模型的高效计算提供了新的量子解决方案。

Abstract: Given the query, key and value matrices $Q, K, V\in \mathbb{R}^{n\times d}$, the attention module is defined as $\mathrm{Att}(Q, K, V)=D^{-1}AV$ where $A=\exp(QK^\top/\sqrt{d})$ with $\exp(\cdot)$ applied entrywise, $D=\mathrm{diag}(A{\bf 1}_n)$. The attention module is the backbone of modern transformers and large language models, but explicitly forming the softmax matrix $D^{-1}A$ incurs $Ω(n^2)$ time, motivating numerous approximation schemes that reduce runtime to $\widetilde O(nd)$ via sparsity or low-rank factorization.
  We propose a quantum data structure that approximates any row of $\mathrm{Att}(Q, K, V)$ using only row queries to $Q, K, V$. Our algorithm preprocesses these matrices in $\widetilde{O}\left( ε^{-1} n^{0.5} \left( s_λ^{2.5} + s_λ^{1.5} d + α^{0.5} d \right) \right)$ time, where $ε$ is the target accuracy, $s_λ$ is the $λ$-statistical dimension of the exponential kernel defined by $Q$ and $K$, and $α$ measures the row distortion of $V$ that is at most $d/{\rm srank}(V)$, the stable rank of $V$. Each row query can be answered in $\widetilde{O}(s_λ^2 + s_λd)$ time.
  To our knowledge, this is the first quantum data structure that approximates rows of the attention matrix in sublinear time with respect to $n$. Our approach relies on a quantum Nyström approximation of the exponential kernel, quantum multivariate mean estimation for computing $D$, and quantum leverage score sampling for the multiplication with $V$.

</details>


### [989] [Rethinking Multinomial Logistic Mixture of Experts with Sigmoid Gating Function](https://arxiv.org/abs/2602.01466)
*Tuan Minh Pham,Thinh Cao,Viet Nguyen,Huy Nguyen,Nhat Ho,Alessandro Rinaldo*

Main category: stat.ML

Relevance: 75.0

TL;DR: 本文对使用改进sigmoid门的混合专家模型进行了理论分析，证明了sigmoid门在分类任务中比softmax门具有更低的样本复杂度，并提出了使用欧几里得评分来避免温度参数导致的指数级样本复杂度问题。


<details>
  <summary>Details</summary>
Motivation: 尽管sigmoid门在MoE模型中已被经验证明优于softmax门，但存在三个未解决的问题：1) 在分类任务中的优势未得到理论证明；2) 现有sigmoid门MoE可能无法收敛到真实模型；3) 温度参数的影响缺乏理论分析。

Method: 使用改进的sigmoid门确保模型收敛，对多项式逻辑回归MoE进行全面理论分析。提出用欧几里得评分替换传统内积评分，以消除温度参数与门控参数之间的不良交互。

Result: sigmoid门在参数和专家估计方面都比softmax门具有更低的样本复杂度。但传统sigmoid门中的温度参数会导致指数级样本复杂度，而使用欧几里得评分可将复杂度降低到多项式级别。

Conclusion: sigmoid门在分类任务中确实优于softmax门，但需要采用欧几里得评分来避免温度参数带来的指数级样本复杂度问题，从而显著提高学习效率。

Abstract: The sigmoid gate in mixture-of-experts (MoE) models has been empirically shown to outperform the softmax gate across several tasks, ranging from approximating feed-forward networks to language modeling. Additionally, recent efforts have demonstrated that the sigmoid gate is provably more sample-efficient than its softmax counterpart under regression settings. Nevertheless, there are three notable concerns that have not been addressed in the literature, namely (i) the benefits of the sigmoid gate have not been established under classification settings; (ii) existing sigmoid-gated MoE models may not converge to their ground-truth; and (iii) the effects of a temperature parameter in the sigmoid gate remain theoretically underexplored. To tackle these open problems, we perform a comprehensive analysis of multinomial logistic MoE equipped with a modified sigmoid gate to ensure model convergence. Our results indicate that the sigmoid gate exhibits a lower sample complexity than the softmax gate for both parameter and expert estimation. Furthermore, we find that incorporating a temperature into the sigmoid gate leads to a sample complexity of exponential order due to an intrinsic interaction between the temperature and gating parameters. To overcome this issue, we propose replacing the vanilla inner product score in the gating function with a Euclidean score that effectively removes that interaction, thereby substantially improving the sample complexity to a polynomial order.

</details>


### [990] [Efficient Softmax Reformulation for Homomorphic Encryption via Moment Generating Function](https://arxiv.org/abs/2602.01621)
*Hanjun Park,Byeong-Seo Min,Jiheon Woo,Min-Wook Jeong,Jongho Shin,Yongwoo Lee,Young-Sik Kim,Yongjune Kim*

Main category: cs.CR

Relevance: 75.0

TL;DR: 提出MGF-softmax，一种基于矩生成函数的softmax重构方法，用于同态加密中的Transformer推理，通过降低乘法深度提高计算效率，同时保持准确性。


<details>
  <summary>Details</summary>
Motivation: 同态加密(HE)在隐私保护机器学习中很重要，但Transformer中的softmax计算特别困难，因为其多变量结构、指数函数的大动态范围以及归一化中的精确除法需求。

Method: 提出MGF-softmax，基于矩生成函数重构softmax，用矩基对应物替换softmax分母，显著降低乘法深度，同时保持softmax关键特性，并随着输入token数量增加渐近收敛到精确softmax。

Result: 在Vision Transformers和大型语言模型上的实验表明，MGF-softmax在加密推理中提供了高效准确的softmax近似，接近高深度精确方法的推理准确度，同时通过降低乘法深度显著减少计算成本。

Conclusion: MGF-softmax是同态加密中Transformer推理的有效解决方案，平衡了计算效率和准确性，为隐私保护机器学习中的softmax计算提供了实用方法。

Abstract: Homomorphic encryption (HE) is a prominent framework for privacy-preserving machine learning, enabling inference directly on encrypted data. However, evaluating softmax, a core component of transformer architectures, remains particularly challenging in HE due to its multivariate structure, the large dynamic range induced by exponential functions, and the need for accurate division during normalization. In this paper, we propose MGF-softmax, a novel softmax reformulation based on the moment generating function (MGF) that replaces the softmax denominator with its moment-based counterpart. This reformulation substantially reduces multiplicative depth while preserving key properties of softmax and asymptotically converging to the exact softmax as the number of input tokens increases. Extensive experiments on Vision Transformers and large language models show that MGF-softmax provides an efficient and accurate approximation of softmax in encrypted inference. In particular, it achieves inference accuracy close to that of high-depth exact methods, while requiring substantially lower computational cost through reduced multiplicative depth.

</details>


### [991] [Cross-Domain Fake News Detection on Unseen Domains via LLM-Based Domain-Aware User Modeling](https://arxiv.org/abs/2602.01726)
*Xuankai Yang,Yan Wang,Jiajie Zhu,Pengfei Ding,Hongyang Liu,Xiuzhen Zhang,Huan Liu*

Main category: cs.SI

Relevance: 75.0

TL;DR: DAUD是一个基于LLM的领域感知框架，用于未见领域的假新闻检测，通过提取新闻内容的高层语义和用户跨领域参与行为，提升知识迁移能力。


<details>
  <summary>Details</summary>
Motivation: 现有跨领域假新闻检测方法在面对未见领域时存在两个关键局限：1) 对新闻和用户参与的高层语义建模不足；2) 未见领域标注数据稀缺。LLMs为此提供了潜力，但如何有效利用仍具挑战。

Method: DAUD框架：1) 使用LLMs提取新闻内容的高层语义；2) 建模用户的单领域和跨领域参与行为，生成领域感知的行为表示；3) 捕捉原始数据驱动特征与LLM衍生特征之间的关系，提取更可靠的领域共享表示。

Result: 在真实世界数据集上的广泛实验表明，DAUD在一般和未见领域的跨领域假新闻检测设置中都优于最先进的基线方法。

Conclusion: DAUD通过有效利用LLMs提取高层语义和建模用户行为，解决了跨领域假新闻检测在未见领域中的关键挑战，实现了更好的知识迁移和检测性能。

Abstract: Cross-domain fake news detection (CD-FND) transfers knowledge from a source domain to a target domain and is crucial for real-world fake news mitigation. This task becomes particularly important yet more challenging when the target domain is previously unseen (e.g., the COVID-19 outbreak or the Russia-Ukraine war). However, existing CD-FND methods overlook such scenarios and consequently suffer from the following two key limitations: (1) insufficient modeling of high-level semantics in news and user engagements; and (2) scarcity of labeled data in unseen domains. Targeting these limitations, we find that large language models (LLMs) offer strong potential for CD-FND on unseen domains, yet their effective use remains non-trivial. Nevertheless, two key challenges arise: (1) how to capture high-level semantics from both news content and user engagements using LLMs; and (2) how to make LLM-generated features more reliable and transferable for CD-FND on unseen domains. To tackle these challenges, we propose DAUD, a novel LLM-Based Domain-Aware framework for fake news detection on Unseen Domains. DAUD employs LLMs to extract high-level semantics from news content. It models users' single- and cross-domain engagements to generate domain-aware behavioral representations. In addition, DAUD captures the relations between original data-driven features and LLM-derived features of news, users, and user engagements. This allows it to extract more reliable domain-shared representations that improve knowledge transfer to unseen domains. Extensive experiments on real-world datasets demonstrate that DAUD outperforms state-of-the-art baselines in both general and unseen-domain CD-FND settings.

</details>


### [992] [HumanX: Toward Agile and Generalizable Humanoid Interaction Skills from Human Videos](https://arxiv.org/abs/2602.02473)
*Yinhuai Wang,Qihan Zhao,Yuen Fui Lau,Runyi Yu,Hok Wai Tsui,Qifeng Chen,Jingbo Wang,Jiangmiao Pang,Ping Tan*

Main category: cs.RO

Relevance: 75.0

TL;DR: HumanX框架通过从人类视频生成机器人交互数据，无需任务特定奖励，实现了人形机器人通用交互技能的零样本迁移


<details>
  <summary>Details</summary>
Motivation: 当前人形机器人交互任务面临两大瓶颈：1）真实交互数据稀缺；2）需要精细的任务特定奖励工程，限制了方法的可扩展性。需要一种无需任务特定奖励、能从视频学习通用交互技能的方法

Method: HumanX包含两个协同设计的组件：1）XGen数据生成管道，从视频合成多样化、物理合理的机器人交互数据，支持可扩展数据增强；2）XMimic统一模仿学习框架，学习通用交互技能。框架在五个领域（篮球、足球、羽毛球、货物拾取、反应性格斗）评估

Result: 成功学习了10种不同技能，并零样本迁移到物理Unitree G1人形机器人。包括复杂动作如无外部感知的假动作转身后仰跳投，以及交互任务如连续10个周期的人机传球序列。相比先前方法实现了8倍以上的泛化成功率

Conclusion: HumanX展示了一种可扩展、任务无关的途径，用于学习多功能、真实世界的机器人交互技能，解决了当前机器人学习中的数据稀缺和奖励工程难题

Abstract: Enabling humanoid robots to perform agile and adaptive interactive tasks has long been a core challenge in robotics. Current approaches are bottlenecked by either the scarcity of realistic interaction data or the need for meticulous, task-specific reward engineering, which limits their scalability. To narrow this gap, we present HumanX, a full-stack framework that compiles human video into generalizable, real-world interaction skills for humanoids, without task-specific rewards. HumanX integrates two co-designed components: XGen, a data generation pipeline that synthesizes diverse and physically plausible robot interaction data from video while supporting scalable data augmentation; and XMimic, a unified imitation learning framework that learns generalizable interaction skills. Evaluated across five distinct domains--basketball, football, badminton, cargo pickup, and reactive fighting--HumanX successfully acquires 10 different skills and transfers them zero-shot to a physical Unitree G1 humanoid. The learned capabilities include complex maneuvers such as pump-fake turnaround fadeaway jumpshots without any external perception, as well as interactive tasks like sustained human-robot passing sequences over 10 consecutive cycles--learned from a single video demonstration. Our experiments show that HumanX achieves over 8 times higher generalization success than prior methods, demonstrating a scalable and task-agnostic pathway for learning versatile, real-world robot interactive skills.

</details>


### [993] [RAPTOR-AI for Disaster OODA Loop: Hierarchical Multimodal RAG with Experience-Driven Agentic Decision-Making](https://arxiv.org/abs/2602.00030)
*Takato Yasuno*

Main category: cs.LG

Relevance: 65.0

TL;DR: 该论文提出了一个用于人道主义援助和灾害响应的智能RAG框架，通过多模态知识库和自适应检索策略，支持灾害响应的三个阶段：初期救援、中期恢复和长期重建。


<details>
  <summary>Details</summary>
Motivation: 人道主义援助和灾害响应需要快速理解现场情况、可靠的决策支持，并能够泛化到各种未见过的灾害场景。现有系统缺乏对灾害响应完整生命周期的支持，以及多模态信息的有效整合。

Method: 1. 构建分层知识库：整合文本灾害手册、历史经验（如2011年东北地震）、空中和地面图像
2. 多模态处理：使用BLIP图像描述、ColVBERT嵌入和长上下文摘要处理46个海啸相关PDF
3. 智能控制器：通过熵感知场景抽象动态选择检索策略（RAPTOR、ColBERT）
4. LoRA后训练：注入历史灾害经验知识，增强模型对专家和非专家响应者的支持能力

Result: 在真实灾害数据集上的实验显示：改善了情境理解、增强了任务分解准确性、提升了应急操作的可用性。系统通过自适应检索增强生成、自我推理和多模态思维链能力取得了显著提升。

Conclusion: 该智能RAG框架通过整合多模态知识、自适应检索策略和经验知识注入，为灾害响应提供了有效的决策支持系统，能够泛化到不同灾害场景并支持完整的灾害响应生命周期。

Abstract: Effective humanitarian assistance and disaster relief (HADR) requires rapid situational understanding, reliable decision support, and the ability to generalize across diverse and previously unseen disaster contexts. This work introduces an agentic Retrieval-Augmented Generation (RAG) framework designed to support the three canonical phases of disaster response: initial rescue, mid-term recovery, and long-term reconstruction. To achieve robust multimodal grounding, we construct a hierarchical knowledge base that integrates textual disaster manuals, historical lessons (e.g., the 2011 Tohoku earthquake), and both aerial and ground-level imagery. Our system builds on the open-source multimodal implementation, which processes 46 tsunami-related PDFs (2,378 pages) using BLIP-based image captioning, ColVBERT embeddings, and long-context summarization to generate an efficient, structured multimodal retrieval tree optimized for disaster knowledge preservation. An agentic controller dynamically selects retrieval strategies (e.g., RAPTOR, ColBERT) through entropy-aware scene abstraction, enabling adaptive reasoning across heterogeneous inputs. Additionally, a lightweight LoRA-based post-training method injects experiential knowledge from past disasters, enhancing the models' capacity to support both expert and non-expert responders. Experiments on real disaster datasets demonstrate improved situational grounding, enhanced task decomposition accuracy, and superior usability for emergency operations. Incorporating recent advances in long-context RAG systems, agentic information retrieval, and contemporary emergency response AI, our system achieves substantial gains through adaptive retrieval-augmented generation with self-reasoning and multimodal chain-of-thought capabilities.

</details>


### [994] [Trade-offs Between Individual and Group Fairness in Machine Learning: A Comprehensive Review](https://arxiv.org/abs/2602.00094)
*Sandra Benítez-Peña,Blas Kolic,Victoria Menendez,Belén Pulido*

Main category: cs.LG

Relevance: 65.0

TL;DR: 该综述系统回顾了同时处理群体公平性和个体公平性的混合公平方法，分析了统一框架中的权衡机制、理论基础和优化策略，为研究人员提供综合资源。


<details>
  <summary>Details</summary>
Motivation: 传统公平性研究将群体公平性（GF）和个体公平性（IF）分开研究，但实际决策系统需要同时考虑这两种公平性。本文旨在填补这一空白，系统回顾能够联合处理GF和IF的混合公平方法，为设计同时保证个体和群体层面公平性的算法提供指导。

Method: 采用系统性文献综述方法，对混合公平性方法进行分类整理：1）按采用的公平机制分类；2）按调和多种公平标准的算法和数学策略分类。对每类方法分析其理论基础、优化机制和实证评估实践，并讨论其局限性。

Result: 建立了混合公平性方法的系统分类框架，识别了不同方法在理论保证、优化效率和实际应用方面的优缺点。揭示了GF和IF之间的固有权衡关系，并总结了当前方法的局限性。

Conclusion: 混合公平性方法对于构建全面公平的决策系统至关重要。未来研究需要开发更具原则性、上下文感知的混合方法，以在实际应用中提供可靠的个体和群体层面公平性保证。

Abstract: Algorithmic fairness has become a central concern in computational decision-making systems, where ensuring equitable outcomes is essential for both ethical and legal reasons. Two dominant notions of fairness have emerged in the literature: Group Fairness (GF), which focuses on mitigating disparities across demographic subpopulations, and Individual Fairness (IF), which emphasizes consistent treatment of similar individuals. These notions have traditionally been studied in isolation. In contrast, this survey examines methods that jointly address GF and IF, integrating both perspectives within unified frameworks and explicitly characterizing the trade-offs between them. We provide a systematic and critical review of hybrid fairness approaches, organizing existing methods according to the fairness mechanisms they employ and the algorithmic and mathematical strategies used to reconcile multiple fairness criteria. For each class of methods, we examine their theoretical foundations, optimization mechanisms, and empirical evaluation practices, and discuss their limitations. Additionally, we discuss the challenges and identify open research directions for developing principled, context-aware hybrid fairness methods. By synthesizing insights across the literature, this survey aims to serve as a comprehensive resource for researchers and practitioners seeking to design hybrid algorithms that provide reliable fairness guarantees at both the individual and group levels.

</details>


### [995] [Monte Carlo Tree Search for Execution-Guided Program Repair with Large Language Models](https://arxiv.org/abs/2602.00129)
*Yixuan Liang*

Main category: cs.LG

Relevance: 65.0

TL;DR: CodePilot：结合蒙特卡洛树搜索与LLM的混合框架，通过执行引导的程序修复解决真实GitHub问题，在SWE-bench Lite上达到24.67%的问题解决率。


<details>
  <summary>Details</summary>
Motivation: 当前基于大语言模型的自动化程序修复在仓库级别面临挑战，主要由于长时程推理需求和自回归解码的限制。需要一种能够处理真实世界GitHub问题的执行感知修复方法。

Method: CodePilot采用混合框架，将蒙特卡洛树搜索与大语言模型集成：1）从仓库到文件再到函数级别的分层故障定位；2）使用MCTS探索多样化的补丁轨迹；3）利用执行反馈作为奖励信号指导搜索和优化；4）引入置信度校准生成，选择性优化低置信度输出。

Result: 在SWE-bench Lite基准测试中，CodePilot使用开源权重模型实现了24.67%的问题解决率，优于可比基线方法。

Conclusion: 将符号搜索与神经语言模型相结合是构建可扩展、执行感知的软件工程自动化的有效策略。

Abstract: Automated program repair with large language models remains challenging at the repository level due to long-horizon reasoning requirements and the limitations of autoregressive decoding. We present CodePilot, a hybrid framework that integrates Monte Carlo Tree Search (MCTS) with large language models to enable execution-guided program repair for real-world GitHub issues. CodePilot performs hierarchical fault localization from repository to file and function level, explores diverse patch trajectories using MCTS, and leverages execution feedback as a reward signal to guide search and refinement. The framework further incorporates confidence-calibrated generation to selectively refine low-confidence outputs. Experiments on SWE-bench Lite demonstrate that CodePilot achieves a 24.67% issue resolution rate using open-weight models, outperforming comparable baselines. These results suggest that combining symbolic search with neural language models is an effective strategy for scalable, execution-aware software engineering automation.

</details>


### [996] [VoxServe: Streaming-Centric Serving System for Speech Language Models](https://arxiv.org/abs/2602.00269)
*Keisuke Kamahori,Wei-Tzu Lee,Atindra Jha,Rohan Kadekodi,Stephanie Wang,Arvind Krishnamurthy,Baris Kasikci*

Main category: cs.LG

Relevance: 65.0

TL;DR: VoxServe是一个用于语音语言模型的统一流式服务系统，通过解耦模型架构与系统优化，实现低延迟、高吞吐的流式部署。


<details>
  <summary>Details</summary>
Motivation: 当前语音语言模型在流式部署中存在挑战：需要低延迟、高吞吐和流式保证，现有系统无法灵活高效地支持多样化模型。

Method: 提出模型执行抽象层，解耦模型架构与系统优化；实现流式感知调度和异步推理流水线；支持多种语音语言模型架构。

Result: 在多个现代语音语言模型上评估，VoxServe相比现有实现实现10-20倍吞吐量提升，同时保持可比的延迟和高流式可行性。

Conclusion: VoxServe为语音语言模型提供了统一的流式服务框架，显著提升了部署效率，支持多样化模型架构。

Abstract: Deploying modern Speech Language Models (SpeechLMs) in streaming settings requires systems that provide low latency, high throughput, and strong guarantees of streamability. Existing systems fall short of supporting diverse models flexibly and efficiently. We present VoxServe, a unified serving system for SpeechLMs that optimizes streaming performance. VoxServe introduces a model-execution abstraction that decouples model architecture from system-level optimizations, thereby enabling support for diverse SpeechLM architectures within a single framework. Building on this abstraction, VoxServe implements streaming-aware scheduling and an asynchronous inference pipeline to improve end-to-end efficiency. Evaluations across multiple modern SpeechLMs show that VoxServe achieves 10-20x higher throughput than existing implementations at comparable latency while maintaining high streaming viability. The code of VoxServe is available at https://github.com/vox-serve/vox-serve.

</details>


### [997] [Planning with Language and Generative Models: Toward General Reward-Guided Wireless Network Design](https://arxiv.org/abs/2602.00357)
*Chenyang Yuan,Xiaoyuan Cheng*

Main category: cs.LG

Relevance: 65.0

TL;DR: 该论文提出使用扩散模型进行智能AP部署规划，通过统一奖励函数替代传统LLM代理优化器，解决了计算成本高和可扩展性有限的问题。


<details>
  <summary>Details</summary>
Motivation: 下一代无线网络中，复杂室内几何结构和信号传播使智能AP部署面临挑战。作者首先基准测试了通用LLM作为代理优化器，发现尽管LLM具备强大的无线领域知识，但依赖外部验证器导致高计算成本和有限可扩展性。

Method: 1. 使用统一奖励函数捕捉不同楼层平面图中AP部署的核心目标；2. 采用扩散采样器进行生成式推理；3. 扩散过程通过平滑和锐化奖励景观逐步改进采样，而非依赖迭代细化；4. 构建大规模真实世界室内AP部署数据集，需要超过5万CPU小时训练通用奖励函数。

Result: 扩散采样器在生成式方法中表现最佳，扩散过程能有效处理非凸和碎片化目标函数。该方法在分布内和分布外泛化以及鲁棒性评估中表现良好。

Conclusion: 基于扩散的生成式推理与统一奖励函数相结合，为室内AP部署规划提供了可扩展且领域无关的基础框架。

Abstract: Intelligent access point (AP) deployment remains challenging in next-generation wireless networks due to complex indoor geometries and signal propagation. We firstly benchmark general-purpose large language models (LLMs) as agentic optimizers for AP planning and find that, despite strong wireless domain knowledge, their dependence on external verifiers results in high computational costs and limited scalability. Motivated by these limitations, we study generative inference models guided by a unified reward function capturing core AP deployment objectives across diverse floorplans. We show that diffusion samplers consistently outperform alternative generative approaches. The diffusion process progressively improves sampling by smoothing and sharpening the reward landscape, rather than relying on iterative refinement, which is effective for non-convex and fragmented objectives. Finally, we introduce a large-scale real-world dataset for indoor AP deployment, requiring over $50k$ CPU hours to train general reward functions, and evaluate in- and out-of-distribution generalization and robustness. Our results suggest that diffusion-based generative inference with a unified reward function provides a scalable and domain-agnostic foundation for indoor AP deployment planning.

</details>


### [998] [DROGO: Default Representation Objective via Graph Optimization in Reinforcement Learning](https://arxiv.org/abs/2602.00403)
*Hon Tik Tse,Marlos C. Machado*

Main category: cs.LG

Relevance: 65.0

TL;DR: 提出直接近似默认表示主特征向量的目标函数，避免先计算矩阵再分解的高计算成本，应用于奖励塑形等任务


<details>
  <summary>Details</summary>
Motivation: 默认表示及其主特征向量在强化学习中应用广泛（奖励塑形、探索、选项发现等），但现有方法需要先近似矩阵再进行特征分解，计算成本高且难以扩展到高维空间

Method: 推导出直接使用神经网络近似默认表示主特征向量的目标函数，避免了先计算矩阵再分解的两步过程

Result: 在多个环境中实证验证了该目标函数的有效性，并将学习到的特征向量应用于奖励塑形任务

Conclusion: 提出的直接近似方法计算效率更高，可扩展到高维空间，为强化学习中的表示学习提供了更实用的解决方案

Abstract: In computational reinforcement learning, the default representation (DR) and its principal eigenvector have been shown to be effective for a wide variety of applications, including reward shaping, count-based exploration, option discovery, and transfer. However, in prior investigations, the eigenvectors of the DR were computed by first approximating the DR matrix, and then performing an eigendecomposition. This procedure is computationally expensive and does not scale to high-dimensional spaces. In this paper, we derive an objective for directly approximating the principal eigenvector of the DR with a neural network. We empirically demonstrate the effectiveness of the objective in a number of environments, and apply the learned eigenvectors for reward shaping.

</details>


### [999] [Fed-Listing: Federated Label Distribution Inference in Graph Neural Networks](https://arxiv.org/abs/2602.00407)
*Suprim Nakarmi,Junggab Son,Yue Zhao,Zuobin Xiong*

Main category: cs.LG

Relevance: 65.0

TL;DR: 提出Fed-Listing攻击方法，利用联邦图神经网络训练中的梯度信息推断客户端私有标签分布，无需原始数据或节点特征，在非独立同分布场景下仍有效。


<details>
  <summary>Details</summary>
Motivation: 联邦图神经网络(FedGNNs)旨在保护用户隐私，但研究表明共享的模型更新（特别是梯度）可能无意中泄露敏感信息。现有研究主要关注传统联邦学习的隐私推理攻击，而FedGNNs中的标签分布推断问题尚未充分探索。

Method: 提出Fed-Listing攻击方法：仅利用训练过程中交换的最终层梯度，通过分析统计模式来推断目标客户端的类别比例。使用辅助影子数据集生成多样化的标签划分策略，模拟不同客户端分布，并在此上训练攻击模型。

Result: 在四个基准数据集和三种GNN架构上的实验表明，Fed-Listing显著优于现有基线方法（包括随机猜测和Decaf），即使在具有挑战性的非独立同分布场景下也有效。防御机制几乎无法降低攻击性能，除非严重损害模型效用。

Conclusion: Fed-Listing攻击揭示了FedGNNs中梯度信息泄露标签分布的严重隐私风险，现有防御措施难以有效应对，除非牺牲模型性能。这强调了需要更强大的隐私保护机制。

Abstract: Graph Neural Networks (GNNs) have been intensively studied for their expressive representation and learning performance on graph-structured data, enabling effective modeling of complex relational dependencies among nodes and edges in various domains. However, the standalone GNNs can unleash threat surfaces and privacy implications, as some sensitive graph-structured data is collected and processed in a centralized setting. To solve this issue, Federated Graph Neural Networks (FedGNNs) are proposed to facilitate collaborative learning over decentralized local graph data, aiming to preserve user privacy. Yet, emerging research indicates that even in these settings, shared model updates, particularly gradients, can unintentionally leak sensitive information of local users. Numerous privacy inference attacks have been explored in traditional federated learning and extended to graph settings, but the problem of label distribution inference in FedGNNs remains largely underexplored. In this work, we introduce Fed-Listing (Federated Label Distribution Inference in GNNs), a novel gradient-based attack designed to infer the private label statistics of target clients in FedGNNs without access to raw data or node features. Fed-Listing only leverages the final-layer gradients exchanged during training to uncover statistical patterns that reveal class proportions in a stealthy manner. An auxiliary shadow dataset is used to generate diverse label partitioning strategies, simulating various client distributions, on which the attack model is obtained. Extensive experiments on four benchmark datasets and three GNN architectures show that Fed-Listing significantly outperforms existing baselines, including random guessing and Decaf, even under challenging non-i.i.d. scenarios. Moreover, applying defense mechanisms can barely reduce our attack performance, unless the model's utility is severely degraded.

</details>


### [1000] [Search Inspired Exploration in Reinforcement Learning](https://arxiv.org/abs/2602.00460)
*Georgios Sotirchos,Zlatan Ajanović,Jens Kober*

Main category: cs.LG

Relevance: 65.0

TL;DR: SIERL：一种受搜索启发的强化学习探索方法，通过基于学习进度设置子目标来主动引导探索，在稀疏奖励环境中优于现有基线方法。


<details>
  <summary>Details</summary>
Motivation: 稀疏奖励环境中的探索是强化学习的基本挑战。现有方法如课程学习和Go-Explore依赖人工启发式，而好奇心驱动方法可能收敛到次优策略。需要一种能主动引导探索、系统扩展已知状态空间边界的方法。

Method: SIERL（Search-Inspired Exploration in Reinforcement Learning）在每个episode开始时从frontier（已知状态空间边界）选择子目标，然后代理继续向主要任务目标探索。核心是子目标选择机制：基于成本到来和成本到达的估计，从frontier中优先选择既不过于熟悉也不完全新颖的状态-动作对，确保系统扩展边界并能够到达其中的任何状态。

Result: 在具有挑战性的稀疏奖励环境中，SIERL在实现主要任务目标和泛化到环境中任意状态方面都优于主流基线方法。

Conclusion: SIERL通过受搜索启发的子目标选择机制，有效解决了稀疏奖励环境中的探索问题，能够系统扩展已知状态空间并引导探索到最有信息量的区域。

Abstract: Exploration in environments with sparse rewards remains a fundamental challenge in reinforcement learning (RL). Existing approaches such as curriculum learning and Go-Explore often rely on hand-crafted heuristics, while curiosity-driven methods risk converging to suboptimal policies. We propose Search-Inspired Exploration in Reinforcement Learning (SIERL), a novel method that actively guides exploration by setting sub-goals based on the agent's learning progress. At the beginning of each episode, SIERL chooses a sub-goal from the \textit{frontier} (the boundary of the agent's known state space), before the agent continues exploring toward the main task objective. The key contribution of our method is the sub-goal selection mechanism, which provides state-action pairs that are neither overly familiar nor completely novel. Thus, it assures that the frontier is expanded systematically and that the agent is capable of reaching any state within it. Inspired by search, sub-goals are prioritized from the frontier based on estimates of cost-to-come and cost-to-go, effectively steering exploration towards the most informative regions. In experiments on challenging sparse-reward environments, SIERL outperforms dominant baselines in both achieving the main task goal and generalizing to reach arbitrary states in the environment.

</details>


### [1001] [Parallel Stochastic Gradient-Based Planning for World Models](https://arxiv.org/abs/2602.00475)
*Michael Psenka,Michael Rabbat,Aditi Krishnapriyan,Yann LeCun,Amir Bar*

Main category: cs.LG

Relevance: 65.0

TL;DR: GRASP是一种用于视觉世界模型的梯度松弛随机规划器，通过将状态作为优化变量并引入随机性来解决长时域控制任务，在成功率和收敛时间上优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 世界模型可以从原始视觉输入模拟环境动态，但用于规划时面临搜索空间巨大且非结构化的挑战。现有规划方法在处理长时域视觉控制任务时效率有限。

Method: 提出GRASP规划器：1) 将状态视为优化变量（"虚拟状态"）并施加软动态约束；2) 在状态中引入随机性以促进探索；3) 修改梯度结构以缓解高维视觉世界模型的敏感梯度问题，仅需动作输入梯度。

Result: 在基于视频的世界模型实验中，GRASP在长时域任务上优于交叉熵方法（CEM）和普通梯度优化（GD），在成功率和收敛时间方面表现更好。

Conclusion: GRASP提供了一种可并行化、高效的规划方法，能够利用可微世界模型解决长时域视觉控制任务，为基于模型的强化学习提供了新的规划框架。

Abstract: World models simulate environment dynamics from raw sensory inputs like video. However, using them for planning can be challenging due to the vast and unstructured search space. We propose a robust and highly parallelizable planner that leverages the differentiability of the learned world model for efficient optimization, solving long-horizon control tasks from visual input. Our method treats states as optimization variables ("virtual states") with soft dynamics constraints, enabling parallel computation and easier optimization. To facilitate exploration and avoid local optima, we introduce stochasticity into the states. To mitigate sensitive gradients through high-dimensional vision-based world models, we modify the gradient structure to descend towards valid plans while only requiring action-input gradients. Our planner, which we call GRASP (Gradient RelAxed Stochastic Planner), can be viewed as a stochastic version of a non-condensed or collocation-based optimal controller. We provide theoretical justification and experiments on video-based world models, where our resulting planner outperforms existing planning algorithms like the cross-entropy method (CEM) and vanilla gradient-based optimization (GD) on long-horizon experiments, both in success rate and time to convergence.

</details>


### [1002] [Invertible Memory Flow Networks](https://arxiv.org/abs/2602.00535)
*Liyu Zerihun,Alexandr Plashchinsky*

Main category: cs.LG

Relevance: 65.0

TL;DR: IMFN提出了一种可逆记忆流网络，通过二叉树分解将长序列压缩问题简化为2对1的合并任务，实现O(log N)深度和亚线性误差积累，解决了RNN梯度消失和Transformer二次方复杂度的问题。


<details>
  <summary>Details</summary>
Motivation: 解决长序列神经记忆的挑战：RNN存在梯度消失问题，Transformer有二次方复杂度问题，而将长序列压缩为有限固定表示由于优化困难而难以处理。

Method: 采用可逆记忆流网络，通过二叉树分解策略，将端到端压缩分解为"清扫器"模块的成对合并，每个清扫器学习更简单的2对1压缩任务。还通过蒸馏获得恒定成本的循环学生模型用于在线推理。

Result: 在长MNIST序列和UCF-101视频上验证了IMFN的有效性，能够压缩高维数据的长序列，实现O(log N)深度和亚线性误差积累。

Conclusion: IMFN通过分解策略使长序列压缩变得可行，为长序列处理提供了新的架构思路，平衡了效率和表达能力。

Abstract: Long sequence neural memory remains a challenging problem. RNNs and their variants suffer from vanishing gradients, and Transformers suffer from quadratic scaling. Furthermore, compressing long sequences into a finite fixed representation remains an intractable problem due to the difficult optimization landscape. Invertible Memory Flow Networks (IMFN) make long sequence compression tractable through factorization: instead of learning end-to-end compression, we decompose the problem into pairwise merges using a binary tree of "sweeper" modules. Rather than learning to compress long sequences, each sweeper learns a much simpler 2-to-1 compression task, achieving O(log N) depth with sublinear error accumulation in sequence length. For online inference, we distilled into a constant-cost recurrent student achieving O(1) sequential steps. Empirical results validate IMFN on long MNIST sequences and UCF-101 videos, demonstrating compression of high-dimensional data over long sequences.

</details>


### [1003] [OpenDDI: A Comprehensive Benchmark for DDI Prediction](https://arxiv.org/abs/2602.00539)
*Xinmo Jin,Bowen Fan,Xunkai Li,Henan Sun,YuXin Zeng,Zekai Chen,Yuxuan Sun,Jia Li,Qiangqiang Dai,Hongchao Qin,Rong-Hua Li,Guoren Wang*

Main category: cs.LG

Relevance: 65.0

TL;DR: OpenDDI是一个全面的药物相互作用预测基准，统一了多个数据集和评估标准，提供标准化评估框架和LLM增强数据集。


<details>
  <summary>Details</summary>
Motivation: 药物相互作用预测面临两个主要挑战：1）缺乏高质量数据（小规模数据集和单模态药物表示）；2）缺乏标准化评估（不一致的场景、指标和基线）。这些问题阻碍了该领域的进一步发展。

Method: 1）数据层面：统一6个常用DDI数据集和2种现有药物表示形式，新增3个LLM增强的大规模数据集和覆盖5种模态的多模态药物表示。2）评估层面：统一20个SOTA模型基线，涵盖3个下游任务，提供数据质量、有效性、泛化性、鲁棒性和效率的标准化协议。

Result: 基于OpenDDI进行了全面评估，得出10个有价值的药物相互作用预测见解，同时揭示了当前局限性，为该快速发展领域提供关键指导。

Conclusion: OpenDDI为药物相互作用预测提供了一个全面的基准，通过标准化数据、评估和基线，解决了该领域的关键挑战，为未来研究提供了重要指导。

Abstract: Drug-Drug Interactions (DDIs) significantly influence therapeutic efficacy and patient safety. As experimental discovery is resource-intensive and time-consuming, efficient computational methodologies have become essential. The predominant paradigm formulates DDI prediction as a drug graph-based link prediction task. However, further progress is hindered by two fundamental challenges: (1) lack of high-quality data: most studies rely on small-scale DDI datasets and single-modal drug representations; (2) lack of standardized evaluation: inconsistent scenarios, varied metrics, and diverse baselines. To address the above issues, we propose OpenDDI, a comprehensive benchmark for DDI prediction. Specifically, (1) from the data perspective, OpenDDI unifies 6 widely used DDI datasets and 2 existing forms of drug representation, while additionally contributing 3 new large-scale LLM-augmented datasets and a new multimodal drug representation covering 5 modalities. (2) From the evaluation perspective, OpenDDI unifies 20 SOTA model baselines across 3 downstream tasks, with standardized protocols for data quality, effectiveness, generalization, robustness, and efficiency. Based on OpenDDI, we conduct a comprehensive evaluation and derive 10 valuable insights for DDI prediction while exposing current limitations to provide critical guidance for this rapidly evolving field. Our code is available at https://github.com/xiaoriwuguang/OpenDDI

</details>


### [1004] [One Loss to Rule Them All: Marked Time-to-Event for Structured EHR Foundation Models](https://arxiv.org/abs/2602.00541)
*Zilin Jing,Vincent Jeanselme,Yuta Kobayashi,Simon A. Lee,Chao Pang,Aparajita Kashyap,Yanwei Li,Xinzhuo Jiang,Shalmali Joshi*

Main category: cs.LG

Relevance: 65.0

TL;DR: ORA：一种针对电子健康记录（EHR）的标记时间到事件预训练目标，联合建模事件时间和相关测量值，相比传统下一个令牌预测方法能产生更具泛化性的表示


<details>
  <summary>Details</summary>
Motivation: EHR数据具有不规则采样、混合离散事件和数值测量的特点。现有基于下一个令牌预测的EHR基础模型训练方法未能捕捉EHR的完整结构，限制了模型的泛化能力和下游任务表现。

Method: 提出ORA（标记时间到事件）预训练目标，联合建模事件时间和相关连续测量值。该方法超越了传统的下一个令牌预测，能够更好地处理EHR的时间结构和数值特性。

Result: 在多个数据集、下游任务和模型架构上，ORA目标相比下一个令牌预测和忽略连续测量的预训练损失，始终产生更具泛化性的表示。改进不仅限于传统分类评估，还包括更好的回归和时间到事件预测。

Conclusion: ORA不仅引入了一个新的基础模型家族，更重要的是表明：考虑EHR结构的预训练目标对于扩展下游能力和泛化性至关重要。这为EHR基础模型的发展提供了新的方向。

Abstract: Clinical events captured in Electronic Health Records (EHR) are irregularly sampled and may consist of a mixture of discrete events and numerical measurements, such as laboratory values or treatment dosages. The sequential nature of EHR, analogous to natural language, has motivated the use of next-token prediction to train prior EHR Foundation Models (FMs) over events. However, this training fails to capture the full structure of EHR. We propose ORA, a marked time-to-event pretraining objective that jointly models event timing and associated measurements. Across multiple datasets, downstream tasks, and model architectures, this objective consistently yields more generalizable representations than next-token prediction and pretraining losses that ignore continuous measurements. Importantly, the proposed objective yields improvements beyond traditional classification evaluation, including better regression and time-to-event prediction. Beyond introducing a new family of FMs, our results suggest a broader takeaway: pretraining objectives that account for EHR structure are critical for expanding downstream capabilities and generalizability

</details>


### [1005] [Rethinking Zero-Shot Time Series Classification: From Task-specific Classifiers to In-Context Inference](https://arxiv.org/abs/2602.00620)
*Juntao Fang,Shifeng Xie,Shengbin Nie,Yuhui Ling,Yuming Liu,Zijian Li,Keli Zhang,Lujia Pan,Themis Palpanas,Ruichu Cai*

Main category: cs.LG

Relevance: 65.0

TL;DR: 提出TIC-FM框架，用于时间序列基础模型的零样本评估，通过上下文学习避免传统方法中冻结编码器+任务特定分类器带来的训练偏差


<details>
  <summary>Details</summary>
Motivation: 传统时间序列基础模型零样本评估方法使用冻结编码器+任务特定分类器，违反了零样本部署的训练免费前提，并引入分类器依赖的训练选择偏差

Method: TIC-FM框架：将标记训练集作为上下文，通过单次前向传播预测所有测试实例标签，无需参数更新。包含时间序列编码器、轻量级投影适配器和分割掩码潜在记忆Transformer

Result: 在128个UCR数据集上表现出色，在极端低标签情况下获得一致增益，展示了训练免费迁移能力

Conclusion: 提出了一种真正训练免费的时间序列基础模型评估框架，理论上证明上下文推理可以替代训练分类器，并在单次前向传播中模拟基于梯度的分类器训练

Abstract: The zero-shot evaluation of time series foundation models (TSFMs) for classification typically uses a frozen encoder followed by a task-specific classifier. However, this practice violates the training-free premise of zero-shot deployment and introduces evaluation bias due to classifier-dependent training choices. To address this issue, we propose TIC-FM, an in-context learning framework that treats the labeled training set as context and predicts labels for all test instances in a single forward pass, without parameter updates. TIC-FM pairs a time series encoder and a lightweight projection adapter with a split-masked latent memory Transformer. We further provide theoretical justification that in-context inference can subsume trained classifiers and can emulate gradient-based classifier training within a single forward pass. Experiments on 128 UCR datasets show strong accuracy, with consistent gains in the extreme low-label situation, highlighting training-free transfer

</details>


### [1006] [Equilibrium of Feasible Zone and Uncertain Model in Safe Exploration](https://arxiv.org/abs/2602.00636)
*Yujie Yang,Zhilong Zheng,Shengbo Eben Li*

Main category: cs.LG

Relevance: 65.0

TL;DR: 该论文提出了首个面向均衡的安全探索框架SEE，揭示了安全探索的目标是在可行区域与环境模型之间找到均衡点，并通过交替寻找最大可行区域和最小不确定性模型来实现零约束违反的安全探索。


<details>
  <summary>Details</summary>
Motivation: 强化学习中的环境探索安全性是关键问题。虽然将探索限制在可行区域已成为确保安全的广泛接受方法，但两个核心问题仍未解决：通过探索可实现的最大可行区域是什么？如何识别这个区域？本文首次回答了这些问题。

Method: 提出了安全均衡探索（SEE）框架，该框架交替进行两个过程：1）寻找最大可行区域；2）寻找最小不确定性模型。使用不确定模型的图表示，证明SEE获得的不确定模型单调细化，可行区域单调扩展，两者都收敛到安全探索的均衡点。

Result: 在经典控制任务上的实验表明，SEE算法成功实现了零约束违反的可行区域扩展，并在几次迭代内达到了安全探索的均衡状态。

Conclusion: 安全探索的目标是在可行区域与环境模型之间找到均衡，SEE框架首次实现了这一目标，为安全强化学习提供了理论基础和实用算法。

Abstract: Ensuring the safety of environmental exploration is a critical problem in reinforcement learning (RL). While limiting exploration to a feasible zone has become widely accepted as a way to ensure safety, key questions remain unresolved: what is the maximum feasible zone achievable through exploration, and how can it be identified? This paper, for the first time, answers these questions by revealing that the goal of safe exploration is to find the equilibrium between the feasible zone and the environment model. This conclusion is based on the understanding that these two components are interdependent: a larger feasible zone leads to a more accurate environment model, and a more accurate model, in turn, enables exploring a larger zone. We propose the first equilibrium-oriented safe exploration framework called safe equilibrium exploration (SEE), which alternates between finding the maximum feasible zone and the least uncertain model. Using a graph formulation of the uncertain model, we prove that the uncertain model obtained by SEE is monotonically refined, the feasible zones monotonically expand, and both converge to the equilibrium of safe exploration. Experiments on classic control tasks show that our algorithm successfully expands the feasible zones with zero constraint violation, and achieves the equilibrium of safe exploration within a few iterations.

</details>


### [1007] [PHAT: Modeling Period Heterogeneity for Multivariate Time Series Forecasting](https://arxiv.org/abs/2602.00654)
*Jiaming Ma,Guanjun Wang,Qihe Huang,Sheng Huang,Haofeng Ma,Zhengyang Zhou,Pengkun Wang,Binwu Wang,Yang Wang*

Main category: cs.LG

Relevance: 65.0

TL;DR: PHAT：一种针对多元时间序列预测的Transformer模型，专门处理现实数据中的周期性异质性，通过周期性桶结构和正负注意力机制显著提升预测性能。


<details>
  <summary>Details</summary>
Motivation: 现有多元时间序列预测模型虽然能建模周期性，但忽略了现实数据中常见的周期性异质性——不同变量具有不同且动态变化的周期。这种异质性导致传统方法在跨周期交互时产生干扰，影响预测准确性。

Method: 1. 将多元输入组织成三维"周期性桶"张量：维度分别对应具有相似周期性的变量组、按相位对齐的时间步长、周期内偏移量；2. 限制桶内交互并屏蔽跨桶连接，避免不一致周期的干扰；3. 提出正负注意力机制，从周期性对齐和周期性偏差两个角度捕捉周期性依赖；4. 将周期性对齐注意力分数分解为正负分量，并引入编码周期性先验的调制项。

Result: 在14个真实世界数据集上评估，与18个基线方法对比，PHAT显著优于现有方法，取得了极具竞争力的预测性能。

Conclusion: PHAT通过显式建模周期性异质性，有效解决了现实时间序列数据中不同变量具有不同周期的问题，为多元时间序列预测提供了新的有效解决方案。

Abstract: While existing multivariate time series forecasting models have advanced significantly in modeling periodicity, they largely neglect the periodic heterogeneity common in real-world data, where variates exhibit distinct and dynamically changing periods. To effectively capture this periodic heterogeneity, we propose PHAT (Period Heterogeneity-Aware Transformer). Specifically, PHAT arranges multivariate inputs into a three-dimensional "periodic bucket" tensor, where the dimensions correspond to variate group characteristics with similar periodicity, time steps aligned by phase, and offsets within the period. By restricting interactions within buckets and masking cross-bucket connections, PHAT effectively avoids interference from inconsistent periods. We also propose a positive-negative attention mechanism, which captures periodic dependencies from two perspectives: periodic alignment and periodic deviation. Additionally, the periodic alignment attention scores are decomposed into positive and negative components, with a modulation term encoding periodic priors. This modulation constrains the attention mechanism to more faithfully reflect the underlying periodic trends. A mathematical explanation is provided to support this property. We evaluate PHAT comprehensively on 14 real-world datasets against 18 baselines, and the results show that it significantly outperforms existing methods, achieving highly competitive forecasting performance. Our sources is available at GitHub.

</details>


### [1008] [Strong Linear Baselines Strike Back: Closed-Form Linear Models as Gaussian Process Conditional Density Estimators for TSAD](https://arxiv.org/abs/2602.00672)
*Aleksandr Yugay,Hang Cui,Changhua Pei,Alexey Zaytsev*

Main category: cs.LG

Relevance: 65.0

TL;DR: 线性自回归异常检测方法（OLS回归）在时间序列异常检测中表现优于复杂深度学习模型，计算资源需求低几个数量级


<details>
  <summary>Details</summary>
Motivation: 当前时间序列异常检测研究过度关注复杂、难以训练、推理昂贵的神经网络架构，需要重新审视这一范式，探索更简单有效的基线方法

Method: 提出基于普通最小二乘回归的线性自回归异常评分方法，具有闭式解，估计有限历史高斯过程的条件密度

Result: 在广泛的单变量和多变量基准测试中，该方法达到最先进精度，同时计算资源需求比深度学习方法低几个数量级

Conclusion: 未来研究应包含强线性基线，并开发具有更丰富时间结构的新基准，以明确深度学习模型的优势

Abstract: Research in time series anomaly detection (TSAD) has largely focused on developing increasingly sophisticated, hard-to-train, and expensive-to-infer neural architectures. We revisit this paradigm and show that a simple linear autoregressive anomaly score with the closed-form solution provided by ordinary least squares (OLS) regression consistently matches or outperforms state-of-the-art deep detectors. From a theoretical perspective, we show that linear models capture a broad class of anomaly types, estimating a finite-history Gaussian process conditional density. From a practical side, across extensive univariate and multivariate benchmarks, the proposed approach achieves superior accuracy while requiring orders of magnitude fewer computational resources. Thus, future research should consistently include strong linear baselines and, more importantly, develop new benchmarks with richer temporal structures pinpointing the advantages of deep learning models.

</details>


### [1009] [GraphNNK -- Graph Classification and Interpretability](https://arxiv.org/abs/2602.00753)
*Zeljko Bolevic,Milos Brajovic,Isidora Stankovic,Ljubisa Stankovic*

Main category: cs.LG

Relevance: 65.0

TL;DR: 该论文提出使用非负核回归（NNK）替代GNN中的参数分类器，通过训练样本的凸组合进行预测，提高可解释性和泛化能力。


<details>
  <summary>Details</summary>
Motivation: GNN依赖参数分类器（通常是线性softmax层）限制了可解释性，有时阻碍泛化能力。需要更透明、基于相似性的预测方法。

Method: 采用基于插值的方法，特别是非负核回归（NNK），将预测表示为嵌入空间中相似训练样本的凸组合。

Result: NNK方法提供了理论保证和可解释的解释，能够提高GNN的透明度和泛化性能。

Conclusion: 基于插值的非参数方法可以增强GNN的可解释性和泛化能力，为图学习提供更透明的预测机制。

Abstract: Graph Neural Networks (GNNs) have become a standard approach for learning from graph-structured data. However, their reliance on parametric classifiers (most often linear softmax layers) limits interpretability and sometimes hinders generalization. Recent work on interpolation-based methods, particularly Non-Negative Kernel regression (NNK), has demonstrated that predictions can be expressed as convex combinations of similar training examples in the embedding space, yielding both theoretical results and interpretable explanations.

</details>


### [1010] [Reliability-Aware Determinantal Point Processes for Robust Informative Data Selection in Large Language Models](https://arxiv.org/abs/2602.00885)
*Ahmad Sarlak,Abolfazl Razi*

Main category: cs.LG

Relevance: 65.0

TL;DR: 提出ProbDPP方法，在传统多样性数据选择基础上考虑数据访问的可靠性问题，通过正则化项处理概率性数据访问，并设计在线学习算法优化选择策略。


<details>
  <summary>Details</summary>
Motivation: 传统数据选择方法（如DPP）假设数据始终可靠可用，但实际部署中常面临存储中断、通信不完美、随机访问失败等问题。现有方法在这些条件下会失效，需要可靠性感知的数据选择方案。

Method: 1) 提出ProbDPP，将可靠性感知的k-DPP重新表述为包含正则化项的目标函数；2) 将可靠性感知的多样性最大化建模为组合半赌博问题；3) 设计UCB风格算法在线学习未知可靠性。

Result: 1) ProbDPP在概率性数据访问条件下保持良好定义；2) 目标函数可分解为几何多样性项和不可靠性成本；3) 理论分析提供了遗憾界限保证算法性能。

Conclusion: ProbDPP解决了传统数据选择方法在现实不可靠环境中的局限性，为LLM在计算和通信约束下的高效部署提供了可靠性感知的数据选择方案。

Abstract: Informative data selection is a key requirement for large language models (LLMs) to minimize the amount of data required for fine-tuning, network distillation, and token pruning, enabling fast and efficient deployment, especially under computational and communication constraints. Traditional subset selection methods, including those based on Determinantal Point Processes (DPP), focus on maximizing diversity but assume that selected data batches are always available error-free. This presumption prohibits their use under partial storage outage, imperfect communication, and stochastic access failures. Furthermore, we show that the original formulation collapses under such conditions. To address this gap, we introduce ProbDPP, a novel reliability-aware implementation of k-DPP that accounts for probabilistic data access by recasting the objective function with a regularization term that remains well-posed and decomposes into a geometric diversity term and unreliability cost. The resulting objective facilitates robust selection of diverse data batches under uncertainty. Furthermore, we frame this reliability-aware diversity maximization as a combinatorial semi-bandit problem and propose a UCB-style algorithm to efficiently learn the unknown reliability online. Theoretical analysis provides regret bounds for the proposed approach, ensuring performance guarantees.

</details>


### [1011] [Federated Learning With Individualized Privacy Through Client Sampling](https://arxiv.org/abs/2501.17634)
*Lucas Lange,Ole Borchardt,Erhard Rahm*

Main category: cs.LG

Relevance: 65.0

TL;DR: 提出了一种在联邦学习中实现个性化差分隐私的方法，通过根据客户端隐私偏好调整采样率，相比统一DP基线在隐私-效用权衡方面有明显改进。


<details>
  <summary>Details</summary>
Motivation: 随着对用户数据收集的担忧日益增长，个性化隐私成为平衡保护与效用的有前景方案。传统方法对所有用户强制统一匿名化级别，而个性化方法允许用户根据自身舒适度选择隐私设置。

Method: 将SAMPLE算法从集中式设置扩展到联邦学习，基于客户端异构隐私预算计算客户端特定采样率，并将其集成到改进的IDP-FedAvg算法中。

Result: 在现实隐私分布和多个数据集上的实验表明，该方法相比统一DP基线有明显改进，减少了隐私与效用之间的权衡。与相关工作中的SCALE方法相比，性能显著更好。

Conclusion: 该方法在联邦学习中实现个性化差分隐私是有效的，但对于非独立同分布数据的复杂任务仍存在挑战，主要源于去中心化设置的限制。

Abstract: With growing concerns about user data collection, individualized privacy has emerged as a promising solution to balance protection and utility by accounting for diverse user privacy preferences. Instead of enforcing a uniform level of anonymization for all users, this approach allows individuals to choose privacy settings that align with their comfort levels. Building on this idea, we propose an adapted method for enabling Individualized Differential Privacy (IDP) in Federated Learning (FL) by handling clients according to their personal privacy preferences. By extending the SAMPLE algorithm from centralized settings to FL, we calculate client-specific sampling rates based on their heterogeneous privacy budgets and integrate them into a modified IDP-FedAvg algorithm. We test this method under realistic privacy distributions and multiple datasets. The experimental results demonstrate that our approach achieves clear improvements over uniform DP baselines, reducing the trade-off between privacy and utility. Compared to the alternative SCALE method in related work, which assigns differing noise scales to clients, our method performs notably better. However, challenges remain for complex tasks with non-i.i.d. data, primarily stemming from the constraints of the decentralized setting.

</details>


### [1012] [Reinforcement Learning via Conservative Agent for Environments with Random Delays](https://arxiv.org/abs/2507.18992)
*Jongsoo Lee,Jangwon Kim,Jiseok Jeong,Soohee Han*

Main category: cs.LG

Relevance: 65.0

TL;DR: 提出一种保守智能体方法，将随机延迟环境转化为恒定延迟环境，使现有恒定延迟方法可直接应用于随机延迟场景，无需算法修改且性能不降


<details>
  <summary>Details</summary>
Motivation: 现实世界强化学习应用常受环境延迟反馈困扰，延迟违反马尔可夫假设带来挑战。现有方法主要处理恒定延迟环境，而随机延迟环境因其多变性和不可预测性研究较少

Method: 提出保守智能体方法，将随机延迟环境重新表述为其恒定延迟等价形式。这种转换使任何最先进的恒定延迟方法都能直接扩展到随机延迟环境，无需修改算法结构

Result: 在连续控制任务上的实验结果表明，基于保守智能体的算法在渐进性能和样本效率方面显著优于现有基线算法

Conclusion: 保守智能体方法为随机延迟环境中的决策提供了一种简单而强大的解决方案，使现有恒定延迟方法能够直接应用于更复杂的随机延迟场景

Abstract: Real-world reinforcement learning applications are often hindered by delayed feedback from environments, which violates the Markov assumption and introduces significant challenges. Although numerous delay-compensating methods have been proposed for environments with constant delays, environments with random delays remain largely unexplored due to their inherent variability and unpredictability. In this study, we propose a simple yet robust agent for decision-making under random delays, termed the conservative agent, which reformulates the random-delay environment into its constant-delay equivalent. This transformation enables any state-of-the-art constant-delay method to be directly extended to the random-delay environments without modifying the algorithmic structure or sacrificing performance. We evaluate the conservative agent-based algorithm on continuous control tasks, and empirical results demonstrate that it significantly outperforms existing baseline algorithms in terms of asymptotic performance and sample efficiency.

</details>


### [1013] [LASS-ODE: Scaling ODE Computations to Connect Foundation Models with Dynamical Physical Systems](https://arxiv.org/abs/2602.01009)
*Haoran Li,Chenhan Xiao,Lihao Mai,Yang Weng,Erik Blasch*

Main category: cs.LG

Relevance: 65.0

TL;DR: LASS-ODE：一种用于大规模ODE系统预测的基础模型，通过局部线性ODE表示和跨系统注意力机制，解决了物理计算可扩展性和知识共享效率问题。


<details>
  <summary>Details</summary>
Motivation: 尽管基础模型在语言、视觉和时间序列分析中取得了成功，但在物理系统动态预测方面进展有限。主要挑战包括：1）物理计算可扩展性 - 物理约束学习需要昂贵的非线性积分计算；2）知识共享效率 - 注意力机制主要在单个系统内计算，难以提取跨系统的共享ODE结构。

Method: 提出LASS-ODE模型：1）使用token-wise局部线性ODE表示，在保持物理保真度的同时实现可扩展性；2）引入跨系统注意力机制，通过公共结构中心（CSH）存储共享token并聚合跨系统知识；3）在40GB ODE轨迹数据集上进行预训练。

Result: 模型在领域内表现优异，能够零样本泛化到多样ODE系统，并通过微调获得额外改进。局部线性表示显著加速积分计算，同时准确近似局部数据流形。

Conclusion: LASS-ODE通过创新的局部线性ODE表示和跨系统注意力机制，成功解决了物理系统预测中的可扩展性和知识共享问题，为物理系统基础模型的发展提供了新方向。

Abstract: Foundation models have transformed language, vision, and time series data analysis, yet progress on dynamic predictions for physical systems remains limited. Given the complexity of physical constraints, two challenges stand out. $(i)$ Physics-computation scalability: physics-informed learning can enforce physical regularization, but its computation (e.g., ODE integration) does not scale to extensive systems. $(ii)$ Knowledge-sharing efficiency: the attention mechanism is primarily computed within each system, which limits the extraction of shared ODE structures across systems. We show that enforcing ODE consistency does not require expensive nonlinear integration: a token-wise locally linear ODE representation preserves physical fidelity while scaling to foundation-model regimes. Thus, we propose novel token representations that respect locally linear ODE evolution. Such linearity substantially accelerates integration while accurately approximating the local data manifold. Second, we introduce a simple yet effective inter-system attention that augments attention with a common structure hub (CSH) that stores shared tokens and aggregates knowledge across systems. The resulting model, termed LASS-ODE (\underline{LA}rge-\underline{S}cale \underline{S}mall \underline{ODE}), is pretrained on our $40$GB ODE trajectory collections to enable strong in-domain performance, zero-shot generalization across diverse ODE systems, and additional improvements through fine-tuning.

</details>


### [1014] [ChronoSpike: An Adaptive Spiking Graph Neural Network for Dynamic Graphs](https://arxiv.org/abs/2602.01124)
*Md Abrar Jahin,Taufikur Rahman Fuad,Jay Pujara,Craig Knoblock*

Main category: cs.LG

Relevance: 65.0

TL;DR: ChronoSpike：一种自适应脉冲图神经网络，通过可学习的LIF神经元、多头注意力空间聚合和轻量级Transformer时间编码器，在动态图表示学习中实现了线性内存复杂度和高效训练。


<details>
  <summary>Details</summary>
Motivation: 现有动态图表示学习方法面临基本权衡：基于注意力的方法表达能力强但复杂度高（O(T²)），循环架构存在梯度问题和密集状态存储问题。脉冲神经网络虽然事件驱动效率高，但受限于序列传播、二进制信息丢失和局部聚合缺乏全局上下文。

Method: 提出ChronoSpike，集成可学习的LIF神经元（具有每通道膜动力学）、基于连续特征的多头注意力空间聚合，以及轻量级Transformer时间编码器。该方法支持细粒度局部建模和长程依赖捕获，具有线性内存复杂度O(T·d)。

Result: 在三个大规模基准测试中，ChronoSpike在12个最先进基线方法上提升了2.0% Macro-F1和2.4% Micro-F1，训练速度比循环方法快3-10倍，参数预算恒定（105K）且与图大小无关。

Conclusion: ChronoSpike解决了动态图表示学习中的效率-表达能力权衡问题，提供了理论保证（膜电位有界性、梯度流稳定性、BIBO稳定性），可解释性分析揭示了异质时间感受野和学习的首因效应（83-88%稀疏性）。

Abstract: Dynamic graph representation learning requires capturing both structural relationships and temporal evolution, yet existing approaches face a fundamental trade-off: attention-based methods achieve expressiveness at $O(T^2)$ complexity, while recurrent architectures suffer from gradient pathologies and dense state storage. Spiking neural networks offer event-driven efficiency but remain limited by sequential propagation, binary information loss, and local aggregation that misses global context. We propose ChronoSpike, an adaptive spiking graph neural network that integrates learnable LIF neurons with per-channel membrane dynamics, multi-head attentive spatial aggregation on continuous features, and a lightweight Transformer temporal encoder, enabling both fine-grained local modeling and long-range dependency capture with linear memory complexity $O(T \cdot d)$. On three large-scale benchmarks, ChronoSpike outperforms twelve state-of-the-art baselines by $2.0\%$ Macro-F1 and $2.4\%$ Micro-F1 while achieving $3-10\times$ faster training than recurrent methods with a constant 105K-parameter budget independent of graph size. We provide theoretical guarantees for membrane potential boundedness, gradient flow stability under contraction factor $ρ< 1$, and BIBO stability; interpretability analyses reveal heterogeneous temporal receptive fields and a learned primacy effect with $83-88\%$ sparsity.

</details>


### [1015] [Statistical MIA: Rethinking Membership Inference Attack for Reliable Unlearning Auditing](https://arxiv.org/abs/2602.01150)
*Jialong Sun,Zeming Wei,Jiaxuan Zou,Jiacheng Gong,Guanheng Wang,Chengyang Dong,Jialong Li,Bo Liu*

Main category: cs.LG

Relevance: 65.0

TL;DR: 论文提出SMIA（统计成员推断攻击），一种无需训练、基于统计检验的机器遗忘审计框架，解决了传统MIA方法在遗忘审计中的统计误差问题，提供带置信区间的遗忘率评估。


<details>
  <summary>Details</summary>
Motivation: 当前机器遗忘审计主要依赖成员推断攻击（MIA），但MIA作为二元分类问题存在不可避免的统计误差，导致对遗忘性能的评估过于乐观，且需要训练影子模型带来巨大计算开销。需要更可靠、高效的审计方法。

Method: 提出SMIA框架：1）直接比较成员和非成员数据的分布差异，使用统计检验而非学习攻击模型；2）无需训练影子模型，显著降低计算成本；3）输出带置信区间的遗忘率，量化审计结果的可靠性。

Result: 大量实验表明，SMIA相比现有MIA方法提供更可靠的审计结果，计算成本显著降低。SMIA的理论保证和实证效果表明其可作为机器遗忘审计的新范式。

Conclusion: SMIA解决了传统MIA在机器遗忘审计中的根本缺陷，提供统计可靠、计算高效的审计框架，为机器遗忘系统的可靠评估提供了新方法。

Abstract: Machine unlearning (MU) is essential for enforcing the right to be forgotten in machine learning systems. A key challenge of MU is how to reliably audit whether a model has truly forgotten specified training data. Membership Inference Attacks (MIAs) are widely used for unlearning auditing, where samples that evade membership detection are often regarded as successfully forgotten. After carefully revisiting the reliability of MIA, we show that this assumption is flawed: failed membership inference does not imply true forgetting. We theoretically demonstrate that MIA-based auditing, when formulated as a binary classification problem, inevitably incurs statistical errors whose magnitude cannot be observed during the auditing process. This leads to overly optimistic evaluations of unlearning performance, while incurring substantial computational overhead due to shadow model training. To address these limitations, we propose Statistical Membership Inference Attack (SMIA), a novel training-free and highly effective auditing framework. SMIA directly compares the distributions of member and non-member data using statistical tests, eliminating the need for learned attack models. Moreover, SMIA outputs both a forgetting rate and a corresponding confidence interval, enabling quantified reliability of the auditing results. Extensive experiments show that SMIA provides more reliable auditing with significantly lower computational cost than existing MIA-based approaches. Notably, the theoretical guarantees and empirical effectiveness of SMIA suggest it as a new paradigm for reliable machine unlearning auditing.

</details>


### [1016] [Unraveling the Hidden Dynamical Structure in Recurrent Neural Policies](https://arxiv.org/abs/2602.01196)
*Jin Li,Yue Wu,Mengsha Huang,Yuhao Sun,Hao He,Xianyuan Zhan*

Main category: cs.LG

Relevance: 65.0

TL;DR: 该研究发现循环神经网络策略在部分可观测控制和元强化学习任务中会自发形成稳定的循环结构，这些结构与动力系统中的极限环相似，能够解释循环策略的泛化能力和鲁棒性优势。


<details>
  <summary>Details</summary>
Motivation: 循环神经网络策略在部分可观测控制和元强化学习任务中表现出色，但其优越泛化能力和鲁棒性的内在机制尚不清楚。研究者希望通过分析循环策略的隐藏状态动态来揭示这些机制。

Method: 通过分析在不同训练方法、模型架构和任务中学到的循环策略的隐藏状态域，研究者发现这些策略在与环境交互时会自发形成稳定的循环结构。这些结构类似于动力系统中的极限环，研究者将策略和环境视为联合混合动力系统进行分析。

Result: 研究发现循环策略会形成稳定的极限环结构，这些结构能够稳定策略的内部记忆和任务相关的环境状态，同时抑制环境不确定性带来的干扰。极限环的几何形状也与策略行为存在结构化对应关系，这有助于在非平稳环境中进行技能适应。

Conclusion: 循环策略中极限环的出现为其优越性能提供了新的解释框架：极限环稳定了内部记忆和环境状态，抑制了环境噪声，同时极限环的几何结构编码了行为的关系结构，促进了技能适应能力。

Abstract: Recurrent neural policies are widely used in partially observable control and meta-RL tasks. Their abilities to maintain internal memory and adapt quickly to unseen scenarios have offered them unparalleled performance when compared to non-recurrent counterparts. However, until today, the underlying mechanisms for their superior generalization and robustness performance remain poorly understood. In this study, by analyzing the hidden state domain of recurrent policies learned over a diverse set of training methods, model architectures, and tasks, we find that stable cyclic structures consistently emerge during interaction with the environment. Such cyclic structures share a remarkable similarity with \textit{limit cycles} in dynamical system analysis, if we consider the policy and the environment as a joint hybrid dynamical system. Moreover, we uncover that the geometry of such limit cycles also has a structured correspondence with the policies' behaviors. These findings offer new perspectives to explain many nice properties of recurrent policies: the emergence of limit cycles stabilizes both the policies' internal memory and the task-relevant environmental states, while suppressing nuisance variability arising from environmental uncertainty; the geometry of limit cycles also encodes relational structures of behaviors, facilitating easier skill adaptation when facing non-stationary environments.

</details>


### [1017] [Sample Efficient Active Algorithms for Offline Reinforcement Learning](https://arxiv.org/abs/2602.01260)
*Soumyadeep Roy,Shashwat Kushwaha,Ambedkar Dukkipati*

Main category: cs.LG

Relevance: 65.0

TL;DR: 该论文提出了Active Reinforcement Learning (ActiveRL)方法，通过有限在线交互选择性地精化价值函数的不确定区域，解决了离线RL中的覆盖不足和分布偏移问题，并提供了理论分析证明其样本效率优势。


<details>
  <summary>Details</summary>
Motivation: 离线强化学习通常面临状态-动作空间覆盖不足和分布偏移问题。虽然通过有限在线交互选择性精化价值函数不确定区域的ActiveRL方法在实证中表现良好，但缺乏理论分析。本文旨在填补这一理论空白。

Method: 提出ActiveRL算法，基于高斯过程(GP)不确定性建模，利用GP集中不等式和信息增益界限进行理论分析。通过有限在线交互选择性地探索价值函数的不确定区域，加速策略学习。

Result: 理论分析表明ActiveRL能以O(1/ε²)的主动转移学习到ε-最优策略，优于纯离线方法的Ω(1/ε²(1-γ)⁴)速率。实验验证了算法和理论发现，显示ActiveRL实现了近乎最优的信息效率。

Conclusion: ActiveRL通过引导不确定性减少实现了加速的价值函数收敛，以最少的在线数据提高了样本效率。该工作连接了贝叶斯非参数回归和强化学习理论，为有限交互的强化学习提供了理论框架。

Abstract: Offline reinforcement learning (RL) enables policy learning from static data but often suffers from poor coverage of the state-action space and distributional shift problems. This problem can be addressed by allowing limited online interactions to selectively refine uncertain regions of the learned value function, which is referred to as Active Reinforcement Learning (ActiveRL). While there has been good empirical success, no theoretical analysis is available in the literature. We fill this gap by developing a rigorous sample-complexity analysis of ActiveRL through the lens of Gaussian Process (GP) uncertainty modeling. In this respect, we propose an algorithm and using GP concentration inequalities and information-gain bounds, we derive high-probability guarantees showing that an $ε$-optimal policy can be learned with ${\mathcal{O}}(1/ε^2)$ active transitions, improving upon the $Ω(1/ε^2(1-γ)^4)$ rate of purely offline methods. Our results reveal that ActiveRL achieves near-optimal information efficiency, that is, guided uncertainty reduction leads to accelerated value-function convergence with minimal online data. Our analysis builds on GP concentration inequalities and information-gain bounds, bridging Bayesian nonparametric regression and reinforcement learning theories. We conduct several experiments to validate the algorithm and theoretical findings.

</details>


### [1018] [BicKD: Bilateral Contrastive Knowledge Distillation](https://arxiv.org/abs/2602.01265)
*Jiangnan Zhu,Yukai Xu,Li Xiong,Yixuan Liu,Junxu Liu,Hong kyu Lee,Yujie Gu*

Main category: cs.LG

Relevance: 65.0

TL;DR: 提出双边对比知识蒸馏(BicKD)，通过双边对比损失增强不同类间的正交性，同时保持同类一致性，改善传统KD缺乏类间比较和概率空间结构约束的问题


<details>
  <summary>Details</summary>
Motivation: 传统知识蒸馏(vanilla KD)只进行样本级别的概率对齐，缺乏类间比较机制，且对概率空间没有结构约束。这限制了知识从教师模型向学生模型的传递效果

Method: 提出双边对比知识蒸馏(BicKD)，引入双边对比损失，增强不同类泛化空间的正交性，同时保持同类一致性。双边公式能够显式比较教师和学生模型的样本级别和类级别预测模式

Result: 在多种模型架构和基准测试中，BicKD方法显著提升了知识传递效果，一致性地优于最先进的知识蒸馏技术

Conclusion: BicKD通过双边对比损失增强概率正交性，正则化预测分布的几何结构，为知识蒸馏提供了更有效的框架

Abstract: Knowledge distillation (KD) is a machine learning framework that transfers knowledge from a teacher model to a student model. The vanilla KD proposed by Hinton et al. has been the dominant approach in logit-based distillation and demonstrates compelling performance. However, it only performs sample-wise probability alignment between teacher and student's predictions, lacking an mechanism for class-wise comparison. Besides, vanilla KD imposes no structural constraint on the probability space. In this work, we propose a simple yet effective methodology, bilateral contrastive knowledge distillation (BicKD). This approach introduces a novel bilateral contrastive loss, which intensifies the orthogonality among different class generalization spaces while preserving consistency within the same class. The bilateral formulation enables explicit comparison of both sample-wise and class-wise prediction patterns between teacher and student. By emphasizing probabilistic orthogonality, BicKD further regularizes the geometric structure of the predictive distribution. Extensive experiments show that our BicKD method enhances knowledge transfer, and consistently outperforms state-of-the-art knowledge distillation techniques across various model architectures and benchmarks.

</details>


### [1019] [Gradient-Aligned Calibration for Post-Training Quantization of Diffusion Models](https://arxiv.org/abs/2602.01289)
*Dung Anh Hoang,Cuong Pham anh Trung Le,Jianfei Cai,Toan Do*

Main category: cs.LG

Relevance: 65.0

TL;DR: 提出一种针对扩散模型的后训练量化方法，通过学习为不同时间步的校准样本分配最优权重，解决均匀量化导致的梯度冲突问题，显著提升量化效果。


<details>
  <summary>Details</summary>
Motivation: 扩散模型在实际部署中面临推理速度慢、内存占用高、计算需求大的问题。后训练量化是加速采样和减少内存开销的可行方案，但现有方法对跨时间步的校准样本采用均匀权重分配，忽略了不同时间步数据对扩散过程的贡献差异，且激活分布和梯度变化导致均匀量化效果不佳。

Method: 提出一种新颖的后训练量化方法，通过学习为校准样本分配最优权重，使量化模型在不同时间步的梯度对齐，从而优化量化过程。该方法考虑了不同时间步对扩散过程的贡献差异，解决了梯度冲突问题。

Result: 在CIFAR-10、LSUN-Bedrooms和ImageNet等数据集上的大量实验表明，该方法相比其他扩散模型PTQ方法具有优越性，显著提升了量化效果。

Conclusion: 通过为不同时间步的校准样本学习最优权重分配，可以有效解决扩散模型量化中的梯度冲突问题，提升量化性能，为扩散模型的轻量化部署提供了有效方案。

Abstract: Diffusion models have shown remarkable performance in image synthesis by progressively estimating a smooth transition from a Gaussian distribution of noise to a real image. Unfortunately, their practical deployment is limited by slow inference speed, high memory usage, and the computational demands of the noise estimation process. Post-training quantization (PTQ) emerges as a promising solution to accelerate sampling and reduce memory overhead for diffusion models. Existing PTQ methods for diffusion models typically apply uniform weights to calibration samples across timesteps, which is sub-optimal since data at different timesteps may contribute differently to the diffusion process. Additionally, due to varying activation distributions and gradients across timesteps, a uniform quantization approach is sub-optimal. Each timestep requires a different gradient direction for optimal quantization, and treating them equally can lead to conflicting gradients that degrade performance. In this paper, we propose a novel PTQ method that addresses these challenges by assigning appropriate weights to calibration samples. Specifically, our approach learns to assign optimal weights to calibration samples to align the quantized model's gradients across timesteps, facilitating the quantization process. Extensive experiments on CIFAR-10, LSUN-Bedrooms, and ImageNet demonstrate the superiority of our method compared to other PTQ methods for diffusion models.

</details>


### [1020] [Dispelling the Curse of Singularities in Neural Network Optimizations](https://arxiv.org/abs/2602.01308)
*Hengjie Cao,Mengyi Chen,Yifeng Yang,Fang Dong,Ruijun Huang,Anrui Chen,Jixian Zhou,Mingzhi Dong,Yujiang Wang,Dongsheng Li,Wenyi Fang,Yuanyi Lin,Fan Wu,Li Shang*

Main category: cs.LG

Relevance: 65.0

TL;DR: 该论文从参数空间奇异性的角度研究深度神经网络优化不稳定性，发现参数奇异值随训练增长并与表示对齐，导致表示空间奇异性增加，形成"奇异性诅咒"。作者提出参数奇异性平滑方法(PSS)来缓解这一问题。


<details>
  <summary>Details</summary>
Motivation: 深度神经网络训练中经常出现优化不稳定和损失爆炸问题，传统方法主要关注梯度爆炸/消失、学习率调整等，但较少从参数空间奇异性的角度分析。本文从参数奇异性的出现和放大这一新视角来研究训练不稳定性。

Method: 提出参数奇异性平滑(PSS)方法，通过平滑权重矩阵的奇异谱来缓解奇异性问题。该方法轻量、灵活且有效，不改变模型架构，可与各种优化器结合使用。

Result: 实验表明PSS能有效缓解训练不稳定性，即使在训练失败后也能恢复可训练性，同时提高训练效率和泛化性能。方法在多种数据集、架构和优化器上得到验证。

Conclusion: 参数奇异性是深度神经网络优化不稳定的重要原因，PSS方法通过平滑奇异谱有效缓解了"奇异性诅咒"，为训练稳定性和泛化性能提供了新的解决方案。

Abstract: This work investigates the optimization instability of deep neural networks from a less-explored yet insightful perspective: the emergence and amplification of singularities in the parametric space. Our analysis reveals that parametric singularities inevitably grow with gradient updates and further intensify alignment with representations, leading to increased singularities in the representation space. We show that the gradient Frobenius norms are bounded by the top singular values of the weight matrices, and as training progresses, the mutually reinforcing growth of weight and representation singularities, termed the curse of singularities, relaxes these bounds, escalating the risk of sharp loss explosions. To counter this, we propose Parametric Singularity Smoothing (PSS), a lightweight, flexible, and effective method for smoothing the singular spectra of weight matrices. Extensive experiments across diverse datasets, architectures, and optimizers demonstrate that PSS mitigates instability, restores trainability even after failure, and improves both training efficiency and generalization.

</details>


### [1021] [EvoMU: Evolutionary Machine Unlearning](https://arxiv.org/abs/2602.02139)
*Pawel Batorski,Paul Swoboda*

Main category: cs.LG

Relevance: 65.0

TL;DR: EvoMU：一种通过进化搜索自动发现特定任务遗忘损失函数的机器学习遗忘方法，在有限计算资源下实现SotA性能


<details>
  <summary>Details</summary>
Motivation: 机器学习遗忘旨在从模型中移除特定训练数据（如敏感或版权材料）。现有方法通常使用固定的遗忘损失函数进行微调，但合适的损失函数空间巨大，且不存在普遍最优的损失函数——不同遗忘数据和保留数据的结构与重叠差异会导致同一损失在不同场景下表现迥异。

Method: EvoMU采用进化搜索程序，在巨大的可能遗忘损失函数空间中自动发现特定任务的损失函数。该方法无需人工干预，能够找到针对特定数据集的损失函数，匹配或超越文献中现有损失函数。

Result: 在TOFU-5%、TOFU-10%、MUSE和WMDP等基准测试中，EvoMU超越了先前基于损失的遗忘方法。仅使用4B参数模型（Qwen3-4B-Thinking）就实现了SotA结果，展示了有限计算资源下AI协同科学家的潜力。

Conclusion: EvoMU成功解决了机器学习遗忘中损失函数搜索的挑战，展示了自动科学发现（AI协同科学家）在有限计算预算下的可行性。该方法能够合成新颖的遗忘损失函数，为不同遗忘场景提供定制化解决方案。

Abstract: Machine unlearning aims to unlearn specified training data (e.g. sensitive or copyrighted material). A prominent approach is to fine-tune an existing model with an unlearning loss that retains overall utility. The space of suitable unlearning loss functions is vast, making the search for an optimal loss function daunting. Additionally, there might not even exist a universally optimal loss function: differences in the structure and overlap of the forget and retain data can cause a loss to work well in one setting but over-unlearn or under-unlearn in another. Our approach EvoMU tackles these two challenges simultaneously. An evolutionary search procedure automatically finds task-specific losses in the vast space of possible unlearning loss functions. This allows us to find dataset-specific losses that match or outperform existing losses from the literature, without the need for a human-in-the-loop. This work is therefore an instance of automatic scientific discovery, a.k.a. an AI co-scientist. In contrast to previous AI co-scientist works, we do so on a budget: We achieve SotA results using a small 4B parameter model (Qwen3-4B-Thinking), showing the potential of AI co-scientists with limited computational resources. Our experimental evaluation shows that we surpass previous loss-based unlearning formulations on TOFU-5%, TOFU-10%, MUSE and WMDP by synthesizing novel unlearning losses. Our code is available at https://github.com/Batorskq/EvoMU.

</details>


### [1022] [An Odd Estimator for Shapley Values](https://arxiv.org/abs/2602.01399)
*Fabian Fumagalli,Landon Butler,Justin Singh Kang,Kannan Ramchandran,R. Teal Witter*

Main category: cs.LG

Relevance: 65.0

TL;DR: 提出OddSHAP，一种新颖的Shapley值估计器，通过证明Shapley值仅依赖于集合函数的奇分量，并利用配对采样正交化回归目标来过滤无关的偶分量，从而显著提升估计精度。


<details>
  <summary>Details</summary>
Motivation: Shapley值是机器学习中广泛使用的归因框架，但其精确计算通常不可行，需要高效的近似方法。虽然最有效的估计器利用配对采样启发式来减少估计误差，但驱动这种改进的理论机制一直不明确。本文旨在为配对采样提供理论基础，并开发更高效的估计器。

Method: 1. 理论证明：证明Shapley值仅依赖于集合函数的奇分量，配对采样通过正交化回归目标来过滤偶分量。2. 提出OddSHAP：一种新颖的一致估计器，仅在奇子空间上执行多项式回归。3. 技术实现：利用傅里叶基分离奇子空间，使用代理模型识别高影响交互，克服高阶近似的组合爆炸问题。

Result: 通过广泛的基准评估，OddSHAP实现了最先进的估计精度，在Shapley值估计任务中表现出色。

Conclusion: 本文为配对采样提供了理论基础，证明了Shapley值的奇分量特性，并提出了OddSHAP这一高效估计器，显著提升了Shapley值估计的准确性和效率。

Abstract: The Shapley value is a ubiquitous framework for attribution in machine learning, encompassing feature importance, data valuation, and causal inference. However, its exact computation is generally intractable, necessitating efficient approximation methods. While the most effective and popular estimators leverage the paired sampling heuristic to reduce estimation error, the theoretical mechanism driving this improvement has remained opaque. In this work, we provide an elegant and fundamental justification for paired sampling: we prove that the Shapley value depends exclusively on the odd component of the set function, and that paired sampling orthogonalizes the regression objective to filter out the irrelevant even component. Leveraging this insight, we propose OddSHAP, a novel consistent estimator that performs polynomial regression solely on the odd subspace. By utilizing the Fourier basis to isolate this subspace and employing a proxy model to identify high-impact interactions, OddSHAP overcomes the combinatorial explosion of higher-order approximations. Through an extensive benchmark evaluation, we find that OddSHAP achieves state-of-the-art estimation accuracy.

</details>


### [1023] [Plain Transformers are Surprisingly Powerful Link Predictors](https://arxiv.org/abs/2602.01553)
*Quang Truong,Yu Song,Donald Loveland,Mingxuan Ju,Tong Zhao,Neil Shah,Jiliang Tang*

Main category: cs.LG

Relevance: 65.0

TL;DR: PENCIL是一个用于图链接预测的编码器式普通Transformer，通过注意力机制处理采样子图，无需手工先验知识，在保持标准Transformer可扩展性和硬件效率的同时，超越了启发式GNN和基于ID嵌入的方法。


<details>
  <summary>Details</summary>
Motivation: 现有图链接预测方法存在局限性：GNN依赖显式结构启发式或内存密集的节点嵌入，难以泛化和扩展到大规模图；图Transformer虽然提供替代方案，但复杂的结构编码带来显著开销。需要一种既高效又能捕捉丰富拓扑依赖的方法。

Method: PENCIL采用编码器式普通Transformer架构，用注意力机制处理采样的局部子图，替代手工设计的先验知识。该方法保留了标准Transformer的可扩展性和硬件效率，通过理论分析证明能够提取比GNN更丰富的结构信号。

Result: PENCIL在性能上超越了启发式GNN，比基于ID嵌入的替代方案参数效率更高，在多样化基准测试中保持竞争力，即使在没有节点特征的情况下也能表现良好。

Conclusion: 研究挑战了当前对复杂工程技术的依赖，证明简单的设计选择可能足以实现相同的功能。PENCIL展示了普通Transformer在图链接预测任务中的潜力。

Abstract: Link prediction is a core challenge in graph machine learning, demanding models that capture rich and complex topological dependencies. While Graph Neural Networks (GNNs) are the standard solution, state-of-the-art pipelines often rely on explicit structural heuristics or memory-intensive node embeddings -- approaches that struggle to generalize or scale to massive graphs. Emerging Graph Transformers (GTs) offer a potential alternative but often incur significant overhead due to complex structural encodings, hindering their applications to large-scale link prediction. We challenge these sophisticated paradigms with PENCIL, an encoder-only plain Transformer that replaces hand-crafted priors with attention over sampled local subgraphs, retaining the scalability and hardware efficiency of standard Transformers. Through experimental and theoretical analysis, we show that PENCIL extracts richer structural signals than GNNs, implicitly generalizing a broad class of heuristics and subgraph-based expressivity. Empirically, PENCIL outperforms heuristic-informed GNNs and is far more parameter-efficient than ID-embedding--based alternatives, while remaining competitive across diverse benchmarks -- even without node features. Our results challenge the prevailing reliance on complex engineering techniques, demonstrating that simple design choices are potentially sufficient to achieve the same capabilities.

</details>


### [1024] [A Lightweight Sparse Interaction Network for Time Series Forecasting](https://arxiv.org/abs/2602.01585)
*Xu Zhang,Qitong Wang,Peng Wang,Wei Wang*

Main category: cs.LG

Relevance: 65.0

TL;DR: LSINet：用于时间序列预测的轻量级稀疏交互网络，通过多头部稀疏交互机制和共享交互学习，在线性模型框架下实现显式时序交互，在精度和效率上超越现有线性模型和Transformer模型。


<details>
  <summary>Details</summary>
Motivation: 现有线性模型在长期时间序列预测中虽然优于某些Transformer模型，但仅通过堆叠MLP结构隐式进行时序交互，可能无法充分捕捉复杂的时间依赖关系，性能仍有提升空间。

Method: 提出LSINet，包含：1）多头部稀疏交互机制（MSIM），通过学习稀疏诱导的伯努利分布来捕捉时间步之间的重要连接；2）自适应正则化损失确保稀疏性；3）共享交互学习（SIL）利用时序交互的可共享性提升效率和收敛性。

Result: 在公开数据集上的实验表明，LSINet在时间序列预测任务中，相比先进的线性模型和Transformer模型，实现了更高的准确率和更好的效率。

Conclusion: LSINet在线性模型框架下引入了显式的时序交互机制，通过稀疏交互和共享学习，在保持低开销的同时显著提升了时间序列预测的性能。

Abstract: Recent work shows that linear models can outperform several transformer models in long-term time-series forecasting (TSF). However, instead of explicitly performing temporal interaction through self-attention, linear models implicitly perform it based on stacked MLP structures, which may be insufficient in capturing the complex temporal dependencies and their performance still has potential for improvement. To this end, we propose a Lightweight Sparse Interaction Network (LSINet) for TSF task. Inspired by the sparsity of self-attention, we propose a Multihead Sparse Interaction Mechanism (MSIM). Different from self-attention, MSIM learns the important connections between time steps through sparsity-induced Bernoulli distribution to capture temporal dependencies for TSF. The sparsity is ensured by the proposed self-adaptive regularization loss. Moreover, we observe the shareability of temporal interactions and propose to perform Shared Interaction Learning (SIL) for MSIM to further enhance efficiency and improve convergence. LSINet is a linear model comprising only MLP structures with low overhead and equipped with explicit temporal interaction mechanisms. Extensive experiments on public datasets show that LSINet achieves both higher accuracy and better efficiency than advanced linear models and transformer models in TSF tasks. The code is available at the link https://github.com/Meteor-Stars/LSINet.

</details>


### [1025] [Boosting Maximum Entropy Reinforcement Learning via One-Step Flow Matching](https://arxiv.org/abs/2602.01606)
*Zeqiao Li,Yijing Wang,Haoyu Wang,Zheng Li,Zhiqiang Zuo*

Main category: cs.LG

Relevance: 65.0

TL;DR: FLAME提出了一种基于流匹配的最大熵强化学习框架，通过重要性重加权绕过配分函数估计，设计解耦的熵估计器纠正偏差，实现高效探索和一步控制，在保持性能的同时显著降低推理延迟。


<details>
  <summary>Details</summary>
Motivation: 扩散策略表达能力强但推理延迟高，流匹配可实现一步生成但难以集成到最大熵强化学习中，因为最优策略是难以处理的基于能量的分布，且高效的似然估计存在严重的离散化偏差。

Method: 1) 推导Q-重加权流匹配目标，通过重要性重加权绕过配分函数估计；2) 设计解耦的熵估计器，严格纠正偏差，实现高效探索；3) 集成MeanFlow公式，实现表达性强且高效的一步控制。

Result: 在MuJoCo实验中，FLAME优于高斯基线，与多步扩散策略性能相当，但推理成本显著降低。

Conclusion: FLAME提供了一个原则性框架，解决了流匹配在最大熵强化学习中的集成挑战，实现了表达性强、探索高效且推理成本低的一步控制策略。

Abstract: Diffusion policies are expressive yet incur high inference latency. Flow Matching (FM) enables one-step generation, but integrating it into Maximum Entropy Reinforcement Learning (MaxEnt RL) is challenging: the optimal policy is an intractable energy-based distribution, and the efficient log-likelihood estimation required to balance exploration and exploitation suffers from severe discretization bias. We propose \textbf{F}low-based \textbf{L}og-likelihood-\textbf{A}ware \textbf{M}aximum \textbf{E}ntropy RL (\textbf{FLAME}), a principled framework that addresses these challenges. First, we derive a Q-Reweighted FM objective that bypasses partition function estimation via importance reweighting. Second, we design a decoupled entropy estimator that rigorously corrects bias, which enables efficient exploration and brings the policy closer to the optimal MaxEnt policy. Third, we integrate the MeanFlow formulation to achieve expressive and efficient one-step control. Empirical results on MuJoCo show that FLAME outperforms Gaussian baselines and matches multi-step diffusion policies with significantly lower inference cost. Code is available at https://github.com/lzqw/FLAME.

</details>


### [1026] [SUSD: Structured Unsupervised Skill Discovery through State Factorization](https://arxiv.org/abs/2602.01619)
*Seyed Mohammad Hadi Hosseini,Mahdieh Soleymani Baghshah*

Main category: cs.LG

Relevance: 65.0

TL;DR: SUSD提出了一种基于环境因子分解的无监督技能发现框架，通过将状态空间分解为独立组件（如对象或可控实体），为不同因子分配不同的技能变量，从而发现更丰富多样的技能。


<details>
  <summary>Details</summary>
Motivation: 现有的无监督技能发现方法存在局限性：基于互信息的方法倾向于发现简单静态技能，而基于距离最大化的方法虽然能促进动态技能，但仍无法充分利用环境的结构化特性来发现全面的技能集。

Method: SUSD将状态空间分解为独立因子（如对象或可控实体），为每个因子分配独立的技能变量。通过动态模型跟踪各因子的学习进度，自适应地将智能体注意力转向未充分探索的因子，实现结构化技能发现。

Result: 在1-10个因子的三个环境中，SUSD显著优于现有无监督技能发现方法，能够发现更多样化和复杂的技能，并产生因子化的技能表示，便于通过分层强化学习高效训练组合下游任务。

Conclusion: SUSD通过利用环境的组合结构，实现了更精细的技能发现控制，不仅促进了更丰富多样的技能发现，还产生了可分解的技能表示，为组合任务的高效训练提供了基础。

Abstract: Unsupervised Skill Discovery (USD) aims to autonomously learn a diverse set of skills without relying on extrinsic rewards. One of the most common USD approaches is to maximize the Mutual Information (MI) between skill latent variables and states. However, MI-based methods tend to favor simple, static skills due to their invariance properties, limiting the discovery of dynamic, task-relevant behaviors. Distance-Maximizing Skill Discovery (DSD) promotes more dynamic skills by leveraging state-space distances, yet still fall short in encouraging comprehensive skill sets that engage all controllable factors or entities in the environment. In this work, we introduce SUSD, a novel framework that harnesses the compositional structure of environments by factorizing the state space into independent components (e.g., objects or controllable entities). SUSD allocates distinct skill variables to different factors, enabling more fine-grained control on the skill discovery process. A dynamic model also tracks learning across factors, adaptively steering the agent's focus toward underexplored factors. This structured approach not only promotes the discovery of richer and more diverse skills, but also yields a factorized skill representation that enables fine-grained and disentangled control over individual entities which facilitates efficient training of compositional downstream tasks via Hierarchical Reinforcement Learning (HRL). Our experimental results across three environments, with factors ranging from 1 to 10, demonstrate that our method can discover diverse and complex skills without supervision, significantly outperforming existing unsupervised skill discovery methods in factorized and complex environments. Code is publicly available at: https://github.com/hadi-hosseini/SUSD.

</details>


### [1027] [The Effect of Mini-Batch Noise on the Implicit Bias of Adam](https://arxiv.org/abs/2602.01642)
*Matias D. Cattaneo,Boris Shigida*

Main category: cs.LG

Relevance: 65.0

TL;DR: 本文分析了Adam优化器中动量超参数(β₁, β₂)和批次大小如何通过小批量噪声影响多轮训练中的隐式偏置，发现批次大小变化会反转β₁和β₂对泛化的影响方向，并连接了这种转变的规模与临界批次规模。


<details>
  <summary>Details</summary>
Motivation: 随着高质量数据有限而计算资源增长，多轮训练在深度学习各领域重新变得重要。Adam(W)作为许多任务（如下一个token预测）的首选优化器，其动量超参数(β₁, β₂)控制记忆，而批次大小控制小批量噪声。需要理解小批量噪声如何影响Adam中记忆的隐式偏置，以及这种偏置如何影响损失景观的尖锐或平坦区域，这与多轮训练中的泛化差距相关。

Method: 引入理论框架分析小批量噪声如何影响Adam中记忆的隐式偏置。理论推导连接了批次大小转变的规模与临界批次规模。在即将过拟合的小规模数据上进行实验验证。

Result: 发现批次大小变化会反转β₁和β₂对泛化的影响方向：大批次时，较高的β₂增加记忆的反正则化（损害泛化）；小批次时，这种依赖关系反转。类似但方向相反的单调性转变发生在β₁上。常用的默认参数对(0.9, 0.999)适合小批次；对于大批次，在许多情况下将β₁移近β₂能显著提高多轮训练中的验证准确率。

Conclusion: Adam优化器的动量超参数(β₁, β₂)与批次大小之间存在复杂的相互作用，批次大小变化会反转它们对泛化的影响方向。这为多轮训练中Adam超参数调优提供了理论指导，特别是在不同批次规模下的优化策略。

Abstract: With limited high-quality data and growing compute, multi-epoch training is gaining back its importance across sub-areas of deep learning. Adam(W), versions of which are go-to optimizers for many tasks such as next token prediction, has two momentum hyperparameters $(β_1, β_2)$ controlling memory and one very important hyperparameter, batch size, controlling (in particular) the amount mini-batch noise. We introduce a theoretical framework to understand how mini-batch noise influences the implicit bias of memory in Adam (depending on $β_1$, $β_2$) towards sharper or flatter regions of the loss landscape, which is commonly observed to correlate with the generalization gap in multi-epoch training. We find that in the case of large batch sizes, higher $β_2$ increases the magnitude of anti-regularization by memory (hurting generalization), but as the batch size becomes smaller, the dependence of (anti-)regulariation on $β_2$ is reversed. A similar monotonicity shift (in the opposite direction) happens in $β_1$. In particular, the commonly "default" pair $(β_1, β_2) = (0.9, 0.999)$ is a good choice if batches are small; for larger batches, in many settings moving $β_1$ closer to $β_2$ is much better in terms of validation accuracy in multi-epoch training. Moreover, our theoretical derivations connect the scale of the batch size at which the shift happens to the scale of the critical batch size. We illustrate this effect in experiments with small-scale data in the about-to-overfit regime.

</details>


### [1028] [ASGMamba: Adaptive Spectral Gating Mamba for Multivariate Time Series Forecasting](https://arxiv.org/abs/2602.01668)
*Qianyang Li,Xingjun Zhang,Shaoxun Wang,Jia Wei,Yueqi Xing*

Main category: cs.LG

Relevance: 65.0

TL;DR: ASGMamba：一种用于资源受限超算环境的高效多变量时间序列预测框架，结合自适应谱门控机制和Mamba骨干网络，在保持线性复杂度的同时实现SOTA精度


<details>
  <summary>Details</summary>
Motivation: 现有方法面临两难：Transformer模型具有二次复杂度，限制了长序列的可扩展性；而线性状态空间模型（SSMs）难以区分有价值信号与高频噪声，导致状态容量浪费。需要一种在资源受限的超算环境中既高效又准确的解决方案。

Method: 提出ASGMamba框架：1）轻量级自适应谱门控（ASG）机制，基于局部谱能量动态过滤噪声；2）Mamba骨干网络专注于鲁棒的时间动态；3）分层多尺度架构，包含变量特定的节点嵌入以捕捉不同的物理特性

Result: 在9个基准测试中达到最先进的精度，同时保持严格的O(L)线性复杂度，显著减少长时域任务的内存使用，成为资源受限环境中高吞吐量预测的可扩展解决方案

Conclusion: ASGMamba成功解决了Transformer和SSM模型在长时多变量时间序列预测中的局限性，为资源受限的超算环境提供了高效且准确的预测框架

Abstract: Long-term multivariate time series forecasting (LTSF) plays a crucial role in various high-performance computing applications, including real-time energy grid management and large-scale traffic flow simulation. However, existing solutions face a dilemma: Transformer-based models suffer from quadratic complexity, limiting their scalability on long sequences, while linear State Space Models (SSMs) often struggle to distinguish valuable signals from high-frequency noise, leading to wasted state capacity. To bridge this gap, we propose ASGMamba, an efficient forecasting framework designed for resource-constrained supercomputing environments. ASGMamba integrates a lightweight Adaptive Spectral Gating (ASG) mechanism that dynamically filters noise based on local spectral energy, enabling the Mamba backbone to focus its state evolution on robust temporal dynamics. Furthermore, we introduce a hierarchical multi-scale architecture with variable-specific Node Embeddings to capture diverse physical characteristics. Extensive experiments on nine benchmarks demonstrate that ASGMamba achieves state-of-the-art accuracy. While keeping strictly $$\mathcal{O}(L)$$ complexity we significantly reduce the memory usage on long-horizon tasks, thus establishing ASGMamba as a scalable solution for high-throughput forecasting in resource limited environments.The code is available at https://github.com/hit636/ASGMamba

</details>


### [1029] [Position: The Inevitable End of One-Architecture-Fits-All-Domains in Time Series Forecasting](https://arxiv.org/abs/2602.01736)
*Qinwei Ma,Jingzhe Shi,Jiahao Qiu,Zaiwen Yang*

Main category: cs.LG

Relevance: 65.0

TL;DR: 该论文质疑通用时间序列预测神经网络架构的有效性，指出其在特定领域（金融、天气、交通等）表现不佳，呼吁研究重点转向特定领域方法或元学习方法。


<details>
  <summary>Details</summary>
Motivation: 论文动机源于观察到通用时间序列预测神经网络架构存在固有局限性：单一领域SOTA与通用领域泛化能力之间存在不可调和的冲突。这些架构变得越来越复杂，但性能已趋于饱和，对特定领域的实际应用启发有限。

Method: 论文采用批判性分析方法，总结现有研究对时间序列预测神经网络架构有效性和鲁棒性的质疑，分析其固有局限性，并提出研究方向的转变建议。

Result: 分析表明通用时间序列预测神经网络架构研究已趋于饱和，与特定领域SOTA差距逐渐拉大。特定领域（金融、天气、交通等）开发的方法很少利用近2-3年时间序列社区的神经网络架构进展。

Conclusion: 结论呼吁时间序列社区将研究重点从通用领域的时间序列神经网络架构转向：(1) 特定领域的深度学习方法，或(2) 通用领域的元学习方法开发。

Abstract: Recent work has questioned the effectiveness and robustness of neural network architectures for time series forecasting tasks. We summarize these concerns and analyze groundly their inherent limitations: i.e. the irreconcilable conflict between single (or few similar) domains SOTA and generalizability over general domains for time series forecasting neural network architecture designs. Moreover, neural networks architectures for general domain time series forecasting are becoming more and more complicated and their performance has almost saturated in recent years. As a result, network architectures developed aiming at fitting general time series domains are almost not inspiring for real world practices for certain single (or few similar) domains such as Finance, Weather, Traffic, etc: each specific domain develops their own methods that rarely utilize advances in neural network architectures of time series community in recent 2-3 years. As a result, we call for the time series community to shift focus away from research on time series neural network architectures for general domains: these researches have become saturated and away from domain-specific SOTAs over time. We should either (1) focus on deep learning methods for certain specific domain(s), or (2) turn to the development of meta-learning methods for general domains.

</details>


### [1030] [Position: Beyond Model-Centric Prediction -- Agentic Time Series Forecasting](https://arxiv.org/abs/2602.01776)
*Mingyue Cheng,Xiaoyu Tao,Qi Liu,Ze Guo,Enhong Chen*

Main category: cs.LG

Relevance: 65.0

TL;DR: 本文提出"代理时间序列预测(ATSF)"新范式，将传统静态单次预测重构为包含感知、规划、行动、反思、记忆的代理过程，强调通过工具交互、反馈学习和经验积累实现自适应多轮预测。


<details>
  <summary>Details</summary>
Motivation: 传统时间序列预测作为模型中心、静态、单次预测问题存在局限性，无法适应需要信息特征提取、推理驱动推断、迭代优化和持续时间适应的自适应多轮场景。

Method: 提出代理时间序列预测(ATSF)框架，将预测重构为包含感知、规划、行动、反思、记忆的代理过程。提出三种实现范式：基于工作流的设计、代理强化学习、混合代理工作流范式。

Result: 建立了代理预测作为时间序列预测未来研究的基础框架，讨论了从模型中心预测转向代理预测的机遇与挑战。

Conclusion: 代理时间序列预测为时间序列预测研究提供了新范式，强调组织预测作为代理工作流，能够与工具交互、整合结果反馈、通过经验积累演进。

Abstract: Time series forecasting has traditionally been formulated as a model-centric, static, and single-pass prediction problem that maps historical observations to future values. While this paradigm has driven substantial progress, it proves insufficient in adaptive and multi-turn settings where forecasting requires informative feature extraction, reasoning-driven inference, iterative refinement, and continual adaptation over time. In this paper, we argue for agentic time series forecasting (ATSF), which reframes forecasting as an agentic process composed of perception, planning, action, reflection, and memory. Rather than focusing solely on predictive models, ATSF emphasizes organizing forecasting as an agentic workflow that can interact with tools, incorporate feedback from outcomes, and evolve through experience accumulation. We outline three representative implementation paradigms -- workflow-based design, agentic reinforcement learning, and a hybrid agentic workflow paradigm -- and discuss the opportunities and challenges that arise when shifting from model-centric prediction to agentic forecasting. Together, this position aims to establish agentic forecasting as a foundation for future research at the intersection of time series forecasting.

</details>


### [1031] [Stein-Rule Shrinkage for Stochastic Gradient Estimation in High Dimensions](https://arxiv.org/abs/2602.01777)
*M. Arashi,M. Amintoosi*

Main category: cs.LG

Relevance: 65.0

TL;DR: 提出基于Stein规则收缩的随机梯度估计器，通过收缩噪声小批量梯度向动量历史估计，在Adam优化器中实现，在大批量训练中提升性能


<details>
  <summary>Details</summary>
Motivation: 传统随机梯度方法将小批量梯度视为无偏估计，但统计决策理论表明在二次损失下无偏估计通常不可接受，特别是在高维设置中，标准随机梯度可能从风险角度是次优的

Method: 将随机梯度计算形式化为高维估计问题，引入基于Stein规则收缩的决策理论框架。构建收缩梯度估计器，自适应地将噪声小批量梯度收缩到从历史动量导出的稳定受限估计器。收缩强度使用梯度噪声方差的在线估计数据驱动确定，利用自适应优化方法通常维护的二阶矩统计量

Result: 在高斯噪声模型和维度p≥3条件下，提出的估计器在平方误差损失下一致优于标准随机梯度，并在经典决策理论意义下是最小最大最优的。在CIFAR10和CIFAR100上，在不同标签噪声水平下，在大批量训练中相比Adam获得一致改进。消融研究表明增益主要来自选择性地对高维卷积层应用收缩

Conclusion: 经典收缩原则为改进现代深度学习中的随机梯度估计提供了原则性且有效的方法，可以集成到Adam等优化器中，计算开销可忽略

Abstract: Stochastic gradient methods are central to large-scale learning, yet their analysis typically treats mini-batch gradients as unbiased estimators of the population gradient. In high-dimensional settings, however, classical results from statistical decision theory show that unbiased estimators are generally inadmissible under quadratic loss, suggesting that standard stochastic gradients may be suboptimal from a risk perspective. In this work, we formulate stochastic gradient computation as a high-dimensional estimation problem and introduce a decision-theoretic framework based on Stein-rule shrinkage. We construct a shrinkage gradient estimator that adaptively contracts noisy mini-batch gradients toward a stable restricted estimator derived from historical momentum. The shrinkage intensity is determined in a data-driven manner using an online estimate of gradient noise variance, leveraging second-moment statistics commonly maintained by adaptive optimization methods. Under a Gaussian noise model and for dimension p>=3, we show that the proposed estimator uniformly dominates the standard stochastic gradient under squared error loss and is minimax-optimal in the classical decision-theoretic sense. We further demonstrate how this estimator can be incorporated into the Adam optimizer, yielding a practical algorithm with negligible additional computational cost. Empirical evaluations on CIFAR10 and CIFAR100, across multiple levels of label noise, show consistent improvements over Adam in the large-batch regime. Ablation studies indicate that the gains arise primarily from selectively applying shrinkage to high-dimensional convolutional layers, while indiscriminate shrinkage across all parameters degrades performance. These results illustrate that classical shrinkage principles provide a principled and effective approach to improving stochastic gradient estimation in modern deep learning.

</details>


### [1032] [Self-Rewarding Sequential Monte Carlo for Masked Diffusion Language Models](https://arxiv.org/abs/2602.01849)
*Ziwei Luo,Ziqi Jin,Lei Wang,Lidong Bing,Thomas B. Schön*

Main category: cs.LG

Relevance: 65.0

TL;DR: 提出自奖励序列蒙特卡洛（SMC）方法，通过并行多个交互扩散过程（粒子）来改进掩码扩散语言模型的采样质量，使用轨迹级置信度作为自奖励信号分配粒子权重，无需额外训练即可提升生成多样性和质量。


<details>
  <summary>Details</summary>
Motivation: 现有掩码扩散语言模型（MDLMs）主要依赖基于置信度的采样策略，只保留每步预测置信度最高的token，这限制了生成路径的多样性，导致噪声敏感和贪婪解码问题。需要一种方法在推理时扩展采样能力，提高生成质量而不需要额外训练。

Method: 提出自奖励序列蒙特卡洛方法：1）并行启动多个交互扩散过程（粒子）进行轨迹探索；2）引入轨迹级置信度作为自奖励信号分配粒子重要性权重；3）在采样过程中迭代加权和重采样，系统性地引导生成朝向全局置信度高、质量好的样本。

Result: 在多种掩码扩散语言模型和基准测试上验证，取得了显著改进，无需额外训练或奖励指导，有效将并行推理能力转化为采样质量提升。

Conclusion: 自奖励SMC方法成功解决了MDLMs采样多样性受限的问题，通过并行粒子探索和轨迹级置信度加权，实现了推理时的高质量采样扩展。

Abstract: This work presents self-rewarding sequential Monte Carlo (SMC), an inference-time scaling algorithm enabling effective sampling of masked diffusion language models (MDLMs). Our algorithm stems from the observation that most existing MDLMs rely on a confidence-based sampling strategy, where only tokens with the highest prediction confidence are preserved at each step. This restricts the generation to a noise-sensitive, greedy decoding paradigm, resulting in an inevitable collapse in the diversity of possible paths. We address this problem by launching multiple interacting diffusion processes in parallel, referred to as particles, for trajectory exploration. Importantly, we introduce the trajectory-level confidence as a self-rewarding signal for assigning particle importance weights. During sampling, particles are iteratively weighted and resampled to systematically steer generation towards globally confident, high-quality samples. Our self-rewarding SMC is verified on various masked diffusion language models and benchmarks, achieving significant improvement without extra training or reward guidance, while effectively converting parallel inference capacity into improved sampling quality. Our code is available at https://github.com/Algolzw/self-rewarding-smc.

</details>


### [1033] [Designing Time Series Experiments in A/B Testing with Transformer Reinforcement Learning](https://arxiv.org/abs/2602.01853)
*Xiangkun Wu,Qianglin Wen,Yingying Zhang,Hongtu Zhu,Ting Li,Chengchun Shi*

Main category: cs.LG

Relevance: 65.0

TL;DR: 提出基于Transformer强化学习的时间序列实验设计方法，通过Transformer利用完整历史信息，用RL直接优化MSE，在合成数据、调度模拟器和真实网约车数据上优于现有方法


<details>
  <summary>Details</summary>
Motivation: A/B测试在时间序列实验中面临挑战：现有方法无法充分利用完整历史信息，且依赖强假设来近似目标函数。时间序列的动态依赖性使得不利用完整历史会导致次优设计

Method: 提出Transformer强化学习方法：1) 使用Transformer编码完整历史信息进行条件分配；2) 采用强化学习直接优化均方误差(MSE)，避免依赖限制性假设

Result: 在合成数据、公开调度模拟器和真实网约车数据集上的实验表明，该方法一致优于现有设计方法

Conclusion: 通过结合Transformer和强化学习，解决了时间序列A/B测试中的历史信息利用和目标函数优化问题，为时间序列实验设计提供了更有效的解决方案

Abstract: A/B testing has become a gold standard for modern technological companies to conduct policy evaluation. Yet, its application to time series experiments, where policies are sequentially assigned over time, remains challenging. Existing designs suffer from two limitations: (i) they do not fully leverage the entire history for treatment allocation; (ii) they rely on strong assumptions to approximate the objective function (e.g., the mean squared error of the estimated treatment effect) for optimizing the design. We first establish an impossibility theorem showing that failure to condition on the full history leads to suboptimal designs, due to the dynamic dependencies in time series experiments. To address both limitations simultaneously, we next propose a transformer reinforcement learning (RL) approach which leverages transformers to condition allocation on the entire history and employs RL to directly optimize the MSE without relying on restrictive assumptions. Empirical evaluations on synthetic data, a publicly available dispatch simulator, and a real-world ridesharing dataset demonstrate that our proposal consistently outperforms existing designs.

</details>


### [1034] [Grounding Generated Videos in Feasible Plans via World Models](https://arxiv.org/abs/2602.01960)
*Christos Ziakas,Amir Bar,Alessandra Russo*

Main category: cs.LG

Relevance: 65.0

TL;DR: GVP-WM通过世界模型将视频生成计划转化为可行动作序列，解决视频规划中的时间一致性和物理约束问题


<details>
  <summary>Details</summary>
Motivation: 大规模视频生成模型作为零样本视觉规划器展现出潜力，但生成的视频计划经常违反时间一致性和物理约束，导致映射到可执行动作时失败

Method: 提出GVP-WM方法：1) 从初始和目标观测生成视频计划；2) 通过视频引导的潜在共位法将视频指导投影到动态可行的潜在轨迹流形上；3) 将接地问题表述为目标条件潜在空间轨迹优化，在保持与视频计划语义对齐的同时优化潜在状态和动作

Result: GVP-WM能够从违反物理约束的零样本图像到视频生成和运动模糊视频中恢复可行的长时程计划，在导航和操作模拟任务中表现良好

Conclusion: 通过世界模型将视频生成计划接地到可行动作序列的方法有效解决了视频规划中的物理约束问题，提高了规划的可执行性

Abstract: Large-scale video generative models have shown emerging capabilities as zero-shot visual planners, yet video-generated plans often violate temporal consistency and physical constraints, leading to failures when mapped to executable actions. To address this, we propose Grounding Video Plans with World Models (GVP-WM), a planning method that grounds video-generated plans into feasible action sequences using a learned action-conditioned world model. At test-time, GVP-WM first generates a video plan from initial and goal observations, then projects the video guidance onto the manifold of dynamically feasible latent trajectories via video-guided latent collocation. In particular, we formulate grounding as a goal-conditioned latent-space trajectory optimization problem that jointly optimizes latent states and actions under world-model dynamics, while preserving semantic alignment with the video-generated plan. Empirically, GVP-WM recovers feasible long-horizon plans from zero-shot image-to-video-generated and motion-blurred videos that violate physical constraints, across navigation and manipulation simulation tasks.

</details>


### [1035] [FlyPrompt: Brain-Inspired Random-Expanded Routing with Temporal-Ensemble Experts for General Continual Learning](https://arxiv.org/abs/2602.01976)
*Hongwei Yan,Guanglong Sun,Kanglei Zhou,Qian Li,Liyuan Wang,Yi Zhong*

Main category: cs.LG

Relevance: 65.0

TL;DR: FlyPrompt是一个受果蝇大脑启发的通用持续学习框架，通过专家路由和专家能力改进解决单次训练、无任务边界的持续学习问题，在多个数据集上显著优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有持续参数高效调优方法通常需要多轮训练和明确任务边界，难以适应通用持续学习场景。同时缺乏对两个核心问题的针对性设计：如何为演化数据分布分配专家参数，以及如何在有限监督下提升表示能力。

Method: 受果蝇分层记忆系统启发，FlyPrompt将通用持续学习分解为专家路由和专家能力改进两个子问题。使用随机扩展分析路由器进行实例级专家激活，并通过输出头的时序集成动态适应决策边界。

Result: 在CIFAR-100、ImageNet-R和CUB-200数据集上分别取得11.23%、12.43%和7.62%的性能提升，显著优于现有最先进方法。

Conclusion: FlyPrompt通过脑启发的稀疏扩展和模块化集成设计，有效解决了通用持续学习中的参数分配和表示能力问题，为持续学习提供了新的解决方案。

Abstract: General continual learning (GCL) challenges intelligent systems to learn from single-pass, non-stationary data streams without clear task boundaries. While recent advances in continual parameter-efficient tuning (PET) of pretrained models show promise, they typically rely on multiple training epochs and explicit task cues, limiting their effectiveness in GCL scenarios. Moreover, existing methods often lack targeted design and fail to address two fundamental challenges in continual PET: how to allocate expert parameters to evolving data distributions, and how to improve their representational capacity under limited supervision. Inspired by the fruit fly's hierarchical memory system characterized by sparse expansion and modular ensembles, we propose FlyPrompt, a brain-inspired framework that decomposes GCL into two subproblems: expert routing and expert competence improvement. FlyPrompt introduces a randomly expanded analytic router for instance-level expert activation and a temporal ensemble of output heads to dynamically adapt decision boundaries over time. Extensive theoretical and empirical evaluations demonstrate FlyPrompt's superior performance, achieving up to 11.23%, 12.43%, and 7.62% gains over state-of-the-art baselines on CIFAR-100, ImageNet-R, and CUB-200, respectively. Our source code is available at https://github.com/AnAppleCore/FlyGCL.

</details>


### [1036] [Logic-Guided Vector Fields for Constrained Generative Modeling](https://arxiv.org/abs/2602.02009)
*Ali Baheri*

Main category: cs.LG

Relevance: 65.0

TL;DR: 提出Logic-Guided Vector Fields (LGVF)框架，将符号逻辑约束注入流匹配生成模型，通过训练时逻辑损失和推理时约束梯度调整，显著减少约束违反


<details>
  <summary>Details</summary>
Motivation: 神经符号系统旨在结合符号逻辑的表达结构和神经学习的灵活性，但现有生成模型缺乏在生成时强制执行声明性约束的机制。需要一种方法将符号知识注入生成过程，确保输出满足逻辑约束。

Method: 提出LGVF框架，包含两个互补机制：1) 训练时逻辑损失，沿连续流轨迹惩罚约束违反，权重强调目标分布附近的正确性；2) 推理时调整，使用约束梯度引导采样，作为对学习动态的轻量级逻辑修正。

Result: 在三个约束生成案例（线性、非线性、多区域可行性约束）中，LGVF相比标准流匹配减少59-82%的约束违反，每个案例都达到最低违反率。在线性和环形设置中，还改善了MMD测量的分布保真度。

Conclusion: LGVF成功将符号约束注入生成模型，显著减少约束违反并保持分布质量。框架展现出约束感知的向量场，具有避免障碍的涌现行为，无需显式路径规划。

Abstract: Neuro-symbolic systems aim to combine the expressive structure of symbolic logic with the flexibility of neural learning; yet, generative models typically lack mechanisms to enforce declarative constraints at generation time. We propose Logic-Guided Vector Fields (LGVF), a neuro-symbolic framework that injects symbolic knowledge, specified as differentiable relaxations of logical constraints, into flow matching generative models. LGVF couples two complementary mechanisms: (1) a training-time logic loss that penalizes constraint violations along continuous flow trajectories, with weights that emphasize correctness near the target distribution; and (2) an inference-time adjustment that steers sampling using constraint gradients, acting as a lightweight, logic-informed correction to the learned dynamics. We evaluate LGVF on three constrained generation case studies spanning linear, nonlinear, and multi-region feasibility constraints. Across all settings, LGVF reduces constraint violations by 59-82% compared to standard flow matching and achieves the lowest violation rates in each case. In the linear and ring settings, LGVF also improves distributional fidelity as measured by MMD, while in the multi-obstacle setting, we observe a satisfaction-fidelity trade-off, with improved feasibility but increased MMD. Beyond quantitative gains, LGVF yields constraint-aware vector fields exhibiting emergent obstacle-avoidance behavior, routing samples around forbidden regions without explicit path planning.

</details>


### [1037] [Robust Domain Generalization under Divergent Marginal and Conditional Distributions](https://arxiv.org/abs/2602.02015)
*Jewon Yeom,Kyubyung Chae,Hyunggyu Lim,Yoonna Oh,Dongyoon Yang,Taesup Kim*

Main category: cs.LG

Relevance: 65.0

TL;DR: 提出一个统一的领域泛化框架，处理现实世界中同时存在的边际分布和条件分布偏移问题，通过分解风险界限和元学习实现鲁棒泛化。


<details>
  <summary>Details</summary>
Motivation: 现有领域泛化方法主要关注条件分布偏移（P(X|Y)变化），假设标签分布P(Y)稳定。但现实多领域场景常同时存在边际标签分布和条件分布偏移，需要统一框架处理这种复合分布偏移。

Method: 1) 推导新的风险界限，将联合分布分解为边际和条件分量，明确表征两种偏移源的风险差距；2) 设计元学习过程，在可见领域上最小化和验证该风险界限，确保对未见领域的强泛化能力。

Result: 在传统领域泛化基准和具有显著边际与条件偏移的挑战性多领域长尾识别设置中，均实现了最先进的性能。

Conclusion: 提出的统一框架能有效处理现实世界中的复合分布偏移问题，在多种场景下实现鲁棒的领域泛化。

Abstract: Domain generalization (DG) aims to learn predictive models that can generalize to unseen domains. Most existing DG approaches focus on learning domain-invariant representations under the assumption of conditional distribution shift (i.e., primarily addressing changes in $P(X\mid Y)$ while assuming $P(Y)$ remains stable). However, real-world scenarios with multiple domains often involve compound distribution shifts where both the marginal label distribution $P(Y)$ and the conditional distribution $P(X\mid Y)$ vary simultaneously. To address this, we propose a unified framework for robust domain generalization under divergent marginal and conditional distributions. We derive a novel risk bound for unseen domains by explicitly decomposing the joint distribution into marginal and conditional components and characterizing risk gaps arising from both sources of divergence. To operationalize this bound, we design a meta-learning procedure that minimizes and validates the proposed risk bound across seen domains, ensuring strong generalization to unseen ones. Empirical evaluations demonstrate that our method achieves state-of-the-art performance not only on conventional DG benchmarks but also in challenging multi-domain long-tailed recognition settings where both marginal and conditional shifts are pronounced.

</details>


### [1038] [On Stability and Robustness of Diffusion Posterior Sampling for Bayesian Inverse Problems](https://arxiv.org/abs/2602.02045)
*Yiming Yang,Xiaoyuan Cheng,Yi He,Kaiyu Li,Wenxuan Yuan,Zhuo Sun*

Main category: cs.LG

Relevance: 65.0

TL;DR: 该论文研究了扩散模型在贝叶斯逆问题中的稳定性与鲁棒性问题，提出了鲁棒扩散后验采样方法以解决似然函数不匹配问题。


<details>
  <summary>Details</summary>
Motivation: 扩散模型已成为贝叶斯逆问题的强大先验，但现有扩散求解器依赖于预设的观测似然函数。当预设似然与真实数据生成过程不匹配时，扩散求解器缺乏鲁棒性，这会降低性能。目前关于似然函数与恢复质量之间的关系以及扩散求解器的鲁棒性问题尚未得到充分研究。

Method: 首先理论分析了扩散后验近似误差并证明了扩散求解器的稳定性。基于稳定性分析发现扩散求解器缺乏鲁棒性，提出了鲁棒扩散后验采样方法，该方法与现有基于梯度的后验采样器兼容，能够处理似然函数不匹配问题。

Result: 理论分析揭示了扩散求解器的稳定性特性，同时暴露了其在似然不匹配情况下的脆弱性。提出的鲁棒扩散后验采样方法在科学逆问题和自然图像任务中表现出有效性，在具有挑战性的似然不匹配情况下显示出一致的性能提升。

Conclusion: 该工作填补了扩散模型在贝叶斯逆问题中稳定性分析的理论空白，揭示了扩散求解器的鲁棒性问题，并提出了一种简单有效的解决方案。该方法为处理实际应用中常见的似然函数不匹配问题提供了理论保证和实践工具。

Abstract: Diffusion models have recently emerged as powerful learned priors for Bayesian inverse problems (BIPs). Diffusion-based solvers rely on a presumed likelihood for the observations in BIPs to guide the generation process. However, the link between likelihood and recovery quality for BIPs is unclear in previous works. We bridge this gap by characterizing the posterior approximation error and proving the \emph{stability} of the diffusion-based solvers. Meanwhile, an immediate result of our findings on stability demonstrates the lack of robustness in diffusion-based solvers, which remains unexplored. This can degrade performance when the presumed likelihood mismatches the unknown true data generation processes. To address this issue, we propose a simple yet effective solution, \emph{robust diffusion posterior sampling}, which is provably \emph{robust} and compatible with existing gradient-based posterior samplers. Empirical results on scientific inverse problems and natural image tasks validate the effectiveness and robustness of our method, showing consistent performance improvements under challenging likelihood misspecifications.

</details>


### [1039] [Probabilistic Performance Guarantees for Multi-Task Reinforcement Learning](https://arxiv.org/abs/2602.02098)
*Yannik Schnitzer,Mathias Jackermeier,Alessandro Abate,David Parker*

Main category: cs.LG

Relevance: 65.0

TL;DR: 该论文提出了一种为多任务强化学习策略在新任务上性能提供高置信度保证的方法，通过组合任务级泛化边界和每任务置信下界来实现。


<details>
  <summary>Details</summary>
Motivation: 当前多任务强化学习方法缺乏形式化性能保证，这在安全关键场景中部署策略时是不可或缺的。现有方法很少提供对新任务性能的正式保证。

Method: 引入新的泛化边界，组合两个部分：(1) 从有限次rollout中获得的每任务置信下界；(2) 从有限采样任务中获得的任务级泛化边界，从而为从相同未知分布中抽取的新任务提供高置信度保证。

Result: 在多种最先进的多任务RL方法上验证，证明该方法在理论上是可靠的，并且在现实样本量下提供的信息是有意义的。

Conclusion: 该方法为多任务强化学习策略在新任务上的性能提供了形式化保证，填补了安全关键部署中的理论空白。

Abstract: Multi-task reinforcement learning trains generalist policies that can execute multiple tasks. While recent years have seen significant progress, existing approaches rarely provide formal performance guarantees, which are indispensable when deploying policies in safety-critical settings. We present an approach for computing high-confidence guarantees on the performance of a multi-task policy on tasks not seen during training. Concretely, we introduce a new generalisation bound that composes (i) per-task lower confidence bounds from finitely many rollouts with (ii) task-level generalisation from finitely many sampled tasks, yielding a high-confidence guarantee for new tasks drawn from the same arbitrary and unknown distribution. Across state-of-the-art multi-task RL methods, we show that the guarantees are theoretically sound and informative at realistic sample sizes.

</details>


### [1040] [SEDformer: Event-Synchronous Spiking Transformers for Irregular Telemetry Time Series Forecasting](https://arxiv.org/abs/2602.02230)
*Ziyu Zhou,Yuchen Fang,Weilin Ruan,Shiyu Wang,James Kwok,Yuxuan Liang*

Main category: cs.LG

Relevance: 65.0

TL;DR: SEDformer：一种基于脉冲神经网络的稀疏-事件双重性增强的脉冲Transformer，用于不规则多元时间序列预测，通过事件驱动的方式自然对齐IMTS的稀疏性特征，在保持高精度的同时显著降低能耗和内存使用。


<details>
  <summary>Details</summary>
Motivation: 大规模互联网系统（如物联网和在线平台）的遥测数据自然形成不规则多元时间序列（IMTS），其准确预测对运维至关重要。IMTS具有稀疏-事件双重性（SED）特征：长时间稀疏或无观测与短时密集事件爆发交替。现有基于图和Transformer的预测方法忽略SED特性：均匀网格对齐和填充违反稀疏性，关系重构破坏事件语义的局部时间连续性。

Method: 提出SEDformer，一种SED增强的脉冲Transformer，包含三个核心组件：1）SED-based Spike Encoder：使用事件对齐的LIF神经元将原始观测转换为事件同步脉冲；2）Event-Preserving Temporal Downsampling：压缩长时间间隔同时保留显著脉冲；3）SED-based Spike Transformer blocks：基于膜电位的线性注意力机制建模序列内依赖关系。

Result: 在公共遥测IMTS数据集上的实验表明，SEDformer在达到最先进预测精度的同时，显著降低了能量和内存使用，为IMTS建模提供了自然且高效的路径。

Conclusion: 脉冲神经网络因其稀疏二进制脉冲通信和事件驱动更新的特性，自然对齐IMTS的SED特征。SEDformer通过创新的架构设计，为不规则多元时间序列预测提供了更忠实、更自然的建模范式，在精度和效率方面均优于现有方法。

Abstract: Telemetry streams from large-scale Internet-connected systems (e.g., IoT deployments and online platforms) naturally form an irregular multivariate time series (IMTS) whose accurate forecasting is operationally vital. A closer examination reveals a defining Sparsity-Event Duality (SED) property of IMTS, i.e., long stretches with sparse or no observations are punctuated by short, dense bursts where most semantic events (observations) occur. However, existing Graph- and Transformer-based forecasters ignore SED: pre-alignment to uniform grids with heavy padding violates sparsity by inflating sequences and forcing computation at non-informative steps, while relational recasting weakens event semantics by disrupting local temporal continuity. These limitations motivate a more faithful and natural modeling paradigm for IMTS that aligns with its SED property. We find that Spiking Neural Networks meet this requirement, as they communicate via sparse binary spikes and update in an event-driven manner, aligning naturally with the SED nature of IMTS. Therefore, we present SEDformer, an SED-enhanced Spiking Transformer for telemetry IMTS forecasting that couples: (1) a SED-based Spike Encoder converts raw observations into event synchronous spikes using an Event-Aligned LIF neuron, (2) an Event-Preserving Temporal Downsampling module compresses long gaps while retaining salient firings and (3) a stack of SED-based Spike Transformer blocks enable intra-series dependency modeling with a membrane-based linear attention driven by EA-LIF spiking features. Experiments on public telemetry IMTS datasets show that SEDformer attains state-of-the-art forecasting accuracy while reducing energy and memory usage, providing a natural and efficient path for modeling IMTS.

</details>


### [1041] [Interpretability in Deep Time Series Models Demands Semantic Alignment](https://arxiv.org/abs/2602.02239)
*Giovanni De Felice,Riccardo D'Elia,Alberto Termine,Pietro Barbiero,Giuseppe Marra,Silvia Santini*

Main category: cs.LG

Relevance: 65.0

TL;DR: 该论文提出深度时间序列模型应追求语义对齐，即预测应使用对终端用户有意义的变量表达，并通过可接受用户约束的时空机制进行中介，且这种对齐必须在时间演化中保持。


<details>
  <summary>Details</summary>
Motivation: 当前深度时间序列模型虽然预测性能不断提升，但其黑盒特性限制了实际部署。现有的可解释性方法主要关注解释模型内部计算，而没有解决这些解释是否与人类对研究现象的理解方式相一致的问题。作者认为深度时间序列模型的可解释性应追求语义对齐。

Method: 论文形式化了语义对齐的要求：预测应使用对终端用户有意义的变量表达，通过可接受用户约束的时空机制进行中介，且这种对齐必须在时间演化中保持。作者提出了语义对齐深度时间序列模型的蓝图，识别了支持信任的属性，并讨论了模型设计的影响。

Result: 论文提出了语义对齐的形式化定义，这是一个在静态设置中没有类似要求的约束。作者为语义对齐的深度时间序列模型提供了蓝图框架，并识别了支持模型信任的关键属性。

Conclusion: 深度时间序列模型的可解释性需要从单纯解释内部计算转向实现语义对齐，确保模型预测以用户可理解的方式表达，并在时间演化中保持一致，这对模型设计提出了新的要求。

Abstract: Deep time series models continue to improve predictive performance, yet their deployment remains limited by their black-box nature. In response, existing interpretability approaches in the field keep focusing on explaining the internal model computations, without addressing whether they align or not with how a human would reason about the studied phenomenon. Instead, we state interpretability in deep time series models should pursue semantic alignment: predictions should be expressed in terms of variables that are meaningful to the end user, mediated by spatial and temporal mechanisms that admit user-dependent constraints. In this paper, we formalize this requirement and require that, once established, semantic alignment must be preserved under temporal evolution: a constraint with no analog in static settings. Provided with this definition, we outline a blueprint for semantically aligned deep time series models, identify properties that support trust, and discuss implications for model design.

</details>


### [1042] [Unlocking the Duality between Flow and Field Matching](https://arxiv.org/abs/2602.02261)
*Daniil Shlenskii,Alexander Varlamov,Nazar Buzun,Alexander Korotin*

Main category: cs.LG

Relevance: 65.0

TL;DR: 本文揭示了条件流匹配(CFM)与交互场匹配(IFM)之间的理论联系，证明了在正向IFM子类中两者等价，并展示了IFM具有更强的表达能力。


<details>
  <summary>Details</summary>
Motivation: CFM和IFM是两种不同的生成建模框架，分别从概率路径和物理场角度定义生成动态。研究它们是否本质相同还是真正不同，有助于理解生成模型的理论基础。

Method: 通过理论分析构建CFM与正向IFM之间的双射映射，证明两者的等价性。同时展示一般IFM包含CFM无法实现的交互场（如EFM），证明IFM更强的表达能力。

Result: 1) 正向IFM与CFM等价；2) 一般IFM比CFM更具表达力；3) 这种对偶性为两个框架都带来益处：为IFM提供概率解释，并为CFM开发新的IFM驱动技术。

Conclusion: CFM和IFM在正向IFM子类中本质相同，但IFM框架更一般化、更具表达力。这种对偶关系为两个生成建模范式提供了统一的视角。

Abstract: Conditional Flow Matching (CFM) unifies conventional generative paradigms such as diffusion models and flow matching. Interaction Field Matching (IFM) is a newer framework that generalizes Electrostatic Field Matching (EFM) rooted in Poisson Flow Generative Models (PFGM). While both frameworks define generative dynamics, they start from different objects: CFM specifies a conditional probability path in data space, whereas IFM specifies a physics-inspired interaction field in an augmented data space. This raises a basic question: are CFM and IFM genuinely different, or are they two descriptions of the same underlying dynamics? We show that they coincide for a natural subclass of IFM that we call forward-only IFM. Specifically, we construct a bijection between CFM and forward-only IFM. We further show that general IFM is strictly more expressive: it includes EFM and other interaction fields that cannot be realized within the standard CFM formulation. Finally, we highlight how this duality can benefit both frameworks: it provides a probabilistic interpretation of forward-only IFM and yields novel, IFM-driven techniques for CFM.

</details>


### [1043] [An Optimization Method for Autoregressive Time Series Forecasting](https://arxiv.org/abs/2602.02288)
*Zheng Li,Jerry Cheng,Huanying Gu*

Main category: cs.LG

Relevance: 65.0

TL;DR: 提出一种新的时间序列预测训练方法，通过强制自回归预测误差随预测范围增加而增加，并允许模型连接短期预测形成灵活长期预测，在多个基准上实现SOTA性能。


<details>
  <summary>Details</summary>
Motivation: 当前时间序列预测模型主要基于Transformer架构，通过扩大模型规模而非真正的自回归展开实现长期预测。从大语言模型训练角度看，传统时间序列预测训练忽略了时间因果关系。

Method: 提出新颖的训练方法，强制两个关键属性：1) 自回归预测误差应随预测范围增加而增加，违反此原则被视为随机猜测并在损失函数中显式惩罚；2) 使模型能够连接短期自回归预测以形成灵活的长期预测。

Result: 在多个基准上建立新的SOTA，相比iTransformer和其他近期强基线实现超过10%的MSE降低。使短期预测模型能够在超过7.5倍更长的范围内进行可靠的长期预测。

Conclusion: 该方法通过强制时间因果关系和灵活的预测连接，显著提升了时间序列预测性能，特别是在长期预测任务上。

Abstract: Current time-series forecasting models are primarily based on transformer-style neural networks. These models achieve long-term forecasting mainly by scaling up the model size rather than through genuinely autoregressive (AR) rollout. From the perspective of large language model training, the traditional training process for time-series forecasting models ignores temporal causality. In this paper, we propose a novel training method for time-series forecasting that enforces two key properties: (1) AR prediction errors should increase with the forecasting horizon. Any violation of this principle is considered random guessing and is explicitly penalized in the loss function, and (2) the method enables models to concatenate short-term AR predictions for forming flexible long-term forecasts. Empirical results demonstrate that our method establishes a new state-of-the-art across multiple benchmarks, achieving an MSE reduction of more than 10% compared to iTransformer and other recent strong baselines. Furthermore, it enables short-horizon forecasting models to perform reliable long-term predictions at horizons over 7.5 times longer. Code is available at https://github.com/LizhengMathAi/AROpt

</details>


### [1044] [Decoupling Generalizability and Membership Privacy Risks in Neural Networks](https://arxiv.org/abs/2602.02296)
*Xingli Fang,Jung-Eun Kim*

Main category: cs.LG

Relevance: 65.0

TL;DR: 论文提出隐私保护训练原则（PPTP），通过识别深度神经网络中泛化能力与隐私风险存在于不同区域，解耦两者以最大化隐私保护同时最小化泛化能力损失。


<details>
  <summary>Details</summary>
Motivation: 深度学习模型在获得某些能力时通常需要牺牲其他效用，隐私保护与模型效用之间存在权衡关系。不同防御方法之间的损失差异表明存在解耦泛化能力和隐私风险的潜力，从而最大化隐私收益。

Method: 提出隐私保护训练原则（PPTP），基于观察到模型的泛化能力和隐私风险存在于深度神经网络架构的不同区域，保护模型组件免受隐私风险，同时最小化泛化能力的损失。

Result: 通过广泛评估，该方法在增强隐私保护的同时，显著更好地保持了模型的泛化能力。

Conclusion: 通过识别深度神经网络中泛化与隐私风险的区域分离，PPTP能够有效解耦两者，实现隐私保护与模型效用的更好平衡。

Abstract: A deep learning model usually has to sacrifice some utilities when it acquires some other abilities or characteristics. Privacy preservation has such trade-off relationships with utilities. The loss disparity between various defense approaches implies the potential to decouple generalizability and privacy risks to maximize privacy gain. In this paper, we identify that the model's generalization and privacy risks exist in different regions in deep neural network architectures. Based on the observations that we investigate, we propose Privacy-Preserving Training Principle (PPTP) to protect model components from privacy risks while minimizing the loss in generalizability. Through extensive evaluations, our approach shows significantly better maintenance in model generalizability while enhancing privacy preservation.

</details>


### [1045] [Self-Supervised Learning from Structural Invariance](https://arxiv.org/abs/2602.02381)
*Yipeng Zhang,Hafez Ghaemi,Jungyoon Lee,Shahab Bakhtiari,Eilif B. Muller,Laurent Charlin*

Main category: cs.LG

Relevance: 65.0

TL;DR: AdaSSL通过引入潜变量解决自监督学习中的一对多映射问题，适用于对比式和蒸馏式SSL方法，在因果表示学习、细粒度图像理解和视频世界建模中表现优异。


<details>
  <summary>Details</summary>
Motivation: 自监督学习通常假设数据对之间存在确定性映射，但在实际场景中（如连续视频帧），一个数据点可能对应多个有效目标，形成一对多映射问题。现有方法难以灵活捕捉这种条件不确定性。

Method: 引入潜变量来建模条件不确定性，推导出配对嵌入间互信息的变分下界，得到对标准SSL目标的简单正则化项。该方法适用于对比式和蒸馏式SSL目标。

Result: AdaSSL在因果表示学习、细粒度图像理解和视频世界建模等多个任务中展现出优越性能，验证了其处理一对多映射问题的有效性。

Conclusion: 通过显式建模条件不确定性，AdaSSL为自监督学习中的一对多映射问题提供了有效解决方案，扩展了SSL在复杂现实场景中的应用能力。

Abstract: Joint-embedding self-supervised learning (SSL), the key paradigm for unsupervised representation learning from visual data, learns from invariances between semantically-related data pairs. We study the one-to-many mapping problem in SSL, where each datum may be mapped to multiple valid targets. This arises when data pairs come from naturally occurring generative processes, e.g., successive video frames. We show that existing methods struggle to flexibly capture this conditional uncertainty. As a remedy, we introduce a latent variable to account for this uncertainty and derive a variational lower bound on the mutual information between paired embeddings. Our derivation yields a simple regularization term for standard SSL objectives. The resulting method, which we call AdaSSL, applies to both contrastive and distillation-based SSL objectives, and we empirically show its versatility in causal representation learning, fine-grained image understanding, and world modeling on videos.

</details>


### [1046] [Active Causal Experimentalist (ACE): Learning Intervention Strategies via Direct Preference Optimization](https://arxiv.org/abs/2602.02451)
*Patrick Cooper,Alvaro Velasquez*

Main category: cs.LG

Relevance: 65.0

TL;DR: ACE使用直接偏好优化学习序列化实验设计策略，通过干预比较而非奖励幅度来适应因果发现中的顺序决策问题，相比基线方法在同等干预预算下提升70-71%


<details>
  <summary>Details</summary>
Motivation: 因果发现需要受控实验，但实验者面临顺序决策问题：每次干预都揭示信息，应指导下一步尝试。传统方法（随机采样、贪婪信息最大化、轮询覆盖）将每个决策孤立处理，无法从经验中学习自适应策略。

Method: 提出Active Causal Experimentalist (ACE)，将实验设计学习为序列策略。关键洞察：虽然绝对信息增益随知识积累而减少（使基于价值的RL不稳定），但候选干预之间的相对比较始终有意义。ACE通过直接偏好优化，从成对干预比较而非非平稳奖励幅度中学习。

Result: 在合成基准、物理模拟和经济数据上，ACE在同等干预预算下相比基线实现70-71%的改进（p < 0.001，Cohen's d ~ 2）。学习到的策略自主发现碰撞机制需要对父变量进行集中干预，这是从经验中纯粹涌现的理论基础策略。

Conclusion: 偏好学习可以恢复原则性实验策略，用学习到的领域适应补充理论。这表明基于偏好的方法能够发现理论基础的因果发现策略。

Abstract: Discovering causal relationships requires controlled experiments, but experimentalists face a sequential decision problem: each intervention reveals information that should inform what to try next. Traditional approaches such as random sampling, greedy information maximization, and round-robin coverage treat each decision in isolation, unable to learn adaptive strategies from experience. We propose Active Causal Experimentalist (ACE), which learns experimental design as a sequential policy. Our key insight is that while absolute information gains diminish as knowledge accumulates (making value-based RL unstable), relative comparisons between candidate interventions remain meaningful throughout. ACE exploits this via Direct Preference Optimization, learning from pairwise intervention comparisons rather than non-stationary reward magnitudes. Across synthetic benchmarks, physics simulations, and economic data, ACE achieves 70-71% improvement over baselines at equal intervention budgets (p < 0.001, Cohen's d ~ 2). Notably, the learned policy autonomously discovers that collider mechanisms require concentrated interventions on parent variables, a theoretically-grounded strategy that emerges purely from experience. This suggests preference-based learning can recover principled experimental strategies, complementing theory with learned domain adaptation.

</details>


### [1047] [Test-Time Adaptation for Non-stationary Time Series: From Synthetic Regime Shifts to Financial Markets](https://arxiv.org/abs/2602.00073)
*Yurui Wu,Qingying Deng,Wonou Chung,Mairui Li*

Main category: q-fin.ST

Relevance: 65.0

TL;DR: 论文提出了一种轻量级测试时适应（TTA）框架，用于处理非平稳时间序列的因果预测和方向分类。核心思想是冻结主干网络，仅更新归一化仿射参数，通过最小化熵、时间一致性、预测方差等策略进行适应，并在金融时间序列上验证了方法的有效性。


<details>
  <summary>Details</summary>
Motivation: 实际时间序列通常是非平稳的，数据分布的变化会导致基于历史观测训练的预测模型准确性下降。需要一种轻量级的测试时适应方法，使模型能够在部署时适应分布变化，而无需重新训练整个模型。

Method: 提出小规模测试时适应框架：1）冻结主干网络，仅更新归一化仿射参数；2）对于分类任务，最小化熵并强制时间一致性；3）对于回归任务，最小化弱时间保持增强下的预测方差，可选使用EMA教师蒸馏；4）引入二次漂移惩罚和不确定性触发回退机制保持更新稳定性。

Result: 在合成渐变漂移场景中，基于归一化的TTA改善了预测误差；在金融市场数据（SPY、QQQ、EUR/USD）上，简单的批归一化统计更新是稳健的默认选择，而更激进的仅归一化适应可能反而有害。研究为在非平稳时间序列上部署TTA提供了实用指导。

Conclusion: 提出的轻量级TTA框架能有效处理非平稳时间序列的分布变化，但需要根据具体应用场景谨慎选择适应策略。在金融时间序列中，保守的批归一化统计更新比激进的参数适应更稳健。

Abstract: Time series encountered in practice are rarely stationary. When the data distribution changes, a forecasting model trained on past observations can lose accuracy. We study a small-footprint test-time adaptation (TTA) framework for causal timeseries forecasting and direction classification. The backbone is frozen, and only normalization affine parameters are updated using recent unlabeled windows. For classification we minimize entropy and enforce temporal consistency; for regression we minimize prediction variance across weak time-preserving augmentations and optionally distill from an EMA teacher. A quadratic drift penalty and an uncertainty triggered fallback keep updates stable. We evaluate this framework in two stages: synthetic regime shifts on ETT benchmarks, and daily equity and FX series (SPY, QQQ, EUR/USD) across pandemic, high-inflation, and recovery regimes. On synthetic gradual drift, normalization-based TTA improves forecasting error, while in financial markets a simple batch-normalization statistics update is a robust default and more aggressive norm-only adaptation can even hurt. Our results provide practical guidance for deploying TTA on non-stationary time series.

</details>


### [1048] [Neuron Block Dynamics for XOR Classification with Zero-Margin](https://arxiv.org/abs/2602.00172)
*Guillaume Braun,Masaaki Imaizumi*

Main category: stat.ML

Relevance: 65.0

TL;DR: 该论文研究了零边距非线性分类问题，通过分析高斯XOR问题，揭示了神经网络在梯度下降训练中的块级动态特性，提出了不依赖边距假设的泛化分析框架。


<details>
  <summary>Details</summary>
Motivation: 大多数理论分析关注回归或有正边距的分类任务，而实际中许多分类问题（如XOR问题）存在零边距情况，标准边距理论失效。需要理解神经网络在零边距设置下的训练动态和泛化行为。

Method: 基于Glasgow (2024)的分析，将研究从离散输入扩展到高斯输入，开发神经元块动态框架。分析高斯XOR问题，其中输入为高斯分布，标签由XOR决策边界决定。通过块级信号演化分析训练动态。

Result: 发现神经元会聚集成四个方向，块级信号演化具有一致性。在高斯设置下，个体神经元信号变化显著，但块级动态保持稳健。数值实验证实了两阶段块动态，并展示了其在非高斯设置下的鲁棒性。

Conclusion: 提出了不依赖边距假设的泛化分析框架，采用平均情况视角区分可靠预测区域和持续错误区域。块级动态分析为理解神经网络在零边距分类问题中的学习机制提供了新视角。

Abstract: The ability of neural networks to learn useful features through stochastic gradient descent (SGD) is a cornerstone of their success. Most theoretical analyses focus on regression or on classification tasks with a positive margin, where worst-case gradient bounds suffice. In contrast, we study zero-margin nonlinear classification by analyzing the Gaussian XOR problem, where inputs are Gaussian and the XOR decision boundary determines labels. In this setting, a non-negligible fraction of data lies arbitrarily close to the boundary, breaking standard margin-based arguments. Building on Glasgow's (2024) analysis, we extend the study of training dynamics from discrete to Gaussian inputs and develop a framework for the dynamics of neuron blocks. We show that neurons cluster into four directions and that block-level signals evolve coherently, a phenomenon essential in the Gaussian setting where individual neuron signals vary significantly. Leveraging this block perspective, we analyze generalization without relying on margin assumptions, adopting an average-case view that distinguishes regions of reliable prediction from regions of persistent error. Numerical experiments confirm the predicted two-phase block dynamics and demonstrate their robustness beyond the Gaussian setting.

</details>


### [1049] [Singular Bayesian Neural Networks](https://arxiv.org/abs/2602.00387)
*Mame Diarra Toure,David A. Stephens*

Main category: stat.ML

Relevance: 65.0

TL;DR: 提出一种低秩参数化的贝叶斯神经网络方法，通过AB^⊤分解将权重限制在秩r流形上，显著减少参数数量，同时保持竞争性性能并改进不确定性估计。


<details>
  <summary>Details</summary>
Motivation: 传统均值场高斯后验需要O(mn)参数，成本过高。作者认为当权重矩阵具有快速奇异值衰减时，这种成本通常是不必要的。低秩参数化可以更高效地捕捉结构化权重相关性。

Method: 将权重参数化为W=AB^⊤，其中A∈ℝ^{m×r}，B∈ℝ^{n×r}，诱导出相对于Lebesgue测度奇异的低秩流形上的后验分布。这种方法通过共享潜在因子捕捉结构化权重相关性，几何上不同于均值场的独立性假设。

Result: 理论推导了PAC-Bayes泛化边界，复杂度项从√(mn)缩减到√(r(m+n))。实证结果显示，在MLPs、LSTMs和Transformers上，该方法使用最多15倍更少参数，性能与5成员深度集成相当，并显著改进OOD检测和校准。

Conclusion: 低秩参数化贝叶斯神经网络提供了一种参数高效的不确定性量化方法，在保持竞争性预测性能的同时，显著减少参数数量并改进不确定性估计质量。

Abstract: Bayesian neural networks promise calibrated uncertainty but require $O(mn)$ parameters for standard mean-field Gaussian posteriors. We argue this cost is often unnecessary, particularly when weight matrices exhibit fast singular value decay. By parameterizing weights as $W = AB^{\top}$ with $A \in \mathbb{R}^{m \times r}$, $B \in \mathbb{R}^{n \times r}$, we induce a posterior that is singular with respect to the Lebesgue measure, concentrating on the rank-$r$ manifold. This singularity captures structured weight correlations through shared latent factors, geometrically distinct from mean-field's independence assumption. We derive PAC-Bayes generalization bounds whose complexity term scales as $\sqrt{r(m+n)}$ instead of $\sqrt{m n}$, and prove loss bounds that decompose the error into optimization and rank-induced bias using the Eckart-Young-Mirsky theorem. We further adapt recent Gaussian complexity bounds for low-rank deterministic networks to Bayesian predictive means. Empirically, across MLPs, LSTMs, and Transformers on standard benchmarks, our method achieves predictive performance competitive with 5-member Deep Ensembles while using up to $15\times$ fewer parameters. Furthermore, it substantially improves OOD detection and often improves calibration relative to mean-field and perturbation baselines.

</details>


### [1050] [Reinforcement Learning for Control Systems with Time Delays: A Comprehensive Survey](https://arxiv.org/abs/2602.00399)
*Armando Alves Neto*

Main category: stat.ML

Relevance: 65.0

TL;DR: 该论文是关于强化学习中时间延迟问题的综述，系统分类了处理延迟的五大方法家族，为延迟影响下的网络物理系统提供RL控制器的实用指南。


<details>
  <summary>Details</summary>
Motivation: 实际网络物理系统中的传感延迟、执行延迟和通信约束会破坏马尔可夫决策过程假设，引入记忆效应，显著降低RL性能并威胁稳定性，需要专门的方法来处理时间延迟问题。

Method: 1. 形式化延迟分类并分析其对马尔可夫性质的影响；2. 将现有方法系统分为五大类：状态增强和历史表示、具有学习记忆的循环策略、基于预测器和模型感知的方法、鲁棒和领域随机化训练策略、具有显式约束处理的安全RL框架；3. 对各类方法进行对比分析并提供选择指南。

Result: 提供了处理时间延迟的RL方法的全面分类框架，分析了各类方法的原理、优势和局限性，给出了基于延迟特性和安全需求的方法选择实用指南。

Conclusion: 该综述为在延迟影响的网络物理系统中开发可靠RL控制器提供了统一参考，并指出了稳定性认证、大延迟学习、多智能体通信协同设计和标准化基准测试等未来研究方向。

Abstract: In the last decade, Reinforcement Learning (RL) has achieved remarkable success in the control and decision-making of complex dynamical systems. However, most RL algorithms rely on the Markov Decision Process assumption, which is violated in practical cyber-physical systems affected by sensing delays, actuation latencies, and communication constraints. Such time delays introduce memory effects that can significantly degrade performance and compromise stability, particularly in networked and multi-agent environments. This paper presents a comprehensive survey of RL methods designed to address time delays in control systems. We first formalize the main classes of delays and analyze their impact on the Markov property. We then systematically categorize existing approaches into five major families: state augmentation and history-based representations, recurrent policies with learned memory, predictor-based and model-aware methods, robust and domain-randomized training strategies, and safe RL frameworks with explicit constraint handling. For each family, we discuss underlying principles, practical advantages, and inherent limitations. A comparative analysis highlights key trade-offs among these approaches and provides practical guidelines for selecting suitable methods under different delay characteristics and safety requirements. Finally, we identify open challenges and promising research directions, including stability certification, large-delay learning, multi-agent communication co-design, and standardized benchmarking. This survey aims to serve as a unified reference for researchers and practitioners developing reliable RL-based controllers in delay-affected cyber-physical systems.

</details>


### [1051] [Alignment of Diffusion Model and Flow Matching for Text-to-Image Generation](https://arxiv.org/abs/2602.00413)
*Yidong Ouyang,Liyan Xie,Hongyuan Zha,Guang Cheng*

Main category: stat.ML

Relevance: 65.0

TL;DR: 提出了一种新的对齐框架，利用奖励加权分布采样的本质，适用于扩散模型（通过分数引导）和流匹配模型（通过速度引导），显著降低计算成本


<details>
  <summary>Details</summary>
Motivation: 现有对齐方法主要专注于微调预训练生成模型以最大化奖励函数，但需要大量计算资源且在不同目标间泛化能力有限。作者希望利用对齐问题的本质——从奖励加权分布中采样——来开发更高效的对齐方法

Method: 1) 将所需分数函数（速度场）分解为预训练分数（速度场）加上奖励的条件期望；2) 对于扩散模型，提出免微调框架，训练引导网络估计奖励的条件期望；3) 对于流匹配模型，提出免训练框架，无需额外计算成本

Result: 在扩散模型上，与基于微调的模型相比，单步生成达到相当性能，计算成本降低至少60%；在流匹配模型上，无需训练即可提升生成质量，无额外计算成本

Conclusion: 通过利用对齐问题的本质——从奖励加权分布采样，开发了适用于扩散模型和流匹配模型的高效对齐框架，显著降低计算成本，为生成模型对齐提供了新思路

Abstract: Diffusion models and flow matching have demonstrated remarkable success in text-to-image generation. While many existing alignment methods primarily focus on fine-tuning pre-trained generative models to maximize a given reward function, these approaches require extensive computational resources and may not generalize well across different objectives. In this work, we propose a novel alignment framework by leveraging the underlying nature of the alignment problem -- sampling from reward-weighted distributions -- and show that it applies to both diffusion models (via score guidance) and flow matching models (via velocity guidance). The score function (velocity field) required for the reward-weighted distribution can be decomposed into the pre-trained score (velocity field) plus a conditional expectation of the reward. For the alignment on the diffusion model, we identify a fundamental challenge: the adversarial nature of the guidance term can introduce undesirable artifacts in the generated images. Therefore, we propose a finetuning-free framework that trains a guidance network to estimate the conditional expectation of the reward. We achieve comparable performance to finetuning-based models with one-step generation with at least a 60% reduction in computational cost. For the alignment on flow matching, we propose a training-free framework that improves the generation quality without additional computational cost.

</details>


### [1052] [Stabilizing Fixed-Point Iteration for Markov Chain Poisson Equations](https://arxiv.org/abs/2602.00474)
*Yang Xu,Vaneet Aggarwal*

Main category: stat.ML

Relevance: 65.0

TL;DR: 该论文研究了非遍历马尔可夫链（多链和周期链）下的泊松方程求解问题，提出了基于商空间和规范映射的稳定学习框架，应用于平均奖励强化学习的性能评估。


<details>
  <summary>Details</summary>
Motivation: 泊松方程是平均奖励强化学习的理论基础，但在非遍历（非各态历经）马尔可夫链（如可约链或周期链）中，泊松方程可能不适定，导致解不唯一且标准固定点迭代可能振荡。现有方法主要局限于遍历链，需要扩展到更一般的多链和周期链场景。

Method: 1. 理论分析：识别马尔可夫链的实外围不变子空间K(P)，证明在商空间R^n/K(P)上的诱导算子是严格压缩的，从而获得唯一的商解。
2. 学习框架：开发端到端管道：(a) 学习链结构；(b) 估计基于锚点的规范映射；(c) 运行投影随机逼近来估计规范固定代表及其相关的外围残差。
3. 收敛性证明：在投影估计误差范围内，证明Õ(T^{-1/2})的收敛速率。

Result: 1. 理论结果：证明了在商空间上诱导算子的严格压缩性，确保了泊松方程在非遍历链中的适定性。
2. 算法收敛性：证明了所提方法在投影估计误差范围内的Õ(T^{-1/2})收敛速率。
3. 应用扩展：实现了在多链和周期链场景下稳定的泊松方程学习，扩展了平均奖励强化学习的性能评估能力。

Conclusion: 该论文通过引入商空间和规范映射的数学框架，解决了非遍历马尔可夫链下泊松方程的不适定问题，为平均奖励强化学习在更一般场景下的性能评估提供了理论基础和实用算法，具有Õ(T^{-1/2})的收敛保证。

Abstract: Poisson equations underpin average-reward reinforcement learning, but beyond ergodicity they can be ill-posed, meaning that solutions are non-unique and standard fixed point iterations can oscillate on reducible or periodic chains. We study finite-state Markov chains with $n$ states and transition matrix $P$. We show that all non-decaying modes are captured by a real peripheral invariant subspace $\mathcal{K}(P)$, and that the induced operator on the quotient space $\mathbb{R}^n/\mathcal{K}(P)$ is strictly contractive, yielding a unique quotient solution. Building on this viewpoint, we develop an end-to-end pipeline that learns the chain structure, estimates an anchor based gauge map, and runs projected stochastic approximation to estimate a gauge-fixed representative together with an associated peripheral residual. We prove $\widetilde{O}(T^{-1/2})$ convergence up to projection estimation error, enabling stable Poisson equation learning for multichain and periodic regimes with applications to performance evaluation of average-reward reinforcement learning beyond ergodicity.

</details>


### [1053] [Reinforcement Learning-assisted Constraint Relaxation for Constrained Expensive Optimization](https://arxiv.org/abs/2602.00532)
*Qianhao Zhu,Sijie Ma,Zeyuan Ma,Hongshu Guo,Yue-Jiao Gong*

Main category: cs.NE

Relevance: 65.0

TL;DR: 提出基于强化学习的自适应约束处理策略，通过深度Q网络控制约束松弛水平，在有限评估预算下优化约束优化问题性能


<details>
  <summary>Details</summary>
Motivation: 现有约束处理技术主要依赖专家设计，在通用性方面存在不足。受元黑盒优化自动算法设计启发，希望通过强化学习学习有效、自适应且可泛化的约束处理策略

Method: 1. 制定定制化马尔可夫决策过程；2. 使用深度Q网络策略，基于优化动态特征控制约束松弛水平；3. 在CEC 2017约束优化基准上训练，采用有限评估预算条件

Result: 在留一法交叉验证和普通训练测试分割验证中，该方法表现具有竞争力甚至超越CEC/GECCO竞赛中的强基线方法

Conclusion: 强化学习能够学习有效的自适应约束处理策略，在约束优化问题中实现目标导向利用与可行区域探索的灵活权衡

Abstract: Constraint handling plays a key role in solving realistic complex optimization problems. Though intensively discussed in the last few decades, existing constraint handling techniques predominantly rely on human experts' designs, which more or less fall short in utility towards general cases. Motivated by recent progress in Meta-Black-Box Optimization where automated algorithm design can be learned to boost optimization performance, in this paper, we propose learning effective, adaptive and generalizable constraint handling policy through reinforcement learning. Specifically, a tailored Markov Decision Process is first formulated, where given optimization dynamics features, a deep Q-network-based policy controls the constraint relaxation level along the underlying optimization process. Such adaptive constraint handling provides flexible tradeoff between objective-oriented exploitation and feasible-region-oriented exploration, and hence leads to promising optimization performance. We train our approach on CEC 2017 Constrained Optimization benchmark with limited evaluation budget condition (expensive cases) and compare the trained constraint handling policy to strong baselines such as recent winners in CEC/GECCO competitions. Extensive experimental results show that our approach performs competitively or even surpasses the compared baselines under either Leave-one-out cross-validation or ordinary train-test split validation. Further analysis and ablation studies reveal key insights in our designs.

</details>


### [1054] [Audio-to-Image Bird Species Retrieval without Audio-Image Pairs via Text Distillation](https://arxiv.org/abs/2602.00681)
*Ilyass Moummad,Marius Miron,Lukas Rauch,David Robinson,Alexis Joly,Olivier Pietquin,Emmanuel Chemla,Matthieu Geist*

Main category: cs.SD

Relevance: 65.0

TL;DR: 提出通过文本作为语义中介，将预训练图像-文本模型（BioCLIP-2）的文本嵌入空间蒸馏到音频-文本模型（BioLingual）中，实现无音频-图像监督的音频到图像检索


<details>
  <summary>Details</summary>
Motivation: 音频到图像检索为生物声学物种识别提供可解释的替代方案，但缺乏配对的音频-图像数据使得学习对齐表示具有挑战性。需要一种无需音频-图像监督的数据高效方法

Method: 使用文本作为语义中介：通过对比目标微调音频编码器，将预训练图像-文本模型（BioCLIP-2）的文本嵌入空间蒸馏到音频-文本模型（BioLingual）中，将视觉基础语义转移到音频表示中

Result: 蒸馏后的音频编码器保持音频判别能力，同时在焦点录音和声景数据集上显著改善音频-文本对齐。在SSW60基准上，音频到图像检索性能超过基于零样本模型组合或学习文本嵌入映射的基线方法

Conclusion: 通过文本的间接语义转移足以诱导有意义的音频-图像对齐，为数据稀缺的生物声学环境中的视觉基础物种识别提供实用解决方案

Abstract: Audio-to-image retrieval offers an interpretable alternative to audio-only classification for bioacoustic species recognition, but learning aligned audio-image representations is challenging due to the scarcity of paired audio-image data. We propose a simple and data-efficient approach that enables audio-to-image retrieval without any audio-image supervision. Our proposed method uses text as a semantic intermediary: we distill the text embedding space of a pretrained image-text model (BioCLIP-2), which encodes rich visual and taxonomic structure, into a pretrained audio-text model (BioLingual) by fine-tuning its audio encoder with a contrastive objective. This distillation transfers visually grounded semantics into the audio representation, inducing emergent alignment between audio and image embeddings without using images during training. We evaluate the resulting model on multiple bioacoustic benchmarks. The distilled audio encoder preserves audio discriminative power while substantially improving audio-text alignment on focal recordings and soundscape datasets. Most importantly, on the SSW60 benchmark, the proposed approach achieves strong audio-to-image retrieval performance exceeding baselines based on zero-shot model combinations or learned mappings between text embeddings, despite not training on paired audio-image data. These results demonstrate that indirect semantic transfer through text is sufficient to induce meaningful audio-image alignment, providing a practical solution for visually grounded species recognition in data-scarce bioacoustic settings.

</details>


### [1055] [Communications-Incentivized Collaborative Reasoning in NetGPT through Agentic Reinforcement Learning](https://arxiv.org/abs/2602.00766)
*Xiaoxue Yu,Rongpeng Li,Zhifeng Zhao,Honggang Zhang*

Main category: cs.MA

Relevance: 65.0

TL;DR: 提出了NetGPT框架，这是一个用于AI原生xG网络的统一代理框架，通过代理通信实现自主推理和任务委派，结合强化学习优化协作策略。


<details>
  <summary>Details</summary>
Motivation: 现有通信系统中的AI部署通常是孤立的，缺乏内在适应性、动态任务委派或多代理协作能力。xG无线网络正从连接中心架构转向AI原生设计，需要统一的框架来整合数据、计算和通信。

Method: 提出NetGPT框架，包含核心NetGPT组件（可自主推理或委派任务给领域专业代理），通过代理通信实现协作。采用部分可观测条件下的代理强化学习，包含掩码损失（针对外部代理不确定性）、熵引导探索和多目标奖励（任务质量、协调效率、资源约束）。

Result: 该框架为自演化的AI原生xG网络提供了基础架构和训练方法，能够在复杂通信环境中实现自主感知、推理和行动，有效平衡内部推理与代理调用。

Conclusion: NetGPT框架为AI原生xG网络提供了统一的代理架构，通过强化学习优化协作策略，实现了可扩展的分布式智能，支持网络的自演化能力。

Abstract: The evolution of next-Generation (xG) wireless networks marks a paradigm shift from connectivity-centric architectures to Artificial Intelligence (AI)-native designs that tightly integrate data, computing, and communication. Yet existing AI deployments in communication systems remain largely siloed, offering isolated optimizations without intrinsic adaptability, dynamic task delegation, or multi-agent collaboration. In this work, we propose a unified agentic NetGPT framework for AI-native xG networks, wherein a NetGPT core can either perform autonomous reasoning or delegate sub-tasks to domain-specialized agents via agentic communication. The framework establishes clear modular responsibilities and interoperable workflows, enabling scalable, distributed intelligence across the network. To support continual refinement of collaborative reasoning strategies, the framework is further enhanced through Agentic reinforcement learning under partially observable conditions and stochastic external states. The training pipeline incorporates masked loss against external agent uncertainty, entropy-guided exploration, and multi-objective rewards that jointly capture task quality, coordination efficiency, and resource constraints. Through this process, NetGPT learns when and how to collaborate, effectively balancing internal reasoning with agent invocation. Overall, this work provides a foundational architecture and training methodology for self-evolving, AI-native xG networks capable of autonomous sensing, reasoning, and action in complex communication environments.

</details>


### [1056] [Optimal Decision-Making Based on Prediction Sets](https://arxiv.org/abs/2602.00989)
*Tao Wang,Edgar Dobriban*

Main category: stat.ML

Relevance: 65.0

TL;DR: 提出Risk-Optimal Conformal Prediction (ROCP)框架，将预测集与下游决策风险最小化相结合，通过最坏情况分布下的风险优化来减少关键错误。


<details>
  <summary>Details</summary>
Motivation: 预测集虽然能为机器学习模型提供覆盖保证，但如何将其最优地用于下游决策仍不清楚。现有方法主要关注覆盖概率，而未充分考虑决策风险，特别是在安全关键应用中，集外错误的代价可能很高。

Method: 1) 提出决策理论框架，在预测集覆盖保证下最小化最坏情况分布的期望损失；2) 推导固定预测集下的极小极大最优策略；3) 在覆盖约束下推导最优预测集构造；4) 提出ROCP算法，实现风险最小化预测集并保持有限样本分布无关的边际覆盖。

Result: 在医疗诊断和安全关键决策任务上的实证评估表明，ROCP相比基线方法减少了关键错误，特别是在集外错误代价较高的情况下表现更优。

Conclusion: ROCP框架成功地将预测集的覆盖保证与下游决策风险最小化相结合，为安全关键应用提供了更实用的不确定性量化方法。

Abstract: Prediction sets can wrap around any ML model to cover unknown test outcomes with a guaranteed probability. Yet, it remains unclear how to use them optimally for downstream decision-making. Here, we propose a decision-theoretic framework that seeks to minimize the expected loss (risk) against a worst-case distribution consistent with the prediction set's coverage guarantee. We first characterize the minimax optimal policy for a fixed prediction set, showing that it balances the worst-case loss inside the set with a penalty for potential losses outside the set. Building on this, we derive the optimal prediction set construction that minimizes the resulting robust risk subject to a coverage constraint. Finally, we introduce Risk-Optimal Conformal Prediction (ROCP), a practical algorithm that targets these risk-minimizing sets while maintaining finite-sample distribution-free marginal coverage. Empirical evaluations on medical diagnosis and safety-critical decision-making tasks demonstrate that ROCP reduces critical mistakes compared to baselines, particularly when out-of-set errors are costly.

</details>


### [1057] [Non-Uniform Noise-to-Signal Ratio in the REINFORCE Policy-Gradient Estimator](https://arxiv.org/abs/2602.01460)
*Haoyu Han,Heng Yang*

Main category: math.OC

Relevance: 65.0

TL;DR: 本文通过噪声信噪比(NSR)分析策略梯度方法的训练不稳定性，发现REINFORCE估计器的NSR在接近最优策略时会急剧增加，导致训练崩溃。


<details>
  <summary>Details</summary>
Motivation: 策略梯度方法在强化学习中广泛应用，但训练过程经常不稳定或随着学习进展而变慢。本文旨在通过分析策略梯度估计器的噪声信噪比来理解这一现象。

Method: 1) 对有限时域线性系统和高斯策略、线性状态反馈，以及有限时域多项式系统和高斯策略、多项式反馈，精确表征REINFORCE估计器的NSR；2) 对一般非线性动力学和表达性策略（包括神经网络策略），推导方差的上界；3) 通过数值矩评估算法分析NSR在参数空间的变化和优化轨迹上的演化。

Result: NSR景观高度不均匀，通常在策略接近最优时增加，在某些情况下会急剧增大，触发训练不稳定性和策略崩溃。这些特征使得能够直接检查NSR如何随策略参数变化以及在优化轨迹上的演化。

Conclusion: 策略梯度估计器的噪声信噪比是理解训练不稳定性的关键因素，NSR在接近最优策略时的增加解释了训练崩溃现象，为改进策略梯度方法提供了理论基础。

Abstract: Policy-gradient methods are widely used in reinforcement learning, yet training often becomes unstable or slows down as learning progresses. We study this phenomenon through the noise-to-signal ratio (NSR) of a policy-gradient estimator, defined as the estimator variance (noise) normalized by the squared norm of the true gradient (signal). Our main result is that, for (i) finite-horizon linear systems with Gaussian policies and linear state-feedback, and (ii) finite-horizon polynomial systems with Gaussian policies and polynomial feedback, the NSR of the REINFORCE estimator can be characterized exactly-either in closed form or via numerical moment-evaluation algorithms-without approximation. For general nonlinear dynamics and expressive policies (including neural policies), we further derive a general upper bound on the variance. These characterizations enable a direct examination of how NSR varies across policy parameters and how it evolves along optimization trajectories (e.g. SGD and Adam). Across a range of examples, we find that the NSR landscape is highly non-uniform and typically increases as the policy approaches an optimum; in some regimes it blows up, which can trigger training instability and policy collapse.

</details>


### [1058] [Density-Informed Pseudo-Counts for Calibrated Evidential Deep Learning](https://arxiv.org/abs/2602.01477)
*Pietro Carlotti,Nevena Gligić,Arya Farahi*

Main category: stat.ML

Relevance: 65.0

TL;DR: 该论文提出了DIP-EDL方法，通过分离条件标签分布和边际协变量密度估计，解决了标准EDL在分布外数据上过度自信的问题，改善了不确定性校准和鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 尽管证据深度学习（EDL）是流行的不确定性感知分类框架，但其理论基础和在分布偏移下的行为仍不清楚。标准EDL将认知不确定性和偶然不确定性混为一谈，导致在分布外（OOD）输入上系统性过度自信。

Method: 提出密度感知伪计数EDL（DIP-EDL），通过分层贝叶斯模型和温度伪似然的摊销变分推断视角重新解释EDL。新方法分离类别预测和不确定性大小，分别估计条件标签分布和边际协变量密度。

Result: 理论上证明了DIP-EDL实现渐近集中性。实证表明该方法提高了可解释性，在分布偏移下增强了鲁棒性和不确定性校准。

Conclusion: DIP-EDL通过解耦类别预测和不确定性估计，有效解决了标准EDL在OOD数据上的过度自信问题，为不确定性感知分类提供了更可靠的框架。

Abstract: Evidential Deep Learning (EDL) is a popular framework for uncertainty-aware classification that models predictive uncertainty via Dirichlet distributions parameterized by neural networks. Despite its popularity, its theoretical foundations and behavior under distributional shift remain poorly understood. In this work, we provide a principled statistical interpretation by proving that EDL training corresponds to amortized variational inference in a hierarchical Bayesian model with a tempered pseudo-likelihood. This perspective reveals a major drawback: standard EDL conflates epistemic and aleatoric uncertainty, leading to systematic overconfidence on out-of-distribution (OOD) inputs. To address this, we introduce Density-Informed Pseudo-count EDL (DIP-EDL), a new parametrization that decouples class prediction from the magnitude of uncertainty by separately estimating the conditional label distribution and the marginal covariate density. This separation preserves evidence in high-density regions while shrinking predictions toward a uniform prior for OOD data. Theoretically, we prove that DIP-EDL achieves asymptotic concentration. Empirically, we show that our method enhances interpretability and improves robustness and uncertainty calibration under distributional shift.

</details>


### [1059] [Hippasus: Effective and Efficient Automatic Feature Augmentation for Machine Learning Tasks on Relational Data](https://arxiv.org/abs/2602.02025)
*Serafeim Papadias,Kostas Patroumpas,Dimitrios Skoutas*

Main category: cs.DB

Relevance: 65.0

TL;DR: Hippasus是一个用于特征增强的模块化框架，通过结合LLM语义推理和统计信号来高效发现和集成跨多表关系的特征，在准确性和效率上均优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 机器学习模型严重依赖特征质量，但有用特征常分散在多个关系表中。特征增强通过连接操作从相关表中发现和集成特征来丰富基础表，但在复杂模式（多表、多跳路径）中扩展仍具挑战性。现有方法在有效性和效率之间存在根本权衡：高准确性需要探索许多候选路径，但穷举探索计算成本过高。

Method: Hippasus采用三个关键技术：1) 结合轻量级统计信号和LLM语义推理在执行前剪枝无希望的连接路径；2) 使用优化的多路连接算法并整合多路径特征，大幅减少执行时间；3) 集成LLM语义理解与统计度量来选择既有语义意义又有预测能力的特征。

Result: 在公开数据集上的实验评估显示，Hippasus相比最先进的基线方法，特征增强准确性提升高达26.8%，同时提供高运行时性能。

Conclusion: Hippasus通过结合LLM语义推理和统计优化，解决了特征增强中有效性和效率的权衡问题，为复杂关系模式下的特征发现提供了高效准确的解决方案。

Abstract: Machine learning models depend critically on feature quality, yet useful features are often scattered across multiple relational tables. Feature augmentation enriches a base table by discovering and integrating features from related tables through join operations. However, scaling this process to complex schemas with many tables and multi-hop paths remains challenging. Feature augmentation must address three core tasks: identify promising join paths that connect the base table to candidate tables, execute these joins to materialize augmented data, and select the most informative features from the results. Existing approaches face a fundamental tradeoff between effectiveness and efficiency: achieving high accuracy requires exploring many candidate paths, but exhaustive exploration is computationally prohibitive. Some methods compromise by considering only immediate neighbors, limiting their effectiveness, while others employ neural models that require expensive training data and introduce scalability limitations. We present Hippasus, a modular framework that achieves both goals through three key contributions. First, we combine lightweight statistical signals with semantic reasoning from Large Language Models to prune unpromising join paths before execution, focusing computational resources on high-quality candidates. Second, we employ optimized multi-way join algorithms and consolidate features from multiple paths, substantially reducing execution time. Third, we integrate LLM-based semantic understanding with statistical measures to select features that are both semantically meaningful and empirically predictive. Our experimental evaluation on publicly available datasets shows that Hippasus substantially improves feature augmentation accuracy by up to 26.8% over state-of-the-art baselines while also offering high runtime performance.

</details>


### [1060] [Well-Posed KL-Regularized Control via Wasserstein and Kalman-Wasserstein KL Divergences](https://arxiv.org/abs/2602.02250)
*Viktor Stein,Adwait Datar,Nihat Ay*

Main category: math.OC

Relevance: 65.0

TL;DR: 论文提出了一种基于Wasserstein几何的KL散度变体，解决了传统KL正则化在支持集不匹配和低噪声极限下的问题，并在最优控制中展示了优越性能。


<details>
  <summary>Details</summary>
Motivation: 传统KL散度正则化在强化学习中广泛使用，但在支持集不匹配时会变成无穷大，在低噪声极限下会退化。需要一种更鲁棒的替代方案来处理这些限制。

Method: 采用统一的信息几何框架，用基于传输的几何（Wasserstein几何）替换KL动态公式中的Fisher-Rao几何，推导出常见分布族的闭式解。在Kalman集成方法中提供正则化启发式的几何解释。

Result: 新提出的散度在支持集不匹配时保持有限，在线性时不变系统的高斯过程噪声设置中，消除了传统KL在过程噪声消失时的奇异性。在双积分器和倒立摆示例中，基于新散度的控制优于传统KL正则化。

Conclusion: 基于Wasserstein的KL变体提供了更鲁棒的正则化方法，解决了传统KL散度的关键限制，在最优控制应用中表现出优越性能。

Abstract: Kullback-Leibler divergence (KL) regularization is widely used in reinforcement learning, but it becomes infinite under support mismatch and can degenerate in low-noise limits. Utilizing a unified information-geometric framework, we introduce (Kalman)-Wasserstein-based KL analogues by replacing the Fisher-Rao geometry in the dynamical formulation of the KL with transport-based geometries, and we derive closed-form values for common distribution families. These divergences remain finite under support mismatch and yield a geometric interpretation of regularization heuristics used in Kalman ensemble methods. We demonstrate the utility of these divergences in KL-regularized optimal control. In the fully tractable setting of linear time-invariant systems with Gaussian process noise, the classical KL reduces to a quadratic control penalty that becomes singular as process noise vanishes. Our variants remove this singularity, yielding well-posed problems. On a double integrator and a cart-pole example, the resulting controls outperform KL-based regularization.

</details>


### [1061] [Representation Learning Enhanced Deep Reinforcement Learning for Optimal Operation of Hydrogen-based Multi-Energy Systems](https://arxiv.org/abs/2602.00027)
*Zhenyu Pu,Yu Yang,Lun Yang,Qing-Shan Jia,Xiaohong Guan,Costas J. Spanos*

Main category: cs.LG

Relevance: 45.0

TL;DR: 本文针对氢基多能源系统（HMES）的优化运行问题，提出了一种结合表示学习的增强深度强化学习框架，以处理HESS的非线性多物理场耦合动态及供需不确定性。


<details>
  <summary>Details</summary>
Motivation: 氢基多能源系统（HMES）作为低碳高效解决方案，能够协调电力、供热和制冷供需，但面临氢能存储系统（HESS）非线性多物理场耦合动态及供需不确定性的挑战，传统方法难以实现最优运行。

Method: 1. 开发了全面捕捉HESS非线性动态和多物理场过程的HMES运行模型；2. 提出增强深度强化学习（DRL）框架，集成新兴表示学习技术，加速和改进时空耦合复杂网络系统的策略优化。

Result: 基于真实数据集的实验表明：1. 综合模型对确保HESS安全可靠运行至关重要；2. 提出的SR-DRL方法在收敛速度和性能上优于传统DRL，能有效降低HMES运行成本并处理系统运行约束。

Conclusion: 表示学习在DRL中能够重组原始状态空间为结构良好、聚类感知的几何表示，从而平滑和促进DRL的学习过程，为复杂能源系统优化提供了新思路。

Abstract: Hydrogen-based multi-energy systems (HMES) have emerged as a promising low-carbon and energy-efficient solution, as it can enable the coordinated operation of electricity, heating and cooling supply and demand to enhance operational flexibility, improve overall energy efficiency, and increase the share of renewable integration. However, the optimal operation of HMES remains challenging due to the nonlinear and multi-physics coupled dynamics of hydrogen energy storage systems (HESS) (consisting of electrolyters, fuel cells and hydrogen tanks) as well as the presence of multiple uncertainties from supply and demand. To address these challenges, this paper develops a comprehensive operational model for HMES that fully captures the nonlinear dynamics and multi-physics process of HESS. Moreover, we propose an enhanced deep reinforcement learning (DRL) framework by integrating the emerging representation learning techniques, enabling substantially accelerated and improved policy optimization for spatially and temporally coupled complex networked systems, which is not provided by conventional DRL. Experimental studies based on real-world datasets show that the comprehensive model is crucial to ensure the safe and reliable of HESS. In addition, the proposed SR-DRL approaches demonstrate superior convergence rate and performance over conventional DRL counterparts in terms of reducing the operation cost of HMES and handling the system operating constraints. Finally, we provide some insights into the role of representation learning in DRL, speculating that it can reorganize the original state space into a well-structured and cluster-aware geometric representation, thereby smoothing and facilitating the learning process of DRL.

</details>


### [1062] [Lightweight Edge Learning via Dataset Pruning](https://arxiv.org/abs/2602.00047)
*Laha Ale,Hu Luo,Mingsheng Cao,Shichao Li,Huanlai Xing,Haifeng Sun*

Main category: cs.LG

Relevance: 45.0

TL;DR: 提出基于数据集剪枝的数据中心化优化框架，通过轻量级重要性评估构建紧凑训练子集，在边缘设备上实现资源高效学习


<details>
  <summary>Details</summary>
Motivation: 边缘学习虽然能保护隐私和降低延迟，但在电池供电的移动设备上训练的计算和能耗过高。现有研究主要优化模型架构用于高效推理，但训练阶段仍受限于处理大量冗余本地数据

Method: 提出数据中心化优化框架，利用数据集剪枝实现资源高效的边缘学习。使用截断预热阶段得到的平均损失统计来评估样本重要性，按动态剪枝比例确定性地保留最关键数据点。该方法模型无关且无需设备间通信

Result: 在标准图像分类基准测试中，框架实现了与剪枝比例成比例的近乎线性的训练延迟和能耗降低，模型精度下降可忽略不计

Conclusion: 数据集剪枝是增强资源受限移动边缘设备学习可持续性和可扩展性的重要补充范式

Abstract: Edge learning facilitates ubiquitous intelligence by enabling model training and adaptation directly on data-generating devices, thereby mitigating privacy risks and communication latency. However, the high computational and energy overhead of on-device training hinders its deployment on battery-powered mobile systems with strict thermal and memory budgets. While prior research has extensively optimized model architectures for efficient inference, the training phase remains bottlenecked by the processing of massive, often redundant, local datasets. In this work, we propose a data-centric optimization framework that leverages dataset pruning to achieve resource-efficient edge learning. Unlike standard methods that process all available data, our approach constructs compact, highly informative training subsets via a lightweight, on-device importance evaluation. Specifically, we utilize average loss statistics derived from a truncated warm-up phase to rank sample importance, deterministically retaining only the most critical data points under a dynamic pruning ratio. This mechanism is model-agnostic and operates locally without inter-device communication. Extensive experiments on standard image classification benchmarks demonstrate that our framework achieves a near-linear reduction in training latency and energy consumption proportional to the pruning ratio, with negligible degradation in model accuracy. These results validate dataset pruning as a vital, complementary paradigm for enhancing the sustainability and scalability of learning on resource-constrained mobile edge devices.

</details>


### [1063] [SPGCL: Effective Graph Contrastive Learning via SVD-Guided Structural Perturbation](https://arxiv.org/abs/2602.00064)
*Hao Deng,Yingping Li,Shuiping Gou,Bo Liu*

Main category: cs.LG

Relevance: 45.0

TL;DR: SPGCL提出了一种通过SVD引导的结构扰动进行鲁棒图对比学习的框架，结合随机边移除和SVD引导的细化步骤来生成多样且语义有意义的视图，提高GNN对结构噪声的鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 现有图对比学习方法存在局限性：随机扰动（如边丢弃）是结构无关的，可能移除关键边；而基于SVD的视图往往变得稠密且缺乏足够的多样性。需要一种方法既能生成多样视图，又能保留全局结构先验。

Method: SPGCL结合轻量级随机边移除和SVD引导的细化步骤，通过稀疏的top-ranked边选择和合并来恢复被错误移除的信息边并引入语义上有意义的缺失链接，避免图稠密化。还包含一个受全局相似性约束正则化的对比融合模块来更好地对齐两个视图。

Result: 在10个基准数据集上的广泛实验表明，SPGCL持续提高了基础GNN的鲁棒性和准确性，优于最先进的图对比学习和结构学习方法。

Conclusion: SPGCL通过平衡边移除和恢复率，显式控制视图间的结构差异，使对比信号反映语义结构差异而非边数量差距，从而提高了图对比学习的鲁棒性。

Abstract: Graph Neural Networks (GNNs) can be highly sensitive to structural noise, including spurious or missing edges caused by adversarial attacks or non-adversarial imperfections. Existing graph contrastive learning methods typically rely on either random perturbations (e.g., edge dropping) to generate diverse views or purely spectral augmentations (e.g., SVD) to preserve global structural priors. However, random perturbations are structure-agnostic and may remove critical edges, while SVD-based views often become dense and lack sufficient diversity. To bridge this gap, we propose SPGCL, a robust graph contrastive learning framework via SVD-guided structural perturbation. SPGCL couples lightweight stochastic edge removal with an SVD-guided refinement step that can recover mistakenly removed informative edges and introduce semantically meaningful missing links while avoiding graph densification through sparse top-ranked edge selection and merging. By balancing edge removal and recovery rates, SPGCL explicitly controls structural discrepancy between views so that contrastive signals reflect semantic structural differences rather than edge-count gaps. We further incorporate a contrastive fusion module regularized by a global similarity constraint to better align the two views. Extensive experiments on ten benchmark datasets demonstrate that SPGCL consistently improves robustness and accuracy of base GNNs, outperforming state-of-the-art graph contrastive learning and structure learning methods.

</details>


### [1064] [Modality as Heterogeneity: Node Splitting and Graph Rewiring for Multimodal Graph Learning](https://arxiv.org/abs/2602.00067)
*Yihan Zhang,Ercan E. Kuruoglu*

Main category: cs.LG

Relevance: 45.0

TL;DR: NSG-MoE：一种多模态图学习框架，通过节点分裂和图重连机制结合结构化MoE架构，解决多模态图中的模态混淆问题，在保持结构信息和多模态语义的同时提升性能。


<details>
  <summary>Details</summary>
Motivation: 多模态图具有丰富的表示能力和广泛适用性，但面临严重的模态混淆挑战。传统通用GNNs在处理多模态图时会出现不希望的混合效应，需要专门的方法来明确分解模态特定信息。

Method: 提出NSG-MoE框架：1）节点分裂和图重连机制，将每个节点分解为模态特定组件；2）结构化MoE架构，分配关系感知专家处理异构消息流；3）结合谱分析和信息论分析解释其解耦行为。

Result: 在三个多模态基准测试中，NSG-MoE始终超越强基线方法。尽管包含MoE架构，但保持了有竞争力的训练效率。谱分析显示NSG在模态特定子空间上执行自适应滤波，信息论分析表明架构约束减少了数据和参数间的互信息，提升了泛化能力。

Conclusion: NSG-MoE通过明确的模态分解和结构化专家分配，有效解决了多模态图中的模态混淆问题，在保持效率的同时提升了性能，并通过理论分析解释了其解耦机制和泛化优势。

Abstract: Multimodal graphs are gaining increasing attention due to their rich representational power and wide applicability, yet they introduce substantial challenges arising from severe modality confusion. To address this issue, we propose NSG (Node Splitting Graph)-MoE, a multimodal graph learning framework that integrates a node-splitting and graph-rewiring mechanism with a structured Mixture-of-Experts (MoE) architecture. It explicitly decomposes each node into modality-specific components and assigns relation-aware experts to process heterogeneous message flows, thereby preserving structural information and multimodal semantics while mitigating the undesirable mixing effects commonly observed in general-purpose GNNs. Extensive experiments on three multimodal benchmarks demonstrate that NSG-MoE consistently surpasses strong baselines. Despite incorporating MoE -- which is typically computationally heavy -- our method achieves competitive training efficiency. Beyond empirical results, we provide a spectral analysis revealing that NSG performs adaptive filtering over modality-specific subspaces, thus explaining its disentangling behavior. Furthermore, an information-theoretic analysis shows that the architectural constraints imposed by NSG reduces mutual information between data and parameters and improving generalization capability.

</details>


### [1065] [Generative AI-enhanced Probabilistic Multi-Fidelity Surrogate Modeling Via Transfer Learning](https://arxiv.org/abs/2602.00072)
*Jice Zeng,David Barajas-Solano,Hui Chen*

Main category: cs.LG

Relevance: 45.0

TL;DR: 提出基于生成式迁移学习的概率多保真度代理框架，使用归一化流作为主干，通过两阶段训练（先在大量低保真数据上预训练，再在少量高保真数据上微调）解决工程系统中高保真数据稀缺问题。


<details>
  <summary>Details</summary>
Motivation: 机器学习代理的性能严重依赖数据质量和数量，但高保真数据通常稀缺且计算成本高，低保真数据丰富但准确性低。需要解决数据稀缺问题，开发高效的多保真度代理框架。

Method: 使用归一化流生成模型作为主干，采用两阶段训练：1）在大量低保真数据集上预训练学习概率前向模型；2）在少量高保真数据集上微调，通过知识转移校正低保真-高保真差异。引入满射层与标准耦合块结合，实现维度约减同时保持精确似然训练能力。

Result: 在钢筋混凝土板基准测试中验证，结合大量粗网格模拟（低保真）和有限细网格模拟（高保真）。模型实现具有高保真精度的概率预测，显著优于仅使用低保真的基线方法，且使用更少高保真评估。

Conclusion: 该框架为复杂工程系统提供了一条实用路径，实现数据高效、生成式AI驱动的代理模型，能够进行快速概率预测并量化不确定性。

Abstract: The performance of machine learning surrogates is critically dependent on data quality and quantity. This presents a major challenge, as high-fidelity (HF) data is often scarce and computationally expensive to acquire, while low-fidelity (LF) data is abundant but less accurate. To address this data scarcity problem, we develop a probabilistic multi-fidelity surrogate framework based on generative transfer learning. We employ a normalizing flow (NF) generative model as the backbone, which is trained in two phases: (i) the NF is first pretrained on a large LF dataset to learn a probabilistic forward model; (ii) the pretrained model is then fine-tuned on a small HF dataset, allowing it to correct for LF-HF discrepancies via knowledge transfer. To relax the dimension-preserving constraint of standard bijective NFs, we integrate surjective (dimension-reducing) layers with standard coupling blocks. This architecture enables learned dimension reduction while preserving the ability to train with exact likelihoods. The resulting surrogate provides fast probabilistic predictions with quantified uncertainty and significantly outperforms LF-only baselines while using fewer HF evaluations. We validate the approach on a reinforced concrete slab benchmark, combining many coarse-mesh (LF) simulations with a limited set of fine-mesh (HF) simulations. The proposed model achieves probabilistic predictions with HF accuracy, demonstrating a practical path toward data-efficient, generative AI-driven surrogates for complex engineering systems.

</details>


### [1066] [Variational Approach for Job Shop Scheduling](https://arxiv.org/abs/2602.00408)
*Seung Heon Oh,Jiwon Baek,Ki Young Cho,Hee Chang Yoon,Jong Hun Woo*

Main category: cs.LG

Relevance: 45.0

TL;DR: 提出VG2S框架解决作业车间调度问题，首次引入变分推理，通过ELBO目标函数解耦表示学习和策略优化，提升训练稳定性和零样本泛化能力。


<details>
  <summary>Details</summary>
Motivation: 传统深度强化学习方法在作业车间调度问题中面临训练不稳定和泛化能力有限的问题，主要原因是同时优化表示学习和策略执行。需要一种能够解耦这两部分的方法来提升性能。

Method: 提出变分图到调度器框架，首次将变分推理引入JSSP领域，基于ELBO和最大熵强化学习推导概率目标函数。通过变分图编码器学习调度实例的稳健结构表示，实现表示学习和策略优化的数学解耦。

Result: VG2S框架显著提升训练稳定性，对超参数变化具有鲁棒性。在DMU和SWV等大规模基准实例上，相比最先进的DRL基线和传统调度规则，展现出优越的零样本泛化能力。

Conclusion: 通过变分推理解耦表示学习和策略优化是解决JSSP问题的有效方法，VG2S框架在训练稳定性和泛化能力方面优于现有方法，为制造调度问题提供了新解决方案。

Abstract: This paper proposes a novel Variational Graph-to-Scheduler (VG2S) framework for solving the Job Shop Scheduling Problem (JSSP), a critical task in manufacturing that directly impacts operational efficiency and resource utilization. Conventional Deep Reinforcement Learning (DRL) approaches often face challenges such as non-stationarity during training and limited generalization to unseen problem instances because they optimize representation learning and policy execution simultaneously. To address these issues, we introduce variational inference to the JSSP domain for the first time and derive a probabilistic objective based on the Evidence of Lower Bound (ELBO) with maximum entropy reinforcement learning. By mathematically decoupling representation learning from policy optimization, the VG2S framework enables the agent to learn robust structural representations of scheduling instances through a variational graph encoder. This approach significantly enhances training stability and robustness against hyperparameter variations. Extensive experiments demonstrate that the proposed method exhibits superior zero-shot generalization compared with state-of-the-art DRL baselines and traditional dispatching rules, particularly on large-scale and challenging benchmark instances such as DMU and SWV.

</details>


### [1067] [LatentTrack: Sequential Weight Generation via Latent Filtering](https://arxiv.org/abs/2602.00458)
*Omer Haq*

Main category: cs.LG

Relevance: 45.0

TL;DR: LatentTrack (LT) 是一种用于非平稳动态下在线概率预测的序列神经网络架构，通过在低维潜空间进行因果贝叶斯滤波，使用轻量级超网络生成预测模型参数，实现恒定时间在线适应而无需梯度更新。


<details>
  <summary>Details</summary>
Motivation: 解决在非平稳动态环境下进行在线概率预测的挑战，传统方法需要每步梯度更新，计算成本高。需要一种能够持续适应分布变化、保持校准预测且计算效率高的方法。

Method: LT在低维潜空间执行因果贝叶斯滤波，使用轻量级超网络在每个时间步生成预测模型参数。采用预测-生成-更新的滤波框架：学习到的潜模型预测下一个潜分布，通过摊销推理使用新观测更新，支持结构化和非结构化潜动态的统一目标，通过潜轨迹的蒙特卡洛推理产生校准预测混合。

Result: 在Jena Climate基准测试的长时域在线回归评估中，LT始终比有状态的序列基线和静态不确定性感知基线获得更低的负对数似然和均方误差，具有竞争力的校准性能，表明潜条件函数演化是传统潜状态建模在分布变化下的有效替代方案。

Conclusion: 潜条件函数演化是传统潜状态建模在非平稳动态下的有效替代方案，能够在分布变化下实现高效、校准的在线概率预测，无需每步梯度更新。

Abstract: We introduce LatentTrack (LT), a sequential neural architecture for online probabilistic prediction under nonstationary dynamics. LT performs causal Bayesian filtering in a low-dimensional latent space and uses a lightweight hypernetwork to generate predictive model parameters at each time step, enabling constant-time online adaptation without per-step gradient updates.
  At each time step, a learned latent model predicts the next latent distribution, which is updated via amortized inference using new observations, yielding a predict--generate--update filtering framework in function space. The formulation supports both structured (Markovian) and unstructured latent dynamics within a unified objective, while Monte Carlo inference over latent trajectories produces calibrated predictive mixtures with fixed per-step cost. Evaluated on long-horizon online regression using the Jena Climate benchmark, LT consistently achieves lower negative log-likelihood and mean squared error than stateful sequential and static uncertainty-aware baselines, with competitive calibration, demonstrating that latent-conditioned function evolution is an effective alternative to traditional latent-state modeling under distribution shift.

</details>


### [1068] [Partition of Unity Neural Networks for Interpretable Classification with Explicit Class Regions](https://arxiv.org/abs/2602.00511)
*Akram Aldroubi*

Main category: cs.LG

Relevance: 45.0

TL;DR: PUNN是一种可解释的神经网络架构，通过学习的单位分解直接生成类别概率，无需softmax层，提供透明的分类决策。


<details>
  <summary>Details</summary>
Motivation: 传统softmax分类器的类别区域通过logits的不等式系统隐式定义，难以解释和可视化。需要一种能够直接提供透明类别概率分配的架构。

Method: 提出PUNN架构，学习k个非负函数h₁,...,hₖ满足∑hᵢ(x)=1，每个hᵢ(x)直接表示P(class i|x)。门函数gᵢ可使用多种激活函数和参数化设计，从灵活的MLP到参数高效的形状感知设计。

Result: 在合成数据、UCI基准和MNIST上，基于MLP的PUNN准确率与标准多层感知机相差0.3-0.6%。当几何先验匹配数据结构时，形状感知门函数以最多300倍更少的参数实现相当准确率。

Conclusion: PUNN证明可解释性设计架构可以与黑盒模型竞争，同时提供透明的类别概率分配，为可解释AI提供了有前景的方向。

Abstract: Despite their empirical success, neural network classifiers remain difficult to interpret. In softmax-based models, class regions are defined implicitly as solutions to systems of inequalities among logits, making them difficult to extract and visualize. We introduce Partition of Unity Neural Networks (PUNN), an architecture in which class probabilities arise directly from a learned partition of unity, without requiring a softmax layer.
  PUNN constructs $k$ nonnegative functions $h_1, \ldots, h_k$ satisfying $\sum_i h_i(x) = 1$, where each $h_i(x)$ directly represents $P(\text{class } i \mid x)$. Unlike softmax, where class regions are defined implicitly through coupled inequalities among logits, each PUNN partition function $h_i$ directly defines the probability of class $i$ as a standalone function of $x$.
  We prove that PUNN is dense in the space of continuous probability maps on compact domains. The gate functions $g_i$ that define the partition can use various activation functions (sigmoid, Gaussian, bump) and parameterizations ranging from flexible MLPs to parameter-efficient shape-informed designs (spherical shells, ellipsoids, spherical harmonics).
  Experiments on synthetic data, UCI benchmarks, and MNIST show that PUNN with MLP-based gates achieves accuracy within 0.3--0.6\% of standard multilayer perceptrons. When geometric priors match the data structure, shape-informed gates achieve comparable accuracy with up to 300$\times$ fewer parameters. These results demonstrate that interpretable-by-design architectures can be competitive with black-box models while providing transparent class probability assignments.

</details>


### [1069] [When Classes Evolve: A Benchmark and Framework for Stage-Aware Class-Incremental Learning](https://arxiv.org/abs/2602.00573)
*Zheng Zhang,Tao Hu,Xueheng Li,Yang Wang,Rui Li,Jie Zhang,Chengjun Xie*

Main category: cs.LG

Relevance: 45.0

TL;DR: 论文提出Stage-CIL范式，解决类别增量学习中类别内部形态演化问题，引入Stage-Bench数据集和STAGE方法，显著优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 传统类别增量学习假设类别形态静态，忽略了类别内部演化现象（如幼虫变蝴蝶），需要同时处理类间区分和类内形态适应。

Method: 提出Stage-CIL范式，引入Stage-Bench数据集（10个领域，2阶段），开发STAGE方法：在固定大小记忆池中学习抽象可迁移的演化模式，解耦语义身份和变换动态。

Result: STAGE方法在Stage-Bench上显著优于现有最先进方法，能同时有效处理类间区分和类内形态适应问题。

Conclusion: 类别增量学习需要考虑类内形态演化，Stage-CIL范式、Stage-Bench数据集和STAGE方法为解决这一挑战提供了系统框架和有效解决方案。

Abstract: Class-Incremental Learning (CIL) aims to sequentially learn new classes while mitigating catastrophic forgetting of previously learned knowledge. Conventional CIL approaches implicitly assume that classes are morphologically static, focusing primarily on preserving previously learned representations as new classes are introduced. However, this assumption neglects intra-class evolution: a phenomenon wherein instances of the same semantic class undergo significant morphological transformations, such as a larva turning into a butterfly. Consequently, a model must both discriminate between classes and adapt to evolving appearances within a single class. To systematically address this challenge, we formalize Stage-Aware CIL (Stage-CIL), a paradigm in which each class is learned progressively through distinct morphological stages. To facilitate rigorous evaluation within this paradigm, we introduce the Stage-Bench, a 10-domain, 2-stages dataset and protocol that jointly measure inter- and intra-class forgetting. We further propose STAGE, a novel method that explicitly learns abstract and transferable evolution patterns within a fixed-size memory pool. By decoupling semantic identity from transformation dynamics, STAGE enables accurate prediction of future morphologies based on earlier representations. Extensive empirical evaluation demonstrates that STAGE consistently and substantially outperforms existing state-of-the-art approaches, highlighting its effectiveness in simultaneously addressing inter-class discrimination and intra-class morphological adaptation.

</details>


### [1070] [Bridging Time and Frequency: A Joint Modeling Framework for Irregular Multivariate Time Series Forecasting](https://arxiv.org/abs/2602.00582)
*Xiangfei Qiu,Kangjia Yan,Xvyuan Liu,Xingjian Wu,Jilin Hu*

Main category: cs.LG

Relevance: 45.0

TL;DR: TFMixer：一种用于不规则多元时间序列预测的联合时频建模框架，通过可学习的非均匀离散傅里叶变换提取频域表示，并结合基于查询的补丁混合机制进行时域建模。


<details>
  <summary>Details</summary>
Motivation: 不规则多元时间序列预测面临非均匀采样和变量异步性的挑战，这些不规则性违反了标准模型的等距假设，阻碍了局部时间建模，并使经典的频域方法无法有效捕捉全局周期性结构。

Method: TFMixer包含三个核心组件：1) 全局频率模块使用可学习的非均匀离散傅里叶变换直接从不规则时间戳中提取频谱表示；2) 局部时间模块引入基于查询的补丁混合机制，自适应聚合信息时间补丁以缓解信息密度不平衡；3) 融合时域和频域表示生成预测，并利用逆NUDFT进行显式季节性外推。

Result: 在真实世界数据集上的广泛实验表明，TFMixer实现了最先进的性能。

Conclusion: TFMixer通过联合时频建模有效解决了不规则多元时间序列预测的挑战，为处理非均匀采样和异步数据提供了有效的解决方案。

Abstract: Irregular multivariate time series forecasting (IMTSF) is challenging due to non-uniform sampling and variable asynchronicity. These irregularities violate the equidistant assumptions of standard models, hindering local temporal modeling and rendering classical frequency-domain methods ineffective for capturing global periodic structures. To address this challenge, we propose TFMixer, a joint time-frequency modeling framework for IMTS forecasting. Specifically, TFMixer incorporates a Global Frequency Module that employs a learnable Non-Uniform Discrete Fourier Transform (NUDFT) to directly extract spectral representations from irregular timestamps. In parallel, the Local Time Module introduces a query-based patch mixing mechanism to adaptively aggregate informative temporal patches and alleviate information density imbalance. Finally, TFMixer fuses the time-domain and frequency-domain representations to generate forecasts and further leverages inverse NUDFT for explicit seasonal extrapolation. Extensive experiments on real-world datasets demonstrate the state--of-the-art performance of TFMixer.

</details>


### [1071] [Pareto-Conditioned Diffusion Models for Offline Multi-Objective Optimization](https://arxiv.org/abs/2602.00737)
*Jatan Shrestha,Santeri Heiskanen,Kari Hepola,Severi Rissanen,Pekka Jääskeläinen,Joni Pajarinen*

Main category: cs.LG

Relevance: 45.0

TL;DR: 提出Pareto-Conditioned Diffusion (PCD)框架，将离线多目标优化转化为条件采样问题，通过直接条件化于期望权衡来避免显式代理模型，在标准基准测试中表现出色。


<details>
  <summary>Details</summary>
Motivation: 现实应用中常需平衡多个竞争目标的多目标优化，离线场景下主要挑战是从静态数据中泛化。现有方法需要显式代理模型，限制了泛化能力。

Method: PCD将离线MOO构建为条件采样问题，通过扩散模型直接条件化于期望权衡。采用重加权策略聚焦高性能样本，使用参考方向机制引导采样到训练数据外的新颖区域。

Result: 在标准离线MOO基准测试中，PCD取得了极具竞争力的性能，更重要的是在不同任务间表现出比现有方法更好的一致性。

Conclusion: PCD为离线多目标优化提供了新颖有效的框架，通过条件采样方法避免了代理模型限制，在泛化能力和任务一致性方面表现优异。

Abstract: Multi-objective optimization (MOO) arises in many real-world applications where trade-offs between competing objectives must be carefully balanced. In the offline setting, where only a static dataset is available, the main challenge is generalizing beyond observed data. We introduce Pareto-Conditioned Diffusion (PCD), a novel framework that formulates offline MOO as a conditional sampling problem. By conditioning directly on desired trade-offs, PCD avoids the need for explicit surrogate models. To effectively explore the Pareto front, PCD employs a reweighting strategy that focuses on high-performing samples and a reference-direction mechanism to guide sampling towards novel, promising regions beyond the training data. Experiments on standard offline MOO benchmarks show that PCD achieves highly competitive performance and, importantly, demonstrates greater consistency across diverse tasks than existing offline MOO approaches.

</details>


### [1072] [Assessing the Impact of Image Dataset Features on Privacy-Preserving Machine Learning](https://arxiv.org/abs/2409.01329)
*Lucas Lange,Maurice-Maximilian Heykeroth,Erhard Rahm*

Main category: cs.LG

Relevance: 45.0

TL;DR: 该研究分析了图像数据集特征如何影响CNN模型的效用-隐私权衡，发现类别不平衡会增加少数类别的脆弱性，但差分隐私可以缓解这一问题；类别较少的数集能同时提升模型效用和隐私保护；而高熵或低FDR数据集则会恶化效用-隐私权衡。


<details>
  <summary>Details</summary>
Motivation: 机器学习模型在敏感数据上面临安全挑战，可能被攻击并泄露信息。隐私保护机器学习使用差分隐私来平衡效用和隐私，但需要理解数据集特征如何影响这种权衡，以便为实践者提供指导。

Method: 通过分析多个图像数据集和不同的隐私预算，研究识别了影响私有和非私有CNN模型效用和脆弱性的数据集特征。使用差分隐私技术，考察了类别不平衡、类别数量、熵和Fisher判别比等特征对效用-隐私权衡的影响。

Result: 发现类别不平衡会增加少数类别的脆弱性，但差分隐私可以缓解这一问题；类别较少的数集能同时提升模型效用和隐私保护；高熵或低Fisher判别比数据集则会恶化效用-隐私权衡。

Conclusion: 这些见解为实践者和研究人员提供了有价值的指导，帮助他们基于数据集特征估计和优化效用-隐私权衡，从而通过数据修改和隐私调整获得更好的结果。

Abstract: Machine Learning (ML) is crucial in many sectors, including computer vision. However, ML models trained on sensitive data face security challenges, as they can be attacked and leak information. Privacy-Preserving Machine Learning (PPML) addresses this by using Differential Privacy (DP) to balance utility and privacy. This study identifies image dataset characteristics that affect the utility and vulnerability of private and non-private Convolutional Neural Network (CNN) models. Through analyzing multiple datasets and privacy budgets, we find that imbalanced datasets increase vulnerability in minority classes, but DP mitigates this issue. Datasets with fewer classes improve both model utility and privacy, while high entropy or low Fisher Discriminant Ratio (FDR) datasets deteriorate the utility-privacy trade-off. These insights offer valuable guidance for practitioners and researchers in estimating and optimizing the utility-privacy trade-off in image datasets, helping to inform data and privacy modifications for better outcomes based on dataset characteristics.

</details>


### [1073] [Single-Edge Node Injection Threats to GNN-Based Security Monitoring in Industrial Graph Systems](https://arxiv.org/abs/2602.01113)
*Wenjie Liang,Ranhui Yan,Jia Cai,You-Gan Wang*

Main category: cs.LG

Relevance: 45.0

TL;DR: 论文提出SEGIA攻击方法，通过单边注入伪造节点来攻击工业图神经网络系统，在有限资源下实现高攻击成功率


<details>
  <summary>Details</summary>
Motivation: 工业GNN系统面临节点注入攻击风险，攻击者通过注入少量伪造节点（如恶意传感器、虚拟端点）来影响下游决策，同时规避基于拓扑和同质性的防护措施

Method: 提出SEGIA攻击方法：1）使用剪枝SGC代理模型；2）多跳邻域采样；3）基于反向图卷积的特征合成；4）相似性正则化目标以保持局部同质性并规避边剪枝

Result: 理论分析和广泛评估显示，在显著更小的边预算下，攻击成功率比代表性基线至少高25%，揭示了工业GNN部署的系统级风险

Conclusion: 工业GNN部署存在系统级安全风险，需要轻量级准入验证和邻域一致性监控来防御此类攻击

Abstract: Graph neural networks (GNNs) are increasingly adopted in industrial graph-based monitoring systems (e.g., Industrial internet of things (IIoT) device graphs, power-grid topology models, and manufacturing communication networks) to support anomaly detection, state estimation, and asset classification. In such settings, an adversary that compromises a small number of edge devices may inject counterfeit nodes (e.g., rogue sensors, virtualized endpoints, or spoofed substations) to bias downstream decisions while evading topology- and homophily-based sanitization. This paper formulates deployment-oriented node-injection attacks under constrained resources and proposes the \emph{Single-Edge Graph Injection Attack} (SEGIA), in which each injected node attaches to the operational graph through a single edge. SEGIA integrates a pruned SGC surrogate, multi-hop neighborhood sampling, and reverse graph convolution-based feature synthesis with a similarity-regularized objective to preserve local homophily and survive edge pruning. Theoretical analysis and extensive evaluations across datasets and defenses show at least $25\%$ higher attack success than representative baselines under substantially smaller edge budgets. These results indicate a system-level risk in industrial GNN deployments and motivate lightweight admission validation and neighborhood-consistency monitoring.

</details>


### [1074] [A Unified Matrix-Spectral Framework for Stability and Interpretability in Deep Learning](https://arxiv.org/abs/2602.01136)
*Ronald Katende*

Main category: cs.LG

Relevance: 45.0

TL;DR: 该论文提出了一个统一的矩阵谱框架来分析深度神经网络的稳定性和可解释性，通过将网络表示为数据依赖的线性算子乘积，揭示了控制输入扰动、标签噪声和训练动态敏感性的谱量。


<details>
  <summary>Details</summary>
Motivation: 深度神经网络在稳定性和可解释性方面存在挑战，特别是在面对输入扰动、标签噪声时的敏感性。现有方法缺乏统一的框架来分析这些稳定性问题，需要建立连接谱特性与网络鲁棒性的理论框架。

Method: 开发了统一的矩阵谱框架，将网络表示为数据依赖的线性算子乘积。引入了全局矩阵稳定性指数，聚合了雅可比矩阵、参数梯度、神经正切核算子和损失海森矩阵的谱信息。使用谱熵来捕捉典型敏感性而非最坏情况敏感性。

Result: 在MNIST、CIFAR-10和CIFAR-100上的合成实验和控制研究表明，适度的谱正则化显著改善了归因稳定性，即使全局谱摘要变化很小。建立了谱集中度与分析稳定性之间的精确联系。

Conclusion: 该框架为鲁棒性感知的模型设计和训练提供了实用指导，证明了谱分析在理解深度神经网络稳定性方面的重要性，并提供了可计算的诊断工具和稳定性导向的正则化原则。

Abstract: We develop a unified matrix-spectral framework for analyzing stability and interpretability in deep neural networks. Representing networks as data-dependent products of linear operators reveals spectral quantities governing sensitivity to input perturbations, label noise, and training dynamics.
  We introduce a Global Matrix Stability Index that aggregates spectral information from Jacobians, parameter gradients, Neural Tangent Kernel operators, and loss Hessians into a single stability scale controlling forward sensitivity, attribution robustness, and optimization conditioning. We further show that spectral entropy refines classical operator-norm bounds by capturing typical, rather than purely worst-case, sensitivity.
  These quantities yield computable diagnostics and stability-oriented regularization principles. Synthetic experiments and controlled studies on MNIST, CIFAR-10, and CIFAR-100 confirm that modest spectral regularization substantially improves attribution stability even when global spectral summaries change little.
  The results establish a precise connection between spectral concentration and analytic stability, providing practical guidance for robustness-aware model design and training.

</details>


### [1075] [Learning from Anonymized and Incomplete Tabular Data](https://arxiv.org/abs/2602.01217)
*Lucas Lange,Adrian Böttinger,Victor Christen,Anushka Vidanage,Peter Christen,Erhard Rahm*

Main category: cs.LG

Relevance: 45.0

TL;DR: 该论文研究如何在用户驱动隐私保护下（数据包含原始值、泛化值和缺失值）进行机器学习，提出了考虑异构匿名化的数据转换策略，并评估了标准插补和LLM方法。


<details>
  <summary>Details</summary>
Motivation: 用户驱动隐私保护允许个人控制数据共享粒度，导致数据集混合了原始值、泛化值和缺失值。这种表示对隐私保护直观，但对机器学习构成挑战，因为传统方法将非原始值视为新类别或缺失值，丢弃了泛化语义。

Method: 提出了考虑异构匿名化的新型数据转换策略，并与标准插补方法和基于LLM的方法进行比较评估。使用多个数据集、隐私配置和部署场景进行实验。

Result: 实验表明：1）泛化值优于纯抑制；2）最佳数据准备策略取决于具体场景；3）一致的数据表示对保持下游效用至关重要；4）提出的方法能可靠地恢复效用。

Conclusion: 有效学习与适当处理匿名化值密切相关。在用户驱动隐私保护下，需要考虑泛化语义的数据处理方法才能获得良好的机器学习性能。

Abstract: User-driven privacy allows individuals to control whether and at what granularity their data is shared, leading to datasets that mix original, generalized, and missing values within the same records and attributes. While such representations are intuitive for privacy, they pose challenges for machine learning, which typically treats non-original values as new categories or as missing, thereby discarding generalization semantics. For learning from such tabular data, we propose novel data transformation strategies that account for heterogeneous anonymization and evaluate them alongside standard imputation and LLM-based approaches. We employ multiple datasets, privacy configurations, and deployment scenarios, demonstrating that our method reliably regains utility. Our results show that generalized values are preferable to pure suppression, that the best data preparation strategy depends on the scenario, and that consistent data representations are crucial for maintaining downstream utility. Overall, our findings highlight that effective learning is tied to the appropriate handling of anonymized values.

</details>


### [1076] [Mechanistic Interpretability of Brain-to-Speech Models Across Speech Modes](https://arxiv.org/abs/2602.01247)
*Maryam Maghsoudi,Ayushi Mishra*

Main category: cs.LG

Relevance: 45.0

TL;DR: 该论文使用机制可解释性方法研究神经语音解码器中不同语音模态（发声、默读、想象）的内部表征机制，发现语音模态共享连续的因果流形，跨模态传输由紧凑的层特定子空间介导。


<details>
  <summary>Details</summary>
Motivation: 尽管脑到语音解码模型在多种语音模态中表现稳健，但这些模型如何在不同语音模态间捕获和传输信息的基本机制尚不清楚。研究者希望理解神经语音解码器内部表征的因果结构。

Method: 采用机制可解释性方法：1) 跨模态激活修补；2) 三模态插值检查表征连续性；3) 粗到细因果追踪和因果擦除以定位因果结构；4) 神经元级激活修补分析分布特性。

Result: 发现语音模态位于共享的连续因果流形上，跨模态传输由紧凑的层特定子空间介导，而非扩散活动。小但非分布式的神经元子集影响跨模态传输，揭示了跨语音模态的层次化和方向依赖表征结构。

Conclusion: 研究为脑到语音解码模型中语音模态信息的组织和使用提供了因果解释，揭示了跨语音模态的层次化和方向依赖表征结构，对理解神经解码器的内部工作机制有重要意义。

Abstract: Brain-to-speech decoding models demonstrate robust performance in vocalized, mimed, and imagined speech; yet, the fundamental mechanisms via which these models capture and transmit information across different speech modalities are less explored. In this work, we use mechanistic interpretability to causally investigate the internal representations of a neural speech decoder. We perform cross-mode activation patching of internal activations across speech modes, and use tri-modal interpolation to examine whether speech representations vary discretely or continuously. We use coarse-to-fine causal tracing and causal scrubbing to find localized causal structure, allowing us to find internal subspaces that are sufficient for cross-mode transfer. In order to determine how finely distributed these effects are within layers, we perform neuron-level activation patching. We discover that small but not distributed subsets of neurons, rather than isolated units, affect the cross-mode transfer. Our results show that speech modes lie on a shared continuous causal manifold, and cross-mode transfer is mediated by compact, layer-specific subspaces rather than diffuse activity. Together, our findings give a causal explanation for how speech modality information is organized and used in brain-to-speech decoding models, revealing hierarchical and direction-dependent representational structure across speech modes.

</details>


### [1077] [From Intents to Actions: Agentic AI in Autonomous Networks](https://arxiv.org/abs/2602.01271)
*Burak Demirel,Pablo Soldati,Yu Wang*

Main category: cs.LG

Relevance: 45.0

TL;DR: 论文提出了一种基于多智能体的AI系统，用于意图驱动的自治网络，通过三个专门智能体（解析器、优化器、控制器）将高层意图转化为网络控制动作。


<details>
  <summary>Details</summary>
Motivation: 电信网络需要自主运行并支持具有多样化意图的异构服务，但现有启发式方法无法将高层意图（如低延迟、高吞吐量）转化为具体的控制动作。

Method: 设计了三个专门智能体：1) 基于语言模型的监督解析器，将意图解析为可执行优化模板；2) 优化器，将模板转化为可处理的优化问题并分析权衡；3) 基于多目标强化学习的偏好驱动控制器，在帕累托前沿附近操作。

Result: 该系统使网络能够以可扩展的方式自主解释、推理、适应和响应多样化意图和网络条件，实现意图驱动的自治网络。

Conclusion: 提出的多智能体AI系统成功解决了将高层意图转化为具体网络控制动作的挑战，为自治网络提供了可扩展的解决方案。

Abstract: Telecommunication networks are increasingly expected to operate autonomously while supporting heterogeneous services with diverse and often conflicting intents -- that is, performance objectives, constraints, and requirements specific to each service. However, transforming high-level intents -- such as ultra-low latency, high throughput, or energy efficiency -- into concrete control actions (i.e., low-level actuator commands) remains beyond the capability of existing heuristic approaches. This work introduces an Agentic AI system for intent-driven autonomous networks, structured around three specialized agents. A supervisory interpreter agent, powered by language models, performs both lexical parsing of intents into executable optimization templates and cognitive refinement based on feedback, constraint feasibility, and evolving network conditions. An optimizer agent converts these templates into tractable optimization problems, analyzes trade-offs, and derives preferences across objectives. Lastly, a preference-driven controller agent, based on multi-objective reinforcement learning, leverages these preferences to operate near the Pareto frontier of network performance that best satisfies the original intent. Collectively, these agents enable networks to autonomously interpret, reason over, adapt to, and act upon diverse intents and network conditions in a scalable manner.

</details>


### [1078] [Finding Differentially Private Second Order Stationary Points in Stochastic Minimax Optimization](https://arxiv.org/abs/2602.01339)
*Difei Xu,Youming Tao,Meng Ding,Chenglin Fan,Di Wang*

Main category: cs.LG

Relevance: 45.0

TL;DR: 本文首次研究了在随机非凸极小极大优化中寻找差分隐私二阶平稳点的问题，提出了结合嵌套梯度下降-上升、SPIDER方差缩减和高斯扰动的纯一阶方法。


<details>
  <summary>Details</summary>
Motivation: 现有文献要么只关注极小极大问题的一阶平稳点，要么关注经典随机最小化问题的二阶平稳点。本文首次统一处理了经验风险和总体风险，填补了差分隐私二阶平稳点在随机极小极大优化中的研究空白。

Method: 提出纯一阶方法：结合嵌套梯度下降-上升方案、SPIDER风格方差缩减和高斯扰动来确保隐私。关键技术是块式(q周期)分析，控制随机方差和隐私噪声的累积，无需在整个迭代范围内求和。

Result: 在标准平滑性、Hessian-Lipschitz性和强凹性假设下，建立了达到(α,√(ρ_Φα))近似二阶平稳点的高概率保证。经验风险目标的α=O((√d/nε)^{2/3})，总体目标的α=O(1/n^{1/3} + (√d/nε)^{1/2})，匹配了隐私一阶平稳性的最佳已知速率。

Conclusion: 本文首次为随机极小极大优化中的差分隐私二阶平稳点提供了系统研究，提出的方法在经验风险和总体风险设置下都达到了最优收敛速率，为隐私保护的非凸优化提供了重要理论保证。

Abstract: We provide the first study of the problem of finding differentially private (DP) second-order stationary points (SOSP) in stochastic (non-convex) minimax optimization. Existing literature either focuses only on first-order stationary points for minimax problems or on SOSP for classical stochastic minimization problems. This work provides, for the first time, a unified and detailed treatment of both empirical and population risks. Specifically, we propose a purely first-order method that combines a nested gradient descent--ascent scheme with SPIDER-style variance reduction and Gaussian perturbations to ensure privacy. A key technical device is a block-wise ($q$-period) analysis that controls the accumulation of stochastic variance and privacy noise without summing over the full iteration horizon, yielding a unified treatment of both empirical-risk and population formulations. Under standard smoothness, Hessian-Lipschitzness, and strong concavity assumptions, we establish high-probability guarantees for reaching an $(α,\sqrt{ρ_Φα})$-approximate second-order stationary point with $α= \mathcal{O}( (\frac{\sqrt{d}}{n\varepsilon})^{2/3})$ for empirical risk objectives and $\mathcal{O}(\frac{1}{n^{1/3}} + (\frac{\sqrt{d}}{n\varepsilon})^{1/2})$ for population objectives, matching the best known rates for private first-order stationarity.

</details>


### [1079] [Theoretical Analysis of Measure Consistency Regularization for Partially Observed Data](https://arxiv.org/abs/2602.01437)
*Yinsong Wang,Shahin Shahrampour*

Main category: cs.LG

Relevance: 45.0

TL;DR: 该论文提出了测量一致性正则化(MCR)的理论分析框架，通过神经网路距离视角解释了MCR在部分观测数据下提升插补质量的理论机制，并提出了基于对偶间隙的早停策略来保持泛化优势。


<details>
  <summary>Details</summary>
Motivation: 机器学习中普遍存在数据损坏、特征缺失或多模态缺失的问题。测量一致性正则化(MCR)方法通过强制插补数据与完整观测数据之间的一致性来提升模型泛化能力，但缺乏理论基础。本文旨在填补这一理论空白。

Method: 1. 从神经网路距离视角对MCR进行理论分析，识别出导致泛化优势的关键项；2. 将分析扩展到不完美训练机制；3. 提出基于对偶间隙监控的早停训练协议；4. 在不同数据源的不同模型架构上进行实证验证。

Result: 理论分析揭示了MCR泛化优势的条件和机制，证明这种优势并非总是保证。提出的早停策略能有效保持泛化优势。实证结果支持理论主张，展示了MCR在不同数据源和模型架构下的有效性。

Conclusion: 本文为MCR提供了理论基础，解释了其在部分观测数据下提升插补质量的机制，并提出了实用的训练策略。研究展示了MCR在不同应用场景下的通用性。

Abstract: The problem of corrupted data, missing features, or missing modalities continues to plague the modern machine learning landscape. To address this issue, a class of regularization methods that enforce consistency between imputed and fully observed data has emerged as a promising approach for improving model generalization, particularly in partially observed settings. We refer to this class of methods as Measure Consistency Regularization (MCR). Despite its empirical success in various applications, such as image inpainting, data imputation and semi-supervised learning, a fundamental understanding of the theoretical underpinnings of MCR remains limited. This paper bridges this gap by offering theoretical insights into why, when, and how MCR enhances imputation quality under partial observability, viewed through the lens of neural network distance.
  Our theoretical analysis identifies the term responsible for MCR's generalization advantage and extends to the imperfect training regime, demonstrating that this advantage is not always guaranteed. Guided by these insights, we propose a novel training protocol that monitors the duality gap to determine an early stopping point that preserves the generalization benefit. We then provide detailed empirical evidence to support our theoretical claims and to show the effectiveness and accuracy of our proposed stopping condition. We further provide a set of real-world data simulations to show the versatility of MCR under different model architectures designed for different data sources.

</details>


### [1080] [Multi-Scale Wavelet Transformers for Operator Learning of Dynamical Systems](https://arxiv.org/abs/2602.01486)
*Xuesong Wang,Michael Groom,Rafael Oliveira,He Zhao,Terence O'Kane,Edwin V. Bonilla*

Main category: cs.LG

Relevance: 45.0

TL;DR: 该论文提出多尺度小波变换器(MSWT)来解决神经算子等数据驱动动力学系统代理模型中的频谱偏差问题，通过在小波域中学习系统动力学，显著改善了长期预测的稳定性和频谱保真度。


<details>
  <summary>Details</summary>
Motivation: 当前数据驱动的动力学系统代理模型（如神经算子）存在频谱偏差问题，会衰减高频分量，而这些高频分量通常编码了小尺度结构。在天气预报等应用中，这种限制尤其严重，因为被错误表示的高频会导致长期预测不稳定。

Method: 提出多尺度小波变换器(MSWT)，在小波域中学习系统动力学。小波变换明确分离了不同尺度的低频和高频内容。MSWT采用保留小波特性的下采样方案来保持高频特征，并使用基于小波的注意力机制来捕捉跨尺度和频带的依赖关系。

Result: 在混沌动力学系统实验中，MSWT显著减少了误差并改善了长期频谱保真度。在ERA5气候再分析数据上，MSWT进一步减少了气候学偏差，证明了其在真实世界预测场景中的有效性。

Conclusion: 多尺度小波变换器通过在小波域中学习动力学，有效解决了数据驱动代理模型中的频谱偏差问题，提高了长期预测的稳定性和准确性，特别是在天气预报等需要精确高频表示的应用中。

Abstract: Recent years have seen a surge in data-driven surrogates for dynamical systems that can be orders of magnitude faster than numerical solvers. However, many machine learning-based models such as neural operators exhibit spectral bias, attenuating high-frequency components that often encode small-scale structure. This limitation is particularly damaging in applications such as weather forecasting, where misrepresented high frequencies can induce long-horizon instability. To address this issue, we propose multi-scale wavelet transformers (MSWTs), which learn system dynamics in a tokenized wavelet domain. The wavelet transform explicitly separates low- and high-frequency content across scales. MSWTs leverage a wavelet-preserving downsampling scheme that retains high-frequency features and employ wavelet-based attention to capture dependencies across scales and frequency bands. Experiments on chaotic dynamical systems show substantial error reductions and improved long horizon spectral fidelity. On the ERA5 climate reanalysis, MSWTs further reduce climatological bias, demonstrating their effectiveness in a real-world forecasting setting.

</details>


### [1081] [Time2Vec-Integrated Transformer for Robust Gesture Recognition from Low-Density sEMG](https://arxiv.org/abs/2602.01855)
*Blagoj Hristov,Hristijan Gjoreski,Vesna Ojleska Latkoska,Gorjan Nadzinski*

Main category: cs.LG

Relevance: 45.0

TL;DR: 提出了一种用于稀疏双通道表面肌电信号的数据高效深度学习框架，通过混合Transformer架构、可学习时间嵌入和归一化融合策略，在肌电假肢控制中实现了高精度，挑战了高密度传感的必要性。


<details>
  <summary>Details</summary>
Motivation: 当前肌电假肢控制通常依赖复杂密集的多传感器阵列，限制了消费者可及性。需要开发使用最少传感器硬件就能实现精确控制的数据高效方法。

Method: 提出混合Transformer架构，采用Time2Vec可学习时间嵌入捕捉生物信号的随机时间扭曲，使用归一化加性融合策略对齐空间和时间特征的潜在分布，采用两阶段课程学习协议确保数据稀缺下的鲁棒特征提取。

Result: 在10类运动集上实现了95.7% ± 0.20%的多受试者F1分数，显著优于标准Transformer和CNN-LSTM模型。快速校准协议仅需每个手势两次试验就能将新受试者性能从21.0% ± 2.98%恢复到96.9% ± 0.52%。

Conclusion: 高保真时间嵌入可以补偿低空间分辨率，挑战了高密度传感的必要性。该框架为能够快速个性化的下一代假肢接口提供了鲁棒、经济高效的蓝图。

Abstract: Accurate and responsive myoelectric prosthesis control typically relies on complex, dense multi-sensor arrays, which limits consumer accessibility. This paper presents a novel, data-efficient deep learning framework designed to achieve precise and accurate control using minimal sensor hardware. Leveraging an external dataset of 8 subjects, our approach implements a hybrid Transformer optimized for sparse, two-channel surface electromyography (sEMG). Unlike standard architectures that use fixed positional encodings, we integrate Time2Vec learnable temporal embeddings to capture the stochastic temporal warping inherent in biological signals. Furthermore, we employ a normalized additive fusion strategy that aligns the latent distributions of spatial and temporal features, preventing the destructive interference common in standard implementations. A two-stage curriculum learning protocol is utilized to ensure robust feature extraction despite data scarcity. The proposed architecture achieves a state-of-the-art multi-subject F1-score of 95.7% $\pm$ 0.20% for a 10-class movement set, statistically outperforming both a standard Transformer with fixed encodings and a recurrent CNN-LSTM model. Architectural optimization reveals that a balanced allocation of model capacity between spatial and temporal dimensions yields the highest stability. Furthermore, while direct transfer to a new unseen subject led to poor accuracy due to domain shifts, a rapid calibration protocol utilizing only two trials per gesture recovered performance from 21.0% $\pm$ 2.98% to 96.9% $\pm$ 0.52%. By validating that high-fidelity temporal embeddings can compensate for low spatial resolution, this work challenges the necessity of high-density sensing. The proposed framework offers a robust, cost-effective blueprint for next-generation prosthetic interfaces capable of rapid personalization.

</details>


### [1082] [PIMCST: Physics-Informed Multi-Phase Consensus and Spatio-Temporal Few-Shot Learning for Traffic Flow Forecasting](https://arxiv.org/abs/2602.01936)
*Abdul Joseph Fofanah,Lian Wen,David Chen*

Main category: cs.LG

Relevance: 45.0

TL;DR: MCPST是一个用于少样本交通预测的多阶段共识时空框架，通过扩散、同步和谱嵌入建模交通动态，采用自适应共识机制融合预测，并利用结构化元学习快速适应新城市。


<details>
  <summary>Details</summary>
Motivation: 智能交通系统中的交通流预测面临跨域数据稀缺的挑战，特别是在历史数据有限的情况下，模型训练和泛化能力受到限制。城市交通网络的复杂时空依赖性和非线性动态使得不同城市间的少样本学习更加困难。

Method: 1) 多阶段引擎：通过扩散、同步和谱嵌入建模交通动态；2) 自适应共识机制：动态融合各阶段预测并强制一致性；3) 结构化元学习策略：用最少数据快速适应新城市。提供了理论保证，包括有界近似误差的表示定理和少样本适应的泛化界。

Result: 在四个真实世界数据集上，MCPST在时空图学习方法、动态图迁移学习方法、基于提示的时空预测方法和跨域少样本设置中，优于14种最先进方法，提高了预测准确性，减少了所需训练数据，并提供了可解释的见解。

Conclusion: MCPST通过将交通预测重新概念化为多阶段共识学习问题，有效解决了跨域数据稀缺场景下的交通流预测挑战，为智能交通系统提供了强大的少样本预测框架。

Abstract: Accurate traffic flow prediction remains a fundamental challenge in intelligent transportation systems, particularly in cross-domain, data-scarce scenarios where limited historical data hinders model training and generalisation. The complex spatio-temporal dependencies and nonlinear dynamics of urban mobility networks further complicate few-shot learning across different cities. This paper proposes MCPST, a novel Multi-phase Consensus Spatio-Temporal framework for few-shot traffic forecasting that reconceptualises traffic prediction as a multi-phase consensus learning problem. Our framework introduces three core innovations: (1) a multi-phase engine that models traffic dynamics through diffusion, synchronisation, and spectral embeddings for comprehensive dynamic characterisation; (2) an adaptive consensus mechanism that dynamically fuses phase-specific predictions while enforcing consistency; and (3) a structured meta-learning strategy for rapid adaptation to new cities with minimal data. We establish extensive theoretical guarantees, including representation theorems with bounded approximation errors and generalisation bounds for few-shot adaptation. Through experiments on four real-world datasets, MCPST outperforms fourteen state-of-the-art methods in spatio-temporal graph learning methods, dynamic graph transfer learning methods, prompt-based spatio-temporal prediction methods and cross-domain few-shot settings, improving prediction accuracy while reducing required training data and providing interpretable insights. The implementation code is available at https://github.com/afofanah/MCPST.

</details>


### [1083] [Asynchronous MultiAgent Reinforcement Learning for 5G Routing under Side Constraints](https://arxiv.org/abs/2602.00035)
*Sebastian Racedo,Brigitte Jaumard,Oscar Delgado,Meysam Masoudi*

Main category: cs.NI

Relevance: 45.0

TL;DR: 提出异步多智能体强化学习框架，用于5G及未来网络中异构流量的实时路由决策，通过服务专用PPO智能体并行规划路由，在共享资源环境中协调资源分配。


<details>
  <summary>Details</summary>
Motivation: 当前5G及未来网络承载着具有多样化服务质量约束的异构流量，使得实时路由决策既复杂又时间关键。传统方法（如启发式人工干预、单一集中式RL策略或跨多个学习者的同步更新）在可扩展性和拖尾效应方面存在困难。

Method: 提出异步多智能体强化学习框架，每个服务配备独立的PPO智能体，并行规划路由，并将资源增量提交到共享的全局资源环境。通过状态协调保持跨服务可行性，并实现服务特定目标的专业化。

Result: 在基于蒙特利尔实时流量数据的O-RAN类网络模拟中评估，与单智能体PPO基线相比，AMARL实现了相似的GoS（接受率）和端到端延迟，同时减少了训练时间并提高了对需求变化的鲁棒性。

Conclusion: 异步、服务专用智能体为分布式路由提供了可扩展且实用的方法，其适用性可扩展到O-RAN领域之外。

Abstract: Networks in the current 5G and beyond systems increasingly carry heterogeneous traffic with diverse quality-of-service constraints, making real-time routing decisions both complex and time-critical. A common approach, such as a heuristic with human intervention or training a single centralized RL policy or synchronizing updates across multiple learners, struggles with scalability and straggler effects. We address this by proposing an asynchronous multi-agent reinforcement learning (AMARL) framework in which independent PPO agents, one per service, plan routes in parallel and commit resource deltas to a shared global resource environment. This coordination by state preserves feasibility across services and enables specialization for service-specific objectives. We evaluate the method on an O-RAN like network simulation using nearly real-time traffic data from the city of Montreal. We compared against a single-agent PPO baseline. AMARL achieves a similar Grade of Service (acceptance rate) (GoS) and end-to-end latency, with reduced training wall-clock time and improved robustness to demand shifts. These results suggest that asynchronous, service-specialized agents provide a scalable and practical approach to distributed routing, with applicability extending beyond the O-RAN domain.

</details>


### [1084] [Sampling from multi-modal distributions on Riemannian manifolds with training-free stochastic interpolants](https://arxiv.org/abs/2602.00641)
*Alain Durmus,Maxence Noble,Thibaut Pellerin*

Main category: stat.ML

Relevance: 45.0

TL;DR: 提出了一种在黎曼流形上从非归一化密度采样的通用方法，特别针对多模态目标分布，该方法基于扩散模型框架但完全无需训练。


<details>
  <summary>Details</summary>
Motivation: 现有采样方法在处理黎曼流形上的多模态目标分布时面临挑战，特别是在高维和重尾分布情况下。需要一种既尊重流形几何结构又无需训练的采样方法。

Method: 基于扩散模型框架，引入非平衡确定性动力学模拟，将易采样的噪声分布传输到目标分布。在边缘层面，密度路径遵循噪声与目标分布之间的随机插值，专门构建以尊重底层黎曼几何。完全无需训练，仅使用标准蒙特卡洛技术的迭代后验采样过程。

Result: 该方法在多种多模态采样问题上表现出有效性，包括高维和重尾分布示例。理论分析严谨，扩展了基于扩散的采样方法到非欧几里得设置。

Conclusion: 提出了一种在黎曼流形上处理多模态目标分布的有效采样方法，无需训练且理论严谨，扩展了扩散采样方法的应用范围。

Abstract: In this paper, we propose a general methodology for sampling from un-normalized densities defined on Riemannian manifolds, with a particular focus on multi-modal targets that remain challenging for existing sampling methods. Inspired by the framework of diffusion models developed for generative modeling, we introduce a sampling algorithm based on the simulation of a non-equilibrium deterministic dynamics that transports an easy-to-sample noise distribution toward the target. At the marginal level, the induced density path follows a prescribed stochastic interpolant between the noise and target distributions, specifically constructed to respect the underlying Riemannian geometry. In contrast to related generative modeling approaches that rely on machine learning, our method is entirely training-free. It instead builds on iterative posterior sampling procedures using only standard Monte Carlo techniques, thereby extending recent diffusion-based sampling methodologies beyond the Euclidean setting. We complement our approach with a rigorous theoretical analysis and demonstrate its effectiveness on a range of multi-modal sampling problems, including high-dimensional and heavy-tailed examples.

</details>


### [1085] [Zero-Flow Encoders](https://arxiv.org/abs/2602.00797)
*Yakun Wang,Leyang Wang,Song Liu,Taiji Suzuki*

Main category: stat.ML

Relevance: 45.0

TL;DR: 提出基于流模型的表示学习框架，利用零流准则验证条件独立性，提取充分信息，用于图模型中的马尔可夫毯学习和自监督学习任务。


<details>
  <summary>Details</summary>
Motivation: 流模型在生成建模任务中表现出色，能够捕捉复杂数据分布的细微细节，但现有研究很少利用这种能力来解决生成任务之外的细粒度结构细节问题。本文旨在探索流模型在表示学习中的应用潜力。

Method: 提出流启发的表示学习框架：1) 证明使用独立耦合训练的整流流在t=0.5处处处为零当且仅当源分布和目标分布相同，称为零流准则；2) 展示该准则可以验证条件独立性，从而提取数据的充分信息；3) 将该准则转化为可处理的、无需模拟的损失函数，用于学习图模型中的摊销马尔可夫毯和自监督学习中的潜在表示。

Result: 在模拟和真实世界数据集上的实验证明了该方法的有效性。代码已开源。

Conclusion: 流模型不仅可用于生成任务，还能有效应用于表示学习，通过零流准则提取数据的充分信息，为图模型和自监督学习提供了新的解决方案。

Abstract: Flow-based methods have achieved significant success in various generative modeling tasks, capturing nuanced details within complex data distributions. However, few existing works have exploited this unique capability to resolve fine-grained structural details beyond generation tasks. This paper presents a flow-inspired framework for representation learning. First, we demonstrate that a rectified flow trained using independent coupling is zero everywhere at $t=0.5$ if and only if the source and target distributions are identical. We term this property the \emph{zero-flow criterion}. Second, we show that this criterion can certify conditional independence, thereby extracting \emph{sufficient information} from the data. Third, we translate this criterion into a tractable, simulation-free loss function that enables learning amortized Markov blankets in graphical models and latent representations in self-supervised learning tasks. Experiments on both simulated and real-world datasets demonstrate the effectiveness of our approach. The code reproducing our experiments can be found at: https://github.com/probabilityFLOW/zfe.

</details>


### [1086] [RoDiF: Robust Direct Fine-Tuning of Diffusion Policies with Corrupted Human Feedback](https://arxiv.org/abs/2602.00886)
*Amitesh Vatsa,Zhixian Xie,Wanxin Jin*

Main category: cs.RO

Relevance: 45.0

TL;DR: 本文提出RoDiF方法，通过统一MDP公式将扩散去噪过程与环境动态结合，实现无需奖励的直接偏好优化，并采用几何假设切割策略处理损坏的人类偏好标签，在长时程操作任务中优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 扩散策略是机器人控制的强大范式，但通过人类偏好进行微调面临挑战，因为去噪过程的多步结构使得传统偏好优化方法难以直接应用。此外，现实世界中的人类偏好标签可能存在损坏，需要鲁棒的处理方法。

Method: 1. 提出统一MDP公式，将扩散去噪链与环境动态连贯整合；2. 基于此公式实现无需奖励的直接偏好优化(DPO)；3. 提出RoDiF方法，从几何假设切割视角重新解释DPO目标，采用保守切割策略处理损坏偏好，无需假设特定噪声分布。

Result: 在长时程操作任务上的广泛实验表明：1. RoDiF持续优于最先进的基线方法；2. 能有效引导预训练的扩散策略（多种架构）朝向人类偏好模式；3. 即使在30%损坏的偏好标签下仍保持强劲性能。

Conclusion: RoDiF通过统一MDP公式和几何假设切割策略，成功解决了扩散策略微调中的偏好优化问题，特别是在存在损坏偏好标签的情况下表现出鲁棒性，为扩散策略的实际应用提供了有效工具。

Abstract: Diffusion policies are a powerful paradigm for robotic control, but fine-tuning them with human preferences is fundamentally challenged by the multi-step structure of the denoising process. To overcome this, we introduce a Unified Markov Decision Process (MDP) formulation that coherently integrates the diffusion denoising chain with environmental dynamics, enabling reward-free Direct Preference Optimization (DPO) for diffusion policies. Building on this formulation, we propose RoDiF (Robust Direct Fine-Tuning), a method that explicitly addresses corrupted human preferences. RoDiF reinterprets the DPO objective through a geometric hypothesis-cutting perspective and employs a conservative cutting strategy to achieve robustness without assuming any specific noise distribution. Extensive experiments on long-horizon manipulation tasks show that RoDiF consistently outperforms state-of-the-art baselines, effectively steering pretrained diffusion policies of diverse architectures to human-preferred modes, while maintaining strong performance even under 30% corrupted preference labels.

</details>


### [1087] [Online Social Welfare Function-based Resource Allocation](https://arxiv.org/abs/2602.01400)
*Kanad Pardeshi,Samsara Foubert,Aarti Singh*

Main category: stat.ML

Relevance: 45.0

TL;DR: 该论文提出了一个基于社会福利函数（SWF）的在线学习和推断通用置信序列框架，适用于资源分配问题，并设计了SWF-UCB算法实现接近最优的遗憾界。


<details>
  <summary>Details</summary>
Motivation: 在现实世界的资源分配问题中，决策者需要在多个时间步中向人群分配有限资源。个体获得资源后产生随机效用，需要通过社会福利函数（SWF）聚合个体效用以评估分配效果。现有方法缺乏适用于任意单调、凹且Lipschitz连续SWF的通用在线学习和推断框架。

Method: 提出了一个通用置信序列框架，关键洞察是单调性足以将个体效用的置信序列提升为最优福利的任意时间有效边界。基于此设计了SWF-UCB算法，这是一个SWF无关的在线学习算法。框架在三个规范不同的SWF家族上实例化：加权幂均值、Kolm和Gini，为每个提供了专门的oracle算法。

Result: SWF-UCB算法实现了接近最优的遗憾界$\tilde{O}(n+\sqrt{nkT})$（其中$k$个资源在$T$个时间步中分配给$n$个个体）。实验证实了$\sqrt{T}$的缩放规律，并揭示了$k$与SWF参数之间的丰富交互作用。框架自然支持序列假设检验、最优停止和政策评估等推断应用。

Conclusion: 该研究为基于社会福利函数的在线学习和推断提供了一个通用框架，适用于任意单调、凹且Lipschitz连续的SWF。框架具有理论保证和实际应用价值，能够处理复杂的资源分配问题并支持多种推断任务。

Abstract: In many real-world settings, a centralized decision-maker must repeatedly allocate finite resources to a population over multiple time steps. Individuals who receive a resource derive some stochastic utility; to characterize the population-level effects of an allocation, the expected individual utilities are then aggregated using a social welfare function (SWF). We formalize this setting and present a general confidence sequence framework for SWF-based online learning and inference, valid for any monotonic, concave, and Lipschitz-continuous SWF. Our key insight is that monotonicity alone suffices to lift confidence sequences from individual utilities to anytime-valid bounds on optimal welfare. Building on this foundation, we propose SWF-UCB, a SWF-agnostic online learning algorithm that achieves near-optimal $\tilde{O}(n+\sqrt{nkT})$ regret (for $k$ resources distributed among $n$ individuals at each of $T$ time steps). We instantiate our framework on three normatively distinct SWF families: Weighted Power Mean, Kolm, and Gini, providing bespoke oracle algorithms for each. Experiments confirm $\sqrt{T}$ scaling and reveal rich interactions between $k$ and SWF parameters. This framework naturally supports inference applications such as sequential hypothesis testing, optimal stopping, and policy evaluation.

</details>


### [1088] [Importance Weighted Variational Inference without the Reparameterization Trick](https://arxiv.org/abs/2602.01412)
*Kamélia Daudel,Minh-Ngoc Tran,Cheng Zhang*

Main category: stat.ML

Relevance: 45.0

TL;DR: 本文首次全面分析了重要性加权变分推断中的REINFORCE梯度估计器，揭示了现有VIMCO估计器存在信噪比随样本数增加而消失的问题，并提出了新的VIMCO-*估计器来解决这一问题。


<details>
  <summary>Details</summary>
Motivation: 重要性加权变分推断通过优化随蒙特卡洛样本数增加而收紧的界来近似密度。标准优化依赖重参数化梯度估计器，但这限制了数据生成过程和变分近似的选择。REINFORCE梯度估计器没有这些限制，但缺乏严格的理论基础。本文旨在填补这一理论空白并改进现有方法。

Method: 1. 对重要性加权VI中的REINFORCE梯度估计器进行首次全面理论分析
2. 引入并研究广义的VIMCO梯度估计器家族
3. 证明现有VIMCO估计器存在信噪比随样本数增加而消失的问题
4. 提出新的VIMCO-*梯度估计器，通过实现√N的信噪比缩放来避免信噪比崩溃

Result: 理论分析表明现有VIMCO梯度估计器的信噪比随样本数增加而消失，阻碍有效优化。提出的VIMCO-*估计器避免了这一问题，在重参数化梯度通常不可用的挑战性设置中表现出优于现有VIMCO实现的性能。

Conclusion: 本文为重要性加权VI中的REINFORCE梯度估计器提供了首个全面理论分析，揭示了现有方法的根本缺陷，并提出了VIMCO-*这一改进的梯度估计器，在理论保证和实际性能上都优于现有方法。

Abstract: Importance weighted variational inference (VI) approximates densities known up to a normalizing constant by optimizing bounds that tighten with the number of Monte Carlo samples $N$. Standard optimization relies on reparameterized gradient estimators, which are well-studied theoretically yet restrict both the choice of the data-generating process and the variational approximation. While REINFORCE gradient estimators do not suffer from such restrictions, they lack rigorous theoretical justification. In this paper, we provide the first comprehensive analysis of REINFORCE gradient estimators in importance weighted VI, leveraging this theoretical foundation to diagnose and resolve fundamental deficiencies in current state-of-the-art estimators. Specifically, we introduce and examine a generalized family of variational inference for Monte Carlo objectives (VIMCO) gradient estimators. We prove that state-of-the-art VIMCO gradient estimators exhibit a vanishing signal-to-noise ratio (SNR) as $N$ increases, which prevents effective optimization. To overcome this issue, we propose the novel VIMCO-$\star$ gradient estimator and show that it averts the SNR collapse of existing VIMCO gradient estimators by achieving a $\sqrt{N}$ SNR scaling instead. We demonstrate its superior empirical performance compared to current VIMCO implementations in challenging settings where reparameterized gradients are typically unavailable.

</details>


### [1089] [Robust Generalization with Adaptive Optimal Transport Priors for Decision-Focused Learning](https://arxiv.org/abs/2602.01427)
*Haixiang Sun,Andrew L. Liu*

Main category: stat.ML

Relevance: 45.0

TL;DR: 提出原型引导的分布鲁棒优化框架，通过分层最优传输从基础数据学习类别自适应先验，并将其嵌入Sinkhorn DRO，实现少样本场景下更强的鲁棒泛化能力。


<details>
  <summary>Details</summary>
Motivation: 现有Sinkhorn分布鲁棒优化方法依赖固定参考分布，限制了其在少样本学习中的适应性。少样本学习需要在有限监督下泛化并保持对分布偏移的鲁棒性，需要更灵活的框架。

Method: 提出原型引导的分布鲁棒优化框架，通过分层最优传输从丰富的基础数据学习类别自适应先验，并将其嵌入Sinkhorn DRO公式。该设计使少样本信息能够有机整合，产生类别特定的鲁棒决策。

Result: 实验表明PG-DRO在少样本场景下实现了更强的鲁棒泛化，优于标准学习器和DRO基线方法。

Conclusion: PG-DRO框架通过将可迁移的结构知识融入不确定性集合，为少样本学习提供了理论可靠且高效的分布鲁棒优化方法。

Abstract: Few-shot learning requires models to generalize under limited supervision while remaining robust to distribution shifts. Existing Sinkhorn Distributionally Robust Optimization (DRO) methods provide theoretical guarantees but rely on a fixed reference distribution, which limits their adaptability. We propose a Prototype-Guided Distributionally Robust Optimization (PG-DRO) framework that learns class-adaptive priors from abundant base data via hierarchical optimal transport and embeds them into the Sinkhorn DRO formulation. This design enables few-shot information to be organically integrated into producing class-specific robust decisions that are both theoretically grounded and efficient, and further aligns the uncertainty set with transferable structural knowledge. Experiments show that PG-DRO achieves stronger robust generalization in few-shot scenarios, outperforming both standard learners and DRO baselines.

</details>


### [1090] [SpikingGamma: Surrogate-Gradient Free and Temporally Precise Online Training of Spiking Neural Networks with Smoothed Delays](https://arxiv.org/abs/2602.01978)
*Roel Koopman,Sebastian Otte,Sander Bohté*

Main category: cs.NE

Relevance: 45.0

TL;DR: 提出SpikingGamma模型，通过内部递归记忆结构和sigma-delta脉冲编码，实现无需替代梯度的直接误差反向传播，能够在线学习精细时间模式，且对时间分辨率不敏感。


<details>
  <summary>Details</summary>
Motivation: 脉冲神经网络在神经形态硬件上具有能效优势，但精细时间离散化下的训练仍然困难，现有方法（基于替代梯度的BPTT/RTRL变体）时间分辨率扩展性差，在线近似不稳定，难以精确捕捉时间模式。

Method: 开发具有内部递归记忆结构的脉冲神经元，结合sigma-delta脉冲编码，形成SpikingGamma模型，支持直接误差反向传播而无需替代梯度。

Result: SpikingGamma能够在线学习精细时间模式且脉冲活动最少，将前馈SNN扩展到复杂任务和基准测试中达到竞争性准确率，且对模型时间分辨率不敏感。

Conclusion: 该方法为当前使用替代梯度训练的循环SNN提供了替代方案，并为将SNN映射到神经形态硬件提供了直接途径。

Abstract: Neuromorphic hardware implementations of Spiking Neural Networks (SNNs) promise energy-efficient, low-latency AI through sparse, event-driven computation. Yet, training SNNs under fine temporal discretization remains a major challenge, hindering both low-latency responsiveness and the mapping of software-trained SNNs to efficient hardware. In current approaches, spiking neurons are modeled as self-recurrent units, embedded into recurrent networks to maintain state over time, and trained with BPTT or RTRL variants based on surrogate gradients. These methods scale poorly with temporal resolution, while online approximations often exhibit instability for long sequences and tend to fail at capturing temporal patterns precisely. To address these limitations, we develop spiking neurons with internal recursive memory structures that we combine with sigma-delta spike-coding. We show that this SpikingGamma model supports direct error backpropagation without surrogate gradients, can learn fine temporal patterns with minimal spiking in an online manner, and scale feedforward SNNs to complex tasks and benchmarks with competitive accuracy, all while being insensitive to the temporal resolution of the model. Our approach offers both an alternative to current recurrent SNNs trained with surrogate gradients, and a direct route for mapping SNNs to neuromorphic hardware.

</details>


### [1091] [PRISM: Performer RS-IMLE for Single-pass Multisensory Imitation Learning](https://arxiv.org/abs/2602.02396)
*Amisha Bhaskar,Pratap Tokekar,Stefano Di Cairano,Alexander Schperberg*

Main category: cs.RO

Relevance: 45.0

TL;DR: PRISM是一种用于机器人模仿学习的单次通过策略，基于批量全局拒绝采样的IMLE变体，结合多感官编码器和线性注意力生成器，在实时控制速率下实现多模态动作分布。


<details>
  <summary>Details</summary>
Motivation: 当前机器人模仿学习中的生成方法（如扩散模型、流匹配、IMLE）通常只能满足部分需求：要么能处理多模态动作分布但速度慢，要么速度快但覆盖范围有限。需要一种能同时满足实时控制、多感官整合和多模态动作覆盖的方法。

Method: PRISM基于批量全局拒绝采样的IMLE变体，结合时间多感官编码器（整合RGB、深度、触觉、音频和本体感觉）和使用Performer架构的线性注意力生成器，实现单次通过推理。

Result: 在真实硬件（Unitree Go2+D1手臂、UR5机械臂）上，PRISM在预操作停车、高精度插入、多物体抓放等任务中，成功率比最先进的扩散策略高10-25%，同时保持30-50Hz的闭环控制。在CALVIN（10%数据分割）等大规模仿真基准测试中，PRISM比扩散模型成功率提高约25%，比流匹配提高约20%，同时轨迹抖动减少20-50倍。

Conclusion: PRISM是一种快速、准确、多感官的模仿策略，在保持多模态动作覆盖的同时，避免了迭代采样的延迟，为机器人模仿学习提供了高效的解决方案。

Abstract: Robotic imitation learning typically requires models that capture multimodal action distributions while operating at real-time control rates and accommodating multiple sensing modalities. Although recent generative approaches such as diffusion models, flow matching, and Implicit Maximum Likelihood Estimation (IMLE) have achieved promising results, they often satisfy only a subset of these requirements. To address this, we introduce PRISM, a single-pass policy based on a batch-global rejection-sampling variant of IMLE. PRISM couples a temporal multisensory encoder (integrating RGB, depth, tactile, audio, and proprioception) with a linear-attention generator using a Performer architecture. We demonstrate the efficacy of PRISM on a diverse real-world hardware suite, including loco-manipulation using a Unitree Go2 with a 7-DoF arm D1 and tabletop manipulation with a UR5 manipulator. Across challenging physical tasks such as pre-manipulation parking, high-precision insertion, and multi-object pick-and-place, PRISM outperforms state-of-the-art diffusion policies by 10-25% in success rate while maintaining high-frequency (30-50 Hz) closed-loop control. We further validate our approach on large-scale simulation benchmarks, including CALVIN, MetaWorld, and Robomimic. In CALVIN (10% data split), PRISM improves success rates by approximately 25% over diffusion and approximately 20% over flow matching, while simultaneously reducing trajectory jerk by 20x-50x. These results position PRISM as a fast, accurate, and multisensory imitation policy that retains multimodal action coverage without the latency of iterative sampling.

</details>


### [1092] [Full-Batch Gradient Descent Outperforms One-Pass SGD: Sample Complexity Separation in Single-Index Learning](https://arxiv.org/abs/2602.02431)
*Filip Kovačević,Hong Chang Ji,Denny Wu,Mahdi Soltanolkotabi,Marco Mondelli*

Main category: stat.ML

Relevance: 45.0

TL;DR: 本文研究了单索引模型学习中，全批次梯度下降（GD）相比单次随机梯度下降（SGD）的统计效率优势。理论分析表明，通过简单的激活函数截断，全批次GD可以在n≈d样本量下实现弱恢复，优于需要n≳d log d样本的单次SGD。


<details>
  <summary>Details</summary>
Motivation: 研究训练数据重复使用对统计效率的影响。虽然经验上认为重复使用数据能提高效率，但除了线性回归外，全批次GD相比单次SGD的理论优势尚不明确。本文旨在探究在单索引模型学习中，全批次GD是否以及如何超越单次SGD。

Method: 研究d维单索引模型（使用二次激活函数）。首先分析全批次球形GD在相关性损失上的样本复杂度，然后提出激活函数截断方法。同时进行从小初始化开始的平方损失轨迹分析，研究GD的优化动态。

Result: 1. 全批次球形GD在相关性损失上需要n≳d log d样本，与单次SGD相同；2. 通过激活函数截断，全批次GD在n≈d样本量下展现出有利的优化景观；3. 轨迹分析表明，n≳d样本和T≳log d梯度步数足以实现强恢复。

Conclusion: 全批次GD通过简单的激活函数截断，在单索引模型学习中能够超越单次SGD的统计效率。这为理解数据重复使用的优势提供了理论依据，特别是在优化景观分析方面。

Abstract: It is folklore that reusing training data more than once can improve the statistical efficiency of gradient-based learning. However, beyond linear regression, the theoretical advantage of full-batch gradient descent (GD, which always reuses all the data) over one-pass stochastic gradient descent (online SGD, which uses each data point only once) remains unclear. In this work, we consider learning a $d$-dimensional single-index model with a quadratic activation, for which it is known that one-pass SGD requires $n\gtrsim d\log d$ samples to achieve weak recovery. We first show that this $\log d$ factor in the sample complexity persists for full-batch spherical GD on the correlation loss; however, by simply truncating the activation, full-batch GD exhibits a favorable optimization landscape at $n \simeq d$ samples, thereby outperforming one-pass SGD (with the same activation) in statistical efficiency. We complement this result with a trajectory analysis of full-batch GD on the squared loss from small initialization, showing that $n \gtrsim d$ samples and $T \gtrsim\log d$ gradient steps suffice to achieve strong (exact) recovery.

</details>


### [1093] [The Impact of Machine Learning Uncertainty on the Robustness of Counterfactual Explanations](https://arxiv.org/abs/2602.00063)
*Leonidas Christodoulou,Chang Sun*

Main category: cs.LG

Relevance: 40.0

TL;DR: 研究探讨了在数据和模型不确定性存在时，反事实解释的鲁棒性问题，发现即使模型准确率轻微下降也会导致反事实解释显著变化


<details>
  <summary>Details</summary>
Motivation: 反事实解释被广泛用于解释机器学习预测，但现有方法很少在模型和数据不确定性变化的情况下进行测试，导致在现实世界变化中可能产生不稳定或无效的解释

Method: 通过合成和真实世界表格数据集实验，研究常见机器学习模型与反事实生成算法组合在存在随机不确定性和认知不确定性时的鲁棒性

Result: 反事实解释对模型不确定性高度敏感，即使由噪声增加或数据有限引起的模型准确率轻微下降，也会导致生成的反事实在平均水平和个体实例上出现大幅变化

Conclusion: 在金融和社会科学等领域需要不确定性感知的解释方法，因为反事实解释在不确定性存在时不够稳定

Abstract: Counterfactual explanations are widely used to interpret machine learning predictions by identifying minimal changes to input features that would alter a model's decision. However, most existing counterfactual methods have not been tested when model and data uncertainty change, resulting in explanations that may be unstable or invalid under real-world variability. In this work, we investigate the robustness of common combinations of machine learning models and counterfactual generation algorithms in the presence of both aleatoric and epistemic uncertainty. Through experiments on synthetic and real-world tabular datasets, we show that counterfactual explanations are highly sensitive to model uncertainty. In particular, we find that even small reductions in model accuracy - caused by increased noise or limited data - can lead to large variations in the generated counterfactuals on average and on individual instances. These findings underscore the need for uncertainty-aware explanation methods in domains such as finance and the social sciences.

</details>


### [1094] [Lossless Embedding Compression via Spherical Coordinates](https://arxiv.org/abs/2602.00079)
*Han Xiao*

Main category: cs.LG

Relevance: 40.0

TL;DR: 提出一种无损压缩单位范数嵌入的方法，实现1.5倍压缩，比现有最佳方法提升25%。该方法利用高维单位向量球坐标集中在π/2附近的特性，使IEEE 754指数坍缩到单一值，从而实现熵编码。


<details>
  <summary>Details</summary>
Motivation: 单位范数嵌入（如文本、图像嵌入）在AI系统中广泛应用，但存储和传输成本高。现有压缩方法要么有损，要么压缩率有限。需要一种无损压缩方法来减少存储和带宽需求。

Method: 利用高维单位向量在球坐标系中的统计特性：当维度增加时，球坐标角度集中在π/2附近。这使得IEEE 754浮点数的指数部分趋于一致，从而可以通过熵编码实现高效压缩。方法无需训练，完全基于数学特性。

Result: 在26种配置（涵盖文本、图像和多向量嵌入）的评估中，该方法实现了1.5倍压缩，比之前最佳方法提升25%。压缩完全无损，在float32精度范围内保持原始数据完整性。

Conclusion: 提出了一种高效的无损单位范数嵌入压缩方法，利用高维几何特性实现显著压缩比提升。方法简单有效，无需训练，适用于各种嵌入类型。

Abstract: We present a lossless compression method for unit-norm embeddings that achieves 1.5$\times$ compression, 25\% better than the best prior method. The method exploits that spherical coordinates of high-dimensional unit vectors concentrate around $π/2$, causing IEEE 754 exponents to collapse to a single value and enabling entropy coding. Evaluation across 26 configurations spanning text, image, and multi-vector embeddings confirms consistent improvement. The method requires no training and is fully lossless within float32 precision.

</details>


### [1095] [Federated Learning at the Forefront of Fairness: A Multifaceted Perspective](https://arxiv.org/abs/2602.00718)
*Noorain Mukhtiar,Adnan Mahmood,Yipeng Zhou,Jian Yang,Jing Teng,Quan Z. Sheng*

Main category: cs.LG

Relevance: 40.0

TL;DR: 关于联邦学习中公平性研究的综述论文，从多角度对现有公平感知方法进行分类，提出评估框架和度量标准，并探讨未来研究方向


<details>
  <summary>Details</summary>
Motivation: 联邦学习中的公平性问题日益重要，主要驱动因素包括异构客户端的约束条件以及在不同场景下平衡模型性能的需求。随着FL应用扩展，确保所有参与方都能公平受益变得至关重要。

Method: 1) 从多角度对现有公平感知方法进行分类：模型性能导向和能力导向；2) 提供分类框架解决各种公平关切和技术方面；3) 考察平衡公平与性能的有效性；4) 分析重要的公平性量化评估指标

Result: 建立了联邦学习公平性研究的系统分类框架，识别了关键评估指标，为研究人员提供了全面的技术路线图，并为未来研究奠定了坚实基础

Conclusion: 联邦学习公平性是重要研究方向，需要平衡公平与性能，本文提供了系统分类和评估框架，并指出了未来研究方向和潜在解决方案

Abstract: Fairness in Federated Learning (FL) is emerging as a critical factor driven by heterogeneous clients' constraints and balanced model performance across various scenarios. In this survey, we delineate a comprehensive classification of the state-of-the-art fairness-aware approaches from a multifaceted perspective, i.e., model performance-oriented and capability-oriented. Moreover, we provide a framework to categorize and address various fairness concerns and associated technical aspects, examining their effectiveness in balancing equity and performance within FL frameworks. We further examine several significant evaluation metrics leveraged to measure fairness quantitatively. Finally, we explore exciting open research directions and propose prospective solutions that could drive future advancements in this important area, laying a solid foundation for researchers working toward fairness in FL.

</details>


### [1096] [Multimodal Scientific Learning Beyond Diffusions and Flows](https://arxiv.org/abs/2602.00960)
*Leonardo Ferreira Guilhoto,Akshat Kaushal,Paris Perdikaris*

Main category: cs.LG

Relevance: 40.0

TL;DR: 该论文提出在科学机器学习中使用混合密度网络作为多模态不确定性量化的高效替代方案，相比扩散模型等隐式生成方法，MDNs在数据稀缺的科学场景中具有更好的数据效率、可解释性和泛化能力。


<details>
  <summary>Details</summary>
Motivation: 科学机器学习面临多模态条件不确定性的挑战，如病态逆问题、多稳态和混沌动力学。现有方法如扩散模型和流模型虽然表达能力强，但数据需求大、计算成本高，且与科学问题中常见的结构化解空间不匹配。

Method: 采用混合密度网络作为显式参数密度估计器，通过统一的概率框架对比显式和隐式分布网络。MDNs为低维多模态物理问题提供归纳偏置，能够直接全局分配不同解分支的概率质量。

Result: 在逆问题、多稳态和混沌科学回归任务中，MDNs展现出优越的泛化能力、可解释性和样本效率，特别是在科学数据稀缺的情况下能够可靠地恢复分离的模式。

Conclusion: 混合密度网络为科学机器学习中的多模态不确定性量化提供了一个被忽视但有效的替代方案，特别适合数据稀缺的科学场景，具有更好的数据效率和可解释性。

Abstract: Scientific machine learning (SciML) increasingly requires models that capture multimodal conditional uncertainty arising from ill-posed inverse problems, multistability, and chaotic dynamics. While recent work has favored highly expressive implicit generative models such as diffusion and flow-based methods, these approaches are often data-hungry, computationally costly, and misaligned with the structured solution spaces frequently found in scientific problems. We demonstrate that Mixture Density Networks (MDNs) provide a principled yet largely overlooked alternative for multimodal uncertainty quantification in SciML. As explicit parametric density estimators, MDNs impose an inductive bias tailored to low-dimensional, multimodal physics, enabling direct global allocation of probability mass across distinct solution branches. This structure delivers strong data efficiency, allowing reliable recovery of separated modes in regimes where scientific data is scarce. We formalize these insights through a unified probabilistic framework contrasting explicit and implicit distribution networks, and demonstrate empirically that MDNs achieve superior generalization, interpretability, and sample efficiency across a range of inverse, multistable, and chaotic scientific regression tasks.

</details>


### [1097] [Scalable Random Wavelet Features: Efficient Non-Stationary Kernel Approximation with Convergence Guarantees](https://arxiv.org/abs/2602.00987)
*Sawan Kumar,Souvik Chakraborty*

Main category: cs.LG

Relevance: 40.0

TL;DR: 提出Random Wavelet Features (RWF)框架，通过从小波族中采样构建可扩展的非平稳核近似，解决传统方法在表达能力和计算效率之间的权衡问题。


<details>
  <summary>Details</summary>
Motivation: 现实世界中的许多过程是非平稳的（统计特性随输入域变化），但现有可扩展方法大多依赖平稳性假设。这导致一个困难权衡：要么使用表达能力强但计算成本高的模型（如深度高斯过程），要么使用可扩展但表达能力有限的方法（如随机傅里叶特征）。

Method: 引入Random Wavelet Features (RWF)框架，通过从小波族中采样构建可扩展的非平稳核近似。利用小波固有的局部化和多分辨率结构，生成显式特征映射来捕捉复杂的输入依赖模式。该框架为将RFF推广到非平稳设置提供了原则性方法。

Result: 在多个具有挑战性的合成和真实世界数据集上，RWF优于平稳随机特征方法，并在与更复杂模型的比较中提供了有竞争力的精度-效率权衡。理论分析包括正定性、无偏性和一致收敛保证。

Conclusion: RWF填补了表达能力和可扩展性之间的空白，为广泛的真实世界非平稳问题解锁了可扩展且表达力强的核方法。

Abstract: Modeling non-stationary processes, where statistical properties vary across the input domain, is a critical challenge in machine learning; yet most scalable methods rely on a simplifying assumption of stationarity. This forces a difficult trade-off: use expressive but computationally demanding models like Deep Gaussian Processes, or scalable but limited methods like Random Fourier Features (RFF). We close this gap by introducing Random Wavelet Features (RWF), a framework that constructs scalable, non-stationary kernel approximations by sampling from wavelet families. By harnessing the inherent localization and multi-resolution structure of wavelets, RWF generates an explicit feature map that captures complex, input-dependent patterns. Our framework provides a principled way to generalize RFF to the non-stationary setting and comes with a comprehensive theoretical analysis, including positive definiteness, unbiasedness, and uniform convergence guarantees. We demonstrate empirically on a range of challenging synthetic and real-world datasets that RWF outperforms stationary random features and offers a compelling accuracy-efficiency trade-off against more complex models, unlocking scalable and expressive kernel methods for a broad class of real-world non-stationary problems.

</details>


### [1098] [Analyzing and Improving Diffusion Models for Time-Series Data Imputation: A Proximal Recursion Perspective](https://arxiv.org/abs/2602.01182)
*Zhichao Chen,Hao Wang,Fangyikang Wang,Licheng Pan,Zhengnan Li,Yunfei Teng,Haoxuan Li,Zhouchen Lin*

Main category: cs.LG

Relevance: 40.0

TL;DR: 提出SPIRIT框架，通过半近端传输正则化解决扩散模型在时间序列数据填补中的非平稳性和目标不一致问题


<details>
  <summary>Details</summary>
Motivation: 扩散模型在时间序列数据填补中表现不稳定，主要面临两个障碍：1) 非平稳时间动态导致推理轨迹偏差和异常值敏感；2) 目标不一致，填补需要准确的点恢复而扩散模型本质是生成多样样本

Method: 从近端算子角度分析扩散模型填补过程，发现隐式Wasserstein距离正则化阻碍模型对抗非平稳性。提出SPIRIT框架：引入熵诱导的Bregman散度松弛Wasserstein距离的质量保持约束，构建半近端传输差异，理论证明其对非平稳性的鲁棒性，并移除耗散结构

Result: 大量实验证明SPIRIT方法的有效性

Conclusion: 通过半近端传输正则化解决了扩散模型在时间序列填补中的关键限制，提高了在复杂场景下的性能

Abstract: Diffusion models (DMs) have shown promise for Time-Series Data Imputation (TSDI); however, their performance remains inconsistent in complex scenarios. We attribute this to two primary obstacles: (1) non-stationary temporal dynamics, which can bias the inference trajectory and lead to outlier-sensitive imputations; and (2) objective inconsistency, since imputation favors accurate pointwise recovery whereas DMs are inherently trained to generate diverse samples. To better understand these issues, we analyze DM-based TSDI process through a proximal-operator perspective and uncover that an implicit Wasserstein distance regularization inherent in the process hinders the model's ability to counteract non-stationarity and dissipative regularizer, thereby amplifying diversity at the expense of fidelity. Building on this insight, we propose a novel framework called SPIRIT (Semi-Proximal Transport Regularized time-series Imputation). Specifically, we introduce entropy-induced Bregman divergence to relax the mass preserving constraint in the Wasserstein distance, formulate the semi-proximal transport (SPT) discrepancy, and theoretically prove the robustness of SPT against non-stationarity. Subsequently, we remove the dissipative structure and derive the complete SPIRIT workflow, with SPT serving as the proximal operator. Extensive experiments demonstrate the effectiveness of the proposed SPIRIT approach.

</details>


### [1099] [Toward Enhancing Representation Learning in Federated Multi-Task Settings](https://arxiv.org/abs/2602.01626)
*Mehdi Setayesh,Mahdi Beitollahi,Yasser H. Khalil,Hongliang Li*

Main category: cs.LG

Relevance: 40.0

TL;DR: 提出FedMuscle算法，通过Muscle损失函数实现联邦多任务学习，处理模型和任务异质性，优于现有方法


<details>
  <summary>Details</summary>
Motivation: 现有联邦多任务学习方法通常假设模型同质性，限制了在实际场景中的应用。需要处理模型和任务异质性，同时保护数据隐私

Method: 提出Muscle损失函数，一种对比学习目标，同时对齐所有参与模型的表示。基于此开发FedMuscle算法，通信高效，能处理模型和任务异质性

Result: 在多样化的图像和语言任务上，FedMuscle始终优于最先进的基线方法，在异构设置下提供显著改进和鲁棒性能

Conclusion: 通过共享表示空间而非模型参数，FedMuscle能有效处理联邦多任务学习中的异质性问题，具有实际应用价值

Abstract: Federated multi-task learning (FMTL) seeks to collaboratively train customized models for users with different tasks while preserving data privacy. Most existing approaches assume model congruity (i.e., the use of fully or partially homogeneous models) across users, which limits their applicability in realistic settings. To overcome this limitation, we aim to learn a shared representation space across tasks rather than shared model parameters. To this end, we propose Muscle loss, a novel contrastive learning objective that simultaneously aligns representations from all participating models. Unlike existing multi-view or multi-model contrastive methods, which typically align models pairwise, Muscle loss can effectively capture dependencies across tasks because its minimization is equivalent to the maximization of mutual information among all the models' representations. Building on this principle, we develop FedMuscle, a practical and communication-efficient FMTL algorithm that naturally handles both model and task heterogeneity. Experiments on diverse image and language tasks demonstrate that FedMuscle consistently outperforms state-of-the-art baselines, delivering substantial improvements and robust performance across heterogeneous settings.

</details>


### [1100] [MGKAN: Predicting Asymmetric Drug-Drug Interactions via a Multimodal Graph Kolmogorov-Arnold Network](https://arxiv.org/abs/2602.01751)
*Kunyi Fan,Mengjie Chen,Longlong Li,Cunquan Qu*

Main category: cs.LG

Relevance: 40.0

TL;DR: MGKAN是一种用于预测药物相互作用的图神经网络模型，它引入可学习的基函数和不对称网络结构来捕捉非线性、异质性的药物关系模式。


<details>
  <summary>Details</summary>
Motivation: 现有的图神经网络模型在预测药物相互作用时主要依赖线性聚合和对称假设，这限制了它们捕捉非线性、异质性模式的能力。药物相互作用本质上是非线性和方向性的，需要更先进的建模方法。

Method: 提出MGKAN（Graph Kolmogorov-Arnold Network），将可学习的基函数引入不对称DDI预测中。用KAN驱动的基函数替代传统的MLP变换，整合三个网络视图（不对称DDI网络、共相互作用网络、生化相似性网络），使用角色特定嵌入保持方向语义，并通过融合模块结合线性注意力和非线性变换增强表示能力。

Result: 在两个基准数据集上，MGKAN超越了七个最先进的基线模型。消融研究和案例研究证实了其预测准确性以及在建模方向性药物效应方面的有效性。

Conclusion: MGKAN通过引入可学习的基函数和不对称网络结构，能够更有效地捕捉药物相互作用的非线性、异质性模式，在DDI预测任务上表现出优越性能。

Abstract: Predicting drug-drug interactions (DDIs) is essential for safe pharmacological treatments. Previous graph neural network (GNN) models leverage molecular structures and interaction networks but mostly rely on linear aggregation and symmetric assumptions, limiting their ability to capture nonlinear and heterogeneous patterns. We propose MGKAN, a Graph Kolmogorov-Arnold Network that introduces learnable basis functions into asymmetric DDI prediction. MGKAN replaces conventional MLP transformations with KAN-driven basis functions, enabling more expressive and nonlinear modeling of drug relationships. To capture pharmacological dependencies, MGKAN integrates three network views-an asymmetric DDI network, a co-interaction network, and a biochemical similarity network-with role-specific embeddings to preserve directional semantics. A fusion module combines linear attention and nonlinear transformation to enhance representational capacity. On two benchmark datasets, MGKAN outperforms seven state-of-the-art baselines. Ablation studies and case studies confirm its predictive accuracy and effectiveness in modeling directional drug effects.

</details>


### [1101] [Hyperbolic Graph Neural Networks Under the Microscope: The Role of Geometry-Task Alignment](https://arxiv.org/abs/2602.01828)
*Dionisia Naddeo,Jonas Linkerhägner,Nicola Toschi,Geri Skenderi,Veronica Lachi*

Main category: cs.LG

Relevance: 40.0

TL;DR: 本文质疑双曲图神经网络（HGNNs）在树状图上的应用范式，提出几何-任务对齐条件，证明HGNNs仅在任务需要保持度量结构时才有优势。


<details>
  <summary>Details</summary>
Motivation: 许多复杂网络具有双曲结构特性，因此双曲空间自然适合表示层次化和树状图。基于此，HGNNs被广泛认为是树状图表征学习的合理选择。本文质疑这一范式，提出几何-任务对齐条件，即目标任务的度量结构是否与输入图的度量结构一致。

Method: 1. 提出几何-任务对齐条件；2. 在合成回归问题上理论和实证证明HGNNs恢复低失真表示的能力；3. 在链接预测和节点分类任务上联合分析预测性能和嵌入失真；4. 比较HGNNs与欧几里得模型在不同对齐条件下的表现。

Result: 1. HGNNs在需要保持度量结构的问题中表现出几何归纳偏置的优势；2. 链接预测是几何对齐的任务，而节点分类不是；3. 在几何对齐条件下，HGNNs始终优于欧几里得模型，否则优势消失。

Conclusion: 研究焦点应从"图是否是双曲的？"扩展到"任务是否与双曲几何对齐？"，强调任务几何对齐对HGNNs有效性的关键作用。

Abstract: Many complex networks exhibit hyperbolic structural properties, making hyperbolic space a natural candidate for representing hierarchical and tree-like graphs with low distortion. Based on this observation, Hyperbolic Graph Neural Networks (HGNNs) have been widely adopted as a principled choice for representation learning on tree-like graphs. In this work, we question this paradigm by proposing an additional condition of geometry-task alignment, i.e., whether the metric structure of the target follows that of the input graph. We theoretically and empirically demonstrate the capability of HGNNs to recover low-distortion representations on two synthetic regression problems, and show that their geometric inductive bias becomes helpful when the problem requires preserving metric structure. Additionally, we evaluate HGNNs on the tasks of link prediction and node classification by jointly analyzing predictive performance and embedding distortion, revealing that only link prediction is geometry-aligned. Overall, our findings shift the focus from only asking "Is the graph hyperbolic?" to also questioning "Is the task aligned with hyperbolic geometry?", showing that HGNNs consistently outperform Euclidean models under such alignment, while their advantage vanishes otherwise.

</details>


### [1102] [Observation-dependent Bayesian active learning via input-warped Gaussian processes](https://arxiv.org/abs/2602.01898)
*Sanna Jarl,Maria Bånkestad,Jonathan J. S. Scragg,Jens Sjölund*

Main category: cs.LG

Relevance: 40.0

TL;DR: 提出一种通过可学习的单调重参数化扭曲输入空间的贝叶斯主动学习方法，使探索策略能够根据观测变异性扩展或压缩输入空间区域，从而提高采样效率


<details>
  <summary>Details</summary>
Motivation: 传统高斯过程代理在贝叶斯主动学习中，其后验方差仅通过超参数依赖于观测输出，导致探索对实际测量不敏感。需要注入观测依赖的反馈机制来改善探索效率

Method: 通过学习单调重参数化扭曲输入空间，使设计策略能够根据观测变异性扩展或压缩输入空间区域，从而影响基于方差的采集函数行为。提出通过边际似然训练扭曲函数，并引入新的自监督目标以获得更好性能

Result: 该方法在一系列主动学习基准测试中提高了采样效率，特别是在非平稳性挑战传统方法的场景下表现优异

Conclusion: 通过输入空间扭曲注入观测依赖反馈是提高贝叶斯主动学习采样效率的有效方法，自监督训练目标相比边际似然训练能获得更好性能

Abstract: Bayesian active learning relies on the precise quantification of predictive uncertainty to explore unknown function landscapes. While Gaussian process surrogates are the standard for such tasks, an underappreciated fact is that their posterior variance depends on the observed outputs only through the hyperparameters, rendering exploration largely insensitive to the actual measurements. We propose to inject observation-dependent feedback by warping the input space with a learned, monotone reparameterization. This mechanism allows the design policy to expand or compress regions of the input space in response to observed variability, thereby shaping the behavior of variance-based acquisition functions. We demonstrate that while such warps can be trained via marginal likelihood, a novel self-supervised objective yields substantially better performance. Our approach improves sample efficiency across a range of active learning benchmarks, particularly in regimes where non-stationarity challenges traditional methods.

</details>


### [1103] [Data- and Variance-dependent Regret Bounds for Online Tabular MDPs](https://arxiv.org/abs/2602.01903)
*Mingyi Li,Taira Tsuchiya,Kenji Yamanishi*

Main category: cs.LG

Relevance: 40.0

TL;DR: 该论文研究具有已知转移的表格MDP在线学习，提出"两全其美"算法，在对抗性和随机性两种机制下分别实现数据依赖和方差依赖的遗憾界。


<details>
  <summary>Details</summary>
Motivation: 现有MDP在线学习算法通常在对抗性或随机性机制下分别优化，缺乏统一的适应性框架。本文旨在开发能同时适应两种机制的算法，实现更精细的复杂度度量。

Method: 提出两种算法：基于全局优化的算法（使用乐观跟随正则化领导者方法和对数障碍正则化）和基于策略优化的算法（利用新的乐观Q函数估计器）。引入一阶、二阶、路径长度等数据依赖复杂度度量。

Result: 全局优化算法在对抗机制下实现一阶、二阶和路径长度遗憾界，在随机机制下实现方差感知的gap无关和gap相关遗憾界（后者对数依赖episode数）。策略优化算法达到类似适应性（相差一个horizon因子）。建立了复杂度度量的遗憾下界。

Conclusion: 本文开发了能同时适应对抗性和随机性MDP环境的统一算法框架，实现了精细的复杂度度量，证明了全局优化方法的近乎最优性，为在线强化学习提供了理论保证。

Abstract: This work studies online episodic tabular Markov decision processes (MDPs) with known transitions and develops best-of-both-worlds algorithms that achieve refined data-dependent regret bounds in the adversarial regime and variance-dependent regret bounds in the stochastic regime. We quantify MDP complexity using a first-order quantity and several new data-dependent measures for the adversarial regime, including a second-order quantity and a path-length measure, as well as variance-based measures for the stochastic regime. To adapt to these measures, we develop algorithms based on global optimization and policy optimization, both built on optimistic follow-the-regularized-leader with log-barrier regularization. For global optimization, our algorithms achieve first-order, second-order, and path-length regret bounds in the adversarial regime, and in the stochastic regime, they achieve a variance-aware gap-independent bound and a variance-aware gap-dependent bound that is polylogarithmic in the number of episodes. For policy optimization, our algorithms achieve the same data- and variance-dependent adaptivity, up to a factor of the episode horizon, by exploiting a new optimistic $Q$-function estimator. Finally, we establish regret lower bounds in terms of data-dependent complexity measures for the adversarial regime and a variance measure for the stochastic regime, implying that the regret upper bounds achieved by the global-optimization approach are nearly optimal.

</details>


### [1104] [Efficient Neural Controlled Differential Equations via Attentive Kernel Smoothing](https://arxiv.org/abs/2602.02157)
*Egor Serov,Ilya Kuleshov,Alexey Zaytsev*

Main category: cs.LG

Relevance: 40.0

TL;DR: 提出一种用于神经控制微分方程的新路径构建方法，用核和高斯过程平滑替代精确插值，通过多视图CDE恢复细节，显著减少函数评估次数和推理时间


<details>
  <summary>Details</summary>
Motivation: 神经控制微分方程在序列建模中很强大，但驱动控制路径的粗糙性限制了其效率。标准样条引入高频变化，迫使自适应求解器采取过小步长，增加了函数评估次数

Method: 1) 用核和高斯过程平滑替代精确插值，控制轨迹正则性；2) 提出基于注意力的多视图CDE及其卷积扩展，使用可学习查询来重建路径；3) 让模型在多个轨迹上分配表示能力，每个捕获不同的时间模式

Result: MVC-CDE with GP方法在保持最先进精度的同时，显著减少了函数评估次数和总推理时间，相比基于样条的基线方法

Conclusion: 提出的平滑路径构建和多视图CDE框架有效解决了神经控制微分方程的效率问题，在序列建模任务中实现了更好的精度-效率权衡

Abstract: Neural Controlled Differential Equations (Neural CDEs) provide a powerful continuous-time framework for sequence modeling, yet the roughness of the driving control path often restricts their efficiency. Standard splines introduce high-frequency variations that force adaptive solvers to take excessively small steps, driving up the Number of Function Evaluations (NFE). We propose a novel approach to Neural CDE path construction that replaces exact interpolation with Kernel and Gaussian Process (GP) smoothing, enabling explicit control over trajectory regularity. To recover details lost during smoothing, we propose an attention-based Multi-View CDE (MV-CDE) and its convolutional extension (MVC-CDE), which employ learnable queries to inform path reconstruction. This framework allows the model to distribute representational capacity across multiple trajectories, each capturing distinct temporal patterns. Empirical results demonstrate that our method, MVC-CDE with GP, achieves state-of-the-art accuracy while significantly reducing NFEs and total inference time compared to spline-based baselines.

</details>


### [1105] [Repurposing Protein Language Models for Latent Flow-Based Fitness Optimization](https://arxiv.org/abs/2602.02425)
*Amaru Caceres Arroyo,Lea Bogensperger,Ahmed Allam,Michael Krauthammer,Konrad Schindler,Dominik Narnhofer*

Main category: cs.LG

Relevance: 40.0

TL;DR: CHASE是一个蛋白质适应性优化框架，通过压缩预训练蛋白质语言模型的嵌入到紧凑潜在空间，使用条件流匹配模型和分类器自由引导，直接生成高适应性变体，无需预测器引导。


<details>
  <summary>Details</summary>
Motivation: 蛋白质适应性优化面临组合空间巨大、高适应性变体极其稀疏的挑战。现有方法要么性能不足，要么需要计算昂贵的基于梯度的采样。

Method: CHASE框架重新利用预训练蛋白质语言模型的进化知识，将其嵌入压缩到紧凑潜在空间。通过训练带有分类器自由引导的条件流匹配模型，在ODE采样步骤中无需预测器引导即可直接生成高适应性变体。

Result: CHASE在AAV和GFP蛋白质设计基准测试中实现了最先进的性能。在数据受限设置中，使用合成数据进行引导可以进一步提升性能。

Conclusion: CHASE提供了一种有效的蛋白质适应性优化方法，通过利用预训练语言模型的进化知识和条件生成模型，避免了昂贵的梯度采样，在蛋白质设计任务中表现出色。

Abstract: Protein fitness optimization is challenged by a vast combinatorial landscape where high-fitness variants are extremely sparse. Many current methods either underperform or require computationally expensive gradient-based sampling. We present CHASE, a framework that repurposes the evolutionary knowledge of pretrained protein language models by compressing their embeddings into a compact latent space. By training a conditional flow-matching model with classifier-free guidance, we enable the direct generation of high-fitness variants without predictor-based guidance during the ODE sampling steps. CHASE achieves state-of-the-art performance on AAV and GFP protein design benchmarks. Finally, we show that bootstrapping with synthetic data can further enhance performance in data-constrained settings.

</details>


### [1106] [Linear-PAL: A Lightweight Ranker for Mitigating Shortcut Learning in Personalized, High-Bias Tabular Ranking](https://arxiv.org/abs/2602.00013)
*Vipul Dinesh Pawar*

Main category: cs.IR

Relevance: 40.0

TL;DR: 论文提出Linear-PAL框架，通过显式特征组合和强正则化解决电商排序中的位置偏差问题，相比深度集成方法在去偏排序质量和训练效率上都有显著提升。


<details>
  <summary>Details</summary>
Motivation: 电商排序中，用户隐式反馈存在严重的位置偏差（用户倾向于点击顶部商品），现有深度学习方法在高偏差场景下会出现"捷径学习"问题，过度拟合排名信号而非相关性，导致排序质量下降。

Method: 提出Linear-PAL框架：1）通过显式特征组合结构约束实现去偏；2）采用强正则化防止过拟合；3）引入向量化整数哈希技术替代字符串操作，将特征生成复杂度降至O(N)。

Result: 在大规模数据集（420万样本）上，Linear-PAL在去偏排序质量（相关性AUC：0.7626 vs 0.6736）和训练效率（40秒 vs 1762秒，提升43倍）上都显著优于深度集成方法。

Conclusion: Linear-PAL通过结构约束和高效实现，有效解决了高偏差场景下的捷径学习问题，实现了帕累托最优，既能提升排序质量又能大幅降低计算成本，支持高频重训练以适应实时市场变化。

Abstract: In e-commerce ranking, implicit user feedback is systematically confounded by Position Bias -- the strong propensity of users to interact with top-ranked items regardless of relevance. While Deep Learning architectures (e.g., Two-Tower Networks) are the standard solution for de-biasing, we demonstrate that in High-Bias Regimes, state-of-the-art Deep Ensembles suffer from Shortcut Learning: they minimize training loss by overfitting to the rank signal, leading to degraded ranking quality despite high prediction accuracy. We propose Linear Position-bias Aware Learning (Linear-PAL), a lightweight framework that enforces de-biasing through structural constraints: explicit feature conjunctions and aggressive regularization. We further introduce a Vectorized Integer Hashing technique for feature generation, replacing string-based operations with $O(N)$ vectorized arithmetic. Evaluating on a large-scale dataset (4.2M samples), Linear-PAL achieves Pareto Dominance: it outperforms Deep Ensembles in de-biased ranking quality (Relevance AUC: 0.7626 vs. 0.6736) while reducing training latency by 43x (40s vs 1762s). This computational efficiency enables high-frequency retraining, allowing the system to capture user-specific emerging market trends and deliver robust, personalized ranking in near real-time.

</details>


### [1107] [Learning Sequential Decisions from Multiple Sources via Group-Robust Markov Decision Processes](https://arxiv.org/abs/2602.01825)
*Mingyuan Xu,Zongqi Xia,Tianxi Cai,Doudou Zhou,Nian Si*

Main category: stat.ME

Relevance: 40.0

TL;DR: 该论文提出了一种从多站点离线数据中学习鲁棒顺序决策策略的方法，使用具有组线性结构的分布鲁棒MDP，引入特征级不确定性集，并开发了基于悲观值迭代的离线算法。


<details>
  <summary>Details</summary>
Motivation: 从多个异构站点（如医院）收集的数据具有共同结构但存在异质性，需要学习鲁棒的顺序决策策略。现有方法通常依赖强假设（如状态-动作矩形性），难以处理跨站点不确定性。

Method: 1) 使用具有组线性结构的分布鲁棒MDP建模跨站点不确定性；2) 引入特征级(d-矩形)不确定性集；3) 开发基于悲观值迭代的离线算法，包括：站点级岭回归、特征级最坏情况聚合、数据依赖的悲观惩罚；4) 提出基于站点相似性的聚类级扩展。

Result: 在鲁棒部分覆盖假设下，证明了所得策略的次优性界。框架能够处理多站点异构数据，提供无需强状态-动作矩形性假设的鲁棒规划方法。

Conclusion: 该框架有效解决了多站点学习中的异构数据源问题，提供了处理跨站点不确定性的原则性方法，在保持可处理性的同时避免了过强假设。

Abstract: We often collect data from multiple sites (e.g., hospitals) that share common structure but also exhibit heterogeneity. This paper aims to learn robust sequential decision-making policies from such offline, multi-site datasets. To model cross-site uncertainty, we study distributionally robust MDPs with a group-linear structure: all sites share a common feature map, and both the transition kernels and expected reward functions are linear in these shared features. We introduce feature-wise (d-rectangular) uncertainty sets, which preserve tractable robust Bellman recursions while maintaining key cross-site structure. Building on this, we then develop an offline algorithm based on pessimistic value iteration that includes: (i) per-site ridge regression for Bellman targets, (ii) feature-wise worst-case (row-wise minimization) aggregation, and (iii) a data-dependent pessimism penalty computed from the diagonals of the inverse design matrices. We further propose a cluster-level extension that pools similar sites to improve sample efficiency, guided by prior knowledge of site similarity. Under a robust partial coverage assumption, we prove a suboptimality bound for the resulting policy. Overall, our framework addresses multi-site learning with heterogeneous data sources and provides a principled approach to robust planning without relying on strong state-action rectangularity assumptions.

</details>


### [1108] [Online Fine-Tuning of Pretrained Controllers for Autonomous Driving via Real-Time Recurrent RL](https://arxiv.org/abs/2602.02236)
*Julian Lemmel,Felix Resch,Mónika Farsang,Ramin Hasani,Daniela Rus,Radu Grosu*

Main category: cs.RO

Relevance: 40.0

TL;DR: 论文提出使用实时循环强化学习(RTRRL)在线微调预训练策略，结合液体电阻-液体电容RNN模型，在模拟和真实自动驾驶任务中实现自适应控制


<details>
  <summary>Details</summary>
Motivation: 预训练策略在真实世界部署面临环境动态变化、传感器漂移和任务目标变化等挑战，固定策略性能会迅速下降，需要在线自适应方法

Method: 采用实时循环强化学习(RTRRL)进行在线策略微调，结合液体电阻-液体电容RNN模型，在模拟CarRacing环境和真实RoboRacer事件相机线跟随任务中验证

Result: RTRRL能有效微调预训练策略，提高自动驾驶任务性能，且与生物启发的循环网络模型有协同效应

Conclusion: 实时循环强化学习为预训练策略的在线自适应提供有效解决方案，结合生物启发模型在模拟和真实自动驾驶任务中表现良好

Abstract: Deploying pretrained policies in real-world applications presents substantial challenges that fundamentally limit the practical applicability of learning-based control systems. When autonomous systems encounter environmental changes in system dynamics, sensor drift, or task objectives, fixed policies rapidly degrade in performance. We show that employing Real-Time Recurrent Reinforcement Learning (RTRRL), a biologically plausible algorithm for online adaptation, can effectively fine-tune a pretrained policy to improve autonomous agents' performance on driving tasks. We further show that RTRRL synergizes with a recent biologically inspired recurrent network model, the Liquid-Resistance Liquid-Capacitance RNN. We demonstrate the effectiveness of this closed-loop approach in a simulated CarRacing environment and in a real-world line-following task with a RoboRacer car equipped with an event camera.

</details>


### [1109] [Masked Autoencoders as Universal Speech Enhancer](https://arxiv.org/abs/2602.02413)
*Rajalaxmi Rajagopalan,Ritwik Giri,Zhiqiang Tang,Kyu Han*

Main category: cs.SD

Relevance: 40.0

TL;DR: 提出一种基于掩码自编码器的通用语音增强器，采用自监督学习方式，能够处理多种失真类型，并通过微调适应特定下游任务。


<details>
  <summary>Details</summary>
Motivation: 在实际场景中缺乏干净语音数据，需要开发性能可比、可应用于其他语音相关下游任务的自监督学习语音增强方法。

Method: 使用掩码自编码器架构，通过增强堆栈向噪声输入添加更多失真，模型学习去除添加的失真并重建掩码区域。预训练嵌入用于微调模型处理特定下游任务。

Result: 该方法不仅优于基线，还在领域内和领域外评估数据集上实现了最先进的性能。

Conclusion: 提出的自监督学习通用语音增强器能够有效处理多种失真，通过预训练和微调框架在语音增强任务中表现出色。

Abstract: Supervised speech enhancement methods have been very successful. However, in practical scenarios, there is a lack of clean speech, and self-supervised learning-based (SSL) speech enhancement methods that offer comparable enhancement performance and can be applied to other speech-related downstream applications are desired. In this work, we develop a masked autoencoder based universal speech enhancer that is agnostic to the type of distortion affecting speech, can handle multiple distortions simultaneously, and is trained in a self-supervised manner. An augmentation stack adds further distortions to the noisy input data. The masked autoencoder model learns to remove the added distortions along with reconstructing the masked regions of the spectrogram during pre-training. The pre-trained embeddings are then used by fine-tuning models trained on a small amount of paired data for specific downstream tasks. We evaluate the pre-trained features for denoising and dereverberation downstream tasks. We explore different augmentations (like single or multi-speaker) in the pre-training augmentation stack and the effect of different noisy input feature representations (like $log1p$ compression) on pre-trained embeddings and downstream fine-tuning enhancement performance. We show that the proposed method not only outperforms the baseline but also achieves state-of-the-art performance for both in-domain and out-of-domain evaluation datasets.

</details>


### [1110] [Measurement for Opaque Systems: Multi-source Triangulation with Interpretable Machine Learning](https://arxiv.org/abs/2602.00022)
*Margaret Foster*

Main category: cs.LG

Relevance: 35.0

TL;DR: 提出一个针对难以直接观测场景的测量框架，使用间接数据痕迹、可解释机器学习模型和理论指导的三角验证来填补不可访问的测量空间。


<details>
  <summary>Details</summary>
Motivation: 许多高风险的系统和政策相关场景难以直接观测：关键动态不可观察、数据间接且分散在不同来源、真实情况缺失或被隐藏。在这些情况下，现有数据通常不支持传统的分析策略。

Method: 结合多源三角验证与可解释机器学习模型，不依赖无法观测的理想数据准确性，而是寻求不同部分信息模型之间的一致性，通过交叉信号一致性或与预期状态的偏离来得出可靠结论。

Result: 通过一个关于秘密军事组织的实证分析，展示了该方法如何恢复具有实质意义的变异，同时明确揭示了推断的局限性。

Conclusion: 该框架为在缺乏足够数据进行传统统计或因果推断的情况下，提供了定量的表征分析方法，能够从多个不完整且有偏见的观测信号中提取有意义的见解。

Abstract: We propose a measurement framework for difficult-to-access contexts that uses indirect data traces, interpretable machine-learning models, and theory-guided triangulation to fill inaccessible measurement spaces. Many high-stakes systems of scientific and policy interest are difficult, if not impossible, to reach directly: dynamics of interest are unobservable, data are indirect and fragmented across sources, and ground truth is absent or concealed. In these settings, available data often do not support conventional strategies for analysis, such as statistical inference on a single authoritative data stream or model validation against labeled outcomes. To address this problem, we introduce a general framework for measurement in data regimes characterized by structurally missing or adversarial data. We propose combining multi-source triangulation with interpretable machine learning models. Rather than relying on accuracy against unobservable, unattainable ideal data, our framework seeks consistency across separate, partially informative models. This allows users to draw defensible conclusions about the state of the world based on cross-signal consistency or divergence from an expected state. Our framework provides an analytical workflow tailored to quantitative characterization in the absence of data sufficient for conventional statistical or causal inference. We demonstrate our approach and explicitly surface inferential limits through an empirical analysis of organizational growth and internal pressure dynamics in a clandestine militant organization, drawing on multiple observational signals that individually provide incomplete and biased views of the underlying process. The results show how triangulated, interpretable ML can recover substantively meaningful variation.

</details>


### [1111] [Distributional Reinforcement Learning for Condition-Based Maintenance of Multi-Pump Equipment](https://arxiv.org/abs/2602.00051)
*Takato Yasuno*

Main category: cs.LG

Relevance: 35.0

TL;DR: 提出一种基于分位数回归深度Q网络（QR-DQN）与老化因子结合的新型分布强化学习方法，用于多设备状态维护（CBM），通过三种策略场景验证，安全优先策略在投资回报率和运行稳定性方面表现最优。


<details>
  <summary>Details</summary>
Motivation: 传统基于时间的维护策略常导致不必要的开支和意外设备故障，而状态维护（CBM）利用实时设备状态数据优化维护时机和资源配置，但现有方法在多设备协同维护方面仍有改进空间。

Method: 采用分位数回归深度Q网络（QR-DQN）结合老化因子的分布强化学习方法，通过三种策略场景（安全优先、平衡、成本效益）对多个泵单元进行并发管理。

Result: 经过3000个训练周期的实验验证，所有策略均显著改善。安全优先策略的投资回报率（ROI）达3.91，性能比替代方案提高152%，仅需增加31%投资，系统运行稳定性达95.66%，具备工业环境即时应用性。

Conclusion: 提出的QR-DQN与老化因子结合的分布强化学习方法在多设备状态维护中有效，安全优先策略在成本效益和运行稳定性方面表现最佳，具备工业应用潜力。

Abstract: Condition-Based Maintenance (CBM) signifies a paradigm shift from reactive to proactive equipment management strategies in modern industrial systems. Conventional time-based maintenance schedules frequently engender superfluous expenditures and unanticipated equipment failures. In contrast, CBM utilizes real-time equipment condition data to enhance maintenance timing and optimize resource allocation. The present paper proposes a novel distributional reinforcement learning approach for multi-equipment CBM using Quantile Regression Deep Q-Networks (QR-DQN) with aging factor integration. The methodology employed in this study encompasses the concurrent administration of multiple pump units through three strategic scenarios. The implementation of safety-first, balanced, and cost-efficient approaches is imperative. Comprehensive experimental validation over 3,000 training episodes demonstrates significant performance improvements across all strategies. The Safety-First strategy demonstrates superior cost efficiency, with a return on investment (ROI) of 3.91, yielding 152\% better performance than alternatives while requiring only 31\% higher investment. The system exhibits 95.66\% operational stability and immediate applicability to industrial environments.

</details>


### [1112] [MiniTensor: A Lightweight, High-Performance Tensor Operations Library](https://arxiv.org/abs/2602.00125)
*Soumyadip Sarkar*

Main category: cs.LG

Relevance: 35.0

TL;DR: MiniTensor是一个开源张量运算库，专注于简洁性、正确性和性能，提供类似PyTorch的Python API，但用Rust引擎执行关键代码，包大小仅几MB，比主流框架小几个数量级。


<details>
  <summary>Details</summary>
Motivation: 现有深度学习框架（如PyTorch、TensorFlow）通常体积庞大、依赖复杂，对于研究和开发环境造成负担。MiniTensor旨在提供一个极简但功能完整的张量运算库，保持核心功能的同时大幅减小安装体积。

Method: 采用Rust引擎实现性能关键代码，通过PyO3与Python集成，提供PyTorch-like API。支持n维张量、广播、归约、矩阵乘法、反向模式自动微分、神经网络层和优化器。设计包括高效内存管理和动态计算图。

Result: MiniTensor包大小仅几MB，比PyTorch和TensorFlow小几个数量级，同时保留了CPU上进行研究和开发所需的核心功能。

Conclusion: MiniTensor证明了可以在保持功能完整性的同时大幅减小深度学习框架的体积，为轻量级研究和开发提供了实用工具。

Abstract: We present MiniTensor, an open source tensor operations library that focuses on minimalism, correctness, and performance. MiniTensor exposes a familiar PyTorch-like Python API while it executes performance critical code in a Rust engine. The core supports dense $n$ dimensional tensors, broadcasting, reductions, matrix multiplication, reverse mode automatic differentiation, a compact set of neural network layers, and standard optimizers. In this paper, we describe the design of MiniTensor's architecture, including its efficient memory management, dynamic computation graph for gradients, and integration with Python via PyO3. We also compare the install footprint with PyTorch and TensorFlow to demonstrate that MiniTensor achieves a package size of only a few megabytes, several orders of magnitude smaller than mainstream frameworks, while preserving the essentials needed for research and development on CPUs. The repository can be found at https://github.com/neuralsorcerer/minitensor

</details>


### [1113] [On the Relationship Between Representation Geometry and Generalization in Deep Neural Networks](https://arxiv.org/abs/2602.00130)
*Sumit Yadav*

Main category: cs.LG

Relevance: 35.0

TL;DR: 有效维度（一种无监督几何度量）能强预测神经网络性能，在图像和NLP任务中都成立，且具有因果性


<details>
  <summary>Details</summary>
Motivation: 研究神经网络表示几何与性能之间的关系，探索能否通过无监督的几何度量来预测模型性能，而不依赖标签或模型规模

Method: 分析52个预训练ImageNet模型（13种架构），使用有效维度作为几何度量，在ImageNet和CIFAR-10上验证，并推广到NLP任务（8个编码器模型和15个仅解码器LLM），通过添加噪声和PCA进行因果实验

Result: 有效维度能强预测模型性能（部分r=0.75，p<10^(-10)），在NLP任务中也成立（r=0.69，p=0.004），而模型规模不能预测（r=0.07）。几何退化导致性能下降（r=-0.94），PCA改善几何能保持性能（仅损失0.03pp）

Conclusion: 有效维度提供领域无关的神经网络性能预测和因果信息，完全无需标签，为模型评估和理解提供了新的几何视角

Abstract: We investigate the relationship between representation geometry and neural network performance. Analyzing 52 pretrained ImageNet models across 13 architecture families, we show that effective dimension -- an unsupervised geometric metric -- strongly predicts accuracy. Output effective dimension achieves partial r=0.75 ($p < 10^(-10)$) after controlling for model capacity, while total compression achieves partial r=-0.72. These findings replicate across ImageNet and CIFAR-10, and generalize to NLP: effective dimension predicts performance for 8 encoder models on SST-2/MNLI and 15 decoder-only LLMs on AG News (r=0.69, p=0.004), while model size does not (r=0.07). We establish bidirectional causality: degrading geometry via noise causes accuracy loss (r=-0.94, $p < 10^(-9)$), while improving geometry via PCA maintains accuracy across architectures (-0.03pp at 95% variance). This relationship is noise-type agnostic -- Gaussian, Uniform, Dropout, and Salt-and-pepper noise all show $|r| > 0.90$. These results establish that effective dimension provides domain-agnostic predictive and causal information about neural network performance, computed entirely without labels.

</details>


### [1114] [How Understanding Forecast Uncertainty Resolves the Explainability Problem in Machine Learning Models](https://arxiv.org/abs/2602.00179)
*Joseph L. Breeden*

Main category: cs.LG

Relevance: 35.0

TL;DR: 本文指出局部线性解释方法（如LIME和SHAP）在决策边界附近的不稳定性反映了预测不确定性高的问题，而非方法本身的缺陷。作者提出应首先评估预测是否可用，只有在低不确定性区域才寻求解释，否则应使用更简单的模型。


<details>
  <summary>Details</summary>
Motivation: 在关键决策应用中，可解释性是主要关注点，但现有局部线性解释方法在决策边界附近被批评为不稳定。作者认为这种批评源于对问题的误解，实际问题是预测不确定性在边界区域较高。

Method: 提出新的解释框架：1）首先评估预测不确定性，判断是否存在可用的预测；2）只有在低不确定性区域才使用局部线性近似进行解释；3）当预测不可用时，应回退到更简单的整体模型（如传统逻辑回归）。

Result: 论证了局部线性解释方法的不稳定性与预测不确定性直接相关，提出只有当预测不确定性足够低时，解释才有意义。同时指出某些声称处处可解释的方法（如ReLU网络）实际上在分段边界处存在预测不确定性过高的问题。

Conclusion: 解释性分析应首先关注预测的可用性，而非盲目寻求解释。只有在预测不确定性低的区域，局部线性解释才有效且稳定。对于不可用的预测，解释是无意义的，应使用更简单的模型。

Abstract: For applications of machine learning in critical decisions, explainability is a primary concern, and often a regulatory requirement. Local linear methods for generating explanations, such as LIME and SHAP, have been criticized for being unstable near decision boundaries. In this paper, we explain that such concerns reflect a misunderstanding of the problem. The forecast uncertainty is high at decision boundaries, so consequently, the explanatory instability is high. The correct approach is to change the sequence of events and questions being asked. Nonlinear models can be highly predictive in some regions while having little or no predictability in others. Therefore, the first question is whether a usable forecast exists. When there is a forecast with low enough uncertainty to be useful, an explanation can be sought via a local linear approximation. In such cases, the explanatory instability is correspondingly low. When no usable forecast exists, the decision must fall to a simpler overall model such as traditional logistic regression. Additionally, these results show that some methods that purport to be explainable everywhere, such as ReLU networks or any piecewise linear model, have only an illusory explainability, because the forecast uncertainty at the segment boundaries is too high to be useful. Explaining an unusable forecast is pointless.

</details>


### [1115] [GEPC: Group-Equivariant Posterior Consistency for Out-of-Distribution Detection in Diffusion Models](https://arxiv.org/abs/2602.00191)
*Yadang Alexis Rouzoumka,Jean Pinsolle,Eugénie Terreaux,Christèle Morisseau,Jean-Philippe Ovarlez,Chengfang Ren*

Main category: cs.LG

Relevance: 35.0

TL;DR: 提出GEPC方法，通过检测扩散模型分数场的等变性破坏来识别分布外数据，无需训练，仅需分数评估，在图像和雷达数据上表现优异。


<details>
  <summary>Details</summary>
Motivation: 现有基于扩散模型的OOD检测方法主要利用分数大小或局部几何特征，忽略了扩散模型分数场继承的近似等变性（翻转、旋转、循环移位等）。当数据分布发生变化时，这种等变性会被破坏，但现有方法无法检测这种变化。

Method: 提出Group-Equivariant Posterior Consistency (GEPC)：1) 通过有限群变换测量学习到的分数场变换的一致性；2) 在群体层面定义理想的GEPC残差，平均群上的等变性残差函数；3) 仅需分数评估，无需额外训练；4) 生成可解释的等变性破坏图。

Result: 1) 在OOD图像基准数据集上，GEPC达到与近期基于扩散的基线方法竞争或更好的AUROC；2) 计算轻量；3) 在高分辨率合成孔径雷达图像上，GEPC实现强目标-背景分离，生成视觉可解释的等变性破坏图。

Conclusion: GEPC通过检测扩散模型分数场的等变性破坏来有效识别OOD数据，提供了一种训练免费、计算高效且可解释的OOD检测方法，在图像和雷达异常检测中表现优异。

Abstract: Diffusion models learn a time-indexed score field $\mathbf{s}_θ(\mathbf{x}_t,t)$ that often inherits approximate equivariances (flips, rotations, circular shifts) from in-distribution (ID) data and convolutional backbones. Most diffusion-based out-of-distribution (OOD) detectors exploit score magnitude or local geometry (energies, curvature, covariance spectra) and largely ignore equivariances. We introduce Group-Equivariant Posterior Consistency (GEPC), a training-free probe that measures how consistently the learned score transforms under a finite group $\mathcal{G}$, detecting equivariance breaking even when score magnitude remains unchanged. At the population level, we propose the ideal GEPC residual, which averages an equivariance-residual functional over $\mathcal{G}$, and we derive ID upper bounds and OOD lower bounds under mild assumptions. GEPC requires only score evaluations and produces interpretable equivariance-breaking maps. On OOD image benchmark datasets, we show that GEPC achieves competitive or improved AUROC compared to recent diffusion-based baselines while remaining computationally lightweight. On high-resolution synthetic aperture radar imagery where OOD corresponds to targets or anomalies in clutter, GEPC yields strong target-background separation and visually interpretable equivariance-breaking maps. Code is available at https://github.com/RouzAY/gepc-diffusion/.

</details>


### [1116] [Reducing Class-Wise Performance Disparity via Margin Regularization](https://arxiv.org/abs/2602.00205)
*Beier Zhu,Kesen Zhao,Jiequan Cui,Qianru Sun,Yuan Zhou,Xun Yang,Hanwang Zhang*

Main category: cs.LG

Relevance: 35.0

TL;DR: 提出MR²正则化方法，通过动态调整logit和表示空间的边界来减少分类中的性能差异，基于理论分析建立类别敏感的泛化边界，实验证明能提升整体精度并改善困难类别的性能。


<details>
  <summary>Details</summary>
Motivation: 深度神经网络即使在类别平衡数据上训练，也经常表现出显著的类别间准确率差异，这对可靠部署构成问题。虽然已有经验性解决方法，但对分类中这种性能差异的理论理解仍然有限。

Method: 提出MR²（Margin Regularization for Performance Disparity Reduction），一种理论上有原则的正则化方法，通过在logit空间和表示空间中动态调整边界。基于理论分析建立了基于边界的类别敏感泛化边界，揭示每个类别的特征变异性如何影响误差，从而激励对困难类别使用更大的边界。MR²根据特征扩展比例优化每个类别的logit边界，并惩罚过大的表示边界以增强类内紧凑性。

Result: 在7个数据集（包括ImageNet）和多种预训练骨干网络（MAE、MoCov2、CLIP）上的实验表明，MR²不仅提高了整体准确率，还显著提升了困难类别的性能，同时不牺牲简单类别的性能，从而减少了性能差异。

Conclusion: MR²通过理论指导的正则化方法有效解决了分类中的性能差异问题，为减少类别间准确率差异提供了有原则的解决方案。

Abstract: Deep neural networks often exhibit substantial disparities in class-wise accuracy, even when trained on class-balanced data, posing concerns for reliable deployment. While prior efforts have explored empirical remedies, a theoretical understanding of such performance disparities in classification remains limited. In this work, we present Margin Regularization for Performance Disparity Reduction (MR$^2$), a theoretically principled regularization for classification by dynamically adjusting margins in both the logit and representation spaces. Our analysis establishes a margin-based, class-sensitive generalization bound that reveals how per-class feature variability contributes to error, motivating the use of larger margins for hard classes. Guided by this insight, MR$^2$ optimizes per-class logit margins proportional to feature spread and penalizes excessive representation margins to enhance intra-class compactness. Experiments on seven datasets, including ImageNet, and diverse pre-trained backbones (MAE, MoCov2, CLIP) demonstrate that MR$^2$ not only improves overall accuracy but also significantly boosts hard class performance without trading off easy classes, thus reducing performance disparity. Code is available at: https://github.com/BeierZhu/MR2

</details>


### [1117] [Analyzing Shapley Additive Explanations to Understand Anomaly Detection Algorithm Behaviors and Their Complementarity](https://arxiv.org/abs/2602.00208)
*Jordan Levy,Paul Saves,Moncef Garouani,Nicolas Verstaevel,Benoit Gaudou*

Main category: cs.LG

Relevance: 35.0

TL;DR: 该论文提出了一种基于解释多样性的异常检测集成方法，通过SHAP解释量化检测器的决策机制，构建互补性更强的集成模型。


<details>
  <summary>Details</summary>
Motivation: 无监督异常检测面临数据分布多样性和标签缺失的挑战。现有集成方法常因检测器依赖相似决策线索而产生冗余，难以构建真正互补的集成模型。

Method: 使用SHAP解释量化异常检测器的决策机制，通过解释相似性度量检测器间的互补性，结合解释多样性和模型性能构建集成。

Result: 解释相似的检测器产生相关异常分数并识别重叠异常，解释差异可靠指示互补检测行为。解释驱动指标为集成选择提供新标准。

Conclusion: 通过显式优化解释多样性同时保持模型质量，可以构建更互补、更有效的无监督异常检测集成模型。

Abstract: Unsupervised anomaly detection is a challenging problem due to the diversity of data distributions and the lack of labels. Ensemble methods are often adopted to mitigate these challenges by combining multiple detectors, which can reduce individual biases and increase robustness. Yet building an ensemble that is genuinely complementary remains challenging, since many detectors rely on similar decision cues and end up producing redundant anomaly scores. As a result, the potential of ensemble learning is often limited by the difficulty of identifying models that truly capture different types of irregularities. To address this, we propose a methodology for characterizing anomaly detectors through their decision mechanisms. Using SHapley Additive exPlanations, we quantify how each model attributes importance to input features, and we use these attribution profiles to measure similarity between detectors. We show that detectors with similar explanations tend to produce correlated anomaly scores and identify largely overlapping anomalies. Conversely, explanation divergence reliably indicates complementary detection behavior. Our results demonstrate that explanation-driven metrics offer a different criterion than raw outputs for selecting models in an ensemble. However, we also demonstrate that diversity alone is insufficient; high individual model performance remains a prerequisite for effective ensembles. By explicitly targeting explanation diversity while maintaining model quality, we are able to construct ensembles that are more diverse, more complementary, and ultimately more effective for unsupervised anomaly detection.

</details>


### [1118] [GRIP2: A Robust and Powerful Deep Knockoff Method for Feature Selection](https://arxiv.org/abs/2602.00218)
*Bob Junyi Zou,Lu Tian*

Main category: cs.LG

Relevance: 35.0

TL;DR: GRIP2是一种基于深度学习的特征选择方法，通过二维正则化表面集成特征重要性，在非线性、高相关性和低信噪比场景下提供严格的错误发现率控制。


<details>
  <summary>Details</summary>
Motivation: 在非线性、高相关性和低信噪比的复杂场景中，深度学习特征选择方法面临识别真正预测性协变量并严格控制错误发现的挑战。现有方法在这些困难条件下可能失效，需要更稳健的特征重要性统计量。

Method: 提出GRIP2（Group Regularization Importance Persistence in 2 Dimensions）深度knockoff特征重要性统计量，通过二维正则化表面（控制稀疏强度和稀疏化几何）集成第一层特征活性。引入高效的块随机采样，在单次训练中近似该表面积分，聚合不同正则化机制下的特征活性幅度，确保构造的统计量具有反对称性。

Result: 在合成和半真实数据实验中，GRIP2对特征相关性和噪声水平表现出更好的鲁棒性：在高相关性和低信噪比条件下，标准深度学习方法可能失效，而GRIP2保持高功效和稳定性。在真实HIV耐药性数据上，GRIP2比现有线性基线方法更好地恢复了已知的耐药相关突变。

Conclusion: GRIP2提供了一种在复杂非线性场景中具有严格错误发现率控制的深度学习特征选择方法，在高相关性和低信噪比条件下表现出优越的鲁棒性和实用性。

Abstract: Identifying truly predictive covariates while strictly controlling false discoveries remains a fundamental challenge in nonlinear, highly correlated, and low signal-to-noise regimes, where deep learning based feature selection methods are most attractive. We propose Group Regularization Importance Persistence in 2 Dimensions (GRIP2), a deep knockoff feature importance statistic that integrates first-layer feature activity over a two-dimensional regularization surface controlling both sparsity strength and sparsification geometry. To approximate this surface integral in a single training run, we introduce efficient block-stochastic sampling, which aggregates feature activity magnitudes across diverse regularization regimes along the optimization trajectory. The resulting statistics are antisymmetric by construction, ensuring finite-sample FDR control. In extensive experiments on synthetic and semi-real data, GRIP2 demonstrates improved robustness to feature correlation and noise level: in high correlation and low signal-to-noise ratio regimes where standard deep learning based feature selectors may struggle, our method retains high power and stability. Finally, on real-world HIV drug resistance data, GRIP2 recovers known resistance-associated mutations with power better than established linear baselines, confirming its reliability in practice.

</details>


### [1119] [Green-NAS: A Global-Scale Multi-Objective Neural Architecture Search for Robust and Efficient Edge-Native Weather Forecasting](https://arxiv.org/abs/2602.00240)
*Md Muhtasim Munif Fahim,Soyda Humyra Yesmin,Saiful Islam,Md. Palash Bin Faruque,Md. A. Salam,Md. Mahfuz Uddin,Samiul Islam,Tofayel Ahmed,Md. Binyamin,Md. Rezaul Karim*

Main category: cs.LG

Relevance: 35.0

TL;DR: Green-NAS是一个面向低资源环境的神经架构搜索框架，以天气预报为案例，遵循"绿色AI"原则，在最小化计算能耗和碳足迹的同时优化模型精度和效率。


<details>
  <summary>Details</summary>
Motivation: 当前天气预测模型通常计算密集且参数庞大，导致高能耗和碳足迹。研究旨在开发可持续的AI解决方案，在低资源环境下实现高效准确的天气预报。

Method: 采用多目标神经架构搜索（NAS）框架，同时优化模型精度和效率指标。通过优化过程寻找轻量级高精度模型，并利用迁移学习提升数据有限城市的预测准确性。

Result: 最佳模型Green-NAS-A仅使用15.3万参数，RMSE达到0.0988（比手动调优基线仅差1.4%），参数数量比GraphCast等全球天气预报模型少239倍。迁移学习可将预测准确性提升约5.2%。

Conclusion: Green-NAS框架证明了在保持高精度的同时大幅减少模型复杂性的可行性，为可持续AI在资源受限环境中的应用提供了有效方案，特别是在天气预报等计算密集型任务中。

Abstract: We introduce Green-NAS, a multi-objective NAS (neural architecture search) framework designed for low-resource environments using weather forecasting as a case study. By adhering to 'Green AI' principles, the framework explicitly minimizes computational energy costs and carbon footprints, prioritizing sustainable deployment over raw computational scale. The Green-NAS architecture search method is optimized for both model accuracy and efficiency to find lightweight models with high accuracy and very few model parameters; this is accomplished through an optimization process that simultaneously optimizes multiple objectives. Our best-performing model, Green-NAS-A, achieved an RMSE of 0.0988 (i.e., within 1.4% of our manually tuned baseline) using only 153k model parameters, which is 239 times fewer than other globally applied weather forecasting models, such as GraphCast. In addition, we also describe how the use of transfer learning will improve the weather forecasting accuracy by approximately 5.2%, in comparison to a naive approach of training a new model for each city, when there is limited historical weather data available for that city.

</details>


### [1120] [Agentic Framework for Epidemiological Modeling](https://arxiv.org/abs/2602.00299)
*Rituparna Datta,Zihan Guan,Baltazar Espinoza,Yiqi Su,Priya Pitre,Srini Venkatramanan,Naren Ramakrishnan,Anil Vullikanti*

Main category: cs.LG

Relevance: 35.0

TL;DR: EPIAGENT是一个自动合成、校准、验证和优化流行病学模拟器的智能体框架，将疾病进展建模为迭代程序合成问题，通过流行病学流图中间表示连接场景规范与模型结构，支持在物理和流行病学约束下的可解释参数学习。


<details>
  <summary>Details</summary>
Motivation: 传统流行病学建模方法依赖固定的模型类别，需要随着病原体、政策和场景假设的变化而手动重新设计，这限制了模型的适应性和效率。需要一种能够自动适应变化、减少人工干预的建模框架。

Method: EPIAGENT采用智能体框架，将疾病进展建模为迭代程序合成问题。核心设计是流行病学流图中间表示，连接场景规范与模型结构，支持模块化正确性检查。验证后的流图被编译为支持物理和流行病学约束下可解释参数学习的机制模型。

Result: 在流行病学场景案例研究中，EPIAGENT能够捕捉复杂的增长动态，并在不同疫苗接种和免疫逃逸假设下产生流行病学一致的反事实预测。智能体反馈循环防止模型退化，并显著加速向有效模型的收敛。

Conclusion: EPIAGENT通过模仿专业专家工作流程，实现了流行病学模拟器的自动合成和优化，解决了传统方法需要手动重新设计的问题，提高了建模的适应性和效率。

Abstract: Epidemic modeling is essential for public health planning, yet traditional approaches rely on fixed model classes that require manual redesign as pathogens, policies, and scenario assumptions evolve. We introduce EPIAGENT, an agentic framework that automatically synthesizes, calibrates, verifies, and refines epidemiological simulators by modeling disease progression as an iterative program synthesis problem. A central design choice is an explicit epidemiological flow graph intermediate representation that links scenario specifications to model structure and enables strong, modular correctness checks before code is generated. Verified flow graphs are then compiled into mechanistic models supporting interpretable parameter learning under physical and epidemiological constraints. Evaluation on epidemiological scenario case studies demonstrates that EPIAGENT captures complex growth dynamics and produces epidemiologically consistent counterfactual projections across varying vaccination and immune escape assumptions. Our results show that the agentic feedback loop prevents degeneration and significantly accelerates convergence toward valid models by mimicking professional expert workflows.

</details>


### [1121] [Neural Ising Machines via Unrolling and Zeroth-Order Training](https://arxiv.org/abs/2602.00302)
*Sam Reifenstein,Timothee Leleu*

Main category: cs.LG

Relevance: 35.0

TL;DR: 提出NPIM（神经网络参数化伊辛机），通过数据驱动学习迭代动力系统的更新规则，用于NP难的伊辛模型和最大割优化问题。


<details>
  <summary>Details</summary>
Motivation: 传统伊辛机启发式算法需要人工设计更新规则，而学习型方法可以通过数据驱动自动发现有效的优化策略，提高在复杂非凸能量景观中的搜索效率。

Method: 1. 学习共享的节点级更新规则，将局部相互作用场映射到自旋更新；2. 使用紧凑的多层感知机参数化，参数数量少；3. 采用零阶优化器训练，避免长循环动力系统反向传播的不稳定梯度问题。

Result: NPIM在标准伊辛和神经组合优化基准测试中，相比最近的学习方法和经典伊辛机启发式算法，取得了有竞争力的解质量和求解时间。

Conclusion: 尽管参数数量少，学习的动力系统恢复了有效的算法结构（如动量行为和时变调度），能够在高度非凸的能量景观中进行高效搜索。

Abstract: We propose a data-driven heuristic for NP-hard Ising and Max-Cut optimization that learns the update rule of an iterative dynamical system. The method learns a shared, node-wise update rule that maps local interaction fields to spin updates, parameterized by a compact multilayer perceptron with a small number of parameters. Training is performed using a zeroth-order optimizer, since backpropagation through long, recurrent Ising-machine dynamics leads to unstable and poorly informative gradients. We call this approach a neural network parameterized Ising machine (NPIM). Despite its low parameter count, the learned dynamics recover effective algorithmic structure, including momentum-like behavior and time-varying schedules, enabling efficient search in highly non-convex energy landscapes. Across standard Ising and neural combinatorial optimization benchmarks, NPIM achieves competitive solution quality and time-to-solution relative to recent learning-based methods and strong classical Ising-machine heuristics.

</details>


### [1122] [Optimal Transport-Guided Adversarial Attacks on Graph Neural Network-Based Bot Detection](https://arxiv.org/abs/2602.00318)
*Kunal Mukherjee,Zulfikar Alom,Tran Gia Bao Ngo,Cuneyt Gurcan Akcora,Murat Kantarcioglu*

Main category: cs.LG

Relevance: 35.0

TL;DR: BOCLOAK：一个基于最优传输理论的对抗攻击框架，用于在现实约束下评估GNN社交机器人检测器的鲁棒性


<details>
  <summary>Details</summary>
Motivation: 社交媒体上机器人账户的兴起对公共话语构成重大威胁。虽然现代机器人检测器越来越多地依赖图神经网络（GNNs），但这些检测器在现实世界中的有效性仍不清楚。攻击者会不断调整策略，并且必须在特定领域和时间约束下操作，这限制了现有攻击方法的适用性。因此，需要在现实约束下评估GNN机器人检测器的鲁棒性。

Method: BOCLOAK通过边编辑和节点注入对抗攻击来系统评估GNN社交机器人检测的鲁棒性。该方法构建了时空邻居特征的概率度量，学习分离人类和机器人行为的最优传输几何，然后将传输计划解码为稀疏、合理的边编辑，这些编辑既能逃避检测，又遵守现实世界约束。

Result: 在三个社交机器人数据集、五个最先进的机器人检测器、三个对抗防御方法上评估BOCLOAK，并与四个领先的图对抗攻击基线比较。BOCLOAK在现实约束下实现了高达80.13%的攻击成功率提升，同时使用99.80%更少的GPU内存。

Conclusion: 最优传输提供了一个轻量级、有原则的框架，用于弥合对抗攻击与现实世界机器人检测之间的差距。BOCLOAK展示了在现实约束下评估GNN检测器鲁棒性的重要性。

Abstract: The rise of bot accounts on social media poses significant risks to public discourse. To address this threat, modern bot detectors increasingly rely on Graph Neural Networks (GNNs). However, the effectiveness of these GNN-based detectors in real-world settings remains poorly understood. In practice, attackers continuously adapt their strategies as well as must operate under domain-specific and temporal constraints, which can fundamentally limit the applicability of existing attack methods. As a result, there is a critical need for robust GNN-based bot detection methods under realistic, constraint-aware attack scenarios.
  To address this gap, we introduce BOCLOAK to systematically evaluate the robustness of GNN-based social bot detection via both edge editing and node injection adversarial attacks under realistic constraints. BOCLOAK constructs a probability measure over spatio-temporal neighbor features and learns an optimal transport geometry that separates human and bot behaviors. It then decodes transport plans into sparse, plausible edge edits that evade detection while obeying real-world constraints. We evaluate BOCLOAK across three social bot datasets, five state-of-the-art bot detectors, three adversarial defenses, and compare it against four leading graph adversarial attack baselines. BOCLOAK achieves up to 80.13% higher attack success rates while using 99.80% less GPU memory under realistic real-world constraints. Most importantly, BOCLOAK shows that optimal transport provides a lightweight, principled framework for bridging the gap between adversarial attacks and real-world bot detection.

</details>


### [1123] [Prototype-based Explainable Neural Networks with Channel-specific Reasoning for Geospatial Learning Tasks](https://arxiv.org/abs/2602.00331)
*Anushka Narayanan,Karianne J. Bergen*

Main category: cs.LG

Relevance: 35.0

TL;DR: 提出了一种针对多通道地理空间数据的原型可解释AI方法，能够识别通道特定的原型特征，为地理科学任务提供局部和全局解释。


<details>
  <summary>Details</summary>
Motivation: 现有原型XAI方法主要针对标准RGB图像设计，不适用于地理科学中常见的多通道、变量特定的图像和栅格数据集。需要专门针对地理空间数据特点的可解释方法。

Method: 开发了针对多通道地理空间数据的原型XAI方法，每个通道代表不同的物理环境变量或光谱通道。模型能够识别来自多个训练样本的通道特定原型特征，并解释这些特征如何单独和组合影响预测。

Result: 通过两个地理科学案例验证：1)使用多变量气候数据分类Madden Julian Oscillation相位；2)多光谱卫星图像的土地利用分类。方法在保持与标准神经网络相当性能的同时，提供局部和全局解释。

Conclusion: 通过将通道原型明确纳入预测过程，该方法增强了地理科学机器学习任务的透明度和可信度，为多通道地理空间数据提供了专门的可解释AI解决方案。

Abstract: Explainable AI (XAI) is essential for understanding machine learning (ML) decision-making and ensuring model trustworthiness in scientific applications. Prototype-based XAI methods offer an intrinsically interpretable alternative to post-hoc approaches which often yield inconsistent explanations. Prototype-based XAI methods make predictions based on the similarity between inputs and learned prototypes that represent typical characteristics of target classes. However, existing prototype-based models are primarily designed for standard RGB image data and are not optimized for the distinct, variable-specific channels commonly found in geoscientific image and raster datasets. In this study, we develop a prototype-based XAI approach tailored for multi-channel geospatial data, where each channel represents a distinct physical environmental variable or spectral channel. Our approach enables the model to identify separate, channel-specific prototypical characteristics sourced from multiple distinct training examples that inform how these features individually and in combination influence model prediction while achieving comparable performance to standard neural networks. We demonstrate this method through two geoscientific case studies: (1) classification of Madden Julian Oscillation phases using multi-variable climate data and (2) land-use classification from multispectral satellite imagery. This approach produces both local (instance-level) and global (model-level) explanations for providing insights into feature-relevance across channels. By explicitly incorporating channel-prototypes into the prediction process, we discuss how this approach enhances the transparency and trustworthiness of ML models for geoscientific learning tasks.

</details>


### [1124] [Leveraging Textual-Cues for Enhancing Multimodal Sentiment Analysis by Object Recognition](https://arxiv.org/abs/2602.00360)
*Sumana Biswas,Karen Young,Josephine Griffith*

Main category: cs.LG

Relevance: 35.0

TL;DR: 论文提出TEMSA方法，通过图像物体识别提取物体名称并与文本结合，改善多模态情感分析性能


<details>
  <summary>Details</summary>
Motivation: 多模态情感分析面临文本和图像模态差异、情感歧义和上下文复杂性等挑战，需要有效结合两种模态信息的方法

Method: 提出TEMSA方法：从图像中提取所有检测到的物体名称，将其与相关文本结合形成TEMS（文本增强多模态数据），用于情感分析

Result: 实验表明，仅使用TEMS（结合所有物体名称）相比单独分析能改善多模态数据的整体情感分析结果

Conclusion: TEMSA方法能有效结合图像和文本数据，推进多模态情感分析发展，为模态融合提供新思路

Abstract: Multimodal sentiment analysis, which includes both image and text data, presents several challenges due to the dissimilarities in the modalities of text and image, the ambiguity of sentiment, and the complexities of contextual meaning. In this work, we experiment with finding the sentiments of image and text data, individually and in combination, on two datasets. Part of the approach introduces the novel `Textual-Cues for Enhancing Multimodal Sentiment Analysis' (TEMSA) based on object recognition methods to address the difficulties in multimodal sentiment analysis. Specifically, we extract the names of all objects detected in an image and combine them with associated text; we call this combination of text and image data TEMS. Our results demonstrate that only TEMS improves the results when considering all the object names for the overall sentiment of multimodal data compared to individual analysis. This research contributes to advancing multimodal sentiment analysis and offers insights into the efficacy of TEMSA in combining image and text data for multimodal sentiment analysis.

</details>


### [1125] [Federated-inspired Single-cell Batch Integration in Latent Space](https://arxiv.org/abs/2602.00423)
*Quang-Huy Nguyen,Zongliang Yue,Hao Chen,Wei-Shinn Ku,Jiaqi Wang*

Main category: cs.LG

Relevance: 35.0

TL;DR: scBatchProx：一种基于联邦学习思想的单细胞RNA测序数据批次效应校正方法，通过优化批次特定的适配器参数，在潜在空间中直接校正批次结构，无需原始表达数据或集中式优化。


<details>
  <summary>Details</summary>
Motivation: 单细胞RNA测序产生大量高维数据，但跨实验的数据积累会引入批次效应，掩盖真实生物信号。现有批次校正方法要么校正不足，要么需要在完整数据集上集中重新训练，限制了它们在分布式和持续演化的单细胞数据环境中的应用。

Method: 受联邦学习启发，将每个批次视为客户端，在近端正则化下学习批次条件适配器，直接在潜在空间中校正批次结构，无需原始表达数据或集中式优化。该方法轻量级且可部署，仅优化批次特定的适配器参数。

Result: 实验表明scBatchProx在整体嵌入质量上持续获得约3-8%的相对提升，批次校正和生物保守性分别在90%和85%的数据-方法对中得到改善。

Conclusion: 这项工作朝着在动态单细胞数据系统中实际优化学习表示迈出了一步，为分布式和持续演化的单细胞数据分析提供了实用的批次校正解决方案。

Abstract: Advances in single-cell RNA sequencing enable the rapid generation of massive, high-dimensional datasets, yet the accumulation of data across experiments introduces batch effects that obscure true biological signals. Existing batch correction approaches either insufficiently correct batch effects or require centralized retraining on the complete dataset, limiting their applicability in distributed and continually evolving single-cell data settings. We introduce scBatchProx, a post-hoc optimization method inspired by federated learning principles for refining cell-level embeddings produced by arbitrary upstream methods. Treating each batch as a client, scBatchProx learns batch-conditioned adapters under proximal regularization, correcting batch structure directly in latent space without requiring raw expression data or centralized optimization. The method is lightweight and deployable, optimizing batch-specific adapter parameters only. Extensive experiments show that scBatchProx consistently yields relative gains of approximately 3-8% in overall embedding quality, with batch correction and biological conservation improving in 90% and 85% of data-method pairs, respectively. We envision this work as a step toward the practical refinement of learned representations in dynamic single-cell data systems.

</details>


### [1126] [Stabilizing Decentralized Federated Fine-Tuning via Topology-Aware Alternating LoRA](https://arxiv.org/abs/2602.00451)
*Xiaoyu Wang,Xiaotian Li,Zhixiang Zhou,Chen Li,Yong Liu*

Main category: cs.LG

Relevance: 35.0

TL;DR: 提出了TAD-LoRA框架，用于解决去中心化联邦学习中LoRA参数高效微调的拓扑依赖交叉项问题，通过协调LoRA因子更新和混合来控制客户端间错位


<details>
  <summary>Details</summary>
Motivation: 去中心化联邦学习（DFL）作为无服务器联邦学习变体，在参数高效微调方面面临独特挑战。与线性参数不同，LoRA更新的去中心化聚合会引入拓扑依赖的交叉项，这些项在动态通信图下可能破坏训练稳定性

Method: 提出TAD-LoRA（拓扑感知去中心化低秩适应）框架，协调LoRA因子的更新和混合以控制客户端间错位。理论上证明了在非凸目标下的收敛性，明确描述了拓扑诱导交叉项误差与块坐标表示偏差之间的权衡关系

Result: 在各种通信条件下的实验验证了分析，TAD-LoRA在不同通信场景下实现稳健性能，在强连接拓扑中保持竞争力，在中度和弱连接拓扑下提供明显增益，在MNLI数据集上表现尤为突出

Conclusion: TAD-LoRA有效解决了去中心化联邦学习中LoRA微调的拓扑依赖问题，通过理论分析和实验验证展示了其在不同通信条件下的稳健性和性能优势

Abstract: Decentralized federated learning (DFL), a serverless variant of federated learning, poses unique challenges for parameter-efficient fine-tuning due to the factorized structure of low-rank adaptation (LoRA). Unlike linear parameters, decentralized aggregation of LoRA updates introduces topology-dependent cross terms that can destabilize training under dynamic communication graphs. We propose \texttt{TAD-LoRA}, a Topology-Aware Decentralized Low-Rank Adaptation framework that coordinates the updates and mixing of LoRA factors to control inter-client misalignment. We theoretically prove the convergence of \texttt{TAD-LoRA} under non-convex objectives, explicitly characterizing the trade-off between topology-induced cross-term error and block-coordinate representation bias governed by the switching interval of alternative training. Experiments under various communication conditions validate our analysis, showing that \texttt{TAD-LoRA} achieves robust performance across different communication scenarios, remaining competitive in strongly connected topologies and delivering clear gains under moderately and weakly connected topologies, with particularly strong results on the MNLI dataset.

</details>


### [1127] [PAIR-Former: Budgeted Relational MIL for miRNA Target Prediction](https://arxiv.org/abs/2602.00465)
*Jiaqi Yin,Baiming Chen,Jia Fei,Mingjun Yang*

Main category: cs.LG

Relevance: 35.0

TL;DR: PAIR-Former是一个用于miRNA-mRNA靶向预测的预算感知多实例学习框架，通过廉价的全池扫描和多样化实例选择，在计算预算限制下实现高性能预测。


<details>
  <summary>Details</summary>
Motivation: miRNA-mRNA靶向预测是一个典型的大袋预测问题：每个转录本产生大量候选靶位点，但只有配对级标签可用。现有方法难以在计算预算限制下有效处理这种关系结构。

Method: 提出BR-MIL（预算关系多实例学习）框架，包含廉价全池扫描、CPU上的多样化实例选择（最多K个），以及使用置换不变Set Transformer聚合器处理选定标记。

Result: 在miRAW数据集上，PAIR-Former在实用操作预算（K*=64）下优于强池化基线，并提供可控制的精度-计算权衡。理论分析表明选择误差随K减小，泛化项由K控制。

Conclusion: PAIR-Former为大规模关系多实例学习问题提供了有效的预算感知解决方案，在生物信息学领域具有应用价值，其框架可扩展到其他类似问题。

Abstract: Functional miRNA--mRNA targeting is a large-bag prediction problem: each transcript yields a heavy-tailed pool of candidate target sites (CTSs), yet only a pair-level label is observed. We formalize this regime as \emph{Budgeted Relational Multi-Instance Learning (BR-MIL)}, where at most $K$ instances per bag may receive expensive encoding and relational processing under a hard compute budget. We propose \textbf{PAIR-Former} (Pool-Aware Instance-Relational Transformer), a BR-MIL pipeline that performs a cheap full-pool scan, selects up to $K$ diverse CTSs on CPU, and applies a permutation-invariant Set Transformer aggregator on the selected tokens. On miRAW, PAIR-Former outperforms strong pooling baselines at a practical operating budget ($K^\star{=}64$) while providing a controllable accuracy--compute trade-off as $K$ varies. We further provide theory linking budgeted selection to (i) approximation error decreasing with $K$ and (ii) generalization terms governed by $K$ in the expensive relational component.

</details>


### [1128] [Quality-Diversity Optimization as Multi-Objective Optimization](https://arxiv.org/abs/2602.00478)
*Xi Lin,Ping Guo,Yilu Liu,Qingfu Zhang,Jianyong Sun*

Main category: cs.LG

Relevance: 35.0

TL;DR: 该论文将质量-多样性（QD）优化重新表述为具有大量目标的多目标优化（MOO）问题，使得可以直接应用成熟的MOO方法（特别是基于集合的标量化技术）来解决QD问题。


<details>
  <summary>Details</summary>
Motivation: QD优化旨在发现既高性能又具有多样行为特征的解决方案集合，在机器人控制、创意设计和对抗样本生成等领域有广泛应用。虽然已有多种QD算法，但作者希望建立QD与MOO之间的理论联系，从而利用成熟的MOO方法来解决QD问题。

Method: 将QD优化重新表述为具有大量优化目标（每个行为空间区域对应一个目标）的MOO问题。然后应用基于集合的标量化技术，通过协作搜索过程解决该MOO问题。该方法继承了MOO的理论保证，同时为QD优化提供了理想特性。

Result: 在多个QD应用上的实验研究表明，该方法与最先进的QD算法相比具有竞争力。理论分析表明该方法继承了MOO的理论保证，同时为QD优化提供了理想特性。

Conclusion: 通过将QD优化重新表述为MOO问题，可以直接应用成熟的MOO方法解决QD问题，这为QD优化提供了新的理论框架和实用工具，在多个应用领域表现出竞争力。

Abstract: The Quality-Diversity (QD) optimization aims to discover a collection of high-performing solutions that simultaneously exhibit diverse behaviors within a user-defined behavior space. This paradigm has stimulated significant research interest and demonstrated practical utility in domains including robot control, creative design, and adversarial sample generation. A variety of QD algorithms with distinct design principles have been proposed in recent years. Instead of proposing a new QD algorithm, this work introduces a novel reformulation by casting the QD optimization as a multi-objective optimization (MOO) problem with a huge number of optimization objectives. By establishing this connection, we enable the direct adoption of well-established MOO methods, particularly set-based scalarization techniques, to solve QD problems through a collaborative search process. We further provide a theoretical analysis demonstrating that our approach inherits theoretical guarantees from MOO while providing desirable properties for the QD optimization. Experimental studies across several QD applications confirm that our method achieves performance competitive with state-of-the-art QD algorithms.

</details>


### [1129] [OD-DEAL: Dynamic Expert-Guided Adversarial Learning with Online Decomposition for Scalable Capacitated Vehicle Routing](https://arxiv.org/abs/2602.00488)
*Dongbin Jiao,Zisheng Chen,Xianyi Wang,Jintao Shi,Shengcai Liu,Shi Yan*

Main category: cs.LG

Relevance: 35.0

TL;DR: OD-DEAL是一个对抗学习框架，结合混合遗传搜索和在线重心聚类分解，通过知识蒸馏将专家启发式行为转移到图注意力网络生成策略中，实现大规模车辆路径问题的高质量实时求解。


<details>
  <summary>Details</summary>
Motivation: 大规模带容量车辆路径问题（CVRP）面临两大挑战：传统启发式算法复杂度高，神经网络求解器在大规模图上泛化能力有限。需要开发能够在大规模实例上实现高质量实时推理的解决方案。

Method: 提出OD-DEAL对抗学习框架：1）整合混合遗传搜索（HGS）和在线重心聚类（BCC）分解；2）通过高保真知识蒸馏将专家启发式行为转移到图注意力网络（GAT）生成策略；3）通过极小极大博弈训练，将分治策略蒸馏为密集代理奖励；4）实现无需聚类的大规模实例推理。

Result: OD-DEAL实现了最先进的实时CVRP性能，能够求解10000节点实例，并表现出接近恒定的神经缩放特性。实现了亚秒级、启发式质量的推理，满足动态大规模部署需求。

Conclusion: OD-DEAL通过对抗学习和知识蒸馏，成功将专家启发式行为转移到神经网络求解器中，解决了大规模CVRP的实时求解难题，为动态大规模部署提供了实用解决方案。

Abstract: Solving large-scale capacitated vehicle routing problems (CVRP) is hindered by the high complexity of heuristics and the limited generalization of neural solvers on massive graphs. We propose OD-DEAL, an adversarial learning framework that tightly integrates hybrid genetic search (HGS) and online barycenter clustering (BCC) decomposition, and leverages high-fidelity knowledge distillation to transfer expert heuristic behavior. OD-DEAL trains a graph attention network (GAT)-based generative policy through a minimax game, in which divide-and-conquer strategies from a hybrid expert are distilled into dense surrogate rewards. This enables high-quality, clustering-free inference on large-scale instances. Empirical results demonstrate that OD-DEAL achieves state-of-the-art (SOTA) real-time CVRP performance, solving 10000-node instances with near-constant neural scaling. This uniquely enables the sub-second, heuristic-quality inference required for dynamic large-scale deployment.

</details>


### [1130] [Physiology as Language: Translating Respiration to Sleep EEG](https://arxiv.org/abs/2602.00526)
*Kaiwen Zha,Chao Li,Hao He,Peng Cao,Tianhong Li,Ali Mirzazadeh,Ellen Zhang,Jong Woo Lee,Yoon Kim,Dina Katabi*

Main category: cs.LG

Relevance: 35.0

TL;DR: 提出跨生理信号翻译新任务：从呼吸信号合成睡眠脑电图(EEG)，使用波形条件生成框架，通过离散化token约束EEG目标空间，在28,000+个体数据上训练，在EEG频谱重建中达到7% MAE，合成EEG在下游任务中表现接近真实EEG。


<details>
  <summary>Details</summary>
Motivation: 解决不同生理信号模态间的显著复杂性差距，探索从简单呼吸信号合成复杂脑电图的可能性，为远程、非接触式神经评估提供新途径。

Method: 提出波形条件生成框架，保留细粒度呼吸动态，通过离散token化约束EEG目标空间，使用大规模数据集（28,000+个体）训练生成模型。

Result: EEG频谱重建MAE为7%，合成EEG在下游任务表现接近真实EEG：年龄估计（MAE 5.0 vs 5.1年）、性别检测（AUROC 0.81 vs 0.82）、睡眠分期（准确率0.84 vs 0.88），显著优于直接在呼吸信号上训练的基线。

Conclusion: 该框架成功实现跨生理信号翻译，合成EEG可用于多种下游任务，并扩展到无线射频反射的非接触式传感，展示了远程睡眠神经评估的可行性。

Abstract: This paper introduces a novel cross-physiology translation task: synthesizing sleep electroencephalography (EEG) from respiration signals. To address the significant complexity gap between the two modalities, we propose a waveform-conditional generative framework that preserves fine-grained respiratory dynamics while constraining the EEG target space through discrete tokenization. Trained on over 28,000 individuals, our model achieves a 7% Mean Absolute Error in EEG spectrogram reconstruction. Beyond reconstruction, the synthesized EEG supports downstream tasks with performance comparable to ground truth EEG on age estimation (MAE 5.0 vs. 5.1 years), sex detection (AUROC 0.81 vs. 0.82), and sleep staging (Accuracy 0.84 vs. 0.88), significantly outperforming baselines trained directly on breathing. Finally, we demonstrate that the framework generalizes to contactless sensing by synthesizing EEG from wireless radio-frequency reflections, highlighting the feasibility of remote, non-contact neurological assessment during sleep.

</details>


### [1131] [Contrastive Domain Generalization for Cross-Instrument Molecular Identification in Mass Spectrometry](https://arxiv.org/abs/2602.00547)
*Seunghyun Yoo,Sanghong Kim,Namkyung Yoon,Hwangnam Kim*

Main category: cs.LG

Relevance: 35.0

TL;DR: 提出跨模态对齐框架，将质谱数据映射到预训练化学语言模型的分子结构嵌入空间，解决分子识别中的泛化瓶颈问题


<details>
  <summary>Details</summary>
Motivation: 现有深度学习方法将质谱匹配视为封闭集识别任务，限制了其对未见分子骨架的泛化能力。质谱峰值与化学结构之间存在语义鸿沟，需要解决这一泛化瓶颈

Method: 提出跨模态对齐框架，直接将质谱数据映射到预训练化学语言模型的分子结构嵌入空间，实现物理光谱分辨率与分子结构嵌入的显式集成

Result: 在严格骨架不相交基准测试中，模型在固定256路零样本检索中达到42.2%的Top-1准确率，在全局检索设置下表现出强泛化能力，学习到的嵌入空间具有强化学一致性，在5路5样本分子重识别中达到95.4%准确率

Conclusion: 显式集成物理光谱分辨率与分子结构嵌入是解决质谱数据分子识别泛化瓶颈的关键，跨模态对齐框架为分子识别提供了有效的泛化解决方案

Abstract: Identifying molecules from mass spectrometry (MS) data remains a fundamental challenge due to the semantic gap between physical spectral peaks and underlying chemical structures. Existing deep learning approaches often treat spectral matching as a closed-set recognition task, limiting their ability to generalize to unseen molecular scaffolds. To overcome this limitation, we propose a cross-modal alignment framework that directly maps mass spectra into the chemically meaningful molecular structure embedding space of a pretrained chemical language model. On a strict scaffold-disjoint benchmark, our model achieves a Top-1 accuracy of 42.2% in fixed 256-way zero-shot retrieval and demonstrates strong generalization under a global retrieval setting. Moreover, the learned embedding space demonstrates strong chemical coherence, reaching 95.4% accuracy in 5-way 5-shot molecular re-identification. These results suggest that explicitly integrating physical spectral resolution with molecular structure embedding is key to solving the generalization bottleneck in molecular identification from MS data.

</details>


### [1132] [Forget by Uncertainty: Orthogonal Entropy Unlearning for Quantized Neural Networks](https://arxiv.org/abs/2602.00567)
*Tian Zhang,Yujia Tong,Junhao Dong,Ke Xu,Yuze Wang,Jingling Yuan*

Main category: cs.LG

Relevance: 35.0

TL;DR: 提出OEU（正交熵遗忘）框架，用于量化神经网络的机器遗忘，通过最大化遗忘数据的预测不确定性和梯度正交投影，实现真正的遗忘而非错误记忆，同时保持模型性能。


<details>
  <summary>Details</summary>
Motivation: 量化神经网络在边缘设备上的部署与GDPR等隐私法规相结合，产生了对量化模型进行机器遗忘的迫切需求。现有方法存在关键挑战：通过训练模型记忆错误标签来诱导遗忘，混淆了遗忘与错误记忆，并采用标量梯度重加权无法解决梯度间的方向冲突。

Method: 提出OEU框架，包含两个关键创新：1）熵引导的遗忘：最大化遗忘数据的预测不确定性，实现真正的遗忘而非自信的错误预测；2）梯度正交投影：通过将遗忘梯度投影到保留梯度的正交补空间上，消除干扰，在一阶近似下为效用保持提供理论保证。

Result: 大量实验表明，OEU在遗忘效果和保留准确性方面均优于现有方法。

Conclusion: OEU框架有效解决了量化神经网络中的机器遗忘问题，通过熵引导和梯度正交投影实现了真正的遗忘同时保持了模型性能，为隐私合规的边缘AI部署提供了实用解决方案。

Abstract: The deployment of quantized neural networks on edge devices, combined with privacy regulations like GDPR, creates an urgent need for machine unlearning in quantized models. However, existing methods face critical challenges: they induce forgetting by training models to memorize incorrect labels, conflating forgetting with misremembering, and employ scalar gradient reweighting that cannot resolve directional conflicts between gradients. We propose OEU, a novel Orthogonal Entropy Unlearning framework with two key innovations: 1) Entropy-guided unlearning maximizes prediction uncertainty on forgotten data, achieving genuine forgetting rather than confident misprediction, and 2) Gradient orthogonal projection eliminates interference by projecting forgetting gradients onto the orthogonal complement of retain gradients, providing theoretical guarantees for utility preservation under first-order approximation. Extensive experiments demonstrate that OEU outperforms existing methods in both forgetting effectiveness and retain accuracy.

</details>


### [1133] [SEER: Transformer-based Robust Time Series Forecasting via Automated Patch Enhancement and Replacement](https://arxiv.org/abs/2602.00589)
*Xiangfei Qiu,Xvyuan Liu,Tianen Shen,Xingjian Wu,Hanyin Cheng,Bin Yang,Jilin Hu*

Main category: cs.LG

Relevance: 35.0

TL;DR: SEER提出了一种鲁棒的时间序列预测框架，通过可学习的补丁替换模块动态过滤低质量补丁，并使用增强嵌入模块改进表示学习。


<details>
  <summary>Details</summary>
Motivation: 现有基于补丁的时间序列预测方法通常使用所有补丁进行预测，无法动态选择补丁。现实世界时间序列数据常存在缺失值、分布偏移、异常和噪声等低质量问题，这些低质量补丁会负面影响预测结果。

Method: 1. 增强嵌入模块：使用混合专家架构改进补丁级表示，通过通道自适应感知机制获得序列级标记表示。2. 可学习补丁替换模块：采用两阶段过程：动态过滤机制消除负面的补丁级标记；替换注意力模块用全局序列级标记替代低质量补丁，并通过因果注意力机制进一步精化表示。

Result: 全面的实验结果表明SEER达到了最先进的性能。

Conclusion: SEER框架通过动态过滤和替换低质量补丁，有效提升了时间序列预测的鲁棒性和准确性。

Abstract: Time series forecasting is important in many fields that require accurate predictions for decision-making. Patching techniques, commonly used and effective in time series modeling, help capture temporal dependencies by dividing the data into patches. However, existing patch-based methods fail to dynamically select patches and typically use all patches during the prediction process. In real-world time series, there are often low-quality issues during data collection, such as missing values, distribution shifts, anomalies and white noise, which may cause some patches to contain low-quality information, negatively impacting the prediction results. To address this issue, this study proposes a robust time series forecasting framework called SEER. Firstly, we propose an Augmented Embedding Module, which improves patch-wise representations using a Mixture-of-Experts (MoE) architecture and obtains series-wise token representations through a channel-adaptive perception mechanism. Secondly, we introduce a Learnable Patch Replacement Module, which enhances forecasting robustness and model accuracy through a two-stage process: 1) a dynamic filtering mechanism eliminates negative patch-wise tokens; 2) a replaced attention module substitutes the identified low-quality patches with global series-wise token, further refining their representations through a causal attention mechanism. Comprehensive experimental results demonstrate the SOTA performance of SEER.

</details>


### [1134] [Kernelized Edge Attention: Addressing Semantic Attention Blurring in Temporal Graph Neural Networks](https://arxiv.org/abs/2602.00596)
*Govind Waghmare,Srini Rohan Gujulla Leel,Nikhil Tumbde,Sumedh B G,Sonia Gupta,Srikanta Bedathur*

Main category: cs.LG

Relevance: 35.0

TL;DR: KEAT提出了一种新的注意力机制，通过连续时间核函数调制边特征，解决了TGNN中节点和边特征纠缠导致的语义注意力模糊问题，显著提升了时序图预测性能。


<details>
  <summary>Details</summary>
Motivation: 现有时序图神经网络在处理节点和边特征时存在语义注意力模糊问题。节点嵌入缓慢演化（聚合长期结构上下文），而边特征反映瞬时的带时间戳交互（如消息、交易），两者具有不同的时序行为。现有模型将节点和边表示纠缠在一起计算注意力，无法区分缓慢漂移的节点状态和快速变化的信息丰富的边交互，导致模型难以捕捉细粒度时序依赖且缺乏透明度。

Method: KEAT（Kernelized Edge Attention for Temporal Graphs）使用连续时间核函数族（包括拉普拉斯核、RBF核和可学习的MLP变体）调制边特征。该方法保留了节点和边的不同角色，可无缝集成到Transformer风格（如DyGFormer）和消息传递（如TGN）架构中。

Result: 在链接预测任务上，KEAT相比最近的DyGFormer实现了高达18%的MRR提升，相比TGN实现了7%的提升，实现了更准确、可解释和时序感知的消息传递。

Conclusion: KEAT通过核函数调制边特征，有效解决了时序图中节点和边特征纠缠的问题，显著提升了时序图神经网络的性能、准确性和可解释性。

Abstract: Temporal Graph Neural Networks (TGNNs) aim to capture the evolving structure and timing of interactions in dynamic graphs. Although many models incorporate time through encodings or architectural design, they often compute attention over entangled node and edge representations, failing to reflect their distinct temporal behaviors. Node embeddings evolve slowly as they aggregate long-term structural context, while edge features reflect transient, timestamped interactions (e.g. messages, trades, or transactions). This mismatch results in semantic attention blurring, where attention weights cannot distinguish between slowly drifting node states and rapidly changing, information-rich edge interactions. As a result, models struggle to capture fine-grained temporal dependencies and provide limited transparency into how temporal relevance is computed. This paper introduces KEAT (Kernelized Edge Attention for Temporal Graphs), a novel attention formulation that modulates edge features using a family of continuous-time kernels, including Laplacian, RBF, and learnable MLP variant. KEAT preserves the distinct roles of nodes and edges, and integrates seamlessly with both Transformer-style (e.g., DyGFormer) and message-passing (e.g., TGN) architectures. It achieves up to 18% MRR improvement over the recent DyGFormer and 7% over TGN on link prediction tasks, enabling more accurate, interpretable and temporally aware message passing in TGNNs.

</details>


### [1135] [Actor-Dual-Critic Dynamics for Zero-sum and Identical-Interest Stochastic Games](https://arxiv.org/abs/2602.00606)
*Ahmed Said Donmez,Yuksel Arslantas,Muhammed O. Sayin*

Main category: cs.LG

Relevance: 35.0

TL;DR: 提出了一种用于随机博弈的新型独立、基于收益的学习框架，该框架无需模型、与游戏无关且无需梯度。采用最佳响应型演员-评论家架构，通过快速评论家和慢速评论家更新策略，在非均衡适应中收敛到近似均衡。


<details>
  <summary>Details</summary>
Motivation: 现有随机博弈学习算法通常需要模型知识、梯度信息或集中式协调。本文旨在开发一种完全去中心化、基于收益的学习框架，能够在零和博弈和共同利益博弈中实现理论保证的收敛。

Method: 提出基于收益的学习框架，采用最佳响应型演员-评论家架构：演员更新策略，快速评论家对有限信息下的观察收益做出直觉响应，慢速评论家审慎近似底层动态规划问题的解。学习过程通过平滑最佳响应实现非均衡适应。

Result: 在无限时域的双智能体零和博弈和多智能体共同利益随机博弈中建立了收敛到（近似）均衡的理论保证。这是首个在两种设置下都具有理论保证的完全去中心化、基于收益的学习算法之一。实证结果验证了该方法的鲁棒性和有效性。

Conclusion: 提出了一种新颖的独立、基于收益的学习框架，能够在随机博弈中实现去中心化学习，并在零和博弈和共同利益博弈中提供理论收敛保证，为多智能体强化学习提供了新的解决方案。

Abstract: We propose a novel independent and payoff-based learning framework for stochastic games that is model-free, game-agnostic, and gradient-free. The learning dynamics follow a best-response-type actor-critic architecture, where agents update their strategies (actors) using feedback from two distinct critics: a fast critic that intuitively responds to observed payoffs under limited information, and a slow critic that deliberatively approximates the solution to the underlying dynamic programming problem. Crucially, the learning process relies on non-equilibrium adaptation through smoothed best responses to observed payoffs. We establish convergence to (approximate) equilibria in two-agent zero-sum and multi-agent identical-interest stochastic games over an infinite horizon. This provides one of the first payoff-based and fully decentralized learning algorithms with theoretical guarantees in both settings. Empirical results further validate the robustness and effectiveness of the proposed approach across both classes of games.

</details>


### [1136] [MoDEx: Mixture of Depth-specific Experts for Multivariate Long-term Time Series Forecasting](https://arxiv.org/abs/2602.00624)
*Hyekyung Yoon,Minhyuk Lee,Imseung Park,Myungjoo Kang*

Main category: cs.LG

Relevance: 35.0

TL;DR: MoDEx：一种轻量级深度特定专家混合模型，用于多元长期时间序列预测，通过层敏感性分析揭示深度特定时间动态建模，在多个基准测试中达到SOTA性能。


<details>
  <summary>Details</summary>
Motivation: 现有长期时间序列预测（LTSF）采用嵌入、骨干网络精炼和长期预测的三阶段流程，但各骨干层的具体行为未被充分探索。作者希望理解不同深度层如何建模时间动态，并基于此设计更高效的架构。

Method: 提出层敏感性（layer sensitivity）度量，受GradCAM和有效感受野理论启发，量化每个时间点对层潜在特征的正负贡献。基于三层MLP骨干的分析发现深度特定时间动态建模模式，进而提出MoDEx（Mixture of Depth-specific Experts），用深度特定MLP专家替代复杂骨干网络。

Result: 在7个真实世界基准测试中达到最先进精度，在78%的情况下排名第一，同时使用显著更少的参数和计算资源。可无缝集成到Transformer变体中，持续提升其性能，展示了作为高效高性能LTSF框架的鲁棒泛化能力。

Conclusion: 层敏感性分析揭示了深度特定时间动态建模模式，MoDEx基于此设计实现了高效高性能的长期时间序列预测，为时间序列建模提供了新的架构设计思路。

Abstract: Multivariate long-term time series forecasting (LTSF) supports critical applications such as traffic-flow management, solar-power scheduling, and electricity-transformer monitoring. The existing LTSF paradigms follow a three-stage pipeline of embedding, backbone refinement, and long-horizon prediction. However, the behaviors of individual backbone layers remain underexplored. We introduce layer sensitivity, a gradient-based metric inspired by GradCAM and effective receptive field theory, which quantifies both positive and negative contributions of each time point to a layer's latent features. Applying this metric to a three-layer MLP backbone reveals depth-specific specialization in modeling temporal dynamics in the input sequence. Motivated by these insights, we propose MoDEx, a lightweight Mixture of Depth-specific Experts, which replaces complex backbones with depth-specific MLP experts. MoDEx achieves state-of-the-art accuracy on seven real-world benchmarks, ranking first in 78 percent of cases, while using significantly fewer parameters and computational resources. It also integrates seamlessly into transformer variants, consistently boosting their performance and demonstrating robust generalizability as an efficient and high-performance LTSF framework.

</details>


### [1137] [Combinatorial Bandit Bayesian Optimization for Tensor Outputs](https://arxiv.org/abs/2602.00640)
*Jingru Huang,Haijie Xu,Jie Guo,Manrui Jiang,Chen Zhang*

Main category: cs.LG

Relevance: 35.0

TL;DR: 提出了一种面向张量输出函数的贝叶斯优化方法，包括张量输出高斯过程和组合老虎机贝叶斯优化框架


<details>
  <summary>Details</summary>
Motivation: 现有贝叶斯优化方法主要针对标量输出函数，无法处理张量输出函数。实际应用中存在大量张量输出的优化问题，需要开发专门的方法来捕捉张量内部的结构依赖关系。

Method: 1. 提出张量输出高斯过程作为代理模型，包含两类张量输出核函数；2. 基于上置信界获取函数选择查询点；3. 提出组合老虎机贝叶斯优化框架，处理部分输出可观测场景；4. 设计组合多臂老虎机-UCB2准则同时选择查询点和最优输出子集。

Result: 建立了两种方法的理论遗憾界，证明了次线性性能。在合成和真实世界实验中展示了方法的优越性。

Conclusion: 成功填补了贝叶斯优化在张量输出函数领域的空白，提出的方法能够有效处理张量结构依赖，并在组合优化场景中表现出色。

Abstract: Bayesian optimization (BO) has been widely used to optimize expensive and black-box functions across various domains. Existing BO methods have not addressed tensor-output functions. To fill this gap, we propose a novel tensor-output BO method. Specifically, we first introduce a tensor-output Gaussian process (TOGP) with two classes of tensor-output kernels as a surrogate model of the tensor-output function, which can effectively capture the structural dependencies within the tensor. Based on it, we develop an upper confidence bound (UCB) acquisition function to select the queried points. Furthermore, we introduce a more complex and practical problem setting, named combinatorial bandit Bayesian optimization (CBBO), where only a subset of the outputs can be selected to contribute to the objective function. To tackle this, we propose a tensor-output CBBO method, which extends TOGP to handle partially observed outputs, and accordingly design a novel combinatorial multi-arm bandit-UCB2 (CMAB-UCB2) criterion to sequentially select both the queried points and the optimal output subset. Theoretical regret bounds for the two methods are established, ensuring their sublinear performance. Extensive synthetic and real-world experiments demonstrate their superiority.

</details>


### [1138] [CoRe-Fed: Bridging Collaborative and Representation Fairness via Federated Embedding Distillation](https://arxiv.org/abs/2602.00647)
*Noorain Mukhtiar,Adnan Mahmood,Quan Z. Sheng*

Main category: cs.LG

Relevance: 35.0

TL;DR: CoRe-Fed是一个联邦学习公平性优化框架，通过嵌入对齐和动态聚合解决表示偏差和协作偏差问题


<details>
  <summary>Details</summary>
Motivation: 联邦学习中存在数据分布异构和参与不平等导致的性能差异问题，具体表现为表示偏差（客户端表示不对齐）和协作偏差（聚合贡献不平等），这两种偏差会降低模型性能和泛化能力

Method: 提出CoRe-Fed统一优化框架：1）嵌入对齐机制促进本地和全局嵌入的语义一致性以减少表示偏差；2）基于动态奖励惩罚的聚合策略，根据参与历史和嵌入对齐调整客户端权重，实现贡献感知的聚合

Result: 在多种模型和数据集上的广泛实验表明，CoRe-Fed在公平性和模型性能方面均优于现有基线算法

Conclusion: CoRe-Fed通过桥接协作公平性和表示公平性，有效缓解了联邦学习中的公平性问题，提高了模型的整体性能

Abstract: With the proliferation of distributed data sources, Federated Learning (FL) has emerged as a key approach to enable collaborative intelligence through decentralized model training while preserving data privacy. However, conventional FL algorithms often suffer from performance disparities across clients caused by heterogeneous data distributions and unequal participation, which leads to unfair outcomes. Specifically, we focus on two core fairness challenges, i.e., representation bias, arising from misaligned client representations, and collaborative bias, stemming from inequitable contribution during aggregation, both of which degrade model performance and generalizability. To mitigate these disparities, we propose CoRe-Fed, a unified optimization framework that bridges collaborative and representation fairness via embedding-level regularization and fairness-aware aggregation. Initially, an alignment-driven mechanism promotes semantic consistency between local and global embeddings to reduce representational divergence. Subsequently, a dynamic reward-penalty-based aggregation strategy adjusts each client's weight based on participation history and embedding alignment to ensure contribution-aware aggregation. Extensive experiments across diverse models and datasets demonstrate that CoRe-Fed improves both fairness and model performance over the state-of-the-art baseline algorithms.

</details>


### [1139] [Riemannian Flow Matching for Disentangled Graph Domain Adaptation](https://arxiv.org/abs/2602.00656)
*Yingxu Wang,Xinwang Liu,Mengzhu Wang,Siyang Gao,Nan Yin*

Main category: cs.LG

Relevance: 35.0

TL;DR: DisRFM：一种几何感知的图域自适应框架，通过黎曼流形嵌入和基于流的传输解决结构退化和优化不稳定问题


<details>
  <summary>Details</summary>
Motivation: 传统图域自适应方法在欧几里得空间中使用对抗学习对齐图嵌入，面临两个关键挑战：1）结构退化：层次和语义表示纠缠；2）优化不稳定：极小极大对抗训练的振荡动态

Method: 1）将图嵌入黎曼流形，使用极坐标显式解耦结构（半径）和语义（角度）；2）通过径向Wasserstein对齐保持拓扑结构，通过角度聚类实现语义区分；3）使用黎曼流匹配学习平滑向量场，引导源特征沿测地线路径向目标移动

Result: 大量实验表明DisRFM持续优于最先进方法，理论证明了流匹配的渐近稳定性并推导了更紧的目标风险界限

Conclusion: DisRFM通过几何感知方法有效解决了图域自适应中的结构退化和优化不稳定问题，为图表示学习提供了新的理论保证

Abstract: Graph Domain Adaptation (GDA) typically uses adversarial learning to align graph embeddings in Euclidean space. However, this paradigm suffers from two critical challenges: Structural Degeneration, where hierarchical and semantic representations are entangled, and Optimization Instability, which arises from oscillatory dynamics of minimax adversarial training. To tackle these issues, we propose DisRFM, a geometry-aware GDA framework that unifies Riemannian embedding and flow-based transport. First, to overcome structural degeneration, we embed graphs into a Riemannian manifold. By adopting polar coordinates, we explicitly disentangle structure (radius) from semantics (angle). Then, we enforce topology preservation through radial Wasserstein alignment and semantic discrimination via angular clustering, thereby preventing feature entanglement and collapse. Second, we address the instability of adversarial alignment by using Riemannian flow matching. This method learns a smooth vector field to guide source features toward the target along geodesic paths, guaranteeing stable convergence. The geometric constraints further guide the flow to maintain the disentangled structure during transport. Theoretically, we prove the asymptotic stability of the flow matching and derive a tighter bound for the target risk. Extensive experiments demonstrate that DisRFM consistently outperforms state-of-the-art methods.

</details>


### [1140] [Topology and Geometry of the Learning Space of ReLU Networks: Connectivity and Singularities](https://arxiv.org/abs/2602.00693)
*Marco Nurisso,Pierrick Leroy,Giovanni Petri,Francesco Vaccarino*

Main category: cs.LG

Relevance: 35.0

TL;DR: 本文研究了前馈ReLU网络参数空间的拓扑性质，重点关注基于有向无环图架构的网络参数空间的连通性和奇异性，揭示了这些性质与网络拓扑结构的内在联系。


<details>
  <summary>Details</summary>
Motivation: 理解前馈ReLU网络参数空间的性质对于分析和指导训练动态至关重要。梯度流训练将参数空间限制在由ReLU激活函数的齐次性产生的代数簇上，需要深入研究该空间的拓扑特性。

Method: 基于一般有向无环图架构，分析前馈ReLU网络的参数空间连通性和奇异性。通过瓶颈节点和平衡条件等概念，系统表征连通性，并探讨奇异性与网络拓扑结构的关系。

Result: 发现参数空间的连通性取决于瓶颈节点和特定子网络的平衡条件，奇异性与底层DAG拓扑及其诱导子网络密切相关。建立了奇异性可达性与可微分剪枝之间的理论联系。

Conclusion: 前馈ReLU网络的参数空间拓扑性质（连通性和奇异性）与网络架构的拓扑结构有深刻联系，这为理解训练动态和网络优化提供了理论基础。

Abstract: Understanding the properties of the parameter space in feed-forward ReLU networks is critical for effectively analyzing and guiding training dynamics. After initialization, training under gradient flow decisively restricts the parameter space to an algebraic variety that emerges from the homogeneous nature of the ReLU activation function. In this study, we examine two key challenges associated with feed-forward ReLU networks built on general directed acyclic graph (DAG) architectures: the (dis)connectedness of the parameter space and the existence of singularities within it. We extend previous results by providing a thorough characterization of connectedness, highlighting the roles of bottleneck nodes and balance conditions associated with specific subsets of the network. Our findings clearly demonstrate that singularities are intricately connected to the topology of the underlying DAG and its induced sub-networks. We discuss the reachability of these singularities and establish a principled connection with differentiable pruning. We validate our theory with simple numerical experiments.

</details>


### [1141] [Deep Time-series Forecasting Needs Kernelized Moment Balancing](https://arxiv.org/abs/2602.00717)
*Licheng Pan,Hao Wang,Haocheng Yang,Yuqi Li,Qingsong Wen,Xiaoxi Li,Zhichao Chen,Haoxuan Li,Zhixuan Chu,Yuan Lu*

Main category: cs.LG

Relevance: 35.0

TL;DR: 本文提出KMB-DF方法，通过核化矩平衡解决深度时间序列预测中的分布平衡问题，相比现有方法能更全面地匹配预测与真实数据的分布。


<details>
  <summary>Details</summary>
Motivation: 现有深度时间序列预测方法将问题视为分布平衡问题，但现有目标函数仅匹配一两个预定义的平衡函数，无法满足Imbens准则要求的完全分布平衡，导致预测分布与真实分布对齐不足。

Method: 提出核化矩平衡直接预测(KMB-DF)，从再生核希尔伯特空间(RKHS)自适应选择最具信息量的平衡函数，实现充分的分布平衡。推导出可处理的、可微的目标函数，支持从经验样本高效估计并集成到基于梯度的训练流程中。

Result: 在多个模型和数据集上的广泛实验表明，KMB-DF能持续提升预测精度，达到最先进的性能水平。

Conclusion: KMB-DF通过核化矩平衡方法有效解决了深度时间序列预测中的分布平衡问题，相比现有方法能实现更全面的分布对齐，显著提升预测性能。

Abstract: Deep time-series forecasting can be formulated as a distribution balancing problem aimed at aligning the distribution of the forecasts and ground truths. According to Imbens' criterion, true distribution balance requires matching the first moments with respect to any balancing function. We demonstrate that existing objectives fail to meet this criterion, as they enforce moment matching only for one or two predefined balancing functions, thus failing to achieve full distribution balance. To address this limitation, we propose direct forecasting with kernelized moment balancing (KMB-DF). Unlike existing objectives, KMB-DF adaptively selects the most informative balancing functions from a reproducing kernel hilbert space (RKHS) to enforce sufficient distribution balancing. We derive a tractable and differentiable objective that enables efficient estimation from empirical samples and seamless integration into gradient-based training pipelines. Extensive experiments across multiple models and datasets show that KMB-DF consistently improves forecasting accuracy and achieves state-of-the-art performance. Code is available at https://anonymous.4open.science/r/KMB-DF-403C.

</details>


### [1142] [Provable Model Provenance Set for Large Language Models](https://arxiv.org/abs/2602.00772)
*Xiaoqi Qiu,Hao Zeng,Zhiyu Hou,Hongxin Wei*

Main category: cs.LG

Relevance: 35.0

TL;DR: 提出Model Provenance Set (MPS)方法，通过序列化测试和排除过程构建具有可证明保证的小型模型来源集合，解决现有启发式指纹匹配方法缺乏可证明错误控制的问题。


<details>
  <summary>Details</summary>
Motivation: 随着未经授权模型使用和错误归因的增加，需要可靠的模型来源分析。现有方法主要依赖启发式指纹匹配规则，缺乏可证明的错误控制，且常忽略多来源存在，导致来源声明的可靠性无法验证。

Method: 首先形式化具有可证明保证的模型来源问题，要求以规定置信水平严格覆盖所有真实来源。然后提出Model Provenance Set (MPS)，采用序列化测试和排除过程自适应构建满足保证的小型集合。核心思想是在候选池中测试来源存在的显著性，从而建立用户特定置信水平下的可证明渐近保证。

Result: 大量实验表明，MPS能有效实现目标来源覆盖，同时严格限制无关模型的包含，并展示了其在归因和审计任务中实际来源分析的潜力。

Conclusion: MPS为模型来源问题提供了具有可证明保证的解决方案，相比现有启发式方法具有更好的可靠性和实用性。

Abstract: The growing prevalence of unauthorized model usage and misattribution has increased the need for reliable model provenance analysis. However, existing methods largely rely on heuristic fingerprint-matching rules that lack provable error control and often overlook the existence of multiple sources, leaving the reliability of their provenance claims unverified. In this work, we first formalize the model provenance problem with provable guarantees, requiring rigorous coverage of all true provenances at a prescribed confidence level. Then, we propose the Model Provenance Set (MPS), which employs a sequential test-and-exclusion procedure to adaptively construct a small set satisfying the guarantee. The key idea of MPS is to test the significance of provenance existence within a candidate pool, thereby establishing a provable asymptotic guarantee at a user-specific confidence level. Extensive experiments demonstrate that MPS effectively achieves target provenance coverage while strictly limiting the inclusion of unrelated models, and further reveal its potential for practical provenance analysis in attribution and auditing tasks.

</details>


### [1143] [Stable Time Series Prediction of Enterprise Carbon Emissions Based on Causal Inference](https://arxiv.org/abs/2602.00775)
*Zitao Hong,Zhen Peng,Xueping Liu*

Main category: cs.LG

Relevance: 35.0

TL;DR: 提出了一种针对企业碳排放预测的稳定时间序列预测机制，结合因果推断、稳定学习和时间序列建模，解决跨区域、跨企业的数据分布漂移问题。


<details>
  <summary>Details</summary>
Motivation: 在碳达峰和碳中和目标背景下，准确预测企业碳排放趋势对能源结构优化和低碳转型决策至关重要。然而，不同地区、行业和企业在能源结构、生产规模、政策强度等方面存在显著异质性，导致碳排放数据在时空维度上出现明显的分布漂移和非平稳性，这影响了预测模型的准确性和对生产规划、碳配额交易决策的指导价值。

Method: 整合因果推断视角、稳定学习方法和时间序列建模，提出面向分布漂移环境的稳定时间预测机制。该方法包含企业级能源投入、资本投资、劳动力配置、碳定价、政府干预和政策实施强度等变量，构建风险一致性约束的稳定学习框架，从不同政策、地区和工业部门的多环境样本中提取因果稳定特征。通过自适应归一化和样本重加权策略，动态校正经济波动和政策转型引起的时间非平稳性。

Result: 该方法增强了模型在复杂环境中的泛化能力和可解释性，能够提取对外部扰动具有鲁棒性且对二氧化碳排放具有长期稳定影响的因果稳定特征。

Conclusion: 提出的稳定时间预测机制为解决碳排放预测中的分布漂移问题提供了有效方案，通过因果推断和稳定学习的结合，提高了预测模型在异质环境中的可靠性和实用性。

Abstract: Against the backdrop of ongoing carbon peaking and carbon neutrality goals, accurate prediction of enterprise carbon emission trends constitutes an essential foundation for energy structure optimization and low-carbon transformation decision-making. Nevertheless, significant heterogeneity persists across regions, industries and individual enterprises regarding energy structure, production scale, policy intensity and governance efficacy, resulting in pronounced distribution shifts and non-stationarity in carbon emission data across both temporal and spatial dimensions. Such cross-regional and cross-enterprise data drift not only compromises the accuracy of carbon emission reporting but substantially undermines the guidance value of predictive models for production planning and carbon quota trading decisions. To address this critical challenge, we integrate causal inference perspectives with stable learning methodologies and time-series modelling, proposing a stable temporal prediction mechanism tailored to distribution shift environments. This mechanism incorporates enterprise-level energy inputs, capital investment, labour deployment, carbon pricing, governmental interventions and policy implementation intensity, constructing a risk consistency-constrained stable learning framework that extracts causal stable features (robust against external perturbations yet demonstrating long-term stable effects on carbon dioxide emissions) from multi-environment samples across diverse policies, regions and industrial sectors. Furthermore, through adaptive normalization and sample reweighting strategies, the approach dynamically rectifies temporal non-stationarity induced by economic fluctuations and policy transitions, ultimately enhancing model generalization capability and explainability in complex environments.

</details>


### [1144] [Fast Non-Episodic Finite-Horizon RL with K-Step Lookahead Thresholding](https://arxiv.org/abs/2602.00781)
*Jiamin Xu,Kyra Gan*

Main category: cs.LG

Relevance: 35.0

TL;DR: 提出一种新的在线强化学习方法，通过K步前瞻Q函数和阈值机制在有限时域MDP中实现高效学习，在理论和实验上均优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 在线强化学习在非片段式有限时域MDP中研究不足，现有无限时域方法依赖折扣收缩，不适用于固定时域结构。需要设计能有效估计固定终止时间回报的方法。

Method: 引入改进的K步前瞻Q函数，将规划截断到未来K步；采用阈值机制，仅当估计的K步前瞻价值超过时变阈值时才选择动作；提出高效的表格学习算法，随时间自适应增加K以平衡前瞻深度和估计方差。

Result: 理论证明：K=1时达到极小极大最优常数遗憾，K≥2时遗憾为O(max((K-1),C_{K-1})√(SAT log T))。实验在JumpRiverswim、FrozenLake和AnyTrading等环境中优于现有表格RL方法。

Conclusion: K步前瞻Q函数和阈值机制在有限时域MDP中有效，实现了理论保证和实际性能的平衡，为在线强化学习提供了新思路。

Abstract: Online reinforcement learning in non-episodic, finite-horizon MDPs remains underexplored and is challenged by the need to estimate returns to a fixed terminal time. Existing infinite-horizon methods, which often rely on discounted contraction, do not naturally account for this fixed-horizon structure. We introduce a modified Q-function: rather than targeting the full-horizon, we learn a K-step lookahead Q-function that truncates planning to the next K steps. To further improve sample efficiency, we introduce a thresholding mechanism: actions are selected only when their estimated K-step lookahead value exceeds a time-varying threshold. We provide an efficient tabular learning algorithm for this novel objective, proving it achieves fast finite-sample convergence: it achieves minimax optimal constant regret for $K=1$ and $\mathcal{O}(\max((K-1),C_{K-1})\sqrt{SAT\log(T)})$ regret for any $K \geq 2$. We numerically evaluate the performance of our algorithm under the objective of maximizing reward. Our implementation adaptively increases K over time, balancing lookahead depth against estimation variance. Empirical results demonstrate superior cumulative rewards over state-of-the-art tabular RL methods across synthetic MDPs and RL environments: JumpRiverswim, FrozenLake and AnyTrading.

</details>


### [1145] [Multi-Objective Multi-Fidelity Bayesian Optimization with Causal Priors](https://arxiv.org/abs/2602.00788)
*Md Abir Hossen,Mohammad Ali Javidian,Vignesh Narayanan,Jason M. O'Kane,Pooyan Jamshidi*

Main category: cs.LG

Relevance: 35.0

TL;DR: RESCUE是一种多保真度贝叶斯优化方法，通过结合因果计算来学习输入、保真度和目标之间的因果关系，从而提高样本效率


<details>
  <summary>Details</summary>
Motivation: 现有的多保真度贝叶斯优化方法主要捕捉输入、保真度和目标之间的关联依赖关系，而不是因果机制，当低保真度代理与目标保真度对齐不佳时性能较差

Method: RESCUE学习一个结构因果模型来捕捉输入、保真度和目标之间的因果关系，构建概率多保真度代理模型来编码干预效应，并引入因果超体积知识梯度采集策略来选择输入-保真度对

Result: 在机器人、机器学习（AutoML）和医疗保健等领域的合成和实际问题上，RESCUE比最先进的多保真度优化方法提高了样本效率

Conclusion: 通过结合因果计算，RESCUE能够更有效地平衡低保真度代理的成本效益和准确性，从而加速寻找全局最优解

Abstract: Multi-fidelity Bayesian optimization (MFBO) accelerates the search for the global optimum of black-box functions by integrating inexpensive, low-fidelity approximations. The central task of an MFBO policy is to balance the cost-efficiency of low-fidelity proxies against their reduced accuracy to ensure effective progression toward the high-fidelity optimum. Existing MFBO methods primarily capture associational dependencies between inputs, fidelities, and objectives, rather than causal mechanisms, and can perform poorly when lower-fidelity proxies are poorly aligned with the target fidelity. We propose RESCUE (REducing Sampling cost with Causal Understanding and Estimation), a multi-objective MFBO method that incorporates causal calculus to systematically address this challenge. RESCUE learns a structural causal model capturing causal relationships between inputs, fidelities, and objectives, and uses it to construct a probabilistic multi-fidelity (MF) surrogate that encodes intervention effects. Exploiting the causal structure, we introduce a causal hypervolume knowledge-gradient acquisition strategy to select input-fidelity pairs that balance expected multi-objective improvement and cost. We show that RESCUE improves sample efficiency over state-of-the-art MF optimization methods on synthetic and real-world problems in robotics, machine learning (AutoML), and healthcare.

</details>


### [1146] [Sporadic Gradient Tracking over Directed Graphs: A Theoretical Perspective on Decentralized Federated Learning](https://arxiv.org/abs/2602.00791)
*Shahryar Zehtabi,Dong-Jun Han,Seyyedali Hosseinalipour,Christopher Brinton*

Main category: cs.LG

Relevance: 35.0

TL;DR: 提出Spod-GT算法，首次在去中心化联邦学习中同时解决数据异质性和资源多样性问题，支持客户特定的梯度计算频率和异构不对称通信频率。


<details>
  <summary>Details</summary>
Motivation: 去中心化联邦学习(DFL)面临两大挑战：数据异质性导致收敛困难，以及客户资源多样性（计算和通信能力不同）。现有工作分别解决了这两个问题，但缺乏统一框架。

Method: 提出Sporadic Gradient Tracking (Spod-GT)算法，在一般有向图上同时支持：(1) 客户特定的梯度计算频率，(2) 异构且不对称的通信频率。算法结合梯度跟踪技术缓解数据异质性，并考虑资源多样性。

Result: 在图像分类数据集上的实验表明，Spod-GT相比现有梯度跟踪基线方法表现更优。理论分析提供了在有向图上间歇性客户参与情况下的共识和最优性保证。

Conclusion: Spod-GT是首个在一般有向图上统一解决DFL中数据异质性和资源多样性挑战的算法，具有实际部署价值。

Abstract: Decentralized Federated Learning (DFL) enables clients with local data to collaborate in a peer-to-peer manner to train a generalized model. In this paper, we unify two branches of work that have separately solved important challenges in DFL: (i) gradient tracking techniques for mitigating data heterogeneity and (ii) accounting for diverse availability of resources across clients. We propose $\textit{Sporadic Gradient Tracking}$ ($\texttt{Spod-GT}$), the first DFL algorithm that incorporates these factors over general directed graphs by allowing (i) client-specific gradient computation frequencies and (ii) heterogeneous and asymmetric communication frequencies. We conduct a rigorous convergence analysis of our methodology with relaxed assumptions on gradient estimation variance and gradient diversity of clients, providing consensus and optimality guarantees for GT over directed graphs despite intermittent client participation. Through numerical experiments on image classification datasets, we demonstrate the efficacy of $\texttt{Spod-GT}$ compared to well-known GT baselines.

</details>


### [1147] [Don't Forget Its Variance! The Minimum Path Variance Principle for Accurate and Stable Score-Based Density Ratio Estimation](https://arxiv.org/abs/2602.00834)
*Wei Chen,Jiacheng Li,Shigui Li,Zhiqi Lin,Junmei Yang,John Paisley,Delu Zeng*

Main category: cs.LG

Relevance: 35.0

TL;DR: 论文提出MinPV原则解决基于分数方法的密度比估计中的路径依赖问题，通过最小化路径方差改进估计器性能


<details>
  <summary>Details</summary>
Motivation: 基于分数方法在密度比估计中存在一个重要悖论：理论上路径独立，但实际性能严重依赖于选择的路径调度。现有方法忽略了时间分数路径方差这一关键项，导致训练目标与理想目标存在差异。

Method: 提出MinPV（最小路径方差）原则，推导出路径方差的闭式表达式，将难以处理的问题转化为可优化问题。使用灵活的Kumaraswamy混合模型参数化路径，学习数据自适应、低方差的路径，无需启发式选择。

Result: 该方法在具有挑战性的基准测试中建立了新的最先进结果，产生更准确和稳定的估计器。

Conclusion: 通过最小化被忽视的路径方差，MinPV原则解决了基于分数方法的密度比估计中的路径依赖悖论，提供了更原则化的优化方法。

Abstract: Score-based methods have emerged as a powerful framework for density ratio estimation (DRE), but they face an important paradox in that, while theoretically path-independent, their practical performance depends critically on the chosen path schedule. We resolve this issue by proving that tractable training objectives differ from the ideal, ground-truth objective by a crucial, overlooked term: the path variance of the time score. To address this, we propose MinPV (\textbf{Min}imum \textbf{P}ath \textbf{V}ariance) Principle, which introduces a principled heuristic to minimize the overlooked path variance. Our key contribution is the derivation of a closed-form expression for the variance, turning an intractable problem into a tractable optimization. By parameterizing the path with a flexible Kumaraswamy Mixture Model, our method learns a data-adaptive, low-variance path without heuristic selection. This principled optimization of the complete objective yields more accurate and stable estimators, establishing new state-of-the-art results on challenging benchmarks.

</details>


### [1148] [Towards Multiscale Graph-based Protein Learning with Geometric Secondary Structural Motifs](https://arxiv.org/abs/2602.00862)
*Shih-Hsin Wang,Yuhao Huang,Taos Transue,Justin Baker,Jonathan Forstater,Thomas Strohmer,Bao Wang*

Main category: cs.LG

Relevance: 35.0

TL;DR: 提出了一种用于蛋白质结构学习的高效多尺度图神经网络框架，通过构建包含细粒度子图和粗粒度图的层次化表示，分别捕捉局部二级结构基序和全局空间关系。


<details>
  <summary>Details</summary>
Motivation: 现有基于GNN的蛋白质结构学习方法难以有效学习多尺度表示和建模长程依赖关系，需要更高效的框架来同时捕捉局部相互作用和全局结构关系。

Method: 构建包含细粒度子图（对应二级结构基序）和粗粒度图（连接这些基序）的层次化图表示；使用两个GNN分别学习局部基序内特征和全局基序间关系。

Result: 理论证明该层次化框架保持了最大表达能力，不会丢失关键结构信息；实验表明将基线GNN集成到该多尺度框架中显著提高了预测精度并降低了计算成本。

Conclusion: 提出的多尺度图学习框架为蛋白质结构分析提供了一种高效且表达力强的解决方案，能够更好地捕捉蛋白质的多层次结构特征。

Abstract: Graph neural networks (GNNs) have emerged as powerful tools for learning protein structures by capturing spatial relationships at the residue level. However, existing GNN-based methods often face challenges in learning multiscale representations and modeling long-range dependencies efficiently. In this work, we propose an efficient multiscale graph-based learning framework tailored to proteins. Our proposed framework contains two crucial components: (1) It constructs a hierarchical graph representation comprising a collection of fine-grained subgraphs, each corresponding to a secondary structure motif (e.g., $α$-helices, $β$-strands, loops), and a single coarse-grained graph that connects these motifs based on their spatial arrangement and relative orientation. (2) It employs two GNNs for feature learning: the first operates within individual secondary motifs to capture local interactions, and the second models higher-level structural relationships across motifs. Our modular framework allows a flexible choice of GNN in each stage. Theoretically, we show that our hierarchical framework preserves the desired maximal expressiveness, ensuring no loss of critical structural information. Empirically, we demonstrate that integrating baseline GNNs into our multiscale framework remarkably improves prediction accuracy and reduces computational cost across various benchmarks.

</details>


### [1149] [Improving Flow Matching by Aligning Flow Divergence](https://arxiv.org/abs/2602.00869)
*Yuhao Huang,Taos Transue,Shih-Hsin Wang,William Feldman,Hong Zhang,Bao Wang*

Main category: cs.LG

Relevance: 35.0

TL;DR: 提出了一种改进条件流匹配的方法，通过同时匹配流场及其散度来提升生成模型性能，在多个基准任务上表现优于原始CFM


<details>
  <summary>Details</summary>
Motivation: 条件流匹配（CFM）虽然是训练流基生成模型的高效方法，但在学习概率路径的准确性方面存在不足。作者发现CFM不能完全保证概率路径的精确学习，需要新的理论框架来改进。

Method: 1. 提出了描述学习概率路径与真实概率路径之间误差的偏微分方程表征及其解
2. 证明了两个概率路径之间的总变差上界由CFM损失和关联散度损失共同约束
3. 设计了同时匹配流场及其散度的新目标函数，提升生成模型性能而不牺牲生成效率

Result: 新方法在多个重要基准任务上显著优于原始CFM，包括动力系统生成建模、DNA序列生成和视频生成，性能提升明显且保持生成效率

Conclusion: 通过同时匹配流场及其散度，可以显著改进流基生成模型的训练效果，为条件流匹配提供了更强大的理论框架和实践方法

Abstract: Conditional flow matching (CFM) stands out as an efficient, simulation-free approach for training flow-based generative models, achieving remarkable performance for data generation. However, CFM is insufficient to ensure accuracy in learning probability paths. In this paper, we introduce a new partial differential equation characterization for the error between the learned and exact probability paths, along with its solution. We show that the total variation gap between the two probability paths is bounded above by a combination of the CFM loss and an associated divergence loss. This theoretical insight leads to the design of a new objective function that simultaneously matches the flow and its divergence. Our new approach improves the performance of the flow-based generative model by a noticeable margin without sacrificing generation efficiency. We showcase the advantages of this enhanced training approach over CFM on several important benchmark tasks, including generative modeling for dynamical systems, DNA sequences, and videos. Code is available at \href{https://github.com/Utah-Math-Data-Science/Flow_Div_Matching}{Utah-Math-Data-Science}.

</details>


### [1150] [Test-time Generalization for Physics through Neural Operator Splitting](https://arxiv.org/abs/2602.00884)
*Louis Serrano,Jiequn Han,Edouard Oyallon,Shirley Ho,Rudy Morel*

Main category: cs.LG

Relevance: 35.0

TL;DR: 提出一种神经算子分裂策略，在测试时通过组合训练算子来近似未见过的PDE动态，实现零样本泛化


<details>
  <summary>Details</summary>
Motivation: 神经算子在处理训练分布外的PDE问题时泛化能力有限，现有方法需要新动态的示例进行微调，无法实现真正的零样本泛化

Method: 基于DISCO框架，在测试时不修改预训练权重，通过搜索训练算子的组合来近似未见过的动态，实现神经算子的分裂策略

Result: 在参数外推和新物理现象组合等挑战性任务上，实现了最先进的零样本泛化结果，并能恢复底层PDE参数

Conclusion: 测试时计算是构建灵活、可组合、可泛化神经算子的关键途径

Abstract: Neural operators have shown promise in learning solution maps of partial differential equations (PDEs), but they often struggle to generalize when test inputs lie outside the training distribution, such as novel initial conditions, unseen PDE coefficients or unseen physics. Prior works address this limitation with large-scale multiple physics pretraining followed by fine-tuning, but this still requires examples from the new dynamics, falling short of true zero-shot generalization. In this work, we propose a method to enhance generalization at test time, i.e., without modifying pretrained weights. Building on DISCO, which provides a dictionary of neural operators trained across different dynamics, we introduce a neural operator splitting strategy that, at test time, searches over compositions of training operators to approximate unseen dynamics. On challenging out-of-distribution tasks including parameter extrapolation and novel combinations of physics phenomena, our approach achieves state-of-the-art zero-shot generalization results, while being able to recover the underlying PDE parameters. These results underscore test-time computation as a key avenue for building flexible, compositional, and generalizable neural operators.

</details>


### [1151] [Privacy in Practice: Private COVID-19 Detection in X-Ray Images (Extended Version)](https://arxiv.org/abs/2211.11434)
*Lucas Lange,Maja Schneider,Peter Christen,Erhard Rahm*

Main category: cs.LG

Relevance: 35.0

TL;DR: 该论文研究在COVID-19图像分类任务中应用差分隐私(DP)的实际效果，通过成员推理攻击(MIA)实证评估DP对实际隐私保护的影响，发现DP对MIA防御效果有限，且实际隐私需求因任务而异。


<details>
  <summary>Details</summary>
Motivation: 现有COVID-19隐私保护模型存在数据集小、隐私保证弱或不明确、未研究实际隐私效果等问题。作者旨在改进这些不足，通过更严格的隐私预算评估和实际隐私威胁分析，为医疗图像分类提供更好的隐私保护方案。

Method: 采用差分隐私(DP)技术保护COVID-19图像分类模型，考虑类别不平衡问题，在更严格的隐私预算下评估效用-隐私权衡。通过黑盒成员推理攻击(MIA)实证估计实际隐私泄露情况，分析DP对MIA防御的实际效果。

Result: 研究发现：1) 不同任务的实际隐私需求可能不同；2) 随着DP保证增强，实际隐私泄露仅边际改善，DP对MIA防御效果有限；3) 基于攻击的实证隐私评估有助于调整实际隐私保护。

Conclusion: DP在COVID-19分类任务中对实际MIA防御效果有限，需要根据具体任务的实际威胁调整隐私保护策略。实证攻击特定的隐私评估在调优实际隐私保护中具有重要作用，可实现更好的效用-隐私权衡。

Abstract: Machine learning (ML) can help fight pandemics like COVID-19 by enabling rapid screening of large volumes of images. To perform data analysis while maintaining patient privacy, we create ML models that satisfy Differential Privacy (DP). Previous works exploring private COVID-19 models are in part based on small datasets, provide weaker or unclear privacy guarantees, and do not investigate practical privacy. We suggest improvements to address these open gaps. We account for inherent class imbalances and evaluate the utility-privacy trade-off more extensively and over stricter privacy budgets. Our evaluation is supported by empirically estimating practical privacy through black-box Membership Inference Attacks (MIAs). The introduced DP should help limit leakage threats posed by MIAs, and our practical analysis is the first to test this hypothesis on the COVID-19 classification task. Our results indicate that needed privacy levels might differ based on the task-dependent practical threat from MIAs. The results further suggest that with increasing DP guarantees, empirical privacy leakage only improves marginally, and DP therefore appears to have a limited impact on practical MIA defense. Our findings identify possibilities for better utility-privacy trade-offs, and we believe that empirical attack-specific privacy estimation can play a vital role in tuning for practical privacy.

</details>


### [1152] [Generating Synthetic Health Sensor Data for Privacy-Preserving Wearable Stress Detection](https://arxiv.org/abs/2401.13327)
*Lucas Lange,Nils Wenzlitschke,Erhard Rahm*

Main category: cs.LG

Relevance: 35.0

TL;DR: 该论文提出了一种基于生成对抗网络和差分隐私保护的智能手表健康传感器数据合成方法，用于隐私保护的应力检测研究，在保护患者隐私的同时提高数据可用性。


<details>
  <summary>Details</summary>
Motivation: 智能手表健康传感器数据包含敏感个人信息且获取成本高，需要在保护隐私的前提下为医疗研究提供足够的数据支持。

Method: 采用生成对抗网络结合差分隐私保护机制，合成多传感器智能手表健康读数，并通过多种数据增强策略在真实应力检测任务中验证合成数据的有效性。

Result: 差分隐私训练场景下F1分数提升11.90-15.48%，非隐私训练场景下仍有0.45%提升，但隐私要求增强会显著影响合成数据质量。

Conclusion: 差分隐私合成数据在优化效用-隐私权衡方面具有潜力，特别是在真实训练样本有限的情况下，但需要平衡隐私保护与数据质量。

Abstract: Smartwatch health sensor data are increasingly utilized in smart health applications and patient monitoring, including stress detection. However, such medical data often comprise sensitive personal information and are resource-intensive to acquire for research purposes. In response to this challenge, we introduce the privacy-aware synthetization of multi-sensor smartwatch health readings related to moments of stress, employing Generative Adversarial Networks (GANs) and Differential Privacy (DP) safeguards. Our method not only protects patient information but also enhances data availability for research. To ensure its usefulness, we test synthetic data from multiple GANs and employ different data enhancement strategies on an actual stress detection task. Our GAN-based augmentation methods demonstrate significant improvements in model performance, with private DP training scenarios observing an 11.90-15.48% increase in F1-score, while non-private training scenarios still see a 0.45% boost. These results underline the potential of differentially private synthetic data in optimizing utility-privacy trade-offs, especially with the limited availability of real training samples. Through rigorous quality assessments, we confirm the integrity and plausibility of our synthetic data, which, however, are significantly impacted when increasing privacy requirements.

</details>


### [1153] [GAPNet: Plug-in Jointly Learning Task-Specific Graph for Dynamic Stock Relation](https://arxiv.org/abs/2602.00888)
*Yingjie Niu,Lanxin Lu,Changhong Jin,Ruihai Dong*

Main category: cs.LG

Relevance: 35.0

TL;DR: GAPNet：一种图自适应插件网络，通过联合学习任务特定拓扑和表示来改进金融预测，可动态适应和重连边拓扑，提升盈利能力和稳定性。


<details>
  <summary>Details</summary>
Motivation: 现有金融预测方法依赖预定义图来捕捉股票关系，但网络信号噪声大、异步且难以获取，导致泛化性差且预定义图与下游任务不对齐。需要能够动态适应任务特定关系的解决方案。

Method: 提出GAPNet（图自适应插件网络），可附加到现有成对图或超图骨干模型上，通过空间感知层（捕捉短期资产共动）和时间感知层（在分布偏移下保持长期依赖）联合学习任务特定拓扑和表示。

Result: 在两个真实世界股票数据集上，GAPNet相比最先进模型持续提升盈利能力和稳定性，RT-GCN年化累计收益达0.47，CI-STHPAN达0.63，峰值夏普比率分别为2.20和2.12。

Conclusion: 联合学习图结构和表示对于任务特定关系建模至关重要，GAPNet的即插即用设计确保其广泛适用于各种基于GNN的架构。

Abstract: The advent of the web has led to a paradigm shift in the financial relations, with the real-time dissemination of news, social discourse, and financial filings contributing significantly to the reshaping of financial forecasting. The existing methods rely on establishing relations a priori, i.e. predefining graphs to capture inter-stock relationships. However, the stock-related web signals are characterised by high levels of noise, asynchrony, and challenging to obtain, resulting in poor generalisability and non-alignment between the predefined graphs and the downstream tasks. To address this, we propose GAPNet, a Graph Adaptation Plug-in Network that jointly learns task-specific topology and representations in an end-to-end manner. GAPNet attaches to existing pairwise graph or hypergraph backbone models, enabling the dynamic adaptation and rewiring of edge topologies via two complementary components: a Spatial Perception Layer that captures short-term co-movements across assets, and a Temporal Perception Layer that maintains long-term dependency under distribution shift. Across two real-world stock datasets, GAPNet has been shown to consistently enhance the profitability and stability in comparision to the state-of-the-art models, yielding annualised cumulative returns of up to 0.47 for RT-GCN and 0.63 for CI-STHPAN, with peak Sharpe Ratio of 2.20 and 2.12 respectively. The plug-and-play design of GAPNet ensures its broad applicability to diverse GNN-based architectures. Our results underscore that jointly learning graph structures and representations is essential for task-specific relational modeling.

</details>


### [1154] [Domain-Adaptive and Scalable Dense Retrieval for Content-Based Recommendation](https://arxiv.org/abs/2602.00899)
*Mritunjay Pandey*

Main category: cs.LG

Relevance: 35.0

TL;DR: 该论文提出了一种基于稠密检索的电商推荐系统，使用双塔编码器和对比学习，相比传统关键词匹配显著提升了召回率，同时满足实际部署的延迟和模型大小约束。


<details>
  <summary>Details</summary>
Motivation: 传统电商推荐和搜索依赖稀疏关键词匹配（如BM25），在用户意图与产品元数据词汇不匹配时效果不佳。需要解决词汇不匹配问题，实现基于语义相似度的内容推荐。

Method: 采用双塔编码器架构，在Amazon Reviews 2023（时尚）数据集上使用监督对比学习和多重负样本排序损失进行微调。构建训练对：评论文本作为查询代理，商品元数据作为正文档。使用FAISS HNSW索引和ONNX Runtime推理管道，结合INT8动态量化实现高效服务。

Result: 在826,402个商品目录的评论到标题基准测试中，Recall@10从BM25的0.26提升到0.66。同时满足实际约束：中位CPU推理延迟6.1毫秒（批量大小1），模型大小减少4倍。

Conclusion: 提供了一个端到端、可复现的蓝图，用于将领域适应的稠密检索从离线训练扩展到CPU高效的大规模目录服务，显著优于传统关键词匹配方法。

Abstract: E-commerce recommendation and search commonly rely on sparse keyword matching (e.g., BM25), which breaks down under vocabulary mismatch when user intent has limited lexical overlap with product metadata. We cast content-based recommendation as recommendation-as-retrieval: given a natural-language intent signal (a query or review), retrieve the top-K most relevant items from a large catalog via semantic similarity.
  We present a scalable dense retrieval system based on a two-tower bi-encoder, fine-tuned on the Amazon Reviews 2023 (Fashion) subset using supervised contrastive learning with Multiple Negatives Ranking Loss. We construct training pairs from review text (as a query proxy) and item metadata (as the positive document) and fine-tune on 50,000 sampled interactions with a maximum sequence length of 500 tokens.
  For efficient serving, we combine FAISS HNSW indexing with an ONNX Runtime inference pipeline using INT8 dynamic quantization. On a review-to-title benchmark over 826,402 catalog items, our approach improves Recall@10 from 0.26 (BM25) to 0.66, while meeting practical latency and model-size constraints: 6.1 ms median CPU inference latency (batch size 1) and a 4x reduction in model size.
  Overall, we provide an end-to-end, reproducible blueprint for taking domain-adapted dense retrieval from offline training to CPU-efficient serving at catalog scale.

</details>


### [1155] [Efficient Deep Learning for Medical Imaging: Bridging the Gap Between High-Performance AI and Clinical Deployment](https://arxiv.org/abs/2602.00910)
*Cuong Manh Nguyen,Truong-Son Hy*

Main category: cs.LG

Relevance: 35.0

TL;DR: 这篇综述系统回顾了医疗图像分析中的高效轻量深度学习架构，主要分为CNN、轻量Transformer和线性复杂度模型三类，并探讨了模型压缩策略在保持诊断性能的同时降低硬件需求。


<details>
  <summary>Details</summary>
Motivation: 解决大规模模型在真实临床环境中部署的挑战，包括高计算成本、延迟限制和患者数据隐私问题，推动从云端处理向设备端智能的转变。

Method: 将现代高效模型分为三类：卷积神经网络、轻量Transformer和新兴线性复杂度模型；同时系统分析模型压缩策略（剪枝、量化、知识蒸馏、低秩分解）及其在医疗领域的应用效果。

Result: 提供了医疗领域高效深度学习架构的全面分类和分析框架，评估了不同压缩策略在保持诊断性能的同时降低硬件需求的有效性，为资源受限的临床环境提供了技术路线图。

Conclusion: 高效轻量模型是实现医疗AI在真实临床环境中部署的关键，需要继续研究以平衡性能、效率和隐私保护，推动设备端智能的发展。

Abstract: Deep learning has revolutionized medical image analysis, playing a vital role in modern clinical applications. However, the deployment of large-scale models in real-world clinical settings remains challenging due to high computational costs, latency constraints, and patient data privacy concerns associated with cloud-based processing. To address these bottlenecks, this review provides a comprehensive synthesis of efficient and lightweight deep learning architectures specifically tailored for the medical domain. We categorize the landscape of modern efficient models into three primary streams: Convolutional Neural Networks (CNNs), Lightweight Transformers, and emerging Linear Complexity Models. Furthermore, we examine key model compression strategies (including pruning, quantization, knowledge distillation, and low-rank factorization) and evaluate their efficacy in maintaining diagnostic performance while reducing hardware requirements. By identifying current limitations and discussing the transition toward on-device intelligence, this review serves as a roadmap for researchers and practitioners aiming to bridge the gap between high-performance AI and resource-constrained clinical environments.

</details>


### [1156] [Early Classification of Time Series in Non-Stationary Cost Regimes](https://arxiv.org/abs/2602.00918)
*Aurélien Renault,Alexis Bondu,Antoine Cornuéjols,Vincent Lemaire*

Main category: cs.LG

Relevance: 35.0

TL;DR: 论文研究时间序列早期分类（ECTS）在成本非平稳性下的鲁棒性问题，提出在线学习方法应对成本漂移和随机变化，通过强化学习等策略提升部署时的适应性。


<details>
  <summary>Details</summary>
Motivation: 现有ECTS方法通常假设决策成本固定且已知，但实际应用中成本往往不确定且随时间变化，导致训练目标与部署目标不匹配。论文旨在解决成本非平稳性（成本漂移和随机变化）对ECTS系统的影响。

Method: 将代表性ECTS方法适配到在线学习设置，针对可分离方法仅更新触发模型而保持分类器固定。提出多种在线适应方法和基线，包括基于bandit和强化学习（RL）的方法，在合成数据上进行受控实验。

Result: 实验结果表明，在线学习能有效提升ECTS方法对成本漂移的鲁棒性，其中基于RL的策略在不同成本机制下表现出强大且稳定的性能。

Conclusion: 在线学习是应对ECTS中成本非平稳性的有效方法，RL-based策略在成本漂移环境下具有优越的适应性和鲁棒性，为实际部署中的动态成本变化提供了解决方案。

Abstract: Early Classification of Time Series (ECTS) addresses decision-making problems in which predictions must be made as early as possible while maintaining high accuracy. Most existing ECTS methods assume that the time-dependent decision costs governing the learning objective are known, fixed, and correctly specified. In practice, however, these costs are often uncertain and may change over time, leading to mismatches between training-time and deployment-time objectives. In this paper, we study ECTS under two practically relevant forms of cost non-stationarity: drift in the balance between misclassification and decision delay costs, and stochastic realizations of decision costs that deviate from the nominal training-time model. To address these challenges, we revisit representative ECTS approaches and adapt them to an online learning setting. Focusing on separable methods, we update only the triggering model during deployment, while keeping the classifier fixed. We propose several online adaptations and baselines, including bandit-based and RL-based approaches, and conduct controlled experiments on synthetic data to systematically evaluate robustness under cost non-stationarity. Our results demonstrate that online learning can effectively improve the robustness of ECTS methods to cost drift, with RL-based strategies exhibiting strong and stable performance across varying cost regimes.

</details>


### [1157] [SAGE: Agentic Framework for Interpretable and Clinically Translatable Computational Pathology Biomarker Discovery](https://arxiv.org/abs/2602.00953)
*Sahar Almahfouz Nasser,Juan Francisco Pesantez Borja,Jincheng Liu,Tanvir Hasan,Zenghan Wang,Suman Ghosh,Sandeep Manandhar,Shikhar Shiromani,Twisha Shah,Naoto Tokuyama,Anant Madabhushi*

Main category: cs.LG

Relevance: 35.0

TL;DR: SAGE是一个基于代理的AI系统，用于通过生物证据识别可解释的病理学生物标志物，将图像特征与分子生物标志物和临床结果关联起来。


<details>
  <summary>Details</summary>
Motivation: 当前计算病理学中的AI模型大多是黑盒且难以解释，这阻碍了临床采用。虽然工程化的图像生物标志物更具可解释性，但通常基于轶事证据或零散的文献提出，缺乏系统的生物验证。

Method: SAGE是一个结构化代理系统，整合了文献锚定推理和多模态数据分析，通过协调专门代理进行生物情境化和经验假设验证，将图像特征与基因表达等分子生物标志物及临床结果关联。

Result: SAGE能够识别透明且生物支持的生物标志物，优先考虑可解释的工程化病理学生物标志物，推进计算病理学的临床转化。

Conclusion: SAGE通过代理AI系统将图像特征与生物证据系统关联，解决了计算病理学中可解释性不足的问题，为临床采用提供了透明且生物验证的生物标志物。

Abstract: Despite significant progress in computational pathology, many AI models remain black-box and difficult to interpret, posing a major barrier to clinical adoption due to limited transparency and explainability. This has motivated continued interest in engineered image-based biomarkers, which offer greater interpretability but are often proposed based on anecdotal evidence or fragmented prior literature rather than systematic biological validation. We introduce SAGE (Structured Agentic system for hypothesis Generation and Evaluation), an agentic AI system designed to identify interpretable, engineered pathology biomarkers by grounding them in biological evidence. SAGE integrates literature-anchored reasoning with multimodal data analysis to correlate image-derived features with molecular biomarkers, such as gene expression, and clinically relevant outcomes. By coordinating specialized agents for biological contextualization and empirical hypothesis validation, SAGE prioritizes transparent, biologically supported biomarkers and advances the clinical translation of computational pathology.

</details>


### [1158] [Forest-Guided Semantic Transport for Label-Supervised Manifold Alignment](https://arxiv.org/abs/2602.00974)
*Adrien Aumon,Myriam Lizotte,Guy Wolf,Kevin R. Moon,Jake S. Rhodes*

Main category: cs.LG

Relevance: 35.0

TL;DR: FoSTA是一个利用森林诱导几何进行多模态数据对齐的框架，通过标签信息构建语义表示并进行层次语义传输，在单细胞分析等应用中表现优异


<details>
  <summary>Details</summary>
Motivation: 现有标签监督的流形对齐方法大多依赖欧几里得几何建模域内关系，当特征与任务相关性较弱时会产生噪声和误导性结构，导致对齐质量下降。需要一种能去噪并恢复任务相关流形的方法。

Method: FoSTA框架：1) 利用森林诱导几何去噪域内结构；2) 直接从标签信息的森林亲和度构建语义表示；3) 通过快速层次语义传输对齐这些表示，捕捉跨域有意义的关联关系。

Result: 在合成基准测试中，FoSTA改进了对应关系恢复和标签迁移；在实际单细胞应用中（包括批次校正和生物保守性分析）表现出强大性能，优于现有基线方法。

Conclusion: FoSTA通过森林诱导几何和语义传输，有效解决了传统欧几里得几何在弱相关特征情况下的局限性，为多模态数据对齐提供了更鲁棒和语义相关的解决方案。

Abstract: Label-supervised manifold alignment bridges the gap between unsupervised and correspondence-based paradigms by leveraging shared label information to align multimodal datasets. Still, most existing methods rely on Euclidean geometry to model intra-domain relationships. This approach can fail when features are only weakly related to the task of interest, leading to noisy, semantically misleading structure and degraded alignment quality. To address this limitation, we introduce FoSTA (Forest-guided Semantic Transport Alignment), a scalable alignment framework that leverages forest-induced geometry to denoise intra-domain structure and recover task-relevant manifolds prior to alignment. FoSTA builds semantic representations directly from label-informed forest affinities and aligns them via fast, hierarchical semantic transport, capturing meaningful cross-domain relationships. Extensive comparisons with established baselines demonstrate that FoSTA improves correspondence recovery and label transfer on synthetic benchmarks and delivers strong performance in practical single-cell applications, including batch correction and biological conservation.

</details>


### [1159] [Adaptive Dual-Weighting Framework for Federated Learning via Out-of-Distribution Detection](https://arxiv.org/abs/2602.01039)
*Zhiwei Ling,Hailiang Zhao,Chao Zhang,Xiang Ao,Ziqi Wang,Cheng Zhang,Zhen Qin,Xinkui Zhao,Kingsum Chow,Yuanqing Wu,MengChu Zhou*

Main category: cs.LG

Relevance: 35.0

TL;DR: FLood：一个基于OOD检测的联邦学习框架，通过双重加权机制（客户端损失重加权和服务器聚合加权）来应对非IID数据异质性，提升模型收敛稳定性和泛化能力。


<details>
  <summary>Details</summary>
Motivation: 现实世界联邦学习部署中，来自不同用户、设备和场景的数据具有固有的非IID特性，严重影响了全局模型的收敛稳定性、泛化能力和服务质量。现有方法难以有效应对这种数据异质性挑战。

Method: 提出FLood框架，受OOD检测启发，采用双重加权机制：1）客户端层面：自适应重加权监督损失，对伪OOD样本赋予更高权重，鼓励从分布不一致或挑战性数据中学习；2）服务器层面：根据客户端OOD置信度分数加权聚合，优先考虑具有更高分布一致性的客户端更新。

Result: 在多种非IID设置下的多个基准测试中，FLood在准确率和泛化能力上持续优于最先进的联邦学习方法。此外，FLood可作为正交插件模块，无需修改核心优化逻辑即可提升现有FL算法在异质性下的性能。

Conclusion: FLood是一个实用且可扩展的解决方案，能够为现实世界联邦环境部署可靠的智能服务提供支持，通过OOD检测机制有效应对数据异质性挑战。

Abstract: Federated Learning (FL) enables collaborative model training across large-scale distributed service nodes while preserving data privacy, making it a cornerstone of intelligent service systems in edge-cloud environments. However, in real-world service-oriented deployments, data generated by heterogeneous users, devices, and application scenarios are inherently non-IID. This severe data heterogeneity critically undermines the convergence stability, generalization ability, and ultimately the quality of service delivered by the global model. To address this challenge, we propose FLood, a novel FL framework inspired by out-of-distribution (OOD) detection. FLood dynamically counteracts the adverse effects of heterogeneity through a dual-weighting mechanism that jointly governs local training and global aggregation. At the client level, it adaptively reweights the supervised loss by upweighting pseudo-OOD samples, thereby encouraging more robust learning from distributionally misaligned or challenging data. At the server level, it refines model aggregation by weighting client contributions according to their OOD confidence scores, prioritizing updates from clients with higher in-distribution consistency and enhancing the global model's robustness and convergence stability. Extensive experiments across multiple benchmarks under diverse non-IID settings demonstrate that FLood consistently outperforms state-of-the-art FL methods in both accuracy and generalization. Furthermore, FLood functions as an orthogonal plug-in module: it seamlessly integrates with existing FL algorithms to boost their performance under heterogeneity without modifying their core optimization logic. These properties make FLood a practical and scalable solution for deploying reliable intelligent services in real-world federated environments.

</details>


### [1160] [Key Principles of Graph Machine Learning: Representation, Robustness, and Generalization](https://arxiv.org/abs/2602.01139)
*Yassine Abbahaddou*

Main category: cs.LG

Relevance: 35.0

TL;DR: 该论文针对图神经网络(GNNs)在泛化性、对抗鲁棒性和表示学习能力方面的挑战，提出了基于图移位算子的表示学习技术、图数据增强的泛化提升方法，以及利用正交化和噪声防御的鲁棒性增强方案。


<details>
  <summary>Details</summary>
Motivation: 尽管图神经网络在各种应用中取得了成功，但其在泛化能力、对抗扰动鲁棒性和表示学习效果方面仍存在显著限制。论文旨在通过系统研究解决这些核心挑战，为GNNs提供更原则性的理解和改进方案。

Method: 1) 基于图移位算子(GSOs)开发新的表示学习技术；2) 通过图数据增强引入泛化增强方法；3) 利用正交化技术和基于噪声的防御机制开发更鲁棒的GNNs以对抗对抗攻击。

Result: 论文提出了三个主要贡献：改进了GNNs在不同上下文和应用中的性能表现；增强了模型的泛化能力；提高了GNNs对对抗攻击的鲁棒性。这些方法为理解GNNs的局限性和潜力提供了更原则性的框架。

Conclusion: 通过系统解决GNNs在表示学习、泛化和鲁棒性方面的核心挑战，该研究为图神经网络的发展提供了重要的理论和方法基础，推动了GNNs在结构化数据学习中的进一步应用。

Abstract: Graph Neural Networks (GNNs) have emerged as powerful tools for learning representations from structured data. Despite their growing popularity and success across various applications, GNNs encounter several challenges that limit their performance. in their generalization, robustness to adversarial perturbations, and the effectiveness of their representation learning capabilities. In this dissertation, I investigate these core aspects through three main contributions: (1) developing new representation learning techniques based on Graph Shift Operators (GSOs, aiming for enhanced performance across various contexts and applications, (2) introducing generalization-enhancing methods through graph data augmentation, and (3) developing more robust GNNs by leveraging orthonormalization techniques and noise-based defenses against adversarial attacks. By addressing these challenges, my work provides a more principled understanding of the limitations and potential of GNNs.

</details>


### [1161] [The Gaussian-Head OFL Family: One-Shot Federated Learning from Client Global Statistics](https://arxiv.org/abs/2602.01186)
*Fabio Turazza,Marco Picone,Marco Mamei*

Main category: cs.LG

Relevance: 35.0

TL;DR: GH-OFL：基于高斯假设的单轮联邦学习方法，客户端仅传输统计量，服务器构建分类头，无需公共数据集或额外数据上传


<details>
  <summary>Details</summary>
Motivation: 传统联邦学习存在多轮通信开销大和隐私风险高的问题，而现有单轮联邦学习方法要么不实用（依赖公共数据集），要么受限（假设同质客户端模型或需要上传额外数据）。需要一种更实用、数据无关的单轮联邦学习方案。

Method: 提出GH-OFL方法族，假设预训练嵌入具有类条件高斯性。客户端仅传输统计量（每类计数、一阶/二阶矩），服务器通过三部分构建分类头：1) 闭式高斯头（NB/LDA/QDA）；2) FisherMix：在估计的Fisher子空间中采样合成样本训练带余弦间隔的线性头；3) Proto-Hyper：通过知识蒸馏在合成样本上精炼高斯logits的轻量级低秩残差头。

Result: GH-OFL方法在强非IID偏斜下实现了最先进的鲁棒性和准确性，同时严格保持数据无关性。

Conclusion: GH-OFL提供了一种实用、高效的单轮联邦学习框架，显著降低通信开销和隐私风险，适用于实际部署场景。

Abstract: Classical Federated Learning relies on a multi-round iterative process of model exchange and aggregation between server and clients, with high communication costs and privacy risks from repeated model transmissions. In contrast, one-shot federated learning (OFL) alleviates these limitations by reducing communication to a single round, thereby lowering overhead and enhancing practical deployability. Nevertheless, most existing one-shot approaches remain either impractical or constrained, for example, they often depend on the availability of a public dataset, assume homogeneous client models, or require uploading additional data or model information. To overcome these issues, we introduce the Gaussian-Head OFL (GH-OFL) family, a suite of one-shot federated methods that assume class-conditional Gaussianity of pretrained embeddings. Clients transmit only sufficient statistics (per-class counts and first/second-order moments) and the server builds heads via three components: (i) Closed-form Gaussian heads (NB/LDA/QDA) computed directly from the received statistics; (ii) FisherMix, a linear head with cosine margin trained on synthetic samples drawn in an estimated Fisher subspace; and (iii) Proto-Hyper, a lightweight low-rank residual head that refines Gaussian logits via knowledge distillation on those synthetic samples. In our experiments, GH-OFL methods deliver state-of-the-art robustness and accuracy under strong non-IID skew while remaining strictly data-free.

</details>


### [1162] [PaAno: Patch-Based Representation Learning for Time-Series Anomaly Detection](https://arxiv.org/abs/2602.01359)
*Jinju Park,Seokho Kang*

Main category: cs.LG

Relevance: 35.0

TL;DR: 提出PaAno方法，一种轻量级的时间序列异常检测方法，使用1D卷积神经网络提取时间片段表示，结合三元组损失和预训练损失训练，在TSB-AD基准上达到SOTA性能。


<details>
  <summary>Details</summary>
Motivation: 当前时间序列异常检测研究倾向于使用大型神经网络架构（如Transformer和基础模型），但这些方法计算成本高、内存占用大，不适用于实时和资源受限场景，且性能提升有限。

Method: PaAno方法从时间序列训练数据中提取短时间片段，使用1D卷积神经网络将每个片段嵌入为向量表示。模型通过三元组损失和预训练损失的组合进行训练，确保嵌入能捕捉输入片段的信息性时间模式。推理时，通过比较当前时间步周围片段的嵌入与训练数据中正常片段的嵌入来计算异常分数。

Result: 在TSB-AD基准测试中，PaAno实现了最先进的性能，在单变量和多变量时间序列异常检测的各种范围性和点性性能指标上，显著优于现有方法，包括基于重型架构的方法。

Conclusion: PaAno是一种轻量级但有效的时间序列异常检测方法，在保持高性能的同时显著降低了计算成本和内存使用，适用于实时和资源受限场景。

Abstract: Although recent studies on time-series anomaly detection have increasingly adopted ever-larger neural network architectures such as transformers and foundation models, they incur high computational costs and memory usage, making them impractical for real-time and resource-constrained scenarios. Moreover, they often fail to demonstrate significant performance gains over simpler methods under rigorous evaluation protocols. In this study, we propose Patch-based representation learning for time-series Anomaly detection (PaAno), a lightweight yet effective method for fast and efficient time-series anomaly detection. PaAno extracts short temporal patches from time-series training data and uses a 1D convolutional neural network to embed each patch into a vector representation. The model is trained using a combination of triplet loss and pretext loss to ensure the embeddings capture informative temporal patterns from input patches. During inference, the anomaly score at each time step is computed by comparing the embeddings of its surrounding patches to those of normal patches extracted from the training time-series. Evaluated on the TSB-AD benchmark, PaAno achieved state-of-the-art performance, significantly outperforming existing methods, including those based on heavy architectures, on both univariate and multivariate time-series anomaly detection across various range-wise and point-wise performance measures.

</details>


### [1163] [Semi-supervised CAPP Transformer Learning via Pseudo-labeling](https://arxiv.org/abs/2602.01419)
*Dennis Gross,Helge Spieker,Arnaud Gotlieb,Emmanuel Stathatos,Panorios Benardos,George-Christopher Vosniakos*

Main category: cs.LG

Relevance: 35.0

TL;DR: 提出一种半监督学习方法，用于改进基于Transformer的计算机辅助工艺规划模型，通过训练一个oracle来筛选未标注数据的正确预测，然后进行一次性重训练，在数据稀缺的制造环境中提升模型性能。


<details>
  <summary>Details</summary>
Motivation: 工业中计算机辅助工艺规划（CAPP）面临数据集有限的问题，这限制了模型的泛化能力。由于制造数据的稀缺性和标注成本高，需要一种能够在少量标注数据下提升模型性能的方法。

Method: 提出半监督学习方法：1）在可用的Transformer行为数据上训练一个oracle模型；2）使用该oracle筛选未见过零件的正确预测；3）将这些筛选出的预测作为伪标签进行一次性重训练。该方法旨在利用未标注数据提升模型性能。

Result: 在模拟完整数据分布的小规模数据集上的实验表明，该方法相比基线模型获得了持续的准确率提升，证明了在数据稀缺的制造环境中该方法的有效性。

Conclusion: 提出的半监督学习方法能够有效提升基于Transformer的CAPP模型在数据稀缺环境下的性能，无需人工标注，为制造领域的工艺规划提供了实用的解决方案。

Abstract: High-level Computer-Aided Process Planning (CAPP) generates manufacturing process plans from part specifications. It suffers from limited dataset availability in industry, reducing model generalization. We propose a semi-supervised learning approach to improve transformer-based CAPP transformer models without manual labeling. An oracle, trained on available transformer behaviour data, filters correct predictions from unseen parts, which are then used for one-shot retraining. Experiments on small-scale datasets with simulated ground truth across the full data distribution show consistent accuracy gains over baselines, demonstrating the method's effectiveness in data-scarce manufacturing environments.

</details>


### [1164] [DCD: Decomposition-based Causal Discovery from Autocorrelated and Non-Stationary Temporal Data](https://arxiv.org/abs/2602.01433)
*Muhammad Hasan Ferdous,Md Osman Gani*

Main category: cs.LG

Relevance: 35.0

TL;DR: 提出基于分解的因果发现框架，将多变量时间序列分解为趋势、季节性和残差分量，分别进行因果分析，最后整合为统一的多尺度因果结构。


<details>
  <summary>Details</summary>
Motivation: 金融、气候科学和医疗等领域的时间序列常呈现长期趋势、季节模式和短期波动，在非平稳性和自相关条件下进行因果推断非常复杂。现有方法直接在原始观测数据上操作，容易产生虚假边和错误归因的时间依赖。

Method: 1) 将每个时间序列分解为趋势、季节性和残差分量；2) 对趋势分量使用平稳性检验；3) 对季节性分量使用基于核的依赖度量；4) 对残差分量使用基于约束的因果发现；5) 将分量级图整合为统一的多尺度因果结构。

Result: 在广泛的合成基准测试和真实世界气候数据中，该框架比最先进的基线方法更准确地恢复真实因果结构，特别是在强非平稳性和时间自相关条件下表现更优。

Conclusion: 分解方法能够分离长期和短期因果效应，减少虚假关联，提高可解释性，为复杂时间序列的因果发现提供了有效框架。

Abstract: Multivariate time series in domains such as finance, climate science, and healthcare often exhibit long-term trends, seasonal patterns, and short-term fluctuations, complicating causal inference under non-stationarity and autocorrelation. Existing causal discovery methods typically operate on raw observations, making them vulnerable to spurious edges and misattributed temporal dependencies. We introduce a decomposition-based causal discovery framework that separates each time series into trend, seasonal, and residual components and performs component-specific causal analysis. Trend components are assessed using stationarity tests, seasonal components using kernel-based dependence measures, and residual components using constraint-based causal discovery. The resulting component-level graphs are integrated into a unified multi-scale causal structure. This approach isolates long- and short-range causal effects, reduces spurious associations, and improves interpretability. Across extensive synthetic benchmarks and real-world climate data, our framework more accurately recovers ground-truth causal structure than state-of-the-art baselines, particularly under strong non-stationarity and temporal autocorrelation.

</details>


### [1165] [Phase Transitions for Feature Learning in Neural Networks](https://arxiv.org/abs/2602.01434)
*Andrea Montanari,Zihao Wang*

Main category: cs.LG

Relevance: 35.0

TL;DR: 该论文研究两层神经网络在比例渐近条件下学习多索引模型的梯度下降动态，推导出特征学习的关键阈值δ_NN，揭示了训练过程中梯度主导和Hessian主导两个阶段的转变机制。


<details>
  <summary>Details</summary>
Motivation: 研究神经网络如何从数据中学习低维表示，特别是在多索引模型设置下，理解梯度下降动态中特征学习的相变阈值，以及网络架构和训练算法对学习过程的影响。

Method: 采用比例渐近分析（n,d→∞，n/d→δ），保持潜在空间维度k和隐藏神经元数m固定，研究两层神经网络的梯度下降动态，通过分析经验风险梯度和Hessian矩阵的谱特性来推导特征学习阈值δ_NN。

Result: 推导出两层神经网络特征学习的阈值δ_NN，该阈值对应于Hessian矩阵谱的相变点。训练过程分为两个阶段：首先是梯度主导阶段学习梯度方向，然后是Hessian主导阶段，δ_NN决定了第二阶段中Hessian负方向的动态行为。

Conclusion: 该研究为理解神经网络特征学习的动态过程提供了理论框架，δ_NN阈值的表征为研究网络架构和训练算法对学习动态的依赖关系开辟了道路，揭示了梯度下降训练中梯度主导和Hessian主导阶段的转变机制。

Abstract: According to a popular viewpoint, neural networks learn from data by first identifying low-dimensional representations, and subsequently fitting the best model in this space. Recent works provide a formalization of this phenomenon when learning multi-index models. In this setting, we are given $n$ i.i.d. pairs $({\boldsymbol x}_i,y_i)$, where the covariate vectors ${\boldsymbol x}_i\in\mathbb{R}^d$ are isotropic, and responses $y_i$ only depend on ${\boldsymbol x}_i$ through a $k$-dimensional projection ${\boldsymbol Θ}_*^{\sf T}{\boldsymbol x}_i$. Feature learning amounts to learning the latent space spanned by ${\boldsymbol Θ}_*$.
  In this context, we study the gradient descent dynamics of two-layer neural networks under the proportional asymptotics $n,d\to\infty$, $n/d\toδ$, while the dimension of the latent space $k$ and the number of hidden neurons $m$ are kept fixed. Earlier work establishes that feature learning via polynomial-time algorithms is possible if $δ> δ_{\text{alg}}$, for $δ_{\text{alg}}$ a threshold depending on the data distribution, and is impossible (within a certain class of algorithms) below $δ_{\text{alg}}$. Here we derive an analogous threshold $δ_{\text{NN}}$ for two-layer networks. Our characterization of $δ_{\text{NN}}$ opens the way to study the dependence of learning dynamics on the network architecture and training algorithm.
  The threshold $δ_{\text{NN}}$ is determined by the following scenario. Training first visits points for which the gradient of the empirical risk is large and learns the directions spanned by these gradients. Then the gradient becomes smaller and the dynamics becomes dominated by negative directions of the Hessian. The threshold $δ_{\text{NN}}$ corresponds to a phase transition in the spectrum of the Hessian in this second phase.

</details>


### [1166] [Provable Cooperative Multi-Agent Exploration for Reward-Free MDPs](https://arxiv.org/abs/2602.01453)
*Idan Barnea,Orin Levy,Yishay Mansour*

Main category: cs.LG

Relevance: 35.0

TL;DR: 本文研究多智能体强化学习中的无奖励探索问题，在表格有限时域MDP中分析学习阶段数与智能体数量之间的权衡关系，发现由时域H控制的尖锐转变。


<details>
  <summary>Details</summary>
Motivation: 研究多智能体强化学习在无奖励探索设置下的合作问题，多个智能体共同探索未知MDP以学习其动态（不观察奖励）。关注学习阶段数与智能体数量之间的权衡，特别是在学习阶段数较少时。

Method: 采用分阶段学习框架，每个学习阶段中多个智能体独立与环境交互。每个智能体被分配一个策略，执行该策略并观察结果轨迹。主要分析学习阶段数与智能体数量之间的权衡关系。

Result: 发现由时域H控制的尖锐转变：当学习阶段数等于H时，提出计算高效算法仅需Õ(S⁶H⁶A/ε²)个智能体即可获得动态的ε近似；当阶段数ρ<H时，任何算法至少需要A^{H/ρ}个智能体才能达到恒定精度。

Conclusion: 如果限制智能体数量为多项式级别，则必须有大约H个学习阶段。这揭示了多智能体无奖励探索中阶段数与智能体数量之间的基本权衡。

Abstract: We study cooperative multi-agent reinforcement learning in the setting of reward-free exploration, where multiple agents jointly explore an unknown MDP in order to learn its dynamics (without observing rewards). We focus on a tabular finite-horizon MDP and adopt a phased learning framework. In each learning phase, multiple agents independently interact with the environment. More specifically, in each learning phase, each agent is assigned a policy, executes it, and observes the resulting trajectory. Our primary goal is to characterize the tradeoff between the number of learning phases and the number of agents, especially when the number of learning phases is small.
  Our results identify a sharp transition governed by the horizon $H$. When the number of learning phases equals $H$, we present a computationally efficient algorithm that uses only $\tilde{O}(S^6 H^6 A / ε^2)$ agents to obtain an $ε$ approximation of the dynamics (i.e., yields an $ε$-optimal policy for any reward function). We complement our algorithm with a lower bound showing that any algorithm restricted to $ρ< H$ phases requires at least $A^{H/ρ}$ agents to achieve constant accuracy. Thus, we show that it is essential to have an order of $H$ learning phases if we limit the number of agents to be polynomial.

</details>


### [1167] [Causal Preference Elicitation](https://arxiv.org/abs/2602.01483)
*Edwin V. Bonilla,He Zhao,Daniel M. Steinberg*

Main category: cs.LG

Relevance: 35.0

TL;DR: 提出因果偏好启发框架，通过主动查询专家对局部边关系的判断来集中有向无环图的后验分布，实现专家参与的因果发现。


<details>
  <summary>Details</summary>
Motivation: 传统因果发现方法通常仅依赖观测数据，而专家知识可以显著提高因果结构推断的准确性。然而，如何有效整合专家判断并主动选择最有信息量的查询来加速后验集中是一个挑战。

Method: 提出贝叶斯框架，从任意黑盒观测后验出发，通过三向似然函数建模专家对边存在性和方向的噪声判断。使用灵活的粒子近似进行后验推断，并基于专家分类响应的期望信息增益准则高效选择查询。

Result: 在合成图、蛋白质信号数据和人类基因扰动基准测试中，该方法在有限查询预算下实现了更快的后验集中和更好的有向效应恢复效果。

Conclusion: 因果偏好启发框架有效整合了专家知识，通过主动查询策略显著提高了因果发现效率，为专家参与的因果推断提供了系统化方法。

Abstract: We propose causal preference elicitation, a Bayesian framework for expert-in-the-loop causal discovery that actively queries local edge relations to concentrate a posterior over directed acyclic graphs (DAGs). From any black-box observational posterior, we model noisy expert judgments with a three-way likelihood over edge existence and direction. Posterior inference uses a flexible particle approximation, and queries are selected by an efficient expected information gain criterion on the expert's categorical response. Experiments on synthetic graphs, protein signaling data, and a human gene perturbation benchmark show faster posterior concentration and improved recovery of directed effects under tight query budgets.

</details>


### [1168] [Enhancing Generalization in Evolutionary Feature Construction for Symbolic Regression through Vicinal Jensen Gap Minimization](https://arxiv.org/abs/2602.01510)
*Hengzhe Zhang,Qi Chen,Bing Xue,Wolfgang Banzhaf,Mengjie Zhang*

Main category: cs.LG

Relevance: 35.0

TL;DR: 提出基于遗传编程的特征构造框架，通过优化经验风险和vicinal Jensen gap来控制过拟合，结合噪声估计和流形入侵检测机制，在58个数据集上验证了有效性。


<details>
  <summary>Details</summary>
Motivation: 遗传编程特征构造虽然成功，但过拟合问题限制了其广泛应用。需要改进泛化能力，特别是在不同噪声水平和数据分布下的鲁棒性。

Method: 1) 证明vicinal risk可分解为经验风险和正则项；2) 提出进化特征构造框架，联合优化经验风险和vicinal Jensen gap；3) 开发噪声估计策略动态调整正则强度；4) 引入流形入侵检测机制防止生成不现实样本。

Result: 在58个数据集上，Jensen gap最小化相比其他复杂度度量更有效。与15种机器学习算法比较，提出的过拟合控制策略使遗传编程获得更优性能。

Conclusion: 通过理论分析和实验验证，提出的框架能有效控制遗传编程特征构造中的过拟合问题，提高泛化能力，为自动化机器学习提供新方法。

Abstract: Genetic programming-based feature construction has achieved significant success in recent years as an automated machine learning technique to enhance learning performance. However, overfitting remains a challenge that limits its broader applicability. To improve generalization, we prove that vicinal risk, estimated through noise perturbation or mixup-based data augmentation, is bounded by the sum of empirical risk and a regularization term-either finite difference or the vicinal Jensen gap. Leveraging this decomposition, we propose an evolutionary feature construction framework that jointly optimizes empirical risk and the vicinal Jensen gap to control overfitting. Since datasets may vary in noise levels, we develop a noise estimation strategy to dynamically adjust regularization strength. Furthermore, to mitigate manifold intrusion-where data augmentation may generate unrealistic samples that fall outside the data manifold-we propose a manifold intrusion detection mechanism. Experimental results on 58 datasets demonstrate the effectiveness of Jensen gap minimization compared to other complexity measures. Comparisons with 15 machine learning algorithms further indicate that genetic programming with the proposed overfitting control strategy achieves superior performance.

</details>


### [1169] [White-Box Neural Ensemble for Vehicular Plasticity: Quantifying the Efficiency Cost of Symbolic Auditability in Adaptive NMPC](https://arxiv.org/abs/2602.01516)
*Enzo Nicolas Spotorno,Matheus Wagner,Antonio Augusto Medeiros Frohlich*

Main category: cs.LG

Relevance: 35.0

TL;DR: 提出了一种白盒自适应NMPC架构，通过模块化主权范式仲裁多个冻结的、特定于工况的神经专家，解决车辆可塑性问题，实现无需重新训练即可适应不同工况。架构将集成动态维护为CasADi中的完全可遍历符号图，确保运行时可审计性。


<details>
  <summary>Details</summary>
Motivation: 解决车辆控制系统中的可塑性问题——即系统需要适应不同工况（如摩擦、质量、阻力变化）而无需重新训练。现有非自适应基线方法在复合工况变化下会失效，需要一种既能快速适应又能保持白盒可解释性的解决方案。

Method: 采用模块化主权范式，仲裁多个冻结的、特定于工况的神经专家。将集成动态维护为CasADi中的完全可遍历符号图，实现最大运行时可审计性。通过同步仿真验证快速适应能力。

Result: 系统实现快速适应（约7.3毫秒），在复合工况变化下达到接近理想的跟踪精度，而非自适应基线方法会失效。量化了透明度成本：符号图维护使求解器延迟增加72-102倍，相对于编译的参数化物理模型。

Conclusion: 提出的白盒自适应NMPC架构成功解决了车辆可塑性问题，在保持严格白盒实现的同时实现了快速适应。研究明确了严格白盒实现的效率代价，为可解释AI与控制系统的结合提供了实证基准。

Abstract: We present a white-box adaptive NMPC architecture that resolves vehicular plasticity (adaptation to varying operating regimes without retraining) by arbitrating among frozen, regime-specific neural specialists using a Modular Sovereignty paradigm. The ensemble dynamics are maintained as a fully traversable symbolic graph in CasADi, enabling maximal runtime auditability. Synchronous simulation validates rapid adaptation (~7.3 ms) and near-ideal tracking fidelity under compound regime shifts (friction, mass, drag) where non-adaptive baselines fail. Empirical benchmarking quantifies the transparency cost: symbolic graph maintenance increases solver latency by 72-102X versus compiled parametric physics models, establishing the efficiency price of strict white-box implementation.

</details>


### [1170] [The Inlet Rank Collapse in Implicit Neural Representations: Diagnosis and Unified Remedy](https://arxiv.org/abs/2602.01526)
*Jianqiao Zheng,Hemanth Saratchandran,Simon Lucey*

Main category: cs.LG

Relevance: 35.0

TL;DR: 该论文提出了一个结构诊断框架，通过层间NTK分解识别了"入口秩塌陷"现象，并开发了秩扩展初始化方法，使标准MLP能够实现高保真重建，无需架构修改或计算开销。


<details>
  <summary>Details</summary>
Motivation: 隐式神经表示（INRs）在连续信号建模中表现出色，但在有限训练预算下难以恢复细粒度细节。现有技术（如位置编码、正弦激活、批归一化）的理论解释大多是后验的，缺乏统一的理论框架来解释这些技术为何有效。

Method: 1. 提出结构诊断框架，通过层间NTK分解进行数学分析
2. 识别"入口秩塌陷"现象：低维输入坐标无法跨越高维嵌入空间，导致第一层出现基本秩缺陷
3. 将PE、SIREN和BN重新解释为不同形式的秩恢复方法
4. 开发秩扩展初始化方法，确保表示秩随层宽度扩展

Result: 秩扩展初始化使标准MLP能够实现高保真重建，证明INRs的关键在于初始秩传播的结构优化，以有效填充潜在空间。

Conclusion: 该研究提供了一个统一的理论框架来解释现有INR技术的有效性，并提出了一种简单而有效的初始化方法，为隐式神经表示的结构优化提供了新思路。

Abstract: Implicit Neural Representations (INRs) have revolutionized continuous signal modeling, yet they struggle to recover fine-grained details within finite training budgets. While empirical techniques, such as positional encoding (PE), sinusoidal activations (SIREN), and batch normalization (BN), effectively mitigate this, their theoretical justifications are predominantly post hoc, focusing on the global NTK spectrum only after modifications are applied. In this work, we reverse this paradigm by introducing a structural diagnostic framework. By performing a layer-wise decomposition of the NTK, we mathematically identify the ``Inlet Rank Collapse'': a phenomenon where the low-dimensional input coordinates fail to span the high-dimensional embedding space, creating a fundamental rank deficiency at the first layer that acts as an expressive bottleneck for the entire network. This framework provides a unified perspective to re-interpret PE, SIREN, and BN as different forms of rank restoration. Guided by this diagnosis, we derive a Rank-Expanding Initialization, a minimalist remedy that ensures the representation rank scales with the layer width without architectural modifications or computational overhead. Our results demonstrate that this principled remedy enables standard MLPs to achieve high-fidelity reconstructions, proving that the key to empowering INRs lies in the structural optimization of the initial rank propagation to effectively populate the latent space.

</details>


### [1171] [Local Exponential Stability of Mean-Field Langevin Descent-Ascent in Wasserstein Space](https://arxiv.org/abs/2602.01564)
*Geuntaek Seo,Minseop Shin,Pierre Monmarché,Beomjun Choi*

Main category: cs.LG

Relevance: 35.0

TL;DR: 本文研究了平均场Langevin下降-上升动力学在熵正则化两人零和博弈中的局部稳定性，证明了混合纳什均衡在Wasserstein度量下是局部指数稳定的。


<details>
  <summary>Details</summary>
Motivation: 尽管平均场目标函数存在唯一的混合纳什均衡，但原始MFL-DA动力学在一般非凸-非凹支付函数下的长期行为仍然未知。本文旨在回答Wang和Chizat在COLT 2024中提出的一个开放性问题。

Method: 通过谱分析线性化算子，在均衡点附近建立熵的强制性估计，揭示局部位移凸-凹结构，从而证明动力学收缩性质。

Result: 证明了均衡点是局部指数稳定的：当初始化在Wasserstein度量下足够接近均衡点时，动力学以指数速率收敛到均衡点。

Conclusion: 该结果解决了Wang和Chizat提出的局部稳定性和定量速率问题，但全局收敛仍然是一个未解决的挑战。

Abstract: We study the mean-field Langevin descent-ascent (MFL-DA), a coupled optimization dynamics on the space of probability measures for entropically regularized two-player zero-sum games. Although the associated mean-field objective admits a unique mixed Nash equilibrium, the long-time behavior of the original MFL-DA for general nonconvex-nonconcave payoffs has remained largely open. Answering an open question posed by Wang and Chizat (COLT 2024), we provide a partial resolution by proving that this equilibrium is locally exponentially stable: if the initialization is sufficiently close in Wasserstein metric, the dynamics trends to the equilibrium at an exponential rate. The key to our analysis is to establish a coercivity estimate for the entropy near equilibrium via spectral analysis of the linearized operator. We show that this coercivity effectively reveals a local displacement convex-concave structure, thereby driving contraction. This result settles the local stability and quantitative rate questions of Wang and Chizat, leaving global convergence as a remaining open challenge.

</details>


### [1172] [Spectral Text Fusion: A Frequency-Aware Approach to Multimodal Time-Series Forecasting](https://arxiv.org/abs/2602.01588)
*Huu Hiep Nguyen,Minh Hoang Nguyen,Dung Nguyen,Hung Le*

Main category: cs.LG

Relevance: 35.0

TL;DR: SpecTF是一个用于多模态时间序列预测的框架，通过在频域融合文本和时间序列数据，解决了传统方法中局部对齐与全局文本上下文不匹配的问题。


<details>
  <summary>Details</summary>
Motivation: 现有方法在将文本特征与时间序列模式对齐时，通常采用逐步对齐的方式，忽略了上下文信息（如时间序列周期和动态变化）的多尺度时间影响。这种局部对齐与全局文本上下文之间的不匹配需要通过频域分析来解决。

Method: 提出SpecTF框架：提取文本嵌入，将其投影到频域，然后使用轻量级交叉注意力机制与时间序列的频谱分量融合。该方法根据文本相关性自适应地重新加权频带，然后将结果映射回时域进行预测。

Result: 实验结果表明，SpecTF在多种多模态时间序列数据集上显著优于现有最先进模型，同时使用的参数数量大大减少。

Conclusion: SpecTF通过在频域集成文本数据对时间序列的影响，有效解决了多模态时间序列预测中的上下文对齐问题，提供了一种简单而有效的解决方案。

Abstract: Multimodal time series forecasting is crucial in real-world applications, where decisions depend on both numerical data and contextual signals. The core challenge is to effectively combine temporal numerical patterns with the context embedded in other modalities, such as text. While most existing methods align textual features with time-series patterns one step at a time, they neglect the multiscale temporal influences of contextual information such as time-series cycles and dynamic shifts. This mismatch between local alignment and global textual context can be addressed by spectral decomposition, which separates time series into frequency components capturing both short-term changes and long-term trends. In this paper, we propose SpecTF, a simple yet effective framework that integrates the effect of textual data on time series in the frequency domain. Our method extracts textual embeddings, projects them into the frequency domain, and fuses them with the time series' spectral components using a lightweight cross-attention mechanism. This adaptively reweights frequency bands based on textual relevance before mapping the results back to the temporal domain for predictions. Experimental results demonstrate that SpecTF significantly outperforms state-of-the-art models across diverse multi-modal time series datasets while utilizing considerably fewer parameters. Code is available at https://github.com/hiepnh137/SpecTF.

</details>


### [1173] [AgroFlux: A Spatial-Temporal Benchmark for Carbon and Nitrogen Flux Prediction in Agricultural Ecosystems](https://arxiv.org/abs/2602.01614)
*Qi Cheng,Licheng Liu,Yao Zhang,Mu Hong,Yiqun Xie,Xiaowei Jia*

Main category: cs.LG

Relevance: 35.0

TL;DR: 论文提出了首个时空农业生态系统温室气体基准数据集，整合了物理模型模拟和真实观测数据，评估了多种序列深度学习模型在碳氮通量预测上的性能，并探索了迁移学习提升模型泛化能力的方法。


<details>
  <summary>Details</summary>
Motivation: 农业生态系统对全球气候变化影响重大，但准确量化其碳、养分和水循环通量面临挑战。传统方法如土壤采样、过程模型和黑箱机器学习模型存在数据稀疏、时空异质性强、地下过程复杂等问题。缺乏AI-ready的基准数据集和标准化协议阻碍了可信AI模型的发展。

Method: 1) 创建首个时空农业生态系统温室气体基准数据集，整合Ecosys和DayCent物理模型模拟数据、涡度协方差通量塔观测数据和受控环境设施数据；2) 评估多种序列深度学习模型（LSTM、时序CNN、Transformer）在碳氮通量预测上的性能；3) 探索迁移学习方法，利用模拟数据提升深度学习模型在真实观测数据上的泛化能力。

Result: 建立了首个综合性的农业生态系统温室气体基准数据集，为AI模型开发提供了标准化测试平台。评估了不同深度学习架构在碳氮通量预测任务上的表现，并展示了迁移学习如何利用模拟数据提升模型在真实观测数据上的泛化性能。

Conclusion: 该基准数据集和评估框架有助于开发更准确、可扩展的AI驱动农业生态系统模型，推动对生态系统-气候相互作用的理解，为气候减缓策略提供科学支持。

Abstract: Agroecosystem, which heavily influenced by human actions and accounts for a quarter of global greenhouse gas emissions (GHGs), plays a crucial role in mitigating global climate change and securing environmental sustainability. However, we can't manage what we can't measure. Accurately quantifying the pools and fluxes in the carbon, nutrient, and water nexus of the agroecosystem is therefore essential for understanding the underlying drivers of GHG and developing effective mitigation strategies. Conventional approaches like soil sampling, process-based models, and black-box machine learning models are facing challenges such as data sparsity, high spatiotemporal heterogeneity, and complex subsurface biogeochemical and physical processes. Developing new trustworthy approaches such as AI-empowered models, will require the AI-ready benchmark dataset and outlined protocols, which unfortunately do not exist. In this work, we introduce a first-of-its-kind spatial-temporal agroecosystem GHG benchmark dataset that integrates physics-based model simulations from Ecosys and DayCent with real-world observations from eddy covariance flux towers and controlled-environment facilities. We evaluate the performance of various sequential deep learning models on carbon and nitrogen flux prediction, including LSTM-based models, temporal CNN-based model, and Transformer-based models. Furthermore, we explored transfer learning to leverage simulated data to improve the generalization of deep learning models on real-world observations. Our benchmark dataset and evaluation framework contribute to the development of more accurate and scalable AI-driven agroecosystem models, advancing our understanding of ecosystem-climate interactions.

</details>


### [1174] [De Novo Molecular Generation from Mass Spectra via Many-Body Enhanced Diffusion](https://arxiv.org/abs/2602.01643)
*Xichen Sun,Wentao Wei,Jiahua Rao,Jiancong Xie,Yuedong Yang*

Main category: cs.LG

Relevance: 35.0

TL;DR: MBGen：一种基于多体增强扩散框架的质谱分子结构生成方法，通过多体注意力机制和高阶边建模，显著提升质谱分子结构生成性能


<details>
  <summary>Details</summary>
Motivation: 现有质谱分子结构生成方法主要采用原子中心和成对相互作用建模，忽略了高阶边相互作用，无法系统捕捉多体特征，限制了复杂异构体和非局部碎裂机制的解析能力

Method: 提出MBGen框架，集成多体注意力机制和高阶边建模，利用扩散模型从质谱数据中生成分子结构，能够全面利用MS/MS谱图中的丰富结构信息

Result: 在NPLIB1和MassSpecGym基准测试中，MBGen性能显著优于现有方法，提升幅度高达230%，有效捕捉高阶相互作用，对复杂异构体和非局部碎裂信息表现出增强的敏感性

Conclusion: 多体建模在质谱分子结构生成中具有重要科学价值和实际效用，MBGen框架为从质谱数据生成分子结构提供了有效解决方案

Abstract: Molecular structure generation from mass spectrometry is fundamental for understanding cellular metabolism and discovering novel compounds. Although tandem mass spectrometry (MS/MS) enables the high-throughput acquisition of fragment fingerprints, these spectra often reflect higher-order interactions involving the concerted cleavage of multiple atoms and bonds-crucial for resolving complex isomers and non-local fragmentation mechanisms. However, most existing methods adopt atom-centric and pairwise interaction modeling, overlooking higher-order edge interactions and lacking the capacity to systematically capture essential many-body characteristics for structure generation. To overcome these limitations, we present MBGen, a Many-Body enhanced diffusion framework for de novo molecular structure Generation from mass spectra. By integrating a many-body attention mechanism and higher-order edge modeling, MBGen comprehensively leverages the rich structural information encoded in MS/MS spectra, enabling accurate de novo generation and isomer differentiation for novel molecules. Experimental results on the NPLIB1 and MassSpecGym benchmarks demonstrate that MBGen achieves superior performance, with improvements of up to 230% over state-of-the-art methods, highlighting the scientific value and practical utility of many-body modeling for mass spectrometry-based molecular generation. Further analysis and ablation studies show that our approach effectively captures higher-order interactions and exhibits enhanced sensitivity to complex isomeric and non-local fragmentation information.

</details>


### [1175] [DIA-CLIP: a universal representation learning framework for zero-shot DIA proteomics](https://arxiv.org/abs/2602.01772)
*Yucheng Liao,Han Wen,Weinan E,Weijie Zhang*

Main category: cs.LG

Relevance: 35.0

TL;DR: DIA-CLIP是一个基于预训练模型的蛋白质组学分析工具，通过跨模态表示学习实现零样本肽段-谱图匹配，相比现有方法显著提升了蛋白质鉴定性能。


<details>
  <summary>Details</summary>
Motivation: 当前数据非依赖采集质谱（DIA-MS）分析框架需要每个实验中进行半监督训练来重新评分肽段-谱图匹配（PSM），这种方法容易过拟合且缺乏跨物种和实验条件的泛化能力。

Method: 提出DIA-CLIP预训练模型，采用双编码器对比学习框架与编码器-解码器架构相结合，建立肽段和对应谱图特征的统一跨模态表示，实现高精度、零样本的PSM推断。

Result: 在多个基准测试中，DIA-CLIP始终优于最先进工具，蛋白质鉴定数量增加高达45%，同时诱饵鉴定减少12%。在单细胞和空间蛋白质组学等应用中具有巨大潜力。

Conclusion: DIA-CLIP将DIA分析范式从半监督训练转变为通用跨模态表示学习，为蛋白质组学分析提供了更强大、更通用的解决方案。

Abstract: Data-independent acquisition mass spectrometry (DIA-MS) has established itself as a cornerstone of proteomic profiling and large-scale systems biology, offering unparalleled depth and reproducibility. Current DIA analysis frameworks, however, require semi-supervised training within each run for peptide-spectrum match (PSM) re-scoring. This approach is prone to overfitting and lacks generalizability across diverse species and experimental conditions. Here, we present DIA-CLIP, a pre-trained model shifting the DIA analysis paradigm from semi-supervised training to universal cross-modal representation learning. By integrating dual-encoder contrastive learning framework with encoder-decoder architecture, DIA-CLIP establishes a unified cross-modal representation for peptides and corresponding spectral features, achieving high-precision, zero-shot PSM inference. Extensive evaluations across diverse benchmarks demonstrate that DIA-CLIP consistently outperforms state-of-the-art tools, yielding up to a 45% increase in protein identification while achieving a 12% reduction in entrapment identifications. Moreover, DIA-CLIP holds immense potential for diverse practical applications, such as single-cell and spatial proteomics, where its enhanced identification depth facilitates the discovery of novel biomarkers and the elucidates of intricate cellular mechanisms.

</details>


### [1176] [DOGMA: Weaving Structural Information into Data-centric Single-cell Transcriptomics Analysis](https://arxiv.org/abs/2602.01839)
*Ru Zhang,Xunkai Li,Yaxin Deng,Sicheng Liu,Daohan Su,Qiangqiang Dai,Hongchao Qin,Rong-Hua Li,Guoren Wang,Jia Li*

Main category: cs.LG

Relevance: 35.0

TL;DR: DOGMA是一个数据中心的AI框架，通过整合多层次生物先验知识（统计锚点、细胞本体、系统发育树、基因本体）来重构单细胞转录组数据的结构和语义，实现确定性图结构发现和跨物种对齐，在计算成本显著降低的情况下达到SOTA性能。


<details>
  <summary>Details</summary>
Motivation: 现有单细胞转录组分析方法存在两个主要问题：1）早期序列方法将细胞视为独立实体，忽略了生物系统功能机制驱动的潜在细胞间关系；2）结构化方法虽然尝试捕捉细胞间关系，但依赖启发式规则而忽略了生物先验知识，导致图表示次优且计算开销大。

Method: DOGMA框架通过整合多层次生物先验知识来重构数据：1）使用统计锚点、细胞本体和系统发育树实现确定性图结构发现和跨物种对齐；2）利用基因本体填补特征级语义鸿沟，融入功能先验知识。该方法超越了依赖随机启发式规则的传统方法。

Result: 在复杂的多物种、多器官基准测试中，DOGMA实现了最先进的性能，表现出优异的零样本鲁棒性和样本效率，同时计算成本显著降低。

Conclusion: DOGMA通过系统性地整合生物先验知识，为单细胞转录组分析提供了一个高效的数据中心AI框架，解决了现有方法在捕捉细胞间关系和利用生物知识方面的不足，在性能、鲁棒性和效率方面均有显著提升。

Abstract: Recently, data-centric AI methodology has been a dominant paradigm in single-cell transcriptomics analysis, which treats data representation rather than model complexity as the fundamental bottleneck. In the review of current studies, earlier sequence methods treat cells as independent entities and adapt prevalent ML models to analyze their directly inherited sequence data. Despite their simplicity and intuition, these methods overlook the latent intercellular relationships driven by the functional mechanisms of biological systems and the inherent quality issues of the raw sequence data. Therefore, a series of structured methods has emerged. Although they employ various heuristic rules to capture intricate intercellular relationships and enhance the raw sequencing data, these methods often neglect biological prior knowledge. This omission incurs substantial overhead and yields suboptimal graph representations, thereby hindering the utility of ML models.
  To address them, we propose DOGMA, a holistic data-centric framework designed for the structural reshaping and semantic enhancement of raw data through multi-level biological prior knowledge. Transcending reliance on stochastic heuristics, DOGMA redefines graph construction by integrating Statistical Anchors with Cell Ontology and Phylogenetic Trees to enable deterministic structure discovery and robust cross-species alignment. Furthermore, Gene Ontology is utilized to bridge the feature-level semantic gap by incorporating functional priors. In complex multi-species and multi-organ benchmarks, DOGMA achieves SOTA performance, exhibiting superior zero-shot robustness and sample efficiency while operating with significantly lower computational cost.

</details>


### [1177] [FUPareto: Bridging the Forgetting-Utility Gap in Federated Unlearning via Pareto Augmented Optimization](https://arxiv.org/abs/2602.01852)
*Zeyan Wang,Zhengmao Liu,Yongxin Cai,Chi Li,Xiaoying Tang,Jingchao Chen,Zibin Pan,Jing Qiu*

Main category: cs.LG

Relevance: 35.0

TL;DR: FUPareto：基于帕累托优化的联邦遗忘框架，通过最小边界偏移损失和零空间投影多梯度下降算法，解决联邦遗忘中遗忘与效用冲突、多客户端并发遗忘梯度冲突等问题。


<details>
  <summary>Details</summary>
Motivation: 联邦遗忘旨在从联邦模型中高效移除特定客户端数据的影响，同时保持对其他客户端的效用。现有方法面临三个关键挑战：1）遗忘目标常损害模型效用或增加成员推理攻击风险；2）遗忘与效用存在固有冲突；3）多客户端并发遗忘支持差，梯度冲突降低遗忘质量。

Method: 提出FUPareto框架：1）引入最小边界偏移损失，通过抑制目标类logit低于最高非目标类logit来提升遗忘效率并降低MIA风险；2）采用帕累托改进步骤保持模型效用；3）执行帕累托扩展保证遗忘，其中集成零空间投影多梯度下降算法解耦梯度冲突，实现多客户端公平并发遗忘。

Result: 在多种场景下的广泛实验表明，FUPareto在遗忘效果和保留效用方面均优于现有最先进的联邦遗忘方法。

Conclusion: FUPareto通过帕累托增强优化有效解决了联邦遗忘中的关键挑战，实现了高效、公平的多客户端并发遗忘，同时最小化效用损失。

Abstract: Federated Unlearning (FU) aims to efficiently remove the influence of specific client data from a federated model while preserving utility for the remaining clients. However, three key challenges remain: (1) existing unlearning objectives often compromise model utility or increase vulnerability to Membership Inference Attacks (MIA); (2) there is a persistent conflict between forgetting and utility, where further unlearning inevitably harms retained performance; and (3) support for concurrent multi-client unlearning is poor, as gradient conflicts among clients degrade the quality of forgetting. To address these issues, we propose FUPareto, an efficient unlearning framework via Pareto-augmented optimization. We first introduce the Minimum Boundary Shift (MBS) Loss, which enforces unlearning by suppressing the target class logit below the highest non-target class logit; this can improve the unlearning efficiency and mitigate MIA risks. During the unlearning process, FUPareto performs Pareto improvement steps to preserve model utility and executes Pareto expansion to guarantee forgetting. Specifically, during Pareto expansion, the framework integrates a Null-Space Projected Multiple Gradient Descent Algorithm (MGDA) to decouple gradient conflicts. This enables effective, fair, and concurrent unlearning for multiple clients while minimizing utility degradation. Extensive experiments across diverse scenarios demonstrate that FUPareto consistently outperforms state-of-the-art FU methods in both unlearning efficacy and retained utility.

</details>


### [1178] [Bayesian Integration of Nonlinear Incomplete Clinical Data](https://arxiv.org/abs/2602.01924)
*Lucía González-Zamorano,Nuria Balbás-Esteban,Vanessa Gómez-Verdejo,Albert Belenguer-Llorens,Carlos Sevilla-Salcedo*

Main category: cs.LG

Relevance: 35.0

TL;DR: BIONIC是一个贝叶斯概率框架，用于整合高维异构的多模态临床数据，特别处理结构化缺失问题，通过生成-判别联合潜在架构实现鲁棒学习和可解释性。


<details>
  <summary>Details</summary>
Motivation: 多模态临床数据具有高维度、异构表示和结构化缺失的特点，这给预测建模、数据整合和可解释性带来了重大挑战。需要一种统一的框架来处理这些复杂数据，特别是在数据不完整的情况下。

Method: BIONIC采用贝叶斯多模态集成框架，使用预训练嵌入处理复杂模态（如医学图像和临床文本），同时将结构化临床变量直接纳入贝叶斯多模态公式中。该框架通过显式建模模态级和变量级缺失以及缺失标签，实现在部分观测和半监督设置下的鲁棒学习。

Result: 在三个多模态临床和生物医学数据集上评估，BIONIC相比代表性多模态基线表现出强大且一致的判别性能，特别是在不完整数据场景下。除了预测准确性外，BIONIC还通过其潜在结构提供内在可解释性。

Conclusion: BIONIC提供了一个统一的概率框架，能够有效整合异构多模态临床数据，处理缺失问题，同时保持预测性能和可解释性，支持临床有意义的洞察。

Abstract: Multimodal clinical data are characterized by high dimensionality, heterogeneous representations, and structured missingness, posing significant challenges for predictive modeling, data integration, and interpretability. We propose BIONIC (Bayesian Integration of Nonlinear Incomplete Clinical data), a unified probabilistic framework that integrates heterogeneous multimodal data under missingness through a joint generative-discriminative latent architecture. BIONIC uses pretrained embeddings for complex modalities such as medical images and clinical text, while incorporating structured clinical variables directly within a Bayesian multimodal formulation. The proposed framework enables robust learning in partially observed and semi-supervised settings by explicitly modeling modality-level and variable-level missingness, as well as missing labels. We evaluate BIONIC on three multimodal clinical and biomedical datasets, demonstrating strong and consistent discriminative performance compared to representative multimodal baselines, particularly under incomplete data scenarios. Beyond predictive accuracy, BIONIC provides intrinsic interpretability through its latent structure, enabling population-level analysis of modality relevance and supporting clinically meaningful insight.

</details>


### [1179] [Boundary-Constrained Diffusion Models for Floorplan Generation: Balancing Realism and Diversity](https://arxiv.org/abs/2602.01949)
*Leonardo Stoppani,Davide Bacciu,Shahab Mokarizadeh*

Main category: cs.LG

Relevance: 35.0

TL;DR: 该论文提出了一种用于建筑平面图生成的多样性评分指标和边界交叉注意力模块，以解决扩散模型在追求真实感时牺牲设计多样性的问题。


<details>
  <summary>Details</summary>
Motivation: 当前基于扩散模型的自动平面图生成系统虽然能产生高度真实的布局，但过度优化感知指标（如FID）会导致设计多样性受限。现有方法缺乏对几何一致性的有效控制，且无法量化在固定约束条件下的布局多样性。

Method: 1. 提出多样性评分（DS）指标，用于量化在固定约束条件下的布局多样性；2. 引入边界交叉注意力（BCA）模块，使模型能够更好地以建筑边界为条件进行生成；3. 通过实验分析训练时间对多样性的影响。

Result: BCA模块显著提高了边界一致性，而长时间训练会导致多样性崩溃（FID无法检测到）。研究揭示了真实感与多样性之间的关键权衡，并发现模型过度依赖数据集先验，在分布外评估中表现不佳。

Conclusion: 建筑设计中需要明确平衡保真度、多样性和泛化能力的生成系统。提出的DS指标和BCA模块为解决这一平衡问题提供了有效工具。

Abstract: Diffusion models have become widely popular for automated floorplan generation, producing highly realistic layouts conditioned on user-defined constraints. However, optimizing for perceptual metrics such as the Fréchet Inception Distance (FID) causes limited design diversity. To address this, we propose the Diversity Score (DS), a metric that quantifies layout diversity under fixed constraints. Moreover, to improve geometric consistency, we introduce a Boundary Cross-Attention (BCA) module that enables conditioning on building boundaries. Our experiments show that BCA significantly improves boundary adherence, while prolonged training drives diversity collapse undiagnosed by FID, revealing a critical trade-off between realism and diversity. Out-Of-Distribution evaluations further demonstrate the models' reliance on dataset priors, emphasizing the need for generative systems that explicitly balance fidelity, diversity, and generalization in architectural design tasks.

</details>


### [1180] [Deep Multivariate Models with Parametric Conditionals](https://arxiv.org/abs/2602.01953)
*Dmitrij Schlesinger,Boris Flach,Alexander Shekhovtsov*

Main category: cs.LG

Relevance: 35.0

TL;DR: 提出一种用于异构变量集合的深度多元模型，通过条件概率分布表示联合分布，可适用于多种下游任务，采用参数化马尔可夫链核训练方法


<details>
  <summary>Details</summary>
Motivation: 现有模型通常针对特定应用任务设计，限制了在其他下游任务中的适用性。本文旨在开发一种更通用的模型，能够处理计算机视觉中异构变量集合（如图像、分割、属性、潜变量），并适用于各种可能的任务。

Method: 通过条件概率分布表示联合概率分布，每个变量组基于其他变量进行条件建模。采用参数化马尔可夫链核训练方法，通过最大化其极限分布的数据似然进行学习。

Result: 该方法能够处理异构变量集合，支持广泛的下游任务应用，并允许各种半监督学习场景。

Conclusion: 提出的条件概率分布建模方法相比传统任务特定设计具有更好的通用性和灵活性，适用于多种计算机视觉任务。

Abstract: We consider deep multivariate models for heterogeneous collections of random variables. In the context of computer vision, such collections may e.g. consist of images, segmentations, image attributes, and latent variables. When developing such models, most existing works start from an application task and design the model components and their dependencies to meet the needs of the chosen task. This has the disadvantage of limiting the applicability of the resulting model for other downstream tasks. Here, instead, we propose to represent the joint probability distribution by means of conditional probability distributions for each group of variables conditioned on the rest. Such models can then be used for practically any possible downstream task. Their learning can be approached as training a parametrised Markov chain kernel by maximising the data likelihood of its limiting distribution. This has the additional advantage of allowing a wide range of semi-supervised learning scenarios.

</details>


### [1181] [Optimizing Tensor Train Decomposition in DNNs for RISC-V Architectures Using Design Space Exploration and Compiler Optimizations](https://arxiv.org/abs/2602.01996)
*Theologos Anthimopoulos,Milad Kokhazadeh,Vasilios Kelefouras,Benjamin Himpel,Georgios Keramidas*

Main category: cs.LG

Relevance: 35.0

TL;DR: 提出一种基于张量列分解的端到端低秩分解设计空间探索方法，用于优化RISC-V处理器上的全连接层压缩，实现边缘设备上DNN的高效部署。


<details>
  <summary>Details</summary>
Motivation: 在资源受限的RISC-V平台上部署DNN面临挑战，全连接层占据主要计算和内存资源。低秩分解虽能压缩模型，但其设计空间庞大，涉及FLOPs、内存、推理时间和准确性的复杂权衡，导致优化过程复杂耗时。

Method: 使用TensorFlow T3F库的张量列分解，通过两步剪枝设计空间：1) 排除低效分解形状；2) 排除在RISC-V架构上推理性能差的方案。然后应用编译器优化提升自定义T3F层性能，最小化推理时间。

Result: TT分解层平均比IREE快3倍，比Pluto快8倍（相同压缩模型下）。为RISC-V架构的边缘和嵌入式设备提供了高效的DNN部署解决方案。

Conclusion: 提出了一种端到端的低秩分解设计空间探索方法和专用设计工具，有效优化了RISC-V处理器上的全连接层，显著提升了边缘设备上DNN的部署效率。

Abstract: Deep neural networks (DNNs) have become indispensable in many real-life applications like natural language processing, and autonomous systems. However, deploying DNNs on resource-constrained devices, e.g., in RISC-V platforms, remains challenging due to the high computational and memory demands of fully connected (FC) layers, which dominate resource consumption. Low-rank factorization (LRF) offers an effective approach to compressing FC layers, but the vast design space of LRF solutions involves complex trade-offs among FLOPs, memory size, inference time, and accuracy, making the LRF process complex and time-consuming. This paper introduces an end-to-end LRF design space exploration methodology and a specialized design tool for optimizing FC layers on RISC-V processors. Using Tensor Train Decomposition (TTD) offered by TensorFlow T3F library, the proposed work prunes the LRF design space by excluding first, inefficient decomposition shapes and second, solutions with poor inference performance on RISC-V architectures. Compiler optimizations are then applied to enhance custom T3F layer performance, minimizing inference time and boosting computational efficiency. On average, our TT-decomposed layers run 3x faster than IREE and 8x faster than Pluto on the same compressed model. This work provides an efficient solution for deploying DNNs on edge and embedded devices powered by RISC-V architectures.

</details>


### [1182] [SNAP: A Self-Consistent Agreement Principle with Application to Robust Computation](https://arxiv.org/abs/2602.02013)
*Xiaoyi Jiang,Andreas Nienkötter*

Main category: cs.LG

Relevance: 35.0

TL;DR: SNAP是一个基于相互一致性的自监督鲁棒计算框架，通过一致性-可靠性假设为数据项分配权重，强调可信项并降低异常值影响，无需监督或先验知识。


<details>
  <summary>Details</summary>
Motivation: 现有鲁棒计算方法通常需要迭代优化或对异常值分布有先验假设，SNAP旨在提供一个无需监督、无需先验知识、非迭代的鲁棒计算框架，通过数据项之间的相互一致性来自动识别可信项和异常值。

Method: SNAP基于一致性-可靠性假设，通过计算数据项之间的相互一致性来分配权重。关键特性是异常值权重指数抑制，确保即使在多维设置中异常值对计算的贡献也微不足道。该方法是非迭代的，相比迭代的Weiszfeld算法和多变量中位数均值变体更高效。

Result: 在向量平均和子空间估计任务上，非迭代的SNAP优于迭代的Weiszfeld算法和两种多变量中位数均值变体。SNAP权重方案具有理论保证，特别是异常值权重的指数抑制特性。

Conclusion: SNAP提供了一个灵活、易用、广泛适用的鲁棒计算方法，无需监督或先验知识，通过自监督的一致性原则实现鲁棒计算。

Abstract: We introduce SNAP (Self-coNsistent Agreement Principle), a self-supervised framework for robust computation based on mutual agreement. Based on an Agreement-Reliability Hypothesis SNAP assigns weights that quantify agreement, emphasizing trustworthy items and downweighting outliers without supervision or prior knowledge. A key result is the Exponential Suppression of Outlier Weights, ensuring that outliers contribute negligibly to computations, even in high-dimensional settings. We study properties of SNAP weighting scheme and show its practical benefits on vector averaging and subspace estimation. Particularly, we demonstrate that non-iterative SNAP outperforms the iterative Weiszfeld algorithm and two variants of multivariate median of means. SNAP thus provides a flexible, easy-to-use, broadly applicable approach to robust computation.

</details>


### [1183] [Learning Half-Spaces from Perturbed Contrastive Examples](https://arxiv.org/abs/2602.02080)
*Aryan Alavi Razavi Ravari,Farnam Mansouri,Yuxin Chen,Valentio Iverson,Adish Singla,Sandra Zilles*

Main category: cs.LG

Relevance: 35.0

TL;DR: 该论文研究在带噪声的对比样本oracle下的学习问题，其中对比样本的扰动程度由点到决策边界的距离控制，分析了在固定和随机扰动下的样本复杂度。


<details>
  <summary>Details</summary>
Motivation: 研究在更现实的噪声设置下对比学习的效果，扩展Mansouri等人的理想化对比样本假设，探索对比样本质量随点到决策边界距离变化的机制。

Method: 引入参数化噪声函数f的机制，控制对比样本的扰动程度，分析一维阈值和半空间在均匀分布下的主动和被动对比样本复杂度。

Result: 在特定f函数条件下，对比样本的存在能加速学习，降低渐近查询复杂度和期望查询复杂度。

Conclusion: 对比样本在噪声环境下仍能提升学习效率，特别是当对比样本质量随点到决策边界距离变化时。

Abstract: We study learning under a two-step contrastive example oracle, as introduced by Mansouri et. al. (2025), where each queried (or sampled) labeled example is paired with an additional contrastive example of opposite label. While Mansouri et al. assume an idealized setting, where the contrastive example is at minimum distance of the originally queried/sampled point, we introduce and analyze a mechanism, parameterized by a non-decreasing noise function $f$, under which this ideal contrastive example is perturbed. The amount of perturbation is controlled by $f(d)$, where $d$ is the distance of the queried/sampled point to the decision boundary. Intuitively, this results in higher-quality contrastive examples for points closer to the decision boundary. We study this model in two settings: (i) when the maximum perturbation magnitude is fixed, and (ii) when it is stochastic.
  For one-dimensional thresholds and for half-spaces under the uniform distribution on a bounded domain, we characterize active and passive contrastive sample complexity in dependence on the function $f$. We show that, under certain conditions on $f$, the presence of contrastive examples speeds up learning in terms of asymptotic query complexity and asymptotic expected query complexity.

</details>


### [1184] [Efficient Swap Regret Minimization in Combinatorial Bandits](https://arxiv.org/abs/2602.02087)
*Andreas Kontogiannis,Vasilis Pollatos,Panayotis Mertikopoulos,Ioannis Panageas*

Main category: cs.LG

Relevance: 35.0

TL;DR: 该论文提出了一种针对组合赌博机的高效无交换遗憾算法，解决了在动作数量N指数级增长时实现多项式对数级遗憾的挑战。


<details>
  <summary>Details</summary>
Motivation: 在组合赌博机问题中，动作数量N相对于问题维度呈指数级增长。虽然外部遗憾最小化问题已有较好理解，但实现多项式对数级依赖N的无交换遗憾算法一直是个难题。论文旨在解决这一挑战。

Method: 引入了一种无交换遗憾学习算法，其遗憾在N上呈多项式对数级缩放。算法设计确保在组合赌博机类别中达到紧致边界，并通过多种应用场景展示其高效实现。

Result: 提出的算法在N上实现了多项式对数级的遗憾缩放，这在组合赌博机类别中是紧致的。同时展示了算法在多种应用中的高效实现，每次迭代复杂度也呈多项式对数级。

Conclusion: 该论文成功解决了组合赌博机中无交换遗憾算法的设计难题，实现了多项式对数级依赖N的遗憾边界，并通过高效实现验证了算法的实用性。

Abstract: This paper addresses the problem of designing efficient no-swap regret algorithms for combinatorial bandits, where the number of actions $N$ is exponentially large in the dimensionality of the problem. In this setting, designing efficient no-swap regret translates to sublinear -- in horizon $T$ -- swap regret with polylogarithmic dependence on $N$. In contrast to the weaker notion of external regret minimization - a problem which is fairly well understood in the literature - achieving no-swap regret with a polylogarithmic dependence on $N$ has remained elusive in combinatorial bandits. Our paper resolves this challenge, by introducing a no-swap-regret learning algorithm with regret that scales polylogarithmically in $N$ and is tight for the class of combinatorial bandits. To ground our results, we also demonstrate how to implement the proposed algorithm efficiently -- that is, with a per-iteration complexity that also scales polylogarithmically in $N$ -- across a wide range of well-studied applications.

</details>


### [1185] [Scalable Spatio-Temporal SE(3) Diffusion for Long-Horizon Protein Dynamics](https://arxiv.org/abs/2602.02128)
*Nima Shoghi,Yuxuan Liu,Yuning Shen,Rob Brekelmans,Pan Li,Quanquan Gu*

Main category: cs.LG

Relevance: 35.0

TL;DR: STAR-MD是一种用于蛋白质动力学模拟的SE(3)等变扩散模型，通过联合时空注意力机制实现微秒级轨迹生成，在ATLAS基准测试中达到SOTA性能。


<details>
  <summary>Details</summary>
Motivation: 传统分子动力学模拟计算成本高，难以达到生物相关时间尺度；现有生成模型在长时程生成方面存在架构限制、误差累积和时空动态建模不足的问题。

Method: 提出STAR-MD：可扩展的SE(3)等变扩散模型，采用具有联合时空注意力的因果扩散Transformer，高效捕捉复杂时空依赖关系，避免现有方法的内存瓶颈。

Result: 在ATLAS基准测试中，STAR-MD在所有指标上达到最先进性能，显著改善构象覆盖、结构有效性和动态保真度；能够稳定生成微秒级轨迹，而基线方法完全失败。

Conclusion: STAR-MD的联合时空建模能够在生物相关时间尺度上实现稳健的动力学模拟，为加速探索蛋白质功能开辟了新途径。

Abstract: Molecular dynamics (MD) simulations remain the gold standard for studying protein dynamics, but their computational cost limits access to biologically relevant timescales. Recent generative models have shown promise in accelerating simulations, yet they struggle with long-horizon generation due to architectural constraints, error accumulation, and inadequate modeling of spatio-temporal dynamics. We present STAR-MD (Spatio-Temporal Autoregressive Rollout for Molecular Dynamics), a scalable SE(3)-equivariant diffusion model that generates physically plausible protein trajectories over microsecond timescales. Our key innovation is a causal diffusion transformer with joint spatio-temporal attention that efficiently captures complex space-time dependencies while avoiding the memory bottlenecks of existing methods. On the standard ATLAS benchmark, STAR-MD achieves state-of-the-art performance across all metrics--substantially improving conformational coverage, structural validity, and dynamic fidelity compared to previous methods. STAR-MD successfully extrapolates to generate stable microsecond-scale trajectories where baseline methods fail catastrophically, maintaining high structural quality throughout the extended rollout. Our comprehensive evaluation reveals severe limitations in current models for long-horizon generation, while demonstrating that STAR-MD's joint spatio-temporal modeling enables robust dynamics simulation at biologically relevant timescales, paving the way for accelerated exploration of protein function.

</details>


### [1186] [Back to the Future: Look-ahead Augmentation and Parallel Self-Refinement for Time Series Forecasting](https://arxiv.org/abs/2602.02146)
*Sunho Kim,Susik Yoon*

Main category: cs.LG

Relevance: 35.0

TL;DR: BTTF框架通过前瞻增强和自校正优化，提升长期时间序列预测的稳定性，无需复杂架构即可显著改善预测精度


<details>
  <summary>Details</summary>
Motivation: 解决长期时间序列预测中并行效率与时间一致性之间的权衡问题。直接多步预测方法虽然并行高效但失去时间一致性，而迭代多步预测保持时间依赖但存在误差累积和推理缓慢的问题

Method: 提出BTTF框架，通过前瞻增强和自校正优化来增强预测稳定性。该方法不依赖复杂模型架构，而是重新审视基础预测过程，通过集成第二阶段模型（使用初始预测进行增强）来优化基础模型

Result: BTTF显著提高了长期预测精度，改善了线性预测模型的不稳定性，最高可获得58%的精度提升。即使在第一阶段模型训练条件不理想的情况下，也能保持稳定的改进效果

Conclusion: 利用模型生成的预测作为增强信息，即使没有复杂架构，也是提升长期预测能力的简单而有效的方法。这种方法为时间序列预测提供了新的视角

Abstract: Long-term time series forecasting (LTSF) remains challenging due to the trade-off between parallel efficiency and sequential modeling of temporal coherence. Direct multi-step forecasting (DMS) methods enable fast, parallel prediction of all future horizons but often lose temporal consistency across steps, while iterative multi-step forecasting (IMS) preserves temporal dependencies at the cost of error accumulation and slow inference. To bridge this gap, we propose Back to the Future (BTTF), a simple yet effective framework that enhances forecasting stability through look-ahead augmentation and self-corrective refinement. Rather than relying on complex model architectures, BTTF revisits the fundamental forecasting process and refines a base model by ensembling the second-stage models augmented with their initial predictions. Despite its simplicity, our approach consistently improves long-horizon accuracy and mitigates the instability of linear forecasting models, achieving accuracy gains of up to 58% and demonstrating stable improvements even when the first-stage model is trained under suboptimal conditions. These results suggest that leveraging model-generated forecasts as augmentation can be a simple yet powerful way to enhance long-term prediction, even without complex architectures.

</details>


### [1187] [Generating Causal Temporal Interaction Graphs for Counterfactual Validation of Temporal Link Prediction](https://arxiv.org/abs/2602.02161)
*Aniq Ur Rahman,Justin P. Coon*

Main category: cs.LG

Relevance: 35.0

TL;DR: 提出了一个用于时间链接预测模型的反事实验证框架，通过生成具有已知因果结构的时间交互图来评估模型是否捕捉到因果机制，而不仅仅是预测准确性。


<details>
  <summary>Details</summary>
Motivation: 当前时间链接预测模型主要基于预测准确性进行评估，但这种方法无法评估模型是否真正捕捉到了时间交互背后的因果机制。需要一种能够验证模型因果理解能力的评估框架。

Method: 1) 引入支持兴奋和抑制效应的连续时间事件序列结构方程模型；2) 将该机制扩展到时间交互图；3) 提出基于跨模型预测误差的因果距离度量；4) 在两种场景下实例化反事实评估：控制因果转移和时间戳洗牌。

Result: 实证验证了假设：在一个因果模型上训练的预测器，在足够远的因果模型上评估时性能会下降。框架为因果感知的基准测试提供了基础。

Conclusion: 该框架超越了传统的预测准确性评估，能够验证时间链接预测模型是否真正理解因果机制，为更全面的模型评估提供了新方法。

Abstract: Temporal link prediction (TLP) models are commonly evaluated based on predictive accuracy, yet such evaluations do not assess whether these models capture the causal mechanisms that govern temporal interactions. In this work, we propose a framework for counterfactual validation of TLP models by generating causal temporal interaction graphs (CTIGs) with known ground-truth causal structure. We first introduce a structural equation model for continuous-time event sequences that supports both excitatory and inhibitory effects, and then extend this mechanism to temporal interaction graphs. To compare causal models, we propose a distance metric based on cross-model predictive error, and empirically validate the hypothesis that predictors trained on one causal model degrade when evaluated on sufficiently distant models. Finally, we instantiate counterfactual evaluation under (i) controlled causal shifts between generating models and (ii) timestamp shuffling as a stochastic distortion with measurable causal distance. Our framework provides a foundation for causality-aware benchmarking.

</details>


### [1188] [SurvKAN: A Fully Parametric Survival Model Based on Kolmogorov-Arnold Networks](https://arxiv.org/abs/2602.02179)
*Marina Mastroleo,Alberto Archetti,Federico Mastroleo,Matteo Matteucci*

Main category: cs.LG

Relevance: 35.0

TL;DR: SurvKAN：基于KAN架构的完全参数化、时间连续生存模型，消除了比例风险约束，在保持可解释性的同时实现竞争性性能


<details>
  <summary>Details</summary>
Motivation: 传统生存模型（如Cox）依赖线性协变量关系和比例风险等限制性假设，无法捕捉真实临床动态。深度学习模型（如DeepSurv）提高了表达能力但牺牲了可解释性，限制了临床采用。需要一种既保持可解释性又具有表达能力的生存分析方法。

Method: SurvKAN基于Kolmogorov-Arnold Networks（KAN）架构，将时间作为显式输入，直接预测对数风险函数。模型通过可学习的单变量函数保持可解释性，这些函数显示个体特征如何随时间影响风险。采用端到端训练，基于完整的生存似然。

Result: 在标准生存基准测试中，SurvKAN在一致性指数和校准指标方面达到或优于经典和最先进的基线方法。可解释性分析揭示了与医学领域知识一致的临床有意义模式。

Conclusion: SurvKAN提供了一种完全参数化、时间连续的生存分析方法，消除了比例风险约束，在保持可解释性的同时实现了竞争性性能，有望促进临床采用。

Abstract: Accurate prediction of time-to-event outcomes is critical for clinical decision-making, treatment planning, and resource allocation in modern healthcare. While classical survival models such as Cox remain widely adopted in standard practice, they rely on restrictive assumptions, including linear covariate relationships and proportional hazards over time, that often fail to capture real-world clinical dynamics. Recent deep learning approaches like DeepSurv and DeepHit offer improved expressivity but sacrifice interpretability, limiting clinical adoption where trust and transparency are paramount. Hybrid models incorporating Kolmogorov-Arnold Networks (KANs), such as CoxKAN, have begun to address this trade-off but remain constrained by the semi-parametric Cox framework. In this work we introduce SurvKAN, a fully parametric, time-continuous survival model based on KAN architectures that eliminates the proportional hazards constraint. SurvKAN treats time as an explicit input to a KAN that directly predicts the log-hazard function, enabling end-to-end training on the full survival likelihood. Our architecture preserves interpretability through learnable univariate functions that indicate how individual features influence risk over time. Extensive experiments on standard survival benchmarks demonstrate that SurvKAN achieves competitive or superior performance compared to classical and state-of-the-art baselines across concordance and calibration metrics. Additionally, interpretability analyses reveal clinically meaningful patterns that align with medical domain knowledge.

</details>


### [1189] [Cardinality-Preserving Structured Sparse Graph Transformers for Molecular Property Prediction](https://arxiv.org/abs/2602.02201)
*Abhijit Gupta*

Main category: cs.LG

Relevance: 35.0

TL;DR: CardinalGraphFormer是一个用于分子性质预测的图Transformer模型，通过结合Graphormer的结构偏置和结构化稀疏注意力，在有限标记数据下实现了更好的性能。


<details>
  <summary>Details</summary>
Motivation: 药物发现需要高效的分子性质预测，但化学空间巨大（约10^60个药物样分子），而仅有数千种获批药物，导致标记数据有限。因此，需要在大规模未标记分子语料上进行自监督预训练，以实现数据高效的分子表示学习。

Method: CardinalGraphFormer是一个图Transformer模型，结合了Graphormer的结构偏置（最短路径距离、中心性、直接键边偏置）和结构化稀疏注意力（限制最短路径距离≤3）。模型还增加了在相同支持集上的基数保持非归一化聚合通道。预训练结合了对比图级对齐和掩码属性重建。

Result: 在完全匹配的评估协议下，CardinalGraphFormer在所有11个评估任务上均提高了平均性能，并在MoleculeNet、OGB和TDC ADMET任务的10个公共基准测试中取得了统计显著的增益。

Conclusion: CardinalGraphFormer通过结合结构偏置和稀疏注意力机制，在分子性质预测任务中实现了优越的性能，为药物发现中的高效分子表示学习提供了有效解决方案。

Abstract: Drug discovery motivates efficient molecular property prediction under limited labeled data. Chemical space is vast, often estimated at approximately 10^60 drug-like molecules, while only thousands of drugs have been approved. As a result, self-supervised pretraining on large unlabeled molecular corpora has become essential for data-efficient molecular representation learning. We introduce **CardinalGraphFormer**, a graph transformer that incorporates Graphormer-inspired structural biases, including shortest-path distance and centrality, as well as direct-bond edge bias, within a structured sparse attention regime limited to shortest-path distance <= 3. The model further augments this design with a cardinality-preserving unnormalized aggregation channel over the same support set. Pretraining combines contrastive graph-level alignment with masked attribute reconstruction. Under a fully matched evaluation protocol, CardinalGraphFormer improves mean performance across all 11 evaluated tasks and achieves statistically significant gains on 10 of 11 public benchmarks spanning MoleculeNet, OGB, and TDC ADMET tasks when compared to strong reproduced baselines.

</details>


### [1190] [Generating Physically Sound Designs from Text and a Set of Physical Constraints](https://arxiv.org/abs/2602.02213)
*Gregory Barber,Todd C. Henry,Mulugeta A. Haile*

Main category: cs.LG

Relevance: 35.0

TL;DR: TIDES是一种基于文本描述的物理设计生成方法，通过联合优化结构拓扑和视觉属性，结合预训练文本-图像模型评估视觉对齐和可微分物理模拟器评估物理性能。


<details>
  <summary>Details</summary>
Motivation: 当前设计生成方法通常只关注物理性能或视觉美观性，缺乏能够同时满足工程约束和文本描述视觉特征的综合设计方法。需要一种能够将文本描述转化为物理上合理设计的方法。

Method: 使用预训练文本-图像模型测量设计与文本提示的视觉对齐度，结合可微分物理模拟器评估物理性能。通过联合优化结构拓扑和视觉属性，在满足工程设计要求的同时实现文本指定的特征。

Result: 在多种载荷和支撑条件下的结构优化问题中进行了评估，包括不同分辨率的2D梁设计。通过3D打印和三点弯曲实验验证，能够同时优化两个目标，返回满足工程设计要求（柔度和密度）并利用文本指定特征的设计。

Conclusion: TIDES能够成功地将文本描述转化为物理上合理的设计，实现了工程约束和视觉特征的联合优化，为基于文本的物理设计生成提供了有效方法。

Abstract: We present TIDES, a text informed design approach for generating physically sound designs based on a textual description and a set of physical constraints. TIDES jointly optimizes structural (topology) and visual properties. A pre-trained text-image model is used to measure the design's visual alignment with a text prompt and a differentiable physics simulator is used to measure its physical performance. We evaluate TIDES on a series of structural optimization problems operating under different load and support conditions, at different resolutions, and experimentally in the lab by performing the 3-point bending test on 2D beam designs that are extruded and 3D printed. We find that it can jointly optimize the two objectives and return designs that satisfy engineering design requirements (compliance and density) while utilizing features specified by text.

</details>


### [1191] [Unsupervised Physics-Informed Operator Learning through Multi-Stage Curriculum Training](https://arxiv.org/abs/2602.02264)
*Paolo Marcandelli,Natansh Mathur,Stefano Markidis,Martina Siena,Stefano Mariani*

Main category: cs.LG

Relevance: 35.0

TL;DR: 提出多阶段物理信息训练策略和PhIS-FNO模型，通过渐进式边界条件强制执行和样条傅里叶神经算子，在仅使用边界标签的情况下达到接近监督学习的精度


<details>
  <summary>Details</summary>
Motivation: 解决科学机器学习中偏微分方程求解的挑战。神经算子虽然能学习函数空间映射并实现分辨率无关推理，但通常需要监督数据；物理信息神经网络通过物理约束进行无监督训练，但存在收敛不稳定和泛化能力有限的问题。

Method: 1. 多阶段物理信息训练策略：渐进式在损失函数中强制执行边界条件，然后加入内部残差；每阶段重新初始化优化器作为延续机制；2. PhIS-FNO：结合傅里叶层和Hermite样条核进行平滑残差评估

Result: 在标准基准测试中，PhIS-FNO仅使用狭窄边界区域的标签信息，达到了与监督学习相当的精度水平

Conclusion: 分阶段、基于样条的优化为物理信息算子学习提供了一个稳健的范式

Abstract: Solving partial differential equations remains a central challenge in scientific machine learning. Neural operators offer a promising route by learning mappings between function spaces and enabling resolution-independent inference, yet they typically require supervised data. Physics-informed neural networks address this limitation through unsupervised training with physical constraints but often suffer from unstable convergence and limited generalization capability. To overcome these issues, we introduce a multi-stage physics-informed training strategy that achieves convergence by progressively enforcing boundary conditions in the loss landscape and subsequently incorporating interior residuals. At each stage the optimizer is re-initialized, acting as a continuation mechanism that restores stability and prevents gradient stagnation. We further propose the Physics-Informed Spline Fourier Neural Operator (PhIS-FNO), combining Fourier layers with Hermite spline kernels for smooth residual evaluation. Across canonical benchmarks, PhIS-FNO attains a level of accuracy comparable to that of supervised learning, using labeled information only along a narrow boundary region, establishing staged, spline-based optimization as a robust paradigm for physics-informed operator learning.

</details>


### [1192] [Backpropagation as Physical Relaxation: Exact Gradients in Finite Time](https://arxiv.org/abs/2602.02281)
*Antonino Emanuele Scurria*

Main category: cs.LG

Relevance: 35.0

TL;DR: 该论文提出了一种名为"Dyadic Backpropagation"的框架，将反向传播重新解释为物理动力系统的有限时间松弛过程，通过连续时间公式和拉格朗日理论，在双倍状态空间中推导出全局能量泛函，其鞍点动力学能在有限时间内精确恢复标准反向传播。


<details>
  <summary>Details</summary>
Motivation: 传统反向传播通常被理解为符号计算中的链式法则递归应用。本文旨在从物理动力系统的角度重新理解反向传播，将其视为连续物理松弛过程的数字优化影子，为模拟和神经形态计算平台提供精确梯度计算的严格理论基础。

Method: 将前向推理建模为连续时间过程，应用非保守系统的拉格朗日理论处理非对称交互，在编码激活值和敏感度的双倍状态空间上推导全局能量泛函。该能量泛函的鞍点动力学通过局部交互同时执行推理和信用分配。证明单位步长欧拉离散化（层转换的自然时间尺度）能在2L步内精确恢复L层网络的标准反向传播。

Result: 提出的Dyadic Backpropagation框架能够在有限时间内保证精确梯度计算，无需对称权重、渐近收敛或微小扰动等近似条件。与先前需要这些条件的能量基方法不同，该框架为模拟和神经形态基板中的精确梯度计算提供了严格基础。

Conclusion: 反向传播是连续物理松弛过程的数字优化影子，Dyadic Backpropagation框架为在连续动力学原生的模拟和神经形态计算平台上实现精确梯度计算提供了理论基础，将符号计算与物理动力系统统一起来。

Abstract: Backpropagation, the foundational algorithm for training neural networks, is typically understood as a symbolic computation that recursively applies the chain rule. We show it emerges exactly as the finite-time relaxation of a physical dynamical system. By formulating feedforward inference as a continuous-time process and applying Lagrangian theory of non-conservative systems to handle asymmetric interactions, we derive a global energy functional on a doubled state space encoding both activations and sensitivities. The saddle-point dynamics of this energy perform inference and credit assignment simultaneously through local interactions. We term this framework ''Dyadic Backpropagation''. Crucially, we prove that unit-step Euler discretization, the natural timescale of layer transitions, recovers standard backpropagation exactly in precisely 2L steps for an L-layer network, with no approximations. Unlike prior energy-based methods requiring symmetric weights, asymptotic convergence, or vanishing perturbations, our framework guarantees exact gradients in finite time. This establishes backpropagation as the digitally optimized shadow of a continuous physical relaxation, providing a rigorous foundation for exact gradient computation in analog and neuromorphic substrates where continuous dynamics are native.

</details>


### [1193] [MoLF: Mixture-of-Latent-Flow for Pan-Cancer Spatial Gene Expression Prediction from Histology](https://arxiv.org/abs/2602.02282)
*Susu Hu,Stefanie Speidel*

Main category: cs.LG

Relevance: 35.0

TL;DR: MoLF是一种用于泛癌组织基因组预测的生成模型，通过条件流匹配和专家混合架构，在跨癌症类型的数据稀缺场景中实现最先进的性能。


<details>
  <summary>Details</summary>
Motivation: 当前空间转录组学预测方法局限于单一组织模型，无法利用跨癌症类型的共享生物学原理，且在数据稀缺场景中应用受限。泛癌训练虽然提供了解决方案，但异质性给单一架构带来了挑战。

Method: MoLF采用条件流匹配目标将噪声映射到基因潜在流形，通过专家混合速度场参数化。该架构动态路由输入到专门子网络，有效解耦不同组织模式的优化。

Result: MoLF在泛癌基准测试中建立了新的最先进水平，持续优于专业模型和基础模型基线。此外，MoLF展现出对跨物种数据的零样本泛化能力。

Conclusion: MoLF通过专家混合架构有效处理泛癌异质性，捕捉了保守的组织分子机制，为数据稀缺场景下的组织基因组分析提供了可扩展解决方案。

Abstract: Inferring spatial transcriptomics (ST) from histology enables scalable histogenomic profiling, yet current methods are largely restricted to single-tissue models. This fragmentation fails to leverage biological principles shared across cancer types and hinders application to data-scarce scenarios. While pan-cancer training offers a solution, the resulting heterogeneity challenges monolithic architectures. To bridge this gap, we introduce MoLF (Mixture-of-Latent-Flow), a generative model for pan-cancer histogenomic prediction. MoLF leverages a conditional Flow Matching objective to map noise to the gene latent manifold, parameterized by a Mixture-of-Experts (MoE) velocity field. By dynamically routing inputs to specialized sub-networks, this architecture effectively decouples the optimization of diverse tissue patterns. Our experiments demonstrate that MoLF establishes a new state-of-the-art, consistently outperforming both specialized and foundation model baselines on pan-cancer benchmarks. Furthermore, MoLF exhibits zero-shot generalization to cross-species data, suggesting it captures fundamental, conserved histo-molecular mechanisms.

</details>


### [1194] [Choice-Model-Assisted Q-learning for Delayed-Feedback Revenue Management](https://arxiv.org/abs/2602.02283)
*Owen Shen,Patrick Jaillet*

Main category: cs.LG

Relevance: 35.0

TL;DR: 该论文提出了一种结合离散选择模型与强化学习的方法，用于解决收益管理中延迟反馈问题。通过使用校准的选择模型作为部分世界模型来估算延迟的学习目标，在固定模型部署机制下证明了收敛性，并在酒店预订模拟实验中验证了方法的有效性。


<details>
  <summary>Details</summary>
Motivation: 收益管理中存在延迟反馈问题，大量价值由客户取消和修改决定，这些信息在预订后数天才观察到。传统强化学习方法难以处理这种延迟反馈，需要新的方法来在决策时估算延迟的学习目标。

Method: 提出"选择模型辅助强化学习"方法：使用校准的离散选择模型作为固定的部分世界模型，在决策时估算延迟的学习目标。在固定模型部署机制下，使用表格Q学习结合模型估算目标，并证明了收敛性。

Result: 在基于61,619个酒店预订数据的模拟实验中：(1)在平稳环境下与成熟缓冲DQN基线无统计显著差异；(2)在参数偏移场景下，10个场景中有5个显示显著收益提升（最高12.4%）；(3)在结构误设下，收益降低1.4-2.6%。

Conclusion: 部分行为模型在参数偏移下能提高鲁棒性，但在结构误设时会引入有害偏差。该方法为延迟反馈的强化学习提供了理论保证和实用框架。

Abstract: We study reinforcement learning for revenue management with delayed feedback, where a substantial fraction of value is determined by customer cancellations and modifications observed days after booking. We propose \emph{choice-model-assisted RL}: a calibrated discrete choice model is used as a fixed partial world model to impute the delayed component of the learning target at decision time. In the fixed-model deployment regime, we prove that tabular Q-learning with model-imputed targets converges to an $O(\varepsilon/(1-γ))$ neighborhood of the optimal Q-function, where $\varepsilon$ summarizes partial-model error, with an additional $O(t^{-1/2})$ sampling term. Experiments in a simulator calibrated from 61{,}619 hotel bookings (1{,}088 independent runs) show: (i) no statistically detectable difference from a maturity-buffer DQN baseline in stationary settings; (ii) positive effects under in-family parameter shifts, with significant gains in 5 of 10 shift scenarios after Holm--Bonferroni correction (up to 12.4\%); and (iii) consistent degradation under structural misspecification, where the choice model assumptions are violated (1.4--2.6\% lower revenue). These results characterize when partial behavioral models improve robustness under shift and when they introduce harmful bias.

</details>


### [1195] [Active Transfer Bagging: A New Approach for Accelerated Active Learning Acquisition of Data by Combined Transfer Learning and Bagging Based Models](https://arxiv.org/abs/2602.02415)
*Vivienne Pelletier,Daniel J. Rivera,Obinna Nwokonkwo,Steven A. Wilson,Christopher L. Muhich*

Main category: cs.LG

Relevance: 35.0

TL;DR: ATBagging是一种新的主动学习种子集选择方法，通过贝叶斯解释的bagging集成模型估计候选数据点的信息量，结合DPP多样性采样，在低数据量场景下显著提升主动学习早期性能。


<details>
  <summary>Details</summary>
Motivation: 主动学习虽然能降低标注成本，但其早期性能通常受限于随机选择的初始种子集。许多应用中存在相关或近似数据集，可用于构建更好的种子集，但现有方法未能充分利用这些信息。

Method: ATBagging结合bagging集成模型的贝叶斯解释来估计候选数据点的信息量（通过比较袋内和袋外预测分布），并使用确定性点过程（DPP）进行特征空间多样性采样，避免冗余选择。该方法既用于初始种子集选择，也用于主动学习阶段的新数据点收集。

Result: 在四个真实数据集（QM9、ERA5、Forbes 2000、Beijing PM2.5）上，种子大小nseed=10-100时，ATBagging在几乎所有情况下都优于或持平其他种子选择方法，特别是在低数据量场景下表现最强，提高了学习曲线下面积。

Conclusion: ATBagging提供了一种低成本、高回报的主动学习数据收集启动方法，通过智能种子集选择显著提升主动学习早期性能，尤其适用于标注成本高的应用场景。

Abstract: Modern machine learning has achieved remarkable success on many problems, but this success often depends on the existence of large, labeled datasets. While active learning can dramatically reduce labeling cost when annotations are expensive, early performance is frequently dominated by the initial seed set, typically chosen at random. In many applications, however, related or approximate datasets are readily available and can be leveraged to construct a better seed set. We introduce a new method for selecting the seed data set for active learning, Active-Transfer Bagging (ATBagging). ATBagging estimates the informativeness of candidate data point from a Bayesian interpretation of bagged ensemble models by comparing in-bag and out-of-bag predictive distributions from the labeled dataset, yielding an information-gain proxy. To avoid redundant selections, we impose feature-space diversity by sampling a determinantal point process (DPP) whose kernel uses Random Fourier Features and a quality-diversity factorization that incorporates the informativeness scores. This same blended method is used for selection of new data points to collect during the active learning phase. We evaluate ATBagging on four real-world datasets covering both target-transfer and feature-shift scenarios (QM9, ERA5, Forbes 2000, and Beijing PM2.5). Across seed sizes nseed = 10-100, ATBagging improves or ties early active learning and increases area under the learning-curve relative to alternative seed subset selection methodologies in almost all cases, with strongest benefits in low-data regimes. Thus, ATBagging provides a low-cost, high reward means to initiating active learning-based data collection.

</details>


### [1196] [Maximizing Reliability with Bayesian Optimization](https://arxiv.org/abs/2602.02432)
*Jack M. Buckingham,Ivo Couckuyt,Juergen Branke*

Main category: cs.LG

Relevance: 35.0

TL;DR: 提出两种基于贝叶斯优化的方法（Thompson采样和知识梯度），用于极小失效概率（10^-6-10^-8）的可靠性优化问题，结合重要性采样提升效率。


<details>
  <summary>Details</summary>
Motivation: 制造业中需要优化设计的可靠性，即最小化随机扰动下的失效概率。传统方法在处理极小失效概率（10^-6-10^-8）时效率低下，需要更有效的贝叶斯优化方法。

Method: 提出两种贝叶斯优化方法：1）基于Thompson采样的方法；2）基于知识梯度的方法，后者近似最小化失效概率对数的一步贝叶斯最优策略。两种方法都结合重要性采样来处理极小失效概率。

Result: 实验结果表明，提出的方法在极端（极小失效概率）和非极端情况下都优于现有方法。

Conclusion: 提出的基于Thompson采样和知识梯度的贝叶斯优化方法，结合重要性采样，能有效处理极小失效概率的可靠性优化问题，在极端和非极端情况下均表现优异。

Abstract: Bayesian optimization (BO) is a popular, sample-efficient technique for expensive, black-box optimization. One such problem arising in manufacturing is that of maximizing the reliability, or equivalently minimizing the probability of a failure, of a design which is subject to random perturbations - a problem that can involve extremely rare failures ($P_\mathrm{fail} = 10^{-6}-10^{-8}$). In this work, we propose two BO methods based on Thompson sampling and knowledge gradient, the latter approximating the one-step Bayes-optimal policy for minimizing the logarithm of the failure probability. Both methods incorporate importance sampling to target extremely small failure probabilities. Empirical results show the proposed methods outperform existing methods in both extreme and non-extreme regimes.

</details>


### [1197] [Conflict-Aware Client Selection for Multi-Server Federated Learning](https://arxiv.org/abs/2602.02458)
*Mingwei Hong,Zheng Lin,Zehang Lin,Lin Li,Miao Yang,Xia Du,Zihan Fang,Zhaolu Kang,Dianxin Luan,Shunzhi Zhu*

Main category: cs.LG

Relevance: 35.0

TL;DR: 提出RL-CRP框架，通过冲突风险预测和去中心化强化学习优化多服务器联邦学习中的客户端选择，减少服务器间冲突并提高训练效率


<details>
  <summary>Details</summary>
Motivation: 传统单服务器联邦学习存在高通信延迟问题，而多服务器联邦学习虽然能分散工作负载，但客户端覆盖重叠和选择不协调会导致资源竞争、带宽冲突和训练失败，需要优化客户端选择策略

Method: 提出RL-CRP框架：1) 使用分类隐马尔可夫模型基于稀疏历史选择序列预测客户端选择冲突风险；2) 结合公平感知奖励机制促进长期客户端参与；3) 采用去中心化强化学习优化客户端选择

Result: 实验表明RL-CRP框架能有效减少服务器间冲突，显著提高训练效率，包括更快的收敛速度和更低的通信成本

Conclusion: RL-CRP框架通过冲突风险预测和公平感知强化学习，为多服务器联邦学习系统提供了有效的客户端选择优化方案，解决了资源竞争问题并提升了整体训练性能

Abstract: Federated learning (FL) has emerged as a promising distributed machine learning (ML) that enables collaborative model training across clients without exposing raw data, thereby preserving user privacy and reducing communication costs. Despite these benefits, traditional single-server FL suffers from high communication latency due to the aggregation of models from a large number of clients. While multi-server FL distributes workloads across edge servers, overlapping client coverage and uncoordinated selection often lead to resource contention, causing bandwidth conflicts and training failures. To address these limitations, we propose a decentralized reinforcement learning with conflict risk prediction, named RL CRP, to optimize client selection in multi-server FL systems. Specifically, each server estimates the likelihood of client selection conflicts using a categorical hidden Markov model based on its sparse historical client selection sequence. Then, a fairness-aware reward mechanism is incorporated to promote long-term client participation for minimizing training latency and resource contention. Extensive experiments demonstrate that the proposed RL-CRP framework effectively reduces inter-server conflicts and significantly improves training efficiency in terms of convergence speed and communication cost.

</details>


### [1198] [MEG-XL: Data-Efficient Brain-to-Text via Long-Context Pre-Training](https://arxiv.org/abs/2602.02494)
*Dulhan Jayalath,Oiwi Parker Jones*

Main category: cs.LG

Relevance: 35.0

TL;DR: MEG-XL：一种使用2.5分钟MEG上下文进行预训练的脑机接口模型，比现有方法长5-300倍，在单词解码任务上仅需少量数据即可达到监督学习性能。


<details>
  <summary>Details</summary>
Motivation: 临床脑机接口需要为瘫痪患者设计，这些患者无法提供大量训练数据。现有预训练方法通常只使用几秒的上下文，而自然语音可能持续数分钟，因此需要更长的神经上下文来学习更好的统计先验。

Method: 提出MEG-XL模型，使用2.5分钟MEG上下文（相当于191k tokens）进行预训练，比现有方法长5-300倍。然后在下游的脑数据单词解码任务上进行微调。

Result: MEG-XL仅需少量数据（如1小时vs 50小时）即可达到监督学习性能，并优于其他脑基础模型。研究发现，使用更长上下文预训练的模型学习到的表示在单词解码任务上迁移效果更好。

Conclusion: 长上下文预训练有助于利用其他方法不必要丢弃的扩展神经上下文，对于数据高效的脑机接口系统具有重要意义。

Abstract: Clinical brain-to-text interfaces are designed for paralysed patients who cannot provide extensive training recordings. Pre-training improves data-efficient generalisation by learning statistical priors across subjects, but these priors critically depend on context. While natural speech might unfold gradually over minutes, most methods pre-train with only a few seconds of context. Thus, we propose MEG-XL, a model pre-trained with 2.5 minutes of MEG context per sample, 5-300x longer than prior work, and equivalent to 191k tokens, capturing extended neural context. Fine-tuning on the task of word decoding from brain data, MEG-XL matches supervised performance with a fraction of the data (e.g. 1hr vs 50hrs) and outperforms brain foundation models. We find that models pre-trained with longer contexts learn representations that transfer better to word decoding. Our results indicate that long-context pre-training helps exploit extended neural context that other methods unnecessarily discard. Code, model weights, and instructions are available at https://github.com/neural-processing-lab/MEG-XL .

</details>


### [1199] [Exploring the Interpretability of Forecasting Models for Energy Balancing Market](https://arxiv.org/abs/2602.00049)
*Oskar Våle,Shiliang Zhang,Sabita Maharjan,Gro Klæboe*

Main category: q-fin.ST

Relevance: 35.0

TL;DR: 该研究探索能源平衡市场中模型准确性与可解释性的权衡，以手动频率恢复储备(mFRR)激活价格预测为例，比较XGBoost、EBM及其组合模型，发现EBM在保持可比准确性的同时提供显著可解释性。


<details>
  <summary>Details</summary>
Motivation: 能源平衡市场对电网稳定性和能源安全至关重要，但复杂机器学习模型的黑箱特性限制了可解释性。研究旨在探索准确性与可解释性之间的平衡，为平衡市场预测提供透明解决方案。

Method: 使用真实市场数据预测mFRR激活价格，比较极端梯度提升(XGBoost)、可解释提升机(EBM)及其组合模型，并以朴素模型为基准。特别关注价格显著偏离现货价格时的预测挑战。

Result: EBM提供与XGBoost相当的预测准确性，同时具有显著可解释性。EBM的可解释特性揭示了mFRR价格的非线性驱动因素和区域市场动态，特别是在价格显著偏离现货价格时。

Conclusion: EBM是平衡市场预测中复杂黑箱AI模型的可行且有价值的可解释替代方案，在保持准确性的同时提供市场洞察。

Abstract: The balancing market in the energy sector plays a critical role in physically and financially balancing the supply and demand. Modeling dynamics in the balancing market can provide valuable insights and prognosis for power grid stability and secure energy supply. While complex machine learning models can achieve high accuracy, their black-box nature severely limits the model interpretability. In this paper, we explore the trade-off between model accuracy and interpretability for the energy balancing market. Particularly, we take the example of forecasting manual frequency restoration reserve (mFRR) activation price in the balancing market using real market data from different energy price zones. We explore the interpretability of mFRR forecasting using two models: extreme gradient boosting (XGBoost) machine and explainable boosting machine (EBM). We also integrate the two models, and we benchmark all the models against a baseline naive model. Our results show that EBM provides forecasting accuracy comparable to XGBoost while yielding a considerable level of interpretability. Our analysis also underscores the challenge of accurately predicting the mFRR price for the instances when the activation price deviates significantly from the spot price. Importantly, EBM's interpretability features reveal insights into non-linear mFRR price drivers and regional market dynamics. Our study demonstrates that EBM is a viable and valuable interpretable alternative to complex black-box AI models in the forecast for the balancing market.

</details>


### [1200] [On finite-dimensional encoding/decoding theorems for neural operators](https://arxiv.org/abs/2602.00068)
*Vinícius Luz Oliveira,Vladimir G. Pestov*

Main category: math.FA

Relevance: 35.0

TL;DR: 该论文证明了无限维神经算子网络的有限维逼近定理：对于任意局部凸空间（无需额外假设），连续映射可通过有限维空间因子化逼近；但对于C^k光滑映射，仅当空间具有逼近性质时才成立。


<details>
  <summary>Details</summary>
Motivation: 神经算子网络在微分方程求解中应用广泛，但实际计算需要有限维逼近。现有理论要求Banach空间具有逼近性质，限制了在非规范局部凸函数空间（微分方程理论中常见）的应用。本文旨在放宽这些限制条件。

Method: 通过泛函分析和拓扑学方法，重新分析神经算子网络的逼近定理。证明对于连续映射，结果对任意局部凸空间都成立（无需逼近性质）；对于C^k光滑映射，则给出充要条件：当且仅当空间具有逼近性质时定理成立。

Result: 1. 连续映射的逼近定理对任意局部凸空间都成立，无需任何额外假设
2. C^k光滑映射的逼近定理成立当且仅当空间E具有逼近性质
3. 结果适用于非规范局部凸函数空间，这在微分方程理论中很常见

Conclusion: 本文显著扩展了神经算子网络逼近定理的适用范围，特别是对于微分方程理论中常见的非规范局部凸函数空间。这为神经算子在实际计算中的应用提供了更坚实的理论基础。

Abstract: Recently, versions of neural networks with infinite-dimensional affine operators inside the computational units (``neural operator'' networks) have been applied to learn solutions to differential equations. To enable practical computations, one employs finite-dimensional encoding/decoding theorems of the following kind: every continuous mapping $f$ between function spaces $E$ and $F$ is approximated in the topology of uniform convergence on compacta by continuous mappings factoring through two finite dimensional Banach spaces. Such a result is known (Kovachki et al., 2023) for $E,F$ being Banach spaces having the approximation property. We point out that the result needs no assumptions on $E,F$ whatsoever and remains true not only for all normed spaces, but for arbitrary locally convex spaces as well. At the same time, an analogous result for $C^k$-smooth mappings and the $C^k$ compact open topology, $k\geq 1$, holds if and only if the space $E$ has the approximation property. This analysis may be useful already because non-normable locally convex function spaces are common in the theory of differential equations, the main field of applications for the emerging theory.

</details>


### [1201] [Repair Brain Damage: Real-Numbered Error Correction Code for Neural Network](https://arxiv.org/abs/2602.00076)
*Ziqing Li,Myung Cho,Qiutong Jin,Weiyu Xu*

Main category: cs.NE

Relevance: 35.0

TL;DR: 提出一种基于实数的纠错码，用于检测和纠正神经网络中的内存错误和计算错误，通过实数线性约束实现，不牺牲分类性能或增加参数数量。


<details>
  <summary>Details</summary>
Motivation: 神经网络在实际部署中可能遇到内存故障和计算错误，这些硬件错误会影响模型的可靠性和安全性。现有方法通常无法同时处理内存和计算错误，或者会牺牲模型性能。需要一种能够在不增加参数的情况下检测和纠正这些错误的方法。

Method: 提出基于实数的纠错码，通过在神经网络权重上引入实数线性约束结构来实现错误检测和纠正。这些约束被设计为能够识别内存错误（权重值错误）和计算错误（前向传播中的错误），同时保持原始网络的分类性能。

Result: 该方法能够在神经网络中同时检测和纠正内存错误和计算错误，而不需要增加实数参数的数量。实验表明，该方法在保持分类准确率的同时，提供了有效的错误恢复能力。

Conclusion: 提出的基于实数的纠错码为神经网络提供了一种有效的容错机制，能够在硬件错误发生时保护模型性能，这对于在安全关键应用中部署可靠的神经网络系统具有重要意义。

Abstract: We consider a neural network (NN) that may experience memory faults and computational errors. In this paper, we propose a novel real-number-based error correction code (ECC) capable of detecting and correcting both memory errors and computational errors. The proposed approach introduces structures in the form of real-number-based linear constraints on the NN weights to enable error detection and correction, without sacrificing classification performance or increasing the number of real-valued NN parameters.

</details>


### [1202] [The GT-Score: A Robust Objective Function for Reducing Overfitting in Data-Driven Trading Strategies](https://arxiv.org/abs/2602.00080)
*Alexander Sheppert*

Main category: q-fin.ST

Relevance: 35.0

TL;DR: 本文提出GT-Score，一种集成性能、统计显著性、一致性和下行风险的复合目标函数，用于指导开发更稳健的交易策略，直接解决量化策略开发中的数据窥探和非正态分布下的统计推断不可靠问题。


<details>
  <summary>Details</summary>
Motivation: 数据驱动的金融建模中存在严重的过拟合问题，机器学习系统学习历史价格中的虚假模式，导致样本外和部署时失败。需要解决量化策略开发中的关键陷阱，特别是优化过程中的数据窥探和非正态收益分布下的统计推断不可靠性。

Method: 提出GT-Score复合目标函数，集成四个维度：性能、统计显著性、一致性和下行风险。使用2010-2024年50家标普500公司历史股票数据进行实证评估，包括九个连续时间分割的向前验证和三个交易策略上15个随机种子的蒙特卡洛研究。

Result: 在向前验证中，GT-Score将泛化比率（验证收益除以训练收益）相对于基线目标函数提高了98%。蒙特卡洛样本外收益的配对统计检验显示目标函数之间存在统计可检测差异（与Sortino和Simple比较p<0.01），效应量较小。

Conclusion: 将抗过拟合结构嵌入目标函数可以提高量化研究中回测的可靠性。GT-Score为开发更稳健的交易策略提供了有效方法。

Abstract: Overfitting remains a critical challenge in data-driven financial modeling, where machine learning (ML) systems learn spurious patterns in historical prices and fail out of sample and in deployment. This paper introduces the GT-Score, a composite objective function that integrates performance, statistical significance, consistency, and downside risk to guide optimization toward more robust trading strategies. This approach directly addresses critical pitfalls in quantitative strategy development, specifically data snooping during optimization and the unreliability of statistical inference under non-normal return distributions. Using historical stock data for 50 S&P 500 companies spanning 2010-2024, we conduct an empirical evaluation that includes walk-forward validation with nine sequential time splits and a Monte Carlo study with 15 random seeds across three trading strategies. In walk-forward validation, GT-Score improves the generalization ratio (validation return divided by training return) by 98% relative to baseline objective functions. Paired statistical tests on Monte Carlo out-of-sample returns indicate statistically detectable differences between objective functions (p < 0.01 for comparisons with Sortino and Simple), with small effect sizes. These results suggest that embedding an anti-overfitting structure into the objective can improve the reliability of backtests in quantitative research. Reproducible code and processed result files are provided as supplementary materials.

</details>


### [1203] [Early warning prediction: Onsager-Machlup vs Schrödinger](https://arxiv.org/abs/2602.00143)
*Xiaoai Xu,Yixuan Zhou,Xiang Zhou,Jingqiao Duan,Ting Gao*

Main category: q-bio.QM

Relevance: 35.0

TL;DR: 提出一种结合流形学习和随机动力系统建模的癫痫发作早期预警框架，通过降维表示和概率演化评分函数，实现更早、更敏感的关键状态转换预测。


<details>
  <summary>Details</summary>
Motivation: 复杂系统（如大脑癫痫发作）中的关键转换预测是重大科学挑战，高维特性和隐藏的关键信号使早期预警任务复杂化。需要从高维数据中提取早期预警信号的理论框架和方法。

Method: 1) 使用扩散映射等6种方法构建低维表示；2) 建立数据驱动的随机微分方程模型估计系统概率演化评分函数；3) 结合薛定谔桥理论定义新的评分函数指标，量化系统显著状态转换的可能性。

Result: 该指标在癫痫预测中表现出更高的敏感性和鲁棒性，能够更早识别关键点，清晰捕捉癫痫发作前后各阶段的动态特征。

Conclusion: 该工作为从高维数据中提取早期预警信号提供了系统的理论框架和实用方法，在复杂系统关键转换预测方面有重要应用价值。

Abstract: Predicting critical transitions in complex systems, such as epileptic seizures in the brain, represents a major challenge in scientific research. The high-dimensional characteristics and hidden critical signals further complicate early-warning tasks. This study proposes a novel early-warning framework that integrates manifold learning with stochastic dynamical system modeling. Through systematic comparison, six methods including diffusion maps (DM) are selected to construct low-dimensional representations. Based on these, a data-driven stochastic differential equation model is established to robustly estimate the probability evolution scoring function of the system. Building on this, a new Score Function (SF) indicator is defined by incorporating Schrödinger bridge theory to quantify the likelihood of significant state transitions in the system. Experiments demonstrate that this indicator exhibits higher sensitivity and robustness in epilepsy prediction, enables earlier identification of critical points, and clearly captures dynamic features across various stages before and after seizure onset. This work provides a systematic theoretical framework and practical methodology for extracting early-warning signals from high-dimensional data.

</details>


### [1204] [Shuffle and Joint Differential Privacy for Generalized Linear Contextual Bandits](https://arxiv.org/abs/2602.00417)
*Sahasrajit Sarmasarkar*

Main category: stat.ML

Relevance: 35.0

TL;DR: 论文提出了在洗牌差分隐私和联合差分隐私下的广义线性上下文赌博机算法，解决了GLM在隐私保护下的挑战，包括无闭式解估计器、跨多个设计矩阵的隐私跟踪以及优化误差对遗憾分析的影响。


<details>
  <summary>Details</summary>
Motivation: 现有隐私保护上下文赌博机研究仅限于线性奖励模型（具有闭式解估计器），而广义线性模型（GLMs）带来新的挑战：无闭式解估计器需要私有凸优化、隐私需要在多个演化设计矩阵中跟踪、优化误差必须明确纳入遗憾分析。

Method: 针对两种隐私模型和上下文设置：对于随机上下文，设计了洗牌差分隐私算法；对于对抗性上下文，提供了联合差分隐私算法。两种算法都消除了对实例特定参数κ的依赖，且不需要超出ℓ2有界性的谱假设。

Result: 随机上下文洗牌差分隐私算法达到$\tilde{O}(d^{3/2}\sqrt{T}/\sqrt{\varepsilon})$遗憾；对抗性上下文联合差分隐私算法达到$\tilde{O}(d\sqrt{T}/\sqrt{\varepsilon})$遗憾，与非私有率相比仅多$1/\sqrt{\varepsilon}$因子。

Conclusion: 首次在洗牌差分隐私和联合差分隐私下解决了广义线性上下文赌博机问题，为隐私保护强化学习提供了新方法，特别是在需要保护用户数据的实际应用中。

Abstract: We present the first algorithms for generalized linear contextual bandits under shuffle differential privacy and joint differential privacy. While prior work on private contextual bandits has been restricted to linear reward models -- which admit closed-form estimators -- generalized linear models (GLMs) pose fundamental new challenges: no closed-form estimator exists, requiring private convex optimization; privacy must be tracked across multiple evolving design matrices; and optimization error must be explicitly incorporated into regret analysis.
  We address these challenges under two privacy models and context settings. For stochastic contexts, we design a shuffle-DP algorithm achieving $\tilde{O}(d^{3/2}\sqrt{T}/\sqrt{\varepsilon})$ regret. For adversarial contexts, we provide a joint-DP algorithm with $\tilde{O}(d\sqrt{T}/\sqrt{\varepsilon})$ regret -- matching the non-private rate up to a $1/\sqrt{\varepsilon}$ factor. Both algorithms remove dependence on the instance-specific parameter $κ$ (which can be exponential in dimension) from the dominant $\sqrt{T}$ term. Unlike prior work on locally private GLM bandits, our methods require no spectral assumptions on the context distribution beyond $\ell_2$ boundedness.

</details>


### [1205] [Surrogate Ensemble in Expensive Multi-Objective Optimization via Deep Q-Learning](https://arxiv.org/abs/2602.00540)
*Yuxin Wu,Hongshu Guo,Ting Huang,Yue-Jiao Gong,Zeyuan Ma*

Main category: cs.NE

Relevance: 35.0

TL;DR: SEEMOO是一个基于强化学习的集成框架，用于在昂贵多目标优化问题中动态调度不同的代理模型，通过深度Q网络选择当前最优的代理模型来提升整体优化性能。


<details>
  <summary>Details</summary>
Motivation: 现有SAEAs中的代理模型选择主要由人工决定，这引入了强烈的偏见，可能损害算法在超出范围任务上的预期性能。特别是在昂贵多目标优化问题中，多个目标函数形成了复杂的组合景观，对代理模型选择提出了挑战。

Method: SEEMOO包含三个核心设计：1) 预收集的模型池维护不同的代理模型；2) 基于注意力的状态提取器支持不同目标数量问题的通用优化状态表示；3) 深度Q网络作为动态代理选择器，根据优化状态选择当前步骤评估所需的代理模型。该框架在训练问题分布下最大化整体优化性能。

Result: 广泛的基准测试结果表明，SEEMOO的代理集成范式显著提升了单代理基线的优化性能。进一步的消融研究强调了SEEMOO设计组件的重要性。

Conclusion: SEEMOO通过强化学习驱动的动态代理模型调度，有效解决了昂贵多目标优化问题中代理模型选择的挑战，展示了优于传统人工选择方法的性能。

Abstract: Surrogate-assisted Evolutionary Algorithms~(SAEAs) have shown promising robustness in solving expensive optimization problems. A key aspect that impacts SAEAs' effectiveness is surrogate model selection, which in existing works is predominantly decided by human developer. Such human-made design choice introduces strong bias into SAEAs and may hurt their expected performance on out-of-scope tasks. In this paper, we propose a reinforcement learning-assisted ensemble framework, termed as SEEMOO, which is capable of scheduling different surrogate models within a single optimization process, hence boosting the overall optimization performance in a cooperative paradigm. Specifically, we focus on expensive multi-objective optimization problems, where multiple objective functions shape a compositional landscape and hence challenge surrogate selection. SEEMOO comprises following core designs: 1) A pre-collected model pool that maintains different surrogate models; 2) An attention-based state-extractor supports universal optimization state representation of problems with varied objective numbers; 3) a deep Q-network serves as dynamic surrogate selector: Given the optimization state, it selects desired surrogate model for current-step evaluation. SEEMOO is trained to maximize the overall optimization performance under a training problem distribution. Extensive benchmark results demonstrate SEEMOO's surrogate ensemble paradigm boosts the optimization performance of single-surrogate baselines. Further ablation studies underscore the importance of SEEMOO's design components.

</details>


### [1206] [Score-based Metropolis-Hastings for Fractional Langevin Algorithms](https://arxiv.org/abs/2602.00835)
*Ahmed Aloui,Junyi Liao,Ali Hasan,Jose Blanchet,Vahid Tarokh*

Main category: stat.ML

Relevance: 35.0

TL;DR: 提出MAFLA方法，一种基于分数的Metropolis-Hastings校正机制，用于解决α稳定Lévy驱动的分数Langevin算法中无法评估目标密度和提议密度的采样问题。


<details>
  <summary>Details</summary>
Motivation: 在α稳定Lévy驱动的分数Langevin算法中，当目标密度和提议密度都无法评估时，传统的基于密度的Metropolis-Hastings校正变得不可行。现有方法在未校正状态下运行，导致显著的有限时间误差和对尾部行为的控制不佳。

Method: 提出Metropolis-Adjusted Fractional Langevin Algorithm (MAFLA)，这是一种基于分数的MH校正机制。使用各向同性对称α稳定噪声下的分数提议分数梯度设计代理，并通过Score Balance Matching学习接受函数。

Result: 在包括组合优化问题在内的一系列任务上，MAFLA表现出强大性能，相比未校正的分数Langevin动力学，显著提高了有限时间采样精度。

Conclusion: MAFLA为解决分数Langevin算法中的采样挑战提供了一种有效的基于分数的校正方法，显著改进了采样精度。

Abstract: Sampling from heavy-tailed and multimodal distributions is challenging when neither the target density nor the proposal density can be evaluated, as in $α$-stable Lévy-driven fractional Langevin algorithms. While the target distribution can be estimated from data via score-based or energy-based models, the $α$-stable proposal density and its score are generally unavailable, rendering classical density-based Metropolis--Hastings (MH) corrections impractical. Consequently, existing fractional Langevin methods operate in an unadjusted regime and can exhibit substantial finite-time errors and poor empirical control of tail behavior. We introduce the Metropolis-Adjusted Fractional Langevin Algorithm (MAFLA), an MH-inspired, fully score-based correction mechanism. MAFLA employs designed proxies for fractional proposal score gradients under isotropic symmetric $α$-stable noise and learns an acceptance function via Score Balance Matching. We empirically illustrate the strong performance of MAFLA on a series of tasks including combinatorial optimization problems where the method significantly improves finite time sampling accuracy over unadjusted fractional Langevin dynamics.

</details>


### [1207] [Multivariate Time Series Data Imputation via Distributionally Robust Regularization](https://arxiv.org/abs/2602.00844)
*Che-Yi Liao,Zheng Dong,Gian-Gabriel Garcia,Kamran Paynabar*

Main category: stat.ML

Relevance: 35.0

TL;DR: 提出DRIO方法，通过分布鲁棒正则化解决多元时间序列插补中的分布偏差问题，在随机缺失和非随机缺失场景下均能提升性能。


<details>
  <summary>Details</summary>
Motivation: 多元时间序列插补常受观测数据与真实数据分布不匹配的影响，这种偏差在非平稳性和系统性缺失情况下更加严重。传统方法最小化重构误差或鼓励分布对齐，容易对偏差观测过拟合。

Method: 提出分布鲁棒正则化插补目标(DRIO)，联合最小化重构误差和插补器与Wasserstein模糊集内最坏情况分布之间的散度。推导出可处理的对偶形式，将测度上的无限维优化转化为样本轨迹上的对抗搜索，并提出与灵活深度学习骨干兼容的对抗学习算法。

Result: 在多样化的真实世界数据集上的综合实验表明，DRIO在随机缺失和非随机缺失设置下均能持续改进插补性能，在重构精度和分布对齐之间达到帕累托最优权衡。

Conclusion: DRIO通过分布鲁棒优化框架有效解决了时间序列插补中的分布偏差问题，为处理非平稳和系统性缺失数据提供了更可靠的解决方案。

Abstract: Multivariate time series (MTS) imputation is often compromised by mismatch between observed and true data distributions -- a bias exacerbated by non-stationarity and systematic missingness. Standard methods that minimize reconstruction error or encourage distributional alignment risk overfitting these biased observations. We propose the Distributionally Robust Regularized Imputer Objective (DRIO), which jointly minimizes reconstruction error and the divergence between the imputer and a worst-case distribution within a Wasserstein ambiguity set. We derive a tractable dual formulation that reduces infinite-dimensional optimization over measures to adversarial search over sample trajectories, and propose an adversarial learning algorithm compatible with flexible deep learning backbones. Comprehensive experiments on diverse real-world datasets show DRIO consistently improves imputation under both missing-completely-at-random and missing-not-at-random settings, reaching Pareto-optimal trade-offs between reconstruction accuracy and distributional alignment.

</details>


### [1208] [On the Convergence of Jacobian-Free Backpropagation for Optimal Control Problems with Implicit Hamiltonians](https://arxiv.org/abs/2602.00921)
*Eric Gelphman,Deepanshu Verma,Nicole Tianjiao Yang,Stanley Osher,Samy Wu Fung*

Main category: math.OC

Relevance: 35.0

TL;DR: 本文为隐式哈密顿量最优控制中的Jacobian-Free Backpropagation (JFB)方法建立了随机小批量设置的收敛保证，并在高维问题上验证了其可扩展性。


<details>
  <summary>Details</summary>
Motivation: 隐式哈密顿量最优控制中缺乏闭式最优控制律，给基于学习的值函数方法带来挑战。现有JFB方法仅有样本级下降保证，需要建立更严格的收敛理论。

Method: 使用Jacobian-Free Backpropagation (JFB)方法处理隐式哈密顿量最优控制问题，在随机小批量设置下分析其收敛性。

Result: 证明了JFB更新收敛到期望最优控制目标的驻点，并在多智能体最优消费、群体四旋翼和自行车控制等高维问题上展示了可扩展性。

Conclusion: JFB为高维隐式哈密顿量最优控制提供了理论依据和实证支持，填补了现有方法的理论空白。

Abstract: Optimal feedback control with implicit Hamiltonians poses a fundamental challenge for learning-based value function methods due to the absence of closed-form optimal control laws. Recent work~\cite{gelphman2025end} introduced an implicit deep learning approach using Jacobian-Free Backpropagation (JFB) to address this setting, but only established sample-wise descent guarantees. In this paper, we establish convergence guarantees for JFB in the stochastic minibatch setting, showing that the resulting updates converge to stationary points of the expected optimal control objective. We further demonstrate scalability on substantially higher-dimensional problems, including multi-agent optimal consumption and swarm-based quadrotor and bicycle control. Together, our results provide both theoretical justification and empirical evidence for using JFB in high-dimensional optimal control with implicit Hamiltonians.

</details>


### [1209] [Improving Minimax Estimation Rates for Contaminated Mixture of Multinomial Logistic Experts via Expert Heterogeneity](https://arxiv.org/abs/2602.00939)
*Fanqi Yan,Dung Le,Trang Pham,Huy Nguyen,Nhat Ho*

Main category: math.ST

Relevance: 35.0

TL;DR: 该论文首次对具有同质和异质结构的污染多项式逻辑专家混合模型进行收敛分析，建立了参数估计的一致收敛速率和极小极大下界，证明了异质专家结构比同质结构具有更快的参数估计速率和更高的样本效率。


<details>
  <summary>Details</summary>
Motivation: 污染专家混合模型在迁移学习中应用广泛，但现有研究存在两个主要问题：1）仅在回归设置中研究，分类设置的理论基础缺失；2）现有分类模型只获得点态收敛速率，缺乏极小最优性保证。本文旨在填补这些理论空白。

Method: 对具有同质和异质结构的污染多项式逻辑专家混合模型进行收敛分析，在真实参数随样本量变化的挑战性设置下，建立参数估计的一致收敛速率，并建立相应的极小极大下界以证明最优性。

Result: 证明了污染专家混合模型在分类设置中的收敛理论，获得了参数估计的一致收敛速率，并建立了极小极大最优性。关键发现是：专家异质性比同质性具有更快的参数估计速率和更高的样本效率。

Conclusion: 本文首次为污染多项式逻辑专家混合模型建立了完整的收敛理论框架，填补了分类设置中的理论空白，并为专家混合模型的设计提供了重要指导：异质专家结构比同质结构更优。

Abstract: Contaminated mixture of experts (MoE) is motivated by transfer learning methods where a pre-trained model, acting as a frozen expert, is integrated with an adapter model, functioning as a trainable expert, in order to learn a new task. Despite recent efforts to analyze the convergence behavior of parameter estimation in this model, there are still two unresolved problems in the literature. First, the contaminated MoE model has been studied solely in regression settings, while its theoretical foundation in classification settings remains absent. Second, previous works on MoE models for classification capture pointwise convergence rates for parameter estimation without any guaranty of minimax optimality. In this work, we close these gaps by performing, for the first time, the convergence analysis of a contaminated mixture of multinomial logistic experts with homogeneous and heterogeneous structures, respectively. In each regime, we characterize uniform convergence rates for estimating parameters under challenging settings where ground-truth parameters vary with the sample size. Furthermore, we also establish corresponding minimax lower bounds to ensure that these rates are minimax optimal. Notably, our theories offer an important insight into the design of contaminated MoE, that is, expert heterogeneity yields faster parameter estimation rates and, therefore, is more sample-efficient than expert homogeneity.

</details>


### [1210] [The Stacked Autoencoder Evolution Hypothesis](https://arxiv.org/abs/2602.01026)
*Hiroyuki Iizuka*

Main category: cs.NE

Relevance: 35.0

TL;DR: 提出"堆叠自编码器演化假说"，认为生物进化系统通过多层自编码和解码过程运作，类似于深度学习中的堆叠自编码器，能够解释间断平衡等进化模式。


<details>
  <summary>Details</summary>
Motivation: 传统进化理论主要关注突变和选择的渐进变化，但难以解释进化中的间断平衡现象和看似目标导向的大规模表型变化。作者希望从信息处理的角度，为连续和不连续的进化变化提供新的理论框架。

Method: 提出理论框架"堆叠自编码器演化假说"，并通过人工化学模拟来验证该机制的合理性，展示分层自编码器结构的自发涌现。

Result: 人工化学模拟证实了分层自编码器结构能够自发形成，支持了该理论框架的合理性。该框架为理解进化中的连续和不连续变化提供了新的信息动力学视角。

Conclusion: 生物进化系统可能通过类似堆叠自编码器的多层信息处理机制运作，这能解释进化中的间断平衡现象和大规模表型变化，为进化生物学提供了新的理论视角。

Abstract: This study introduces a novel theoretical framework, the Stacked Autoencoder Evolution Hypothesis, which proposes that biological evolutionary systems operate through multi-layered self-encoding and decoding processes, analogous to stacked autoencoders in deep learning. Rather than viewing evolution solely as gradual changes driven by mutation and selection, this hypothesis suggests that self-replication inherently compresses and reconstructs genetic information across hierarchical layers of abstraction. This layered structure enables evolutionary systems to explore diverse possibilities not only at the sequence level but also across progressively more abstract layers of representation, making it possible for even simple mutations to navigate these higher-order spaces.Such a mechanism may explain punctuated evolutionary patterns and changes that can appear as if they are goal-directed in natural evolution, by allowing mutations at deeper latent layers to trigger sudden, large-scale phenotypic shifts. To illustrate the plausibility of this mechanism, artificial chemistry simulations were conducted, demonstrating the spontaneous emergence of hierarchical autoencoder structures. This framework offers a new perspective on the informational dynamics underlying both continuous and discontinuous evolutionary change.

</details>


### [1211] [Robust Sublinear Convergence Rates for Iterative Bregman Projections](https://arxiv.org/abs/2602.01372)
*Gabriel Peyré*

Main category: math.OC

Relevance: 35.0

TL;DR: 论文提出了一种基于熵正则化的线性规划近似方法，通过交替KL投影实现O(1/k)收敛率，常数仅线性依赖于正则化参数，扩展了最优传输的Sinkhorn算法到更一般的线性约束问题。


<details>
  <summary>Details</summary>
Motivation: 熵正则化为具有可分约束的线性规划提供了一种简单的近似方法，但现有收敛分析主要局限于最优传输问题。本文旨在将Sinkhorn算法的收敛保证扩展到更广泛的线性约束问题，特别是那些约束可分解为两个或多个可处理块的问题。

Method: 采用交替Kullback-Leibler Bregman投影方法，通过熵正则化将原始线性规划问题转化为可分目标函数。关键创新是使用块商对偶半范数来度量对偶半径，这考虑了约束块分解的结构特性。

Result: 证明了对偶目标函数以O(1/k)速率下降，且常数仅线性依赖于正则化参数1/γ。这种"鲁棒"收敛率确保了在逼近未正则化问题时的良好复杂度界限。应用方面，提出了图上的Wasserstein-1距离的flow-Sinkhorn算法。

Conclusion: 该工作将熵正则化和交替KL投影的收敛保证从最优传输扩展到更广泛的线性约束问题，为近似线性规划提供了一种通用且高效的方法，特别适用于约束可分的情况。

Abstract: Entropic regularization provides a simple way to approximate linear programs whose constraints split into two (or more) tractable blocks. The resulting objectives are amenable to cyclic Kullback-Leibler (KL) Bregman projections, with the classical Sinkhorn algorithm for optimal transport (balanced, unbalanced, gradient flows, barycenters, \dots) as the canonical example. Assuming uniformly bounded primal mass and dual radius, we prove that the dual objective of these KL projections decreases at an $O(1/k)$ rate with a constant that scales only linearly in $1/γ$, where $γ$ is the entropic regularization parameter. This extends the guarantees known for entropic optimal transport to any such linearly constrained problem. Following the terminology introduced in [Chizat et al 2025], we call such rates "robust", because this mild dependence on $γ$ underpins favorable complexity bounds for approximating the unregularized problem via alternating KL projections. The crucial aspect of the analysis is that the dual radius should be measured according to a block-quotient dual seminorm, which depends on the structure of the split of the constraint into blocks. As an application, we derive the flow-Sinkhorn algorithm for the Wasserstein-1 distance on graphs. It achieves $ε$-additive accuracy on the transshipment cost in $O(p/ε^{4})$ arithmetic operations, where $p$ is the number of edges.

</details>


### [1212] [The Enhanced Physics-Informed Kolmogorov-Arnold Networks: Applications of Newton's Laws in Financial Deep Reinforcement Learning (RL) Algorithms](https://arxiv.org/abs/2602.01388)
*Trang Thoi,Hung Tran,Tram Thoi,Huaiyang Zhong*

Main category: cs.CE

Relevance: 35.0

TL;DR: 提出一种结合物理信息Kolmogorov-Arnold网络(PIKANs)的强化学习框架用于投资组合优化，在多个股票市场中相比传统DRL方法获得更高收益和更优风险调整后表现。


<details>
  <summary>Details</summary>
Motivation: 传统深度强化学习在金融交易中存在不稳定性和泛化能力差的问题，特别是在动态噪声市场中。需要更高效、可解释且稳定的方法来进行投资组合优化。

Method: 将Kolmogorov-Arnold网络(KANs)集成到DRL算法中，替代传统的多层感知机。在actor更新中引入物理信息正则化损失，促进观测收益动态与投资组合调整之间的二阶时间一致性。使用可学习的B样条单变量函数实现参数高效和更可解释的函数逼近。

Result: 在中国、越南和美国三个股票市场(涵盖新兴和发达经济体)的评估中，PIKAN-based agents在所有市场中均获得更高的累计和年化收益、更优的夏普和卡尔玛比率、更有利的回撤特性，相比标准DRL基线和经典在线投资组合选择方法。

Conclusion: PIKAN框架在高度动态和噪声的金融市场中特别有价值，能够提供更稳定的训练、更高的夏普比率和更优的性能，解决了传统DRL的不稳定性和泛化问题。

Abstract: Deep Reinforcement Learning (DRL), a subset of machine learning focused on sequential decision-making, has emerged as a powerful approach for tackling financial trading problems. In finance, DRL is commonly used either to generate discrete trade signals or to determine continuous portfolio allocations. In this work, we propose a novel reinforcement learning framework for portfolio optimization that incorporates Physics-Informed Kolmogorov-Arnold Networks (PIKANs) into several DRL algorithms. The approach replaces conventional multilayer perceptrons with Kolmogorov-Arnold Networks (KANs) in both actor and critic components-utilizing learnable B-spline univariate functions to achieve parameter-efficient and more interpretable function approximation. During actor updates, we introduce a physics-informed regularization loss that promotes second-order temporal consistency between observed return dynamics and the action-induced portfolio adjustments. The proposed framework is evaluated across three equity markets-China, Vietnam, and the United States, covering both emerging and developed economies. Across all three markets, PIKAN-based agents consistently deliver higher cumulative and annualized returns, superior Sharpe and Calmar ratios, and more favorable drawdown characteristics compared to both standard DRL baselines and classical online portfolio-selection methods. This yields more stable training, higher Sharpe ratios, and superior performance compared to traditional DRL counterparts. The approach is particularly valuable in highly dynamic and noisy financial markets, where conventional DRL often suffers from instability and poor generalization.

</details>


### [1213] [SSNAPS: Audio-Visual Separation of Speech and Background Noise with Diffusion Inverse Sampling](https://arxiv.org/abs/2602.01394)
*Yochai Yemini,Yoav Ellinson,Rami Ben-Ari,Sharon Gannot,Ethan Fetaya*

Main category: eess.AS

Relevance: 35.0

TL;DR: 本文提出了一种基于生成逆采样的音频-视觉单麦克风语音分离与增强方法，使用扩散先验建模干净语音和环境噪声，在真实世界噪声环境下实现无监督语音分离。


<details>
  <summary>Details</summary>
Motivation: 解决真实世界环境噪声下的音频-视觉单麦克风语音分离与增强问题，传统监督方法需要大量标注数据，而本文探索无监督方法利用扩散先验进行源分离。

Method: 基于生成逆采样框架，使用专用扩散先验分别建模干净语音和环境噪声，通过联合利用这些先验恢复所有潜在源；扩展框架处理屏幕外说话人分离。

Result: 在1、2、3个说话人混合噪声的场景中，尽管完全无监督，该方法在WER指标上始终优于领先的监督基线；分离出的噪声组件保真度高，适合下游声学场景检测。

Conclusion: 提出的生成逆采样方法在无监督音频-视觉语音分离中表现出色，超越了监督方法，且分离噪声质量高，具有实际应用价值。

Abstract: This paper addresses the challenge of audio-visual single-microphone speech separation and enhancement in the presence of real-world environmental noise. Our approach is based on generative inverse sampling, where we model clean speech and ambient noise with dedicated diffusion priors and jointly leverage them to recover all underlying sources. To achieve this, we reformulate a recent inverse sampler to match our setting. We evaluate on mixtures of 1, 2, and 3 speakers with noise and show that, despite being entirely unsupervised, our method consistently outperforms leading supervised baselines in \ac{WER} across all conditions. We further extend our framework to handle off-screen speaker separation. Moreover, the high fidelity of the separated noise component makes it suitable for downstream acoustic scene detection. Demo page: https://ssnapsicml.github.io/ssnapsicml2026/

</details>


### [1214] [RAPT: Model-Predictive Out-of-Distribution Detection and Failure Diagnosis for Sim-to-Real Humanoid Robots](https://arxiv.org/abs/2602.01515)
*Humphrey Munn,Brendan Tidd,Peter Bohm,Marcus Gallagher,David Howard*

Main category: cs.RO

Relevance: 35.0

TL;DR: RAPT是一个用于人形机器人50Hz控制的轻量级自监督部署时监控系统，通过学习仿真中的概率时空流形来检测执行时的预测偏差，提供可靠的在线OOD检测和可解释的仿真到现实不匹配度量，并引入基于梯度的时序显著性和LLM推理的自动化根本原因分析管道。


<details>
  <summary>Details</summary>
Motivation: 将学习到的控制策略部署到人形机器人上具有挑战性：仿真中看似鲁棒的策略在仿真到现实迁移后可能在分布外状态中自信执行，导致可能导致硬件损坏的静默故障。现有异常检测方法通常不兼容高速率控制、在校准极低误报率方面表现不佳，或作为黑盒仅提供二进制停止信号而不解释机器人偏离正常行为的原因。

Method: RAPT从仿真中学习名义执行的概率时空流形，并评估执行时预测偏差作为校准的每维度信号。此外，引入自动化事后根本原因分析管道，结合从RAPT重建目标衍生的基于梯度的时序显著性和基于LLM的推理（以显著性和关节运动学为条件），在零样本设置中产生语义故障诊断。

Result: 在仿真中，RAPT在固定0.5%的episode级误报率下，将真阳性率提高了37%。在实际部署中，RAPT实现了12.5%的真阳性率改进，并提供了可操作的interpretability，仅使用本体感受数据在16个现实世界故障中达到75%的根本原因分类准确率。

Conclusion: RAPT提供了一个轻量级、自监督的部署时监控系统，能够可靠地检测人形机器人控制中的分布外状态，并提供可解释的仿真到现实不匹配度量，同时通过结合梯度显著性和LLM推理的自动化分析管道实现语义故障诊断。

Abstract: Deploying learned control policies on humanoid robots is challenging: policies that appear robust in simulation can execute confidently in out-of-distribution (OOD) states after Sim-to-Real transfer, leading to silent failures that risk hardware damage. Although anomaly detection can mitigate these failures, prior methods are often incompatible with high-rate control, poorly calibrated at the extremely low false-positive rates required for practical deployment, or operate as black boxes that provide a binary stop signal without explaining why the robot drifted from nominal behavior. We present RAPT, a lightweight, self-supervised deployment-time monitor for 50Hz humanoid control. RAPT learns a probabilistic spatio-temporal manifold of nominal execution from simulation and evaluates execution-time predictive deviation as a calibrated, per-dimension signal. This yields (i) reliable online OOD detection under strict false-positive constraints and (ii) a continuous, interpretable measure of Sim-to-Real mismatch that can be tracked over time to quantify how far deployment has drifted from training. Beyond detection, we introduce an automated post-hoc root-cause analysis pipeline that combines gradient-based temporal saliency derived from RAPT's reconstruction objective with LLM-based reasoning conditioned on saliency and joint kinematics to produce semantic failure diagnoses in a zero-shot setting. We evaluate RAPT on a Unitree G1 humanoid across four complex tasks in simulation and on physical hardware. In large-scale simulation, RAPT improves True Positive Rate (TPR) by 37% over the strongest baseline at a fixed episode-level false positive rate of 0.5%. On real-world deployments, RAPT achieves a 12.5% TPR improvement and provides actionable interpretability, reaching 75% root-cause classification accuracy across 16 real-world failures using only proprioceptive data.

</details>


### [1215] [Minimax optimal differentially private synthetic data for smooth queries](https://arxiv.org/abs/2602.01607)
*Rundong Ding,Yiyun He,Yizhe Zhu*

Main category: math.ST

Relevance: 35.0

TL;DR: 本文提出了一种生成差分隐私合成数据的新方法，针对具有k阶有界导数的平滑查询，实现了n^{-min(1, k/d)}的最优误差率，揭示了在k=d处的相变现象。


<details>
  <summary>Details</summary>
Motivation: 现有差分隐私合成数据方法通常保证对广泛查询类（如所有Lipschitz函数）的均匀准确性，但这种通用性往往导致对实际统计量的次优误差率。许多常见数据分析查询具有超出最坏情况Lipschitz界限的平滑性，利用这种额外结构可能提高效用。

Method: 提出多项式时间算法，基于切比雪夫矩匹配框架的推广，生成(ε,δ)-差分隐私合成数据，支持在超立方体[-1,1]^d上的数据集，为所有具有k阶有界导数的平滑查询提供均匀效用保证。

Result: 实现了最小最大误差率n^{-min(1, k/d)}（忽略对数因子），在k=d处发现相变。严格改进了Wang等人(2016)对k-平滑查询的误差率，并首次建立了(ε,δ)-差分隐私合成数据对k-平滑查询效用的最小最大下界。

Conclusion: 通过利用查询的额外平滑结构，可以显著提高差分隐私合成数据的效用，为实际数据分析提供更优的隐私-效用权衡。该工作扩展了差分隐私合成数据理论，为平滑查询提供了最优算法和理论界限。

Abstract: Differentially private synthetic data enables the sharing and analysis of sensitive datasets while providing rigorous privacy guarantees for individual contributors. A central challenge is to achieve strong utility guarantees for meaningful downstream analysis. Many existing methods ensure uniform accuracy over broad query classes, such as all Lipschitz functions, but this level of generality often leads to suboptimal rates for statistics of practical interest. Since many common data analysis queries exhibit smoothness beyond what worst-case Lipschitz bounds capture, we ask whether exploiting this additional structure can yield improved utility.
  We study the problem of generating $(\varepsilon,δ)$-differentially private synthetic data from a dataset of size $n$ supported on the hypercube $[-1,1]^d$, with utility guarantees uniformly for all smooth queries having bounded derivatives up to order $k$. We propose a polynomial-time algorithm that achieves a minimax error rate of $n^{-\min \{1, \frac{k}{d}\}}$, up to a $\log(n)$ factor. This characterization uncovers a phase transition at $k=d$. Our results generalize the Chebyshev moment matching framework of (Musco et al., 2025; Wang et al., 2016) and strictly improve the error rates for $k$-smooth queries established in (Wang et al., 2016). Moreover, we establish the first minimax lower bound for the utility of $(\varepsilon,δ)$-differentially private synthetic data with respect to $k$-smooth queries, extending the Wasserstein lower bound for $\varepsilon$-differential privacy in (Boedihardjo et al., 2024).

</details>


### [1216] [ST-BCP: Tightening Coverage Bound for Backward Conformal Prediction via Non-Conformity Score Transformation](https://arxiv.org/abs/2602.01733)
*Junxian Liu,Hao Zeng,Hongxin Wei*

Main category: stat.ML

Relevance: 35.0

TL;DR: ST-BCP提出了一种新的数据依赖分数变换方法，改进了反向共形预测（BCP）框架，显著缩小了覆盖率估计与实际覆盖率之间的差距。


<details>
  <summary>Details</summary>
Motivation: 传统共形预测（CP）能保证覆盖率但预测集大小不可控，反向共形预测（BCP）通过限制预测集大小来估计覆盖率保证，但受马尔可夫不等式影响，估计覆盖率与实际覆盖率存在显著差距。

Method: 提出ST-BCP方法，引入数据依赖的非共形分数变换来缩小覆盖率差距。开发了可计算的变换函数，并证明其优于基线恒等变换。

Result: 在常见基准测试中，平均覆盖率差距从4.20%降低到1.12%，显著提升了覆盖率估计的准确性。

Conclusion: ST-BCP通过数据依赖的分数变换有效改进了BCP框架，缩小了覆盖率估计差距，为不确定性量化提供了更精确的统计保证。

Abstract: Conformal Prediction (CP) provides a statistical framework for uncertainty quantification that constructs prediction sets with coverage guarantees. While CP yields uncontrolled prediction set sizes, Backward Conformal Prediction (BCP) inverts this paradigm by enforcing a predefined upper bound on set size and estimating the resulting coverage guarantee. However, the looseness induced by Markov's inequality within the BCP framework causes a significant gap between the estimated coverage bound and the empirical coverage. In this work, we introduce ST-BCP, a novel method that introduces a data-dependent transformation of nonconformity scores to narrow the coverage gap. In particular, we develop a computable transformation and prove that it outperforms the baseline identity transformation. Extensive experiments demonstrate the effectiveness of our method, reducing the average coverage gap from 4.20\% to 1.12\% on common benchmarks.

</details>


### [1217] [RIR-Former: Coordinate-Guided Transformer for Continuous Reconstruction of Room Impulse Responses](https://arxiv.org/abs/2602.01861)
*Shaoheng Xu,Chunyi Sun,Jihui,Zhang,Prasanga N. Samarasinghe,Thushara D. Abhayapala*

Main category: eess.AS

Relevance: 35.0

TL;DR: RIR-Former：一种基于Transformer的网格无关、一步前馈模型，用于从稀疏测量中重建房间脉冲响应，通过正弦编码模块整合麦克风位置信息，并使用分段多分支解码器分别处理早期反射和晚期混响。


<details>
  <summary>Details</summary>
Motivation: 房间脉冲响应(RIRs)对许多声学信号处理任务至关重要，但在空间中密集测量通常不切实际。现有方法需要密集网格测量或复杂的迭代过程，限制了实际应用。需要一种能够从稀疏阵列测量中准确重建RIRs的方法。

Method: 提出RIR-Former，一种基于Transformer的网格无关前馈模型。核心创新包括：1）正弦编码模块将麦克风位置信息融入Transformer骨干网络；2）分段多分支解码器分别处理早期反射（时间局部特征）和晚期混响（全局统计特征）；3）支持任意阵列位置的插值。

Result: 在多种模拟声学环境中的实验表明，RIR-Former在归一化均方误差(NMSE)和余弦距离(CD)指标上始终优于现有最优基线，在不同缺失率和阵列配置下均表现优异。

Conclusion: 该方法展示了实际部署潜力，为从随机间隔线性阵列扩展到复杂阵列几何、动态声学场景和真实世界环境提供了基础，推动了稀疏测量RIR重建技术的发展。

Abstract: Room impulse responses (RIRs) are essential for many acoustic signal processing tasks, yet measuring them densely across space is often impractical. In this work, we propose RIR-Former, a grid-free, one-step feed-forward model for RIR reconstruction. By introducing a sinusoidal encoding module into a transformer backbone, our method effectively incorporates microphone position information, enabling interpolation at arbitrary array locations. Furthermore, a segmented multi-branch decoder is designed to separately handle early reflections and late reverberation, improving reconstruction across the entire RIR. Experiments on diverse simulated acoustic environments demonstrate that RIR-Former consistently outperforms state-of-the-art baselines in terms of normalized mean square error (NMSE) and cosine distance (CD), under varying missing rates and array configurations. These results highlight the potential of our approach for practical deployment and motivate future work on scaling from randomly spaced linear arrays to complex array geometries, dynamic acoustic scenes, and real-world environments.

</details>


### [1218] [Grappa: Gradient-Only Communication for Scalable Graph Neural Network Training](https://arxiv.org/abs/2602.01872)
*Chongyang Xu,Christoph Siebenbrunner,Laurent Bindschaedler*

Main category: cs.DC

Relevance: 35.0

TL;DR: Grappa是一个分布式GNN训练框架，通过仅交换梯度而非特征/激活来减少通信开销，使用重新分区和覆盖校正梯度聚合来保持准确性，实现4-13倍加速。


<details>
  <summary>Details</summary>
Motivation: 分布式GNN训练中，跨分区边的通信成本（获取远程特征和激活）随着图深度和分区数量增加而急剧增长，成为主要瓶颈。

Method: 1) 梯度唯一通信：每个迭代中分区独立训练，仅交换梯度进行全局更新；2) 定期重新分区暴露新邻域；3) 轻量级覆盖校正梯度聚合（重要性采样启发）；4) 批处理级变体和收缩版本提高稳定性。

Result: 在真实和合成图上，Grappa平均训练速度比现有系统快4倍（最高13倍），对更深模型获得更好准确性，在商品硬件上支持万亿边规模训练。

Conclusion: Grappa通过梯度唯一通信和覆盖校正梯度聚合，有效解决了分布式GNN训练中的通信瓶颈，实现了高效、可扩展的训练，且不依赖高带宽互连或缓存。

Abstract: Cross-partition edges dominate the cost of distributed GNN training: fetching remote features and activations per iteration overwhelms the network as graphs deepen and partition counts grow. Grappa is a distributed GNN training framework that enforces gradient-only communication: during each iteration, partitions train in isolation and exchange only gradients for the global update. To recover accuracy lost to isolation, Grappa (i) periodically repartitions to expose new neighborhoods and (ii) applies a lightweight coverage-corrected gradient aggregation inspired by importance sampling. We prove the corrected estimator is asymptotically unbiased under standard support and boundedness assumptions, and we derive a batch-level variant for compatibility with common deep-learning packages that minimizes mean-squared deviation from the ideal node-level correction. We also introduce a shrinkage version that improves stability in practice. Empirical results on real and synthetic graphs show that Grappa trains GNNs 4 times faster on average (up to 13 times) than state-of-the-art systems, achieves better accuracy especially for deeper models, and sustains training at the trillion-edge scale on commodity hardware. Grappa is model-agnostic, supports full-graph and mini-batch training, and does not rely on high-bandwidth interconnects or caching.

</details>


### [1219] [Privacy Amplification by Missing Data](https://arxiv.org/abs/2602.01928)
*Simon Roburin,Rafaël Pinot,Erwan Scornet*

Main category: stat.ML

Relevance: 35.0

TL;DR: 该论文提出了一种新颖视角：缺失数据可作为隐私增强机制，首次在差分隐私框架下证明了不完整数据能提供隐私放大效果。


<details>
  <summary>Details</summary>
Motivation: 在医疗、金融等高风险领域，隐私保护至关重要，但这些领域的数据常存在缺失值。传统上缺失数据被视为限制因素，但作者从隐私保护角度重新审视缺失数据，认为特征缺失可能减少个体信息泄露，从而增强隐私。

Method: 在差分隐私框架下形式化分析缺失数据作为隐私放大机制。通过理论分析证明不完整数据能为差分隐私算法提供隐私放大效果。

Result: 首次证明了不完整数据能产生差分隐私的隐私放大效果，为处理缺失数据的隐私保护系统提供了理论基础。

Conclusion: 缺失数据不仅不是限制，反而可以成为隐私增强的有力工具，为高敏感领域的数据分析提供了新的隐私保护思路。

Abstract: Privacy preservation is a fundamental requirement in many high-stakes domains such as medicine and finance, where sensitive personal data must be analyzed without compromising individual confidentiality. At the same time, these applications often involve datasets with missing values due to non-response, data corruption, or deliberate anonymization. Missing data is traditionally viewed as a limitation because it reduces the information available to analysts and can degrade model performance. In this work, we take an alternative perspective and study missing data from a privacy preservation standpoint. Intuitively, when features are missing, less information is revealed about individuals, suggesting that missingness could inherently enhance privacy. We formalize this intuition by analyzing missing data as a privacy amplification mechanism within the framework of differential privacy. We show, for the first time, that incomplete data can yield privacy amplification for differentially private algorithms.

</details>


### [1220] [FluxNet: Learning Capacity-Constrained Local Transport Operators for Conservative and Bounded PDE Surrogates](https://arxiv.org/abs/2602.01941)
*Zishuo Lan,Junjie Li,Lei Wang,Jincheng Wang*

Main category: cond-mat.mtrl-sci

Relevance: 35.0

TL;DR: FluxNet：一种学习保守传输算子的框架，通过局部传输算子更新而非直接预测下一状态，保证离散守恒和状态边界约束，在多个PDE问题上展示出改进的稳定性和物理一致性。


<details>
  <summary>Details</summary>
Motivation: 数据驱动的PDE模拟中，自回归学习时间步进算子常因违反全局守恒和状态边界约束（如非负质量、密度在[0,1]区间）而导致长时程模拟不稳定。直接下一状态回归难以强制这些耦合约束。

Method: 受晶格玻尔兹曼离散速度传输表示启发，提出学习保守传输算子的框架。模型输出局部传输算子，通过邻域交换更新网格单元，构造上保证离散守恒。对有界量，将传输参数化为容量约束可行集，结构性地而非后处理裁剪地强制边界。

Result: 在1D对流扩散、2D浅水方程、1D交通流和2D旋节分解上验证。浅水方程和交通流实验显示相比强基线有改进的模拟稳定性和物理一致性。旋节分解中，方法支持大时间步长和长程传输，在点态和统计度量上保持微结构演化同时加速模拟。

Conclusion: FluxNet框架通过结构性地强制守恒和边界约束，为数据驱动的PDE模拟提供了更稳定和物理一致的保守传输算子学习方法，在多种守恒律问题上表现优异。

Abstract: Autoregressive learning of time-stepping operators offers an effective approach to data-driven PDE simulation on grids. For conservation laws, however, long-horizon rollouts are often destabilized when learned updates violate global conservation and, in many applications, additional state bounds such as nonnegative mass and densities or concentrations constrained to [0,1]. Enforcing these coupled constraints via direct next-state regression remains difficult. We introduce a framework for learning conservative transport operators on regular grids, inspired by lattice Boltzmann-style discrete-velocity transport representations. Instead of predicting the next state, the model outputs local transport operators that update cells through neighborhood exchanges, guaranteeing discrete conservation by construction. For bounded quantities, we parameterize transport within a capacity-constrained feasible set, enforcing bounds structurally rather than by post-hoc clipping. We validate FluxNet on 1D convection-diffusion, 2D shallow water equations, 1D traffic flow, and 2D spinodal decomposition. Experiments on shallow-water equations and traffic flow show improved rollout stability and physical consistency over strong baselines. On phase-field spinodal decomposition, the method enables large time-steps with long-range transport, accelerating simulation while preserving microstructure evolution in both pointwise and statistical measures.

</details>


### [1221] [Stochastic Interpolants in Hilbert Spaces](https://arxiv.org/abs/2602.01988)
*James Boran Yu,RuiKang OuYang,Julien Horwood,José Miguel Hernández-Lobato*

Main category: stat.ML

Relevance: 35.0

TL;DR: 该论文提出了一个在无限维希尔伯特空间中构建随机插值的理论框架，将原本局限于有限维的随机插值方法扩展到函数值数据，特别适用于偏微分方程基准问题的条件生成。


<details>
  <summary>Details</summary>
Motivation: 虽然扩散模型已成功扩展到函数值数据，但随机插值方法（提供连接任意分布的灵活方式）仍局限于有限维设置。本文旨在填补这一空白，为无限维希尔伯特空间中的随机插值建立严格的理论框架。

Method: 建立无限维希尔伯特空间中随机插值的理论框架，包括适定性证明和显式误差界。该框架支持任意函数分布之间的生成桥接，特别关注复杂的基于PDE的基准问题。

Result: 提出的框架在条件生成方面表现出有效性，在复杂的PDE基准问题上取得了最先进的结果，为科学发现提供了一个强大的通用工具。

Conclusion: 通过将随机插值扩展到无限维空间，该工作为函数值数据的生成建模提供了理论基础，特别在科学计算领域具有重要应用价值。

Abstract: Although diffusion models have successfully extended to function-valued data, stochastic interpolants -- which offer a flexible way to bridge arbitrary distributions -- remain limited to finite-dimensional settings. This work bridges this gap by establishing a rigorous framework for stochastic interpolants in infinite-dimensional Hilbert spaces. We provide comprehensive theoretical foundations, including proofs of well-posedness and explicit error bounds. We demonstrate the effectiveness of the proposed framework for conditional generation, focusing particularly on complex PDE-based benchmarks. By enabling generative bridges between arbitrary functional distributions, our approach achieves state-of-the-art results, offering a powerful, general-purpose tool for scientific discovery.

</details>


### [1222] [Position: The Need for Ultrafast Training](https://arxiv.org/abs/2602.02005)
*Duc Hoang*

Main category: cs.AR

Relevance: 35.0

TL;DR: 提出从仅推理的FPGA加速器转向超快速片上学习，在FPGA架构中实现推理和训练，满足亚微秒级延迟约束，使系统能够以物理过程的速度自适应。


<details>
  <summary>Details</summary>
Motivation: 现有领域专用FPGA加速器假设静态模型离线训练，将学习和适应任务交给较慢的CPU或GPU，这限制了在非平稳、高频环境中必须实时更新的系统性能。

Method: 主张重新思考算法、架构和工具流程，将学习功能集成到与推理相同的实时数据路径中，实现FPGA架构内的直接推理和训练执行。

Result: 提出了一种新的FPGA加速器范式，能够支持量子纠错、低温量子比特校准、等离子体和聚变控制、加速器调谐和自主科学实验等应用。

Conclusion: 将FPGA从静态推理引擎转变为实时学习机器，有望实现与物理过程速度相匹配的闭环自适应系统。

Abstract: Domain-specialized FPGAs have delivered unprecedented performance for low-latency inference across scientific and industrial workloads, yet nearly all existing accelerators assume static models trained offline, relegating learning and adaptation to slower CPUs or GPUs. This separation fundamentally limits systems that must operate in non-stationary, high-frequency environments, where model updates must occur at the timescale of the underlying physics. In this paper, I argue for a shift from inference-only accelerators to ultrafast on-chip learning, in which both inference and training execute directly within the FPGA fabric under deterministic, sub-microsecond latency constraints. Bringing learning into the same real-time datapath as inference would enable closed-loop systems that adapt as fast as the physical processes they control, with applications spanning quantum error correction, cryogenic qubit calibration, plasma and fusion control, accelerator tuning, and autonomous scientific experiments. Enabling such regimes requires rethinking algorithms, architectures, and toolflows jointly, but promises to transform FPGAs from static inference engines into real-time learning machines.

</details>


### [1223] [Adaptive Quality-Diversity Trade-offs for Large-Scale Batch Recommendation](https://arxiv.org/abs/2602.02024)
*Clémence Réda,Tomas Rigaux,Hiba Bederina,Koh Takeuchi,Hisashi Kashima,Jill-Jênn Vie*

Main category: cs.IR

Relevance: 35.0

TL;DR: B-DivRec算法结合行列式点过程和模糊去重程序，在推荐系统中平衡相关性与多样性，并自适应调整质量-多样性权衡


<details>
  <summary>Details</summary>
Motivation: 推荐系统需要同时提供高度相关且多样化的物品，既要个性化又要让用户走出舒适区，但实际应用中存在避免推荐过于相似物品和计算成本高的问题

Method: 1) 当用户反馈模型已知时，结合行列式点过程和模糊去重程序调整物品多样性程度；2) 提出自适应方法根据用户反馈动态调整质量-多样性权衡

Result: 在电影推荐和药物再利用的合成和真实数据集上展示了B-DivRec的性能和多功能性

Conclusion: B-DivRec能有效平衡推荐系统的相关性与多样性，并通过自适应机制优化用户体验

Abstract: A core research question in recommender systems is to propose batches of highly relevant and diverse items, that is, items personalized to the user's preferences, but which also might get the user out of their comfort zone. This diversity might induce properties of serendipidity and novelty which might increase user engagement or revenue. However, many real-life problems arise in that case: e.g., avoiding to recommend distinct but too similar items to reduce the churn risk, and computational cost for large item libraries, up to millions of items. First, we consider the case when the user feedback model is perfectly observed and known in advance, and introduce an efficient algorithm called B-DivRec combining determinantal point processes and a fuzzy denuding procedure to adjust the degree of item diversity. This helps enforcing a quality-diversity trade-off throughout the user history. Second, we propose an approach to adaptively tailor the quality-diversity trade-off to the user, so that diversity in recommendations can be enhanced if it leads to positive feedback, and vice-versa. Finally, we illustrate the performance and versatility of B-DivRec in the two settings on synthetic and real-life data sets on movie recommendation and drug repurposing.

</details>


### [1224] [Learning Beyond the Gaussian Data: Learning Dynamics of Neural Networks on an Expressive and Cumulant-Controllable Data Model](https://arxiv.org/abs/2602.02153)
*Onat Ure,Samet Demir,Zafer Dogan*

Main category: stat.ML

Relevance: 35.0

TL;DR: 本文提出了一种可控制高阶统计量的非高斯数据模型，用于研究神经网络学习动态。通过使用Hermite多项式展开构造生成式两层神经网络数据模型，实现了对偏度和峰度等高阶累积量的可解释控制。实验表明神经网络训练呈现"阶数渐进"模式：先学习低阶统计量，再逐步学习高阶统计量。


<details>
  <summary>Details</summary>
Motivation: 研究数据的高阶统计量（如偏度、峰度）对神经网络学习动态的影响。传统研究通常假设数据服从高斯分布，忽略了真实数据中的非高斯特性。本文旨在建立一种既能控制高阶统计量又保持现实性的数据模型，以填补简化数据假设与实际数据复杂性之间的空白。

Method: 1. 构造可控制高阶统计量的非高斯数据模型：使用Hermite多项式展开激活函数，构建生成式两层神经网络作为数据模型，通过Hermite系数实现对高阶累积量的可解释控制。
2. 在线学习实验：使用该数据模型生成样本，对两层神经网络进行受控的在线学习实验。
3. 真实数据验证：在Fashion-MNIST数据集上预训练生成模型，利用生成样本进行进一步实验验证。

Result: 1. 揭示了神经网络训练的"阶数渐进"模式：网络首先学习均值和协方差等低阶统计量，然后逐步学习高阶累积量。
2. 在Fashion-MNIST上的实验验证了该结论，并展示了数据模型在真实场景中的实用性。
3. 提出的方法为研究机器学习中的分布效应提供了原则性框架。

Conclusion: 本文提出的可控制高阶统计量的数据模型有效连接了简化数据假设与实际数据复杂性，为研究分布效应对神经网络学习的影响提供了系统方法。研究发现神经网络学习遵循从低阶到高阶统计量的渐进模式，这一发现对理解神经网络学习动态具有重要意义。

Abstract: We study the effect of high-order statistics of data on the learning dynamics of neural networks (NNs) by using a moment-controllable non-Gaussian data model. Considering the expressivity of two-layer neural networks, we first construct the data model as a generative two-layer NN where the activation function is expanded by using Hermite polynomials. This allows us to achieve interpretable control over high-order cumulants such as skewness and kurtosis through the Hermite coefficients while keeping the data model realistic. Using samples generated from the data model, we perform controlled online learning experiments with a two-layer NN. Our results reveal a moment-wise progression in training: networks first capture low-order statistics such as mean and covariance, and progressively learn high-order cumulants. Finally, we pretrain the generative model on the Fashion-MNIST dataset and leverage the generated samples for further experiments. The results of these additional experiments confirm our conclusions and show the utility of the data model in a real-world scenario. Overall, our proposed approach bridges simplified data assumptions and practical data complexity, which offers a principled framework for investigating distributional effects in machine learning and signal processing.

</details>


### [1225] [Hierarchical Federated Learning with SignSGD: A Highly Communication-Efficient Approach](https://arxiv.org/abs/2602.02355)
*Amirreza Kazemi,Seyed Mohammad Azimi-Abarghouyi,Gabor Fodor,Carlo Fischione*

Main category: cs.DC

Relevance: 35.0

TL;DR: 提出了一种用于分层联邦学习的高效通信框架HierSignSGD，通过单比特梯度压缩和多数投票聚合，在保持精度的同时大幅减少通信开销。


<details>
  <summary>Details</summary>
Motivation: 分层联邦学习在大规模无线和物联网系统中面临严格的通信限制，现有的一比特方法（如SignSGD）在平坦联邦设置中有效，但缺乏对分层架构的理论和算法支持，特别是边缘层多数投票与云层模型聚合的交互影响未知。

Method: 提出HierSignSGD框架：设备仅发送带符号的随机梯度，边缘服务器通过多数投票聚合，云层定期平均边缘模型，同时使用下行链路量化广播全局模型。分析了有偏符号压缩、两级聚合间隔和集群间异质性对收敛的影响。

Result: 在均匀和非均匀数据分割下的数值实验表明，HierSignSGD尽管采用极端压缩，仍能达到与全精度随机梯度下降相当或更好的精度，同时显著降低通信成本，并在激进的下行链路稀疏化下保持鲁棒性。

Conclusion: 该工作为分层联邦学习中的高效通信提供了理论框架和实用算法，证明了在严格通信约束下通过精心设计的压缩和聚合策略仍能保持学习性能。

Abstract: Hierarchical federated learning (HFL) has emerged as a key architecture for large-scale wireless and Internet of Things systems, where devices communicate with nearby edge servers before reaching the cloud. In these environments, uplink bandwidth and latency impose strict communication limits, thereby making aggressive gradient compression essential. One-bit methods such as sign-based stochastic gradient descent (SignSGD) offer an attractive solution in flat federated settings, but existing theory and algorithms do not naturally extend to hierarchical settings. In particular, the interaction between majority-vote aggregation at the edge layer and model aggregation at the cloud layer, and its impact on end-to-end performance, remains unknown. To bridge this gap, we propose a highly communication-efficient sign-based HFL framework and develop its corresponding formulation for nonconvex learning, where devices send only signed stochastic gradients, edge servers combine them through majority-vote, and the cloud periodically averages the obtained edge models, while utilizing downlink quantization to broadcast the global model. We introduce the resulting scalable HFL algorithm, HierSignSGD, and provide the convergence analysis for SignSGD in a hierarchical setting. Our core technical contribution is a characterization of how biased sign compression, two-level aggregation intervals, and inter-cluster heterogeneity collectively affect convergence. Numerical experiments under homogeneous and heterogeneous data splits show that HierSignSGD, despite employing extreme compression, achieves accuracy comparable to or better than full-precision stochastic gradient descent while reducing communication cost in the process, and remains robust under aggressive downlink sparsification.

</details>


### [1226] [Transfer Learning Through Conditional Quantile Matching](https://arxiv.org/abs/2602.02358)
*Yikun Zhang,Steven Wilkins-Reeves,Wesley Lee,Aude Hofleitner*

Main category: stat.ML

Relevance: 35.0

TL;DR: 提出一种用于回归任务的迁移学习框架，通过异质源域提升数据稀缺目标域的预测性能，使用条件生成模型和分位数匹配进行分布对齐，提供高质量数据增强。


<details>
  <summary>Details</summary>
Motivation: 在数据稀缺的目标域中，如何有效利用异质源域的知识来提升预测性能，同时避免对源域和目标域分布关系（如协变量偏移或标签偏移）的严格假设限制。

Method: 为每个源域单独学习条件生成模型，然后通过条件分位数匹配将生成的响应校准到目标域，实现分布对齐，为下游学习任务提供高质量增强数据。

Result: 理论证明：在增强数据集上训练的ERM比仅使用目标域数据的ERM具有更紧的过剩风险界；实践验证：在模拟和真实数据应用中，该方法持续优于仅目标域学习和竞争迁移学习方法。

Conclusion: 该框架提供了一种原则性且灵活的方法，通过分布对齐实现高质量数据增强，有效提升数据稀缺目标域的预测性能，在理论和实践上均表现出色。

Abstract: We introduce a transfer learning framework for regression that leverages heterogeneous source domains to improve predictive performance in a data-scarce target domain. Our approach learns a conditional generative model separately for each source domain and calibrates the generated responses to the target domain via conditional quantile matching. This distributional alignment step corrects general discrepancies between source and target domains without imposing restrictive assumptions such as covariate or label shift. The resulting framework provides a principled and flexible approach to high-quality data augmentation for downstream learning tasks in the target domain. From a theoretical perspective, we show that an empirical risk minimizer (ERM) trained on the augmented dataset achieves a tighter excess risk bound than the target-only ERM under mild conditions. In particular, we establish new convergence rates for the quantile matching estimator that governs the transfer bias-variance tradeoff. From a practical perspective, extensive simulations and real data applications demonstrate that the proposed method consistently improves prediction accuracy over target-only learning and competing transfer learning methods.

</details>


### [1227] [Provably Data-driven Multiple Hyper-parameter Tuning with Structured Loss Function](https://arxiv.org/abs/2602.02406)
*Tung Quoc Le,Anh Tuan Nguyen,Viet Anh Nguyen*

Main category: stat.ML

Relevance: 35.0

TL;DR: 该论文提出了首个用于多维度超参数调优的泛化保证框架，利用实代数几何工具强化了半代数函数类的泛化保证，并将其扩展到验证损失场景，最后展示了在加权组LASSO和加权融合LASSO中的新可学习性结果。


<details>
  <summary>Details</summary>
Motivation: 数据驱动的算法设计虽然能自动化超参数调优，但其统计基础仍然有限，因为模型性能可能以隐式且高度非光滑的方式依赖于超参数。现有保证仅关注一维超参数情况，而实践中重要的多维度超参数调优问题尚未解决。

Method: 通过利用实代数几何工具，强化了半代数函数类的泛化保证框架，建立了多维度超参数调优的泛化保证理论框架。将分析扩展到验证损失场景，并在有额外结构时推导改进的边界。

Result: 建立了首个多维度超参数调优的泛化保证框架，获得了更尖锐、更广泛适用的保证。展示了框架的应用范围，包括数据驱动的加权组LASSO和加权融合LASSO的新可学习性结果。

Conclusion: 该研究解决了多维度超参数调优的理论基础问题，为数据驱动的算法设计提供了更强大的统计保证，扩展了机器学习理论的应用范围。

Abstract: Data-driven algorithm design automates hyperparameter tuning, but its statistical foundations remain limited because model performance can depend on hyperparameters in implicit and highly non-smooth ways. Existing guarantees focus on the simple case of a one-dimensional (scalar) hyperparameter. This leaves the practically important, multi-dimensional hyperparameter tuning setting unresolved. We address this open question by establishing the first general framework for establishing generalization guarantees for tuning multi-dimensional hyperparameters in data-driven settings. Our approach strengthens the generalization guarantee framework for semi-algebraic function classes by exploiting tools from real algebraic geometry, yielding sharper, more broadly applicable guarantees. We then extend the analysis to hyperparameter tuning using the validation loss under minimal assumptions, and derive improved bounds when additional structure is available. Finally, we demonstrate the scope of the framework with new learnability results, including data-driven weighted group lasso and weighted fused lasso.

</details>


### [1228] [Energy-Efficient Neuromorphic Computing for Edge AI: A Framework with Adaptive Spiking Neural Networks and Hardware-Aware Optimization](https://arxiv.org/abs/2602.02439)
*Olaf Yunus Laitinen Imanov,Derya Umut Kulali,Taner Yilmaz,Duygu Erisken,Rana Irem Turhan*

Main category: cs.NE

Relevance: 35.0

TL;DR: NeuEdge是一个结合自适应SNN模型与硬件感知优化的边缘AI框架，通过混合时间编码和硬件感知训练，在边缘设备上实现高效能、低延迟的神经形态计算。


<details>
  <summary>Details</summary>
Motivation: 边缘AI应用需要超低功耗、低延迟的推理。基于事件驱动SNN的神经形态计算虽然有潜力，但在资源受限设备上的实际部署受到训练困难、硬件映射开销和时间动态敏感性的限制。

Method: 1. 混合速率和尖峰时序模式的时间编码方案，减少尖峰活动同时保持精度；2. 硬件感知训练过程，协同优化网络结构和片上布局以提高神经形态处理器利用率；3. 自适应阈值机制，根据输入统计调整神经元兴奋性，降低能耗而不影响性能。

Result: 在标准视觉和音频基准测试中，NeuEdge在边缘硬件上实现91-96%的准确率，推理延迟低至2.3毫秒，估计能效达到847 GOp/s/W。在自主无人机工作负载的案例研究中，相比传统深度神经网络实现高达312倍的节能，同时保持实时操作。

Conclusion: NeuEdge框架通过结合自适应SNN模型和硬件感知优化，有效解决了神经形态计算在边缘部署中的关键挑战，为资源受限设备提供了高效能、低延迟的AI推理解决方案。

Abstract: Edge AI applications increasingly require ultra-low-power, low-latency inference. Neuromorphic computing based on event-driven spiking neural networks (SNNs) offers an attractive path, but practical deployment on resource-constrained devices is limited by training difficulty, hardware-mapping overheads, and sensitivity to temporal dynamics. We present NeuEdge, a framework that combines adaptive SNN models with hardware-aware optimization for edge deployment. NeuEdge uses a temporal coding scheme that blends rate and spike-timing patterns to reduce spike activity while preserving accuracy, and a hardware-aware training procedure that co-optimizes network structure and on-chip placement to improve utilization on neuromorphic processors. An adaptive threshold mechanism adjusts neuron excitability from input statistics, reducing energy consumption without degrading performance. Across standard vision and audio benchmarks, NeuEdge achieves 91-96% accuracy with up to 2.3 ms inference latency on edge hardware and an estimated 847 GOp/s/W energy efficiency. A case study on an autonomous-drone workload shows up to 312x energy savings relative to conventional deep neural networks while maintaining real-time operation.

</details>


### [1229] [THDC: Training Hyperdimensional Computing Models with Backpropagation](https://arxiv.org/abs/2602.00116)
*Hanne Dejonghe,Sam Leroux*

Main category: cs.LG

Relevance: 30.0

TL;DR: 提出可训练超维计算（THDC），通过反向传播实现端到端学习，将维度从10,000降至64，在多个数据集上达到或超过现有HDC性能


<details>
  <summary>Details</summary>
Motivation: 传统超维计算（HDC）依赖超高维度和静态随机初始化向量，导致内存效率低和学习能力有限，需要更高效可训练的HDC方法

Method: 提出THDC框架：1）用可训练嵌入替换随机初始化向量；2）引入单层二进制神经网络优化类别表示；3）通过反向传播实现端到端训练

Result: 在MNIST、Fashion-MNIST和CIFAR-10上评估，THDC达到或超过现有HDC最佳性能，同时将维度从10,000大幅降低到64

Conclusion: THDC显著提高了HDC的内存效率和性能，为轻量级边缘计算提供了更实用的解决方案

Abstract: Hyperdimensional computing (HDC) offers lightweight learning for energy-constrained devices by encoding data into high-dimensional vectors. However, its reliance on ultra-high dimensionality and static, randomly initialized hypervectors limits memory efficiency and learning capacity. Therefore, we propose Trainable Hyperdimensional Computing (THDC), which enables end-to-end HDC via backpropagation. THDC replaces randomly initialized vectors with trainable embeddings and introduces a one-layer binary neural network to optimize class representations. Evaluated on MNIST, Fashion-MNIST and CIFAR-10, THDC achieves equal or better accuracy than state-of-the-art HDC, with dimensionality reduced from 10.000 to 64.

</details>


### [1230] [Quantum Model Parallelism for MRI-Based Classification of Alzheimer's Disease Stages](https://arxiv.org/abs/2602.00128)
*Emine Akpinar,Murat Oduncuoglu*

Main category: cs.LG

Relevance: 30.0

TL;DR: 提出了一种基于量子的并行模型（QBPM）架构，利用量子叠加和纠缠原理，通过两个并行量子电路对阿尔茨海默病阶段进行高效分类，在MRI数据集上实现了高精度和鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 随着人口老龄化，阿尔茨海默病成为全球重大健康问题。传统AI方法在处理大规模数据时面临计算资源限制，而量子AI方法利用量子叠加、纠缠和高维希尔伯特空间，能够超越经典方法的限制，为高维、异构和噪声数据提供更高精度。

Method: 提出量子并行模型（QBPM）架构，受经典模型并行化启发。模型使用两个不同的量子电路，每个电路包含旋转和纠缠块，在同一个量子模拟器上并行运行。使用MRI数据集进行阿尔茨海默病阶段分类。

Result: 在两个不同数据集上均表现出高分类精度，展示了模型的整体鲁棒性和泛化能力。在高斯噪声条件下（模拟真实场景）仍保持良好性能。与五种经典迁移学习方法相比，QBPM实现了更高的分类精度和相当的执行时间，同时使用更少的电路参数。

Conclusion: QBPM架构代表了复杂疾病（如阿尔茨海默病）阶段分类的创新且强大的方法，展示了量子AI在医学诊断中的潜力，特别是在处理高维、噪声数据方面的优势。

Abstract: With increasing life expectancy, AD has become a major global health concern. While classical AI-based methods have been developed for early diagnosis and stage classification of AD, growing data volumes and limited computational resources necessitate faster, more efficient approaches. Quantum-based AI methods, which leverage superposition and entanglement principles along with high-dimensional Hilbert space, can surpass classical approaches' limitations and offer higher accuracy for high-dimensional, heterogeneous, and noisy data. In this study, a Quantum-Based Parallel Model (QBPM) architecture is proposed for the efficient classification of AD stages using MRI datasets, inspired by the principles of classical model parallelism. The proposed model leverages quantum advantages by employing two distinct quantum circuits, each incorporating rotational and entanglement blocks, running in parallel on the same quantum simulator. The classification performance of the model was evaluated on two different datasets to assess its overall robustness and generalization capability. The proposed model demonstrated high classification accuracy across both datasets, highlighting its overall robustness and generalization capability. Results obtained under high-level Gaussian noise, simulating real-world conditions, further provided experimental evidence for the model's applicability not only in theoretical but also in practical scenarios. Moreover, compared with five different classical transfer learning methods, the proposed model demonstrated its efficiency as an alternative to classical approaches by achieving higher classification accuracy and comparable execution time while utilizing fewer circuit parameters. The results indicate that the proposed QBPM architecture represents an innovative and powerful approach for the classification of stages in complex diseases such as Alzheimer's.

</details>


### [1231] [Quantum Generator Kernels](https://arxiv.org/abs/2602.00361)
*Philipp Altmann,Maximilian Mansky,Maximilian Zorn,Jonas Stein,Claudia Linnhoff-Popien*

Main category: cs.LG

Relevance: 30.0

TL;DR: 本文提出量子生成核（QGKs），通过变分生成器组（VGGs）构建可参数化的量子核，解决NISQ硬件限制下大规模数据嵌入问题，在投影和分类性能上优于现有量子与经典核方法。


<details>
  <summary>Details</summary>
Motivation: 量子核方法理论上能将经典不可分特征在量子空间中分离，但受限于NISQ硬件容量，需要有效策略将大规模现实数据（如图像）压缩嵌入到量子设备中。现有混合架构的固定中间嵌入过程可能阻碍充分利用量子计算潜力。

Method: 提出量子生成核（QGKs），包含一组变分生成器组（VGGs），将通用生成器合并为可参数化算子，确保量子空间的规模化覆盖。通过训练权重向量参数化VGGs在当前数据上下文中的投影，优化核与目标域的对齐。

Result: 实证结果表明，QGKs在投影和分类能力上优于最先进的量子与经典核方法，展示了其作为各种QML应用通用框架的潜力。

Conclusion: QGKs通过生成器基方法解决了NISQ硬件下数据嵌入的挑战，提供了可扩展的量子核框架，在量子机器学习应用中表现出优越性能。

Abstract: Quantum kernel methods offer significant theoretical benefits by rendering classically inseparable features separable in quantum space. Yet, the practical application of Quantum Machine Learning (QML), currently constrained by the limitations of Noisy Intermediate-Scale Quantum (NISQ) hardware, necessitates effective strategies to compress and embed large-scale real-world data like images into the constrained capacities of existing quantum devices or simulators. To this end, we propose Quantum Generator Kernels (QGKs), a generator-based approach to quantum kernels, comprising a set of Variational Generator Groups (VGGs) that merge universal generators into a parameterizable operator, ensuring scalable coverage of the available quantum space. Thereby, we address shortcomings of current leading strategies employing hybrid architectures, which might prevent exploiting quantum computing's full potential due to fixed intermediate embedding processes. To optimize the kernel alignment to the target domain, we train a weight vector to parameterize the projection of the VGGs in the current data context. Our empirical results demonstrate superior projection and classification capabilities of the QGK compared to state-of-the-art quantum and classical kernel approaches and show its potential to serve as a versatile framework for various QML applications.

</details>


### [1232] [The BoBW Algorithms for Heavy-Tailed MDPs](https://arxiv.org/abs/2602.01295)
*Yu Chen,Yuhao Liu,Jiatai Huang,Yihan Du,Longbo Huang*

Main category: cs.LG

Relevance: 30.0

TL;DR: 本文研究具有重尾反馈的马尔可夫决策过程，提出了两种算法HT-FTRL-OM和HT-FTRL-UOB，在对抗性和随机环境中均能获得最优性能保证。


<details>
  <summary>Details</summary>
Motivation: 现有处理重尾反馈MDP的方法在随机环境中过于保守，在对抗环境中缺乏适应性。本文旨在设计能够同时适应两种环境的算法，实现"两全其美"的性能保证。

Method: 提出两种算法：HT-FTRL-OM用于已知转移概率的情况，采用FTRL框架结合新颖的跳过损失估计器；HT-FTRL-UOB用于未知转移概率的情况，采用悲观跳过损失估计器。关键技术包括重尾偏移损失的局部控制机制、次优质量传播原理和新的遗憾分解方法。

Result: HT-FTRL-OM在对抗环境中达到$\widetilde{\mathcal{O}}(T^{1/α})$遗憾界，在随机环境中达到$\mathcal{O}(\log T)$遗憾界；HT-FTRL-UOB在对抗环境中达到$\widetilde{\mathcal{O}}(T^{1/α} + \sqrt{T})$遗憾界，在随机环境中达到$\mathcal{O}(\log^2(T))$遗憾界。

Conclusion: 本文提出的算法在重尾反馈MDP中实现了对抗性和随机环境下的最优性能平衡，通过技术创新克服了重尾估计误差和转移不确定性的关键障碍。

Abstract: We investigate episodic Markov Decision Processes with heavy-tailed feedback (HTMDPs). Existing approaches for HTMDPs are conservative in stochastic environments and lack adaptivity in adversarial regimes. In this work, we propose algorithms ```HT-FTRL-OM``` and ```HT-FTRL-UOB``` for HTMDPs that achieve Best-of-Both-Worlds (BoBW) guarantees: instance-independent regret in adversarial environments and logarithmic instance-dependent regret in self-bounding (including the stochastic case) environments. For the known transition setting, ```HT-FTRL-OM``` applies the Follow-The-Regularized-Leader (FTRL) framework over occupancy measures with novel skipping loss estimators, achieving a $\widetilde{\mathcal{O}}(T^{1/α})$ regret bound in adversarial regimes and a $\mathcal{O}(\log T)$ regret in stochastic regimes. Building upon this framework, we develop a novel algorithm ```HT-FTRL-UOB``` to tackle the more challenging unknown-transition setting. This algorithm employs a pessimistic skipping loss estimator and achieves a $\widetilde{\mathcal{O}}(T^{1/α} + \sqrt{T})$ regret in adversarial regimes and a $\mathcal{O}(\log^2(T))$ regret in stochastic regimes. Our analysis overcomes key barriers through several technical insights, including a local control mechanism for heavy-tailed shifted losses, a new suboptimal-mass propagation principle, and a novel regret decomposition that isolates transition uncertainty from heavy-tailed estimation errors and skipping bias.

</details>


### [1233] [Statistical Learning Theory in Lean 4: Empirical Processes from Scratch](https://arxiv.org/abs/2602.02285)
*Yuanhe Zhang,Jason D. Lee,Fanghui Liu*

Main category: cs.LG

Relevance: 30.0

TL;DR: 首个基于经验过程理论的统计学习理论（SLT）的完整Lean 4形式化，填补了Lean 4 Mathlib库的空白，包括高斯Lipschitz集中性、Dudley熵积分定理的形式化，并应用于最小二乘（稀疏）回归。


<details>
  <summary>Details</summary>
Motivation: 为统计学习理论建立可重用的形式化基础，通过人机协作工作流程实现理论的形式验证，揭示并解决标准SLT教科书中的隐含假设和缺失细节。

Method: 采用人机协作工作流程：人类设计证明策略，AI代理执行战术证明构造，最终得到人类验证的Lean 4工具箱。实现了高斯Lipschitz集中性、Dudley熵积分定理的完整形式化，并应用于最小二乘回归。

Result: 建立了首个基于经验过程理论的统计学习理论完整Lean 4形式化，填补了Lean 4 Mathlib库的关键空白，提供了可重用的形式化基础，并展示了在最小二乘回归中的实际应用。

Conclusion: 这项工作为机器学习理论建立了可重用的形式化基础，开启了未来发展的可能性，同时通过形式化过程加深了对理论的理解。

Abstract: We present the first comprehensive Lean 4 formalization of statistical learning theory (SLT) grounded in empirical process theory. Our end-to-end formal infrastructure implement the missing contents in latest Lean 4 Mathlib library, including a complete development of Gaussian Lipschitz concentration, the first formalization of Dudley's entropy integral theorem for sub-Gaussian processes, and an application to least-squares (sparse) regression with a sharp rate. The project was carried out using a human-AI collaborative workflow, in which humans design proof strategies and AI agents execute tactical proof construction, leading to the human-verified Lean 4 toolbox for SLT. Beyond implementation, the formalization process exposes and resolves implicit assumptions and missing details in standard SLT textbooks, enforcing a granular, line-by-line understanding of the theory. This work establishes a reusable formal foundation and opens the door for future developments in machine learning theory. The code is available at https://github.com/YuanheZ/lean-stat-learning-theory

</details>


### [1234] [PIMPC-GNN: Physics-Informed Multi-Phase Consensus Learning for Enhancing Imbalanced Node Classification in Graph Neural Networks](https://arxiv.org/abs/2602.01920)
*Abdul Joseph Fofanah,Lian Wen,David Chen*

Main category: cs.LG

Relevance: 30.0

TL;DR: PIMPC-GNN：一种物理信息多相共识框架，通过热力学扩散、Kuramoto同步和谱嵌入三种互补动力学解决图神经网络在类别不平衡节点分类中的问题。


<details>
  <summary>Details</summary>
Motivation: 图神经网络在类别不平衡设置中表现不佳，少数类别代表性不足导致预测偏向多数类别。现有方法难以有效处理图结构中的长程依赖和少数类别节点对齐问题。

Method: 提出物理信息多相共识框架，整合三种动力学：1) 热力学扩散传播少数标签捕获长程依赖；2) Kuramoto同步通过振荡共识对齐少数节点；3) 谱嵌入通过结构正则化分离类别。通过类别自适应集成权重和结合平衡交叉熵与物理约束的不平衡感知损失进行训练。

Result: 在5个基准数据集和5-100的不平衡比率下，PIMPC-GNN优于16个最先进基线，少数类别召回率提升高达+12.7%，平衡准确率提升高达+8.3%。

Conclusion: PIMPC-GNN有效解决了图神经网络中的类别不平衡问题，不仅提供实证改进，还为图学习中的共识动力学提供可解释性见解。

Abstract: Graph neural networks (GNNs) often struggle in class-imbalanced settings, where minority classes are under-represented and predictions are biased toward majorities. We propose \textbf{PIMPC-GNN}, a physics-informed multi-phase consensus framework for imbalanced node classification. Our method integrates three complementary dynamics: (i) thermodynamic diffusion, which spreads minority labels to capture long-range dependencies, (ii) Kuramoto synchronisation, which aligns minority nodes through oscillatory consensus, and (iii) spectral embedding, which separates classes via structural regularisation. These perspectives are combined through class-adaptive ensemble weighting and trained with an imbalance-aware loss that couples balanced cross-entropy with physics-based constraints. Across five benchmark datasets and imbalance ratios from 5-100, PIMPC-GNN outperforms 16 state-of-the-art baselines, achieving notable gains in minority-class recall (up to +12.7\%) and balanced accuracy (up to +8.3\%). Beyond empirical improvements, the framework also provides interpretable insights into consensus dynamics in graph learning. The code is available at \texttt{https://github.com/afofanah/PIMPC-GNN}.

</details>


### [1235] [Embedding Learning on Multiplex Networks for Link Prediction](https://arxiv.org/abs/2602.01922)
*Orell Trautmann,Olaf Wolkenhauer,Clémence Réda*

Main category: cs.LG

Relevance: 30.0

TL;DR: 这篇综述论文回顾了多路网络嵌入学习方法在链接预测任务中的应用，提出了新的分类体系，并解决了可重复性和公平评估的问题，特别针对有向多路网络提出了新的测试流程。


<details>
  <summary>Details</summary>
Motivation: 随着网络复杂性的增加（连接数量和交互类型的增多），网络嵌入学习变得越来越具有挑战性。多路网络嵌入学习在链接预测任务中显示出巨大潜力，但现有模型缺乏统一的分类体系，评估方法也存在可重复性和公平性问题，特别是有向多路网络的评估尚未得到充分解决。

Method: 1. 提出了细化的分类体系，根据嵌入类型和嵌入技术对多路网络嵌入模型进行分类和比较；2. 回顾并解决了多路网络嵌入学习在链接预测任务中的可重复性和公平评估问题；3. 针对有向多路网络提出了一种新颖且公平的测试流程。

Result: 1. 建立了系统化的多路网络嵌入模型分类框架；2. 提出了确保评估可重复性和公平性的方法；3. 开发了专门针对有向多路网络的评估协议；4. 为未来更高效、可扩展的多路网络嵌入方法开发奠定了基础。

Conclusion: 这篇综述为多路网络嵌入学习的发展提供了重要指导，提出了模型评估的指导原则，并展望了当前可用于处理多路网络下游分析的挑战和工具。这是开发更高效、可扩展的多路网络嵌入方法及其公平评估的关键一步。

Abstract: Over the past years, embedding learning on networks has shown tremendous results in link prediction tasks for complex systems, with a wide range of real-life applications. Learning a representation for each node in a knowledge graph allows us to capture topological and semantic information, which can be processed in downstream analyses later. In the link prediction task, high-dimensional network information is encoded into low-dimensional vectors, which are then fed to a predictor to infer new connections between nodes in the network. As the network complexity (that is, the numbers of connections and types of interactions) grows, embedding learning turns out increasingly challenging. This review covers published models on embedding learning on multiplex networks for link prediction. First, we propose refined taxonomies to classify and compare models, depending on the type of embeddings and embedding techniques. Second, we review and address the problem of reproducible and fair evaluation of embedding learning on multiplex networks for the link prediction task. Finally, we tackle evaluation on directed multiplex networks by proposing a novel and fair testing procedure. This review constitutes a crucial step towards the development of more performant and tractable embedding learning approaches for multiplex networks and their fair evaluation for the link prediction task. We also suggest guidelines on the evaluation of models, and provide an informed perspective on the challenges and tools currently available to address downstream analyses applied to multiplex networks.

</details>


### [1236] [The Maximum von Neumann Entropy Principle: Theory and Applications in Machine Learning](https://arxiv.org/abs/2602.02117)
*Youqi Wu,Farzan Farnia*

Main category: cs.LG

Relevance: 30.0

TL;DR: 该论文将冯·诺依曼熵的最大熵原理扩展到机器学习领域，提出了基于博弈论解释的最大VNE框架，并应用于核表示选择和核矩阵补全问题。


<details>
  <summary>Details</summary>
Motivation: 冯·诺依曼熵在量子信息理论中是基本概念，近年来被引入机器学习作为核矩阵和核协方差算子的谱多样性度量。然而，在数据驱动场景中，缺乏类似经典最大熵原理的决策理论和博弈论解释的VNE最大化框架。

Method: 扩展Grünwald和Dawid的最大熵极小极大公式到冯·诺依曼熵设置，为密度矩阵和迹归一化半正定算子的VNE最大化提供博弈论证明。将最大VNE原理应用于两个机器学习问题：1）从多个归一化嵌入中选择核表示；2）从部分观测条目补全核矩阵。

Result: 提出了一个统一的信息论基础，为核学习中的VNE方法提供理论支撑。最大VNE解在部分信息下具有稳健解释，可作为谱域中最不承诺的推断。

Conclusion: 该工作为机器学习中的冯·诺依曼熵最大化建立了决策理论和博弈论基础，为核学习方法提供了统一的信息论框架。

Abstract: Von Neumann entropy (VNE) is a fundamental quantity in quantum information theory and has recently been adopted in machine learning as a spectral measure of diversity for kernel matrices and kernel covariance operators. While maximizing VNE under constraints is well known in quantum settings, a principled analogue of the classical maximum entropy framework, particularly its decision theoretic and game theoretic interpretation, has not been explicitly developed for VNE in data driven contexts. In this paper, we extend the minimax formulation of the maximum entropy principle due to Grünwald and Dawid to the setting of von Neumann entropy, providing a game-theoretic justification for VNE maximization over density matrices and trace-normalized positive semidefinite operators. This perspective yields a robust interpretation of maximum VNE solutions under partial information and clarifies their role as least committed inferences in spectral domains. We then illustrate how the resulting Maximum VNE principle applies to modern machine learning problems by considering two representative applications, selecting a kernel representation from multiple normalized embeddings via kernel-based VNE maximization, and completing kernel matrices from partially observed entries. These examples demonstrate how the proposed framework offers a unifying information-theoretic foundation for VNE-based methods in kernel learning.

</details>


### [1237] [Variational Entropic Optimal Transport](https://arxiv.org/abs/2602.02241)
*Roman Dyachenko,Nikita Gushchin,Kirill Sokolov,Petr Mokrov,Evgeny Burnaev,Alexander Korotin*

Main category: cs.LG

Relevance: 30.0

TL;DR: 本文提出了VarEOT方法，通过变分重构log-partition项来解决连续空间中熵最优传输的计算难题，避免了现有方法的限制性参数化或模拟训练需求。


<details>
  <summary>Details</summary>
Motivation: 连续空间中二次成本的熵最优传输是解决领域转换问题的经典工具，但实践中优化弱对偶EOT目标时，由于log-partition项难以处理导致计算效率低下。现有方法要么限制传输族（如高斯混合参数化），要么需要模拟训练过程，都存在局限性。

Method: 提出VarEOT方法，基于log-partition项log 𝔼[exp(·)]的精确变分重构，将其转化为对辅助正归一化器的可处理最小化问题。这产生了可用随机梯度优化的可微学习目标，避免了训练期间的MCMC模拟。

Result: 在合成数据和未配对图像到图像转换实验中，展示了竞争性或改进的转换质量。与使用相同弱对偶EOT目标的其他求解器比较，支持了所提优化原则的益处。

Conclusion: VarEOT提供了一种计算高效的熵最优传输方法，具有理论保证（包括有限样本泛化界限和通用函数逼近下的近似结果），避免了现有方法的限制。

Abstract: Entropic optimal transport (EOT) in continuous spaces with quadratic cost is a classical tool for solving the domain translation problem. In practice, recent approaches optimize a weak dual EOT objective depending on a single potential, but doing so is computationally not efficient due to the intractable log-partition term. Existing methods typically resolve this obstacle in one of two ways: by significantly restricting the transport family to obtain closed-form normalization (via Gaussian-mixture parameterizations), or by using general neural parameterizations that require simulation-based training procedures. We propose Variational Entropic Optimal Transport (VarEOT), based on an exact variational reformulation of the log-partition $\log \mathbb{E}[\exp(\cdot)]$ as a tractable minimization over an auxiliary positive normalizer. This yields a differentiable learning objective optimized with stochastic gradients and avoids the necessity of MCMC simulations during the training. We provide theoretical guarantees, including finite-sample generalization bounds and approximation results under universal function approximation. Experiments on synthetic data and unpaired image-to-image translation demonstrate competitive or improved translation quality, while comparisons within the solvers that use the same weak dual EOT objective support the benefit of the proposed optimization principle.

</details>


### [1238] [Exact Instance Compression for Convex Empirical Risk Minimization via Color Refinement](https://arxiv.org/abs/2602.00437)
*Bryan Zhu,Ziang Chen*

Main category: math.OC

Relevance: 30.0

TL;DR: 提出基于颜色细化的无损压缩框架，用于凸经验风险最小化问题，扩展了线性规划和凸二次规划到更广泛的凸优化问题


<details>
  <summary>Details</summary>
Motivation: 经验风险最小化（ERM）计算成本高，标准求解器在凸设置下扩展性差，需要更高效的求解方法

Method: 基于颜色细化的无损压缩框架，将凸ERM问题压缩为更小规模的问题，适用于线性/多项式回归、逻辑回归、弹性网络正则化回归、核方法等多种模型

Result: 在代表性数据集上的数值实验证明了该方法的有效性，能够显著减少计算复杂度

Conclusion: 提出的颜色细化框架为凸ERM问题提供了有效的无损压缩方法，扩展了先前工作的适用范围

Abstract: Empirical risk minimization (ERM) can be computationally expensive, with standard solvers scaling poorly even in the convex setting. We propose a novel lossless compression framework for convex ERM based on color refinement, extending prior work from linear programs and convex quadratic programs to a broad class of differentiable convex optimization problems. We develop concrete algorithms for a range of models, including linear and polynomial regression, binary and multiclass logistic regression, regression with elastic-net regularization, and kernel methods such as kernel ridge regression and kernel logistic regression. Numerical experiments on representative datasets demonstrate the effectiveness of the proposed approach.

</details>


### [1239] [Harmful Overfitting in Sobolev Spaces](https://arxiv.org/abs/2602.00825)
*Kedar Karhadkar,Alexander Sietsema,Deanna Needell,Guido Montufar*

Main category: stat.ML

Relevance: 30.0

TL;DR: 研究Sobolev空间中函数在过参数化情况下的泛化行为，发现即使训练样本量趋于无穷，近似范数最小化的插值器仍会表现出有害过拟合，泛化误差保持正下界。


<details>
  <summary>Details</summary>
Motivation: 受到近期关于过参数化机器学习中良性过拟合研究的启发，旨在理解Sobolev空间中完美拟合噪声训练数据集的函数的泛化行为，特别是探究平滑性偏置选择的规范最小化插值器是否会导致有害过拟合。

Method: 研究Sobolev空间W^{k,p}(ℝ^d)中的函数，在标签噪声和数据分布足够正则性的假设下，分析近似范数最小化插值器的泛化性能。使用几何论证方法，通过Sobolev不等式识别训练数据的有害邻域，对比先前仅研究希尔伯特空间(p=2)的核方法。

Result: 发现近似范数最小化插值器表现出有害过拟合：即使训练样本量n→∞，泛化误差以高概率保持正下界。这一结果适用于任意p∈[1,∞)值，扩展了先前仅针对p=2情况的研究。

Conclusion: 在Sobolev空间中，平滑性偏置选择的规范最小化插值器会导致有害过拟合，即使在无限训练数据下也无法避免泛化误差的正下界，这对理解过参数化模型的泛化行为具有重要意义。

Abstract: Motivated by recent work on benign overfitting in overparameterized machine learning, we study the generalization behavior of functions in Sobolev spaces $W^{k, p}(\mathbb{R}^d)$ that perfectly fit a noisy training data set. Under assumptions of label noise and sufficient regularity in the data distribution, we show that approximately norm-minimizing interpolators, which are canonical solutions selected by smoothness bias, exhibit harmful overfitting: even as the training sample size $n \to \infty$, the generalization error remains bounded below by a positive constant with high probability. Our results hold for arbitrary values of $p \in [1, \infty)$, in contrast to prior results studying the Hilbert space case ($p = 2$) using kernel methods. Our proof uses a geometric argument which identifies harmful neighborhoods of the training data using Sobolev inequalities.

</details>


### [1240] [Sheaf Neural Networks and biomedical applications](https://arxiv.org/abs/2602.00159)
*Aneeqa Mehrab,Jan Willem Van Looy,Pietro Demurtas,Stefano Iotti,Emil Malucelli,Francesca Rossi,Ferdinando Zanchetta,Rita Fioresi*

Main category: cs.LG

Relevance: 25.0

TL;DR: 论文介绍了层状神经网络（SNN）的理论与数学模型，并通过生物医学案例研究证明SNN在回答生物医学问题上优于主流图神经网络（GCN、GAT、GraphSage）


<details>
  <summary>Details</summary>
Motivation: 当前图神经网络（GNNs）在处理复杂生物医学问题时存在局限性，需要更强大的数学框架来建模复杂关系。层状神经网络（SNN）基于层状理论，能更好地捕捉数据中的局部-全局关系，为生物医学问题提供更有效的解决方案。

Method: 提出层状神经网络（SNN）算法，基于层状理论构建数学模型，将数据表示为层状结构，通过层状同调等数学工具进行信息传递和聚合。在具体生物医学案例研究中与主流GNNs（GCN、GAT、GraphSage）进行对比实验。

Result: 在生物医学案例研究中，SNN算法在回答生物医学问题上显著优于图卷积网络（GCN）、图注意力网络（GAT）和GraphSage等主流图神经网络，证明了其在实际应用中的优越性。

Conclusion: 层状神经网络（SNN）基于严格的数学理论框架，在生物医学等复杂领域应用中展现出比传统图神经网络更强的性能，为处理具有复杂结构关系的数据提供了新的有效方法。

Abstract: The purpose of this paper is to elucidate the theory and mathematical modelling behind the sheaf neural network (SNN) algorithm and then show how SNN can effectively answer to biomedical questions in a concrete case study and outperform the most popular graph neural networks (GNNs) as graph convolutional networks (GCNs), graph attention networks (GAT) and GraphSage.

</details>


### [1241] [RePaint-Enhanced Conditional Diffusion Model for Parametric Engineering Designs under Performance and Parameter Constraints](https://arxiv.org/abs/2602.00384)
*Ke Wang,Nguyen Gia Hien Vu,Yifan Tang,Mostafa Rahmani Dehaghani,G. Gary Wang*

Main category: cs.LG

Relevance: 25.0

TL;DR: 提出基于RePaint增强的框架，将预训练的性能引导DDPM用于工程设计中，能在不重新训练模型的情况下，根据部分参考设计生成满足性能约束的缺失组件。


<details>
  <summary>Details</summary>
Motivation: 传统基于DDPM的设计生成方法无法同时处理性能约束和参数约束，特别是在基于部分参考设计生成缺失组件时缺乏可控性。工程设计中需要能够根据已有设计部分生成满足特定性能要求的新设计。

Method: 采用RePaint增强框架，集成预训练的性能引导DDPM，在推理过程中使用基于掩码的重采样技术，实现部分设计的可控重绘，同时满足性能和参数约束。

Result: 在参数化船体设计和翼型设计两个代表性问题上验证，能够基于部分参考设计生成具有预期性能的新颖设计，精度达到或优于预训练模型，同时通过固定部分设计实现可控创新。

Conclusion: 该方法为工程应用中的参数约束感知生成设计提供了高效、无需重新训练的解决方案，扩展了DDPM在工程设计生成中的应用范围。

Abstract: This paper presents a RePaint-enhanced framework that integrates a pre-trained performance-guided denoising diffusion probabilistic model (DDPM) for performance- and parameter-constraint engineering design generation. The proposed method enables the generation of missing design components based on a partial reference design while satisfying performance constraints, without retraining the underlying model. By applying mask-based resampling during inference process, RePaint allows efficient and controllable repainting of partial designs under both performance and parameter constraints, which is not supported by conventional DDPM-base methods. The framework is evaluated on two representative design problems, parametric ship hull design and airfoil design, demonstrating its ability to generate novel designs with expected performance based on a partial reference design. Results show that the method achieves accuracy comparable to or better than pre-trained models while enabling controlled novelty through fixing partial designs. Overall, the proposed approach provides an efficient, training-free solution for parameter-constraint-aware generative design in engineering applications.

</details>


### [1242] [Localized, High-resolution Geographic Representations with Slepian Functions](https://arxiv.org/abs/2602.00392)
*Arjun Rao,Ruth Crasto,Tessa Ooms,David Rolnick,Konstantin Klemmer,Marc Rußwurm*

Main category: cs.LG

Relevance: 25.0

TL;DR: 提出了一种基于球面Slepian函数的地理位置编码器，能够将表示能力集中在感兴趣区域内，并在高分辨率下保持计算效率，同时提供混合Slepian-球谐编码器来平衡局部-全局性能。


<details>
  <summary>Details</summary>
Motivation: 地理数据本质上是局部的（如疾病爆发集中在人口中心、生态模式沿海岸线出现），但现有机器学习模型的地理位置编码器将表示能力均匀分布在全球范围，难以满足局部应用所需的高分辨率需求。

Method: 1. 使用球面Slepian函数构建地理位置编码器，将表示能力集中在感兴趣区域内
2. 提出混合Slepian-球谐编码器，在需要全局上下文时有效平衡局部-全局性能
3. 保持极地安全性和球面距离保持等理想特性

Result: 在五个任务（分类、回归、图像增强预测）中，Slepian编码优于基线方法，并在广泛的神经网络架构中保持性能优势。

Conclusion: Slepian函数为地理位置编码提供了有效的解决方案，能够集中表示能力于局部区域，同时通过混合方法平衡局部和全局需求，在多种任务和架构中表现优异。

Abstract: Geographic data is fundamentally local. Disease outbreaks cluster in population centers, ecological patterns emerge along coastlines, and economic activity concentrates within country borders. Machine learning models that encode geographic location, however, distribute representational capacity uniformly across the globe, struggling at the fine-grained resolutions that localized applications require. We propose a geographic location encoder built from spherical Slepian functions that concentrate representational capacity inside a region-of-interest and scale to high resolutions without extensive computational demands. For settings requiring global context, we present a hybrid Slepian-Spherical Harmonic encoder that efficiently bridges the tradeoff between local-global performance, while retaining desirable properties such as pole-safety and spherical-surface-distance preservation. Across five tasks spanning classification, regression, and image-augmented prediction, Slepian encodings outperform baselines and retain performance advantages across a wide range of neural network architectures.

</details>


### [1243] [Robustness of AutoML on Dirty Categorical Data](https://arxiv.org/abs/2602.00412)
*Marcos L. P. Bueno,Joaquin Vanschoren*

Main category: cs.LG

Relevance: 25.0

TL;DR: 提出一个将分类数据转换为数值数据的管道，使AutoML方法能够处理经过高级编码方案处理的脏分类数据，并评估当前AutoML方法在脏数据集上的鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 虽然AutoML方法在分类任务中能够处理数据缺陷（如异常值、多尺度和缺失数据），但它们在脏分类数据集上的行为尚不清楚。这些数据集通常具有高基数分类特征，而最近研究表明形态编码器可以提升模型性能，但这类编码器在AutoML中的效果未知。

Method: 提出一个管道，将分类数据转换为数值数据，使AutoML能够处理经过高级编码方案处理的分类数据。在脏数据集上对当前AutoML方法的鲁棒性进行基准测试，并与提出的管道进行比较，分析预测性能差异，并检查AutoML构建的ML管道以获取更深入洞察。

Result: 论文通过基准测试比较了当前AutoML方法与提出的管道在脏分类数据集上的性能差异，提供了关于预测性能的深入分析，并超越了AutoML通常返回的最佳模型，提供了更全面的ML管道洞察。

Conclusion: 提出的管道能够有效处理脏分类数据，提升AutoML方法在具有高基数分类特征的脏数据集上的性能，为AutoML处理分类数据提供了更先进的编码方案。

Abstract: The goal of automated machine learning (AutoML) is to reduce trial and error when doing machine learning (ML). Although AutoML methods for classification are able to deal with data imperfections, such as outliers, multiple scales and missing data, their behavior is less known on dirty categorical datasets. These datasets often have several categorical features with high cardinality arising from issues such as lack of curation and automated collection. Recent research has shown that ML models can benefit from morphological encoders for dirty categorical data, leading to significantly superior predictive performance. However the effects of using such encoders in AutoML methods are not known at the moment. In this paper, we propose a pipeline that transforms categorical data into numerical data so that an AutoML can handle categorical data transformed by more advanced encoding schemes. We benchmark the current robustness of AutoML methods on a set of dirty datasets and compare it with the proposed pipeline. This allows us to get insight on differences in predictive performance. We also look at the ML pipelines built by AutoMLs in order to gain insight beyond the best model as typically returned by these methods.

</details>


### [1244] [Three-Way Emotion Classification of EEG-based Signals using Machine Learning](https://arxiv.org/abs/2602.00670)
*Ashna Purwar,Gaurav Simkar,Madhumita,Sachin Kadam*

Main category: cs.LG

Relevance: 25.0

TL;DR: 该论文使用机器学习模型（逻辑回归、支持向量机、随机森林）对EEG信号进行三分类情感识别（负向、中性、正向），发现随机森林模型在准确率和F1分数上表现最佳。


<details>
  <summary>Details</summary>
Motivation: EEG信号能够直接反映大脑活动，可用于识别人的情感状态。情感感知系统和EEG情感识别是新兴研究领域，需要探索有效的机器学习方法来进行多类别情感分类。

Method: 使用有限数据集，比较三种常用机器学习模型：逻辑回归(LR)、支持向量机(SVM)和随机森林(RF)。完整工作流程包括数据预处理、模型训练和测试，通过准确率和F1分数评估性能。

Result: 机器学习模型能有效用于EEG信号的三分类情感识别。在三种模型中，随机森林表现最佳，其更高的准确率和F1分数表明它能更准确有效地捕捉情感模式，且在准确率参数上超越了现有最先进分类模型。

Conclusion: 随机森林模型在EEG情感三分类任务中表现最优，为EEG情感识别提供了有效的机器学习解决方案。

Abstract: Electroencephalography (EEG) is a widely used technique for measuring brain activity. EEG-based signals can reveal a persons emotional state, as they directly reflect activity in different brain regions. Emotion-aware systems and EEG-based emotion recognition are a growing research area. This paper presents how machine learning (ML) models categorize a limited dataset of EEG signals into three different classes, namely Negative, Neutral, or Positive. It also presents the complete workflow, including data preprocessing and comparison of ML models. To understand which ML classification model works best for this kind of problem, we train and test the following three commonly used models: logistic regression (LR), support vector machine (SVM), and random forest (RF). The performance of each is evaluated with respect to accuracy and F1-score. The results indicate that ML models can be effectively utilized for three-way emotion classification of EEG signals. Among the three ML models trained on the available dataset, the RF model gave the best results. Its higher accuracy and F1-score suggest that it is able to capture the emotional patterns more accurately and effectively than the other two models. The RF model also outperformed the existing state-of-the-art classification models in terms of the accuracy parameter.

</details>


### [1245] [A novel VAE-DML fusion framework for casual analysis of greenwashing in the mining industry](https://arxiv.org/abs/2602.00774)
*Yuxin Lu,Zhen Peng,Xiqiang Xia,Jie Wang*

Main category: cs.LG

Relevance: 25.0

TL;DR: 该研究探讨股权制衡对矿业产业链企业绿色漂洗行为的抑制作用，采用变分自编码器和双重机器学习模型构建反事实场景，验证了股权制衡通过缓解管理层业绩压力、增强高管团队稳定性、强化媒体监督三条路径抑制绿色漂洗。


<details>
  <summary>Details</summary>
Motivation: 在全球绿色转型和"双碳"目标背景下，矿业产业链企业作为资源消耗和环境影响的重点实体，其环境绩效直接影响区域生态安全和国家资源战略。确保企业环境信息披露的真实可靠是可持续发展的核心问题。从公司治理角度，研究股权制衡这一基础治理机制对绿色漂洗行为的抑制作用及其路径。

Method: 创新性地采用变分自编码器（VAE）和双重机器学习（DML）模型构建反事实场景，有效缓解内生性问题，精确识别股权制衡与绿色漂洗之间的因果关系。

Result: 1. 股权制衡与企业绿色漂洗存在显著负向因果关系，证实其治理效应；2. 抑制作用存在显著异质性，在西部地区、产业链上游环节、高环境敏感性行业表现更强；3. 治理效应具有明显时序动态性，当期效应最强，滞后效应递减但仍显著，最终形成稳定长期累积影响；4. 机制分析显示股权制衡通过缓解管理层业绩压力、增强高管团队稳定性、强化媒体监督三条路径抑制绿色漂洗。

Conclusion: 股权制衡作为有效的公司治理机制，能够显著抑制矿业产业链企业的绿色漂洗行为，其作用具有异质性和时序动态特征，主要通过缓解管理层压力、稳定高管团队和强化外部监督三条路径实现。

Abstract: Against the backdrop of the global green transition and "dual carbon" goals, mining industry chain enterprises are pivotal entities in terms of resource consumption and environmental impact. Their environmental performance directly affects regional ecological security and is closely tied to national resource strategies and green transformation outcomes. Ensuring the authenticity and reliability of their environmental disclosure is thus a core and urgent issue for sustainable development and national strategic objectives.From a corporate governance perspective, this study examines equity balance as a fundamental governance mechanism, investigating its inhibitory effect on greenwashing behavior among these enterprises and the underlying pathways involved. Methodologically, the paper innovatively employs a Variational Autoencoder (VAE) and a Double Machine Learning (DML) model to construct counterfactual scenarios, mitigating endogeneity concerns and precisely identifying the causal relationship between equity balance and greenwashing. The findings indicate, first, a significant negative causal relationship between equity balance and corporate greenwashing, confirming its substantive governance effect. Second, this inhibitory effect exhibits notable heterogeneity, manifesting more strongly in western regions, upstream segments of the industrial chain, and industries with high environmental sensitivity. Third, the governance effect demonstrates clear temporal dynamics, with the strongest impact occurring in the current period, followed by a diminishing yet statistically significant lagged effect, and ultimately a stable long-term cumulative influence. Finally, mechanism analysis reveals that equity balance operates through three distinct channels to curb greenwashing: alleviating management performance pressure, enhancing the stability of the executive team, and intensifying media scrutiny.

</details>


### [1246] [From drift to adaptation to the failed ml model: Transfer Learning in Industrial MLOps](https://arxiv.org/abs/2602.00957)
*Waqar Muhammad Ashraf,Talha Ansar,Fahad Ahmed,Jawad Hussain,Muhammad Mujtaba Abbas,Vivek Dua*

Main category: cs.LG

Relevance: 25.0

TL;DR: 该论文比较了三种迁移学习方法（ETL、ALTL、LLTL）在数据漂移下更新失效ANN模型的性能，基于电厂烟气差压案例研究发现ETL在5天批次下表现最佳，ALTL在8天批次下更有效。


<details>
  <summary>Details</summary>
Motivation: 当前MLOps中缺乏系统化的模型更新框架来处理数据漂移导致的模型失效问题，特别是在工业批处理过程中，需要有效的迁移学习策略来适应数据变化。

Method: 使用三种迁移学习方法：集成迁移学习（ETL）、全层迁移学习（ALTL）和最后一层迁移学习（LLTL），在660MW热电厂空气预热器烟气差压监测案例中更新失效的ANN模型，比较不同批次大小下的性能。

Result: ETL在5天批次大小下提供相对更高的预测精度，而ALTL在8天批次大小下更有效。不同批次大小的计算需求呈现混合趋势。

Conclusion: 基于批处理的工业案例研究为MLOps从业者提供了适应数据漂移的实用指导，不同迁移学习策略在不同批次大小下各有优势。

Abstract: Model adaptation to production environment is critical for reliable Machine Learning Operations (MLOps), less attention is paid to developing systematic framework for updating the ML models when they fail under data drift. This paper compares the transfer learning enabled model update strategies including ensemble transfer learning (ETL), all-layers transfer learning (ALTL), and last-layer transfer learning (LLTL) for updating the failed feedforward artificial neural network (ANN) model. The flue gas differential pressure across the air preheater unit installed in a 660 MW thermal power plant is analyzed as a case study since it mimics the batch processes due to load cycling in the power plant. Updating the failed ANN model by three transfer learning techniques reveals that ETL provides relatively higher predictive accuracy for the batch size of 5 days than those of LLTL and ALTL. However, ALTL is found to be suitable for effective update of the model trained on large batch size (8 days). A mixed trend is observed for computational requirement (hyperparameter tuning and model training) of model update techniques for different batch sizes. These fundamental and empiric insights obtained from the batch process-based industrial case study can assist the MLOps practitioners in adapting the failed models to data drifts for the accurate monitoring of industrial processes.

</details>


### [1247] [SwiftRepertoire: Few-Shot Immune-Signature Synthesis via Dynamic Kernel Codes](https://arxiv.org/abs/2602.01051)
*Rong Fu,Wenxin Zhang,Muge Qi,Yang Li,Yabin Jin,Jiekai Wu,Jiaxuan Lu,Chunlei Meng,Youjin Wang,Zeli Su,Juntao Gao,Li Bao,Qi Zhao,Wei Luo,Simon Fong*

Main category: cs.LG

Relevance: 25.0

TL;DR: 提出一个用于T细胞受体(TCR)分析的新框架，通过原型字典和轻量任务描述符合成任务特定参数化，实现小样本适应，无需完整微调预训练模型。


<details>
  <summary>Details</summary>
Motivation: T细胞受体分析为疾病检测和免疫监测提供生物信号，但实际部署面临标签稀疏、队列异质性以及大型编码器适应新任务的计算负担等问题。

Method: 1) 从学习的原型字典合成紧凑的任务特定参数化；2) 使用基于repertoire探针和池化嵌入统计的轻量任务描述符；3) 生成小型适配器模块应用于冻结的预训练骨干网络；4) 通过motif感知探针和校准的motif发现管道保持可解释性。

Result: 该框架能够在仅有少量支持样本的情况下立即适应新任务，无需完整模型微调，提供实用、样本高效且可解释的解决方案。

Conclusion: 为在标签数据稀缺和计算资源受限的临床和研究环境中部署repertoire信息模型提供了可行的技术路径。

Abstract: Repertoire-level analysis of T cell receptors offers a biologically grounded signal for disease detection and immune monitoring, yet practical deployment is impeded by label sparsity, cohort heterogeneity, and the computational burden of adapting large encoders to new tasks. We introduce a framework that synthesizes compact task-specific parameterizations from a learned dictionary of prototypes conditioned on lightweight task descriptors derived from repertoire probes and pooled embedding statistics. This synthesis produces small adapter modules applied to a frozen pretrained backbone, enabling immediate adaptation to novel tasks with only a handful of support examples and without full model fine-tuning. The architecture preserves interpretability through motif-aware probes and a calibrated motif discovery pipeline that links predictive decisions to sequence-level signals. Together, these components yield a practical, sample-efficient, and interpretable pathway for translating repertoire-informed models into diverse clinical and research settings where labeled data are scarce and computational resources are constrained.

</details>


### [1248] [Multi-Horizon Electricity Price Forecasting with Deep Learning in the Australian National Electricity Market](https://arxiv.org/abs/2602.01157)
*Mohammed Osman Gani,Zhipeng He,Chun Ouyang,Sara Khalifa*

Main category: cs.LG

Relevance: 25.0

TL;DR: 该论文提出一个多日前电价预测框架，采用先进的时间序列深度学习模型，在澳大利亚电力市场进行综合评估，发现标准DL模型在多数区域表现更优，而SOTA模型对预测时域扩展更具鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 电价预测对电力系统运营至关重要，但现有深度学习应用存在三个主要不足：1) 预测时域局限于日前，缺乏多日前预测；2) 未充分探索最先进的时间序列深度学习模型；3) 评估主要基于聚合水平，忽略了日内时段预测差异。

Method: 提出一个新颖的电价预测框架，将预测时域扩展到多日前，系统性地构建基于基准SOTA时间序列深度学习模型的预测模型。在澳大利亚国家电力市场五个区域进行综合评估，分析日内时段预测性能。

Result: 结果显示：1) 没有单一模型在所有区域、指标和时域上表现一致最优；2) 标准DL模型在多数区域表现更优；3) SOTA时间序列DL模型对预测时域扩展更具鲁棒性；4) 日内评估揭示明显的昼夜误差模式：绝对误差在晚间爬坡期最高，相对误差在午间负电价时段膨胀，方向准确性在趋势频繁变化期下降。

Conclusion: 未来基于DL的电价预测研究应关注增强特征表示和建模策略，以提高长期预测鲁棒性，同时保持对日内波动和结构性价格动态的敏感性。

Abstract: Accurate electricity price forecasting (EPF) is essential for operational planning, trading, and flexible asset scheduling in liberalised power systems, yet remains challenging due to volatility, heavy-tailed spikes, and frequent regime shifts. While deep learning (DL) has been increasingly adopted in EPF to capture complex and nonlinear price dynamics, several important gaps persist: (i) limited attention to multi-day horizons beyond day-ahead forecasting, (ii) insufficient exploration of state-of-the-art (SOTA) time series DL models, and (iii) a predominant reliance on aggregated horizon-level evaluation that obscures time-of-day forecasting variation. To address these gaps, we propose a novel EPF framework that extends the forecast horizon to multi-day-ahead by systematically building forecasting models that leverage benchmarked SOTA time series DL models. We conduct a comprehensive evaluation to analyse time-of-day forecasting performance by integrating model assessment at intraday interval levels across all five regions in the Australian National Electricity Market (NEM). The results show that no single model consistently dominates across regions, metrics, and horizons. Overall, standard DL models deliver superior performance in most regions, while SOTA time series DL models demonstrate greater robustness to forecast horizon extension. Intraday interval-level evaluation reveals pronounced diurnal error patterns, indicating that absolute errors peak during the evening ramp, relative errors inflate during midday negative-price regimes, and directional accuracy degrades during periods of frequent trend changes. These findings suggest that future research on DL-based EPF can benefit from enriched feature representations and modelling strategies that enhance longer-term forecasting robustness while maintaining sensitivity to intraday volatility and structural price dynamics.

</details>


### [1249] [Rethinking the Flow-Based Gradual Domain Adaption: A Semi-Dual Optimal Transport Perspective](https://arxiv.org/abs/2602.01179)
*Zhichao Chen,Zhan Zhuang,Yunfei Teng,Hao Wang,Fangyikang Wang,Zhengnan Li,Tianqiao Liu,Haoxuan Li,Zhouchen Lin*

Main category: cs.LG

Relevance: 25.0

TL;DR: 提出E-SUOT框架，通过熵正则化半对偶不平衡最优传输构建中间域，解决流模型在渐进域适应中因似然估计导致的信息丢失问题


<details>
  <summary>Details</summary>
Motivation: 渐进域适应(GDA)需要中间域来缓解域偏移，但真实中间域通常不可得或无效。现有流模型方法通过插值源域和目标域分布来合成中间样本，但基于样本的似然估计训练会丢失有用信息，降低GDA性能

Method: 将基于流的GDA重新表述为拉格朗日对偶问题，推导出等效的半对偶目标函数，避免似然估计需求。引入熵正则化将不稳定的min-max训练过程转换为更稳定的交替优化过程，提出E-SUOT框架

Result: 广泛的实验验证了E-SUOT框架的有效性，理论分析证明了其在稳定性和泛化性方面的优势

Conclusion: E-SUOT框架通过最优传输理论有效解决了渐进域适应中中间域构建的问题，避免了传统流模型的信息丢失缺陷

Abstract: Gradual domain adaptation (GDA) aims to mitigate domain shift by progressively adapting models from the source domain to the target domain via intermediate domains. However, real intermediate domains are often unavailable or ineffective, necessitating the synthesis of intermediate samples. Flow-based models have recently been used for this purpose by interpolating between source and target distributions; however, their training typically relies on sample-based log-likelihood estimation, which can discard useful information and thus degrade GDA performance. The key to addressing this limitation is constructing the intermediate domains via samples directly. To this end, we propose an Entropy-regularized Semi-dual Unbalanced Optimal Transport (E-SUOT) framework to construct intermediate domains. Specifically, we reformulate flow-based GDA as a Lagrangian dual problem and derive an equivalent semi-dual objective that circumvents the need for likelihood estimation. However, the dual problem leads to an unstable min-max training procedure. To alleviate this issue, we further introduce entropy regularization to convert it into a more stable alternative optimization procedure. Based on this, we propose a novel GDA training framework and provide theoretical analysis in terms of stability and generalization. Finally, extensive experiments are conducted to demonstrate the efficacy of the E-SUOT framework.

</details>


### [1250] [Deep Variational Contrastive Learning for Joint Risk Stratification and Time-to-Event Estimation](https://arxiv.org/abs/2602.01367)
*Pinar Erbil,Alberto Archetti,Eugenio Lomurno,Matteo Matteucci*

Main category: cs.LG

Relevance: 25.0

TL;DR: CONVERSE是一个结合变分自编码器和对比学习的深度生存分析模型，旨在平衡预测性能与可解释性，通过聚类实现患者风险分层。


<details>
  <summary>Details</summary>
Motivation: 临床决策需要生存分析来估计时间到事件结果、分层患者风险并指导治疗规划。深度学习虽然具有强大的预测能力，但黑盒特性限制了临床应用。而基于深度聚类的方法虽然可解释，但通常牺牲预测性能。需要一种能同时兼顾预测准确性和可解释性的生存分析模型。

Method: CONVERSE结合变分自编码器与对比学习，使用变分嵌入和多种集群内/集群间对比损失。采用自步学习从易到难逐步纳入样本以提高训练稳定性。模型支持集群特定的生存头，实现准确的集成预测。

Result: 在四个基准数据集上的综合评估表明，CONVERSE相比现有深度生存方法实现了竞争性或更优的性能，同时保持了有意义的患者分层。

Conclusion: CONVERSE成功弥合了生存分析中预测性能与可解释性之间的差距，为临床决策提供了既准确又可解释的风险分层工具。

Abstract: Survival analysis is essential for clinical decision-making, as it allows practitioners to estimate time-to-event outcomes, stratify patient risk profiles, and guide treatment planning. Deep learning has revolutionized this field with unprecedented predictive capabilities but faces a fundamental trade-off between performance and interpretability. While neural networks achieve high accuracy, their black-box nature limits clinical adoption. Conversely, deep clustering-based methods that stratify patients into interpretable risk groups typically sacrifice predictive power. We propose CONVERSE (CONtrastive Variational Ensemble for Risk Stratification and Estimation), a deep survival model that bridges this gap by unifying variational autoencoders with contrastive learning for interpretable risk stratification. CONVERSE combines variational embeddings with multiple intra- and inter-cluster contrastive losses. Self-paced learning progressively incorporates samples from easy to hard, improving training stability. The model supports cluster-specific survival heads, enabling accurate ensemble predictions. Comprehensive evaluation on four benchmark datasets demonstrates that CONVERSE achieves competitive or superior performance compared to existing deep survival methods, while maintaining meaningful patient stratification.

</details>


### [1251] [Rod Flow: A Continuous-Time Model for Gradient Descent at the Edge of Stability](https://arxiv.org/abs/2602.01480)
*Eric Regis,Sinho Chewi*

Main category: cs.LG

Relevance: 25.0

TL;DR: 本文提出Rod Flow，一种基于物理"杆"模型的ODE近似方法，用于理解梯度下降在非凸景观中的动力学，相比之前的Central Flow方法具有原理推导更严谨、计算更便宜等优势。


<details>
  <summary>Details</summary>
Motivation: 理解梯度下降在非凸景观中的训练动态是一个重要问题。Cohen等人(2021)发现的"稳定性边缘"现象表明，大学习率下的梯度下降会偏离梯度流。虽然Cohen等人(2025)提出的Central Flow提供了有效的ODE近似，但需要更原理性的推导和更高效的计算方法。

Method: 提出Rod Flow方法，将梯度下降迭代视为一维扩展物体（"杆"），基于物理原理推导ODE近似。该方法显式且计算便宜，能够准确预测临界锐度阈值并解释四次势中的自稳定现象。

Result: Rod Flow在简单玩具示例中更好地捕捉GD动态，在代表性神经网络架构中与Central Flow精度相当。理论证明了Rod Flow能正确预测临界锐度阈值并解释自稳定机制。通过数值实验验证了理论。

Conclusion: Rod Flow提供了一个原理性强、计算高效的ODE框架，用于理解梯度下降在非凸景观中的动力学，特别是在稳定性边缘区域，为优化理论提供了新的视角。

Abstract: How can we understand gradient-based training over non-convex landscapes? The edge of stability phenomenon, introduced in Cohen et al. (2021), indicates that the answer is not so simple: namely, gradient descent (GD) with large step sizes often diverges away from the gradient flow. In this regime, the "Central Flow", recently proposed in Cohen et al. (2025), provides an accurate ODE approximation to the GD dynamics over many architectures. In this work, we propose Rod Flow, an alternative ODE approximation, which carries the following advantages: (1) it rests on a principled derivation stemming from a physical picture of GD iterates as an extended one-dimensional object -- a "rod"; (2) it better captures GD dynamics for simple toy examples and matches the accuracy of Central Flow for representative neural network architectures, and (3) is explicit and cheap to compute. Theoretically, we prove that Rod Flow correctly predicts the critical sharpness threshold and explains self-stabilization in quartic potentials. We validate our theory with a range of numerical experiments.

</details>


### [1252] [COMET: Codebook-based Online-adaptive Multi-scale Embedding for Time-series Anomaly Detection](https://arxiv.org/abs/2602.01635)
*Jinwoo Park,Hyeongwon Kang,Seung Hun Han,Pilsung Kang*

Main category: cs.LG

Relevance: 25.0

TL;DR: COMET提出了一种基于码本的在线自适应多尺度嵌入方法用于时间序列异常检测，通过多尺度补丁编码、向量量化核心集和在线码本适应三个组件，在多个基准数据集上取得了最佳性能。


<details>
  <summary>Details</summary>
Motivation: 当前时间序列异常检测存在三个主要问题：1) 在补丁级表示学习中捕捉时间依赖性和多变量相关性不足；2) 依赖单尺度模式限制了不同时间范围异常的检测；3) 专注于正常数据表示使模型在推理时容易受到分布偏移的影响。

Method: COMET包含三个关键组件：1) 多尺度补丁编码：捕捉多个补丁尺度的时间依赖性和变量间相关性；2) 向量量化核心集：通过码本学习代表性正常模式，使用量化误差和内存距离的双重分数检测异常；3) 在线码本适应：基于码本条目生成伪标签，通过对比学习在推理时动态适应模型。

Result: 在五个基准数据集上的实验表明，COMET在45个评估指标中的36个上取得了最佳性能，验证了其在多样化环境中的有效性。

Conclusion: COMET通过结合多尺度表示学习、向量量化和在线适应机制，有效解决了时间序列异常检测中的关键挑战，在多个数据集上表现出优越性能。

Abstract: Time series anomaly detection is a critical task across various industrial domains. However, capturing temporal dependencies and multivariate correlations within patch-level representation learning remains underexplored, and reliance on single-scale patterns limits the detection of anomalies across different temporal ranges. Furthermore, focusing on normal data representations makes models vulnerable to distribution shifts at inference time. To address these limitations, we propose Codebook-based Online-adaptive Multi-scale Embedding for Time-series anomaly detection (COMET), which consists of three key components: (1) Multi-scale Patch Encoding captures temporal dependencies and inter-variable correlations across multiple patch scales. (2) Vector-Quantized Coreset learns representative normal patterns via codebook and detects anomalies with a dual-score combining quantization error and memory distance. (3) Online Codebook Adaptation generates pseudo-labels based on codebook entries and dynamically adapts the model at inference through contrastive learning. Experiments on five benchmark datasets demonstrate that COMET achieves the best performance in 36 out of 45 evaluation metrics, validating its effectiveness across diverse environments.

</details>


### [1253] [Finite and Corruption-Robust Regret Bounds in Online Inverse Linear Optimization under M-Convex Action Sets](https://arxiv.org/abs/2602.01682)
*Taihei Oki,Shinsaku Sakaue*

Main category: cs.LG

Relevance: 25.0

TL;DR: 该论文研究了在线逆线性优化问题，证明了在M-凸可行集上可以实现O(d log d)的有限遗憾界，解决了该领域的一个开放性问题。


<details>
  <summary>Details</summary>
Motivation: 在线逆线性优化（也称为上下文推荐）中，学习者需要从随时间变化的可行集中观察最优动作来推断代理的隐藏目标向量。先前研究已建立了O(d log T)的遗憾界和指数级有限界exp(O(d log d))，而Ω(d)的下界已知。是否存在多项式有限遗憾界一直是一个开放性问题。

Method: 结合M-凸集上最优解的结构特征和几何体积论证方法。对于对抗性损坏反馈，通过监测观察反馈诱导的有向图来自适应检测损坏，无需事先知道损坏轮数C。

Result: 当可行集是M-凸集时（包括拟阵），实现了O(d log d)的有限遗憾界。对于最多C轮对抗性损坏反馈，获得了O((C+1)d log d)的遗憾界，且无需事先知道C。

Conclusion: 该研究部分解决了在线逆线性优化中是否存在多项式有限遗憾界的开放性问题，证明了在M-凸集上可以实现O(d log d)的有限遗憾界，并扩展到对抗性损坏反馈场景。

Abstract: We study online inverse linear optimization, also known as contextual recommendation, where a learner sequentially infers an agent's hidden objective vector from observed optimal actions over feasible sets that change over time. The learner aims to recommend actions that perform well under the agent's true objective, and the performance is measured by the regret, defined as the cumulative gap between the agent's optimal values and those achieved by the learner's recommended actions. Prior work has established a regret bound of $O(d\log T)$, as well as a finite but exponentially large bound of $\exp(O(d\log d))$, where $d$ is the dimension of the optimization problem and $T$ is the time horizon, while a regret lower bound of $Ω(d)$ is known (Gollapudi et al. 2021; Sakaue et al. 2025). Whether a finite regret bound polynomial in $d$ is achievable or not has remained an open question. We partially resolve this by showing that when the feasible sets are M-convex -- a broad class that includes matroids -- a finite regret bound of $O(d\log d)$ is possible. We achieve this by combining a structural characterization of optimal solutions on M-convex sets with a geometric volume argument. Moreover, we extend our approach to adversarially corrupted feedback in up to $C$ rounds. We obtain a regret bound of $O((C+1)d\log d)$ without prior knowledge of $C$, by monitoring directed graphs induced by the observed feedback to detect corruptions adaptively.

</details>


### [1254] [Autocorrelated Optimize-via-Estimate: Predict-then-Optimize versus Finite-sample Optimal](https://arxiv.org/abs/2602.01877)
*Zichun Wang,Gar Goei Loke,Ruiting Zuo*

Main category: cs.LG

Relevance: 25.0

TL;DR: 论文提出了一种针对自相关不确定性的优化-估计模型（A-OVE），在有限样本条件下直接优化样本外性能，应用于带交易成本的组合优化问题，相比传统机器学习方法表现更好。


<details>
  <summary>Details</summary>
Motivation: 传统的数据驱动优化方法采用"先估计后优化"的两阶段方法，但在有限样本条件下可能不是最优的。特别是在自相关不确定性（如VARMA过程）的背景下，需要开发能够直接优化样本外性能的方法。

Method: 提出自相关优化-估计模型（A-OVE），将样本外最优解表示为充分统计量的函数，并提出了计算充分统计量的递归形式。该方法在自相关不确定性（VARMA过程）下直接优化决策质量。

Result: A-OVE在带交易成本的组合优化问题中实现了相对于完美信息oracle的低遗憾值，优于传统的预测-优化机器学习基准。值得注意的是，准确性更高的机器学习模型可能具有更差的决策质量。

Conclusion: 在自相关不确定性下，直接优化样本外性能的A-OVE方法优于传统两阶段方法，即使在模型轻微误设的情况下也能保持良好性能，验证了数据驱动优化中决策质量与预测准确性可能脱节的现象。

Abstract: Models that directly optimize for out-of-sample performance in the finite-sample regime have emerged as a promising alternative to traditional estimate-then-optimize approaches in data-driven optimization. In this work, we compare their performance in the context of autocorrelated uncertainties, specifically, under a Vector Autoregressive Moving Average VARMA(p,q) process. We propose an autocorrelated Optimize-via-Estimate (A-OVE) model that obtains an out-of-sample optimal solution as a function of sufficient statistics, and propose a recursive form for computing its sufficient statistics. We evaluate these models on a portfolio optimization problem with trading costs. A-OVE achieves low regret relative to a perfect information oracle, outperforming predict-then-optimize machine learning benchmarks. Notably, machine learning models with higher accuracy can have poorer decision quality, echoing the growing literature in data-driven optimization. Performance is retained under small mis-specification.

</details>


### [1255] [Generalized Optimal Classification Trees: A Mixed-Integer Programming Approach](https://arxiv.org/abs/2602.02173)
*Jiancheng Tu,Wenqi Fan,Zhibin Wu*

Main category: cs.LG

Relevance: 25.0

TL;DR: 本文提出了一种基于混合整数规划（MIP）的框架，用于在非线性性能指标（如F1分数）下学习最优分类树，特别针对类别不平衡问题，并开发了加速技术以提高可扩展性。


<details>
  <summary>Details</summary>
Motivation: 决策树的全局优化是组合优化中长期存在的挑战，但这类模型在可解释机器学习中扮演重要角色。尽管问题已被研究数十年，但直到最近离散优化的进展才使得在实际数据集上解决最优分类树问题成为可能。混合整数规划提供了高度的建模灵活性，因此作者提出MIP框架来学习在非线性性能指标下的最优分类树，特别是针对类别不平衡问题。

Method: 提出基于混合整数规划（MIP）的框架，用于在非线性性能指标（如F1分数）下学习最优分类树。开发了问题特定的加速技术，包括定制的分支切割算法、实例缩减方案和热启动策略，以提高可扩展性。

Result: 在50个基准数据集上评估了所提出的方法。计算结果表明，该框架能够高效优化非线性指标，同时与现有方法相比，实现了强大的预测性能和减少的求解时间。

Conclusion: 该MIP框架为在非线性性能指标下学习最优分类树提供了有效的解决方案，特别适用于处理类别不平衡问题，并通过加速技术提高了实际应用的可扩展性。

Abstract: Global optimization of decision trees is a long-standing challenge in combinatorial optimization, yet such models play an important role in interpretable machine learning. Although the problem has been investigated for several decades, only recent advances in discrete optimization have enabled practical algorithms for solving optimal classification tree problems on real-world datasets. Mixed-integer programming (MIP) offers a high degree of modeling flexibility, and we therefore propose a MIP-based framework for learning optimal classification trees under nonlinear performance metrics, such as the F1-score, that explicitly addresses class imbalance. To improve scalability, we develop problem-specific acceleration techniques, including a tailored branch-and-cut algorithm, an instance-reduction scheme, and warm-start strategies. We evaluate the proposed approach on 50 benchmark datasets. The computational results show that the framework can efficiently optimize nonlinear metrics while achieving strong predictive performance and reduced solution times compared with existing methods.

</details>


### [1256] [Geometry- and Relation-Aware Diffusion for EEG Super-Resolution](https://arxiv.org/abs/2602.02238)
*Laura Yao,Gengwei Zhang,Moajjem Chowdhury,Yunmei Liu,Tianlong Chen*

Main category: cs.LG

Relevance: 25.0

TL;DR: TopoDiff：一种用于EEG空间超分辨率的几何和关系感知扩散模型，通过结合拓扑感知图像嵌入和动态通道关系图来提升EEG信号的空间生成性能。


<details>
  <summary>Details</summary>
Motivation: 现有EEG空间超分辨率方法缺乏对生理空间结构的认知，限制了空间生成性能。人类专家在解释EEG空间模式时会考虑拓扑结构，因此需要开发能够整合几何上下文和电极间关系的模型。

Method: TopoDiff结合了从EEG地形表示中提取的拓扑感知图像嵌入（提供全局几何上下文）和动态通道关系图（编码电极间关系并随时间演化）。这种设计形成了空间基础扎实的EEG超分辨率框架。

Result: 在多个EEG数据集（SEED/SEED-IV情感识别、PhysioNet运动想象、TUSZ癫痫检测）上，该方法在生成保真度方面取得显著提升，并带来下游EEG任务性能的明显改进。

Conclusion: TopoDiff通过整合几何和关系感知，为EEG空间超分辨率提供了有效的解决方案，在生成质量和下游任务性能方面均优于现有方法。

Abstract: Recent electroencephalography (EEG) spatial super-resolution (SR) methods, while showing improved quality by either directly predicting missing signals from visible channels or adapting latent diffusion-based generative modeling to temporal data, often lack awareness of physiological spatial structure, thereby constraining spatial generation performance. To address this issue, we introduce TopoDiff, a geometry- and relation-aware diffusion model for EEG spatial super-resolution. Inspired by how human experts interpret spatial EEG patterns, TopoDiff incorporates topology-aware image embeddings derived from EEG topographic representations to provide global geometric context for spatial generation, together with a dynamic channel-relation graph that encodes inter-electrode relationships and evolves with temporal dynamics. This design yields a spatially grounded EEG spatial super-resolution framework with consistent performance improvements. Across multiple EEG datasets spanning diverse applications, including SEED/SEED-IV for emotion recognition, PhysioNet motor imagery (MI/MM), and TUSZ for seizure detection, our method achieves substantial gains in generation fidelity and leads to notable improvements in downstream EEG task performance.

</details>


### [1257] [Finite-Sample Wasserstein Error Bounds and Concentration Inequalities for Nonlinear Stochastic Approximation](https://arxiv.org/abs/2602.02445)
*Seo Taek Kong,R. Srikant*

Main category: cs.LG

Relevance: 25.0

TL;DR: 本文为非线性和线性随机逼近算法在Wasserstein-p距离下推导了非渐近误差界，通过耦合方法分析最后迭代的有限样本保证，并处理Polyak-Ruppert平均的收敛速率。


<details>
  <summary>Details</summary>
Motivation: 随机逼近算法（如随机梯度下降）在机器学习中广泛应用，但现有理论多为渐近分析或基于矩界的高概率保证。需要建立非渐近的分布收敛理论，特别是最后迭代的有限样本保证，以弥合有限样本分析与渐近理论之间的差距。

Method: 1. 开发耦合论证方法，将离散时间过程与极限Ornstein-Uhlenbeck过程比较；2. 在一般噪声条件下分析算法（包括鞅差和遍历马尔可夫链函数）；3. 直接分析Polyak-Ruppert平均的收敛速率；4. 假设驱动噪声满足非渐近中心极限定理。

Result: 1. 归一化最后迭代在p-Wasserstein距离中以γ_n^{1/6}速率收敛到高斯分布（γ_n为步长）；2. Polyak-Ruppert平均在Wasserstein距离中以n^{-1/6}速率收敛；3. 分布保证导出比矩界和马尔可夫不等式更好的高概率集中不等式；4. 在线性随机逼近中量化从重尾到高斯行为的转变；5. 在随机梯度下降中建立到中心极限定理的收敛速率。

Conclusion: 本文为非线性和线性随机逼近算法提供了严格的非渐近分布收敛理论，通过Wasserstein距离分析建立了有限样本保证，弥合了有限样本分析与渐近理论之间的差距，为随机优化算法的理论分析提供了新工具。

Abstract: This paper derives non-asymptotic error bounds for nonlinear stochastic approximation algorithms in the Wasserstein-$p$ distance. To obtain explicit finite-sample guarantees for the last iterate, we develop a coupling argument that compares the discrete-time process to a limiting Ornstein-Uhlenbeck process. Our analysis applies to algorithms driven by general noise conditions, including martingale differences and functions of ergodic Markov chains. Complementing this result, we handle the convergence rate of the Polyak-Ruppert average through a direct analysis that applies under the same general setting.
  Assuming the driving noise satisfies a non-asymptotic central limit theorem, we show that the normalized last iterates converge to a Gaussian distribution in the $p$-Wasserstein distance at a rate of order $γ_n^{1/6}$, where $γ_n$ is the step size. Similarly, the Polyak-Ruppert average is shown to converge in the Wasserstein distance at a rate of order $n^{-1/6}$. These distributional guarantees imply high-probability concentration inequalities that improve upon those derived from moment bounds and Markov's inequality. We demonstrate the utility of this approach by considering two applications: (1) linear stochastic approximation, where we explicitly quantify the transition from heavy-tailed to Gaussian behavior of the iterates, thereby bridging the gap between recent finite-sample analyses and asymptotic theory and (2) stochastic gradient descent, where we establish rate of convergence to the central limit theorem.

</details>


### [1258] [Comparison of Multiple Classifiers for Android Malware Detection with Emphasis on Feature Insights Using CICMalDroid 2020 Dataset](https://arxiv.org/abs/2602.00058)
*Md Min-Ha-Zul Abedin,Tazqia Mehrub*

Main category: cs.CR

Relevance: 25.0

TL;DR: 该论文使用CICMalDroid2020数据集，通过提取564维混合特征（301个静态+263个动态特征），评估了7种分类器在三种特征处理方案下的Android恶意软件检测性能。XGBoost在原始特征上表现最佳，准确率达0.9747，而PCA会降低所有模型性能，LDA能保持中高精度并显示可分离聚类。


<details>
  <summary>Details</summary>
Motivation: Android恶意软件检测对大规模用户保护至关重要，但传统签名扫描器无法跟上应用商店的快速发布周期。研究旨在通过结合全面数据集和严谨透明的评估来构建可信赖的检测器，并识别可解释的决策驱动因素。

Method: 使用CICMalDroid2020数据集（17,341个应用，涵盖良性、广告软件、银行恶意软件、短信恶意软件和风险软件）。提取301个静态特征和263个动态特征，形成564维混合特征向量。在三种特征处理方案下评估7种分类器：原始特征、主成分分析（PCA）和线性判别分析（LDA），采用70%训练/30%测试划分。

Result: XGBoost在原始特征上表现最佳：准确率0.9747，精确率0.9703，召回率0.9731，F1分数0.9716。HistGradientBoosting达到0.9741准确率和0.9708 F1。CatBoost和Random Forest稍低（准确率0.9678/0.9687，F1 0.9636/0.9637）。KNN和SVM表现较差。PCA降低所有模型性能（XGBoost降至0.9164准确率）。LDA保持中高精度并显示可分离聚类。深度为2的代理树识别出包名、主活动和目标SDK为关键决策驱动因素。

Conclusion: 研究为Android恶意软件检测建立了高保真度的监督基线，表明丰富的混合特征结合梯度提升方法为实际部署提供了实用且可解释的基础。特征降维（PCA）会损害性能，而LDA在保持性能的同时提供了更好的可解释性。

Abstract: Accurate Android malware detection was critical for protecting users at scale. Signature scanners lagged behind fast release cycles on public app stores. We aimed to build a trustworthy detector by pairing a comprehensive dataset with a rigorous, transparent evaluation, and to identify interpretable drivers of decisions. We used CICMalDroid2020, which contained 17,341 apps across Benign, Adware, Banking, SMS malware, and Riskware. We extracted 301 static and 263 dynamic features into a 564 dimensional hybrid vector, then evaluated seven classifiers under three schemes, original features, principal component analysis, PCA, and linear discriminant analysis, LDA, with a 70 percent training and 30 percent test split. Results showed that gradient boosting on the original features performed best. XGBoost achieved 0.9747 accuracy, 0.9703 precision, 0.9731 recall, and 0.9716 F1, and the confusion matrix indicated rare benign labels for malicious apps. HistGradientBoosting reached 0.9741 accuracy and 0.9708 F1, while CatBoost and Random Forest were slightly lower at 0.9678 and 0.9687 accuracy with 0.9636 and 0.9637 F1. KNN and SVM lagged. PCA reduced performance for all models, with XGBoost dropping to 0.9164 accuracy and 0.8988 F1. LDA maintained mid 90s accuracy and clarified separable clusters in projections. A depth two surrogate tree highlighted package name, main activity, and target SDK as key drivers. These findings established high fidelity supervised baselines for Android malware detection and indicated that rich hybrid features with gradient boosting offered a practical and interpretable foundation for deployment.

</details>


### [1259] [Parametrization of subgrid scales in long-term simulations of the shallow-water equations using machine learning and convex limiting](https://arxiv.org/abs/2602.00378)
*Md Amran Hossan Mojamder,Zhihang Xu,Min Wang,Ilya Timofeyev*

Main category: physics.flu-dyn

Relevance: 25.0

TL;DR: 提出一种参数化浅水方程中亚网格过程的方法，使用前馈神经网络学习亚网格通量，得到基于四点计算模板的局部参数化方案


<details>
  <summary>Details</summary>
Motivation: 解决浅水方程中亚网格过程的参数化问题，传统全局耦合参数化方法存在计算效率低、稳定性差等问题，需要开发更高效可靠的局部参数化方案

Method: 定义粗粒度变量和局部空间平均，使用前馈神经网络学习亚网格通量，构建基于四点计算模板的局部参数化方案，可结合通量限制器减少激波附近的振荡

Result: 数值实验表明该方法能改善长期湍流模拟中的能量平衡，准确再现个体解，即使在训练数据未包含的动态机制下也能提供可靠的参数化

Conclusion: 提出的神经网络参数化方法在浅水方程中表现出色，具有局部性、计算高效性和泛化能力强的优点，为亚网格过程参数化提供了新思路

Abstract: We present a method for parametrizing sub-grid processes in the Shallow Water equations. We define coarse variables and local spatial averages and use a feed-forward neural network to learn sub-grid fluxes. Our method results in a local parametrization that uses a four-point computational stencil, which has several advantages over globally coupled parametrizations. We demonstrate numerically that our method improves energy balance in long-term turbulent simulations and also accurately reproduces individual solutions. The neural network parametrization can be easily combined with flux limiting to reduce oscillations near shocks. More importantly, our method provides reliable parametrizations, even in dynamical regimes that are not included in the training data.

</details>


### [1260] [Topological Residual Asymmetry for Bivariate Causal Direction](https://arxiv.org/abs/2602.00427)
*Mouad El Bouchattaoui*

Main category: stat.ML

Relevance: 25.0

TL;DR: TRA是一种基于拓扑几何的因果方向推断方法，通过比较回归残差云的形状差异来识别因果方向，在低噪声条件下特别有效。


<details>
  <summary>Details</summary>
Motivation: 现有因果方向推断方法在模糊或近不可识别的情况下容易错误地确定方向，需要更稳健的几何基础方法来解决这一问题。

Method: 提出拓扑残差不对称性(TRA)：1) 使用秩基copula标准化进行交叉拟合回归；2) 比较两个方向上的回归残差云形状（正确方向形成二维体，反向方向形成一维管）；3) 使用0维持久同调功能量化这种差异；4) 提出TRA-s（分箱变体）处理固定噪声；5) 提出TRA-C（混淆感知弃权规则）。

Result: 在大量具有挑战性的合成和真实数据场景中，该方法表现出优越性能，特别是在低噪声条件下能够可靠地推断因果方向。

Conclusion: TRA提供了一种基于几何的稳健因果方向推断方法，通过拓扑分析残差云形状差异，在模糊情况下也能有效工作，并提供了处理噪声和混淆的变体。

Abstract: Inferring causal direction from purely observational bivariate data is fragile: many methods commit to a direction even in ambiguous or near non-identifiable regimes. We propose Topological Residual Asymmetry (TRA), a geometry-based criterion for additive-noise models. TRA compares the shapes of two cross-fitted regressor-residual clouds after rank-based copula standardization: in the correct direction, residuals are approximately independent, producing a two-dimensional bulk, while in the reverse direction -- especially under low noise -- the cloud concentrates near a one-dimensional tube. We quantify this bulk-tube contrast using a 0D persistent-homology functional, computed efficiently from Euclidean MST edge-length profiles. We prove consistency in a triangular-array small-noise regime, extend the method to fixed noise via a binned variant (TRA-s), and introduce TRA-C, a confounding-aware abstention rule calibrated by a Gaussian-copula plug-in bootstrap. Extensive experiments across many challenging synthetic and real-data scenarios demonstrate the method's superiority.

</details>


### [1261] [Learning in Bayesian Stackelberg Games With Unknown Follower's Types](https://arxiv.org/abs/2602.00771)
*Matteo Bollini,Francesco Bacchiocchi,Samuel Coutts,Matteo Castiglioni,Alberto Marchesi*

Main category: cs.GT

Relevance: 25.0

TL;DR: 该论文研究贝叶斯Stackelberg博弈中的在线学习问题，领导者与具有未知私有类型且类型分布未知的追随者重复交互，目标是设计最小化领导者遗憾的算法。


<details>
  <summary>Details</summary>
Motivation: 研究贝叶斯Stackelberg博弈中领导者面临的最具挑战性的情况：既不知道追随者的类型分布，也不知道追随者可能的收益函数。这比文献中通常研究的已知追随者收益的情况更具现实意义且更具挑战性。

Method: 首先证明了在仅观察追随者最优响应（动作反馈）的情况下无法实现无遗憾学习。因此转向类型反馈模型（同时观察追随者类型），提出了一种实现$\widetilde{O}(\sqrt{T})$遗憾的无遗憾算法。

Result: 证明了在动作反馈下无遗憾学习不可行，在类型反馈下设计了$\widetilde{O}(\sqrt{T})$遗憾的算法，这是首次在完全未知追随者收益的情况下解决此类问题。

Conclusion: 在贝叶斯Stackelberg博弈中，当领导者对追随者类型和收益都未知时，仅凭动作反馈无法实现无遗憾学习，但通过类型反馈可以获得次线性遗憾的算法。

Abstract: We study online learning in Bayesian Stackelberg games, where a leader repeatedly interacts with a follower whose unknown private type is independently drawn at each round from an unknown probability distribution. The goal is to design algorithms that minimize the leader's regret with respect to always playing an optimal commitment computed with knowledge of the game. We consider, for the first time to the best of our knowledge, the most realistic case in which the leader does not know anything about the follower's types, i.e., the possible follower payoffs. This raises considerable additional challenges compared to the commonly studied case in which the payoffs of follower types are known. First, we prove a strong negative result: no-regret is unattainable under action feedback, i.e., when the leader only observes the follower's best response at the end of each round. Thus, we focus on the easier type feedback model, where the follower's type is also revealed. In such a setting, we propose a no-regret algorithm that achieves a regret of $\widetilde{O}(\sqrt{T})$, when ignoring the dependence on other parameters.

</details>


### [1262] [Robust Machine Learning Framework for Reliable Discovery of High-Performance Half-Heusler Thermoelectrics](https://arxiv.org/abs/2602.01149)
*Shoeb Athar,Adrien Mecibah,Philippe Jund*

Main category: cond-mat.mtrl-sci

Relevance: 25.0

TL;DR: 该研究提出了一种改进机器学习模型在热电材料发现中实验泛化能力的稳健工作流程，通过PCA分割方法、贝叶斯超参数优化和特征筛选，识别关键材料参数，并筛选出新型高性能热电材料候选物。


<details>
  <summary>Details</summary>
Motivation: 机器学习在热电材料发现中具有潜力，但现有模型虽然指标表现好，实验泛化能力却很差。传统方法过于关注测试集的RMSE/R²值，而忽略了测试集作为模型泛化能力真实代理的重要性，以及现有ML工作流程中常被忽视的模块。

Method: 1) 引入基于PCA的严格分割方法，确保训练集和测试集无偏且能代表完整化学空间；2) 结合贝叶斯超参数优化和k-best特征筛选，应用于随机森林、XGBoost和神经网络三种架构；3) 使用SISSO符号回归进行物理洞察和比较；4) 通过SHAP和SISSO分析识别关键特征；5) 高通量筛选约6.6×10^8个潜在组合，并应用稳定性约束。

Result: 1) 识别出A位掺杂浓度(xA')和A位汽化热(HVA)是除温度(T)外影响热电优值(zT)的主要驱动因素；2) 筛选出多个新型高zT候选材料；3) 建立了测试集作为模型泛化能力真实代理的方法，强化了现有ML工作流程中常被忽视的模块。

Conclusion: 该研究打破了传统上仅关注改进模型测试RMSE/R²值的做法，将注意力转向建立测试集作为模型泛化能力的真实代理，并强化了现有机器学习工作流程中常被忽视的模块，为下一代热电材料的数据驱动设计提供了更可靠的方法。

Abstract: Machine learning (ML) can facilitate efficient thermoelectric (TE) material discovery essential to address the environmental crisis. However, ML models often suffer from poor experimental generalizability despite high metrics. This study presents a robust workflow, applied to the half-Heusler (hH) structural prototype, for figure of merit (zT) prediction, to improve the generalizability of ML models. To resolve challenges in dataset handling and feature filtering, we first introduce a rigorous PCA-based splitting method that ensures training and test sets are unbiased and representative of the full chemical space. We then integrate Bayesian hyperparameter optimization with k-best feature filtering across three architectures-Random Forest, XGBoost, and Neural Networks - while employing SISSO symbolic regression for physical insight and comparison. Using SHAP and SISSO analysis, we identify A-site dopant concentration (xA'), and A-site Heat of Vaporization (HVA) as the primary drivers of zT besides Temperature (T). Finally, a high-throughput screening of approximately 6.6x10^8 potential compositions, filtered by stability constraints, yielded several novel high-zT candidates. Breaking from the traditional focus of improving test RMSE/R^2 values of the models, this work shifts the attention on establishing the test set a true proxy for model generalizability and strengthening the often neglected modules of the existing ML workflows for the data-driven design of next-generation thermoelectric materials.

</details>


### [1263] [Equivalence of Privacy and Stability with Generalization Guarantees in Quantum Learning](https://arxiv.org/abs/2602.01177)
*Ayanava Dasgupta,Naqueeb Ahmad Warsi,Masahito Hayashi*

Main category: quant-ph

Relevance: 25.0

TL;DR: 该论文提出了一个统一的信息论框架，用于分析差分隐私量子学习算法的泛化性能，建立了隐私、算法稳定性和泛化误差之间的理论联系。


<details>
  <summary>Details</summary>
Motivation: 随着量子机器学习的发展，需要理解量子学习算法在隐私保护下的泛化性能。现有研究缺乏对差分隐私量子学习算法泛化性能的系统理论分析，特别是隐私、算法稳定性和泛化误差之间的理论联系。

Method: 1. 建立统一的信息论框架，将隐私与算法稳定性联系起来
2. 证明(ε,δ)-量子差分隐私对训练数据和算法输出之间的互信息施加了强约束
3. 为满足1-邻居隐私约束的学习算法推导机制无关的互信息上界
4. 将稳定性保证与泛化联系起来，证明泛化误差受隐私诱导稳定性项的平方根限制
5. 扩展到不可信数据处理器场景，引入信息论可容许性概念

Result: 1. 建立了(ε,δ)-QDP与算法稳定性之间的严格理论联系
2. 推导了隐私诱导的互信息上界，为量子学习算法的泛化性能提供了理论保证
3. 证明了QDP学习算法的期望泛化误差受隐私稳定性项的平方根限制
4. 为不可信数据处理器场景提供了信息论可容许性分析框架

Conclusion: 该研究为差分隐私量子学习算法提供了系统的泛化性能分析框架，建立了隐私、稳定性和泛化之间的理论桥梁，对量子机器学习的安全性和可靠性有重要意义。

Abstract: We present a unified information-theoretic framework to analyze the generalization performance of differentially private (DP) quantum learning algorithms. By leveraging the connection between privacy and algorithmic stability, we establish that $(\varepsilon, δ)$-Quantum Differential Privacy (QDP) imposes a strong constraint on the mutual information between the training data and the algorithm's output. We derive a rigorous, mechanism-agnostic upper bound on this mutual information for learning algorithms satisfying a 1-neighbor privacy constraint. Furthermore, we connect this stability guarantee to generalization, proving that the expected generalization error of any $(\varepsilon, δ)$-QDP learning algorithm is bounded by the square root of the privacy-induced stability term. Finally, we extend our framework to the setting of an untrusted Data Processor, introducing the concept of Information-Theoretic Admissibility (ITA) to characterize the fundamental limits of privacy in scenarios where the learning map itself must remain oblivious to the specific dataset instance.

</details>


### [1264] [WAKESET: A Large-Scale, High-Reynolds Number Flow Dataset for Machine Learning of Turbulent Wake Dynamics](https://arxiv.org/abs/2602.01379)
*Zachary Cooper-Baldock,Paulo E. Santos,Russell S. A. Brinkworth,Karl Sammut*

Main category: physics.flu-dyn

Relevance: 25.0

TL;DR: WAKESET是一个用于机器学习的大规模高湍流CFD数据集，包含1,091个高保真RANS模拟，扩展到4,364个实例，覆盖高雷诺数（达1.09×10^8）和转弯角度的复杂水下航行器回收场景。


<details>
  <summary>Details</summary>
Motivation: 机器学习在计算流体动力学中具有变革潜力，但缺乏适合训练鲁棒模型的大型、多样化、高保真数据集，特别是对于主导实际工程应用的高湍流流动。现有数据集通常局限于简化的低雷诺数工况，无法捕捉真实世界流动的复杂多尺度物理特性。

Method: 创建了WAKESET数据集，通过1,091个高保真雷诺平均Navier-Stokes（RANS）模拟，捕捉大型无人水下航行器回收自主水下航行器时的复杂水动力相互作用。数据集扩展到4,364个实例，覆盖宽范围的操作包线（速度高达雷诺数1.09×10^8和不同转弯角度）。

Result: WAKESET是一个专注于实际工程问题、规模大且具有高湍流特性的数据集，为开发流场预测、代理建模和复杂水下环境自主导航的机器学习模型提供了宝贵资源。

Conclusion: WAKESET填补了机器学习在流体动力学应用中的关键数据缺口，特别针对高湍流流动，为开发能够泛化到真实世界复杂流动的机器学习模型提供了必要的基础设施。

Abstract: Machine learning (ML) offers transformative potential for computational fluid dynamics (CFD), promising to accelerate simulations, improve turbulence modelling, and enable real-time flow prediction and control-capabilities that could fundamentally change how engineers approach fluid dynamics problems. However, the exploration of ML in fluid dynamics is critically hampered by the scarcity of large, diverse, and high-fidelity datasets suitable for training robust models. This limitation is particularly acute for highly turbulent flows, which dominate practical engineering applications yet remain computationally prohibitive to simulate at scale. High-Reynolds number turbulent datasets are essential for ML models to learn the complex, multi-scale physics characteristic of real-world flows, enabling generalisation beyond the simplified, low-Reynolds number regimes often represented in existing datasets. This paper introduces WAKESET, a novel, large-scale CFD dataset of highly turbulent flows, designed to address this critical gap. The dataset captures the complex hydrodynamic interactions during the underwater recovery of an autonomous underwater vehicle by a larger extra-large uncrewed underwater vehicle. It comprises 1,091 high-fidelity Reynolds-Averaged Navier-Stokes simulations, augmented to 4,364 instances, covering a wide operational envelope of speeds (up to Reynolds numbers of 1.09 x 10^8) and turning angles. This work details the motivation for this new dataset by reviewing existing resources, outlines the hydrodynamic modelling and validation underpinning its creation, and describes its structure. The dataset's focus on a practical engineering problem, its scale, and its high turbulence characteristics make it a valuable resource for developing and benchmarking ML models for flow field prediction, surrogate modelling, and autonomous navigation in complex underwater environments.

</details>


### [1265] [Physics-Informed Chebyshev Polynomial Neural Operator for Parametric Partial Differential Equations](https://arxiv.org/abs/2602.01737)
*Biao Chen,Jing Wang,Hairun Xie,Qineng Wang,Shuai Zhang,Yifan Xia,Jifa Zhang*

Main category: physics.flu-dyn

Relevance: 25.0

TL;DR: 提出Physics-Informed Chebyshev Polynomial Neural Operator (CPNO)，一种基于切比雪夫谱基的神经算子框架，用于解决参数化偏微分方程，通过谱基变换克服传统MLP的谱偏差和激活函数限制，实现更稳定、准确的物理信息求解。


<details>
  <summary>Details</summary>
Motivation: 现有神经算子方法主要依赖多层感知机(MLP)将输入映射到解，在物理信息设置下存在训练鲁棒性问题，主要源于固有的谱偏差和固定激活函数。需要克服这些架构限制，提升参数化PDE求解的稳定性和准确性。

Method: 提出CPNO框架，用数值稳定的切比雪夫谱基替换不稳定的单项式展开，通过基变换将模型从MLP特定约束中解耦。集成参数依赖调制机制到主网络，在接近最优的函数空间中构造PDE解，增强多尺度表示能力。

Result: 理论分析显示切比雪夫基具有接近极小极大一致逼近性质和优越的条件数，Lebesgue常数随度数对数增长，减轻谱偏差并确保优化期间稳定的梯度流。在基准参数化PDE上的数值实验表明CPNO实现了更高的精度、更快的收敛和更强的超参数鲁棒性。跨音速翼型流动实验展示了CPNO处理复杂几何问题的能力。

Conclusion: CPNO通过切比雪夫谱基变换有效克服了传统神经算子的架构限制，为参数化PDE求解提供了更稳定、准确且鲁棒的框架，特别适用于复杂物理问题的多尺度表示。

Abstract: Neural operators have emerged as powerful deep learning frameworks for approximating solution operators of parameterized partial differential equations (PDE). However, current methods predominantly rely on multilayer perceptrons (MLPs) for mapping inputs to solutions, which impairs training robustness in physics-informed settings due to inherent spectral biases and fixed activation functions. To overcome the architectural limitations, we introduce the Physics-Informed Chebyshev Polynomial Neural Operator (CPNO), a novel mesh-free framework that leverages a basis transformation to replace unstable monomial expansions with the numerically stable Chebyshev spectral basis. By integrating parameter dependent modulation mechanism to main net, CPNO constructs PDE solutions in a near-optimal functional space, decoupling the model from MLP-specific constraints and enhancing multi-scale representation. Theoretical analysis demonstrates the Chebyshev basis's near-minimax uniform approximation properties and superior conditioning, with Lebesgue constants growing logarithmically with degree, thereby mitigating spectral bias and ensuring stable gradient flow during optimization. Numerical experiments on benchmark parameterized PDEs show that CPNO achieves superior accuracy, faster convergence, and enhanced robustness to hyperparameters. The experiment of transonic airfoil flow has demonstrated the capability of CPNO in characterizing complex geometric problems.

</details>


### [1266] [Cost-Aware Bayesian Optimization for Prototyping Interactive Devices](https://arxiv.org/abs/2602.01774)
*Thomas Langerak,Renate Zhang,Ziyuan Wang,Per Ola Kristensson,Antti Oulasvirta*

Main category: cs.HC

Relevance: 25.0

TL;DR: 本文提出了一种考虑多样化原型成本的贝叶斯优化扩展方法，通过设计者预估的成本指导采样，在保持相似效用的同时显著降低原型成本。


<details>
  <summary>Details</summary>
Motivation: 在迭代设计中，决定哪些创意值得制作原型是一个核心问题。原型制作成本差异巨大（从简单的参数调整到硬件制造），这种不对称性可能阻碍设计者探索设计空间。现有方法通常忽略成本多样性，导致资源分配效率低下。

Method: 扩展了成本感知的贝叶斯优化方法，通过最小化修改采集函数来考虑多样化的原型成本。核心思想是利用设计者预估的成本指导采样，选择更具成本效益的原型进行测试。

Result: 技术评估中，该方法在保持与成本无关基线相似效用的同时，仅需约70%的成本；在严格预算下，性能优于基线三倍。在12名参与者的真实摇杆设计任务中，也显示出类似优势。

Conclusion: 考虑原型成本可以使贝叶斯优化更适用于实际设计项目，提高资源利用效率，促进设计空间探索。

Abstract: Deciding which idea is worth prototyping is a central concern in iterative design. A prototype should be produced when the expected improvement is high and the cost is low. However, this is hard to decide, because costs can vary drastically: a simple parameter tweak may take seconds, while fabricating hardware consumes material and energy. Such asymmetries, can discourage a designer from exploring the design space. In this paper, we present an extension of cost-aware Bayesian optimization to account for diverse prototyping costs. The method builds on the power of Bayesian optimization and requires only a minimal modification to the acquisition function. The key idea is to use designer-estimated costs to guide sampling toward more cost-effective prototypes. In technical evaluations, the method achieved comparable utility to a cost-agnostic baseline while requiring only ${\approx}70\%$ of the cost; under strict budgets, it outperformed the baseline threefold. A within-subjects study with 12 participants in a realistic joystick design task demonstrated similar benefits. These results show that accounting for prototyping costs can make Bayesian optimization more compatible with real-world design projects.

</details>


### [1267] [Propagating the prior from far to near offset: A self-supervised diffusion framework for progressively recovering near-offsets of towed-streamer data](https://arxiv.org/abs/2602.01909)
*Shijun Cheng,Tariq Alkhalifah*

Main category: physics.geo-ph

Relevance: 25.0

TL;DR: 提出一种自监督扩散模型框架，用于重建海洋拖缆地震采集中的缺失近偏移距数据，无需近偏移距参考数据，通过从远偏移距数据学习统计模式进行递归外推。


<details>
  <summary>Details</summary>
Motivation: 海洋拖缆地震采集时，最近的水听器距离震源约200米，导致近偏移距数据缺失，影响表面相关多次波消除、速度分析和全波形反演等关键处理流程。现有方法存在运动学不一致和振幅失真问题，而监督深度学习方法需要完整的近偏移距地面真值数据，这在现实采集场景中不可得。

Method: 提出自监督扩散框架：1) 从可用的远偏移距剖面中提取重叠补丁（单道偏移）；2) 训练条件扩散模型学习偏移距相关的统计模式（事件曲率、振幅变化、子波特征）；3) 推理时从最近记录偏移距向零偏移距进行逐道递归外推；4) 通过集成采样提供不确定性估计。

Result: 在合成和现场数据集上的控制验证实验显示，相比传统抛物Radon变换基线有显著性能提升。在实际近偏移距间隙上的操作部署证明了实用性。重建波形保留了真实的振幅-偏移距趋势，不确定性图准确识别了具有挑战性的外推区域。

Conclusion: 该自监督扩散框架能够有效重建缺失的近偏移距地震数据，无需近偏移距参考数据，通过从远偏移距学习统计模式进行递归外推，并提供不确定性估计，在实际应用中具有可行性。

Abstract: In marine towed-streamer seismic acquisition, the nearest hydrophone is often two hundred meter away from the source resulting in missing near-offset traces, which degrades critical processing workflows such as surface-related multiple elimination, velocity analysis, and full-waveform inversion. Existing reconstruction methods, like transform-domain interpolation, often produce kinematic inconsistencies and amplitude distortions, while supervised deep learning approaches require complete ground-truth near-offset data that are unavailable in realistic acquisition scenarios. To address these limitations, we propose a self-supervised diffusion-based framework that reconstructs missing near-offset traces without requiring near-offset reference data. Our method leverages overlapping patch extraction with single-trace shifts from the available far-offset section to train a conditional diffusion model, which learns offset-dependent statistical patterns governing event curvature, amplitude variation, and wavelet characteristics. At inference, we perform trace-by-trace recursive extrapolation from the nearest recorded offset toward zero offset, progressively propagating learned prior information from far to near offsets. The generative formulation further provides uncertainty estimates via ensemble sampling, quantifying prediction confidence where validation data are absent. Controlled validation experiments on synthetic and field datasets show substantial performance gains over conventional parabolic Radon transform baselines. Operational deployment on actual near-offset gaps demonstrates practical viability where ground-truth validation is impossible. Notably, the reconstructed waveforms preserve realistic amplitude-versus-offset trends despite training exclusively on far-offset observations, and uncertainty maps accurately identify challenging extrapolation regions.

</details>


### [1268] [Twinning Complex Networked Systems: Data-Driven Calibration of the mABCD Synthetic Graph Generator](https://arxiv.org/abs/2602.02044)
*Piotr Bródka,Michał Czuba,Bogumił Kamiński,Łukasz Kraiński,Katarzyna Musial,Paweł Prałat,Mateusz Stolarski*

Main category: cs.SI

Relevance: 25.0

TL;DR: 本文提出了一种解决多层网络生成器逆问题的方法，通过从真实系统推断mABCD生成器的配置参数，使生成的合成网络能够作为原始结构的数字孪生。


<details>
  <summary>Details</summary>
Motivation: 多层网络分析常受限于大规模实证数据的缺乏，现有图生成器作为替代方案会引入系统性偏差。需要解决逆生成器问题，从真实系统推断生成器参数，创建能够准确反映原始结构的数字孪生网络。

Method: 提出从真实多层网络推断mABCD生成器配置参数的方法，包括参数估计和误差量化。研究发现参数间存在强相互依赖，因此采用联合预测而非独立估计的方法。

Result: 研究表明多层网络生成器参数推断任务具有挑战性，参数间的强相互依赖关系使得独立估计效果不佳，联合预测方法更为有效。

Conclusion: 成功解决了多层网络生成器的逆问题，能够从真实系统推断配置参数，为创建数字孪生网络提供了可行方法，强调了参数间相互依赖对估计方法选择的重要性。

Abstract: The increasing availability of relational data has contributed to a growing reliance on network-based representations of complex systems. Over time, these models have evolved to capture more nuanced properties, such as the heterogeneity of relationships, leading to the concept of multilayer networks. However, the analysis and evaluation of methods for these structures is often hindered by the limited availability of large-scale empirical data. As a result, graph generators are commonly used as a workaround, albeit at the cost of introducing systematic biases. In this paper, we address the inverse-generator problem by inferring the configuration parameters of a multilayer network generator, mABCD, from a real-world system. Our goal is to identify parameter settings that enable the generator to produce synthetic networks that act as digital twins of the original structure. We propose a method for estimating matching configurations and for quantifying the associated error. Our results demonstrate that this task is non-trivial, as strong interdependencies between configuration parameters weaken independent estimation and instead favour a joint-prediction approach.

</details>


### [1269] [Ultrafast On-chip Online Learning via Spline Locality in Kolmogorov-Arnold Networks](https://arxiv.org/abs/2602.02056)
*Duc Hoang,Aarush Gupta,Philip Harris*

Main category: cs.AR

Relevance: 25.0

TL;DR: KANs在FPGA上实现亚微秒级在线学习，比MLPs更高效、更具表达能力，适用于量子计算和核聚变等高频率系统控制


<details>
  <summary>Details</summary>
Motivation: 量子计算和核聚变等高频率系统需要亚微秒级的在线学习能力，传统MLPs在低延迟、固定精度计算和严格内存约束下效率低下且数值不稳定

Method: 利用KANs的B样条局部性实现稀疏更新，展示KANs对定点量化的固有鲁棒性，在FPGA上实现定点在线训练

Result: KAN-based在线学习器在低延迟和资源受限任务中比MLPs更高效、更具表达能力，首次实现亚微秒级无模型在线学习

Conclusion: KANs在严格资源约束下具有显著优势，为高频率系统的实时自适应控制提供了可行方案

Abstract: Ultrafast online learning is essential for high-frequency systems, such as controls for quantum computing and nuclear fusion, where adaptation must occur on sub-microsecond timescales. Meeting these requirements demands low-latency, fixed-precision computation under strict memory constraints, a regime in which conventional Multi-Layer Perceptrons (MLPs) are both inefficient and numerically unstable. We identify key properties of Kolmogorov-Arnold Networks (KANs) that align with these constraints. Specifically, we show that: (i) KAN updates exploiting B-spline locality are sparse, enabling superior on-chip resource scaling, and (ii) KANs are inherently robust to fixed-point quantization. By implementing fixed-point online training on Field-Programmable Gate Arrays (FPGAs), a representative platform for on-chip computation, we demonstrate that KAN-based online learners are significantly more efficient and expressive than MLPs across a range of low-latency and resource-constrained tasks. To our knowledge, this work is the first to demonstrate model-free online learning at sub-microsecond latencies.

</details>


### [1270] [Training-free score-based diffusion for parameter-dependent stochastic dynamical systems](https://arxiv.org/abs/2602.02113)
*Minglei Yang,Sicheng He*

Main category: stat.ML

Relevance: 25.0

TL;DR: 提出一种无需训练的、基于核加权蒙特卡洛估计的条件扩散模型框架，用于学习参数依赖随机微分方程（SDEs）的随机流映射，能够通过离散参数值采样的轨迹数据近似条件得分函数，实现状态空间和连续参数域的双重插值。


<details>
  <summary>Details</summary>
Motivation: 模拟参数依赖随机微分方程（SDEs）存在显著计算挑战，通常需要为每个参数值进行单独的高保真模拟。现有机器学习方法要么需要昂贵的神经网络训练来估计得分函数，要么缺乏处理连续参数依赖的能力。

Method: 提出训练免费的条件扩散模型框架，核心技术创新是联合核加权蒙特卡洛估计器，利用在离散参数值处采样的轨迹数据来近似条件得分函数，从而在状态空间和连续参数域内实现插值。

Result: 该方法在三个复杂度递增的数值示例中表现出色，能够准确近似不同参数值下的条件分布。训练后的生成模型可以在训练范围内的任何参数值处生成样本轨迹，无需重新训练。

Conclusion: 该框架显著加速了参数研究、不确定性量化和实时滤波应用，为参数依赖SDEs的高效模拟提供了有效的训练免费解决方案。

Abstract: Simulating parameter-dependent stochastic differential equations (SDEs) presents significant computational challenges, as separate high-fidelity simulations are typically required for each parameter value of interest. Despite the success of machine learning methods in learning SDE dynamics, existing approaches either require expensive neural network training for score function estimation or lack the ability to handle continuous parameter dependence. We present a training-free conditional diffusion model framework for learning stochastic flow maps of parameter-dependent SDEs, where both drift and diffusion coefficients depend on physical parameters. The key technical innovation is a joint kernel-weighted Monte Carlo estimator that approximates the conditional score function using trajectory data sampled at discrete parameter values, enabling interpolation across both state space and the continuous parameter domain. Once trained, the resulting generative model produces sample trajectories for any parameter value within the training range without retraining, significantly accelerating parameter studies, uncertainty quantification, and real-time filtering applications. The performance of the proposed approach is demonstrated via three numerical examples of increasing complexity, showing accurate approximation of conditional distributions across varying parameter values.

</details>


### [1271] [Age-Aware Edge-Blind Federated Learning via Over-the-Air Aggregation](https://arxiv.org/abs/2602.02469)
*Ahmed M. Elshazly,Ahmed Arafa*

Main category: cs.IT

Relevance: 25.0

TL;DR: 提出了一种无需设备端信道状态信息的无线联邦学习方案，使用多天线MRC检测，通过AgeTop-k选择参数子集以降低延迟


<details>
  <summary>Details</summary>
Motivation: 无线联邦学习中，多设备同时发送模型更新时面临信道状态信息获取困难、正交子载波有限导致延迟增加的问题

Method: 提出年龄感知的边缘盲空中联邦学习方法：PS使用多天线和MRC检测，采用AgeTop-k算法选择参数子集，优先选择幅度最大且等待时间最长的k个坐标

Result: 实验表明：(1)更多PS天线显著提高精度和收敛速度；(2)在较好信道条件下，AgeTop-k优于随机选择；(3)最优k值取决于信道条件，噪声较大时较小的k更好

Conclusion: 该方法有效解决了无线联邦学习中的延迟问题，通过天线数量和参数选择策略的权衡优化了系统性能

Abstract: We study federated learning (FL) over wireless fading channels where multiple devices simultaneously send their model updates. We propose an efficient \emph{age-aware edge-blind over-the-air FL} approach that does not require channel state information (CSI) at the devices. Instead, the parameter server (PS) uses multiple antennas and applies maximum-ratio combining (MRC) based on its estimated sum of the channel gains to detect the parameter updates. A key challenge is that the number of orthogonal subcarriers is limited; thus, transmitting many parameters requires multiple Orthogonal Frequency Division Multiplexing (OFDM) symbols, which increases latency. To address this, the PS selects only a small subset of model coordinates each round using \emph{AgeTop-\(k\)}, which first picks the largest-magnitude entries and then chooses the \(k\) coordinates with the longest waiting times since they were last selected. This ensures that all selected parameters fit into a single OFDM symbol, reducing latency. We provide a convergence bound that highlights the advantages of using a higher number of antenna array elements and demonstrates a key trade-off: increasing \(k\) decreases compression error at the cost of increasing the effect of channel noise. Experimental results show that (i) more PS antennas greatly improve accuracy and convergence speed; (ii) AgeTop-\(k\) outperforms random selection under relatively good channel conditions; and (iii) the optimum \(k\) depends on the channel, with smaller \(k\) being better in noisy settings.

</details>


### [1272] [Automated univariate time series forecasting with regression trees](https://arxiv.org/abs/2602.00077)
*Francisco Martínez,María P. Frías*

Main category: cs.LG

Relevance: 20.0

TL;DR: 提出了一种基于回归树及其集成方法（装袋和随机森林）的自动化单变量时间序列预测方法，通过自回归特征选择和递归预测处理趋势与季节性，准确率与指数平滑、ARIMA等经典统计模型相当，并提供了开源实现。


<details>
  <summary>Details</summary>
Motivation: 传统时间序列预测方法（如指数平滑、ARIMA）通常需要专业知识进行参数调整和模型选择。本文旨在开发一种自动化、无需人工干预的预测方法，利用机器学习技术（回归树集成）实现与传统统计模型相当的预测精度。

Method: 采用自回归方法，将时间序列预测转化为回归问题。使用回归树及其集成方法（装袋和随机森林）进行建模。关键创新包括：自回归特征选择策略、递归预测机制、趋势处理方法和季节性应对方案。

Result: 实验结果表明，该方法在预测准确率上与成熟的统计模型（如指数平滑、ARIMA）相当。同时开发了公开可用的软件实现，验证了方法的实用性和可复现性。

Conclusion: 基于回归树集成的时间序列预测方法能够实现与传统统计模型相当的预测性能，同时具有自动化优势，为时间序列预测提供了有效的机器学习替代方案。

Abstract: This paper describes a methodology for automated univariate time series forecasting using regression trees and their ensembles: bagging and random forests. The key aspects that are addressed are: the use of an autoregressive approach and recursive forecasts, how to select the autoregressive features, how to deal with trending series and how to cope with seasonal behavior. Experimental results show a forecast accuracy comparable with well-established statistical models such as exponential smoothing or ARIMA. Furthermore, a publicly available software implementing all the proposed strategies has been developed and is described in the paper.

</details>


### [1273] [High-accuracy sampling for diffusion models and log-concave distributions](https://arxiv.org/abs/2602.01338)
*Fan Chen,Sinho Chewi,Constantinos Daskalakis,Alexander Rakhlin*

Main category: cs.LG

Relevance: 20.0

TL;DR: 本文提出了一种扩散模型采样算法，能在polylog(1/δ)步内达到δ误差，相比之前结果实现指数级改进，复杂度与数据维度相关。


<details>
  <summary>Details</summary>
Motivation: 扩散模型采样通常需要大量计算步骤才能达到高精度，现有方法复杂度较高。本文旨在开发更高效的采样算法，显著减少达到目标精度所需的步骤数。

Method: 提出新的扩散模型采样算法，在L²范数下使用δ-精确的分数估计。算法复杂度根据数据特性而变化：最小数据假设下为Õ(d polylog(1/δ))；非均匀L-Lipschitz条件下为Õ(√(dL) polylog(1/δ))；数据分布具有内在维度d⋆时复杂度降至Õ(d⋆ polylog(1/δ))。

Result: 实现了指数级改进，在polylog(1/δ)步内达到δ误差。同时首次实现了仅使用梯度评估就能对一般对数凹分布进行polylog(1/δ)复杂度采样。

Conclusion: 本文提出的算法显著提高了扩散模型采样效率，为高效生成模型提供了理论基础，特别是在高维数据场景下具有重要应用价值。

Abstract: We present algorithms for diffusion model sampling which obtain $δ$-error in $\mathrm{polylog}(1/δ)$ steps, given access to $\widetilde O(δ)$-accurate score estimates in $L^2$. This is an exponential improvement over all previous results. Specifically, under minimal data assumptions, the complexity is $\widetilde O(d\,\mathrm{polylog}(1/δ))$ where $d$ is the dimension of the data; under a non-uniform $L$-Lipschitz condition, the complexity is $\widetilde O(\sqrt{dL}\,\mathrm{polylog}(1/δ))$; and if the data distribution has intrinsic dimension $d_\star$, then the complexity reduces to $\widetilde O(d_\star\,\mathrm{polylog}(1/δ))$. Our approach also yields the first $\mathrm{polylog}(1/δ)$ complexity sampler for general log-concave distributions using only gradient evaluations.

</details>


### [1274] [Active learning from positive and unlabeled examples](https://arxiv.org/abs/2602.02081)
*Farnam Mansouri,Sandra Zilles,Shai Ben-David*

Main category: cs.LG

Relevance: 20.0

TL;DR: 该论文首次从理论上分析了主动正无标记学习（Active PU Learning）的标签复杂度，研究了一种弱监督学习场景，其中学习者只能自适应地查询未标记池中的实例，且仅当实例为正例且独立硬币投掷成功时才获得标签信息。


<details>
  <summary>Details</summary>
Motivation: 动机源于广告和异常检测等实际应用场景，在这些场景中，通常只能获得部分正例标签，而其他实例保持未标记状态。研究者希望理解在这种弱监督设置下，学习者需要多少标签查询才能达到一定的分类性能。

Method: 研究了一种主动PU学习框架，学习者可以自适应地从未标记池中查询实例，但查询的标签仅在实例为正例且一个独立的硬币投掷成功时才被揭示；否则学习者不会获得任何信息。论文提供了该设置下标签复杂度的理论分析。

Result: 论文提供了主动PU学习标签复杂度的首次理论分析，建立了在这种弱监督设置下达到特定分类性能所需标签数量的理论界限。

Conclusion: 该研究为弱监督学习中的主动PU学习提供了理论基础，特别是在标签信息有限且获取成本高的实际应用场景中，为设计更高效的标签获取策略提供了理论指导。

Abstract: Learning from positive and unlabeled data (PU learning) is a weakly supervised variant of binary classification in which the learner receives labels only for (some) positively labeled instances, while all other examples remain unlabeled. Motivated by applications such as advertising and anomaly detection, we study an active PU learning setting where the learner can adaptively query instances from an unlabeled pool, but a queried label is revealed only when the instance is positive and an independent coin flip succeeds; otherwise the learner receives no information. In this paper, we provide the first theoretical analysis of the label complexity of active PU learning.

</details>


### [1275] [Dimensional Peeking for Low-Variance Gradients in Zeroth-Order Discrete Optimization via Simulation](https://arxiv.org/abs/2602.00075)
*Philipp Andelfinger,Wentong Cai*

Main category: cs.LG

Relevance: 15.0

TL;DR: 提出了一种称为"dimensional peeking"的方差减少方法，用于离散仿真优化中的梯度估计，通过将采样粒度从标量值提升到遵循相同控制流路径的值类来增加每次仿真评估收集的信息。


<details>
  <summary>Details</summary>
Motivation: 基于梯度的优化方法常用于高维空间中寻找局部最优解。当无法直接计算导数时，随机估计器可以提供近似梯度，但这些基于扰动的目标函数采样估计器会引入方差，导致收敛缓慢。

Method: 提出了dimensional peeking方法，这是一种用于离散仿真优化中梯度估计的方差减少技术。通过将采样粒度从标量值提升到遵循相同控制流路径的值类，增加每次仿真评估收集的信息。该方法从已建立的平滑梯度估计器推导而来，证明不会引入任何偏差。通过自定义数值数据类型在C++程序中透明地实现dimensional peeking。

Result: 在三个具有高维输入的仿真优化问题中观察到方差减少高达7.9倍。与三种元启发式算法的优化进展比较表明，dimensional peeking提高了零阶优化在离散和非凸仿真中的竞争力。

Conclusion: dimensional peeking是一种有效的方差减少方法，能够显著提高零阶优化在离散非凸仿真优化中的性能，通过更高效的信息收集机制改进了梯度估计的质量。

Abstract: Gradient-based optimization methods are commonly used to identify local optima in high-dimensional spaces. When derivatives cannot be evaluated directly, stochastic estimators can provide approximate gradients. However, these estimators' perturbation-based sampling of the objective function introduces variance that can lead to slow convergence. In this paper, we present dimensional peeking, a variance reduction method for gradient estimation in discrete optimization via simulation. By lifting the sampling granularity from scalar values to classes of values that follow the same control flow path, we increase the information gathered per simulation evaluation. Our derivation from an established smoothed gradient estimator shows that the method does not introduce any bias. We present an implementation via a custom numerical data type to transparently carry out dimensional peeking over C++ programs. Variance reductions by factors of up to 7.9 are observed for three simulation-based optimization problems with high-dimensional input. The optimization progress compared to three meta-heuristics shows that dimensional peeking increases the competitiveness of zeroth-order optimization for discrete and non-convex simulations.

</details>


### [1276] [Gauss-Newton Natural Gradient Descent for Shape Learning](https://arxiv.org/abs/2602.00099)
*James King,Arturs Berzins,Siddhartha Mishra,Marius Zeinhofer*

Main category: cs.LG

Relevance: 15.0

TL;DR: 该论文探索了在形状学习中应用高斯-牛顿法进行优化，包括隐式神经表面和几何信息神经网络，解决了微分约束的病态性和参数空间与函数空间不匹配等关键挑战。


<details>
  <summary>Details</summary>
Motivation: 形状学习面临两个主要挑战：1）底层微分约束的病态性；2）参数空间优化问题与自然问题所在函数空间之间的不匹配。传统一阶方法在这些问题上收敛慢且不稳定。

Method: 采用高斯-牛顿法进行形状学习优化，该方法通过近似Hessian矩阵来更好地处理微分约束的病态性，并弥合参数空间与函数空间之间的差距，实现更稳定高效的优化。

Result: 在基准形状优化任务上的实验表明，高斯-牛顿法相比标准一阶方法显著提高了训练速度和解的精度，收敛更快且更稳定，所需迭代次数大幅减少。

Conclusion: 高斯-牛顿法为形状学习提供了一种更有效的优化方法，能够解决传统方法面临的病态性和空间不匹配问题，在训练速度和最终解精度方面均有显著提升。

Abstract: We explore the use of the Gauss-Newton method for optimization in shape learning, including implicit neural surfaces and geometry-informed neural networks. The method addresses key challenges in shape learning, such as the ill-conditioning of the underlying differential constraints and the mismatch between the optimization problem in parameter space and the function space where the problem is naturally posed. This leads to significantly faster and more stable convergence than standard first-order methods, while also requiring far fewer iterations. Experiments across benchmark shape optimization tasks demonstrate that the Gauss-Newton method consistently improves both training speed and final solution accuracy.

</details>


### [1277] [Predicting Mortgage Default with Machine Learning: AutoML, Class Imbalance, and Leakage Control](https://arxiv.org/abs/2602.00120)
*Xianghong Hu,Tianning Xu,Ying Chen,Shuai Wang*

Main category: cs.LG

Relevance: 15.0

TL;DR: 论文比较了多种机器学习方法在抵押贷款违约预测中的应用，重点关注数据泄露控制和类别不平衡处理，发现AutoGluon在严格控制泄露和不同正负样本比例下表现最佳。


<details>
  <summary>Details</summary>
Motivation: 抵押贷款违约预测是金融风险管理的核心任务，但在真实数据集中存在三个主要问题：违约标签模糊性、严重的类别不平衡、以及时间结构和事后变量导致的信息泄露，这些问题会影响评估有效性和部署可靠性。

Method: 使用真实贷款数据集，采用泄露感知的特征选择、严格的时间分割（限制贷款发放和报告时期）、控制性下采样多数类，比较多种机器学习方法，包括AutoML（AutoGluon）。

Result: 在不同正负样本比例下，模型性能保持稳定，AutoGluon在所有评估模型中取得了最强的AUROC表现。

Conclusion: 通过严格的泄露控制和类别不平衡处理，机器学习方法可以有效应用于抵押贷款违约预测，AutoML方法在真实金融数据中表现优异。

Abstract: Mortgage default prediction is a core task in financial risk management, and machine learning models are increasingly used to estimate default probabilities and provide interpretable signals for downstream decisions. In real-world mortgage datasets, however, three factors frequently undermine evaluation validity and deployment reliability: ambiguity in default labeling, severe class imbalance, and information leakage arising from temporal structure and post-event variables. We compare multiple machine learning approaches for mortgage default prediction using a real-world loan-level dataset, with emphasis on leakage control and imbalance handling. We employ leakage-aware feature selection, a strict temporal split that constrains both origination and reporting periods, and controlled downsampling of the majority class. Across multiple positive-to-negative ratios, performance remains stable, and an AutoML approach (AutoGluon) achieves the strongest AUROC among the models evaluated. An extended and pedagogical version of this work will appear as a book chapter.

</details>


### [1278] [Contrastive Learning for Privacy Enhancements in Industrial Internet of Things](https://arxiv.org/abs/2602.00515)
*Lin Liu,Rita Machacy,Simi Kuniyilh*

Main category: cs.LG

Relevance: 15.0

TL;DR: 本文对工业物联网(IIoT)中的隐私保护技术进行了全面综述，特别关注基于对比学习的隐私保护方法，分析了工业数据的独特特性、系统架构和应用场景，并讨论了解决方案、开放挑战及未来研究方向。


<details>
  <summary>Details</summary>
Motivation: 工业物联网(IIoT)在制造业、能源和关键基础设施等领域集成了智能感知、通信和分析功能，虽然实现了预测性维护和跨站点优化，但也因操作数据的敏感性带来了显著的隐私和机密性风险。对比学习作为一种自监督表示学习范式，通过减少对标记数据和原始数据共享的依赖，成为隐私保护分析的有前景方法。

Method: 这是一篇综述性论文，系统性地回顾了基于对比学习的隐私保护技术在工业物联网(IIoT)领域的应用。文章重点分析了工业数据的独特特性、系统架构设计以及各种应用场景，并对现有解决方案进行了分类和评估。

Result: 论文提供了工业物联网隐私保护技术的全面概览，识别了基于对比学习方法的优势和局限性，总结了当前的研究现状，并指出了该领域的关键技术挑战。

Conclusion: 基于对比学习的隐私保护技术在工业物联网中具有重要应用价值，但仍面临诸多挑战。论文为未来研究指明了方向，包括改进算法效率、增强隐私保护强度、适应更复杂的工业场景等。

Abstract: The Industrial Internet of Things (IIoT) integrates intelligent sensing, communication, and analytics into industrial environments, including manufacturing, energy, and critical infrastructure. While IIoT enables predictive maintenance and cross-site optimization of modern industrial control systems, such as those in manufacturing and energy, it also introduces significant privacy and confidentiality risks due to the sensitivity of operational data. Contrastive learning, a self-supervised representation learning paradigm, has recently emerged as a promising approach for privacy-preserving analytics by reducing reliance on labeled data and raw data sharing. Although contrastive learning-based privacy-preserving techniques have been explored in the Internet of Things (IoT) domain, this paper offers a comprehensive review of these techniques specifically for privacy preservation in Industrial Internet of Things (IIoT) systems. It emphasizes the unique characteristics of industrial data, system architectures, and various application scenarios. Additionally, the paper discusses solutions and open challenges and outlines future research directions.

</details>


### [1279] [Forecasting Energy Availability in Local Energy Communities via LSTM Federated Learning](https://arxiv.org/abs/2602.00694)
*Fabio Turazza,Marcello Pietri,Natalia Selini Hadjidimitriou,Marco Mamei*

Main category: cs.LG

Relevance: 15.0

TL;DR: 论文探讨了在本地能源社区中使用联邦学习和LSTM网络进行能源预测，以解决隐私保护与预测准确性之间的权衡问题。


<details>
  <summary>Details</summary>
Motivation: 本地能源社区在实现自给自足时面临能源生产与消费平衡管理的挑战，需要准确的预测模型来支持优化算法。然而，用户隐私顾虑和法规限制阻碍了传统预测解决方案的应用，因为用户不愿共享其消费模式数据。

Method: 采用联邦学习（FL）框架结合长短期记忆（LSTM）网络，在保护用户隐私的前提下构建预测模型。联邦学习允许在本地设备上训练模型，只共享模型更新而非原始数据，从而避免敏感信息泄露。

Result: 研究表明联邦学习与LSTM网络结合能够有效实现能源预测目标，同时揭示了数据共享程度与预测准确性之间的权衡关系。该方法在保护隐私的同时仍能获得可接受的预测性能。

Conclusion: 联邦学习为解决本地能源社区中的隐私敏感预测问题提供了可行方案，通过LSTM网络实现了在数据隐私保护与预测准确性之间的有效平衡。

Abstract: Local Energy Communities are emerging as crucial players in the landscape of sustainable development. A significant challenge for these communities is achieving self-sufficiency through effective management of the balance between energy production and consumption. To meet this challenge, it is essential to develop and implement forecasting models that deliver accurate predictions, which can then be utilized by optimization and planning algorithms. However, the application of forecasting solutions is often hindered by privacy constrains and regulations as the users participating in the Local Energy Community can be (rightfully) reluctant sharing their consumption patterns with others. In this context, the use of Federated Learning (FL) can be a viable solution as it allows to create a forecasting model without the need to share privacy sensitive information among the users. In this study, we demonstrate how FL and long short-term memory (LSTM) networks can be employed to achieve this objective, highlighting the trade-off between data sharing and forecasting accuracy.

</details>


### [1280] [Mobile Exergames: Activity Recognition Based on Smartphone Sensors](https://arxiv.org/abs/2602.00809)
*David Craveiro,Hugo Silva*

Main category: cs.LG

Relevance: 15.0

TL;DR: 本文提出了一款名为Duck Catch & Fit的2D无尽游戏概念验证，结合智能手机传感器（加速度计、陀螺仪、磁力计）和语音识别系统，通过特征提取和机器学习机制实现人体活动识别。


<details>
  <summary>Details</summary>
Motivation: 智能手机传感器能够提供用户活动和行为信息，人体活动识别在游戏、医疗和监控领域应用日益广泛。本文旨在探索如何利用智能手机传感器和机器学习技术创建更沉浸式的游戏体验。

Method: 1. 使用智能手机加速度计、陀螺仪和磁力计传感器收集运动数据
2. 应用特征提取和机器学习机制识别人体活动（静止、侧向移动、虚假侧向移动）
3. 结合语音识别系统识别"fire"指令
4. 开发2D无尽游戏Duck Catch & Fit作为概念验证平台

Result: 结果表明，使用机器学习技术能够以高识别率识别人体活动。运动识别和语音识别的结合为游戏提供了更沉浸式的体验。

Conclusion: 智能手机传感器结合机器学习技术能够有效识别人体活动，这种多模态（运动+语音）集成方法能够增强游戏沉浸感，为游戏设计和活动识别应用提供了新的可能性。

Abstract: Smartphone sensors can be extremely useful in providing information on the activities and behaviors of persons. Human activity recognition is increasingly used for games, medical, or surveillance. In this paper, we propose a proof-of-concept 2D endless game called Duck Catch & Fit, which implements a detailed activity recognition system that uses a smartphone accelerometer, gyroscope, and magnetometer sensors. The system applies feature extraction and learning mechanism to detect human activities like staying, side movements, and fake side movements. In addition, a voice recognition system is combined to recognize the word "fire" and raise the game's complexity. The results show that it is possible to use machine learning techniques to recognize human activity with high recognition levels. Also, the combination of movement-based and voice-based integrations contributes to a more immersive gameplay.

</details>


### [1281] [Learning Heat-based Equations in Self-similar variables](https://arxiv.org/abs/2602.00872)
*Shihao Wang,Qipeng Qian,Jingquan Wang*

Main category: cs.LG

Relevance: 15.0

TL;DR: 该论文提出了一个自相似变量训练框架，用于学习热基方程的长期动力学，在Navier-Stokes和Burgers方程上验证了该方法能显著提高外推准确性和稳定性。


<details>
  <summary>Details</summary>
Motivation: 研究如何更好地学习热基方程的长期动力学行为，特别是解决标准神经网络训练在外推时准确性和稳定性不足的问题。自相似变量提供了数学上有意义的归纳偏置，有助于捕捉方程的长期趋势。

Method: 开发了与标准神经算子训练兼容的自相似变量训练框架。在二维不可压缩Navier-Stokes方程和一维粘性Burgers方程上实例化该框架，使用两种简单全连接架构（标准多层感知机和因子化全连接网络）进行对比实验。

Result: 在两种系统和两种架构上，自相似变量训练的网络始终提供更准确和稳定的训练窗口外推，并能更好地捕捉定性长期趋势。相比物理坐标训练，性能有显著提升。

Conclusion: 自相似坐标为学习热基方程的长期动力学提供了数学上有意义的归纳偏置，能显著改善神经网络在外推任务中的表现。

Abstract: We study solution learning for heat-based equations in self-similar variables (SSV). We develop an SSV training framework compatible with standard neural-operator training. We instantiate this framework on the two-dimensional incompressible Navier-Stokes equations and the one-dimensional viscous Burgers equation, and perform controlled comparisons between models trained in physical coordinates and in the corresponding self-similar coordinates using two simple fully connected architectures (standard multilayer perceptrons and a factorized fully connected network). Across both systems and both architectures, SSV-trained networks consistently deliver substantially more accurate and stable extrapolation beyond the training window and better capture qualitative long-time trends. These results suggest that self-similar coordinates provide a mathematically motivated inductive bias for learning the long-time dynamics of heat-based equations.

</details>


### [1282] [PyGALAX: An Open-Source Python Toolkit for Advanced Explainable Geospatial Machine Learning](https://arxiv.org/abs/2602.00907)
*Pingping Wang,Yihong Yuan,Lingcheng Li,Yongmei Lu*

Main category: cs.LG

Relevance: 15.0

TL;DR: PyGALAX是一个用于地理空间分析的Python包，集成了AutoML和XAI技术，通过自动模型选择和SHAP解释性分析来处理空间异质性，改进了传统地理加权回归方法。


<details>
  <summary>Details</summary>
Motivation: 传统地理空间分析方法（如地理加权回归）在处理空间异质性和非平稳性时存在局限性，缺乏自动化的模型选择和解释性。PyGALAX旨在提供一个集成了AutoML和XAI的易用工具，使研究人员能够更好地理解和建模复杂的空间关系。

Method: 基于GALAX框架改进，PyGALAX集成了自动机器学习（AutoML）和可解释AI（XAI）技术。关键增强包括自动带宽选择和灵活核函数选择，使用SHAP进行模型解释，支持回归和分类任务，并打包为可复现的Python工具包。

Result: PyGALAX在性能上超越了传统地理加权回归方法，提供了更灵活和稳健的空间建模能力。它能够有效处理空间非平稳性，在全局和局部尺度上生成透明的空间关系洞察。

Conclusion: PyGALAX为地理、城市规划、环境科学等领域的研究人员提供了一个强大且易用的地理空间分析工具，通过结合AutoML和XAI技术，使先进的地理空间机器学习方法更加可访问。

Abstract: PyGALAX is a Python package for geospatial analysis that integrates automated machine learning (AutoML) and explainable artificial intelligence (XAI) techniques to analyze spatial heterogeneity in both regression and classification tasks. It automatically selects and optimizes machine learning models for different geographic locations and contexts while maintaining interpretability through SHAP (SHapley Additive exPlanations) analysis. PyGALAX builds upon and improves the GALAX framework (Geospatial Analysis Leveraging AutoML and eXplainable AI), which has proven to outperform traditional geographically weighted regression (GWR) methods. Critical enhancements in PyGALAX from the original GALAX framework include automatic bandwidth selection and flexible kernel function selection, providing greater flexibility and robustness for spatial modeling across diverse datasets and research questions. PyGALAX not only inherits all the functionalities of the original GALAX framework but also packages them into an accessible, reproducible, and easily deployable Python toolkit while providing additional options for spatial modeling. It effectively addresses spatial non-stationarity and generates transparent insights into complex spatial relationships at both global and local scales, making advanced geospatial machine learning methods accessible to researchers and practitioners in geography, urban planning, environmental science, and related fields.

</details>


### [1283] [Predicting Anemia Among Under-Five Children in Nepal Using Machine Learning and Deep Learning](https://arxiv.org/abs/2602.01005)
*Deepak Bastola,Pitambar Acharya,Dipak Dulal,Rabina Dhakal,Yang Li*

Main category: cs.LG

Relevance: 15.0

TL;DR: 该研究使用机器学习方法预测尼泊尔儿童贫血状况，通过特征选择确定了关键预测因子（儿童年龄、发热、家庭规模、母亲贫血、驱虫），并比较了多种传统ML和深度学习模型，发现逻辑回归在F1分数和召回率上表现最佳。


<details>
  <summary>Details</summary>
Motivation: 儿童贫血是尼泊尔主要的公共卫生挑战，与生长发育受损、认知障碍和发病率增加相关。研究旨在开发有效的预测模型，帮助公共卫生筛查和风险分层。

Method: 使用尼泊尔人口与健康调查（NDHS 2022）的1,855名儿童数据，采用四种特征选择方法（卡方检验、互信息、点二列相关、Boruta）确定关键特征，然后比较八种传统机器学习分类器（LR、KNN、DT、RF、XGBoost、SVM、NB、LDA）和两种深度学习模型（DNN和TabNet）。

Result: 逻辑回归获得最佳召回率（0.701）和最高F1分数（0.649），DNN达到最高准确率（0.709），SVM获得最高AUC（0.736）。儿童年龄、近期发热、家庭规模、母亲贫血和驱虫是五个一致被选中的关键特征。

Conclusion: 机器学习和深度学习模型都能提供有竞争力的贫血预测，可解释的特征如儿童年龄、感染指标、母亲贫血和驱虫史对于尼泊尔的风险分层和公共卫生筛查至关重要。

Abstract: Childhood anemia remains a major public health challenge in Nepal and is associated with impaired growth, cognition, and increased morbidity. Using World Health Organization hemoglobin thresholds, we defined anemia status for children aged 6-59 months and formulated a binary classification task by grouping all anemia severities as \emph{anemic} versus \emph{not anemic}. We analyzed Nepal Demographic and Health Survey (NDHS 2022) microdata comprising 1,855 children and initially considered 48 candidate features spanning demographic, socioeconomic, maternal, and child health characteristics. To obtain a stable and substantiated feature set, we applied four features selection techniques (Chi-square, mutual information, point-biserial correlation, and Boruta) and prioritized features supported by multi-method consensus. Five features: child age, recent fever, household size, maternal anemia, and parasite deworming were consistently selected by all methods, while amenorrhea, ethnicity indicators, and provinces were frequently retained. We then compared eight traditional machine learning classifiers (LR, KNN, DT, RF, XGBoost, SVM, NB, LDA) with two deep learning models (DNN and TabNet) using standard evaluation metrics, emphasizing F1-score and recall due to class imbalance. Among all models, logistic regression attained the best recall (0.701) and the highest F1-score (0.649), while DNN achieved the highest accuracy (0.709), and SVM yielded the strongest discrimination with the highest AUC (0.736). Overall, the results indicate that both machine learning and deep learning models can provide competitive anemia prediction and the interpretable features such as child age, infection proxy, maternal anemia, and deworming history are central for risk stratification and public health screening in Nepal.

</details>


### [1284] [Multi-Fidelity Physics-Informed Neural Networks with Bayesian Uncertainty Quantification and Adaptive Residual Learning for Efficient Solution of Parametric Partial Differential Equations](https://arxiv.org/abs/2602.01176)
*Olaf Yunus Laitinen Imanov*

Main category: cs.LG

Relevance: 15.0

TL;DR: 提出了MF-BPINN，一个结合物理信息神经网络与贝叶斯不确定性量化的多保真度框架，用于高效求解参数化偏微分方程


<details>
  <summary>Details</summary>
Motivation: 物理信息神经网络在求解偏微分方程方面表现出色，但求解高保真度PDE计算成本过高，特别是对于需要跨多个参数配置进行评估的参数化系统。现有方法难以高效利用不同保真度的数据。

Method: 提出MF-BPINN框架：1）分层神经网络架构学习跨保真度级别的非线性相关性；2）自适应残差网络具有可学习门控机制，动态平衡线性和非线性保真度差异；3）严格的贝叶斯框架采用哈密顿蒙特卡洛方法进行不确定性量化

Result: 该方法能够利用丰富的低保真度模拟和稀疏的高保真度数据，显著提高计算效率，同时提供可靠的不确定性估计

Conclusion: MF-BPINN为求解计算昂贵的参数化偏微分方程提供了一个高效、可扩展的框架，通过多保真度学习和贝叶斯不确定性量化实现了精度和效率的平衡

Abstract: Physics-informed neural networks (PINNs) have emerged as a powerful paradigm for solving partial differential equations (PDEs) by embedding physical laws directly into neural network training. However, solving high-fidelity PDEs remains computationally prohibitive, particularly for parametric systems requiring multiple evaluations across varying parameter configurations. This paper presents MF-BPINN, a novel multi-fidelity framework that synergistically combines physics-informed neural networks with Bayesian uncertainty quantification and adaptive residual learning. Our approach leverages abundant low-fidelity simulations alongside sparse high-fidelity data through a hierarchical neural architecture that learns nonlinear correlations across fidelity levels. We introduce an adaptive residual network with learnable gating mechanisms that dynamically balances linear and nonlinear fidelity discrepancies. Furthermore, we develop a rigorous Bayesian framework employing Hamiltonian Monte Carlo.

</details>


### [1285] [Modeling Topological Impact on Node Attribute Distributions in Attributed Graphs](https://arxiv.org/abs/2602.01454)
*Amirreza Shiralinasab Langari,Leila Yeganeh,Kim Khoa Nguyen*

Main category: cs.LG

Relevance: 15.0

TL;DR: 该论文提出了一种代数方法，将图拓扑与节点属性分布相结合，研究拓扑如何影响节点属性分布，并建立了拓扑感知的分布框架。


<details>
  <summary>Details</summary>
Motivation: 研究图拓扑如何影响节点属性的分布，将拓扑和属性视为结构上不同但相互作用的组件，提供理解图数据中拓扑-属性关系的新视角。

Method: 1. 引入代数方法将图拓扑与节点属性概率分布相结合；2. 开发范畴论框架形式化节点对拓扑的感知；3. 量化节点视角并与属性分布整合；4. 建立充分性条件证明在完全图中恢复原始属性分布；5. 使用简单测试模型ID和异常检测任务进行评估。

Result: 提出了拓扑影响分布的理论框架，将拓扑条件分布解释为后验概率P(·|v)和P(·|G)的近似，并通过异常检测任务验证了方法的有效性。

Conclusion: 该研究为理解图数据中拓扑与属性的相互作用提供了理论基础，提出的框架能够捕获拓扑对节点属性分布的影响，并为图分析任务提供了新的工具。

Abstract: We investigate how the topology of attributed graphs influences the distribution of node attributes. This work offers a novel perspective by treating topology and attributes as structurally distinct but interacting components. We introduce an algebraic approach that combines a graph's topology with the probability distribution of node attributes, resulting in topology-influenced distributions. First, we develop a categorical framework to formalize how a node perceives the graph's topology. We then quantify this point of view and integrate it with the distribution of node attributes to capture topological effects. We interpret these topology-conditioned distributions as approximations of the posteriors $P(\cdot \mid v)$ and $P(\cdot \mid \mathcal{G})$.
  We further establish a principled sufficiency condition by showing that, on complete graphs, where topology carries no informative structure, our construction recovers the original attribute distribution. To evaluate our approach, we introduce an intentionally simple testbed model, $\textbf{ID}$, and use unsupervised graph anomaly detection as a probing task.

</details>


### [1286] [Calibrating Adaptive Smoothing Methods for Freeway Traffic Reconstruction](https://arxiv.org/abs/2602.02072)
*Junyi Ji,Derek Gloudemans,Gergely Zachár,Matthew Nice,William Barbour,Daniel B. Work*

Main category: cs.LG

Relevance: 15.0

TL;DR: 本文提出了一个用于交通状态重建的自适应平滑方法(ASM)的Python实现，采用端到端校准，使用真实世界地面真实数据进行参数化核优化，并在PyTorch中开发以便与深度学习方法集成。


<details>
  <summary>Details</summary>
Motivation: 自适应平滑方法(ASM)是交通状态重建的常用方法，但缺乏可复现的基准实现。本文旨在提供一个可复现的Python实现，通过端到端校准提高准确性，并建立交通重建问题的基准指标。

Method: 1) 在PyTorch中实现ASM，便于与深度学习集成；2) 将校准公式化为参数化核优化问题；3) 使用稀疏雷达传感器网络输入的全状态观测测试台数据进行校准；4) 评估速度分布、时空误差分布和空间误差等指标。

Result: 实现了可复现的ASM基准，提供了交通重建的基准指标，展示了校准方法在多个高速公路上的可用性，并讨论了交通模型校准的可复现性挑战和ASM的局限性。

Conclusion: 本文提供了一个可复现的交通状态重建基准实现，可作为各种高速公路运营任务的基准，同时指出了交通模型校准的可复现性挑战和ASM方法的局限性。

Abstract: The adaptive smoothing method (ASM) is a widely used approach for traffic state reconstruction. This article presents a Python implementation of ASM, featuring end-to-end calibration using real-world ground truth data. The calibration is formulated as a parameterized kernel optimization problem. The model is calibrated using data from a full-state observation testbed, with input from a sparse radar sensor network. The implementation is developed in PyTorch, enabling integration with various deep learning methods. We evaluate the results in terms of speed distribution, spatio-temporal error distribution, and spatial error to provide benchmark metrics for the traffic reconstruction problem. We further demonstrate the usability of the calibrated method across multiple freeways. Finally, we discuss the challenges of reproducibility in general traffic model calibration and the limitations of ASM. This article is reproducible and can serve as a benchmark for various freeway operation tasks.

</details>


### [1287] [C-kNN-LSH: A Nearest-Neighbor Algorithm for Sequential Counterfactual Inference](https://arxiv.org/abs/2602.02371)
*Jing Wang,Jie Shen,Qiaomin Xie,Jeremy C Weiss*

Main category: cs.LG

Relevance: 15.0

TL;DR: 提出C-kNN-LSH框架，使用局部敏感哈希高效识别"临床双胞胎"，在纵向轨迹中进行序列因果推断，特别针对长新冠恢复等复杂临床场景。


<details>
  <summary>Details</summary>
Motivation: 从纵向轨迹中估计因果效应对于理解复杂疾病进展和优化临床决策至关重要，如共病和长新冠恢复。现有方法难以处理高维、混杂的临床数据。

Method: 提出C-kNN-LSH框架：1) 使用局部敏感哈希高效识别具有相似协变量历史的"临床双胞胎"；2) 在演化疾病状态中进行局部条件治疗效果估计；3) 集成邻域估计器和双重稳健校正，减轻不规则采样和患者恢复模式变化带来的偏差。

Result: 在包含13,511名参与者的真实世界长新冠队列中，C-kNN-LSH在捕捉恢复异质性和估计政策价值方面表现出优于现有基线的性能。

Conclusion: C-kNN-LSH为高维混杂情况下的序列因果推断提供了有效框架，特别适用于临床纵向数据分析，理论分析保证估计量的一致性和对干扰误差的二阶稳健性。

Abstract: Estimating causal effects from longitudinal trajectories is central to understanding the progression of complex conditions and optimizing clinical decision-making, such as comorbidities and long COVID recovery. We introduce \emph{C-kNN--LSH}, a nearest-neighbor framework for sequential causal inference designed to handle such high-dimensional, confounded situations. By utilizing locality-sensitive hashing, we efficiently identify ``clinical twins'' with similar covariate histories, enabling local estimation of conditional treatment effects across evolving disease states. To mitigate bias from irregular sampling and shifting patient recovery profiles, we integrate neighborhood estimator with a doubly-robust correction.
  Theoretical analysis guarantees our estimator is consistent and second-order robust to nuisance error.
  Evaluated on a real-world Long COVID cohort with 13,511 participants, \emph{C-kNN-LSH} demonstrates superior performance in capturing recovery heterogeneity and estimating policy values compared to existing baselines.

</details>


### [1288] [Non-Clashing Teaching in Graphs: Algorithms, Complexity, and Bounds](https://arxiv.org/abs/2602.00657)
*Sujoy Bhore,Liana Khazaliya,Fionn Mc Inerney*

Main category: cs.CC

Relevance: 15.0

TL;DR: 该论文研究图论中的非冲突教学模型，针对图的闭邻域概念类，改进了算法结果并推导了更强的下界。


<details>
  <summary>Details</summary>
Motivation: 研究非冲突教学模型在图论概念类中的应用，特别是针对闭邻域这一广泛且具有代表性的概念类，旨在改进现有算法结果并建立更强的理论界限。

Method: 采用图论和算法复杂性理论方法，研究闭邻域概念类的非冲突教学问题，开发FPT算法并推导组合上界。

Result: 获得了比球概念类更优的算法结果，包括更广泛参数类的FPT算法，更强的下界，以及更广泛图类的组合上界。

Conclusion: 闭邻域概念类的非冲突教学问题具有重要的理论和算法意义，本文结果显著推进了该领域的研究。

Abstract: Kirkpatrick et al. [ALT 2019] and Fallat et al. [JMLR 2023] introduced non-clashing teaching and proved that it is the most efficient batch machine teaching model satisfying the collusion-avoidance benchmark established in the seminal work of Goldman and Mathias [COLT 1993]. Recently, (positive) non-clashing teaching was thoroughly studied for balls in graphs, yielding numerous algorithmic and combinatorial results. In particular, Chalopin et al. [COLT 2024] and Ganian et al. [ICLR 2025] gave an almost complete picture of the complexity landscape of the positive variant, showing that it is tractable only for restricted graph classes due to the non-trivial nature of the problem and concept class.
  In this work, we consider (positive) non-clashing teaching for closed neighborhoods in graphs. This concept class is not only extensively studied in various related contexts, but it also exhibits broad generality, as any finite binary concept class can be equivalently represented by a set of closed neighborhoods in a graph. In comparison to the works on balls in graphs, we provide improved algorithmic results, notably including FPT algorithms for more general classes of parameters, and we complement these results by deriving stronger lower bounds. Lastly, we obtain combinatorial upper bounds for wider classes of graphs.

</details>


### [1289] [A New Workflow for Materials Discovery Bridging the Gap Between Experimental Databases and Graph Neural Networks](https://arxiv.org/abs/2602.00756)
*Brandon Schoener,Yuting Hu,Pasit Wanlapha,Akshay Rengarajan,Ian Moog,Michael Wang,Peihong Zhang,Jinjun Xiong,Hao Zeng*

Main category: cond-mat.mtrl-sci

Relevance: 15.0

TL;DR: 该论文提出了一种将实验材料数据库与晶体结构文件对齐的方法，以解决材料属性预测中缺乏原子坐标信息的问题，从而支持更先进的机器学习架构。


<details>
  <summary>Details</summary>
Motivation: 材料属性预测中面临训练数据严重不足的问题，特别是许多实验数据库缺乏完整的原子坐标信息，这限制了使用先进的机器学习架构（如图神经网络）进行材料属性预测。

Method: 通过将实验数据库（如NEMAD）与无机晶体结构数据库（ICSD）中的晶体学信息文件（CIF）进行对齐，创建包含完整原子坐标信息的数据库，从而支持先进的模型架构和迁移学习。

Result: 与原始NEMAD数据库相比，使用对齐后的数据库训练的模型在预测磁性材料的排序温度和磁基态时，均方误差（MAE）和正确分类率（CCR）均有显著提升。

Conclusion: 通过实验数据库与晶体结构文件的对齐，可以创建支持先进机器学习架构的材料数据库，显著提高材料属性预测的准确性，并为迁移学习应用开辟了道路。

Abstract: Incorporating Machine Learning (ML) into material property prediction has become a crucial step in accelerating materials discovery. A key challenge is the severe lack of training data, as many properties are too complicated to calculate with high-throughput first principles techniques. To address this, recent research has created experimental databases from information extracted from scientific literature. However, most existing experimental databases do not provide full atomic coordinate information, which prevents them from supporting advanced ML architectures such as Graph Neural Networks (GNNs). In this work, we propose to bridge this gap through an alignment process between experimental databases and Crystallographic Information Files (CIF) from the Inorganic Crystal Structure Database (ICSD). Our approach enables the creation of a database that can fully leverage state-of-the-art model architectures for material property prediction. It also opens the door to utilizing transfer learning to improve prediction accuracy. To validate our approach, we align NEMAD with the ICSD and compare models trained on the resulting database to those trained on NEMAD originally. We demonstrate significant improvements in both Mean Absolute Error (MAE) and Correct Classification Rate (CCR) in predicting the ordering temperatures and magnetic ground states of magnetic materials, respectively.

</details>


### [1290] [The Quantum Learning Menagerie (A survey on Quantum learning for Classical concepts)](https://arxiv.org/abs/2602.01054)
*Sagnik Chatterjee*

Main category: quant-ph

Relevance: 15.0

TL;DR: 本文综述了量子学习理论领域，特别关注在PAC框架下学习量子编码的经典概念，重点分析了经典与量子学习在查询、样本和时间复杂度上的分离，并提出了23个开放问题。


<details>
  <summary>Details</summary>
Motivation: 量子计算的发展为学习理论带来了新的可能性，本文旨在系统整理量子学习理论中关于量子编码经典概念学习的研究成果，特别是探索经典与量子学习在复杂度上的根本差异。

Method: 采用综述研究方法，在PAC学习框架下，分析不同标注oracle查询访问下的量子学习算法，重点关注查询复杂度、样本复杂度和时间复杂度的理论分析。

Result: 系统整理了量子学习理论中的已知结果，揭示了经典与量子学习在复杂度上存在显著分离，特别是在特定oracle访问模型下，量子算法能实现指数级加速。

Conclusion: 量子学习理论为理解量子计算在学习任务中的优势提供了理论框架，但仍有许多未解问题，本文提出的23个开放问题为该领域的未来研究指明了方向。

Abstract: This paper surveys various results in the field of Quantum Learning theory, specifically focusing on learning quantum-encoded classical concepts in the Probably Approximately Correct (PAC) framework. The cornerstone of this work is the emphasis on query, sample, and time complexity separations between classical and quantum learning that emerge under learning with query access to different labeling oracles. This paper aims to consolidate all known results in the area under the above umbrella and underscore the limits of our understanding by leaving the reader with 23 open problems.

</details>


### [1291] [Nonlinear model reduction for transport-dominated problems](https://arxiv.org/abs/2602.01397)
*Jan S. Hesthaven,Benjamin Peherstorfer,Benjamin Unger*

Main category: math.NA

Relevance: 15.0

TL;DR: 该论文综述了在输运主导问题中有效的非线性模型降阶方法，这些方法能克服线性降维在波状现象和移动相干结构中的固有低效性。


<details>
  <summary>Details</summary>
Motivation: 线性降维方法在处理输运主导问题（如波状现象和移动相干结构）时存在固有低效性，这些情况通常与Kolmogorov障碍相关。需要开发非线性模型降阶方法来克服这些限制。

Method: 围绕三个关键要素组织非线性模型降阶技术：非线性参数化、降阶动力学和在线求解器。将现有方法分类为：基于变换的方法、在线自适应技术，以及结合通用非线性参数化与瞬时残差最小化的公式。

Result: 系统性地分类和分析了针对输运主导问题的非线性模型降阶方法，为克服线性降维在复杂动力学系统中的局限性提供了框架。

Conclusion: 非线性模型降阶方法在处理输运主导问题时比线性方法更有效，通过非线性参数化、降阶动力学和在线求解器的组合可以克服Kolmogorov障碍。

Abstract: This article surveys nonlinear model reduction methods that remain effective in regimes where linear reduced-space approximations are intrinsically inefficient, such as transport-dominated problems with wave-like phenomena and moving coherent structures, which are commonly associated with the Kolmogorov barrier. The article organizes nonlinear model reduction techniques around three key elements -- nonlinear parametrizations, reduced dynamics, and online solvers -- and categorizes existing approaches into transformation-based methods, online adaptive techniques, and formulations that combine generic nonlinear parametrizations with instantaneous residual minimization.

</details>


### [1292] [Scale-covariant spiking wavelets](https://arxiv.org/abs/2602.02020)
*Jens Egholm Pedersen,Tony Lindeberg,Peter Gerstoft*

Main category: cs.NE

Relevance: 15.0

TL;DR: 论文建立了小波变换与脉冲神经网络的理论联系，通过尺度空间理论实现离散母小波逼近连续小波，为更节能的信号处理算法提供了新的脉冲信号表示方法。


<details>
  <summary>Details</summary>
Motivation: 该研究的动机在于建立小波变换与脉冲神经网络之间的理论联系，探索如何利用脉冲神经网络的尺度协变特性来实现离散小波变换，从而开发更节能的信号处理算法。

Method: 基于尺度空间理论，利用泄漏积分发放神经元的尺度协变保证来实现离散母小波，使其能够逼近连续小波。通过重构实验验证方法的可行性。

Result: 重构实验证明了该方法的可行性，但存在当前逼近误差需要进一步分析。研究提出了一种新颖的脉冲信号表示方法。

Conclusion: 该工作建立了小波变换与脉冲神经网络的理论连接，为实现更节能的信号处理算法提供了新的脉冲信号表示途径，需要进一步研究来减少逼近误差。

Abstract: We establish a theoretical connection between wavelet transforms and spiking neural networks through scale-space theory. We rely on the scale-covariant guarantees in the leaky integrate-and-fire neurons to implement discrete mother wavelets that approximate continuous wavelets. A reconstruction experiment demonstrates the feasibility of the approach and warrants further analysis to mitigate current approximation errors. Our work suggests a novel spiking signal representation that could enable more energy-efficient signal processing algorithms.

</details>


### [1293] [PCA of probability measures: Sparse and Dense sampling regimes](https://arxiv.org/abs/2602.02190)
*Gachon Erell,Jérémie Bigot,Elsa Cazelles*

Main category: stat.ML

Relevance: 15.0

TL;DR: 该论文研究了概率测度主成分分析(PCA)的双渐近收敛率，其中n个概率测度各通过m个样本观测，推导出形式为n^{-1/2} + m^{-α}的收敛率，揭示了从稀疏到密集的转换行为。


<details>
  <summary>Details</summary>
Motivation: 现有文献已充分理解从m个样本估计单个测度嵌入的收敛率，但未涉及多个测度的场景。本文旨在研究n个概率测度各通过m个样本观测的双渐近机制下的PCA收敛行为。

Method: 采用希尔伯特空间嵌入方法，将概率测度映射到希尔伯特空间，应用标准函数PCA技术。在双渐近机制下分析经验协方差算子和PCA超额风险的收敛率。

Result: 推导出经验协方差算子和PCA超额风险的收敛率为n^{-1/2} + m^{-α}，其中α>0取决于嵌入选择。揭示了从稀疏(小m)到密集(大m)的转换行为，并证明密集机制下的收敛率对于经验协方差误差是最小极大最优的。

Conclusion: 该研究首次系统分析了多测度PCA的收敛理论，建立了测度数量n与每个测度样本数m之间的定量关系，为概率测度PCA的实际应用提供了理论指导。

Abstract: A common approach to perform PCA on probability measures is to embed them into a Hilbert space where standard functional PCA techniques apply. While convergence rates for estimating the embedding of a single measure from $m$ samples are well understood, the literature has not addressed the setting involving multiple measures. In this paper, we study PCA in a double asymptotic regime where $n$ probability measures are observed, each through $m$ samples. We derive convergence rates of the form $n^{-1/2} + m^{-α}$ for the empirical covariance operator and the PCA excess risk, where $α>0$ depends on the chosen embedding. This characterizes the relationship between the number $n$ of measures and the number $m$ of samples per measure, revealing a sparse (small $m$) to dense (large $m$) transition in the convergence behavior. Moreover, we prove that the dense-regime rate is minimax optimal for the empirical covariance error. Our numerical experiments validate these theoretical rates and demonstrate that appropriate subsampling preserves PCA accuracy while reducing computational cost.

</details>
