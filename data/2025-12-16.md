<div id=toc></div>

# Table of Contents

- [cs.CL](#cs.CL) [Total: 73]
- [cs.CV](#cs.CV) [Total: 223]
- [cs.AI](#cs.AI) [Total: 131]
- [cs.LG](#cs.LG) [Total: 219]


<div id='cs.CL'></div>

# cs.CL [[Back]](#toc)

### [1] [Reinforcement Learning for Latent-Space Thinking in LLMs](https://arxiv.org/abs/2512.11816)
*Enes Özeren,Matthias Aßenmacher*

Main category: cs.CL

Relevance: 85.0

TL;DR: 该论文研究潜在空间思维（latent-space thinking）在数学推理任务中的表现，发现现有监督微调方法存在局限性，探索强化学习方法但仍落后于传统语言空间思维链推理。


<details>
  <summary>Details</summary>
Motivation: 传统思维链推理使用离散语言空间效率低下，因为许多生成的token只是强制执行语言规则而非推理所需。潜在空间思维允许模型使用连续嵌入空间进行思考，但现有方法在复杂任务（如数学推理）中表现不佳。

Method: 1. 分析Coconut方法（潜在空间思维的监督微调）的设计敏感性和局限性；2. 探索强化学习技术（包括GRPO）用于潜在空间思维；3. 设计新颖的Latent RL方法直接优化潜在思维步骤。

Result: 实验结果表明：1. Coconut方法对设计选择高度敏感且存在固有局限性；2. RL训练模型在数学推理领域仍落后于传统语言空间思维链模型。

Conclusion: 潜在空间思维在数学推理等复杂任务中仍面临挑战，强化学习方法虽能改进但仍未超越传统语言空间思维链推理，需要进一步研究。

Abstract: Chain-of-Thought (CoT) reasoning typically utilizes the discrete language space for thinking, which is inherently inefficient, as many generated tokens only enforce linguistic rules that are not required for reasoning. To bypass this, latent-space thinking allows models to think using the continuous embedding space. While existing methods for training those models show domain-specific gains, they fail to maintain performance in complex tasks, such as mathematical reasoning. We experimentally demonstrate that the Coconut approach, a form of supervised fine-tuning for latent-space thinking, is highly sensitive to design choices and exhibits several inherent limitations. To address these issues, we investigate reinforcement learning (RL) techniques -- an underexplored direction in latent-space thinking -- including GRPO and design a novel Latent RL method for directly optimizing the latent thinking steps. Our experimental results reveal that these RL-trained models still lag behind traditional language-space CoT models in the mathematical reasoning domain. We make our codebase publicly available.

</details>


### [2] [Direct Confidence Alignment: Aligning Verbalized Confidence with Internal Confidence In Large Language Models](https://arxiv.org/abs/2512.11998)
*Glenn Zhang,Treasure Mayowa,Jason Fan,Yicheng Fu,Aaron Sandoval,Sean O'Brien,Kevin Zhu*

Main category: cs.CL

Relevance: 85.0

TL;DR: 提出Direct Confidence Alignment (DCA)方法，使用Direct Preference Optimization对齐LLM的口头置信度与内部置信度，而非地面真实准确性，以增强模型透明度和可靠性。


<details>
  <summary>Details</summary>
Motivation: LLM的内部置信度（来自token概率）与口头置信度不一致，导致不同校准方法产生误导性结果。需要提高模型置信度表达的一致性，增强透明度和可靠性。

Method: 提出Direct Confidence Alignment (DCA)，使用Direct Preference Optimization技术，通过偏好优化对齐LLM的口头置信度与内部置信度。引入三种新的基于校准误差的评估指标。

Result: DCA在某些模型架构上改善了置信度对齐指标，减少了置信度表达的不一致性。但在其他模型架构上效果有限，表明需要更针对特定模型的校准方法。

Conclusion: DCA方法能有效对齐LLM的口头与内部置信度，但效果因模型架构而异。为实现更可解释和可信的LLM，需要开发更模型感知的校准方法。

Abstract: Producing trustworthy and reliable Large Language Models (LLMs) has become increasingly important as their usage becomes more widespread. Calibration seeks to achieve this by improving the alignment between the model's confidence and the actual likelihood of its responses being correct or desirable. However, it has been observed that the internal confidence of a model, derived from token probabilities, is not well aligned with its verbalized confidence, leading to misleading results with different calibration methods. In this paper, we propose Direct Confidence Alignment (DCA), a method using Direct Preference Optimization to align an LLM's verbalized confidence with its internal confidence rather than ground-truth accuracy, enhancing model transparency and reliability by ensuring closer alignment between the two confidence measures. We evaluate DCA across multiple open-weight LLMs on a wide range of datasets. To further assess this alignment, we also introduce three new calibration error-based metrics. Our results show that DCA improves alignment metrics on certain model architectures, reducing inconsistencies in a model's confidence expression. However, we also show that it can be ineffective on others, highlighting the need for more model-aware approaches in the pursuit of more interpretable and trustworthy LLMs.

</details>


### [3] [Hold Onto That Thought: Assessing KV Cache Compression On Reasoning](https://arxiv.org/abs/2512.12008)
*Minghui Liu,Aadi Palnitkar,Tahseen Rabbani,Hyunwoo Jae,Kyle Rui Sang,Dixi Yao,Shayan Shabihi,Fuheng Zhao,Tian Li,Ce Zhang,Furong Huang,Kunpeng Zhang*

Main category: cs.CL

Relevance: 85.0

TL;DR: 本文评估了KV缓存压缩策略在长推理任务上的表现，发现不同策略在不同数据集上效果各异，H2O和SnapKV的变体在推理模型上表现最佳，揭示了缓存大小与推理成本之间的权衡。


<details>
  <summary>Details</summary>
Motivation: 大语言模型在处理长上下文任务时，KV缓存会随上下文长度线性增长，导致内存瓶颈。现有压缩策略主要针对预填充阶段，但在需要长解码的推理任务（如GSM8K、MATH500）上表现未得到充分评估，这些任务通常涉及多步推理和自反思，产生数千个token的思考序列。

Method: 本研究对多种流行的KV缓存压缩策略在长推理任务上进行基准测试。使用Llama-3.1-8B-Instruct模型，评估不同策略在推理任务中的表现，特别关注H2O和作者提出的解码支持型SnapKV变体。

Result: 对于非推理模型，没有单一策略适用于所有情况，性能受数据集类型影响很大。对于推理模型，H2O和解码支持型SnapKV变体是主导策略，表明重击者跟踪对推理轨迹有用。在低预算下，驱逐策略可以产生更长的推理轨迹，揭示了缓存大小与推理成本之间的权衡。

Conclusion: KV缓存压缩策略在长推理任务中的表现具有复杂性，需要根据具体任务类型和模型特性选择合适的策略。重击者跟踪策略在推理任务中表现优异，而缓存大小与推理效率之间存在需要权衡的关系。

Abstract: Large language models (LLMs) have demonstrated remarkable performance on long-context tasks, but are often bottlenecked by memory constraints. Namely, the KV cache, which is used to significantly speed up attention computations, grows linearly with context length. A suite of compression algorithms has been introduced to alleviate cache growth by evicting unimportant tokens. However, several popular strategies are targeted towards the prefill phase, i.e., processing long prompt context, and their performance is rarely assessed on reasoning tasks requiring long decoding. In particular, short but complex prompts, such as those in benchmarks like GSM8K and MATH500, often benefit from multi-step reasoning and self-reflection, resulting in thinking sequences thousands of tokens long. In this work, we benchmark the performance of several popular compression strategies on long-reasoning tasks. For the non-reasoning Llama-3.1-8B-Instruct, we determine that no singular strategy fits all, and that performance is heavily influenced by dataset type. However, we discover that H2O and our decoding-enabled variant of SnapKV are dominant strategies for reasoning models, indicating the utility of heavy-hitter tracking for reasoning traces. We also find that eviction strategies at low budgets can produce longer reasoning traces, revealing a tradeoff between cache size and inference costs.

</details>


### [4] [Benchmarking Contextual Understanding for In-Car Conversational Systems](https://arxiv.org/abs/2512.12042)
*Philipp Habicht,Lev Sorokin,Abdullah Saydemir,Ken E. Friedl,Andrea Stocco*

Main category: cs.CL

Relevance: 85.0

TL;DR: 该论文探索使用LLM评估车载对话问答系统的准确性，通过多种提示技术和代理方法测试13个不同规模的LLM，发现DeepSeek-R1在自洽性提示下达到最佳性能，而非推理模型DeepSeek-V3在效果与成本效率间取得最佳平衡。


<details>
  <summary>Details</summary>
Motivation: 车载对话问答系统虽然提升用户体验，但评估其准确性和可靠性仍具挑战。传统人工评估成本高且难以扩展，需要开发基于LLM的自动化评估方法，以衡量系统响应是否符合用户话语和上下文约束。

Method: 1) 合成生成用户话语及正确/包含错误的系统响应；2) 使用输入-输出、思维链、自洽性提示和多代理提示技术；3) 测试13个不同规模和供应商的推理与非推理LLM（OpenAI、DeepSeek、Mistral AI、Meta）；4) 以餐厅推荐为案例研究评估话语-响应一致性。

Result: 1) 高级提示技术（特别是多代理提示）对小规模非推理模型提升最显著；2) 推理模型始终优于非推理模型，最佳性能为DeepSeek-R1在自洽性提示下达到F1分数0.99；3) DeepSeek-V3在效果与成本时间效率间达到最佳平衡；4) LLM评估为ConvQA系统的上下文理解评估提供了可扩展且准确的替代方案。

Conclusion: LLM基评估为车载对话问答系统的上下文理解评估提供了可扩展且准确的替代传统人工评估的方法，推理模型在性能上占优，而非推理模型在成本效率方面更具优势，具体模型选择需根据评估需求权衡。

Abstract: In-Car Conversational Question Answering (ConvQA) systems significantly enhance user experience by enabling seamless voice interactions. However, assessing their accuracy and reliability remains a challenge. This paper explores the use of Large Language Models (LLMs) alongside advanced prompting techniques and agent-based methods to evaluate the extent to which ConvQA system responses adhere to user utterances. The focus lies on contextual understanding and the ability to provide accurate venue recommendations considering user constraints and situational context. To evaluate utterance-response coherence using an LLM, we synthetically generate user utterances accompanied by correct and modified failure-containing system responses. We use input-output, chain-of-thought, self-consistency prompting, and multi-agent prompting techniques with 13 reasoning and non-reasoning LLMs of varying sizes and providers, including OpenAI, DeepSeek, Mistral AI, and Meta. We evaluate our approach on a case study involving restaurant recommendations. The most substantial improvements occur for small non-reasoning models when applying advanced prompting techniques, particularly multi-agent prompting. However, reasoning models consistently outperform non-reasoning models, with the best performance achieved using single-agent prompting with self-consistency. Notably, DeepSeek-R1 reaches an F1-score of 0.99 at a cost of 0.002 USD per request. Overall, the best balance between effectiveness and cost-time efficiency is reached with the non-reasoning model DeepSeek-V3. Our findings show that LLM-based evaluation offers a scalable and accurate alternative to traditional human evaluation for benchmarking contextual understanding in ConvQA systems.

</details>


### [5] [VOYAGER: A Training Free Approach for Generating Diverse Datasets using LLMs](https://arxiv.org/abs/2512.12072)
*Avinash Amballa,Yashas Malur Saidutta,Chi-Heng Lin,Vivek Kulkarni,Srinivas Chappidi*

Main category: cs.CL

Relevance: 85.0

TL;DR: Voyager是一种基于行列式点过程的新方法，用于生成多样化的LLM合成数据集，无需训练且可扩展。


<details>
  <summary>Details</summary>
Motivation: 当前使用LLM生成合成数据集的方法存在多样性不足的问题，这会影响下游模型的评估和训练效果。

Method: 提出Voyager方法：基于行列式点过程的迭代优化框架，直接优化数据集多样性指标，无需训练，适用于闭源模型。

Result: Voyager在多样性方面显著优于基线方法，提供1.5-3倍的多样性提升。

Conclusion: Voyager为LLM生成多样化合成数据集提供了有效的训练免费解决方案，具有理论保证和实际效果。

Abstract: Large language models (LLMs) are increasingly being used to generate synthetic datasets for the evaluation and training of downstream models. However, prior work has noted that such generated data lacks diversity. In this paper, we propose Voyager, a novel principled approach to generate diverse datasets. Our approach is iterative and directly optimizes a mathematical quantity that optimizes the diversity of the dataset using the machinery of determinantal point processes. Furthermore, our approach is training-free, applicable to closed-source models, and scalable. In addition to providing theoretical justification for the working of our method, we also demonstrate through comprehensive experiments that Voyager significantly outperforms popular baseline approaches by providing a 1.5-3x improvement in diversity.

</details>


### [6] [BLASST: Dynamic BLocked Attention Sparsity via Softmax Thresholding](https://arxiv.org/abs/2512.12087)
*Jiayi Yuan,Cameron Shinn,Kai Xu,Jingze Cui,George Klimiashvili,Guangxuan Xiao,Perkz Zheng,Bo Li,Yuxin Zhou,Zhouhai Ye,Weijie You,Tian Zheng,Dominic Brown,Pengbo Wang,Richard Cai,Julien Demouth,John D. Owens,Xia Hu,Song Han,Timmy Liu,Huizi Mao*

Main category: cs.CL

Relevance: 85.0

TL;DR: BLASST是一种无需预计算的动态稀疏注意力方法，通过固定阈值和在线softmax信息剪枝注意力矩阵，显著加速长上下文推理，在保持高精度的同时实现1.62倍预填充和1.48倍解码加速。


<details>
  <summary>Details</summary>
Motivation: 随着LLMs对长上下文推理需求的增长，标准注意力机制的计算和内存瓶颈日益突出。需要一种能够无缝集成到现有FlashAttention内核中的高效稀疏注意力方法，以加速长上下文推理。

Method: 提出BLASST方法：1) 使用固定阈值和在线softmax信息动态识别可忽略的注意力分数；2) 跳过softmax计算、Value块加载和后续矩阵乘法；3) 适用于所有注意力变体（MHA、GQA、MQA、MLA）；4) 开发自动校准程序，发现最优阈值与上下文长度之间的简单反比关系。

Result: 在现代GPU上实现：1) 预填充阶段74.7%稀疏度下1.62倍加速；2) 解码阶段73.2%稀疏度下1.48倍加速；3) 保持高精度；4) 通过稀疏感知训练进一步提高精度-稀疏度前沿。

Conclusion: BLASST提供了一种统一的长上下文推理加速解决方案，无缝集成到现有FlashAttention设计中，通过动态稀疏化显著提升效率，且可通过稀疏感知训练进一步优化。

Abstract: The growing demand for long-context inference capabilities in Large Language Models (LLMs) has intensified the computational and memory bottlenecks inherent to the standard attention mechanism. To address this challenge, we introduce BLASST, a drop-in sparse attention method that dynamically prunes the attention matrix without any pre-computation or proxy scores. Our method uses a fixed threshold and existing information from online softmax to identify negligible attention scores, skipping softmax computation, Value block loading, and the subsequent matrix multiplication. This fits seamlessly into existing FlashAttention kernel designs with negligible latency overhead. The approach is applicable to both prefill and decode stages across all attention variants (MHA, GQA, MQA, and MLA), providing a unified solution for accelerating long-context inference. We develop an automated calibration procedure that reveals a simple inverse relationship between optimal threshold and context length, enabling robust deployment across diverse scenarios. Maintaining high accuracy, we demonstrate a 1.62x speedup for prefill at 74.7% sparsity and a 1.48x speedup for decode at 73.2% sparsity on modern GPUs. Furthermore, we explore sparsity-aware training as a natural extension, showing that models can be trained to be inherently more robust to sparse attention patterns, pushing the accuracy-sparsity frontier even further.

</details>


### [7] [Extending the Context of Pretrained LLMs by Dropping Their Positional Embeddings](https://arxiv.org/abs/2512.12167)
*Yoav Gelberg,Koshi Eguchi,Takuya Akiba,Edoardo Cetin*

Main category: cs.CL

Relevance: 85.0

TL;DR: DroPE方法通过训练后丢弃位置嵌入，实现零样本上下文扩展，无需昂贵的微调即可让语言模型处理更长的序列。


<details>
  <summary>Details</summary>
Motivation: 传统方法需要昂贵的微调才能扩展语言模型的上下文长度，这成为关键瓶颈。位置嵌入在训练时提供重要归纳偏置，但也导致模型无法泛化到未见过的序列长度，即使使用流行的位置嵌入缩放方法也是如此。

Method: 提出DroPE方法：在预训练后丢弃位置嵌入，经过短暂的重新校准阶段，让模型适应没有显式位置信息的情况。该方法简单而有效，无需长上下文微调。

Result: DroPE实现了无缝的零样本上下文扩展，在不同模型和数据集规模上都表现良好，远超先前专门架构和已建立的旋转位置嵌入缩放方法，且不损害模型在原始训练上下文中的能力。

Conclusion: 位置嵌入不是有效语言建模的内在要求，可以在预训练后安全移除。DroPE方法打破了扩展语言模型上下文长度的关键瓶颈，提供了一种简单有效的解决方案。

Abstract: So far, expensive finetuning beyond the pretraining sequence length has been a requirement for effectively extending the context of language models (LM). In this work, we break this key bottleneck by Dropping the Positional Embeddings of LMs after training (DroPE). Our simple method is motivated by three key theoretical and empirical observations. First, positional embeddings (PEs) serve a crucial role during pretraining, providing an important inductive bias that significantly facilitates convergence. Second, over-reliance on this explicit positional information is also precisely what prevents test-time generalization to sequences of unseen length, even when using popular PE-scaling methods. Third, positional embeddings are not an inherent requirement of effective language modeling and can be safely removed after pretraining, following a short recalibration phase. Empirically, DroPE yields seamless zero-shot context extension without any long-context finetuning, quickly adapting pretrained LMs without compromising their capabilities in the original training context. Our findings hold across different models and dataset sizes, far outperforming previous specialized architectures and established rotary positional embedding scaling methods.

</details>


### [8] [Market-Bench: Evaluating Large Language Models on Introductory Quantitative Trading and Market Dynamics](https://arxiv.org/abs/2512.12264)
*Abhay Srivastava,Sam Jung,Spencer Mateega*

Main category: cs.CL

Relevance: 85.0

TL;DR: MARKET-BENCH是一个评估LLMs在量化交易任务上的基准，要求模型根据自然语言策略描述构建可执行的回测器，测试其在结构化代码生成和数值准确性方面的能力。


<details>
  <summary>Details</summary>
Motivation: 现有LLMs在金融领域的应用评估主要集中在问答和文本分析，缺乏对实际可执行交易策略代码生成能力的系统评估。需要评估LLMs是否能够理解交易策略描述并生成正确、可执行的回测代码。

Method: 设计了包含三种经典交易策略的基准：微软的定时交易、可口可乐与百事的配对交易、微软的Delta对冲。评估采用多轮pass@k指标，分离结构可靠性（代码能否运行）和数值准确性（回测指标的绝对误差）。

Result: 评估了12个最先进的模型：大多数模型能可靠执行最简单策略（平均pass@3为0.80），但不同模型和任务间误差差异巨大。Gemini 3 Pro和Claude 4.5 Sonnet在简单策略上表现最佳，GPT-5.1 Codex-Max在前两个策略上达到完美pass@1，Qwen3 Max达到完美pass@3但有时产生不准确的P&L路径。

Conclusion: 当前LLMs能够搭建基本的交易基础设施，但在价格、库存和风险推理方面仍存在困难。该基准揭示了LLMs在金融代码生成任务中的能力边界。

Abstract: We introduce MARKET-BENCH, a benchmark that evaluates large language models (LLMs) on introductory quantitative trading tasks by asking them to construct executable backtesters from natural-language strategy descriptions and market assumptions. Each instance specifies one of three canonical strategies -- scheduled trading on Microsoft (NASDAQ: MSFT), pairs trading on Coca-Cola (NASDAQ: KO) and Pepsi (NASDAQ: PEP), or delta hedging on MSFT -- and models must produce code whose P\&L, drawdown, and position paths match a verifiable reference implementation. We assess twelve state-of-the-art models using a multi-round pass@k metric that separates structural reliability (whether the backtest runs) from numerical accuracy (mean absolute error of the backtest metrics). While most models reliably execute the simplest strategy (average pass@3 of 0.80), errors vary by orders of magnitude across models and tasks: Gemini 3 Pro and Claude 4.5 Sonnet combine strong reliability with low error on simpler strategies, GPT-5.1 Codex-Max achieves perfect pass@1 on the first two strategies and the lowest best-run error on the easiest task, and Qwen3 Max attains perfect pass@3 yet sometimes produces inaccurate P\&L paths. These results show that current LLMs can scaffold basic trading infrastructure but still struggle to reason robustly about prices, inventory, and risk; we release MARKET-BENCH and a public leaderboard at https://marketbench.ai.

</details>


### [9] [Large language models have learned to use language](https://arxiv.org/abs/2512.12447)
*Gary Lupyan*

Main category: cs.CL

Relevance: 85.0

TL;DR: 论文认为大型语言模型已学会使用语言，这为语言科学带来突破机遇，但需要放弃传统评估观念并承认已进入后图灵测试时代


<details>
  <summary>Details</summary>
Motivation: 认识到大型语言模型已经掌握了语言使用能力，这为语言科学研究带来了突破性的机会。然而，要实现这些突破，需要重新审视传统语言知识评估方法，并面对已进入后图灵测试时代的现实挑战

Method: 论文采用概念分析和理论反思的方法，提出需要放弃长期持有的语言评估观念，重新思考在大型语言模型时代如何评估语言知识和能力

Result: 论文论证了大型语言模型已具备真正的语言使用能力，传统评估方法已不再适用，需要建立新的评估框架来理解模型的语言能力

Conclusion: 我们已进入后图灵测试时代，需要重新定义语言评估标准，放弃传统观念，以充分利用大型语言模型为语言科学带来的突破性机遇

Abstract: Acknowledging that large language models have learned to use language can open doors to breakthrough language science. Achieving these breakthroughs may require abandoning some long-held ideas about how language knowledge is evaluated and reckoning with the difficult fact that we have entered a post-Turing test era.

</details>


### [10] [Coupled Variational Reinforcement Learning for Language Model General Reasoning](https://arxiv.org/abs/2512.12576)
*Xueru Wen,Jie Lou,Yanjiang Liu,Hongyu Lin,Ben He,Xianpei Han,Le Sun,Yaojie Lu,Debing Zhang*

Main category: cs.CL

Relevance: 85.0

TL;DR: CoVRL提出了一种结合变分推断和强化学习的方法，通过耦合先验和后验分布来改进语言模型的推理能力，无需可验证奖励信号。


<details>
  <summary>Details</summary>
Motivation: 现有无需验证器的强化学习方法通常仅基于问题采样推理轨迹，导致推理轨迹与最终答案分离，探索效率低且缺乏一致性。需要一种能保持思维-答案一致性的更高效方法。

Method: CoVRL通过混合采样策略耦合先验和后验分布，构建并优化一个复合分布，将变分推断与强化学习结合，实现高效探索同时保持思维-答案一致性。

Result: 在数学和通用推理基准测试中，CoVRL相比基础模型性能提升12.4%，比当前最先进的无需验证器强化学习方法额外提升2.3%。

Conclusion: CoVRL为增强语言模型通用推理能力提供了一个原则性框架，通过耦合变分推断和强化学习解决了现有方法在探索效率和一致性方面的局限性。

Abstract: While reinforcement learning have achieved impressive progress in language model reasoning, they are constrained by the requirement for verifiable rewards. Recent verifier-free RL methods address this limitation by utilizing the intrinsic probabilities of LLMs generating reference answers as reward signals. However, these approaches typically sample reasoning traces conditioned only on the question. This design decouples reasoning-trace sampling from answer information, leading to inefficient exploration and incoherence between traces and final answers. In this paper, we propose \textit{\b{Co}upled \b{V}ariational \b{R}einforcement \b{L}earning} (CoVRL), which bridges variational inference and reinforcement learning by coupling prior and posterior distributions through a hybrid sampling strategy. By constructing and optimizing a composite distribution that integrates these two distributions, CoVRL enables efficient exploration while preserving strong thought-answer coherence. Extensive experiments on mathematical and general reasoning benchmarks show that CoVRL improves performance by 12.4\% over the base model and achieves an additional 2.3\% improvement over strong state-of-the-art verifier-free RL baselines, providing a principled framework for enhancing the general reasoning capabilities of language models.

</details>


### [11] [Human-Inspired Learning for Large Language Models via Obvious Record and Maximum-Entropy Method Discovery](https://arxiv.org/abs/2512.12608)
*Hong Su*

Main category: cs.CL

Relevance: 85.0

TL;DR: 提出一种受人类学习启发的框架，通过显式记忆存储和最大熵方法发现机制，解决LLMs在罕见场景下的学习问题


<details>
  <summary>Details</summary>
Motivation: LLMs在处理罕见、低资源或未见过的场景时表现不佳，因为这些情况在训练数据中稀疏。LLMs主要依赖隐式参数记忆，限制了它们显式获取、回忆和精炼方法的能力，导致它们更像是直觉驱动的预测器而非方法导向的学习者。

Method: 提出人类启发式学习框架，包含两个互补机制：1) Obvious Record：显式存储因果（或问题-解决方案）关系作为符号记忆；2) Maximum-Entropy Method Discovery：优先保留语义差异度高的方法，捕捉多样化和被忽视的策略。

Result: 在包含60个语义多样化问题-解决方案对的基准测试中，提出的熵引导方法相比随机基线，在未见问题的覆盖率和内部多样性方面表现显著更好，证实了其在发现更通用、更受人类启发的方法方面的有效性。

Conclusion: 通过结合显式符号记忆和最大熵方法发现，可以增强LLMs处理罕见场景的能力，使其更像方法导向的学习者而非仅仅是直觉驱动的预测器。

Abstract: Large Language Models (LLMs) excel at extracting common patterns from large-scale corpora, yet they struggle with rare, low-resource, or previously unseen scenarios-such as niche hardware deployment issues or irregular IoT device behaviors-because such cases are sparsely represented in training data. Moreover, LLMs rely primarily on implicit parametric memory, which limits their ability to explicitly acquire, recall, and refine methods, causing them to behave predominantly as intuition-driven predictors rather than deliberate, method-oriented learners. Inspired by how humans learn from rare experiences, this paper proposes a human-inspired learning framework that integrates two complementary mechanisms. The first, Obvious Record, explicitly stores cause--result (or question--solution) relationships as symbolic memory, enabling persistent learning even from single or infrequent encounters. The second, Maximum-Entropy Method Discovery, prioritizes and preserves methods with high semantic dissimilarity, allowing the system to capture diverse and underrepresented strategies that are typically overlooked by next-token prediction. Verification on a benchmark of 60 semantically diverse question--solution pairs demonstrates that the proposed entropy-guided approach achieves stronger coverage of unseen questions and significantly greater internal diversity than a random baseline, confirming its effectiveness in discovering more generalizable and human-inspired methods.

</details>


### [12] [Fine-Tuning Causal LLMs for Text Classification: Embedding-Based vs. Instruction-Based Approaches](https://arxiv.org/abs/2512.12677)
*Amirhossein Yousefiramandi,Ciaran Cooney*

Main category: cs.CL

Relevance: 85.0

TL;DR: 本文探索在资源受限条件下微调仅解码器LLM进行文本分类的高效策略，比较了嵌入分类头微调和指令微调两种方法，发现嵌入方法在F1分数上显著优于指令微调方法，且能与领域特定模型竞争。


<details>
  <summary>Details</summary>
Motivation: 在计算资源受限的情况下，如何高效微调大型语言模型进行下游文本分类任务，比较不同微调策略的性能差异，为实际应用提供指导。

Method: 研究两种微调策略：1）在预训练因果LLM上附加分类头，使用最终token嵌入作为序列表示进行微调；2）以提示-响应格式对LLM进行指令微调进行分类。采用4位模型量化和LoRA技术实现单GPU上8B参数模型的微调。

Result: 在专有单标签数据集和WIPO-Alpha专利数据集（极端多标签分类）上的实验表明，嵌入方法在F1分数上显著优于指令微调方法，且与微调的领域特定模型（如BERT）竞争甚至超越。

Conclusion: 直接利用因果LLM的内部表示结合高效微调技术，在有限计算资源下可获得出色的分类性能，为LLM在分类场景中的优化提供了实用指南。

Abstract: We explore efficient strategies to fine-tune decoder-only Large Language Models (LLMs) for downstream text classification under resource constraints. Two approaches are investigated: (1) attaching a classification head to a pre-trained causal LLM and fine-tuning on the task (using the LLM's final token embedding as a sequence representation), and (2) instruction-tuning the LLM in a prompt->response format for classification. To enable single-GPU fine-tuning of models up to 8B parameters, we combine 4-bit model quantization with Low-Rank Adaptation (LoRA) for parameter-efficient training. Experiments on two datasets - a proprietary single-label dataset and the public WIPO-Alpha patent dataset (extreme multi-label classification) - show that the embedding-based method significantly outperforms the instruction-tuned method in F1-score, and is very competitive with - even surpassing - fine-tuned domain-specific models (e.g. BERT) on the same tasks. These results demonstrate that directly leveraging the internal representations of causal LLMs, along with efficient fine-tuning techniques, yields impressive classification performance under limited computational resources. We discuss the advantages of each approach while outlining practical guidelines and future directions for optimizing LLM fine-tuning in classification scenarios.

</details>


### [13] [CoDA: A Context-Decoupled Hierarchical Agent with Reinforcement Learning](https://arxiv.org/abs/2512.12716)
*Xuanzhang Liu,Jianglun Feng,Zhuoran Zhuang,Junzhe Zhao,Maofei Que,Jieting Li,Dianlei Wang,Hao Tong,Ye Chen,Pan Li*

Main category: cs.CL

Relevance: 85.0

TL;DR: CoDA：一种上下文解耦的分层智能体框架，通过将高层规划与底层执行分离，使用单一共享LLM骨干网络在不同上下文隔离角色中操作，有效解决LLM智能体中的"上下文爆炸"问题。


<details>
  <summary>Details</summary>
Motivation: 解决LLM智能体在强化学习训练中面临的"上下文爆炸"问题——长文本输出的积累会淹没模型的上下文窗口，导致推理失败。现有方法在处理复杂多步任务时性能受限。

Method: 提出CoDA（上下文解耦分层智能体）框架：1）使用单一共享LLM骨干网络在两个上下文隔离角色中操作：高层规划器（在简洁战略上下文中分解任务）和底层执行器（在临时隔离工作空间中处理工具交互）；2）采用PECO（规划器-执行器协同优化）强化学习方法，应用轨迹级奖励联合优化两个角色，通过上下文依赖的策略更新促进无缝协作。

Result: 在复杂多跳问答基准测试中显著优于最先进基线方法；在长上下文场景中表现出强大鲁棒性，保持稳定性能而其他基线方法性能严重下降，验证了分层设计在缓解上下文过载方面的有效性。

Conclusion: CoDA通过上下文解耦的分层设计有效解决了LLM智能体的上下文爆炸问题，实现了高层规划与底层执行的协同优化，为复杂多步任务的解决提供了有效框架。

Abstract: Large Language Model (LLM) agents trained with reinforcement learning (RL) show great promise for solving complex, multi-step tasks. However, their performance is often crippled by "Context Explosion", where the accumulation of long text outputs overwhelms the model's context window and leads to reasoning failures. To address this, we introduce CoDA, a Context-Decoupled hierarchical Agent, a simple but effective reinforcement learning framework that decouples high-level planning from low-level execution. It employs a single, shared LLM backbone that learns to operate in two distinct, contextually isolated roles: a high-level Planner that decomposes tasks within a concise strategic context, and a low-level Executor that handles tool interactions in an ephemeral, isolated workspace. We train this unified agent end-to-end using PECO (Planner-Executor Co-Optimization), a reinforcement learning methodology that applies a trajectory-level reward to jointly optimize both roles, fostering seamless collaboration through context-dependent policy updates. Extensive experiments demonstrate that CoDA achieves significant performance improvements over state-of-the-art baselines on complex multi-hop question-answering benchmarks, and it exhibits strong robustness in long-context scenarios, maintaining stable performance while all other baselines suffer severe degradation, thus further validating the effectiveness of our hierarchical design in mitigating context overload.

</details>


### [14] [NL2Repo-Bench: Towards Long-Horizon Repository Generation Evaluation of Coding Agents](https://arxiv.org/abs/2512.12730)
*Jingzhe Ding,Shengda Long,Changxin Pu,Huan Zhou,Hongwan Gao,Xiang Gao,Chao He,Yue Hou,Fei Hu,Zhaojian Li,Weiran Shi,Zaiyuan Wang,Daoguang Zan,Chenchen Zhang,Xiaoxu Zhang,Qizhi Chen,Xianfu Cheng,Bo Deng,Qingshui Gu,Kai Hua,Juntao Lin,Pai Liu,Mingchen Li,Xuanguang Pan,Zifan Peng,Yujia Qin,Yong Shan,Zhewen Tan,Weihao Xie,Zihan Wang,Yishuo Yuan,Jiayu Zhang,Enduo Zhao,Yunfei Zhao,He Zhu,Chenyang Zou,Ming Ding,Jianpeng Jiao,Jiaheng Liu,Minghao Liu,Qian Liu,Chongyao Tao,Jian Yang,Tong Yang,Zhaoxiang Zhang,Xinjie Chen,Wenhao Huang,Ge Zhang*

Main category: cs.CL

Relevance: 85.0

TL;DR: NL2Repo Bench：一个专门评估编码智能体长时程仓库生成能力的基准测试，要求智能体仅根据自然语言需求文档构建完整可安装的Python库，揭示当前智能体在长时程推理方面的严重不足。


<details>
  <summary>Details</summary>
Motivation: 现有编码智能体基准测试主要关注局部代码生成、脚手架补全或短期修复任务，缺乏对构建完整软件系统所需的长时程能力的严格评估。需要填补这一空白，评估智能体在真实世界仓库构建中所需的持续推理、规划和执行能力。

Method: 提出NL2Repo Bench基准测试，要求编码智能体仅基于单一自然语言需求文档和空工作空间，自主设计架构、管理依赖、实现多模块逻辑，最终生成完全可安装的Python库。通过实验评估最先进的开放和闭源模型。

Result: 实验表明长时程仓库生成能力仍远未解决：即使最强的智能体平均测试通过率也低于40%，很少能正确完成整个仓库。分析揭示了长时程失败的根本模式：过早终止、全局一致性丢失、脆弱的跨文件依赖关系，以及在数百个交互步骤中规划不足。

Conclusion: NL2Repo Bench为衡量持续智能体能力建立了严格可验证的测试平台，并指出长时程推理是下一代自主编码智能体的核心瓶颈。该基准测试强调了在真实世界软件开发中评估长时程能力的重要性。

Abstract: Recent advances in coding agents suggest rapid progress toward autonomous software development, yet existing benchmarks fail to rigorously evaluate the long-horizon capabilities required to build complete software systems. Most prior evaluations focus on localized code generation, scaffolded completion, or short-term repair tasks, leaving open the question of whether agents can sustain coherent reasoning, planning, and execution over the extended horizons demanded by real-world repository construction. To address this gap, we present NL2Repo Bench, a benchmark explicitly designed to evaluate the long-horizon repository generation ability of coding agents. Given only a single natural-language requirements document and an empty workspace, agents must autonomously design the architecture, manage dependencies, implement multi-module logic, and produce a fully installable Python library. Our experiments across state-of-the-art open- and closed-source models reveal that long-horizon repository generation remains largely unsolved: even the strongest agents achieve below 40% average test pass rates and rarely complete an entire repository correctly. Detailed analysis uncovers fundamental long-horizon failure modes, including premature termination, loss of global coherence, fragile cross-file dependencies, and inadequate planning over hundreds of interaction steps. NL2Repo Bench establishes a rigorous, verifiable testbed for measuring sustained agentic competence and highlights long-horizon reasoning as a central bottleneck for the next generation of autonomous coding agents.

</details>


### [15] [Curió-Edu 7B: Examining Data Selection Impacts in LLM Continued Pretraining](https://arxiv.org/abs/2512.12770)
*Thales Sales Almeida,Rodrigo Nogueira,Hélio Pedrini*

Main category: cs.CL

Relevance: 85.0

TL;DR: Curió 7B是基於LLaMA-2的葡萄牙語持續預訓練模型，使用100B tokens。其變體Curió-Edu 7B僅使用教育/STEM領域的10B tokens，卻在評估中表現更好，顯示數據質量比數量更重要。


<details>
  <summary>Details</summary>
Motivation: 研究持續預訓練中數據質量與數量的權衡，探討在特定語言適應中，是否僅靠大量數據就足夠，還是數據質量起決定性作用。

Method: 從LLaMA-2出發，進行葡萄牙語持續預訓練：1) Curió 7B使用100B葡萄牙語tokens；2) Curió-Edu 7B僅使用教育/STEM領域的10B tokens。比較兩者在語言適應任務上的表現。

Result: Curió-Edu 7B僅使用10%數據和20%計算資源，卻在多項評估中超越使用完整語料的Curió 7B，證明數據選擇在語言適應中的關鍵作用。

Conclusion: 在持續預訓練中，數據質量比數量更重要。針對性選擇高質量領域數據(如教育/STEM)能更有效地適應目標語言，即使模型先前對該語言接觸有限。

Abstract: Continued pretraining extends a language model's capabilities by further exposing it to additional data, often tailored to a specific linguistic or domain context. This strategy has emerged as an efficient alternative to full retraining when adapting general-purpose models to new settings. In this work, we investigate this paradigm through Curió 7B, a 7-billion-parameter model derived from LLaMA-2 and trained on 100 billion Portuguese tokens from the ClassiCC-PT corpus - the most extensive Portuguese-specific continued-pretraining effort above the three-billion-parameter scale to date. Beyond scale, we investigate whether quantity alone suffices or whether data quality plays a decisive role in linguistic adaptation. To this end, we introduce Curió-Edu 7B, a variant trained exclusively on the educational and STEM-filtered subset of the same corpus, totaling just 10 billion tokens. Despite using only 10% of the data and 20% of the computation, Curió-Edu 7B surpasses the full-corpus model in our evaluations, demonstrating that data selection can be fundamental even when adapting models with limited prior exposure to the target language. The developed models are available at https://huggingface.co/collections/ClassiCC-Corpus/curio-edu

</details>


### [16] [Persistent Personas? Role-Playing, Instruction Following, and Safety in Extended Interactions](https://arxiv.org/abs/2512.12775)
*Pedro Henrique Luz de Araujo,Michael A. Hedderich,Ali Modarressi,Hinrich Schuetze,Benjamin Roth*

Main category: cs.CL

Relevance: 85.0

TL;DR: 该论文提出了一个评估协议，结合长角色对话（超过100轮）和评估数据集，创建对话条件基准，以稳健测量长上下文效应。研究发现角色保真度在对话过程中会退化，特别是在目标导向对话中，模型需要在保持角色保真度和指令遵循之间权衡。


<details>
  <summary>Details</summary>
Motivation: 角色分配的大型语言模型在教育、医疗和社会人口模拟等领域得到应用，但通常只在短单轮设置中评估，不能反映真实世界使用情况。需要评估长对话中角色保真度的持续性。

Method: 引入评估协议：结合长角色对话（超过100轮）和评估数据集，创建对话条件基准。研究了7个最先进的开放和封闭权重LLM在对话长度对角色保真度、指令遵循和安全性的影响。

Result: 发现角色保真度在对话过程中会退化，特别是在目标导向对话中。识别出保真度和指令遵循之间的权衡：非角色基线最初优于角色分配模型；随着对话进展和保真度衰减，角色响应变得与基线响应越来越相似。

Conclusion: 研究结果突显了角色应用在扩展交互中的脆弱性，提供了系统测量此类失败的协议。对LLM在长对话中的角色一致性提出了重要警示。

Abstract: Persona-assigned large language models (LLMs) are used in domains such as education, healthcare, and sociodemographic simulation. Yet, they are typically evaluated only in short, single-round settings that do not reflect real-world usage. We introduce an evaluation protocol that combines long persona dialogues (over 100 rounds) and evaluation datasets to create dialogue-conditioned benchmarks that can robustly measure long-context effects. We then investigate the effects of dialogue length on persona fidelity, instruction-following, and safety of seven state-of-the-art open- and closed-weight LLMs. We find that persona fidelity degrades over the course of dialogues, especially in goal-oriented conversations, where models must sustain both persona fidelity and instruction following. We identify a trade-off between fidelity and instruction following, with non-persona baselines initially outperforming persona-assigned models; as dialogues progress and fidelity fades, persona responses become increasingly similar to baseline responses. Our findings highlight the fragility of persona applications in extended interactions and our work provides a protocol to systematically measure such failures.

</details>


### [17] [State over Tokens: Characterizing the Role of Reasoning Tokens](https://arxiv.org/abs/2512.12777)
*Mosh Levy,Zohar Elyoseph,Shauli Ravfogel,Yoav Goldberg*

Main category: cs.CL

Relevance: 85.0

TL;DR: 论文提出"State over Tokens"(SoT)框架，将LLM的推理标记重新概念化为外部化的计算状态而非语言叙述，解释了为何这些标记能驱动正确推理却非忠实解释。


<details>
  <summary>Details</summary>
Motivation: 现有LLM生成的推理标记看似人类思考过程，但实证表明它们并非模型实际推理过程的忠实解释。需要解决这种表象与功能之间的差距，以真正理解LLM的内部工作过程。

Method: 引入SoT概念框架，将推理标记重新定义为外部化的计算状态——模型无状态生成周期中唯一持久的信息载体。这解释了标记如何在不作为忠实文本解释的情况下驱动正确推理。

Result: SoT框架揭示了先前被忽视的研究问题，解释了推理标记的功能机制，并指出需要超越将标记作为文本阅读的传统方法。

Conclusion: 要真正理解LLM的工作过程，研究必须超越将推理标记作为文本阅读的层面，专注于将其解码为状态信息。这为LLM可解释性研究提供了新方向。

Abstract: Large Language Models (LLMs) can generate reasoning tokens before their final answer to boost performance on complex tasks. While these sequences seem like human thought processes, empirical evidence reveals that they are not a faithful explanation of the model's actual reasoning process. To address this gap between appearance and function, we introduce the State over Tokens (SoT) conceptual framework. SoT reframes reasoning tokens not as a linguistic narrative, but as an externalized computational state -- the sole persistent information carrier across the model's stateless generation cycles. This explains how the tokens can drive correct reasoning without being a faithful explanation when read as text and surfaces previously overlooked research questions on these tokens. We argue that to truly understand the process that LLMs do, research must move beyond reading the reasoning tokens as text and focus on decoding them as state.

</details>


### [18] [Does Tone Change the Answer? Evaluating Prompt Politeness Effects on Modern LLMs: GPT, Gemini, LLaMA](https://arxiv.org/abs/2512.12812)
*Hanyu Cai,Binqi Shen,Lier Jin,Lan Hu,Xiaojing Fan*

Main category: cs.CL

Relevance: 85.0

TL;DR: 本文系统评估了提示语气（友好、中性、粗鲁）对三种主流LLM（GPT-4o mini、Gemini 2.0 Flash、Llama 4 Scout）在MMMLU基准测试中表现的影响，发现语气敏感性因模型和领域而异，但现代LLM总体上对语气变化具有鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 尽管提示工程对LLM性能至关重要，但语言语气和礼貌等语用元素的影响尚未得到充分探索，特别是在不同模型家族之间。本研究旨在系统评估交互语气如何影响模型准确性。

Method: 提出系统评估框架，在MMMLU基准测试中使用三种语气变体（非常友好、中性、非常粗鲁）评估三种最新LLM（GPT-4o mini、Gemini 2.0 Flash、Llama 4 Scout）在STEM和人文领域六个任务中的表现，并进行统计显著性检验。

Result: 语气敏感性具有模型依赖性和领域特异性：中性或友好提示通常比粗鲁提示获得更高准确率，但统计显著效应仅出现在部分人文任务中；Gemini相对语气不敏感；跨领域聚合后语气效应减弱且大多失去统计显著性。

Conclusion: 虽然交互语气在特定解释性场景中可能重要，但现代LLM在典型的混合领域使用中对语气变化具有广泛鲁棒性，为实际部署中的提示设计和模型选择提供实用指导。

Abstract: Prompt engineering has emerged as a critical factor influencing large language model (LLM) performance, yet the impact of pragmatic elements such as linguistic tone and politeness remains underexplored, particularly across different model families. In this work, we propose a systematic evaluation framework to examine how interaction tone affects model accuracy and apply it to three recently released and widely available LLMs: GPT-4o mini (OpenAI), Gemini 2.0 Flash (Google DeepMind), and Llama 4 Scout (Meta). Using the MMMLU benchmark, we evaluate model performance under Very Friendly, Neutral, and Very Rude prompt variants across six tasks spanning STEM and Humanities domains, and analyze pairwise accuracy differences with statistical significance testing.
  Our results show that tone sensitivity is both model-dependent and domain-specific. Neutral or Very Friendly prompts generally yield higher accuracy than Very Rude prompts, but statistically significant effects appear only in a subset of Humanities tasks, where rude tone reduces accuracy for GPT and Llama, while Gemini remains comparatively tone-insensitive. When performance is aggregated across tasks within each domain, tone effects diminish and largely lose statistical significance. Compared with earlier researches, these findings suggest that dataset scale and coverage materially influence the detection of tone effects. Overall, our study indicates that while interaction tone can matter in specific interpretive settings, modern LLMs are broadly robust to tonal variation in typical mixed-domain use, providing practical guidance for prompt design and model selection in real-world deployments.

</details>


### [19] [Hindsight is 20/20: Building Agent Memory that Retains, Recalls, and Reflects](https://arxiv.org/abs/2512.12818)
*Chris Latimer,Nicoló Boschi,Andrew Neeser,Chris Bartholomew,Gaurav Srivastava,Xuan Wang,Naren Ramakrishnan*

Main category: cs.CL

Relevance: 85.0

TL;DR: Hindsight是一个结构化记忆架构，将智能体记忆组织为四个逻辑网络，支持保留、回忆和反思操作，显著提升长时对话记忆性能


<details>
  <summary>Details</summary>
Motivation: 现有智能体记忆系统将记忆作为外部层处理，存在证据与推理界限模糊、长时信息组织困难、推理解释支持有限等问题，需要更结构化的记忆架构

Method: 提出Hindsight架构，将记忆组织为四个逻辑网络：世界事实、智能体经验、合成实体摘要和演化信念。支持保留、回忆和反思三个核心操作，通过时间感知记忆层和反思层实现结构化记忆处理

Result: 在LongMemEval和LoCoMo基准测试中，使用20B开源模型的Hindsight将准确率从39%提升至83.6%，优于全上下文GPT-4o。进一步扩展骨干网络后，在LongMemEval达到91.4%，LoCoMo达到89.61%，显著超越现有记忆架构

Conclusion: Hindsight通过结构化记忆架构有效解决了智能体记忆的关键挑战，为LLM应用提供了可扩展的记忆系统，支持长时对话和复杂推理任务

Abstract: Agent memory has been touted as a dimension of growth for LLM-based applications, enabling agents that can accumulate experience, adapt across sessions, and move beyond single-shot question answering. The current generation of agent memory systems treats memory as an external layer that extracts salient snippets from conversations, stores them in vector or graph-based stores, and retrieves top-k items into the prompt of an otherwise stateless model. While these systems improve personalization and context carry-over, they still blur the line between evidence and inference, struggle to organize information over long horizons, and offer limited support for agents that must explain their reasoning. We present Hindsight, a memory architecture that treats agent memory as a structured, first-class substrate for reasoning by organizing it into four logical networks that distinguish world facts, agent experiences, synthesized entity summaries, and evolving beliefs. This framework supports three core operations -- retain, recall, and reflect -- that govern how information is added, accessed, and updated. Under this abstraction, a temporal, entity aware memory layer incrementally turns conversational streams into a structured, queryable memory bank, while a reflection layer reasons over this bank to produce answers and to update information in a traceable way. On key long-horizon conversational memory benchmarks like LongMemEval and LoCoMo, Hindsight with an open-source 20B model lifts overall accuracy from 39% to 83.6% over a full-context baseline with the same backbone and outperforms full context GPT-4o. Scaling the backbone further pushes Hindsight to 91.4% on LongMemEval and up to 89.61% on LoCoMo (vs. 75.78% for the strongest prior open system), consistently outperforming existing memory architectures on multi-session and open-domain questions.

</details>


### [20] [Counting Clues: A Lightweight Probabilistic Baseline Can Match an LLM](https://arxiv.org/abs/2512.12868)
*Furong Jia,Yuan Pu,Finn Guo,Monica Agrawal*

Main category: cs.CL

Relevance: 85.0

TL;DR: 论文研究了LLMs在临床诊断基准测试中的表现是否反映概率推理能力，发现基于频率的简单概率排序器能达到与LLMs相当的性能，但两者正确回答的问题重叠度很低，表明LLMs并非简单频率聚合机制。


<details>
  <summary>Details</summary>
Motivation: 研究动机是探究LLMs在临床诊断基准测试（如MedQA）中的优异表现是否真正反映了概率推理能力，还是仅仅基于训练数据中的概念-诊断共现频率统计。

Method: 提出了频率基础概率排序器（FBPR），使用平滑朴素贝叶斯方法，基于大规模语料库中的概念-诊断共现统计对选项进行评分。从OLMo和Llama的预训练语料中提取共现统计，并与相应LLMs的性能进行比较。

Result: FBPR在相同语料上能达到与对应LLMs相当的性能水平。但LLM直接推理和FBPR正确回答的问题重叠度仅略高于随机水平，表明两者具有互补优势。

Conclusion: LLMs的性能似乎不是由简单的频率聚合机制驱动，但类似历史上基于规则的专家系统的低复杂度方法仍能解释基准测试性能的相当大部分。显式概率基线仍有重要价值，可作为性能参考点和潜在混合方法的补充信号。

Abstract: Large language models (LLMs) excel on multiple-choice clinical diagnosis benchmarks, yet it is unclear how much of this performance reflects underlying probabilistic reasoning. We study this through questions from MedQA, where the task is to select the most likely diagnosis. We introduce the Frequency-Based Probabilistic Ranker (FBPR), a lightweight method that scores options with a smoothed Naive Bayes over concept-diagnosis co-occurrence statistics from a large corpus. When co-occurrence statistics were sourced from the pretraining corpora for OLMo and Llama, FBPR achieves comparable performance to the corresponding LLMs pretrained on that same corpus. Direct LLM inference and FBPR largely get different questions correct, with an overlap only slightly above random chance, indicating complementary strengths of each method. These findings highlight the continued value of explicit probabilistic baselines: they provide a meaningful performance reference point and a complementary signal for potential hybridization. While the performance of LLMs seems to be driven by a mechanism other than simple frequency aggregation, we show that an approach similar to the historically grounded, low-complexity expert systems still accounts for a substantial portion of benchmark performance.

</details>


### [21] [QwenLong-L1.5: Post-Training Recipe for Long-Context Reasoning and Memory Management](https://arxiv.org/abs/2512.12967)
*Weizhou Shen,Ziyi Yang,Chenliang Li,Zhiyuan Lu,Miao Peng,Huashan Sun,Yingcheng Shi,Shengyi Liao,Shaopeng Lai,Bo Zhang,Dayiheng Liu,Fei Huang,Jingren Zhou,Ming Yan*

Main category: cs.CL

Relevance: 85.0

TL;DR: QwenLong-L1.5通过系统化的后训练创新实现卓越的长上下文推理能力，包括长上下文数据合成管道、稳定的强化学习训练和内存增强架构，在超长任务上表现优异。


<details>
  <summary>Details</summary>
Motivation: 现有模型在处理超长上下文（百万token级别）时面临挑战，需要解决长距离推理、训练不稳定性和内存管理问题，以实现真正的长上下文理解能力。

Method: 1) 长上下文数据合成管道：将文档分解为原子事实和关系，程序化生成可验证的推理问题；2) 稳定化强化学习：任务平衡采样和自适应熵控制策略优化；3) 内存增强架构：多阶段融合RL训练，集成单次推理和迭代内存处理。

Result: 在长上下文推理基准测试中性能与GPT-5和Gemini-2.5-Pro相当，比基线平均提升9.90分；在超长任务（1M~4M tokens）上比代理基线提升9.48分；在科学推理、记忆工具使用和扩展对话等通用领域也有提升。

Conclusion: QwenLong-L1.5通过系统化的后训练方法成功解决了长上下文推理的关键挑战，实现了在超长序列上的有效推理能力，并将这些能力迁移到通用领域。

Abstract: We introduce QwenLong-L1.5, a model that achieves superior long-context reasoning capabilities through systematic post-training innovations. The key technical breakthroughs of QwenLong-L1.5 are as follows: (1) Long-Context Data Synthesis Pipeline: We develop a systematic synthesis framework that generates challenging reasoning tasks requiring multi-hop grounding over globally distributed evidence. By deconstructing documents into atomic facts and their underlying relationships, and then programmatically composing verifiable reasoning questions, our approach creates high-quality training data at scale, moving substantially beyond simple retrieval tasks to enable genuine long-range reasoning capabilities. (2) Stabilized Reinforcement Learning for Long-Context Training: To overcome the critical instability in long-context RL, we introduce task-balanced sampling with task-specific advantage estimation to mitigate reward bias, and propose Adaptive Entropy-Controlled Policy Optimization (AEPO) that dynamically regulates exploration-exploitation trade-offs. (3) Memory-Augmented Architecture for Ultra-Long Contexts: Recognizing that even extended context windows cannot accommodate arbitrarily long sequences, we develop a memory management framework with multi-stage fusion RL training that seamlessly integrates single-pass reasoning with iterative memory-based processing for tasks exceeding 4M tokens. Based on Qwen3-30B-A3B-Thinking, QwenLong-L1.5 achieves performance comparable to GPT-5 and Gemini-2.5-Pro on long-context reasoning benchmarks, surpassing its baseline by 9.90 points on average. On ultra-long tasks (1M~4M tokens), QwenLong-L1.5's memory-agent framework yields a 9.48-point gain over the agent baseline. Additionally, the acquired long-context reasoning ability translates to enhanced performance in general domains like scientific reasoning, memory tool using, and extended dialogue.

</details>


### [22] [An Open and Reproducible Deep Research Agent for Long-Form Question Answering](https://arxiv.org/abs/2512.13059)
*Ikuya Yamada,Wataru Ikeda,Ko Yoshida,Mengyu Ye,Hinata Sugimoto,Masatoshi Suzuki,Hisanori Ozaki,Jun Suzuki*

Main category: cs.CL

Relevance: 85.0

TL;DR: 提出一个用于长格式问答的开放深度研究系统，结合开源LLM与开放网络搜索API，通过迭代检索、推理和合成，并应用基于LLM作为评判者的偏好调优来提升回答质量。


<details>
  <summary>Details</summary>
Motivation: 解决现实世界开放域设置中的长格式问答问题，特别是在MMU-RAG竞赛的文本到文本赛道中，需要开发能够进行高质量推理和事实准确回答的系统。

Method: 结合开源大语言模型与开放网络搜索API，采用迭代检索、推理和合成方法。应用基于LLM作为评判者的偏好调优，评估清晰度、洞察力和事实性等多方面质量。

Result: 实验结果显示，所提方法在所有三个方面（清晰度、洞察力和事实性）都一致提高了回答质量，并在NeurIPS 2025的MMU-RAG竞赛文本到文本赛道中获胜。

Conclusion: 该系统展示了在开放域长格式问答中结合检索增强生成与基于LLM评判的偏好调优的有效性，为现实世界应用提供了可行的解决方案。

Abstract: We present an open deep research system for long-form question answering, selected as a winning system in the text-to-text track of the MMU-RAG competition at NeurIPS 2025. The system combines an open-source large language model (LLM) with an open web search API to perform iterative retrieval, reasoning, and synthesis in real-world open-domain settings. To enhance reasoning quality, we apply preference tuning based on LLM-as-a-judge feedback that evaluates multiple aspects, including clarity, insightfulness, and factuality. Our experimental results show that the proposed method consistently improves answer quality across all three aspects. Our source code is publicly available at https://github.com/efficient-deep-research/efficient-deep-research.

</details>


### [23] [LLM Rationalis? Measuring Bargaining Capabilities of AI Negotiators](https://arxiv.org/abs/2512.13063)
*Cheril Shah,Akshit Agarwal,Kanak Garg,Mourad Heddaya*

Main category: cs.CL

Relevance: 85.0

TL;DR: 论文提出双曲线框架量化谈判让步动态，比较人类与LLM谈判表现，发现LLM缺乏情境适应性和对手推理能力


<details>
  <summary>Details</summary>
Motivation: 双边谈判是复杂的情境敏感任务，人类谈判者会动态调整锚点、节奏和灵活性来利用权力不对称和非正式线索。当前需要理解LLM在谈判任务中的能力局限，特别是与人类谈判策略的差异。

Method: 1) 提出基于双曲正切曲线的统一数学框架建模让步动态；2) 定义两个指标：爆发性tau和让步刚性指数(CRI)量化报价轨迹；3) 大规模实证比较人类谈判者与4个SOTA LLM；4) 实验设置包括自然语言和数字报价、有无丰富市场情境、6种控制权力不对称场景。

Result: 1) LLM系统性锚定在可能协议区的极端位置；2) LLM优化固定点，不考虑杠杆或情境；3) LLM策略多样性有限，偶尔使用欺骗性策略；4) LLM谈判能力不随模型改进而提升；5) 与人类不同，LLM无法平滑适应情境或推断对手立场和策略。

Conclusion: 当前LLM谈判能力存在根本性局限，需要开发能更好内化对手推理和情境依赖策略的模型。

Abstract: Bilateral negotiation is a complex, context-sensitive task in which human negotiators dynamically adjust anchors, pacing, and flexibility to exploit power asymmetries and informal cues. We introduce a unified mathematical framework for modeling concession dynamics based on a hyperbolic tangent curve, and propose two metrics burstiness tau and the Concession-Rigidity Index (CRI) to quantify the timing and rigidity of offer trajectories. We conduct a large-scale empirical comparison between human negotiators and four state-of-the-art large language models (LLMs) across natural-language and numeric-offers settings, with and without rich market context, as well as six controlled power-asymmetry scenarios. Our results reveal that, unlike humans who smoothly adapt to situations and infer the opponents position and strategies, LLMs systematically anchor at extremes of the possible agreement zone for negotiations and optimize for fixed points irrespective of leverage or context. Qualitative analysis further shows limited strategy diversity and occasional deceptive tactics used by LLMs. Moreover the ability of LLMs to negotiate does not improve with better models. These findings highlight fundamental limitations in current LLM negotiation capabilities and point to the need for models that better internalize opponent reasoning and context-dependent strategy.

</details>


### [24] [Uncovering the Role of Initial Saliency in U-Shaped Attention Bias: Scaling Initial Token Weight for Enhanced Long-Text Processing](https://arxiv.org/abs/2512.13109)
*Zewen Qiang,Sendong Zhao,Haochun Wang,Bing Qin,Ting Liu*

Main category: cs.CL

Relevance: 85.0

TL;DR: 该论文发现LLMs处理长文本时存在"中间丢失"问题，除了已知的位置编码偏差外，首次识别出初始显著性这一新因素，并提出通过缩放初始token与其他token的注意力权重来改善长上下文处理能力。


<details>
  <summary>Details</summary>
Motivation: LLMs在长文本序列上表现不佳，存在"中间丢失"现象，这源于U形注意力偏差（注意力过度集中在文本开头和结尾）。虽然先前研究将此归因于位置编码，但本文旨在探索其他潜在因素。

Method: 首先识别出初始显著性因素：在注意力计算中，相对于初始token具有更高注意力权重的token在预测下一个token时会获得更多关注。基于此发现，提出通过缩放初始token与其他token之间的注意力权重来改善模型的长上下文处理能力。

Result: 在MDQA数据集上实现了最大3.6%的性能提升，在KV-Retrieval任务中，将新方法与现有减少位置编码偏差的方法结合，实现了最大3.4%的改进。

Conclusion: 除了位置编码偏差外，初始显著性也是导致LLMs长文本处理中"中间丢失"现象的重要因素。通过调整初始token的注意力权重可以显著改善模型的长上下文处理能力。

Abstract: Large language models (LLMs) have demonstrated strong performance on a variety of natural language processing (NLP) tasks. However, they often struggle with long-text sequences due to the ``lost in the middle'' phenomenon. This issue has been shown to arise from a U-shaped attention bias, where attention is disproportionately focused on the beginning and end of a text, leaving the middle section underrepresented. While previous studies have attributed this bias to position encoding, our research first identifies an additional factor: initial saliency. It means that in the attention computation for each token, tokens with higher attention weights relative to the initial token tend to receive more attention in the prediction of the next token. We further find that utilizing this property by scaling attention weight between the initial token and others improves the model's ability to process long contexts, achieving a maximum improvement of 3.6\% in MDQA dataset. Moreover, combining this approach with existing methods to reduce position encoding bias further enhances performance, achieving a maximum improvement of 3.4\% in KV-Retrieval tasks.

</details>


### [25] [Efficient Adaptive Rejection Sampling for Accelerating Speculative Decoding in Large Language Models](https://arxiv.org/abs/2512.13194)
*Chendong Sun*

Main category: cs.CL

Relevance: 85.0

TL;DR: 提出EARS方法，通过动态调整接受阈值来改进推测解码中的拒绝采样机制，减少随机拒绝问题，提升推理效率


<details>
  <summary>Details</summary>
Motivation: 推测解码中的拒绝采样机制使用固定的随机阈值，在高不确定性生成场景中会导致大量"随机拒绝"问题，即合理的候选token因随机性被拒绝，降低了推理效率

Method: 提出EARS方法，通过引入目标模型自身的预测不确定性（1 - max(P_target)）来动态调整接受阈值，当模型不确定时智能放宽接受标准，减少随机拒绝

Result: 在创意写作和开放域QA任务上，EARS显著提升推测解码效率，在GSM8K基准上实现最高18.12%的吞吐量提升，仅带来0.84%的准确率下降

Conclusion: EARS有效解决了推测解码中的随机拒绝问题，无需修改模型架构即可无缝集成到现有框架中，显著提升LLM推理效率

Abstract: Speculative Decoding is a prominent technique for accelerating the autoregressive inference of large language models (LLMs) by employing a fast draft model to propose candidate token sequences and a large target model to verify them in parallel. However, its core component -- the rejection sampling mechanism -- relies on a fixed, context-independent random threshold. This leads to a significant "random rejection" problem in high-uncertainty generation scenarios, where plausible candidate tokens are frequently rejected due to random chance, undermining inference efficiency. This paper introduces Efficient Adaptive Rejection Sampling (EARS), a novel method that dynamically adjusts the acceptance threshold by incorporating the target model's own predictive uncertainty, measured as \(1 - \max(P_{\mathrm{target}})\). By introducing a tolerance term proportional to this uncertainty, EARS intelligently relaxes the acceptance criterion when the model is uncertain, effectively reducing random rejections while maintaining strict standards when the model is confident. Experiments on creative writing and open-domain QA tasks demonstrate that EARS significantly enhances the efficiency of speculative decoding, achieving up to an 18.12% increase in throughput with a negligible 0.84% accuracy drop on the GSM8K benchmark. The method requires no modifications to model architectures and can be seamlessly integrated into existing speculative decoding frameworks.

</details>


### [26] [AutoTool: Dynamic Tool Selection and Integration for Agentic Reasoning](https://arxiv.org/abs/2512.13278)
*Jiaru Zou,Ling Yang,Yunzhe Qi,Sirui Chen,Mengting Ai,Ke Shen,Jingrui He,Mengdi Wang*

Main category: cs.CL

Relevance: 85.0

TL;DR: AutoTool是一个让LLM代理具备动态工具选择能力的框架，通过构建包含20万条工具选择理由的数据集和双阶段优化流程，显著提升了LLM在数学、科学、代码生成和多模态理解等任务中的表现。


<details>
  <summary>Details</summary>
Motivation: 现有LLM代理方法假设固定工具集，限制了代理对新出现或不断演化的工具的适应能力。需要让LLM代理能够在推理轨迹中动态选择工具，提高其灵活性和适应性。

Method: 1) 构建包含20万条数据的数据集，涵盖1000+工具和100+任务；2) 双阶段优化：监督和RL轨迹稳定化确保连贯推理，KL正则化Plackett-Luce排序优化多步工具选择一致性。

Result: 在10个基准测试中，AutoTool在Qwen3-8B和Qwen2.5-VL-7B上实现显著提升：数学科学推理平均提升6.4%，搜索QA提升4.5%，代码生成提升7.7%，多模态理解提升6.9%。同时展现出对未见工具的泛化能力。

Conclusion: AutoTool通过动态工具选择框架显著提升了LLM代理的推理能力和工具使用适应性，在多个领域超越现有方法，并具备对新工具的泛化能力。

Abstract: Agentic reinforcement learning has advanced large language models (LLMs) to reason through long chain-of-thought trajectories while interleaving external tool use. Existing approaches assume a fixed inventory of tools, limiting LLM agents' adaptability to new or evolving toolsets. We present AutoTool, a framework that equips LLM agents with dynamic tool-selection capabilities throughout their reasoning trajectories. We first construct a 200k dataset with explicit tool-selection rationales across 1,000+ tools and 100+ tasks spanning mathematics, science, code generation, and multimodal reasoning. Building on this data foundation, AutoTool employs a dual-phase optimization pipeline: (i) supervised and RL-based trajectory stabilization for coherent reasoning, and (ii) KL-regularized Plackett-Luce ranking to refine consistent multi-step tool selection. Across ten diverse benchmarks, we train two base models, Qwen3-8B and Qwen2.5-VL-7B, with AutoTool. With fewer parameters, AutoTool consistently outperforms advanced LLM agents and tool-integration methods, yielding average gains of 6.4% in math & science reasoning, 4.5% in search-based QA, 7.7% in code generation, and 6.9% in multimodal understanding. In addition, AutoTool exhibits stronger generalization by dynamically leveraging unseen tools from evolving toolsets during inference.

</details>


### [27] [AIR: Post-training Data Selection for Reasoning via Attention Head Influence](https://arxiv.org/abs/2512.13279)
*Jinrui Liu,Jeff Wu,Xuanguang Pan,Gavin Cheung,Shuai Ma,Chongyang Tao*

Main category: cs.CL

Relevance: 85.0

TL;DR: 提出AIR框架，利用注意力机制洞察来选择高价值后训练数据，提升LLM推理能力蒸馏效率


<details>
  <summary>Details</summary>
Motivation: 现有数据选择方法（手动筛选、基于长度/熵/损失的启发式方法）无法捕捉推理步骤的因果重要性，限制了LLM推理能力蒸馏的效率

Method: AIR框架：1) 识别现成模型的推理关键注意力头；2) 构建禁用头影响的弱化参考模型；3) 量化损失差异作为注意力影响分数，支持步骤级加权微调和全局样本选择

Result: 在多个推理基准测试中，AIR持续提升推理准确率，超越启发式基线方法，有效识别最关键步骤和样本

Conclusion: 建立了机制驱动、数据高效的LLM推理蒸馏方法，为后训练数据选择提供了新视角

Abstract: LLMs achieve remarkable multi-step reasoning capabilities, yet effectively transferring these skills via post-training distillation remains challenging. Existing data selection methods, ranging from manual curation to heuristics based on length, entropy, or overall loss, fail to capture the causal importance of individual reasoning steps, limiting distillation efficiency. To address this, we propose Attention Influence for Reasoning (AIR), a principled, unsupervised and training-free framework that leverages mechanistic insights of the retrieval head to select high-value post-training data. AIR first identifies reasoning-critical attention heads of an off-the-shelf model, then constructs a weakened reference model with disabled head influence, and finally quantifies the resulting loss divergence as the Attention Influence Score. This score enables fine-grained assessment at both the step and sample levels, supporting step-level weighted fine-tuning and global sample selection. Experiments across multiple reasoning benchmarks show that AIR consistently improves reasoning accuracy, surpassing heuristic baselines and effectively isolating the most critical steps and samples. Our work establishes a mechanism-driven, data-efficient approach for reasoning distillation in LLMs.

</details>


### [28] [Scaling Laws for Code: Every Programming Language Matters](https://arxiv.org/abs/2512.13472)
*Jian Yang,Shawn Guo,Lin Jing,Wei Zhang,Aishan Liu,Chuan Hao,Zhoujun Li,Wayne Xin Zhao,Xianglong Liu,Weifeng Lv,Bryan Dai*

Main category: cs.CL

Relevance: 85.0

TL;DR: 本文首次系统性地探索了多语言代码预训练的缩放定律，通过1000+实验发现解释型语言（如Python）比编译型语言（如Rust）更能从模型大小和数据增加中受益，并提出了基于比例的多语言缩放定律来优化训练token分配。


<details>
  <summary>Details</summary>
Motivation: 现有代码大语言模型（Code LLMs）的训练成本高昂，但不同编程语言在预训练中的影响差异显著，导致性能预测不准确。现有研究多关注语言无关设置，忽视了现代软件开发的多语言本质，因此需要首先研究不同编程语言的缩放定律，再考虑它们之间的相互影响。

Method: 进行了超过1000个实验（相当于336,000+ H800小时），涵盖多种编程语言、模型大小（0.2B到14B参数）和数据集大小（1T tokens）。建立了全面的代码LLMs缩放定律，并提出了并行配对预训练策略（将代码片段与其翻译版本拼接）。

Result: 发现解释型语言（如Python）比编译型语言（如Rust）更能从模型大小和数据增加中受益；多语言预训练具有协同效应，特别是在语法相似的编程语言之间；并行配对策略显著增强了跨语言能力；提出的比例依赖型多语言缩放定律在相同计算预算下比均匀分布获得了更优的平均性能。

Conclusion: 这是首个系统探索多语言代码预训练缩放定律的研究，揭示了不同编程语言的缩放特性差异，提出了优化训练token分配的实用方法，为高效训练多语言代码LLMs提供了理论基础和实践指导。

Abstract: Code large language models (Code LLMs) are powerful but costly to train, with scaling laws predicting performance from model size, data, and compute. However, different programming languages (PLs) have varying impacts during pre-training that significantly affect base model performance, leading to inaccurate performance prediction. Besides, existing works focus on language-agnostic settings, neglecting the inherently multilingual nature of modern software development. Therefore, it is first necessary to investigate the scaling laws of different PLs, and then consider their mutual influences to arrive at the final multilingual scaling law. In this paper, we present the first systematic exploration of scaling laws for multilingual code pre-training, conducting over 1000+ experiments (Equivalent to 336,000+ H800 hours) across multiple PLs, model sizes (0.2B to 14B parameters), and dataset sizes (1T tokens). We establish comprehensive scaling laws for code LLMs across multiple PLs, revealing that interpreted languages (e.g., Python) benefit more from increased model size and data than compiled languages (e.g., Rust). The study demonstrates that multilingual pre-training provides synergistic benefits, particularly between syntactically similar PLs. Further, the pre-training strategy of the parallel pairing (concatenating code snippets with their translations) significantly enhances cross-lingual abilities with favorable scaling properties. Finally, a proportion-dependent multilingual scaling law is proposed to optimally allocate training tokens by prioritizing high-utility PLs (e.g., Python), balancing high-synergy pairs (e.g., JavaScript-TypeScript), and reducing allocation to fast-saturating languages (Rust), achieving superior average performance across all PLs compared to uniform distribution under the same compute budget.

</details>


### [29] [Non-Resolution Reasoning: A Framework for Preserving Semantic Ambiguity in Language Models](https://arxiv.org/abs/2512.13478)
*Kei Saito*

Main category: cs.CL

Relevance: 85.0

TL;DR: 提出Non-Resolution Reasoning (NRR)框架，通过多向量嵌入、非坍缩注意力、上下文身份追踪等技术，解决语言模型中过早语义坍缩问题，将语义歧义作为显式表示状态而非故障模式。


<details>
  <summary>Details</summary>
Motivation: 当前语言模型存在过早语义坍缩的核心架构限制，softmax竞争和贪婪解码导致模型在获得足够上下文前就丢弃有效解释，造成脆弱的推理和上下文失败。

Method: 提出NRR框架，包含三个组件：1) 多向量嵌入，每个token维护多个可行解释；2) 非坍缩注意力，防止跨层的赢家通吃动态；3) 上下文身份追踪，为重复实体分配上下文特定身份。通过外部解析算子ρ实现显式、可控、任务依赖的语义承诺。

Result: 合成评估显示NRR能有效保持歧义和追踪上下文：CIT增强模型在分布外身份转移任务上达到90.9%准确率，而Transformer基线仅为9.1%。

Conclusion: NRR为过早坍缩问题提供了原则性替代方案，将歧义重构为显式表示状态而非故障模式。核心问题不是AI是否应解决歧义，而是何时、如何以及在谁的控制下解决。

Abstract: Premature semantic collapse -- the forced early commitment to a single meaning -- remains a core architectural limitation of current language models. Softmax-driven competition and greedy decoding cause models to discard valid interpretations before sufficient context is available, resulting in brittle reasoning and context failures. We introduce Non-Resolution Reasoning (NRR), a general computational framework that preserves semantic ambiguity during inference and performs resolution only when explicitly required. NRR integrates three components: (1) Multi-Vector Embeddings that maintain multiple viable interpretations per token, (2) Non-Collapsing Attention that prevents winner-take-all dynamics across layers, and (3) Contextual Identity Tracking (CIT), which assigns context-specific identities to recurring entities (e.g., distinguishing "Dr. Smith the cardiologist" from "Dr. Smith the researcher"). These mechanisms are unified by an external Resolution Operator $ρ$ that makes semantic commitment explicit, controllable, and task-dependent. Unlike standard architectures, NRR separates representation from resolution, allowing a single model to shift between creative, factual, and ambiguity-preserving reasoning without retraining. A synthetic evaluation demonstrates NRR's ability to preserve ambiguity and track context: CIT-enhanced models achieve 90.9% accuracy on out-of-distribution identity-shift tasks, compared to 9.1% for transformer baselines. NRR provides a principled alternative to premature collapse, reframing ambiguity as an explicit representational state rather than a failure mode. The question is not whether AI should resolve ambiguity, but when, how, and under whose control.

</details>


### [30] [SkipCat: Rank-Maximized Low-Rank Compression of Large Language Models via Shared Projection and Block Skipping](https://arxiv.org/abs/2512.13494)
*Yu-Chen Lu,Sheng-Feng Yu,Hui-Hsien Weng,Pei-Shuo Wang,Yu-Fang Hu,Liang Hung-Chun,Hung-Yueh Chiang,Kai-Chiang Wu*

Main category: cs.CL

Relevance: 85.0

TL;DR: SkipCat：一种新颖的低秩压缩框架，通过层内共享低秩投影和块跳过技术，在相同压缩率下保留更多有效秩，显著提升压缩后LLM性能


<details>
  <summary>Details</summary>
Motivation: 大型语言模型参数量大，难以部署在计算和内存资源有限的边缘设备上。传统低秩压缩方法需要大幅降低保留秩才能获得效率提升，但会导致显著的性能下降。需要一种方法能在相同压缩率下保留更多有效秩。

Method: 提出SkipCat框架：1）层内共享低秩投影：多个共享相同输入的矩阵使用共同投影，减少冗余；2）块跳过技术：在低秩分解中省略选定子块的计算和内存传输。这两种技术共同使压缩模型在相同压缩预算下保留更多有效秩。

Result: 实验结果显示，在相同压缩率下，无需额外微调，该方法在零样本任务上比先前低秩压缩方法提升7%准确率。证明了在严格资源约束下保持模型性能的有效性。

Conclusion: SkipCat通过最大化保留有效秩的低秩压缩策略，在保持压缩效率的同时显著减少性能损失，为资源受限环境中的LLM部署提供了有效解决方案。

Abstract: Large language models (LLM) have achieved remarkable performance across a wide range of tasks. However, their substantial parameter sizes pose significant challenges for deployment on edge devices with limited computational and memory resources. Low-rank compression is a promising approach to address this issue, as it reduces both computational and memory costs, making LLM more suitable for resource-constrained environments. Nonetheless, naïve low-rank compression methods require a significant reduction in the retained rank to achieve meaningful memory and computation savings. For a low-rank model, the ranks need to be reduced by more than half to yield efficiency gains. Such aggressive truncation, however, typically results in substantial performance degradation. To address this trade-off, we propose SkipCat, a novel low-rank compression framework that enables the use of higher ranks while achieving the same compression rates. First, we introduce an intra-layer shared low-rank projection method, where multiple matrices that share the same input use a common projection. This reduces redundancy and improves compression efficiency. Second, we propose a block skipping technique that omits computations and memory transfers for selected sub-blocks within the low-rank decomposition. These two techniques jointly enable our compressed model to retain more effective ranks under the same compression budget. Experimental results show that, without any additional fine-tuning, our method outperforms previous low-rank compression approaches by 7% accuracy improvement on zero-shot tasks under the same compression rate. These results highlight the effectiveness of our rank-maximized compression strategy in preserving model performance under tight resource constraints.

</details>


### [31] [Memory in the Age of AI Agents](https://arxiv.org/abs/2512.13564)
*Yuyang Hu,Shichun Liu,Yanwei Yue,Guibin Zhang,Boyang Liu,Fangyi Zhu,Jiahang Lin,Honglin Guo,Shihan Dou,Zhiheng Xi,Senjie Jin,Jiejun Tan,Yanbin Yin,Jiongnan Liu,Zeyu Zhang,Zhongxiang Sun,Yutao Zhu,Hao Sun,Boci Peng,Zhenrong Cheng,Xuanbo Fan,Jiaxin Guo,Xinlei Yu,Zhenhong Zhou,Zewen Hu,Jiahao Huo,Junhao Wang,Yuwei Niu,Yu Wang,Zhenfei Yin,Xiaobin Hu,Yue Liao,Qiankun Li,Kun Wang,Wangchunshu Zhou,Yixin Liu,Dawei Cheng,Qi Zhang,Tao Gui,Shirui Pan,Yan Zhang,Philip Torr,Zhicheng Dou,Ji-Rong Wen,Xuanjing Huang,Yu-Gang Jiang,Shuicheng Yan*

Main category: cs.CL

Relevance: 85.0

TL;DR: 该论文对智能体记忆研究进行了系统性综述，提出了基于形式、功能和动态的三维分析框架，整理了相关基准和工具，并展望了未来研究方向。


<details>
  <summary>Details</summary>
Motivation: 智能体记忆已成为基础模型智能体的核心能力，但当前研究碎片化严重，术语定义松散，传统分类方法无法捕捉现代智能体记忆系统的多样性，需要建立统一的分析框架。

Method: 1) 明确界定智能体记忆范围，区分LLM记忆、RAG和上下文工程；2) 从形式（token级、参数化、潜在记忆）、功能（事实、经验、工作记忆）和动态（形成、演化、检索）三个维度分析；3) 整理记忆基准和开源框架。

Result: 提出了统一的三维分析框架，系统梳理了当前智能体记忆研究现状，为领域提供了概念基础和实用工具，并识别了未来研究方向。

Conclusion: 该综述不仅为现有工作提供参考，更重要的是为将记忆作为未来智能体智能设计的"一等公民"提供了概念基础，推动了智能体记忆研究的系统化发展。

Abstract: Memory has emerged, and will continue to remain, a core capability of foundation model-based agents. As research on agent memory rapidly expands and attracts unprecedented attention, the field has also become increasingly fragmented. Existing works that fall under the umbrella of agent memory often differ substantially in their motivations, implementations, and evaluation protocols, while the proliferation of loosely defined memory terminologies has further obscured conceptual clarity. Traditional taxonomies such as long/short-term memory have proven insufficient to capture the diversity of contemporary agent memory systems. This work aims to provide an up-to-date landscape of current agent memory research. We begin by clearly delineating the scope of agent memory and distinguishing it from related concepts such as LLM memory, retrieval augmented generation (RAG), and context engineering. We then examine agent memory through the unified lenses of forms, functions, and dynamics. From the perspective of forms, we identify three dominant realizations of agent memory, namely token-level, parametric, and latent memory. From the perspective of functions, we propose a finer-grained taxonomy that distinguishes factual, experiential, and working memory. From the perspective of dynamics, we analyze how memory is formed, evolved, and retrieved over time. To support practical development, we compile a comprehensive summary of memory benchmarks and open-source frameworks. Beyond consolidation, we articulate a forward-looking perspective on emerging research frontiers, including memory automation, reinforcement learning integration, multimodal memory, multi-agent memory, and trustworthiness issues. We hope this survey serves not only as a reference for existing work, but also as a conceptual foundation for rethinking memory as a first-class primitive in the design of future agentic intelligence.

</details>


### [32] [ReFusion: A Diffusion Large Language Model with Parallel Autoregressive Decoding](https://arxiv.org/abs/2512.13586)
*Jia-Nan Li,Jian Guan,Wei Wu,Chongxuan Li*

Main category: cs.CL

Relevance: 85.0

TL;DR: ReFusion是一种新颖的掩码扩散模型，通过将并行解码从token级别提升到slot级别来解决现有掩码扩散模型的计算开销和生成不连贯问题，实现了性能与效率的双重提升。


<details>
  <summary>Details</summary>
Motivation: 自回归模型存在顺序推理慢的问题，而掩码扩散模型虽然提供并行替代方案，但存在两个关键缺陷：1）由于无法使用KV缓存导致计算开销高；2）在难以处理的token组合空间上学习依赖关系导致生成不连贯。需要解决这些限制。

Method: 提出ReFusion模型，采用slot级别的并行解码设计。每个slot是固定长度的连续子序列。采用迭代的"规划-填充"解码过程：扩散规划步骤识别一组弱依赖的slot，然后自回归填充步骤并行解码这些选定的slot。slot设计实现了完整的KV缓存重用，并将学习复杂度从token组合空间降低到可管理的slot排列空间。

Result: 在七个不同基准测试上的实验表明，ReFusion不仅以34%的性能提升和超过18倍的速度提升大幅超越先前的掩码扩散模型，而且在与强自回归模型的性能差距上取得显著进展，同时保持2.33倍的平均速度优势。

Conclusion: ReFusion通过slot级别的并行解码设计，成功解决了掩码扩散模型的关键限制，在保持并行解码优势的同时实现了与自回归模型相当的性能，为高效语言生成提供了有前景的解决方案。

Abstract: Autoregressive models (ARMs) are hindered by slow sequential inference. While masked diffusion models (MDMs) offer a parallel alternative, they suffer from critical drawbacks: high computational overhead from precluding Key-Value (KV) caching, and incoherent generation arising from learning dependencies over an intractable space of token combinations. To address these limitations, we introduce ReFusion, a novel masked diffusion model that achieves superior performance and efficiency by elevating parallel decoding from the token level to a higher slot level, where each slot is a fixed-length, contiguous sub-sequence. This is achieved through an iterative ``plan-and-infill'' decoding process: a diffusion-based planning step first identifies a set of weakly dependent slots, and an autoregressive infilling step then decodes these selected slots in parallel. The slot-based design simultaneously unlocks full KV cache reuse with a unified causal framework and reduces the learning complexity from the token combination space to a manageable slot-level permutation space. Extensive experiments on seven diverse benchmarks show that ReFusion not only overwhelmingly surpasses prior MDMs with 34% performance gains and an over 18$\times$ speedup on average, but also bridges the performance gap to strong ARMs while maintaining a 2.33$\times$ average speedup.

</details>


### [33] [Textual Gradients are a Flawed Metaphor for Automatic Prompt Optimization](https://arxiv.org/abs/2512.13598)
*Daniel Melcer,Qi Chen,Wen-Hao Chiang,Shweta Garg,Pranav Garg,Christian Bock*

Main category: cs.CL

Relevance: 85.0

TL;DR: 该论文研究了文本梯度方法的自动提示优化技术，发现虽然这些方法能提升LLM性能，但梯度类比并不能准确解释其行为。


<details>
  <summary>Details</summary>
Motivation: 自动提示优化技术旨在无需人工调优即可提升大语言模型性能，其中文本梯度方法是主流方法之一。研究者希望深入理解这些方法的行为机制，验证梯度类比是否准确解释了其工作原理。

Method: 通过一系列实验和案例研究，分析文本梯度方法的实际行为。研究者系统地测试了这些方法在不同场景下的表现，并与梯度类比的理论预期进行对比。

Result: 实验表明，虽然文本梯度方法通常能带来性能提升，但梯度类比并不能准确解释其行为。这意味着这些方法的成功可能源于其他机制，而非简单的梯度优化过程。

Conclusion: 研究结果有助于指导提示优化策略的选择和新方法的开发。对文本梯度方法行为机制的深入理解将推动更有效的自动提示优化技术的发展。

Abstract: A well-engineered prompt can increase the performance of large language models; automatic prompt optimization techniques aim to increase performance without requiring human effort to tune the prompts. One leading class of prompt optimization techniques introduces the analogy of textual gradients. We investigate the behavior of these textual gradient methods through a series of experiments and case studies. While such methods often result in a performance improvement, our experiments suggest that the gradient analogy does not accurately explain their behavior. Our insights may inform the selection of prompt optimization strategies, and development of new approaches.

</details>


### [34] [EMNLP: Educator-role Moral and Normative Large Language Models Profiling](https://arxiv.org/abs/2508.15250)
*Yilin Jiang,Mingzi Zhang,Sheng Jin,Zengyi Yu,Xiangjie Kong,Binghao Tu*

Main category: cs.CL

Relevance: 85.0

TL;DR: 本文提出了EMNLP框架，用于评估教师角色LLMs的心理和伦理特征，包括人格分析、道德发展阶段测量，以及软提示注入下的伦理风险。研究发现教师角色LLMs比人类教师表现出更理想化和两极分化的人格，在抽象道德推理上表现优异，但在情感复杂情境中表现不佳。


<details>
  <summary>Details</summary>
Motivation: 当前LLMs能够模拟专业角色（如教师），但缺乏对这些角色在心理和伦理方面的全面评估。特别是在教育领域，需要评估教师角色LLMs的人格特征、道德发展水平和伦理风险，以确保教育AI的安全性和可靠性。

Method: 提出了EMNLP框架，扩展现有量表并构建了88个教师特定的道德困境场景，用于与人类教师进行专业导向的比较。设计了针对性的软提示注入集来评估教师角色LLMs的合规性和脆弱性。在14个LLMs上进行实验，分析其人格特征、道德推理能力和伦理风险。

Result: 教师角色LLMs表现出比人类教师更理想化和两极分化的人格特征；在抽象道德推理方面表现优异，但在情感复杂情境中表现不佳；推理能力更强的模型对有害提示注入更脆弱，揭示了能力与安全性之间的悖论；模型温度等超参数对风险行为影响有限。

Conclusion: 本文提出了首个评估教师角色LLMs伦理和心理对齐的基准，揭示了LLMs在模拟专业角色时的心理和伦理特征差异，为教育AI的安全部署提供了重要参考。

Abstract: Simulating Professions (SP) enables Large Language Models (LLMs) to emulate professional roles. However, comprehensive psychological and ethical evaluation in these contexts remains lacking. This paper introduces EMNLP, an Educator-role Moral and Normative LLMs Profiling framework for personality profiling, moral development stage measurement, and ethical risk under soft prompt injection. EMNLP extends existing scales and constructs 88 teacher-specific moral dilemmas, enabling profession-oriented comparison with human teachers. A targeted soft prompt injection set evaluates compliance and vulnerability in teacher SP. Experiments on 14 LLMs show teacher-role LLMs exhibit more idealized and polarized personalities than human teachers, excel in abstract moral reasoning, but struggle with emotionally complex situations. Models with stronger reasoning are more vulnerable to harmful prompt injection, revealing a paradox between capability and safety. The model temperature and other hyperparameters have limited influence except in some risk behaviors. This paper presents the first benchmark to assess ethical and psychological alignment of teacher-role LLMs for educational AI. Resources are available at https://e-m-n-l-p.github.io/.

</details>


### [35] [Nemotron-Cascade: Scaling Cascaded Reinforcement Learning for General-Purpose Reasoning Models](https://arxiv.org/abs/2512.13607)
*Boxin Wang,Chankyu Lee,Nayeon Lee,Sheng-Chieh Lin,Wenliang Dai,Yang Chen,Yangyi Chen,Zhuolin Yang,Zihan Liu,Mohammad Shoeybi,Bryan Catanzaro,Wei Ping*

Main category: cs.CL

Relevance: 85.0

TL;DR: 提出Cascade RL方法训练通用推理模型Nemotron-Cascade，通过顺序领域强化学习解决跨领域异质性问题，在多个基准测试中达到SOTA性能


<details>
  <summary>Details</summary>
Motivation: 通用推理模型的强化学习面临跨领域异质性问题，包括推理时响应长度和验证延迟的巨大差异，这导致RL基础设施复杂、训练缓慢，且训练课程和超参数选择困难

Method: 提出级联领域强化学习（Cascade RL），不同于传统混合不同领域提示的方法，采用顺序、领域化的RL方法，降低工程复杂度。先使用RLHF进行对齐作为预处理步骤，然后进行领域化的RLVR阶段

Result: 14B模型在RL后超越了其SFT教师DeepSeek-R1-0528，在LiveCodeBench v5/v6/Pro上表现优异，在2025年国际信息学奥林匹克竞赛中获得银牌性能。RLHF对齐显著提升了模型的推理能力，后续领域RLVR阶段很少降低早期领域获得的基准性能，甚至可能改进

Conclusion: Cascade RL方法能有效训练通用推理模型，减少工程复杂性，实现最先进的跨领域性能。透明分享了训练和数据配方

Abstract: Building general-purpose reasoning models with reinforcement learning (RL) entails substantial cross-domain heterogeneity, including large variation in inference-time response lengths and verification latency. Such variability complicates the RL infrastructure, slows training, and makes training curriculum (e.g., response length extension) and hyperparameter selection challenging. In this work, we propose cascaded domain-wise reinforcement learning (Cascade RL) to develop general-purpose reasoning models, Nemotron-Cascade, capable of operating in both instruct and deep thinking modes. Departing from conventional approaches that blend heterogeneous prompts from different domains, Cascade RL orchestrates sequential, domain-wise RL, reducing engineering complexity and delivering state-of-the-art performance across a wide range of benchmarks. Notably, RLHF for alignment, when used as a pre-step, boosts the model's reasoning ability far beyond mere preference optimization, and subsequent domain-wise RLVR stages rarely degrade the benchmark performance attained in earlier domains and may even improve it (see an illustration in Figure 1). Our 14B model, after RL, outperforms its SFT teacher, DeepSeek-R1-0528, on LiveCodeBench v5/v6/Pro and achieves silver-medal performance in the 2025 International Olympiad in Informatics (IOI). We transparently share our training and data recipes.

</details>


### [36] [Large-Language Memorization During the Classification of United States Supreme Court Cases](https://arxiv.org/abs/2512.13654)
*John E. Ortega,Dhruv D. Joshi,Matt P. Borkowski*

Main category: cs.CL

Relevance: 85.0

TL;DR: 本文研究了LLM在最高法院判决分类任务中的记忆准确性，发现基于提示的模型（如DeepSeek）比传统BERT模型表现更好，在15类和279类分类任务上都有约2分的提升。


<details>
  <summary>Details</summary>
Motivation: 研究LLM在分类任务中的记忆准确性和"幻觉"现象，特别是针对具有挑战性的法律文本分类任务。最高法院判决语料库因其句子长度长、法律术语复杂、结构非标准和领域特定词汇而成为研究LLM记忆准确性的理想测试平台。

Method: 使用最新的LLM微调和检索方法，包括参数高效微调、自动建模等，在两个传统的基于类别的最高法院判决分类任务上进行实验：一个包含15个标记主题，另一个包含279个主题。比较了基于提示的模型（如DeepSeek）与传统BERT模型的性能。

Result: 基于提示的模型（如DeepSeek）在两个分类任务上都比之前的BERT模型更鲁棒，在15类和279类分类任务上都有约2分的性能提升。这表明具有记忆功能的提示模型在处理复杂法律文本分类时具有优势。

Conclusion: 基于提示的LLM模型在复杂的法律文本分类任务中表现出比传统BERT模型更好的性能，特别是在处理具有挑战性的最高法院判决分类时。这为LLM在专业领域分类任务中的应用提供了实证支持。

Abstract: Large-language models (LLMs) have been shown to respond in a variety of ways for classification tasks outside of question-answering. LLM responses are sometimes called "hallucinations" since the output is not what is ex pected. Memorization strategies in LLMs are being studied in detail, with the goal of understanding how LLMs respond. We perform a deep dive into a classification task based on United States Supreme Court (SCOTUS) decisions. The SCOTUS corpus is an ideal classification task to study for LLM memory accuracy because it presents significant challenges due to extensive sentence length, complex legal terminology, non-standard structure, and domain-specific vocabulary. Experimentation is performed with the latest LLM fine tuning and retrieval-based approaches, such as parameter-efficient fine-tuning, auto-modeling, and others, on two traditional category-based SCOTUS classification tasks: one with 15 labeled topics and another with 279. We show that prompt-based models with memories, such as DeepSeek, can be more robust than previous BERT-based models on both tasks scoring about 2 points better than previous models not based on prompting.

</details>


### [37] [Comparative Analysis of LLM Abliteration Methods: A Cross-Architecture Evaluation](https://arxiv.org/abs/2512.13655)
*Richard J. Young*

Main category: cs.CL

Relevance: 85.0

TL;DR: 评估四种拒绝行为消除工具（Heretic、DECCP、ErisForge、FailSpy）在16个指令调优模型上的效果，发现单次通过方法在能力保留上表现更好，而数学推理能力对消除干预最敏感。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型的安全对齐机制通过拒绝行为防止有害查询响应，但这些机制也阻碍了认知建模、对抗测试和安全分析等合法研究应用。虽然消除技术可以通过方向正交化手术式移除拒绝表示，但现有实现的效果差异尚未被系统评估。

Method: 评估四种消除工具（Heretic、DECCP、ErisForge、FailSpy）在16个指令调优模型（7B-14B参数）上的兼容性和效果。在工具支持的子集上报告定量指标，包括GSM8K数学推理能力变化和KL散度分布偏移。

Result: 单次通过方法在能力保留上表现更优（三个模型上GSM8K平均变化：ErisForge -0.28pp；DECCP -0.13pp）。贝叶斯优化的消除产生可变分布偏移（KL散度：0.043-1.646），对能力影响因模型而异。数学推理能力对消除干预最敏感，GSM8K变化范围从+1.51pp到-18.81pp（相对-26.5%）。

Conclusion: 研究为消除工具选择提供了基于证据的标准，表明数学推理能力对消除干预最敏感，工具选择和模型架构对能力保留有显著影响。

Abstract: Safety alignment mechanisms in large language models prevent responses to harmful queries through learned refusal behavior, yet these same mechanisms impede legitimate research applications including cognitive modeling, adversarial testing, and security analysis. While abliteration techniques enable surgical removal of refusal representations through directional orthogonalization, the relative effectiveness of available implementations remains uncharacterized. This study evaluates four abliteration tools (Heretic, DECCP, ErisForge, FailSpy) across sixteen instruction-tuned models (7B-14B parameters), reporting tool compatibility on all 16 models and quantitative metrics on subsets dictated by tool support. Single-pass methods demonstrated superior capability preservation on the benchmarked subset (avg GSM8K change across three models: ErisForge -0.28 pp; DECCP -0.13 pp), while Bayesian-optimized abliteration produced variable distribution shift (KL divergence: 0.043-1.646) with model-dependent capability impact. These findings provide researchers with evidence-based selection criteria for abliteration tool deployment across diverse model architectures. The principal finding indicates that mathematical reasoning capabilities exhibit the highest sensitivity to abliteration interventions, with GSM8K change ranging from +1.51 pp to -18.81 pp (-26.5% relative) depending on tool selection and model architecture.

</details>


### [38] [Towards Effective Model Editing for LLM Personalization](https://arxiv.org/abs/2512.13676)
*Baixiang Huang,Limeng Cui,Jiapeng Liu,Haoran Wang,Jiawei Xu,Zhuiyue Tan,Yutong Chen,Chen Luo,Yi Liu,Kai Shu*

Main category: cs.CL

Relevance: 85.0

TL;DR: 论文提出Personalization Editing框架，将个性化视为模型编辑任务，通过聚类偏好表示指导局部编辑，实现精确偏好对齐同时保持模型整体能力。同时引入UPQA数据集评估模型回忆和应用用户特定偏好的能力。


<details>
  <summary>Details</summary>
Motivation: 当前LLM个性化方法存在计算成本高、数据密集、易发生灾难性遗忘、多轮交互或处理隐式查询时性能下降等问题。现有个性化基准常依赖LLM间对话而非真实用户交互，或侧重风格模仿而忽视需要准确回忆用户偏好的信息检索任务。

Method: 将个性化概念化为模型编辑任务，提出Personalization Editing框架：1) 通过聚类偏好表示指导局部编辑；2) 实现精确偏好对齐更新同时保持模型整体能力；3) 引入UPQA数据集，基于真实用户查询构建短答案QA数据集，评估模型回忆和应用特定用户偏好的能力。

Result: Personalization Editing在编辑准确性和计算效率上优于微调方法，在多轮对话和隐式偏好问题设置中超越基于提示的基线方法。UPQA数据集能有效评估模型对用户特定偏好的回忆和应用能力。

Conclusion: 将个性化视为模型编辑任务并提供相应框架是有效的解决方案，能平衡个性化需求与模型保持能力。UPQA数据集填补了现有基准在评估模型回忆和应用用户特定偏好能力方面的空白。

Abstract: Personalization is becoming indispensable for LLMs to align with individual user preferences and needs. Yet current approaches are often computationally expensive, data-intensive, susceptible to catastrophic forgetting, and prone to performance degradation in multi-turn interactions or when handling implicit queries. To address these challenges, we conceptualize personalization as a model editing task and introduce Personalization Editing, a framework that applies localized edits guided by clustered preference representations. This design enables precise preference-aligned updates while preserving overall model capabilities. In addition, existing personalization benchmarks frequently rely on persona-based dialogs between LLMs rather than user-LLM interactions, or focus primarily on stylistic imitation while neglecting information-seeking tasks that require accurate recall of user-specific preferences. We introduce User Preference Question Answering (UPQA), a short-answer QA dataset constructed from in-situ user queries with varying levels of difficulty. Unlike prior benchmarks, UPQA directly evaluates a model's ability to recall and apply specific user preferences. Across experimental settings, Personalization Editing achieves higher editing accuracy and greater computational efficiency than fine-tuning, while outperforming prompting-based baselines in multi-turn conversations and implicit preference questions settings.

</details>


### [39] [ERA-IT: Aligning Semantic Models with Revealed Economic Preference for Real-Time and Explainable Patent Valuation](https://arxiv.org/abs/2512.12869)
*Yoo Yongmin,Kim Seungwoo,Liu Jingjiang*

Main category: cs.CE

Relevance: 85.0

TL;DR: 提出ERA-IT框架，利用专利续展历史作为经济偏好信号，通过指令微调使LLM生成与经济现实对齐的估值推理，提升专利价值预测准确性并增强可解释性。


<details>
  <summary>Details</summary>
Motivation: 传统文献计量指标（如引用次数）存在系统性延迟，无法及时解决高维技术规范中的信息不对称问题，导致无形资产估值在技术创新战略管理中面临挑战。

Method: 提出ERA-IT框架，将专利续展历史概念化为显示的经济偏好信号，作为监督信号对LLM进行指令微调，实现经济语义对齐。使用10,000个欧洲专利局专利数据集，训练模型预测价值层级并逆向工程经济思维链。

Result: ERA-IT在预测准确性上显著优于传统计量经济学模型和零样本LLM，并能生成明确、逻辑基础的价值评估理由，为决策者提供透明认知支架。

Conclusion: 该框架通过经济语义对齐，将LLM的生成推理与市场现实对齐，为高风险知识产权管理中的黑盒AI提供了透明度，解决了无形资产估值的信息不对称问题。

Abstract: Valuing intangible assets under uncertainty remains a critical challenge in the strategic management of technological innovation due to the information asymmetry inherent in high-dimensional technical specifications. Traditional bibliometric indicators, such as citation counts, fail to address this friction in a timely manner due to the systemic latency inherent in data accumulation. To bridge this gap, this study proposes the Economic Reasoning Alignment via Instruction Tuning (ERA-IT) framework. We theoretically conceptualize patent renewal history as a revealed economic preference and leverage it as an objective supervisory signal to align the generative reasoning of Large Language Models (LLMs) with market realities, a process we term Eco-Semantic Alignment. Using a randomly sampled dataset of 10,000 European Patent Office patents across diverse technological domains, we trained the model not only to predict value tiers but also to reverse-engineer the Economic Chain-of-Thought from unstructured text. Empirical results demonstrate that ERA-IT significantly outperforms both conventional econometric models and zero-shot LLMs in predictive accuracy. More importantly, by generating explicit, logically grounded rationales for valuation, the framework serves as a transparent cognitive scaffold for decision-makers, reducing the opacity of black-box AI in high-stakes intellectual property management.

</details>


### [40] [SIGMA: An AI-Empowered Training Stack on Early-Life Hardware](https://arxiv.org/abs/2512.13488)
*Lei Qu,Lianhai Ren,Peng Cheng,Rui Gao,Ruizhe Wang,Tianyu Chen,Xiao Liu,Xingjian Zhang,Yeyun Gong,Yifan Xiong,Yucheng Ding,Yuting Jiang,Zhenghao Lin,Zhongxin Guo,Ziyue Yang*

Main category: cs.DC

Relevance: 85.0

TL;DR: SIGMA是一个开源训练堆栈，专门针对早期AI加速器的大规模训练，通过LUCIA训练平台(LTP)和训练框架(LTF)解决了可靠性、稳定性和效率三大挑战，成功训练了200B参数的MoE模型。


<details>
  <summary>Details</summary>
Motivation: 早期AI加速器在大规模训练中面临三大核心挑战：1) 系统频繁中断和未定义故障模式影响可靠性；2) 数值误差和训练不稳定性威胁正确性和收敛性；3) 并行优化复杂性与不可预测的局部噪声降低效率。

Method: 开发了SIGMA开源训练堆栈，包含两个核心组件：LUCIA训练平台(LTP)针对早期AI加速器集群优化的系统；LUCIA训练框架(LTF)用于大规模分布式训练。采用MoE架构，在2,048个AI加速器上训练200B参数模型。

Result: LTP实现了94.45%的有效集群加速器利用率，显著减少了节点回收和作业恢复时间。LTF成功训练了SIGMA-MOE模型，达到21.08%的MFU（模型浮点运算利用率），获得最先进的下游精度，在75天训练期间仅发生一次稳定性事件。

Conclusion: SIGMA不仅解决了大规模训练的关键挑战，还为AI基础设施和平台创新设立了新基准，提供了强大且经济高效的替代方案，显著推进了AI能力和可扩展性。

Abstract: An increasing variety of AI accelerators is being considered for large-scale training. However, enabling large-scale training on early-life AI accelerators faces three core challenges: frequent system disruptions and undefined failure modes that undermine reliability; numerical errors and training instabilities that threaten correctness and convergence; and the complexity of parallelism optimization combined with unpredictable local noise that degrades efficiency. To address these challenges, SIGMA is an open-source training stack designed to improve the reliability, stability, and efficiency of large-scale distributed training on early-life AI hardware. The core of this initiative is the LUCIA TRAINING PLATFORM (LTP), the system optimized for clusters with early-life AI accelerators. Since its launch in March 2025, LTP has significantly enhanced training reliability and operational productivity. Over the past five months, it has achieved an impressive 94.45% effective cluster accelerator utilization, while also substantially reducing node recycling and job-recovery times. Building on the foundation of LTP, the LUCIA TRAINING FRAMEWORK (LTF) successfully trained SIGMA-MOE, a 200B MoE model, using 2,048 AI accelerators. This effort delivered remarkable stability and efficiency outcomes, achieving 21.08% MFU, state-of-the-art downstream accuracy, and encountering only one stability incident over a 75-day period. Together, these advances establish SIGMA, which not only tackles the critical challenges of large-scale training but also establishes a new benchmark for AI infrastructure and platform innovation, offering a robust, cost-effective alternative to prevailing established accelerator stacks and significantly advancing AI capabilities and scalability. The source code of SIGMA is available at https://github.com/microsoft/LuciaTrainingPlatform.

</details>


### [41] [Enhancing Urban Visual Place Recognition for Crowdsourced Flood Imagery via LLM-Guided Attention](https://arxiv.org/abs/2512.11811)
*Fengyi Xu,Jun Ma,Waishan Qiu,Cui Guo*

Main category: cs.CL

Relevance: 75.0

TL;DR: VPR-AttLLM：一个模型无关的框架，利用大语言模型的语义推理和地理空间知识，通过注意力引导的描述符增强来提升视觉地点识别模型在跨源社交媒体图像（如洪水灾害图像）上的检索性能，无需重新训练模型。


<details>
  <summary>Details</summary>
Motivation: 社交媒体上的众包街景图像为城市洪水等危机事件提供了实时视觉证据，但通常缺乏可靠的地理元数据。现有的视觉地点识别模型在处理此类图像时，由于视觉失真和跨源场景的领域偏移，性能显著下降。需要一种方法能够利用LLM的语义推理能力来增强VPR模型的性能。

Method: 提出VPR-AttLLM框架，将LLM的语义推理和地理空间知识集成到现有VPR流程中。通过LLM识别城市环境中具有位置信息的区域，抑制瞬态视觉噪声，使用注意力引导的描述符增强来提升检索性能。框架采用即插即用设计，无需重新训练模型或额外数据。

Result: 在扩展基准测试（包括SF-XL增强数据集、合成洪水场景、Mapillary照片和新的HK-URBAN数据集）上评估。将VPR-AttLLM与三个SOTA VPR模型（CosPlace、EigenPlaces、SALAD）集成，召回率持续提升，相对增益通常在1-3%，在最具挑战性的真实洪水图像上可达8%。

Conclusion: VPR-AttLLM不仅提高了检索准确性，还为LLM引导的多模态融合在视觉检索系统中建立了可推广的范式。通过将城市感知理论原理嵌入注意力机制，该框架将类人空间推理与现代VPR架构相结合。其即插即用设计、强大的跨源鲁棒性和可解释性，展示了其在可扩展城市监测和众包危机图像快速地理定位方面的潜力。

Abstract: Crowdsourced street-view imagery from social media provides valuable real-time visual evidence of urban flooding and other crisis events, yet it often lacks reliable geographic metadata for emergency response. Existing Visual Place Recognition (VPR) models exhibit substantial performance degradation when applied to such imagery due to visual distortions and domain shifts inherent in cross-source scenarios. This paper presents VPR-AttLLM, a model-agnostic framework that integrates the semantic reasoning and geospatial knowledge of Large Language Models (LLMs) into established VPR pipelines through attention-guided descriptor enhancement. By leveraging LLMs to identify location-informative regions within the city context and suppress transient visual noise, VPR-AttLLM improves retrieval performance without requiring model retraining or additional data. Comprehensive evaluations are conducted on extended benchmarks including SF-XL enriched with real social-media flood images, synthetic flooding scenarios over established query sets and Mapillary photos, and a new HK-URBAN dataset capturing morphologically distinct cityscapes. Integrating VPR-AttLLM with three state-of-the-art VPR models-CosPlace, EigenPlaces, and SALAD-consistently improves recall performance, yielding relative gains typically between 1-3% and reaching up to 8% on the most challenging real flood imagery. Beyond measurable gains in retrieval accuracy, this study establishes a generalizable paradigm for LLM-guided multimodal fusion in visual retrieval systems. By embedding principles from urban perception theory into attention mechanisms, VPR-AttLLM bridges human-like spatial reasoning with modern VPR architectures. Its plug-and-play design, strong cross-source robustness, and interpretability highlight its potential for scalable urban monitoring and rapid geo-localization of crowdsourced crisis imagery.

</details>


### [42] [Diffusion Language Model Inference with Monte Carlo Tree Search](https://arxiv.org/abs/2512.12168)
*Zheng Huang,Kiran Ramnath,Yueyan Chen,Aosong Feng,Sangmin Woo,Balasubramaniam Srinivasan,Zhichao Xu,Kang Zhou,Shuai Wang,Haibo Ding,Lin Lee Cheong*

Main category: cs.CL

Relevance: 75.0

TL;DR: MEDAL框架将蒙特卡洛树搜索集成到扩散语言模型推理中，通过搜索高置信度解掩码轨迹来优化文本生成，相比现有方法提升达22.0%


<details>
  <summary>Details</summary>
Motivation: 扩散语言模型(DLMs)作为自回归生成的有力替代方案，具有并行生成和全局一致性的优势。但在推理过程中，确定哪些位置解掩码以及选择哪些token是一个组合搜索问题。现有方法使用启发式近似或需要额外训练，往往产生次优解码路径。

Method: MEDAL框架在初始化阶段集成蒙特卡洛树搜索(MCTS)，探索有前景的解掩码轨迹。通过限制搜索空间到高置信度动作，并优先选择能提高剩余掩码位置模型置信度的token，为后续细化提供鲁棒的起点。

Result: 在多个基准测试中，MEDAL相比现有推理策略实现了高达22.0%的改进，为扩散语言模型的搜索式推理建立了新范式。

Conclusion: MEDAL通过将蒙特卡洛树搜索集成到扩散语言模型推理中，提供了一种原则性的搜索机制，显著提升了推理性能，为扩散语言模型的搜索式推理开辟了新方向。

Abstract: Diffusion language models (DLMs) have recently emerged as a compelling alternative to autoregressive generation, offering parallel generation and improved global coherence. During inference, DLMs generate text by iteratively denoising masked sequences in parallel; however, determining which positions to unmask and which tokens to commit forms a large combinatorial search problem. Existing inference methods approximate this search using heuristics, which often yield suboptimal decoding paths; other approaches instead rely on additional training to guide token selection. To introduce a principled search mechanism for DLMs inference, we introduce MEDAL, a framework that integrates Monte Carlo Tree SEarch initialization for Diffusion LAnguage Model inference. We employ Monte Carlo Tree Search at the initialization stage to explore promising unmasking trajectories, providing a robust starting point for subsequent refinement. This integration is enabled by restricting the search space to high-confidence actions and prioritizing token choices that improve model confidence over remaining masked positions. Across multiple benchmarks, MEDAL achieves up to 22.0% improvement over existing inference strategies, establishing a new paradigm for search-based inference in diffusion language models.

</details>


### [43] [SCIR: A Self-Correcting Iterative Refinement Framework for Enhanced Information Extraction Based on Schema](https://arxiv.org/abs/2512.12337)
*Yushen Fang,Jianjun Li,Mingqian Ding,Chang Liu,Xinchi Zou,Wenqi Yang*

Main category: cs.CL

Relevance: 75.0

TL;DR: 提出SCIR框架和MBSC数据集，通过自校正迭代优化降低信息抽取系统训练成本，提升性能


<details>
  <summary>Details</summary>
Motivation: 当前基于LLM的信息抽取系统面临两个主要问题：1) 训练成本高；2) 难以与LLM偏好对齐。需要一种更高效、更灵活的IE范式。

Method: 提出SCIR（自校正迭代优化）框架，包含双路径自校正模块和反馈驱动优化，实现即插即用兼容性。同时构建MBSC多任务双语自校正数据集，通过间接蒸馏GPT-4能力到IE结果检测模型。

Result: 在命名实体识别、关系抽取和事件抽取三个关键任务上，SCIR优于现有最佳方法，span-based Micro-F1平均提升5.27%，同时训练成本降低87%。

Conclusion: SCIR框架不仅提升了IE系统的灵活性和准确性，还为轻量级高效IE范式开辟了新途径，解决了训练成本高和偏好对齐的挑战。

Abstract: Although Large language Model (LLM)-powered information extraction (IE) systems have shown impressive capabilities, current fine-tuning paradigms face two major limitations: high training costs and difficulties in aligning with LLM preferences. To address these issues, we propose a novel universal IE paradigm, the Self-Correcting Iterative Refinement (SCIR) framework, along with a Multi-task Bilingual (Chinese-English) Self-Correcting (MBSC) dataset containing over 100,000 entries. The SCIR framework achieves plug-and-play compatibility with existing LLMs and IE systems through its Dual-Path Self-Correcting module and feedback-driven optimization, thereby significantly reducing training costs. Concurrently, the MBSC dataset tackles the challenge of preference alignment by indirectly distilling GPT-4's capabilities into IE result detection models. Experimental results demonstrate that SCIR outperforms state-of-the-art IE methods across three key tasks: named entity recognition, relation extraction, and event extraction, achieving a 5.27 percent average improvement in span-based Micro-F1 while reducing training costs by 87 percent compared to baseline approaches. These advancements not only enhance the flexibility and accuracy of IE systems but also pave the way for lightweight and efficient IE paradigms.

</details>


### [44] [Can GPT replace human raters? Validity and reliability of machine-generated norms for metaphors](https://arxiv.org/abs/2512.12444)
*Veronica Mangiaterra,Hamad Al-Azary,Chiara Barattieri di San Pietro,Paolo Canal,Valentina Bambini*

Main category: cs.CL

Relevance: 75.0

TL;DR: 评估GPT模型在生成隐喻属性评分（熟悉度、可理解性、意象性）方面的有效性和可靠性，发现较大模型与人类评分相关性良好，能有效预测行为反应和脑电信号，可作为人类评分的替代或补充。


<details>
  <summary>Details</summary>
Motivation: 随着LLMs在科学研究中的应用增加，其可信度问题变得至关重要。在心理语言学中，LLMs已被用于自动扩充人类评分数据集，但在复杂项目（如隐喻）评分方面的表现尚未探索。本研究旨在评估GPT模型生成隐喻属性评分的有效性和可靠性。

Method: 使用三个GPT模型对687个来自意大利比喻档案和三个英语研究的隐喻项目生成熟悉度、可理解性和意象性评分。通过与人脑数据对齐程度以及预测行为和脑电反应的能力进行全面验证，包括相关性分析和预测性能评估。

Result: 机器生成评分与人类评分呈正相关：熟悉度评分在英语和意大利语隐喻中达到中等至强相关（高感觉运动负荷的隐喻相关性减弱）；意象性在英语中呈中等相关，意大利语中呈中等至强相关；可理解性在英语隐喻中表现出最强相关性。较大模型表现优于较小模型，机器生成评分能显著预测反应时间和脑电振幅，强度与人类评分相当，且跨独立会话的GPT评分高度稳定。

Conclusion: GPT模型（尤其是较大模型）可以有效地替代或补充人类被试进行隐喻属性评分，但在处理隐喻意义的常规性和多模态方面与人类对齐较差，需要仔细考虑刺激性质。

Abstract: As Large Language Models (LLMs) are increasingly being used in scientific research, the issue of their trustworthiness becomes crucial. In psycholinguistics, LLMs have been recently employed in automatically augmenting human-rated datasets, with promising results obtained by generating ratings for single words. Yet, performance for ratings of complex items, i.e., metaphors, is still unexplored. Here, we present the first assessment of the validity and reliability of ratings of metaphors on familiarity, comprehensibility, and imageability, generated by three GPT models for a total of 687 items gathered from the Italian Figurative Archive and three English studies. We performed a thorough validation in terms of both alignment with human data and ability to predict behavioral and electrophysiological responses. We found that machine-generated ratings positively correlated with human-generated ones. Familiarity ratings reached moderate-to-strong correlations for both English and Italian metaphors, although correlations weakened for metaphors with high sensorimotor load. Imageability showed moderate correlations in English and moderate-to-strong in Italian. Comprehensibility for English metaphors exhibited the strongest correlations. Overall, larger models outperformed smaller ones and greater human-model misalignment emerged with familiarity and imageability. Machine-generated ratings significantly predicted response times and the EEG amplitude, with a strength comparable to human ratings. Moreover, GPT ratings obtained across independent sessions were highly stable. We conclude that GPT, especially larger models, can validly and reliably replace - or augment - human subjects in rating metaphor properties. Yet, LLMs align worse with humans when dealing with conventionality and multimodal aspects of metaphorical meaning, calling for careful consideration of the nature of stimuli.

</details>


### [45] [The American Ghost in the Machine: How language models align culturally and the effects of cultural prompting](https://arxiv.org/abs/2512.12488)
*James Luther,Donald Brown*

Main category: cs.CL

Relevance: 75.0

TL;DR: 该研究使用霍夫斯泰德文化维度理论评估了8个主流LLM的文化对齐性，发现多数模型默认偏向美国文化，通过文化提示可以调整模型向目标文化对齐，但对中国和日本文化的对齐效果较差。


<details>
  <summary>Details</summary>
Motivation: 随着生成式大语言模型在人机交互中的广泛应用，模型的文化对齐性成为一个重要研究问题。不同文化背景的用户需要模型能够理解和适应其文化价值观，但目前缺乏对主流LLM文化对齐性的系统评估。

Method: 使用VSM13国际调查和霍夫斯泰德文化维度理论，评估了DeepSeek-V3、GPT-5、Claude Opus 4等8个主流LLM的文化对齐性。通过文化提示技术（在系统提示中指定目标国家）测试模型向中国、法国、印度、伊朗、日本和美国等6个国家文化的适应性。

Result: 1. 未指定文化时，8个模型中有多数偏向美国文化
2. 使用文化提示后，8个模型中有7个能够向目标文化方向调整
3. 模型对中国和日本文化的对齐效果较差，即使DeepSeek模型源自中国公司

Conclusion: LLM存在显著的文化偏见，默认偏向美国文化。文化提示是有效的调整方法，但模型对某些文化（特别是中国和日本）的适应性仍有局限，需要进一步改进模型的文化对齐能力。

Abstract: Culture is the bedrock of human interaction; it dictates how we perceive and respond to everyday interactions. As the field of human-computer interaction grows via the rise of generative Large Language Models (LLMs), the cultural alignment of these models become an important field of study. This work, using the VSM13 International Survey and Hofstede's cultural dimensions, identifies the cultural alignment of popular LLMs (DeepSeek-V3, V3.1, GPT-5, GPT-4.1, GPT-4, Claude Opus 4, Llama 3.1, and Mistral Large). We then use cultural prompting, or using system prompts to shift the cultural alignment of a model to a desired country, to test the adaptability of these models to other cultures, namely China, France, India, Iran, Japan, and the United States. We find that the majority of the eight LLMs tested favor the United States when the culture is not specified, with varying results when prompted for other cultures. When using cultural prompting, seven of the eight models shifted closer to the expected culture. We find that models had trouble aligning with Japan and China, despite two of the models tested originating with the Chinese company DeepSeek.

</details>


### [46] [HyperEdit: Unlocking Instruction-based Text Editing in LLMs via Hypernetworks](https://arxiv.org/abs/2512.12544)
*Yiming Zeng,Jinghan Cao,Zexin Li,Wanhao Yu,Zhankai Ye,Dawei Xiang,Ting Hua,Xin Liu,Shangqian Gao,Tingting Yu*

Main category: cs.CL

Relevance: 75.0

TL;DR: HyperEdit提出了一种基于超网络的动态适应方法和差异感知正则化，用于改进基于指令的文本编辑任务，在修改区域上比现有方法提升9%-30%的BLEU分数。


<details>
  <summary>Details</summary>
Motivation: 基于指令的文本编辑在实际应用中越来越重要（如代码编辑器Cursor），但大型语言模型在这方面表现不佳。编辑任务需要忠实执行用户指令同时保持未修改内容不变，现有方法将编辑视为通用文本生成，导致两个关键问题：难以忠实对齐多样化的用户意图，以及经常过度编辑未改变区域。

Method: 1. 基于超网络的动态适应：生成特定请求的参数，使模型能够根据每个指令定制编辑策略。2. 差异感知正则化：将监督集中在修改的文本片段上，防止过度编辑同时确保精确的最小化更改。

Result: HyperEdit在修改区域上比最先进的基线方法相对提升了9%-30%的BLEU分数，尽管只使用了30亿参数。

Conclusion: HyperEdit通过动态适应和差异感知正则化有效解决了基于指令的文本编辑中的两个关键挑战，显著提升了编辑质量和忠实度。

Abstract: Instruction-based text editing is increasingly critical for real-world applications such as code editors (e.g., Cursor), but Large Language Models (LLMs) continue to struggle with this task. Unlike free-form generation, editing requires faithfully implementing user instructions while preserving unchanged content, as even minor unintended modifications can break functionality. Existing approaches treat editing as generic text generation, leading to two key failures: they struggle to faithfully align edits with diverse user intents, and they often over-edit unchanged regions. We propose HyperEdit to address both issues. First, we introduce hypernetwork-based dynamic adaptation that generates request-specific parameters, enabling the model to tailor its editing strategy to each instruction. Second, we develop difference-aware regularization that focuses supervision on modified spans, preventing over-editing while ensuring precise, minimal changes. HyperEdit achieves a 9%--30% relative improvement in BLEU on modified regions over state-of-the-art baselines, despite utilizing only 3B parameters.

</details>


### [47] [LexRel: Benchmarking Legal Relation Extraction for Chinese Civil Cases](https://arxiv.org/abs/2512.12643)
*Yida Cai,Ranjuexiao Hu,Huiyuan Xie,Chenyang Li,Yun Liu,Yuxiao Ye,Zhenghao Liu,Weixing Shen,Zhiyuan Liu*

Main category: cs.CL

Relevance: 75.0

TL;DR: 本文提出了LexRel，一个用于中文民事案件法律关系提取的专家标注基准，并评估了LLMs在此任务上的表现，发现当前LLMs在准确识别民事法律关系方面存在显著局限性。


<details>
  <summary>Details</summary>
Motivation: 法律关系是民法体系中的重要分析框架，但在法律人工智能领域，中文民事案件的法律关系研究仍不足，主要原因是缺乏全面的模式定义。

Method: 首先提出了包含层次分类和论元定义的综合模式，然后基于此模式定义了法律关系提取任务，并创建了LexRel专家标注基准，用于评估最先进的LLMs在法律关系提取上的表现。

Result: 评估显示当前LLMs在准确识别民事法律关系方面存在显著局限性。同时证明，融入法律关系信息能持续提升其他下游法律AI任务的性能。

Conclusion: 法律关系提取是法律AI的重要任务，当前LLMs在此领域仍有很大提升空间，而法律关系信息的融入能有效提升法律AI系统的整体性能。

Abstract: Legal relations form a highly consequential analytical framework of civil law system, serving as a crucial foundation for resolving disputes and realizing values of the rule of law in judicial practice. However, legal relations in Chinese civil cases remain underexplored in the field of legal artificial intelligence (legal AI), largely due to the absence of comprehensive schemas. In this work, we firstly introduce a comprehensive schema, which contains a hierarchical taxonomy and definitions of arguments, for AI systems to capture legal relations in Chinese civil cases. Based on this schema, we then formulate legal relation extraction task and present LexRel, an expert-annotated benchmark for legal relation extraction in Chinese civil law. We use LexRel to evaluate state-of-the-art large language models (LLMs) on legal relation extractions, showing that current LLMs exhibit significant limitations in accurately identifying civil legal relations. Furthermore, we demonstrate that incorporating legal relations information leads to consistent performance gains on other downstream legal AI tasks.

</details>


### [48] [MiniLingua: A Small Open-Source LLM for European Languages](https://arxiv.org/abs/2512.13298)
*Anna Aksenova,Boris Zverkov,Nicola Dainese,Alexander Nikitin,Pekka Marttinen*

Main category: cs.CL

Relevance: 75.0

TL;DR: MiniLingua是一个10亿参数的多语言开源LLM，专门为13种欧洲语言训练，在指令跟随能力上优于更大训练预算的EuroLLM，并在开放生成任务上与先进模型保持竞争力。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型存在计算成本高、隐私问题和英语中心化训练的限制。小型高效模型（约10亿参数）已显示出强大性能并支持设备端使用，但需要专门的多语言解决方案。

Method: 从零开始训练10亿参数的多语言开源LLM，覆盖13种欧洲语言，平衡语言覆盖范围和指令跟随能力，包括指令调优版本。

Result: 指令调优版MiniLingua在摘要、分类、开放/封闭式问答任务上优于训练预算更大的EuroLLM，在开放生成任务上与先进SOTA模型保持竞争力。

Conclusion: 小型高效的多语言LLM可以在有限参数下实现强大性能，为设备端应用提供可行方案，开源发布促进社区发展。

Abstract: Large language models are powerful but often limited by high computational cost, privacy concerns, and English-centric training. Recent progress demonstrates that small, efficient models with around one billion parameters can deliver strong results and enable on-device use. This paper introduces MiniLingua, a multilingual open-source LLM of one billion parameters trained from scratch for 13 European languages, designed to balance coverage and instruction-following capabilities. Based on evaluation results, the instruction-tuned version of MiniLingua outperforms EuroLLM, a model with a similar training approach but a larger training budget, on summarization, classification and both open- and closed-book question answering. Moreover, it remains competitive with more advanced state-of-the-art models on open-ended generation tasks. We release model weights, tokenizer and source code used for data processing and model training.

</details>


### [49] [FIN-bench-v2: A Unified and Robust Benchmark Suite for Evaluating Finnish Large Language Models](https://arxiv.org/abs/2512.13330)
*Joona Kytöniemi,Jousia Piha,Akseli Reunamo,Fedor Vitiugin,Farrokh Mehryary,Sampo Pyysalo*

Main category: cs.CL

Relevance: 75.0

TL;DR: FIN-bench-v2是一个统一的芬兰语大语言模型评估基准套件，整合了多个芬兰语版本的广泛使用基准，覆盖阅读理解、常识推理、情感分析、世界知识和对齐等多个任务类型。


<details>
  <summary>Details</summary>
Motivation: 为芬兰语大语言模型开发一个统一的评估基准，解决现有芬兰语评估资源分散、格式不一致的问题，并提供经过质量控制的机器翻译数据集。

Method: 1) 整合芬兰语版本的广泛使用基准并扩展原始FIN-bench；2) 将所有数据集转换为HuggingFace格式，包含完形填空和多项选择两种提示格式；3) 使用2.15B参数解码器模型通过学习曲线分析任务质量指标（单调性、信噪比、非随机性能、模型排序一致性）；4) 对机器翻译资源进行人工标注或审核；5) 评估更大规模的指令调优模型。

Result: 创建了一个包含多项选择和生成任务的统一芬兰语评估基准，所有任务都满足质量筛选标准，提供了多种提示变体，并通过开源代码库公开了所有数据集、提示和评估配置。

Conclusion: FIN-bench-v2为芬兰语大语言模型评估提供了一个全面、高质量、标准化的基准套件，有助于推动芬兰语NLP研究的发展。

Abstract: We introduce FIN-bench-v2, a unified benchmark suite for evaluating large language models in Finnish. FIN-bench-v2 consolidates Finnish versions of widely used benchmarks together with an updated and expanded version of the original FIN-bench into a single, consistently formatted collection, covering multiple-choice and generative tasks across reading comprehension, commonsense reasoning, sentiment analysis, world knowledge, and alignment. All datasets are converted to HuggingFace Datasets, which include both cloze and multiple-choice prompt formulations with five variants per task, and we incorporate human annotation or review for machine-translated resources such as GoldenSwag and XED. To select robust tasks, we pretrain a set of 2.15B-parameter decoder-only models and use their learning curves to compute monotonicity, signal-to-noise, non-random performance, and model ordering consistency, retaining only tasks that satisfy all criteria. We further evaluate a set of larger instruction-tuned models to characterize performance across tasks and prompt formulations. All datasets, prompts, and evaluation configurations are publicly available via our fork of the Language Model Evaluation Harness at https://github.com/LumiOpen/lm-evaluation-harness. Supplementary resources are released in a separate repository at https://github.com/TurkuNLP/FIN-bench-v2.

</details>


### [50] [Large language models are not about language](https://arxiv.org/abs/2512.13441)
*Johan J. Bolhuis,Andrea Moro,Stephen Crain,Sandiway Fong*

Main category: cs.CL

Relevance: 75.0

TL;DR: 该论文认为大语言模型对语言学无用，因为它们是基于大量数据的概率模型，而人类语言基于内在的计算系统生成层次化思维结构


<details>
  <summary>Details</summary>
Motivation: 论文旨在批判当前大语言模型在语言学领域的应用，强调LLMs与人类语言本质的根本差异。作者认为LLMs只是基于统计的模式匹配工具，而人类语言基于内在的递归计算系统

Method: 通过理论分析和对比研究方法，比较LLMs的统计学习范式与人类语言的内在计算系统。强调人类语言习得只需少量输入，而LLMs需要海量数据

Result: 论证了LLMs无法真正理解语言结构，只是表面模式匹配。人类语言系统能够区分真实语言和不可能语言，而LLMs缺乏这种内在的语法判断能力

Conclusion: 大语言模型对语言学理论发展没有实质性贡献，因为它们无法揭示人类语言的内在计算本质，只是数据驱动的统计工具

Abstract: Large Language Models are useless for linguistics, as they are probabilistic models that require a vast amount of data to analyse externalized strings of words. In contrast, human language is underpinned by a mind-internal computational system that recursively generates hierarchical thought structures. The language system grows with minimal external input and can readily distinguish between real language and impossible languages.

</details>


### [51] [Beyond surface form: A pipeline for semantic analysis in Alzheimer's Disease detection from spontaneous speech](https://arxiv.org/abs/2512.13685)
*Dylan Phelps,Rodrigo Wilkens,Edward Gow-Smith,Lilian Hubner,Bárbara Malcorra,César Rennó-Costa,Marco Idiart,Maria-Cruz Villa-Uriol,Aline Villavicencio*

Main category: cs.CL

Relevance: 75.0

TL;DR: 论文提出一种新方法，通过改变文本表面形式（语法和词汇）但保留语义内容，来评估语言模型在阿尔茨海默病检测中是否真正捕捉语义特征而非表面模式。


<details>
  <summary>Details</summary>
Motivation: 当前基于语言模型的AD筛查工具缺乏可解释性，难以区分真正的认知衰退语言标记与表面文本模式。需要评估语言模型是否真正能表示底层语义指标，而非仅依赖表面相关性。

Method: 提出新颖的文本表面形式变换方法：改变语法和词汇但保持语义内容不变。通过低BLEU/chrF分数（表明结构词汇变化）和高语义相似度分数（表明语义保留）来验证变换效果。使用生成模型测试图片描述是否包含足够细节重建原图。

Result: 变换显著改变了文本结构和词汇内容，但保留了底层语义。模型在变换文本上的表现与原始文本相似（macro-F1仅有微小偏差），表明模型确实使用了语义信息。图像基础变换添加大量噪声降低了分类准确率。

Conclusion: 仅使用语义信息，基于语言模型的分类器仍能检测AD。该方法提供了一种新视角来分析影响模型预测的特征，能够移除可能的虚假相关性。研究表明难以检测的语义损伤可以被识别，为早期检测系统开辟新途径。

Abstract: Alzheimer's Disease (AD) is a progressive neurodegenerative condition that adversely affects cognitive abilities. Language-related changes can be automatically identified through the analysis of outputs from linguistic assessment tasks, such as picture description. Language models show promise as a basis for screening tools for AD, but their limited interpretability poses a challenge in distinguishing true linguistic markers of cognitive decline from surface-level textual patterns. To address this issue, we examine how surface form variation affects classification performance, with the goal of assessing the ability of language models to represent underlying semantic indicators. We introduce a novel approach where texts surface forms are transformed by altering syntax and vocabulary while preserving semantic content. The transformations significantly modify the structure and lexical content, as indicated by low BLEU and chrF scores, yet retain the underlying semantics, as reflected in high semantic similarity scores, isolating the effect of semantic information, and finding models perform similarly to if they were using the original text, with only small deviations in macro-F1. We also investigate whether language from picture descriptions retains enough detail to reconstruct the original image using generative models. We found that image-based transformations add substantial noise reducing classification accuracy. Our methodology provides a novel way of looking at what features influence model predictions, and allows the removal of possible spurious correlations. We find that just using semantic information, language model based classifiers can still detect AD. This work shows that difficult to detect semantic impairment can be identified, addressing an overlooked feature of linguistic deterioration, and opening new pathways for early detection systems.

</details>


### [52] [NagaNLP: Bootstrapping NLP for Low-Resource Nagamese Creole with Human-in-the-Loop Synthetic Data](https://arxiv.org/abs/2512.12537)
*Agniva Maiti,Manya Pandey,Murari Mandal*

Main category: cs.CL

Relevance: 65.0

TL;DR: NagaNLP：针对Nagamese语言的综合开源工具包，通过LLM驱动的人工验证合成数据生成方法，创建了对话数据集和标注语料，并训练了判别式和生成式模型，显著提升了该低资源语言的NLP能力。


<details>
  <summary>Details</summary>
Motivation: 世界上绝大多数语言（特别是像Nagamese这样的克里奥尔语）在自然语言处理中严重缺乏资源，这阻碍了它们在数字技术中的代表性。需要为这些低资源语言开发有效的解决方案。

Method: 采用多阶段流水线方法：专家指导的LLM（Gemini）生成候选语料库，然后由母语者进行精炼和标注。这种合成-混合方法产生了10K对对话数据集和高质量标注语料库。训练了判别式模型（XLM-RoBERTa-base）和生成式模型（Llama-3.2-3B Instruct）。

Result: 微调的XLM-RoBERTa-base模型在词性标注上达到93.81%准确率（0.90 F1-Macro），命名实体识别达到0.75 F1-Macro，显著优于零样本基线。微调的Llama-3.2-3B Instruct模型（NagaLLaMA）在对话任务上表现优异，困惑度达到3.85，比少样本对应模型（96.76）提升了一个数量级。

Conclusion: NagaNLP工具包为先前服务不足的语言提供了基础资源，并为减少其他低资源语境中的数据稀缺性提供了可复现的框架。LLM驱动的合成数据生成结合人工验证是解决低资源语言NLP挑战的有效方法。

Abstract: The vast majority of the world's languages, particularly creoles like Nagamese, remain severely under-resourced in Natural Language Processing (NLP), creating a significant barrier to their representation in digital technology. This paper introduces NagaNLP, a comprehensive open-source toolkit for Nagamese, bootstrapped through a novel methodology that relies on LLM-driven but human-validated synthetic data generation. We detail a multi-stage pipeline where an expert-guided LLM (Gemini) generates a candidate corpus, which is then refined and annotated by native speakers. This synthetic-hybrid approach yielded a 10K pair conversational dataset and a high-quality annotated corpus for foundational tasks. To assess the effectiveness of our methodology, we trained both discriminative and generative models. Our fine-tuned XLM-RoBERTa-base model establishes a new benchmark for Nagamese, achieving a 93.81\% accuracy (0.90 F1-Macro) on Part-of-Speech tagging and a 0.75 F1-Macro on Named Entity Recognition, massively outperforming strong zero-shot baselines. Furthermore, we fine-tuned a Llama-3.2-3B Instruct model, named NagaLLaMA, which demonstrates superior performance on conversational tasks, achieving a Perplexity of 3.85, an order of magnitude improvement over its few-shot counterpart (96.76). We release the NagaNLP toolkit, including all datasets, models, and code, providing a foundational resource for a previously underserved language and a reproducible framework for reducing data scarcity in other low-resource contexts.

</details>


### [53] [Understanding Syllogistic Reasoning in LLMs from Formal and Natural Language Perspectives](https://arxiv.org/abs/2512.12620)
*Aheli Poddar,Saptarshi Sahoo,Sujata Ghosh*

Main category: cs.CL

Relevance: 65.0

TL;DR: 该论文研究LLMs在三段论推理中的表现，从逻辑和自然语言两个角度分析14个大语言模型的推理能力，探讨LLMs是否正在成为形式推理机制而非模拟人类推理


<details>
  <summary>Details</summary>
Motivation: 研究LLMs在三段论推理中的基本推理能力，探索LLMs研究的发展方向，了解LLMs是正在成为形式推理机制还是能够体现人类推理的细微差别

Method: 使用14个大语言模型，从符号推理和自然语言理解两个角度研究它们的三段论推理能力，分析逻辑推理和自然语言处理的表现

Result: 推理机制并非所有LLMs的统一涌现特性，但某些模型在符号推理上表现出完美性能，引发对LLMs是否正成为形式推理机制的疑问

Conclusion: LLMs在三段论推理中表现出混合能力，某些模型展现出强大的形式推理能力，但整体上并非所有模型都能体现人类推理的细微差别

Abstract: We study syllogistic reasoning in LLMs from the logical and natural language perspectives. In process, we explore fundamental reasoning capabilities of the LLMs and the direction this research is moving forward. To aid in our studies, we use 14 large language models and investigate their syllogistic reasoning capabilities in terms of symbolic inferences as well as natural language understanding. Even though this reasoning mechanism is not a uniform emergent property across LLMs, the perfect symbolic performances in certain models make us wonder whether LLMs are becoming more and more formal reasoning mechanisms, rather than making explicit the nuances of human reasoning.

</details>


### [54] [What Matters in Evaluating Book-Length Stories? A Systematic Study of Long Story Evaluation](https://arxiv.org/abs/2512.12839)
*Dingyi Yang,Qin Jin*

Main category: cs.CL

Relevance: 65.0

TL;DR: 提出了首个大规模长故事自动评估基准LongStoryEval，包含600本平均12.1万token的书籍，并开发了高效的8B模型NovelCritique，在人类评估对齐上优于GPT-4o。


<details>
  <summary>Details</summary>
Motivation: 针对长文本（>10万token）自动评估的挑战性领域，缺乏系统研究和有效方法，特别是书籍长度故事的评估标准和评估方法。

Method: 1) 构建LongStoryEval基准：600本新出版书籍，平均12.1万token，包含评分和按评估维度组织的读者评论；2) 分析用户提到的评估维度，提出评估标准结构；3) 比较三种评估方法：基于聚合、增量更新和基于摘要的评估；4) 提出NovelCritique模型：8B参数，利用高效的摘要框架进行多维度故事评审和评分。

Result: 1) 识别出8个最重要的顶级评估标准；2) 发现聚合式和摘要式评估效果更好，前者在细节评估上更优，后者效率更高；3) NovelCritique在人类评估对齐上优于GPT-4o等商业模型。

Conclusion: 为长故事自动评估提供了首个大规模基准和有效方法，提出的NovelCritique模型在效率和效果上都有优势，为长文本评估开辟了新方向。

Abstract: In this work, we conduct systematic research in a challenging area: the automatic evaluation of book-length stories (>100K tokens). Our study focuses on two key questions: (1) understanding which evaluation aspects matter most to readers, and (2) exploring effective methods for evaluating lengthy stories. We introduce the first large-scale benchmark, LongStoryEval, comprising 600 newly published books with an average length of 121K tokens (maximum 397K). Each book includes its average rating and multiple reader reviews, presented as critiques organized by evaluation aspects. By analyzing all user-mentioned aspects, we propose an evaluation criteria structure and conduct experiments to identify the most significant aspects among the 8 top-level criteria. For evaluation methods, we compare the effectiveness of three types: aggregation-based, incremental-updated, and summary-based evaluations. Our findings reveal that aggregation- and summary-based evaluations perform better, with the former excelling in detail assessment and the latter offering greater efficiency. Building on these insights, we further propose NovelCritique, an 8B model that leverages the efficient summary-based framework to review and score stories across specified aspects. NovelCritique outperforms commercial models like GPT-4o in aligning with human evaluations. Our datasets and codes are available at https://github.com/DingyiYang/LongStoryEval.

</details>


### [55] [Building from Scratch: A Multi-Agent Framework with Human-in-the-Loop for Multilingual Legal Terminology Mapping](https://arxiv.org/abs/2512.12950)
*Lingyi Meng,Maolin Liu,Hao Wang,Yilan Cheng,Qi Yang,Idlkaid Mohanmmed*

Main category: cs.CL

Relevance: 65.0

TL;DR: 本文提出了一种人机协作的多智能体框架，用于构建多语言法律术语数据库，特别针对中、日、英三语法律术语的精确映射。


<details>
  <summary>Details</summary>
Motivation: 准确映射跨语言法律术语（特别是中文和日语这种存在大量同形异义词的语言对）具有重大挑战，现有资源和标准化工具有限。需要解决法律术语跨语言映射的准确性和一致性问题。

Method: 采用人机协作的多智能体框架：AI智能体处理OCR、文本分割、语义对齐和初始术语提取等重复性任务；法律领域专家提供监督、审查和上下文知识判断。流程包括原始文档预处理、文章级对齐、术语提取、映射和质量保证。

Result: 使用包含35部关键中国法规及其英、日翻译的三语平行语料库进行测试，结果表明这种人机协作的多智能体工作流程不仅提高了多语言法律术语映射的精确性和一致性，相比传统手动方法还具有更好的可扩展性。

Conclusion: 人机协作的多智能体框架是构建高质量多语言法律术语数据库的有效方法，特别适用于存在同形异义词挑战的语言对，能够平衡自动化效率和专家知识。

Abstract: Accurately mapping legal terminology across languages remains a significant challenge, especially for language pairs like Chinese and Japanese, which share a large number of homographs with different meanings. Existing resources and standardized tools for these languages are limited. To address this, we propose a human-AI collaborative approach for building a multilingual legal terminology database, based on a multi-agent framework. This approach integrates advanced large language models and legal domain experts throughout the entire process-from raw document preprocessing, article-level alignment, to terminology extraction, mapping, and quality assurance. Unlike a single automated pipeline, our approach places greater emphasis on how human experts participate in this multi-agent system. Humans and AI agents take on different roles: AI agents handle specific, repetitive tasks, such as OCR, text segmentation, semantic alignment, and initial terminology extraction, while human experts provide crucial oversight, review, and supervise the outputs with contextual knowledge and legal judgment. We tested the effectiveness of this framework using a trilingual parallel corpus comprising 35 key Chinese statutes, along with their English and Japanese translations. The experimental results show that this human-in-the-loop, multi-agent workflow not only improves the precision and consistency of multilingual legal terminology mapping but also offers greater scalability compared to traditional manual methods.

</details>


### [56] [Authors Should Annotate](https://arxiv.org/abs/2512.12976)
*Marcus Ma,Cole Johnson,Nolan Bridges,Jackson Trager,Georgios Chochlakis,Shrikanth Narayanan*

Main category: cs.CL

Relevance: 65.0

TL;DR: 作者标注：让文档作者在创作时直接标注数据的新方法，相比第三方标注在主观特征上质量更高、更快、更便宜，并在产品推荐任务中实现了534%的点击率提升


<details>
  <summary>Details</summary>
Motivation: 当前文本标注主要依赖第三方标注员，但对于主观特征（如情感、信念等自我中心特征），直接从文档作者获取信息比第三方代理更优。作者标注旨在解决主观特征标注的质量问题。

Method: 1. 与拥有超过10,000用户的商业聊天机器人合作，部署作者标注系统用于产品推荐相关的主观特征标注
2. 系统识别任务相关查询，实时生成标注问题，记录作者回答
3. 训练并部署在线学习模型架构，持续从作者标注中改进产品推荐

Result: 1. 产品推荐模型相比行业广告基线实现了534%的点击率提升
2. 与传统情感分析标注方法相比，作者标注质量更高、获取速度更快、成本更低
3. 验证了自我中心主观信念标注由作者标注比第三方标注质量显著更高

Conclusion: 作者标注是一种高质量、高效、经济的主观特征标注方法，特别适用于自我中心特征。研究团队为科研社区发布了作者标注服务（academic.echollm.io）以促进更广泛采用。

Abstract: The status quo for labeling text is third-party annotation, but there are many cases where information directly from the document's source would be preferable over a third-person proxy, especially for egocentric features like sentiment and belief. We introduce author labeling, an annotation technique where the writer of the document itself annotates the data at the moment of creation. We collaborate with a commercial chatbot with over 10,000 users to deploy an author labeling annotation system for subjective features related to product recommendation. This system identifies task-relevant queries, generates on-the-fly labeling questions, and records authors' answers in real time. We train and deploy an online-learning model architecture for product recommendation that continuously improves from author labeling and find it achieved a 534% increase in click-through rate compared to an industry advertising baseline running concurrently. We then compare the quality and practicality of author labeling to three traditional annotation approaches for sentiment analysis and find author labeling to be higher quality, faster to acquire, and cheaper. These findings reinforce existing literature that annotations, especially for egocentric and subjective beliefs, are significantly higher quality when labeled by the author rather than a third party. To facilitate broader scientific adoption, we release an author labeling service for the research community at academic.echollm.io.

</details>


### [57] [Integrating Causal Reasoning into Automated Fact-Checking](https://arxiv.org/abs/2512.13286)
*Youssra Rebboud,Pasquale Lisena,Raphael Troncy*

Main category: cs.CL

Relevance: 65.0

TL;DR: 提出一种结合事件关系提取、语义相似度计算和基于规则推理的方法，用于检测声明与证据之间事件链的逻辑不一致性，为事实核查建立首个细粒度因果事件关系基线。


<details>
  <summary>Details</summary>
Motivation: 当前自动事实核查方法缺乏专门的因果推理能力，无法有效检测声明中的错误因果关系，错失了语义丰富的可解释性机会。需要将细粒度因果事件关系整合到事实核查中。

Method: 结合事件关系提取、语义相似度计算和基于规则的推理，检测声明与证据之间事件链的逻辑不一致性。通过提取事件关系，计算语义相似度，应用规则推理来识别因果逻辑矛盾。

Result: 在两个事实核查数据集上评估，该方法为整合细粒度因果事件关系到事实核查建立了首个基线，并增强了裁决预测的可解释性。

Conclusion: 提出的方法成功将因果推理整合到事实核查中，填补了当前方法在因果分析方面的空白，为更语义丰富、可解释的事实核查系统奠定了基础。

Abstract: In fact-checking applications, a common reason to reject a claim is to detect the presence of erroneous cause-effect relationships between the events at play. However, current automated fact-checking methods lack dedicated causal-based reasoning, potentially missing a valuable opportunity for semantically rich explainability. To address this gap, we propose a methodology that combines event relation extraction, semantic similarity computation, and rule-based reasoning to detect logical inconsistencies between chains of events mentioned in a claim and in an evidence. Evaluated on two fact-checking datasets, this method establishes the first baseline for integrating fine-grained causal event relationships into fact-checking and enhance explainability of verdict prediction.

</details>


### [58] [Temporal Tokenization Strategies for Event Sequence Modeling with Large Language Models](https://arxiv.org/abs/2512.13618)
*Zefang Liu,Nam Nguyen,Yinzhu Quan,Austin Zhang*

Main category: cs.CL

Relevance: 65.0

TL;DR: 本文首次对事件序列的时间标记化策略进行实证研究，比较了五种编码方法在不同统计分布数据上的表现，发现没有单一最优策略，性能取决于标记器与数据统计特性的匹配程度。


<details>
  <summary>Details</summary>
Motivation: 在大型语言模型建模时间事件序列时，连续时间表示是一个关键但研究不足的挑战。现有方法如字节级表示或日历标记等策略，但最优方法仍不明确，特别是考虑到真实世界事件数据具有从平滑对数正态分布到离散尖峰模式等多样统计分布。

Method: 比较五种不同的时间标记化策略：朴素数字字符串、高精度字节级表示、人类语义日历标记、经典均匀分箱和自适应残差标量量化。通过在体现这些不同分布的真实世界数据集上微调LLMs来评估这些策略。

Result: 分析表明没有单一策略是普遍最优的；预测性能很大程度上取决于标记器与数据统计特性的对齐程度。对数基策略在偏斜分布上表现优异，而人类中心格式在混合模态中表现出鲁棒性。

Conclusion: 时间标记化策略的选择应基于数据的具体统计特性，需要根据数据分布特征选择适当的编码方法，而不是寻找通用解决方案。

Abstract: Representing continuous time is a critical and under-explored challenge in modeling temporal event sequences with large language models (LLMs). Various strategies like byte-level representations or calendar tokens have been proposed. However, the optimal approach remains unclear, especially given the diverse statistical distributions of real-world event data, which range from smooth log-normal to discrete, spiky patterns. This paper presents the first empirical study of temporal tokenization for event sequences, comparing distinct encoding strategies: naive numeric strings, high-precision byte-level representations, human-semantic calendar tokens, classic uniform binning, and adaptive residual scalar quantization. We evaluate these strategies by fine-tuning LLMs on real-world datasets that exemplify these diverse distributions. Our analysis reveals that no single strategy is universally superior; instead, prediction performance depends heavily on aligning the tokenizer with the data's statistical properties, with log-based strategies excelling on skewed distributions and human-centric formats proving robust for mixed modalities.

</details>


### [59] [Love First, Know Later: Persona-Based Romantic Compatibility Through LLM Text World Engines](https://arxiv.org/abs/2512.11844)
*Haoyang Shang,Zhengyang Yan,Xuan Liu*

Main category: cs.HC

Relevance: 65.0

TL;DR: 提出"先互动后评估"的匹配新范式，利用LLM作为文本世界引擎模拟互动，将兼容性评估转化为奖励建模问题，理论上证明当LLM策略逼近人类行为时匹配收敛于最优稳定匹配。


<details>
  <summary>Details</summary>
Motivation: 传统匹配系统基于静态档案比较，忽略了真实关系中互动动态的重要性。作者观察到关系质量往往取决于关键时刻的应对方式，因此需要一种能模拟互动动态的匹配方法。

Method: 使用LLM作为文本世界引擎，既扮演遵循行为策略的角色驱动代理，又作为建模互动动态的环境。将兼容性评估形式化为奖励建模问题：基于观察到的匹配结果，从模拟中学习预测人类偏好的信号。从关系心理学中提取关键时刻应对的数学假设。

Result: 理论上证明当LLM策略更好地逼近人类行为时，诱导的匹配收敛于最优稳定匹配。实证上在速配数据上验证初始化学反应预测，在离婚预测上验证长期稳定性评估。

Conclusion: 该范式实现了交互式、个性化的匹配系统，用户可以迭代优化其代理，为透明和交互式的兼容性评估开辟了新可能性。

Abstract: We propose Love First, Know Later: a paradigm shift in computational matching that simulates interactions first, then assesses compatibility. Instead of comparing static profiles, our framework leverages LLMs as text world engines that operate in dual capacity-as persona-driven agents following behavioral policies and as the environment modeling interaction dynamics. We formalize compatibility assessment as a reward-modeling problem: given observed matching outcomes, we learn to extract signals from simulations that predict human preferences. Our key insight is that relationships hinge on responses to critical moments-we translate this observation from relationship psychology into mathematical hypotheses, enabling effective simulation. Theoretically, we prove that as LLM policies better approximate human behavior, the induced matching converges to optimal stable matching. Empirically, we validate on speed dating data for initial chemistry and divorce prediction for long-term stability. This paradigm enables interactive, personalized matching systems where users iteratively refine their agents, unlocking future possibilities for transparent and interactive compatibility assessment.

</details>


### [60] [Fine-tuned LLM-based Code Migration Framework](https://arxiv.org/abs/2512.13515)
*Oleg Grynets,Vasyl Lyashkevych,Dmytro Baran,Maksym Orliansky,Taras Zelenyy,Markiian Leshchyshyn*

Main category: cs.SE

Relevance: 65.0

TL;DR: 提出一个结合传统软件工程与微调大语言模型的自动化代码迁移框架，专注于Oracle PL/SQL到PostgreSQL的转换，通过迭代优化降低语法错误率并提升特征对齐。


<details>
  <summary>Details</summary>
Motivation: 解决SQL系统迁移中的挑战，特别是Oracle PL/SQL到PostgreSQL的转换问题，传统方法难以处理语法映射、存储过程、触发器等复杂数据库元素的转换。

Method: 结合传统软件工程技术与微调大语言模型，采用迭代、可扩展的迁移框架。核心是微调LLM处理SQL代码转换，包括语法映射、差异解决和数据库元素优化。结合微调与提示工程，采用半监督错误分析和领域专家反馈。

Result: 显著降低语法错误率，提升迁移迭代中的特征对齐，通过数据集采样实现持续改进。框架实现精确特征映射、半自动化错误解决和数据驱动的优化循环，提高工作流效率。

Conclusion: 微调LLM在数据库迁移中至关重要，提出的框架能有效解决SQL系统转换的复杂问题，结合传统工程方法与AI技术实现高效、精确的迁移。

Abstract: The study presents the outcomes of research and experimental validation in the domain of automated codebase migration, with a focus on addressing challenges in transitioning SQL-based systems. The proposed method for migration essentially appears as a framework that leverages the best aspects of traditional software engineering techniques and provides an iterative, scalable, precise and efficient solution for modern database transformations. The central piece of the approach is the integration of a fine-tuned Large Language Model to address critical issues in SQL code conversion, such as syntax mapping, resolving discrepancies between Oracle PL/SQL and PostgreSQL, and optimising database elements such as stored procedures, triggers, views, and overall database logic. Thus, the method involves a trade-off between fine-tuning and prompt engineering. Special attention is given to a fine-tuning approach, which enhances the adaptability and compatibility with migration requirements across the entire database. According to the achieved results, fine-tuning plays a very important role. The study employs targeted evaluation methodologies along with computational metrics to measure the success of iterative conversion cycles. Core innovations include automated SQL feature detection, semi-supervised error analysis and integration of Subject Matter Experts feedback within a systematic migration workflow. The methodology achieves significant reductions in Syntax Error Rates, enhances feature alignment throughout migration iterations, and leverages dataset sampling to ensure continual improvement. By embedding GAI into the migration process, the framework facilitates precise feature mapping, semi-automated error resolution, and data-driven optimisation loops, improving workflow efficiency.

</details>


### [61] [Semantic Distance Measurement based on Multi-Kernel Gaussian Processes](https://arxiv.org/abs/2512.12238)
*Yinzhu Cheng,Haihua Xie,Yaqing Wang,Miao He,Mingming Sun*

Main category: cs.CL

Relevance: 45.0

TL;DR: 提出基于多核高斯过程（MK-GP）的语义距离度量方法，通过结合Matérn和多项式核函数自动学习参数，应用于大语言模型上下文学习中的细粒度情感分类任务。


<details>
  <summary>Details</summary>
Motivation: 传统语义距离度量方法通常是固定的，难以适应特定数据分布和任务需求。需要一种能够自动从数据中学习、适应任务要求的语义距离度量方法。

Method: 将文本的潜在语义函数建模为高斯过程，其协方差函数由Matérn核和多项式核组合而成。核参数在监督下从数据中自动学习，而非手工设计。该方法在大语言模型上下文学习框架下应用于细粒度情感分类。

Result: 实验结果表明，所提出的语义距离度量方法在细粒度情感分类任务中有效，证明了其优于传统固定语义距离方法的性能。

Conclusion: 基于多核高斯过程的语义距离度量方法能够自动适应数据和任务需求，为语义相似性计算提供了更灵活有效的解决方案。

Abstract: Semantic distance measurement is a fundamental problem in computational linguistics, providing a quantitative characterization of similarity or relatedness between text segments, and underpinning tasks such as text retrieval and text classification. From a mathematical perspective, a semantic distance can be viewed as a metric defined on a space of texts or on a representation space derived from them. However, most classical semantic distance methods are essentially fixed, making them difficult to adapt to specific data distributions and task requirements. In this paper, a semantic distance measure based on multi-kernel Gaussian processes (MK-GP) was proposed. The latent semantic function associated with texts was modeled as a Gaussian process, with its covariance function given by a combined kernel combining Matérn and polynomial components. The kernel parameters were learned automatically from data under supervision, rather than being hand-crafted. This semantic distance was instantiated and evaluated in the context of fine-grained sentiment classification with large language models under an in-context learning (ICL) setup. The experimental results demonstrated the effectiveness of the proposed measure.

</details>


### [62] [F5-TTS-RO: Extending F5-TTS to Romanian TTS via Lightweight Input Adaptation](https://arxiv.org/abs/2512.12297)
*Radu-Gabriel Chivereanu,Tiberiu Boros*

Main category: cs.CL

Relevance: 45.0

TL;DR: 为F5-TTS模型开发了一个轻量级输入级适配器，用于支持罗马尼亚语，同时保持原有的语音克隆、英语和中文能力。通过冻结原始权重并添加子网络，实现了罗马尼亚文本到连续表示的转换。


<details>
  <summary>Details</summary>
Motivation: 扩展F5-TTS模型的多语言能力，特别是支持罗马尼亚语，同时保持模型原有的语音克隆、英语和中文支持功能。通过轻量级适配器实现这一目标，避免重新训练整个模型。

Method: 采用冻结原始权重的方法，在文本编码器的文本嵌入矩阵后附加一个子网络作为适配器。使用F5-TTS中已有的ConvNeXt模块建模新字符级嵌入之间的依赖关系，作为"软"字母到声音层。

Result: 模型保持了语音克隆能力，并能在一定程度上实现罗马尼亚语-英语代码切换。但生成的罗马尼亚语仍带有英语口音特征。通过20名人类听众评估，在音频相似性、发音自然度和代码切换方面表现良好。

Conclusion: 轻量级输入级适配器是扩展TTS模型多语言能力的有效方法，能够保持原有功能的同时添加新语言支持。但需要进一步改进以减少残留的口音特征。

Abstract: This work introduces a lightweight input-level adapter for the F5-TTS model that enables Romanian Language support. To preserve the existing capabilities of the model (voice cloning, English and Chinese support), we keep the original weights frozen, append a sub-network to the model and train it as an extension for the textual embedding matrix of the text encoder. For simplicity, we rely on ConvNeXt module implemented in F5-TTS to also model the co-dependencies between the new character-level embeddings. The module serves as a ``soft`` letter-to-sound layer, converting Romanian text into a continuous representation that the F5-TTS model uses to produce naturally sounding Romanian utterances. We evaluate the model with a pool of 20 human listeners across three tasks: (a) audio similarity between reference and generated speech, (b) pronunciation and naturalness and (c) Romanian-English code-switching. The results indicate that our approach maintains voice cloning capabilities and enables, to a certain extent, code-switching within the same utterance; however, residual English accent characteristics remain. We open-source our code and provide example audio samples at https://github.com/racai-ro/Ro-F5TTS.

</details>


### [63] [Detecting Emotion Drift in Mental Health Text Using Pre-Trained Transformers](https://arxiv.org/abs/2512.13363)
*Shibani Sankpal*

Main category: cs.CL

Relevance: 45.0

TL;DR: 该研究提出"情绪漂移"概念，分析心理健康相关文本中情绪状态的变化，使用预训练Transformer模型在句子层面检测情绪并计算漂移分数，揭示情绪动态模式。


<details>
  <summary>Details</summary>
Motivation: 传统情感分析通常将整个文本分类为积极、消极或中性，忽略了文本内部情绪的细微变化。在心理健康相关消息中，情绪在单条消息内的变化（情绪漂移）对于理解用户情绪动态至关重要，但这一现象常被忽视。

Method: 使用预训练的Transformer模型（DistilBERT和RoBERTa）在句子层面检测情绪，计算情绪漂移分数来衡量情绪状态在文本中的变化程度。

Result: 研究揭示了心理健康对话中情绪升级或缓解的模式，提供了对情绪动态的深入洞察，展示了情绪漂移分析在理解文本情感变化方面的有效性。

Conclusion: 情绪漂移分析能够更好地理解文本中的情绪动态，特别是在心理健康领域。该方法可以应用于更广泛的内容分析，为情感理解和心理健康支持提供新视角。

Abstract: This study investigates emotion drift: the change in emotional state across a single text, within mental health-related messages. While sentiment analysis typically classifies an entire message as positive, negative, or neutral, the nuanced shift of emotions over the course of a message is often overlooked. This study detects sentence-level emotions and measures emotion drift scores using pre-trained transformer models such as DistilBERT and RoBERTa. The results provide insights into patterns of emotional escalation or relief in mental health conversations. This methodology can be applied to better understand emotional dynamics in content.

</details>


### [64] [Verifying Rumors via Stance-Aware Structural Modeling](https://arxiv.org/abs/2512.13559)
*Gibson Nkhata,Uttamasha Anjally Oyshi,Quan Mai,Susan Gauch*

Main category: cs.CL

Relevance: 45.0

TL;DR: 提出了一种基于立场感知结构建模的社交媒体谣言验证方法，通过编码帖子立场信号、按立场类别聚合回复嵌入，并引入立场分布和层次深度作为协变量，显著提升了谣言真实性预测性能。


<details>
  <summary>Details</summary>
Motivation: 社交媒体上的谣言验证对于减少虚假信息传播至关重要。现有模型难以同时捕捉语义内容、立场信息和对话结构，特别是在基于Transformer的编码器存在序列长度限制的情况下。

Method: 提出立场感知结构建模：1) 编码每个帖子及其立场信号；2) 按立场类别聚合回复嵌入，实现可扩展且语义丰富的线程表示；3) 引入立场分布和层次深度作为协变量，捕捉立场不平衡和回复深度的影响。

Result: 在基准数据集上的广泛实验表明，该方法在预测谣言真实性方面显著优于先前方法，并展示了在早期检测和跨平台泛化方面的多功能性。

Conclusion: 提出的立场感知结构建模方法有效解决了社交媒体谣言验证中语义、立场和结构信息的联合建模问题，为虚假信息检测提供了更强大的解决方案。

Abstract: Verifying rumors on social media is critical for mitigating the spread of false information. The stances of conversation replies often provide important cues to determine a rumor's veracity. However, existing models struggle to jointly capture semantic content, stance information, and conversation strructure, especially under the sequence length constraints of transformer-based encoders. In this work, we propose a stance-aware structural modeling that encodes each post in a discourse with its stance signal and aggregates reply embedddings by stance category enabling a scalable and semantically enriched representation of the entire thread. To enhance structural awareness, we introduce stance distribution and hierarchical depth as covariates, capturing stance imbalance and the influence of reply depth. Extensive experiments on benchmark datasets demonstrate that our approach significantly outperforms prior methods in the ability to predict truthfulness of a rumor. We also demonstrate that our model is versatile for early detection and cross-platfrom generalization.

</details>


### [65] [StruProKGR: A Structural and Probabilistic Framework for Sparse Knowledge Graph Reasoning](https://arxiv.org/abs/2512.12613)
*Yucan Guo,Saiping Guan,Miao Su,Zeya Zhao,Xiaolong Jin,Jiafeng Guo,Xueqi Cheng*

Main category: cs.CL

Relevance: 35.0

TL;DR: 提出StruProKGR框架，通过距离引导路径收集和概率路径聚合，实现稀疏知识图谱的高效可解释推理


<details>
  <summary>Details</summary>
Motivation: 现实世界中的知识图谱通常是稀疏的，现有基于路径的方法存在计算成本高、路径质量参差不齐、未能充分利用图结构信息等问题

Method: 1) 距离引导路径收集机制降低计算成本；2) 概率路径聚合利用结构信息，优先考虑相互增强的路径

Result: 在五个稀疏知识图谱推理基准测试中，StruProKGR在效果和效率上都超越了现有基于路径的方法

Conclusion: StruProKGR为稀疏知识图谱推理提供了有效、高效且可解释的解决方案

Abstract: Sparse Knowledge Graphs (KGs) are commonly encountered in real-world applications, where knowledge is often incomplete or limited. Sparse KG reasoning, the task of inferring missing knowledge over sparse KGs, is inherently challenging due to the scarcity of knowledge and the difficulty of capturing relational patterns in sparse scenarios. Among all sparse KG reasoning methods, path-based ones have attracted plenty of attention due to their interpretability. Existing path-based methods typically rely on computationally intensive random walks to collect paths, producing paths of variable quality. Additionally, these methods fail to leverage the structured nature of graphs by treating paths independently. To address these shortcomings, we propose a Structural and Probabilistic framework named StruProKGR, tailored for efficient and interpretable reasoning on sparse KGs. StruProKGR utilizes a distance-guided path collection mechanism to significantly reduce computational costs while exploring more relevant paths. It further enhances the reasoning process by incorporating structural information through probabilistic path aggregation, which prioritizes paths that reinforce each other. Extensive experiments on five sparse KG reasoning benchmarks reveal that StruProKGR surpasses existing path-based methods in both effectiveness and efficiency, providing an effective, efficient, and interpretable solution for sparse KG reasoning.

</details>


### [66] [Which Pieces Does Unigram Tokenization Really Need?](https://arxiv.org/abs/2512.12641)
*Sander Land,Yuval Pinter*

Main category: cs.CL

Relevance: 35.0

TL;DR: Unigram分词算法提供了比BPE更理论优雅的概率替代方案，但实现复杂限制了应用。本文提供了清晰的实现指南和参数选择建议，并提出了一个更简单的算法，在略微增加训练损失的情况下获得更好的压缩效果。


<details>
  <summary>Details</summary>
Motivation: Unigram分词算法虽然理论优雅，但实际实现复杂，目前仅限于SentencePiece包及其适配器使用，这限制了该算法的广泛采用。需要弥合理论与实践的差距。

Method: 1. 提供清晰的Unigram算法实现指南和参数选择建议；2. 提出一个更简单的算法变体，该变体接受略微更高的训练损失以换取更好的压缩效果。

Result: 1. 使Unigram算法更易于实现和应用；2. 提出的简化算法在压缩性能上有所提升，虽然训练损失略有增加。

Conclusion: 通过提供实现指南和简化算法，降低了Unigram分词算法的应用门槛，使其更易于在实际LLM训练中使用，有助于改进分词质量。

Abstract: The Unigram tokenization algorithm offers a probabilistic alternative to the greedy heuristics of Byte-Pair Encoding. Despite its theoretical elegance, its implementation in practice is complex, limiting its adoption to the SentencePiece package and adapters thereof. We bridge this gap between theory and practice by providing a clear guide to implementation and parameter choices. We also identify a simpler algorithm that accepts slightly higher training loss in exchange for improved compression.

</details>


### [67] [Modeling Authorial Style in Urdu Novels Using Character Interaction Graphs and Graph Neural Networks](https://arxiv.org/abs/2512.12654)
*Hassan Mujtaba,Hamza Naveed,Hanzlah Munir*

Main category: cs.CL

Relevance: 35.0

TL;DR: 提出基于图的方法分析乌尔都语小说的叙事结构，通过角色互动网络识别作者风格，图神经网络方法在作者分类任务上达到85.7%准确率


<details>
  <summary>Details</summary>
Motivation: 传统作者分析主要关注文本的词汇和风格特征，而更高层次的叙事结构研究不足，特别是对于乌尔都语等低资源语言。本文旨在探索是否仅从叙事结构就能推断作者风格

Method: 提出图框架将乌尔都语小说建模为角色互动网络：节点对应角色，边表示在叙事邻近中的共现。比较多种图表示方法：全局结构特征、节点级语义摘要、无监督图嵌入和监督图神经网络

Result: 在包含7位作者的52部乌尔都语小说数据集上，学习的图表示显著优于手工特征和无监督基线，在严格的作者感知评估协议下达到0.857准确率

Conclusion: 叙事结构包含可识别的作者风格信号，图表示方法在低资源语言作者分析中有效，为超越传统词汇风格分析提供了新视角

Abstract: Authorship analysis has traditionally focused on lexical and stylistic cues within text, while higher-level narrative structure remains underexplored, particularly for low-resource languages such as Urdu. This work proposes a graph-based framework that models Urdu novels as character interaction networks to examine whether authorial style can be inferred from narrative structure alone. Each novel is represented as a graph where nodes correspond to characters and edges denote their co-occurrence within narrative proximity. We systematically compare multiple graph representations, including global structural features, node-level semantic summaries, unsupervised graph embeddings, and supervised graph neural networks. Experiments on a dataset of 52 Urdu novels written by seven authors show that learned graph representations substantially outperform hand-crafted and unsupervised baselines, achieving up to 0.857 accuracy under a strict author-aware evaluation protocol.

</details>


### [68] [PrahokBART: A Pre-trained Sequence-to-Sequence Model for Khmer Natural Language Generation](https://arxiv.org/abs/2512.13552)
*Hour Kaing,Raj Dabre,Haiyue Song,Van-Hien Tran,Hideki Tanaka,Masao Utiyama*

Main category: cs.CL

Relevance: 35.0

TL;DR: PrahokBART是一个专门为高棉语设计的紧凑型预训练序列到序列模型，通过整合分词和规范化等语言组件，在机器翻译、文本摘要和标题生成任务上超越了多语言模型mBART50。


<details>
  <summary>Details</summary>
Motivation: 现有多语言模型忽略了高棉语的语言特性，如分词和规范化问题。本研究旨在通过精心构建的高质量双语语料库和语言组件，专门为高棉语开发一个优化的预训练模型。

Method: 从零开始训练PrahokBART模型，使用精心筛选的高棉语和英语语料库。整合了分词和文本规范化等语言处理模块，专门针对高棉语的语言特性进行优化。

Result: 在机器翻译、文本摘要和标题生成三个生成任务上，PrahokBART均超越了强大的多语言预训练模型mBART50。分析还揭示了各语言模块的影响，并评估了模型处理空格的能力（这对高棉语文本的自然性至关重要）。

Conclusion: 专门针对特定语言（如高棉语）设计的预训练模型，通过整合语言特定的处理组件，能够显著超越通用的多语言模型，特别是在处理该语言特有的语言特性方面。

Abstract: This work introduces {\it PrahokBART}, a compact pre-trained sequence-to-sequence model trained from scratch for Khmer using carefully curated Khmer and English corpora. We focus on improving the pre-training corpus quality and addressing the linguistic issues of Khmer, which are ignored in existing multilingual models, by incorporating linguistic components such as word segmentation and normalization. We evaluate PrahokBART on three generative tasks: machine translation, text summarization, and headline generation, where our results demonstrate that it outperforms mBART50, a strong multilingual pre-trained model. Additionally, our analysis provides insights into the impact of each linguistic module and evaluates how effectively our model handles space during text generation, which is crucial for the naturalness of texts in Khmer.

</details>


### [69] [A stylometric analysis of speaker attribution from speech transcripts](https://arxiv.org/abs/2512.13667)
*Cristina Aggazzotti,Elizabeth Allyn Smith*

Main category: cs.CL

Relevance: 35.0

TL;DR: 提出StyloSpeaker方法，将文本作者识别中的风格计量学特征应用于语音转录文本，通过字符、词汇、句子等多层次特征判断两段转录是否来自同一说话者。


<details>
  <summary>Details</summary>
Motivation: 在法医科学中，当说话者伪装声音或使用文本转语音软件时，传统的声学特征失效，只能依赖语言内容进行分析。需要将文本作者识别方法扩展到语音转录领域。

Method: 提出StyloSpeaker方法，整合风格计量学文献中的字符、词汇、标记、句子和风格特征，评估两段转录是否来自同一说话者。比较两种转录格式（规范书面文本格式和标准化格式）在不同话题控制条件下的表现。

Result: 在标准化转录上通常获得更高的识别性能，但在最强话题控制条件下整体性能最高。与黑盒神经网络方法相比，这种可解释的风格计量模型表现良好，并识别出最有效的说话者区分特征。

Conclusion: 风格计量学方法可有效应用于语音转录的说话者识别，特别是在内容标准化和话题控制条件下。该方法为法医语音分析提供了可解释的替代方案。

Abstract: Forensic scientists often need to identify an unknown speaker or writer in cases such as ransom calls, covert recordings, alleged suicide notes, or anonymous online communications, among many others. Speaker recognition in the speech domain usually examines phonetic or acoustic properties of a voice, and these methods can be accurate and robust under certain conditions. However, if a speaker disguises their voice or employs text-to-speech software, vocal properties may no longer be reliable, leaving only their linguistic content available for analysis. Authorship attribution methods traditionally use syntactic, semantic, and related linguistic information to identify writers of written text (authorship attribution). In this paper, we apply a content-based authorship approach to speech that has been transcribed into text, using what a speaker says to attribute speech to individuals (speaker attribution). We introduce a stylometric method, StyloSpeaker, which incorporates character, word, token, sentence, and style features from the stylometric literature on authorship, to assess whether two transcripts were produced by the same speaker. We evaluate this method on two types of transcript formatting: one approximating prescriptive written text with capitalization and punctuation and another normalized style that removes these conventions. The transcripts' conversation topics are also controlled to varying degrees. We find generally higher attribution performance on normalized transcripts, except under the strongest topic control condition, in which overall performance is highest. Finally, we compare this more explainable stylometric model to black-box neural approaches on the same data and investigate which stylistic features most effectively distinguish speakers.

</details>


### [70] [KH-FUNSD: A Hierarchical and Fine-Grained Layout Analysis Dataset for Low-Resource Khmer Business Document](https://arxiv.org/abs/2512.11849)
*Nimol Thuon,Jun Du*

Main category: cs.CL

Relevance: 25.0

TL;DR: 提出了KH-FUNSD数据集，这是首个公开可用的高棉语表单文档理解数据集，包含三层标注框架，用于支持低资源非拉丁文字的文档布局分析和信息提取。


<details>
  <summary>Details</summary>
Motivation: 高棉语作为柬埔寨超过1700万人使用的语言，在文档AI工具开发中受到较少关注。商业文档对公共管理和私营企业都至关重要，但缺乏专门的资源。本文旨在填补这一空白。

Method: 创建了KH-FUNSD数据集，采用三层标注设计：1)区域检测（核心区域如页眉、表单字段、页脚）；2)FUNSD风格标注（区分问题、答案、标题等实体及其关系）；3)细粒度分类（分配特定语义角色如字段标签、值、页眉等）。

Result: 提供了首个公开可用的高棉语商业文档数据集，对多个领先模型进行了基准测试，为高棉语商业文档提供了第一组基线结果，并讨论了非拉丁低资源文字带来的独特挑战。

Conclusion: KH-FUNSD数据集填补了高棉语文档AI资源的空白，其多层标注框架支持全面的布局分析和精确的信息提取，为低资源非拉丁文字的文档理解研究提供了重要资源。

Abstract: Automated document layout analysis remains a major challenge for low-resource, non-Latin scripts. Khmer is a language spoken daily by over 17 million people in Cambodia, receiving little attention in the development of document AI tools. The lack of dedicated resources is particularly acute for business documents, which are critical for both public administration and private enterprise. To address this gap, we present \textbf{KH-FUNSD}, the first publicly available, hierarchically annotated dataset for Khmer form document understanding, including receipts, invoices, and quotations. Our annotation framework features a three-level design: (1) region detection that divides each document into core zones such as header, form field, and footer; (2) FUNSD-style annotation that distinguishes questions, answers, headers, and other key entities, together with their relationships; and (3) fine-grained classification that assigns specific semantic roles, such as field labels, values, headers, footers, and symbols. This multi-level approach supports both comprehensive layout analysis and precise information extraction. We benchmark several leading models, providing the first set of baseline results for Khmer business documents, and discuss the distinct challenges posed by non-Latin, low-resource scripts. The KH-FUNSD dataset and documentation will be available at URL.

</details>


### [71] [Adversarially Probing Cross-Family Sound Symbolism in 27 Languages](https://arxiv.org/abs/2512.12245)
*Anika Sharma,Tianyi Niu,Emma Wrenn,Shashank Srivastava*

Main category: cs.CL

Relevance: 25.0

TL;DR: 该研究首次进行了大规模跨语言计算分析，探索语音象征现象（声音与意义的非任意映射），特别是在尺寸语义领域。研究收集了27种语言的810个形容词，发现语音形式能预测尺寸语义，且这种关联在不同语系间存在普遍性。


<details>
  <summary>Details</summary>
Motivation: 语音象征现象（如Bouba Kiki效应）长期以来主要通过轶事实验证明，但缺乏大规模系统性验证。本研究旨在通过计算语言学方法，在跨语言尺度上检验语音与尺寸语义之间的非任意映射关系，探索这种象征性是否具有语言普遍性。

Method: 1. 构建了包含27种语言、每种语言30个形容词（共810个）的跨语言数据集，涵盖多种语系类型
2. 所有词汇进行音位转录并通过母语者音频验证
3. 使用基于音段袋特征的可解释分类器分析语音形式对尺寸语义的预测能力
4. 开发对抗性擦除器，抑制语言身份信息同时保留尺寸信号，以检验跨语系的普遍性

Result: 1. 即使在无关语言间，语音形式也能显著高于随机水平预测尺寸语义
2. 元音和辅音都对尺寸语义预测有贡献
3. 对抗性擦除后，语言预测准确率降至随机水平以下，而尺寸预测仍显著高于随机水平，表明存在跨语系的语音象征偏置
4. 提供了数据、代码和诊断工具供未来大规模研究使用

Conclusion: 研究证实了语音象征现象在跨语言尺度上的存在，特别是在尺寸语义领域。这种语音-意义映射不仅存在于特定语言内部，而且在不同语系间表现出普遍性，为语言起源和认知基础提供了重要证据。

Abstract: The phenomenon of sound symbolism, the non-arbitrary mapping between word sounds and meanings, has long been demonstrated through anecdotal experiments like Bouba Kiki, but rarely tested at scale. We present the first computational cross-linguistic analysis of sound symbolism in the semantic domain of size. We compile a typologically broad dataset of 810 adjectives (27 languages, 30 words each), each phonemically transcribed and validated with native-speaker audio. Using interpretable classifiers over bag-of-segment features, we find that phonological form predicts size semantics above chance even across unrelated languages, with both vowels and consonants contributing. To probe universality beyond genealogy, we train an adversarial scrubber that suppresses language identity while preserving size signal (also at family granularity). Language prediction averaged across languages and settings falls below chance while size prediction remains significantly above chance, indicating cross-family sound-symbolic bias. We release data, code, and diagnostic tools for future large-scale studies of iconicity.

</details>


### [72] [Advancing Bangla Machine Translation Through Informal Datasets](https://arxiv.org/abs/2512.13487)
*Ayon Roy,Risat Rahaman,Sadat Shibly,Udoy Saha Joy,Abdulla Al Kafi,Farig Yousuf Sadeque*

Main category: cs.CL

Relevance: 25.0

TL;DR: 该研究针对孟加拉语机器翻译的不足，特别是非正式语言翻译的缺失，通过构建社交媒体和对话文本数据集来改进孟加拉语翻译模型。


<details>
  <summary>Details</summary>
Motivation: 孟加拉语是全球第六大语言，但开源机器翻译进展有限。现有研究主要关注正式语言，忽视了更常用的非正式语言，导致数百万孟加拉语使用者无法充分获取在线信息。

Method: 探索当前最先进的翻译模型，通过从社交媒体和对话文本等非正式来源开发数据集来改进孟加拉语翻译。

Result: 未在摘要中明确说明具体结果，但研究目标是提升孟加拉语机器翻译能力，特别是非正式语言的翻译质量。

Conclusion: 通过专注于非正式语言翻译和改进数据集，该研究旨在推动孟加拉语机器翻译发展，提高孟加拉语使用者在数字世界的信息可及性。

Abstract: Bangla is the sixth most widely spoken language globally, with approximately 234 million native speakers. However, progress in open-source Bangla machine translation remains limited. Most online resources are in English and often remain untranslated into Bangla, excluding millions from accessing essential information. Existing research in Bangla translation primarily focuses on formal language, neglecting the more commonly used informal language. This is largely due to the lack of pairwise Bangla-English data and advanced translation models. If datasets and models can be enhanced to better handle natural, informal Bangla, millions of people will benefit from improved online information access. In this research, we explore current state-of-the-art models and propose improvements to Bangla translation by developing a dataset from informal sources like social media and conversational texts. This work aims to advance Bangla machine translation by focusing on informal language translation and improving accessibility for Bangla speakers in the digital world.

</details>


### [73] [The Morphemic Origin of Zipf's Law: A Factorized Combinatorial Framework](https://arxiv.org/abs/2512.12394)
*Vladimir Berman*

Main category: stat.ME

Relevance: 25.0

TL;DR: 提出基于词素组合结构的单词形成模型，仅通过词素库和位置槽概率激活机制，就能解释单词长度分布和齐夫定律现象，无需意义或通信效率假设。


<details>
  <summary>Details</summary>
Motivation: 传统解释单词统计模式（如齐夫定律）通常依赖随机文本或通信效率假设。本文旨在证明仅通过词素组合结构就能产生这些模式，无需引入意义或优化原则。

Method: 提出词素组合单词模型：单词由多个位置槽构成，每个槽以一定概率激活并从词素库中选择一个词素。词素作为稳定构建块，在单词形成中规律出现并有特定位置。

Result: 模型生成真实的单词长度模式（集中中间区域和细长尾部），与真实语言匹配。合成词素库模拟产生齐夫定律指数1.1-1.4，与英语、俄语和罗曼语相似。

Conclusion: 齐夫定律行为可以在没有意义、通信压力或优化原则的情况下出现。仅词法内部结构结合概率激活机制就足以产生跨语言的稳健统计模式。

Abstract: We present a simple structure based model of how words are formed from morphemes. The model explains two major empirical facts: the typical distribution of word lengths and the appearance of Zipf like rank frequency curves. In contrast to classical explanations based on random text or communication efficiency, our approach uses only the combinatorial organization of prefixes, roots, suffixes and inflections. In this Morphemic Combinatorial Word Model, a word is created by activating several positional slots. Each slot turns on with a certain probability and selects one morpheme from its inventory. Morphemes are treated as stable building blocks that regularly appear in word formation and have characteristic positions. This mechanism produces realistic word length patterns with a concentrated middle zone and a thin long tail, closely matching real languages. Simulations with synthetic morpheme inventories also generate rank frequency curves with Zipf like exponents around 1.1-1.4, similar to English, Russian and Romance languages. The key result is that Zipf like behavior can emerge without meaning, communication pressure or optimization principles. The internal structure of morphology alone, combined with probabilistic activation of slots, is sufficient to create the robust statistical patterns observed across languages.

</details>


<div id='cs.CV'></div>

# cs.CV [[Back]](#toc)

### [74] [Read or Ignore? A Unified Benchmark for Typographic-Attack Robustness and Text Recognition in Vision-Language Models](https://arxiv.org/abs/2512.11899)
*Futa Waseda,Shojiro Yamabe,Daiki Shiono,Kento Sasaki,Tsubasa Takahashi*

Main category: cs.CV

Relevance: 85.0

TL;DR: 本文提出了RIO-VQA任务和RIO-Bench基准，用于评估大型视觉语言模型在对抗性文本攻击下的选择性文本使用能力，揭示了现有模型在文本阅读和对抗鲁棒性之间的权衡问题。


<details>
  <summary>Details</summary>
Motivation: 现有的大型视觉语言模型容易受到排版攻击，而当前的评估方法和防御策略主要鼓励忽略文本来实现鲁棒性。然而，现实场景需要模型能够根据上下文选择性使用文本（如同时识别行人和阅读交通标志）。这种选择性文本使用能力在现有评估中被忽视。

Method: 1. 提出RIO-VQA任务，形式化视觉问答中的选择性文本使用问题
2. 构建RIO-Bench基准数据集，为每个真实图像提供相同场景的反事实样本（阅读/忽略），仅改变文本内容和问题类型
3. 基于RIO-Bench开发数据驱动的防御方法，学习自适应选择性文本使用

Result: 实验表明，现有强大的LVLMs和防御方法无法平衡排版攻击鲁棒性和文本阅读能力。RIO-Bench能够有效评估这种权衡，并为开发改进方法提供基础。

Conclusion: 本文揭示了现有评估范围与现实需求之间的根本错位，为开发可靠的LVLMs提供了原则性路径。RIO-Bench为评估和改进模型的选择性文本使用能力提供了标准化工具。

Abstract: Large vision-language models (LVLMs) are vulnerable to typographic attacks, where misleading text within an image overrides visual understanding. Existing evaluation protocols and defenses, largely focused on object recognition, implicitly encourage ignoring text to achieve robustness; however, real-world scenarios often require joint reasoning over both objects and text (e.g., recognizing pedestrians while reading traffic signs). To address this, we introduce a novel task, Read-or-Ignore VQA (RIO-VQA), which formalizes selective text use in visual question answering (VQA): models must decide, from context, when to read text and when to ignore it. For evaluation, we present the Read-or-Ignore Benchmark (RIO-Bench), a standardized dataset and protocol that, for each real image, provides same-scene counterfactuals (read / ignore) by varying only the textual content and question type. Using RIO-Bench, we show that strong LVLMs and existing defenses fail to balance typographic robustness and text-reading capability, highlighting the need for improved approaches. Finally, RIO-Bench enables a novel data-driven defense that learns adaptive selective text use, moving beyond prior non-adaptive, text-ignoring defenses. Overall, this work reveals a fundamental misalignment between the existing evaluation scope and real-world requirements, providing a principled path toward reliable LVLMs. Our Project Page is at https://turingmotors.github.io/rio-vqa/.

</details>


### [75] [VEGAS: Mitigating Hallucinations in Large Vision-Language Models via Vision-Encoder Attention Guided Adaptive Steering](https://arxiv.org/abs/2512.12089)
*Zihu Wang,Boxun Xu,Yuxuan Xia,Peng Li*

Main category: cs.CV

Relevance: 85.0

TL;DR: VEGAS：通过将视觉编码器的注意力图注入到语言模型中间层来减少LVLM幻觉的推理时方法


<details>
  <summary>Details</summary>
Motivation: 大型视觉语言模型（LVLMs）虽然能联合推理视觉和文本输入，但经常产生与视觉证据事实不一致的输出（即幻觉）。现有方法未能有效回答：什么形式的视觉注意力能在解码过程中有效抑制幻觉？

Method: 提出VEGAS方法：1）发现视觉编码器自身的注意力图能有效减少幻觉；2）分析解码过程中的视觉-文本冲突，发现冲突在语言模型中间层达到峰值；3）将视觉编码器的注意力图注入到这些中间层，并自适应地引导未能聚焦关键图像对象的token

Result: 在多个基准测试上的广泛实验表明，VEGAS在减少幻觉方面始终达到最先进的性能

Conclusion: 视觉编码器的注意力图是抑制LVLM幻觉的有效形式，通过将其注入到语言模型中间层可以显著减少幻觉，VEGAS提供了一种简单而有效的推理时解决方案

Abstract: Large vision-language models (LVLMs) exhibit impressive ability to jointly reason over visual and textual inputs. However, they often produce outputs that are linguistically fluent but factually inconsistent with the visual evidence, i.e., they hallucinate. Despite growing efforts to mitigate such hallucinations, a key question remains: what form of visual attention can effectively suppress hallucinations during decoding? In this work, we provide a simple answer: the vision encoder's own attention map. We show that LVLMs tend to hallucinate when their final visual-attention maps fail to concentrate on key image objects, whereas the vision encoder's more concentrated attention maps substantially reduce hallucinations. To further investigate the cause, we analyze vision-text conflicts during decoding and find that these conflicts peak in the language model's middle layers. Injecting the vision encoder's attention maps into these layers effectively suppresses hallucinations. Building on these insights, we introduce VEGAS, a simple yet effective inference-time method that integrates the vision encoder's attention maps into the language model's mid-layers and adaptively steers tokens which fail to concentrate on key image objects. Extensive experiments across multiple benchmarks demonstrate that VEGAS consistently achieves state-of-the-art performance in reducing hallucinations.

</details>


### [76] [Journey Before Destination: On the importance of Visual Faithfulness in Slow Thinking](https://arxiv.org/abs/2512.12218)
*Rheeya Uppaal,Phu Mon Htut,Min Bai,Nikolaos Pappas,Zheng Qi*

Main category: cs.CV

Relevance: 85.0

TL;DR: 该论文提出了一种评估视觉语言模型推理链视觉忠实性的框架，以及无需训练的自我反思方法来检测和修复不忠实的感知步骤。


<details>
  <summary>Details</summary>
Motivation: 当前具有推理能力的视觉语言模型虽然能生成显式的思维链，但存在新的失败模式：模型可能通过视觉不忠实的中间步骤得出正确答案，或者推理忠实但最终预测失败。仅评估最终答案准确性的标准方法无法区分这些行为。

Method: 1) 提出训练和参考无关的框架，将推理链分解为感知与推理步骤，使用现成的VLM作为评判者进行步骤级忠实性评估；2) 基于该指标，提出轻量级自我反思程序，无需训练即可检测并局部重新生成不忠实的感知步骤。

Result: 在多个经过推理训练的VLM和感知密集型基准测试中，该方法降低了不忠实感知率，同时保持了最终答案准确性，提高了多模态推理的可靠性。

Conclusion: 视觉推理链的忠实性是一个重要的评估维度，提出的框架和方法能有效提升VLM推理的可靠性和透明度。

Abstract: Reasoning-augmented vision language models (VLMs) generate explicit chains of thought that promise greater capability and transparency but also introduce new failure modes: models may reach correct answers via visually unfaithful intermediate steps, or reason faithfully yet fail on the final prediction. Standard evaluations that only measure final-answer accuracy cannot distinguish these behaviors. We introduce the visual faithfulness of reasoning chains as a distinct evaluation dimension, focusing on whether the perception steps of a reasoning chain are grounded in the image. We propose a training- and reference-free framework that decomposes chains into perception versus reasoning steps and uses off-the-shelf VLM judges for step-level faithfulness, additionally verifying this approach through a human meta-evaluation. Building on this metric, we present a lightweight self-reflection procedure that detects and locally regenerates unfaithful perception steps without any training. Across multiple reasoning-trained VLMs and perception-heavy benchmarks, our method reduces Unfaithful Perception Rate while preserving final-answer accuracy, improving the reliability of multimodal reasoning.

</details>


### [77] [Moment and Highlight Detection via MLLM Frame Segmentation](https://arxiv.org/abs/2512.12246)
*I Putu Andika Bagas Jiwanta,Ayu Purwarianti*

Main category: cs.CV

Relevance: 85.0

TL;DR: 提出一种新颖的基于LLM的视频时刻检测方法，通过让LLM输出"0"/"1"字符序列来同时进行语言理解和帧级分割，相比纯文本生成方法能提供直接的梯度信号。


<details>
  <summary>Details</summary>
Motivation: 现有基于生成式MLLM的方法虽然能利用LLM的推理能力，但文本生成无法为帧级预测提供直接梯度。虽然最近有RL方法尝试解决，但本文提出更直接的分割目标方法。

Method: 让LLM处理固定数量帧，通过提示使其输出连续的"0"/"1"字符序列（每个字符对应一帧）。"0"代表背景，"1"代表前景概率。训练时结合分割损失和因果语言模型损失，推理时使用beam search生成序列和logits。

Result: 在QVHighlights数据集上，仅采样25帧（少于同类方法一半）就实现了56.74 HIT@1的强亮点检测性能，并在时刻检索上达到35.28 MAP，超过基线。

Conclusion: 提出的方法成功将分割目标直接应用于LLM输出token，在保持LLM语言能力的同时获得帧级预测能力，分割损失即使在因果LM损失平台期也能提供稳定的补充学习信号。

Abstract: Detecting video moments and highlights from natural-language queries have been unified by transformer-based methods. Other works use generative Multimodal LLM (MLLM) to predict moments and/or highlights as text timestamps, utilizing its reasoning capability. While effective, text-based generation cannot provide direct gradients for frame-level predictions because the model only emits language tokens. Although recent Reinforcement Learning (RL) methods attempt to address the issue, we propose a novel approach by applying segmentation objectives directly on the LLM's output tokens. The LLM is fed with a fixed number of frames alongside a prompt that enforces it to output a sequence of continuous "0" and/or "1" characters, with one character per frame. The "0"/"1" characters benefit from the LLM's inherent language capability while also acting as background and foreground probabilities, respectively. Training employs segmentation losses on the probabilities alongside a normal causal LM loss. At inference, beam search generates sequence and logits, acting as moments and saliency scores, respectively. Despite sampling only 25 frames -- less than half of comparable methods -- our method achieved strong highlight detection (56.74 HIT@1) on QVHighlights. Additionally, our efficient method scores above the baseline (35.28 MAP) for moment retrieval. Empirically, segmentation losses provide a stable complementary learning signal even when the causal LM loss plateaus.

</details>


### [78] [More Than the Final Answer: Improving Visual Extraction and Logical Consistency in Vision-Language Models](https://arxiv.org/abs/2512.12487)
*Hoang Anh Just,Yifei Fan,Handong Zhao,Jiuxiang Gu,Ruiyi Zhang,Simon Jenni,Kushal Kafle,Ruoxi Jia,Jing Shi*

Main category: cs.CV

Relevance: 85.0

TL;DR: PeRL-VL提出了一种解耦的感知与推理学习框架，通过分别改进视觉感知和文本推理来解决RLVR训练视觉语言模型中的视觉提取不准确和思维链逻辑不一致问题。


<details>
  <summary>Details</summary>
Motivation: 尽管RLVR已扩展到视觉语言模型以激发多模态推理，但训练后的模型仍存在两个持续性问题：1）视觉提取不准确（遗漏或幻觉细节）；2）思维链逻辑不一致。这主要是因为可验证信号只监督最终答案，缺乏对中间过程的监督。

Method: PeRL-VL采用解耦框架：1）感知方面：引入基于VLM的描述奖励，评估模型自生成图像描述的真实性和充分性；2）推理方面：在逻辑丰富的思维链数据上添加纯文本推理SFT阶段，独立于视觉增强逻辑一致性和连贯性。

Result: 在多样化多模态基准测试中，PeRL-VL将平均Pass@1准确率从63.3%（基础Qwen2.5-VL-7B）提升到68.8%，优于标准RLVR、纯文本推理SFT和从GPT-4o的朴素多模态蒸馏。

Conclusion: PeRL-VL通过解耦感知和推理学习，有效解决了RLVR训练VLM中的核心问题，为多模态推理提供了更有效的训练框架。

Abstract: Reinforcement learning from verifiable rewards (RLVR) has recently been extended from text-only LLMs to vision-language models (VLMs) to elicit long-chain multimodal reasoning. However, RLVR-trained VLMs still exhibit two persistent failure modes: inaccurate visual extraction (missing or hallucinating details) and logically inconsistent chains-of-thought, largely because verifiable signals supervise only the final answer. We propose PeRL-VL (Perception and Reasoning Learning for Vision-Language Models), a decoupled framework that separately improves visual perception and textual reasoning on top of RLVR. For perception, PeRL-VL introduces a VLM-based description reward that scores the model's self-generated image descriptions for faithfulness and sufficiency. For reasoning, PeRL-VL adds a text-only Reasoning SFT stage on logic-rich chain-of-thought data, enhancing coherence and logical consistency independently of vision. Across diverse multimodal benchmarks, PeRL-VL improves average Pass@1 accuracy from 63.3% (base Qwen2.5-VL-7B) to 68.8%, outperforming standard RLVR, text-only reasoning SFT, and naive multimodal distillation from GPT-4o.

</details>


### [79] [StreamingAssistant: Efficient Visual Token Pruning for Accelerating Online Video Understanding](https://arxiv.org/abs/2512.12560)
*Xinqi Jin,Hanxun Yu,Bohan Yu,Kebin Liu,Jian Liu,Keda Tao,Yixuan Pei,Huan Wang,Fan Dang,Jiangchuan Liu,Weiqiang Wang*

Main category: cs.CV

Relevance: 85.0

TL;DR: 提出MSSAVT冗余度量与掩码剪枝策略，用于减少多模态大语言模型处理视频时的token数量，降低计算开销同时保持关键信息。


<details>
  <summary>Details</summary>
Motivation: 在线视频理解（如公共监控、AI眼镜）需要处理大量视频帧，导致MLLMs面临GPU内存占用高、计算延迟大的挑战，需要高效压缩视频token的方法。

Method: 1) 提出MSSAVT冗余度量，结合token相似度和空间位置；2) 设计掩码剪枝策略解决剪枝与冗余的双向依赖问题；3) 集成现有时间冗余剪枝方法消除视频模态的时间冗余。

Result: 在多个在线和离线视频理解基准测试中，方法显著提升准确率（最多4%），同时剪枝延迟可忽略不计（小于1ms）。

Conclusion: 提出的token剪枝方法能有效减少MLLMs处理视频时的上下文长度，在保持关键信息的同时显著降低计算开销，适用于实时视频理解应用。

Abstract: Online video understanding is essential for applications like public surveillance and AI glasses. However, applying Multimodal Large Language Models (MLLMs) to this domain is challenging due to the large number of video frames, resulting in high GPU memory usage and computational latency. To address these challenges, we propose token pruning as a means to reduce context length while retaining critical information. Specifically, we introduce a novel redundancy metric, Maximum Similarity to Spatially Adjacent Video Tokens (MSSAVT), which accounts for both token similarity and spatial position. To mitigate the bidirectional dependency between pruning and redundancy, we further design a masked pruning strategy that ensures only mutually unadjacent tokens are pruned. We also integrate an existing temporal redundancy-based pruning method to eliminate temporal redundancy of the video modality. Experimental results on multiple online and offline video understanding benchmarks demonstrate that our method significantly improves the accuracy (i.e., by 4\% at most) while incurring a negligible pruning latency (i.e., less than 1ms). Our full implementation will be made publicly available.

</details>


### [80] [Vision-Enhanced Large Language Models for High-Resolution Image Synthesis and Multimodal Data Interpretation](https://arxiv.org/abs/2512.12595)
*Karthikeya KV*

Main category: cs.CV

Relevance: 85.0

TL;DR: 提出一个结合视觉增强LLM与先进Transformer架构的框架，用于高分辨率图像合成和多模态数据理解，采用整流流机制和双向标记化策略，在图像质量和计算效率上超越扩散方法。


<details>
  <summary>Details</summary>
Motivation: 解决高分辨率图像合成和多模态数据解释中的挑战，特别是如何有效整合文本、图像和视频数据，并提高生成质量和计算效率。

Method: 1) 采用整流流机制连接噪声和数据，实现线性路径的高效高质量生成；2) 双向标记化策略融合文本、图像和视频输入；3) 嵌入时空特征和混合文本-图像序列建模；4) 噪声感知学习算法优化架构。

Result: 在基准数据集上评估显示：图像分辨率清晰度提高25%，计算需求比扩散方法减少20%，展现出强大的可扩展性和适应性。

Conclusion: 该框架展示了视觉中心LLM在重新定义计算机视觉和多模态AI能力方面的潜力，适用于自主系统、创意内容生成和高级视频分析等应用。

Abstract: This research introduces a transformative framework for integrating Vision-Enhanced Large Language Models (LLMs) with advanced transformer-based architectures to tackle challenges in high-resolution image synthesis and multimodal data interpretation. The proposed model incorporates a rectified flow mechanism that connects noise and data with linear paths, enabling efficient and high-quality generation. A bidirectional tokenization strategy is employed to seamlessly merge inputs from text, image, and video modalities, fostering a unified understanding across diverse data types. By embedding spatial-temporal features and leveraging a hybrid text-image sequence modeling approach, the framework achieves unparalleled fidelity in synthesized images and coherent multimodal representations. The architecture is optimized with a noise-aware learning algorithm, addressing discrepancies in noisy data distributions and improving generative performance under varying input conditions. Rigorous evaluations on benchmark datasets demonstrate a 25% increase in image resolution clarity and a 20% reduction in computational requirements compared to diffusion-based methods. Furthermore, the model exhibits robust scalability and adaptability, showcasing its potential in applications like autonomous systems, creative content generation, and advanced video analysis. This work underscores the role of vision-centric LLMs in redefining capabilities in computer vision and multimodal artificial intelligence.

</details>


### [81] [Efficient Vision-Language Reasoning via Adaptive Token Pruning](https://arxiv.org/abs/2512.12701)
*Xue Li,Xiaonan Song,Henry Hu*

Main category: cs.CV

Relevance: 85.0

TL;DR: ATP是一种动态推理机制，通过结合视觉注意力和文本-图像相似性评分，自适应地保留最相关的token，在保持准确性的同时显著降低计算开销。


<details>
  <summary>Details</summary>
Motivation: 当前视觉语言模型(VLMs)在真实世界部署中存在高计算需求的问题，现有架构对所有token进行统一处理效率低下。需要一种动态推理机制来减少计算开销而不损失性能。

Method: 提出自适应token剪枝(ATP)机制，在视觉-语言接口处操作，结合ViT CLS注意力（模态内显著性）和CLIP文本-图像相似性（模态间相关性）计算混合重要性分数，仅保留top-K个最相关的token供LLM处理。

Result: 在VQAv2、GQA和COCO等基准测试中，ATP减少约40%的推理FLOPs，实现约1.5倍的端到端延迟加速，准确率损失小于1%。定性分析显示ATP保持了视觉基础并增强了可解释性。

Conclusion: 自适应token剪枝在资源受限的推理和模型可靠性之间不是竞争关系，反而能通过抑制虚假相关性提高稳定性。ATP在高效多模态边缘计算管道中具有应用潜力。

Abstract: Real-world deployment of Vision-Language Models (VLMs) is hindered by high computational demands, as existing architectures inefficiently process all tokens uniformly. We introduce Adaptive Token Pruning (ATP), a dynamic inference mechanism that retains only the most informative tokens based on contextual relevance. ATP operates at the vision-language interface, assigning a hybrid importance score combining ViT CLS attention (intra-modal saliency) and CLIP text-image similarity (inter-modal relevance) to keep top-K tokens for the LLM. Unlike static compression, ATP adapts to each input without modifying the backbone. Proposed as a lightweight gating module, ATP is compatible with popular backbones like BLIP-2, LLaVA, and Flamingo. Preliminary evaluations across VQAv2, GQA, and COCO indicate that ATP reduces inference FLOPs by around 40% and achieves roughly 1.5x speedups in end-to-end latency with negligible accuracy loss (less than 1%). Qualitative analyses suggest ATP preserves visual grounding and enhances interpretability. Beyond efficiency, we investigate robustness under corruptions; observations suggest adaptive pruning suppresses spurious correlations, improving stability. These findings imply that resource-constrained inference and model reliability are not competing objectives. Finally, we discuss ATP's role in efficient multimodal edge computing pipelines.

</details>


### [82] [CogDoc: Towards Unified thinking in Documents](https://arxiv.org/abs/2512.12658)
*Qixin Xu,Haozhe Wang,Che Liu,Fangzhen Lin,Wenhu Chen*

Main category: cs.CV

Relevance: 85.0

TL;DR: CogDoc提出了一种模仿人类认知过程的粗到细思维框架，通过"快速阅读"和"专注思考"两阶段解决文档推理中的可扩展性与保真度权衡问题，采用直接强化学习策略，在7B参数规模下超越了GPT-4o等更大模型。


<details>
  <summary>Details</summary>
Motivation: 当前文档推理范式面临可扩展性（处理长上下文文档）与保真度（捕捉细粒度多模态细节）之间的基本权衡。现有方法难以同时处理长文档和保留视觉丰富细节，需要一种更接近人类认知过程的统一框架。

Method: 提出CogDoc统一粗到细思维框架：1）低分辨率"快速阅读"阶段进行可扩展信息定位；2）高分辨率"专注思考"阶段进行深度推理。研究了后训练策略，发现直接强化学习优于SFT初始化的RL，避免了"策略冲突"问题。

Result: 7B参数模型在其参数类别中达到最先进性能，在具有挑战性的视觉丰富文档基准测试中显著超越了GPT-4o等更大的专有模型。直接RL策略被证明比SFT+RL更有效。

Conclusion: CogDoc框架通过模仿人类认知过程成功解决了文档推理中的可扩展性与保真度权衡问题，直接强化学习策略避免了传统方法的策略冲突，为高效文档理解提供了新思路。

Abstract: Current document reasoning paradigms are constrained by a fundamental trade-off between scalability (processing long-context documents) and fidelity (capturing fine-grained, multimodal details). To bridge this gap, we propose CogDoc, a unified coarse-to-fine thinking framework that mimics human cognitive processes: a low-resolution "Fast Reading" phase for scalable information localization,followed by a high-resolution "Focused Thinking" phase for deep reasoning. We conduct a rigorous investigation into post-training strategies for the unified thinking framework, demonstrating that a Direct Reinforcement Learning (RL) approach outperforms RL with Supervised Fine-Tuning (SFT) initialization. Specifically, we find that direct RL avoids the "policy conflict" observed in SFT. Empirically, our 7B model achieves state-of-the-art performance within its parameter class, notably surpassing significantly larger proprietary models (e.g., GPT-4o) on challenging, visually rich document benchmarks.

</details>


### [83] [DrivePI: Spatial-aware 4D MLLM for Unified Autonomous Driving Understanding, Perception, Prediction and Planning](https://arxiv.org/abs/2512.12799)
*Zhe Liu,Runhui Huang,Rui Yang,Siming Yan,Zining Wang,Lu Hou,Di Lin,Xiang Bai,Hengshuang Zhao*

Main category: cs.CV

Relevance: 85.0

TL;DR: DrivePI是一个空间感知的4D多模态大语言模型，用于自动驾驶中的统一视觉-语言-动作框架，能够并行执行空间理解、3D感知、预测和规划任务。


<details>
  <summary>Details</summary>
Motivation: 虽然多模态大语言模型在多个领域表现出强大能力，但在自动驾驶中生成细粒度3D感知和预测输出方面仍未被充分探索。现有方法通常将感知、预测和规划作为独立模块处理，缺乏统一的端到端优化框架。

Method: 提出DrivePI框架，整合点云、多视角图像和语言指令到统一的MLLM架构中。开发数据引擎生成文本-占用和文本-流问答对用于4D空间理解。使用仅0.5B参数的Qwen2.5作为MLLM骨干，通过端到端优化并行执行空间理解、3D占用感知、占用流预测和动作规划。

Result: DrivePI作为单一统一模型，在多个基准测试中匹配或超越了现有VLA模型和专用VA模型：在nuScenes-QA上比OpenDriveVLA-7B高2.5%平均准确率；碰撞率比ORION降低70%；在OpenOcc上比FB-OCC高10.3 RayIoU；占用流mAVE从0.591降至0.509；规划L2误差比VAD降低32%。

Conclusion: DrivePI证明了通过统一的端到端优化框架，小型MLLM也能在自动驾驶的3D感知、预测和规划任务中取得优异性能，为多模态大语言模型在自动驾驶领域的应用提供了新方向。

Abstract: Although multi-modal large language models (MLLMs) have shown strong capabilities across diverse domains, their application in generating fine-grained 3D perception and prediction outputs in autonomous driving remains underexplored. In this paper, we propose DrivePI, a novel spatial-aware 4D MLLM that serves as a unified Vision-Language-Action (VLA) framework that is also compatible with vision-action (VA) models. Our method jointly performs spatial understanding, 3D perception (i.e., 3D occupancy), prediction (i.e., occupancy flow), and planning (i.e., action outputs) in parallel through end-to-end optimization. To obtain both precise geometric information and rich visual appearance, our approach integrates point clouds, multi-view images, and language instructions within a unified MLLM architecture. We further develop a data engine to generate text-occupancy and text-flow QA pairs for 4D spatial understanding. Remarkably, with only a 0.5B Qwen2.5 model as MLLM backbone, DrivePI as a single unified model matches or exceeds both existing VLA models and specialized VA models. Specifically, compared to VLA models, DrivePI outperforms OpenDriveVLA-7B by 2.5% mean accuracy on nuScenes-QA and reduces collision rate by 70% over ORION (from 0.37% to 0.11%) on nuScenes. Against specialized VA models, DrivePI surpasses FB-OCC by 10.3 RayIoU for 3D occupancy on OpenOcc, reduces the mAVE from 0.591 to 0.509 for occupancy flow on OpenOcc, and achieves 32% lower L2 error than VAD (from 0.72m to 0.49m) for planning on nuScenes. Code will be available at https://github.com/happinesslz/DrivePI

</details>


### [84] [Adapting Multimodal Foundation Models for Few-Shot Learning: A Comprehensive Study on Contrastive Captioners](https://arxiv.org/abs/2512.12824)
*N. K. B. M. P. K. B. Narasinghe,Uthayasanker Thayasivam*

Main category: cs.CV

Relevance: 85.0

TL;DR: 本文系统研究了CoCa多模态基础模型在少样本图像分类任务中的适应策略，评估了从训练免费原型到LoRA参数高效微调等多种方法，发现了数据增强对线性探测和LoRA微调的不同影响，并提出了针对数据稀缺场景的训练配置建议。


<details>
  <summary>Details</summary>
Motivation: 虽然CoCa等大规模多模态基础模型在零样本迁移方面表现出色，但它们在极端数据稀缺（少样本学习）场景下的适应能力尚未得到充分探索。现有研究主要关注CLIP等双编码器架构，而对CoCa这种生成-对比混合模型的潜在空间如何响应参数高效微调的理解存在空白。

Method: 采用层次化策略评估框架：1) 训练免费的混合原型方法；2) 通过低秩适应（LoRA）进行深度参数适应。系统评估了数据增强、监督对比损失（SupCon）与交叉熵损失的混合目标，以及正则化、秩和采样策略等训练配置对数据稀缺的敏感性。

Result: 发现了"增强发散"现象：强数据增强在少样本设置下会降低线性探测性能，但对稳定LoRA微调至关重要。混合目标（结合SupCon损失）在不同样本数量下均比标准交叉熵带来一致的性能提升。提供了针对数据稀缺场景的实证参考设置。

Conclusion: CoCa生成-对比混合模型在少样本适应中展现出独特的特性，需要专门针对数据稀缺场景调整训练策略。研究为高效适应生成-对比基础模型提供了实证指导，特别是在正则化、秩选择和采样策略方面的配置建议。

Abstract: Large-scale multimodal foundation models, particularly Contrastive Captioners (CoCa), have achieved state-of-the-art results by unifying contrastive alignment with generative captioning. While zero-shot transfer capabilities are well-documented, the adaptation of these generative-contrastive hybrids to downstream tasks with extreme data scarcity (few-shot learning) remains under-explored. Existing literature predominantly focuses on dual-encoder architectures like CLIP, leaving a gap in understanding how CoCa's distinct latent space responds to parameter-efficient fine-tuning (PEFT). This paper presents a comprehensive empirical study on adapting the CoCa visual backbone for few-shot image classification. We systematically evaluate a hierarchy of strategies, ranging from training-free hybrid prototyping to deep parameter adaptation via Low-Rank Adaptation (LoRA). First, we identify an "augmentation divergence": while strong data augmentation degrades the performance of linear probing in low-shot settings, it is essential for stabilizing LoRA fine-tuning. We also demonstrate that hybrid objectives incorporating Supervised Contrastive (SupCon) loss yield consistent performance improvements over standard Cross-Entropy across varying shot counts. Crucially, we characterize the sensitivity of training configurations to data scarcity, providing empirical reference settings for scaling regularization, rank, and sampling strategies to facilitate the efficient adaptation of generative-contrastive foundation models.

</details>


### [85] [VLCache: Computing 2% Vision Tokens and Reusing 98% for Vision-Language Inference](https://arxiv.org/abs/2512.12977)
*Shengling Qin,Hao Yu,Chenxin Wu,Zheng Li,Yizhong Cao,Zhengyang Zhuge,Yuxin Zhou,Wentao Yao,Yi Zhang,Zhengheng Wang,Shuai Bai,Jianwei Zhang,Junyang Lin*

Main category: cs.CV

Relevance: 85.0

TL;DR: VLCache是一个缓存重用框架，利用先前多模态输入的KV缓存和编码器缓存来避免重复计算，通过最小化非前缀缓存重用误差和动态层感知重计算策略，在保持精度的同时实现1.2-16倍的TTFT加速。


<details>
  <summary>Details</summary>
Motivation: 多模态大模型在处理重复输入时需要昂贵的重复计算，现有启发式方法存在累积重用误差问题。需要一种系统性的缓存重用方案来平衡计算效率和模型精度。

Method: 1) 形式化识别累积重用误差效应并最小化非前缀缓存重用误差；2) 分析模型层的重要性差异，提出动态层感知重计算策略；3) 集成到SGLang推理框架中。

Result: VLCache在保持与完全重计算相当精度的同时，仅需计算2-5%的token，实现1.2-16倍的TTFT加速，已成功集成到SGLang中。

Conclusion: VLCache通过系统性的缓存重用和智能重计算策略，显著提升了多模态大模型的推理效率，为实际部署提供了有效的加速方案。

Abstract: This paper presents VLCache, a cache reuse framework that exploits both Key-Value (KV) cache and encoder cache from prior multimodal inputs to eliminate costly recomputation when the same multimodal inputs recur. Unlike previous heuristic approaches, we formally identify the cumulative reuse error effect and demonstrate how to minimize the non-prefix cache reuse error effectively. We further analyze the varying importance of model layers and propose a dynamic, layer-aware recomputation strategy to balance accuracy and efficiency. Experimental results show that VLCache achieves an accuracy on par with full recomputation, while requiring only 2-5% of the tokens to compute, yielding 1.2x-16x TTFT speedups. The proposed VLCache pipeline has been integrated into SGLang, enabling significantly faster inference in practical deployments.

</details>


### [86] [Calibrating Uncertainty for Zero-Shot Adversarial CLIP](https://arxiv.org/abs/2512.12997)
*Wenjing lu,Zerui Tao,Dongping Zhang,Yuning Qiu,Yang Yang,Qibin Zhao*

Main category: cs.CV

Relevance: 85.0

TL;DR: 提出针对CLIP模型的新型对抗微调方法，通过狄利克雷分布重新参数化输出，同时优化预测准确性和不确定性校准，在保持零样本泛化能力的同时提升对抗鲁棒性。


<details>
  <summary>Details</summary>
Motivation: CLIP在零样本分类中表现优秀，但对对抗攻击高度脆弱。现有对抗微调方法主要关注干净样本和对抗样本的预测logits匹配，忽视了不确定性校准，可能损害零样本泛化能力。研究发现对抗扰动不仅降低准确性，还会抑制不确定性，导致严重的校准错误和不可靠的过度自信，这揭示了超越鲁棒性的关键可靠性差距。

Method: 提出新颖的对抗微调目标，同时考虑预测准确性和不确定性对齐。通过将CLIP输出重新参数化为狄利克雷分布的浓度参数，创建统一表示来捕捉相对语义结构和预测置信度大小。该目标在扰动下整体对齐这些分布，超越单一logits锚定，恢复校准的不确定性。

Result: 在多个零样本分类基准测试中，该方法有效恢复了校准的不确定性，实现了有竞争力的对抗鲁棒性，同时保持了干净样本的准确性。

Conclusion: 该方法成功解决了CLIP在对抗设置下的不确定性校准问题，填补了可靠性的关键空白，为构建更可靠的视觉语言模型提供了新思路。

Abstract: CLIP delivers strong zero-shot classification but remains highly vulnerable to adversarial attacks. Previous work of adversarial fine-tuning largely focuses on matching the predicted logits between clean and adversarial examples, which overlooks uncertainty calibration and may degrade the zero-shot generalization. A common expectation in reliable uncertainty estimation is that predictive uncertainty should increase as inputs become more difficult or shift away from the training distribution. However, we frequently observe the opposite in the adversarial setting: perturbations not only degrade accuracy but also suppress uncertainty, leading to severe miscalibration and unreliable over-confidence. This overlooked phenomenon highlights a critical reliability gap beyond robustness. To bridge this gap, we propose a novel adversarial fine-tuning objective for CLIP considering both prediction accuracy and uncertainty alignments. By reparameterizing the output of CLIP as the concentration parameter of a Dirichlet distribution, we propose a unified representation that captures relative semantic structure and the magnitude of predictive confidence. Our objective aligns these distributions holistically under perturbations, moving beyond single-logit anchoring and restoring calibrated uncertainty. Experiments on multiple zero-shot classification benchmarks demonstrate that our approach effectively restores calibrated uncertainty and achieves competitive adversarial robustness while maintaining clean accuracy.

</details>


### [87] [GTR-Turbo: Merged Checkpoint is Secretly a Free Teacher for Agentic VLM Training](https://arxiv.org/abs/2512.13043)
*Tong Wei,Yijun Yang,Changhao Zhang,Junliang Xing,Yuanchun Shi,Zongqing Lu,Deheng Ye*

Main category: cs.CV

Relevance: 85.0

TL;DR: GTR-Turbo：一种高效的多模态智能体强化学习方法，通过合并训练过程中的检查点权重作为"免费"教师模型，无需依赖昂贵的特权模型，显著降低计算成本并提升性能。


<details>
  <summary>Details</summary>
Motivation: 基于视觉语言模型的多模态智能体在多轮强化学习中面临稀疏奖励和长视野信用分配问题。现有方法依赖昂贵的特权模型作为教师提供步骤级反馈，限制了实用性和可复现性。

Method: GTR-Turbo在持续RL训练过程中合并检查点权重，使用合并后的模型作为"免费"教师，通过监督微调或软logit蒸馏指导后续RL训练，无需训练或查询昂贵的教师模型。

Result: 在多种视觉智能体任务中，GTR-Turbo将基线模型准确率提升10-30%，相比GTR减少50%的训练时间和60%的计算成本，同时缓解了先前工作中的"熵崩溃"问题。

Conclusion: GTR-Turbo提供了一种高效实用的多模态智能体强化学习框架，消除了对特权模型的依赖，显著降低了计算成本，同时保持了训练稳定性。

Abstract: Multi-turn reinforcement learning (RL) for multi-modal agents built upon vision-language models (VLMs) is hampered by sparse rewards and long-horizon credit assignment. Recent methods densify the reward by querying a teacher that provides step-level feedback, e.g., Guided Thought Reinforcement (GTR) and On-Policy Distillation, but rely on costly, often privileged models as the teacher, limiting practicality and reproducibility. We introduce GTR-Turbo, a highly efficient upgrade to GTR, which matches the performance without training or querying an expensive teacher model. Specifically, GTR-Turbo merges the weights of checkpoints produced during the ongoing RL training, and then uses this merged model as a "free" teacher to guide the subsequent RL via supervised fine-tuning or soft logit distillation. This design removes dependence on privileged VLMs (e.g., GPT or Gemini), mitigates the "entropy collapse" observed in prior work, and keeps training stable. Across diverse visual agentic tasks, GTR-Turbo improves the accuracy of the baseline model by 10-30% while reducing wall-clock training time by 50% and compute cost by 60% relative to GTR.

</details>


### [88] [ADHint: Adaptive Hints with Difficulty Priors for Reinforcement Learning](https://arxiv.org/abs/2512.13095)
*Feng Zhang,Zezhong Tan,Xinhong Ma,Ziqiang Dong,Xi Leng,Jianfei Zhao,Xin Sun,Yang Yang*

Main category: cs.CV

Relevance: 85.0

TL;DR: ADHint：一种基于难度的提示调度方法，通过自适应调整提示比例和优势估计，在SFT和RL之间取得更好平衡，提升推理能力和泛化性能。


<details>
  <summary>Details</summary>
Motivation: 现有基于提示的RL方法在调度提示比例和估计相对优势时忽略难度因素，导致学习不稳定和过度模仿离策略提示。需要更好的探索与模仿平衡。

Method: 1. 自适应提示与样本难度先验：评估样本难度并调度适当提示比例；2. 一致性梯度调制和选择性掩码：调制提示内的token级梯度；3. 基于rollout难度后验的优势估计：利用有/无提示rollout的相对难度估计各自优势。

Result: 在多种模态、模型规模和领域上的实验表明，ADHint在推理能力和分布外泛化方面优于现有方法，在pass@1和avg@8指标上均表现更好。

Conclusion: 将难度作为关键因素整合到提示调度和优势估计中，能实现更好的探索-模仿平衡，提升后训练方法的性能。

Abstract: To combine the advantages of Supervised Fine-Tuning (SFT) and Reinforcement Learning (RL), recent methods have integrated ''hints'' into post-training, which are prefix segments of complete reasoning trajectories, aiming for powerful knowledge expansion and reasoning generalization. However, existing hint-based RL methods typically ignore difficulty when scheduling hint ratios and estimating relative advantages, leading to unstable learning and excessive imitation of off-policy hints. In this work, we propose ADHint, which treats difficulty as a key factor in both hint-ratio schedule and relative-advantage estimation to achieve a better trade-off between exploration and imitation. Specifically, we propose Adaptive Hint with Sample Difficulty Prior, which evaluates each sample's difficulty under the policy model and accordingly schedules an appropriate hint ratio to guide its rollouts. We also introduce Consistency-based Gradient Modulation and Selective Masking for Hint Preservation to modulate token-level gradients within hints, preventing biased and destructive updates. Additionally, we propose Advantage Estimation with Rollout Difficulty Posterior, which leverages the relative difficulty of rollouts with and without hints to estimate their respective advantages, thereby achieving more balanced updates. Extensive experiments across diverse modalities, model scales, and domains demonstrate that ADHint delivers superior reasoning ability and out-of-distribution generalization, consistently surpassing existing methods in both pass@1 and avg@8. Our code and dataset will be made publicly available upon paper acceptance.

</details>


### [89] [Test-Time Modification: Inverse Domain Transformation for Robust Perception](https://arxiv.org/abs/2512.13454)
*Arpit Jadon,Joshua Niemeijer,Yuki M. Asano*

Main category: cs.CV

Relevance: 85.0

TL;DR: 提出一种测试时域泛化方法，使用扩散模型将目标域图像映射回源域分布，避免大规模合成数据生成，在分割、检测和分类任务上取得显著提升


<details>
  <summary>Details</summary>
Motivation: 生成式基础模型具有广泛的视觉知识，可以产生多样化的图像变体，但用于训练数据增强时合成全面的目标域变体仍然缓慢、昂贵且不完整。需要一种更高效的域泛化方法。

Method: 在测试时使用扩散模型将目标域图像映射回源域分布，仅需源域描述，保留任务模型，无需大规模合成数据生成。支持多种生成模型和下游模型的组合，包括集成变体增强鲁棒性。

Result: 在真实到真实的域泛化场景中，对分割、检测和分类任务在未知目标分布下取得一致改进：BDD100K-Night上相对提升137%，ImageNet-R上68%，DarkZurich上62%。

Conclusion: 测试时使用扩散模型进行域映射是一种有效的域泛化方法，避免了传统数据增强的局限性，在多种视觉任务和数据集上实现了显著性能提升。

Abstract: Generative foundation models contain broad visual knowledge and can produce diverse image variations, making them particularly promising for advancing domain generalization tasks. While they can be used for training data augmentation, synthesizing comprehensive target-domain variations remains slow, expensive, and incomplete. We propose an alternative: using diffusion models at test time to map target images back to the source distribution where the downstream model was trained. This approach requires only a source domain description, preserves the task model, and eliminates large-scale synthetic data generation. We demonstrate consistent improvements across segmentation, detection, and classification tasks under challenging environmental shifts in real-to-real domain generalization scenarios with unknown target distributions. Our analysis spans multiple generative and downstream models, including an ensemble variant for enhanced robustness. The method achieves substantial relative gains: 137% on BDD100K-Night, 68% on ImageNet-R, and 62% on DarkZurich.

</details>


### [90] [Transform Trained Transformer: Accelerating Naive 4K Video Generation Over 10$\times$](https://arxiv.org/abs/2512.13492)
*Jiangning Zhang,Junwei Zhu,Teng Hu,Yabiao Wang,Donghao Luo,Weijian Cao,Zhenye Gan,Xiaobin Hu,Zhucun Xue,Chengjie Wang*

Main category: cs.CV

Relevance: 85.0

TL;DR: T3-Video提出了一种Transformer改造策略，通过多尺度权重共享窗口注意力和分层分块设计，在不改变预训练模型核心架构的情况下，将原生4K视频生成速度提升10倍以上，同时提升生成质量。


<details>
  <summary>Details</summary>
Motivation: 原生4K视频生成面临计算复杂度爆炸式增长的挑战，全注意力机制在时空分辨率增加时计算量呈二次方增长，现有模型难以在效率和质量之间取得平衡。

Method: 提出T3（Transform Trained Transformer）策略，通过优化前向逻辑而非改变架构来降低计算需求。具体包括：1）多尺度权重共享窗口注意力机制；2）分层分块设计；3）保持轴的全注意力设计，实现"注意力模式"转换。

Result: 在4K-VBench上表现优异：VQA指标提升4.29，VTC指标提升0.08，同时将原生4K视频生成速度加速10倍以上。

Conclusion: T3-Video通过创新的Transformer改造策略，成功解决了4K视频生成中的计算效率问题，在保持甚至提升生成质量的同时实现了显著加速。

Abstract: Native 4K (2160$\times$3840) video generation remains a critical challenge due to the quadratic computational explosion of full-attention as spatiotemporal resolution increases, making it difficult for models to strike a balance between efficiency and quality. This paper proposes a novel Transformer retrofit strategy termed $\textbf{T3}$ ($\textbf{T}$ransform $\textbf{T}$rained $\textbf{T}$ransformer) that, without altering the core architecture of full-attention pretrained models, significantly reduces compute requirements by optimizing their forward logic. Specifically, $\textbf{T3-Video}$ introduces a multi-scale weight-sharing window attention mechanism and, via hierarchical blocking together with an axis-preserving full-attention design, can effect an "attention pattern" transformation of a pretrained model using only modest compute and data. Results on 4K-VBench show that $\textbf{T3-Video}$ substantially outperforms existing approaches: while delivering performance improvements (+4.29$\uparrow$ VQA and +0.08$\uparrow$ VTC), it accelerates native 4K video generation by more than 10$\times$. Project page at https://zhangzjn.github.io/projects/T3-Video

</details>


### [91] [MindDrive: A Vision-Language-Action Model for Autonomous Driving via Online Reinforcement Learning](https://arxiv.org/abs/2512.13636)
*Haoyu Fu,Diankun Zhang,Zongchuang Zhao,Jianfeng Cui,Hongwei Xie,Bing Wang,Guang Chen,Dingkang Liang,Xiang Bai*

Main category: cs.CV

Relevance: 85.0

TL;DR: MindDrive：首个将在线强化学习应用于自动驾驶VLA模型的框架，通过LLM双专家系统将连续动作空间探索转化为离散语言决策学习


<details>
  <summary>Details</summary>
Motivation: 当前自动驾驶中的视觉-语言-动作范式主要依赖模仿学习，存在分布偏移和因果混淆问题。在线强化学习能通过试错学习解决这些问题，但在连续动作空间中探索效率低下。

Method: 提出MindDrive框架：一个LLM配备两套LoRA参数，分别作为决策专家（场景推理和驾驶决策）和动作专家（将语言决策动态映射为可行轨迹）。通过将轨迹级奖励反馈到推理空间，在离散语言决策集上进行试错学习。

Result: 在Bench2Drive基准测试中取得显著性能：驾驶评分78.04，成功率55.09%。这是首个证明在线强化学习在自动驾驶VLA模型中有效性的工作。

Conclusion: MindDrive成功解决了VLA模型在自动驾驶中应用在线强化学习的探索效率问题，平衡了复杂场景中的最优决策、类人驾驶行为和高效探索。

Abstract: Current Vision-Language-Action (VLA) paradigms in autonomous driving primarily rely on Imitation Learning (IL), which introduces inherent challenges such as distribution shift and causal confusion. Online Reinforcement Learning offers a promising pathway to address these issues through trial-and-error learning. However, applying online reinforcement learning to VLA models in autonomous driving is hindered by inefficient exploration in continuous action spaces. To overcome this limitation, we propose MindDrive, a VLA framework comprising a large language model (LLM) with two distinct sets of LoRA parameters. The one LLM serves as a Decision Expert for scenario reasoning and driving decision-making, while the other acts as an Action Expert that dynamically maps linguistic decisions into feasible trajectories. By feeding trajectory-level rewards back into the reasoning space, MindDrive enables trial-and-error learning over a finite set of discrete linguistic driving decisions, instead of operating directly in a continuous action space. This approach effectively balances optimal decision-making in complex scenarios, human-like driving behavior, and efficient exploration in online reinforcement learning. MindDrive achieves strong closed-loop performance on the challenging Bench2Drive benchmark, with a Driving Score (DS) of 78.04 and a Success Rate (SR) of 55.09%. To the best of our knowledge, this is the first work to demonstrate the effectiveness of online reinforcement learning for the VLA model in autonomous driving.

</details>


### [92] [JointAVBench: A Benchmark for Joint Audio-Visual Reasoning Evaluation](https://arxiv.org/abs/2512.12772)
*Jianghan Chao,Jianzhang Gao,Wenhui Tan,Yuchong Sun,Ruihua Song,Liyun Ru*

Main category: cs.MM

Relevance: 85.0

TL;DR: JointAVBench：一个严格的音频-视频联合理解基准，涵盖多模态依赖、多样音频类型和不同场景跨度，用于评估Omni-LLMs


<details>
  <summary>Details</summary>
Motivation: 现有评估数据集在多模态依赖、音频类型多样性和场景跨度覆盖方面存在不足，无法严格全面评估能够处理视觉和音频信息的Omni-LLMs

Method: 提出自动化流水线，利用最先进的视觉-LLMs、音频-LLMs和通用LLMs合成严格需要音频-视觉联合理解的问题和答案，构建包含5个认知维度、4种音频类型和3种场景跨度的基准

Result: 评估显示，即使表现最佳的Omni-LLM平均准确率仅为62.6%，虽然优于单模态基线，但在跨场景推理等方面仍有很大改进空间

Conclusion: JointAVBench填补了严格音频-视频联合理解评估的空白，揭示了当前Omni-LLMs在跨模态推理方面的局限性，为未来研究提供了重要基准

Abstract: Understanding videos inherently requires reasoning over both visual and auditory information. To properly evaluate Omni-Large Language Models (Omni-LLMs), which are capable of processing multi-modal information including vision and audio, an effective benchmark must comprehensively cover three key aspects: (1) multi-modal dependency (i.e., questions that cannot be answered using vision or audio alone), (2) diverse audio information types (e.g., speech, sound events), and (3) varying scene spans. However, existing datasets fall short in one or more of these dimensions, limiting strict and comprehensive evaluation. To address this gap, we introduce JointAVBench, a novel benchmark with strict audio-video correlation, spanning five cognitive dimensions, four audio information types (speech, sound events, music, vocal traits), and three scene spans (single-, cross-, and full-scene). Given the high cost of manual annotation, we propose an automated pipeline that leverages state-of-the-art vision-LLMs, audio-LLMs, and general-purpose LLMs to synthesize questions and answers that strictly require joint audio-visual understanding. We evaluate leading vision-only, audio-only, and Omni-LLMs on our dataset. Results show that even the best-performing Omni-LLM achieves an average accuracy of only 62.6\%, outperforming uni-modal baselines but revealing substantial room for improvement, especially in cross-scene reasoning.

</details>


### [93] [Generalization vs. Specialization: Evaluating Segment Anything Model (SAM3) Zero-Shot Segmentation Against Fine-Tuned YOLO Detectors](https://arxiv.org/abs/2512.11884)
*Ranjan Sapkota,Konstantinos I. Roumeliotis,Manoj Karkee,Nikolaos D. Tselikas*

Main category: cs.CV

Relevance: 75.0

TL;DR: 该研究系统比较了零样本分割模型SAM3与微调YOLO11在密集苹果实例分割任务上的性能，发现YOLO在检测完整性上更优但边界稳定性差，而SAM3在掩码精度和边界稳定性上表现更佳。


<details>
  <summary>Details</summary>
Motivation: 深度学习实例分割存在两种范式：任务特定微调的专业模型和零样本分割的通用基础模型。本研究旨在系统比较这两种范式在密集实例分割任务上的表现，为实际应用提供指导。

Method: 在MinneApple数据集（670张果园图像，28,179个苹果实例）上，评估SAM3零样本模式与三种YOLO11变体（nano、medium、large）微调模型的性能。分析不同IoU阈值对性能评估的影响。

Result: 在IoU=0.15阈值下，YOLO模型F1分数为68.9%-72.2%，SAM3为59.8%。但YOLO在IoU变化时性能下降48-50点，而SAM3仅下降4点，边界稳定性是YOLO的12倍。IoU选择可夸大性能差距达30%。

Conclusion: YOLO在检测完整性上更优，适合需要高召回率的任务；SAM3在掩码精度和边界稳定性上更强，适合需要精确边界的应用。研究为密集实例分割任务中专业模型与通用模型的选择提供了实证依据。

Abstract: Deep learning has advanced two fundamentally different paradigms for instance segmentation: specialized models optimized through task-specific fine-tuning and generalist foundation models capable of zero-shot segmentation. This work presents a comprehensive comparison between SAM3 (Segment Anything Model, also called SAMv3) operating in zero-shot mode and three variants of Ultralytics YOLO11 (nano, medium, and large) fine-tuned for instance segmentation. The evaluation is conducted on the MinneApple dataset, a dense benchmark comprising 670 orchard images with 28,179 annotated apple instances, enabling rigorous validation of model behavior under high object density and occlusion. Our analysis shows IoU choices can inflate performance gaps by up to 30%. At the appropriate IoU = 0.15 threshold, YOLO models achieve 68.9%, 72.2%, and 71.9% F1, while SAM3 reaches 59.8% in pure zero-shot mode. However, YOLO exhibits steep degradation 48-50 points across IoU ranges whereas SAM3 drops only 4 points, revealing 12 times superior boundary stability of SAM3. This highlights the strength of SAMv3 in mask precision versus specialization in detection completeness of YOLO11. We provide open-source code, evaluation pipelines, and methodological recommendations, contributing to a deeper understanding of when specialized fine-tuned models or generalist foundation models are preferable for dense instance segmentation tasks. This project repository is available on GitHub as https://github.com/Applied-AI-Research-Lab/Segment-Anything-Model-SAM3-Zero-Shot-Segmentation-Against-Fine-Tuned-YOLO-Detectors

</details>


### [94] [V-REX: Benchmarking Exploratory Visual Reasoning via Chain-of-Questions](https://arxiv.org/abs/2512.11995)
*Chenrui Fan,Yijun Liang,Shweta Bhardwaj,Kwesi Cobbina,Ming Li,Tianyi Zhou*

Main category: cs.CV

Relevance: 75.0

TL;DR: V-REX是一个评估视觉语言模型多步探索推理能力的评测套件，包含挑战性视觉推理任务和评估协议，将多步探索推理分解为问题链，评估模型的规划和执行能力。


<details>
  <summary>Details</summary>
Motivation: 当前视觉语言模型在处理需要多步探索推理的复杂开放任务时表现不佳，现有基准主要关注定义明确、目标明确的问题，缺乏对多步视觉思维路径的评估。

Method: 开发V-REX评估套件，将多步探索推理转化为问题链(CoQ)，通过有限选项实现可靠量化分析，评估模型在规划和执行两个维度的能力。

Result: 评估了SOTA专有和开源VLM，揭示了模型在多步探索推理中的一致扩展趋势、规划与执行能力间的显著差异，以及巨大的改进空间。

Conclusion: V-REX填补了视觉语言模型多步探索推理评估的空白，为模型能力提供了细粒度分析，揭示了当前模型的局限性，为未来研究指明了方向。

Abstract: While many vision-language models (VLMs) are developed to answer well-defined, straightforward questions with highly specified targets, as in most benchmarks, they often struggle in practice with complex open-ended tasks, which usually require multiple rounds of exploration and reasoning in the visual space. Such visual thinking paths not only provide step-by-step exploration and verification as an AI detective but also produce better interpretations of the final answers. However, these paths are challenging to evaluate due to the large exploration space of intermediate steps. To bridge the gap, we develop an evaluation suite, ``Visual Reasoning with multi-step EXploration (V-REX)'', which is composed of a benchmark of challenging visual reasoning tasks requiring native multi-step exploration and an evaluation protocol. V-REX covers rich application scenarios across diverse domains. V-REX casts the multi-step exploratory reasoning into a Chain-of-Questions (CoQ) and disentangles VLMs' capability to (1) Planning: breaking down an open-ended task by selecting a chain of exploratory questions; and (2) Following: answering curated CoQ sequentially to collect information for deriving the final answer. By curating finite options of questions and answers per step, V-REX achieves a reliable quantitative and fine-grained analysis of the intermediate steps. By assessing SOTA proprietary and open-sourced VLMs, we reveal consistent scaling trends, significant differences between planning and following abilities, and substantial room for improvement in multi-step exploratory reasoning.

</details>


### [95] [BAgger: Backwards Aggregation for Mitigating Drift in Autoregressive Video Diffusion Models](https://arxiv.org/abs/2512.12080)
*Ryan Po,Eric Ryan Chan,Changan Chen,Gordon Wetzstein*

Main category: cs.CV

Relevance: 75.0

TL;DR: BAgger是一种自监督训练方案，通过从模型自身生成的轨迹中构建纠正轨迹，解决自回归视频模型中的曝光偏差问题，提升长期视频生成的稳定性和视觉一致性。


<details>
  <summary>Details</summary>
Motivation: 自回归视频模型通过下一帧预测进行世界建模，但存在曝光偏差问题：训练时使用干净上下文，推理时使用自生成帧，导致误差累积和质量漂移。需要一种方法让模型学会从自身错误中恢复。

Method: 提出Backwards Aggregation (BAgger)自监督方案，从模型自身生成的轨迹中构建纠正轨迹。与依赖少步蒸馏和分布匹配损失的方法不同，BAgger使用标准的分数匹配或流匹配目标，避免大型教师模型和长时间链的反向传播。

Result: 在因果扩散Transformer上实例化BAgger，在文本到视频、视频扩展和多提示生成任务中评估，观察到更稳定的长期运动和更好的视觉一致性，减少了质量漂移。

Conclusion: BAgger通过自监督纠正轨迹训练，有效解决了自回归视频模型的曝光偏差问题，提升了长期视频生成的质量和稳定性，为世界建模提供了更可靠的框架。

Abstract: Autoregressive video models are promising for world modeling via next-frame prediction, but they suffer from exposure bias: a mismatch between training on clean contexts and inference on self-generated frames, causing errors to compound and quality to drift over time. We introduce Backwards Aggregation (BAgger), a self-supervised scheme that constructs corrective trajectories from the model's own rollouts, teaching it to recover from its mistakes. Unlike prior approaches that rely on few-step distillation and distribution-matching losses, which can hurt quality and diversity, BAgger trains with standard score or flow matching objectives, avoiding large teachers and long-chain backpropagation through time. We instantiate BAgger on causal diffusion transformers and evaluate on text-to-video, video extension, and multi-prompt generation, observing more stable long-horizon motion and better visual consistency with reduced drift.

</details>


### [96] [RePack: Representation Packing of Vision Foundation Model Features Enhances Diffusion Transformer](https://arxiv.org/abs/2512.12083)
*Guanfang Dong,Luke Schultz,Negar Hassanpour,Chao Gao*

Main category: cs.CV

Relevance: 75.0

TL;DR: RePack通过将高维视觉基础模型特征投影到低维流形，解决了信息过载问题，加速了扩散变换器的收敛，在图像重建任务中取得了更好的性能。


<details>
  <summary>Details</summary>
Motivation: 预训练视觉基础模型的高维特征虽然能增强潜在扩散模型，但会导致信息过载问题，特别是当VFM特征尺寸超过原始图像解码需求时。需要一种方法在保留VFM特征效用的同时解决维度问题。

Method: 提出RePack框架，将高维VFM表示投影到低维流形上，生成更紧凑、解码器友好的表示。这种方法能过滤非语义噪声，同时保留高保真重建所需的核心结构信息。

Result: 在DiT-XL/2上，RePack仅用64个epoch就达到了3.66的FID分数，比最先进方法快35%。实验表明RePack能有效加速DiT收敛，并在图像重建任务中优于直接注入原始VFM特征的方法。

Conclusion: RePack成功提取了VFM表示的核心语义，同时避免了高维度带来的副作用，为高效利用视觉基础模型特征提供了简单有效的解决方案。

Abstract: The superior representation capability of pre-trained vision foundation models (VFMs) has been harnessed for enhancing latent diffusion models (LDMs). These approaches inject the rich semantics from high-dimensional VFM representations (e.g., DINOv3) into LDMs at different phases, resulting in accelerated learning and better generation performance. However, the high-dimensionality of VFM representations may also lead to Information Overload, particularly when the VFM features exceed the size of the original image for decoding. To address this issue while preserving the utility of VFM features, we propose RePack (Representation Packing), a simple yet effective framework for improving Diffusion Transformers (DiTs). RePack transforms the VFM representation into a more compact, decoder-friendly representation by projecting onto low-dimensional manifolds. We find that RePack can effectively filter out non-semantic noise while preserving the core structural information needed for high-fidelity reconstruction. Experimental results show that RePack significantly accelerates DiT convergence and outperforms recent methods that directly inject raw VFM features into the decoder for image reconstruction. On DiT-XL/2, RePack achieves an FID of 3.66 in only 64 epochs, which is 35% faster than the state-of-the-art method. This demonstrates that RePack successfully extracts the core semantics of VFM representations while bypassing their high-dimensionality side effects.

</details>


### [97] [SPDMark: Selective Parameter Displacement for Robust Video Watermarking](https://arxiv.org/abs/2512.12090)
*Samar Fares,Nurbek Tastan,Karthik Nandakumar*

Main category: cs.CV

Relevance: 75.0

TL;DR: SPDMark是一种基于选择性参数位移的视频生成水印框架，通过修改生成模型的参数子集嵌入水印，使用LoRA实现基础位移，结合密码哈希实现帧级水印，具有不可感知性、鲁棒性和计算效率。


<details>
  <summary>Details</summary>
Motivation: 高质量视频生成模型的兴起增加了对鲁棒水印方案的需求，用于可靠检测和追踪生成视频的来源。现有视频水印方法（包括后处理和生成中方法）无法同时实现不可感知性、鲁棒性和计算效率。

Method: 提出SPDMark框架，基于视频扩散模型的选择性参数位移。水印通过修改生成模型的参数子集嵌入，建模为层间基础位移的加性组合，使用LoRA实现参数效率。训练时联合学习基础位移和水印提取器，最小化消息恢复、感知相似性和时间一致性损失。使用密码哈希函数从基础水印密钥派生帧特定水印消息，通过最大二分匹配恢复帧顺序。

Result: 在文本到视频和图像到视频生成模型上的评估表明，SPDMark能够生成不可感知的水印，并以高准确率恢复水印，同时对各种常见视频修改具有鲁棒性。

Conclusion: SPDMark为视频生成水印提供了有效的解决方案，解决了现有方法在不可感知性、鲁棒性和计算效率之间的权衡问题，为生成视频的来源追踪提供了可靠工具。

Abstract: The advent of high-quality video generation models has amplified the need for robust watermarking schemes that can be used to reliably detect and track the provenance of generated videos. Existing video watermarking methods based on both post-hoc and in-generation approaches fail to simultaneously achieve imperceptibility, robustness, and computational efficiency. This work introduces a novel framework for in-generation video watermarking called SPDMark (pronounced `SpeedMark') based on selective parameter displacement of a video diffusion model. Watermarks are embedded into the generated videos by modifying a subset of parameters in the generative model. To make the problem tractable, the displacement is modeled as an additive composition of layer-wise basis shifts, where the final composition is indexed by the watermarking key. For parameter efficiency, this work specifically leverages low-rank adaptation (LoRA) to implement the basis shifts. During the training phase, the basis shifts and the watermark extractor are jointly learned by minimizing a combination of message recovery, perceptual similarity, and temporal consistency losses. To detect and localize temporal modifications in the watermarked videos, we use a cryptographic hashing function to derive frame-specific watermark messages from the given base watermarking key. During watermark extraction, maximum bipartite matching is applied to recover the correct frame order, even from temporally tampered videos. Evaluations on both text-to-video and image-to-video generation models demonstrate the ability of SPDMark to generate imperceptible watermarks that can be recovered with high accuracy and also establish its robustness against a variety of common video modifications.

</details>


### [98] [MetaTPT: Meta Test-time Prompt Tuning for Vision-Language Models](https://arxiv.org/abs/2512.12268)
*Yuqing Lei,Yingjun Du,Yawen Huang,Xiantong Zhen,Ling Shao*

Main category: cs.CV

Relevance: 75.0

TL;DR: MetaTPT：一种元学习框架，通过自监督辅助任务动态学习参数化增强，指导测试时提示调优，提升视觉语言模型在域偏移下的适应能力。


<details>
  <summary>Details</summary>
Motivation: 尽管CLIP等视觉语言模型具有强大的零样本泛化能力，但在测试时对域偏移仍然敏感。现有的测试时提示调优方法使用固定增强，在更具挑战性的场景中可能失效。

Method: 提出MetaTPT元学习框架，采用双循环优化范式：内循环学习自监督任务生成信息丰富的视图，外循环通过强制这些视图间的一致性进行提示调优。框架动态学习参数化增强，为每个样本创建更具表达力的变换。

Result: 在域泛化和跨数据集基准测试中，MetaTPT实现了最先进的性能表现。

Conclusion: 通过将增强学习与提示调优耦合，MetaTPT显著改善了视觉语言模型在域偏移下的测试时适应能力。

Abstract: Vision-language models (VLMs) such as CLIP exhibit strong zero-shot generalization but remain sensitive to domain shifts at test time. Test-time prompt tuning (TPT) mitigates this issue by adapting prompts with fixed augmentations, which may falter in more challenging settings. In this work, we propose Meta Test-Time Prompt Tuning (MetaTPT), a meta-learning framework that learns a self-supervised auxiliary task to guide test-time prompt tuning. The auxiliary task dynamically learns parameterized augmentations for each sample, enabling more expressive transformations that capture essential features in target domains. MetaTPT adopts a dual-loop optimization paradigm: an inner loop learns a self-supervised task that generates informative views, while the outer loop performs prompt tuning by enforcing consistency across these views. By coupling augmentation learning with prompt tuning, MetaTPT improves test-time adaptation under domain shifts. Extensive experiments demonstrate that MetaTPT achieves state-of-the-art performance on domain generalization and cross-dataset benchmarks.

</details>


### [99] [Cognitive-YOLO: LLM-Driven Architecture Synthesis from First Principles of Data for Object Detection](https://arxiv.org/abs/2512.12281)
*Jiahao Zhao*

Main category: cs.CV

Relevance: 75.0

TL;DR: Cognitive-YOLO：基于LLM的架构合成框架，直接从数据集特征生成目标检测网络配置，无需传统NAS搜索循环


<details>
  <summary>Details</summary>
Motivation: 传统目标检测架构设计耗时费力，NAS计算成本过高，现有LLM方法仅作为搜索循环中的优化器而非从数据整体理解直接生成架构

Method: 三阶段框架：1) 分析模块提取数据集元特征（目标尺度分布、场景密度等）；2) LLM基于这些特征和RAG检索的SOTA组件进行推理，合成结构化NADL描述；3) 编译器将描述实例化为可部署模型

Result: 在五个不同目标检测数据集上，Cognitive-YOLO生成的架构始终优于基线模型，在多个基准测试中展现出优越的性能-参数权衡

Conclusion: LLM的数据驱动推理是性能提升的主要驱动力，证明对数据"第一原理"的深入理解比单纯检索SOTA组件对获得优越架构更为关键

Abstract: Designing high-performance object detection architectures is a complex task, where traditional manual design is time-consuming and labor-intensive, and Neural Architecture Search (NAS) is computationally prohibitive. While recent approaches using Large Language Models (LLMs) show promise, they often function as iterative optimizers within a search loop, rather than generating architectures directly from a holistic understanding of the data. To address this gap, we propose Cognitive-YOLO, a novel framework for LLM-driven architecture synthesis that generates network configurations directly from the intrinsic characteristics of the dataset. Our method consists of three stages: first, an analysis module extracts key meta-features (e.g., object scale distribution and scene density) from the target dataset; second, the LLM reasons upon these features, augmented with state-of-the-art components retrieved via Retrieval-Augmented Generation (RAG), to synthesize the architecture into a structured Neural Architecture Description Language (NADL); finally, a compiler instantiates this description into a deployable model. Extensive experiments on five diverse object detection datasets demonstrate that our proposed Cognitive-YOLO consistently generates superior architectures, achieving highly competitive performance and demonstrating a superior performance-per-parameter trade-off compared to strong baseline models across multiple benchmarks. Crucially, our ablation studies prove that the LLM's data-driven reasoning is the primary driver of performance, demonstrating that a deep understanding of data "first principles" is more critical for achieving a superior architecture than simply retrieving SOTA components.

</details>


### [100] [GrowTAS: Progressive Expansion from Small to Large Subnets for Efficient ViT Architecture Search](https://arxiv.org/abs/2512.12296)
*Hyunju Lee,Youngmin Oh,Jeimin Jeon,Donghyeon Baek,Bumsub Ham*

Main category: cs.CV

Relevance: 75.0

TL;DR: GrowTAS：一种渐进式Transformer架构搜索框架，通过从小型子网开始训练并逐步加入大型子网，减少权重共享带来的干扰，提升搜索效果。


<details>
  <summary>Details</summary>
Motivation: 现有Transformer架构搜索方法使用超网络共享权重，导致小型子网性能严重下降。研究发现训练良好的小型子网可以作为训练大型子网的良好基础。

Method: 提出渐进式训练框架GrowTAS，从小型子网开始训练，逐步加入更大的子网，减少干扰并稳定训练过程。还提出GrowTAS+，仅微调部分权重以进一步提升大型子网性能。

Result: 在ImageNet和多个迁移学习基准测试（CIFAR-10/100, Flowers, CARS, INAT-19）上验证了方法的有效性，优于现有TAS方法。

Conclusion: 渐进式训练策略能有效缓解超网络中权重共享带来的干扰问题，提升Transformer架构搜索的性能。

Abstract: Transformer architecture search (TAS) aims to automatically discover efficient vision transformers (ViTs), reducing the need for manual design. Existing TAS methods typically train an over-parameterized network (i.e., a supernet) that encompasses all candidate architectures (i.e., subnets). However, all subnets share the same set of weights, which leads to interference that degrades the smaller subnets severely. We have found that well-trained small subnets can serve as a good foundation for training larger ones. Motivated by this, we propose a progressive training framework, dubbed GrowTAS, that begins with training small subnets and incorporate larger ones gradually. This enables reducing the interference and stabilizing a training process. We also introduce GrowTAS+ that fine-tunes a subset of weights only to further enhance the performance of large subnets. Extensive experiments on ImageNet and several transfer learning benchmarks, including CIFAR-10/100, Flowers, CARS, and INAT-19, demonstrate the effectiveness of our approach over current TAS methods

</details>


### [101] [VideoARM: Agentic Reasoning over Hierarchical Memory for Long-Form Video Understanding](https://arxiv.org/abs/2512.12360)
*Yufei Yin,Qianke Meng,Minghao Chen,Jiajun Ding,Zhenwei Shao,Zhou Yu*

Main category: cs.CV

Relevance: 75.0

TL;DR: VideoARM：一种用于长视频理解的代理式分层记忆推理范式，通过自适应、实时的代理推理和记忆构建，显著减少token消耗并提升性能


<details>
  <summary>Details</summary>
Motivation: 现有长视频理解方法依赖手工设计的推理流程或token消耗大的视频预处理，无法有效处理长时间结构和密集多模态线索。需要更高效、自适应的解决方案。

Method: 提出VideoARM（Agentic Reasoning-over-hierarchical-Memory）范式：1）控制器自主调用工具进行粗到细的视频解析；2）分层多模态记忆持续捕获和更新多级线索；3）自适应循环（观察-思考-行动-记忆）减少token消耗

Result: 在主流基准测试中超越当前最优方法DVD，同时显著减少长视频处理的token消耗

Conclusion: VideoARM通过代理式分层记忆推理范式，实现了更高效、自适应的长视频理解，为多模态大语言模型的长视频处理提供了新思路

Abstract: Long-form video understanding remains challenging due to the extended temporal structure and dense multimodal cues. Despite recent progress, many existing approaches still rely on hand-crafted reasoning pipelines or employ token-consuming video preprocessing to guide MLLMs in autonomous reasoning. To overcome these limitations, we introduce VideoARM, an Agentic Reasoning-over-hierarchical-Memory paradigm for long-form video understanding. Instead of static, exhaustive preprocessing, VideoARM performs adaptive, on-the-fly agentic reasoning and memory construction. Specifically, VideoARM performs an adaptive and continuous loop of observing, thinking, acting, and memorizing, where a controller autonomously invokes tools to interpret the video in a coarse-to-fine manner, thereby substantially reducing token consumption. In parallel, a hierarchical multimodal memory continuously captures and updates multi-level clues throughout the operation of the agent, providing precise contextual information to support the controller in decision-making. Experiments on prevalent benchmarks demonstrate that VideoARM outperforms the state-of-the-art method, DVD, while significantly reducing token consumption for long-form videos.

</details>


### [102] [Speedrunning ImageNet Diffusion](https://arxiv.org/abs/2512.12386)
*Swayam Bhanded*

Main category: cs.CV

Relevance: 75.0

TL;DR: SR-DiT框架通过系统整合令牌路由、架构改进和训练修改，在仅140M参数、400K迭代下达到ImageNet-256上FID 3.49的SOTA结果，媲美更大模型性能。


<details>
  <summary>Details</summary>
Motivation: 扩散transformer训练效率已有显著提升，但现有技术多孤立研究，未探索多种方法组合的协同效应。本文旨在系统整合不同技术，发掘其潜在协同作用。

Method: 提出SR-DiT框架，在表示对齐基础上系统集成：1) 令牌路由技术；2) 架构改进；3) 训练修改。通过大量消融实验验证不同技术组合的有效性。

Result: 在ImageNet-256上仅用140M参数模型、400K迭代（无分类器引导）达到FID 3.49和KDD 0.319，与更大模型（685M参数）长时间训练结果相当，是该规模下的SOTA。

Conclusion: 系统整合多种技术能显著提升扩散transformer效率，识别了最有效的技术组合及其协同/不兼容性。发布框架作为未来研究的计算可访问基线。

Abstract: Recent advances have significantly improved the training efficiency of diffusion transformers. However, these techniques have largely been studied in isolation, leaving unexplored the potential synergies from combining multiple approaches. We present SR-DiT (Speedrun Diffusion Transformer), a framework that systematically integrates token routing, architectural improvements, and training modifications on top of representation alignment. Our approach achieves FID 3.49 and KDD 0.319 on ImageNet-256 using only a 140M parameter model at 400K iterations without classifier-free guidance - comparable to results from 685M parameter models trained significantly longer. To our knowledge, this is a state-of the-art result at this model size. Through extensive ablation studies, we identify which technique combinations are most effective and document both synergies and incompatibilities. We release our framework as a computationally accessible baseline for future research.

</details>


### [103] [Advancing Cache-Based Few-Shot Classification via Patch-Driven Relational Gated Graph Attention](https://arxiv.org/abs/2512.12498)
*Tasweer Ahmad,Arindam Sikdar,Sandip Pradhan,Ardhendu Behera*

Main category: cs.CV

Relevance: 75.0

TL;DR: 提出了一种基于图注意力网络的补丁驱动关系精化方法，用于改进CLIP在少样本图像分类中的表现，通过捕捉图像内部补丁依赖关系来增强判别性表示，在保持零样本效率的同时提升分类性能。


<details>
  <summary>Details</summary>
Motivation: 现有基于缓存的适配方法（如Tip-Adapter）虽然通过轻量级残差适配器缓解了少样本图像分类的挑战，但仍继承了CLIP编码全局通用表示的倾向，在低数据域中无法最优地将通用模型适配到特定领域。需要更细粒度的、基于补丁关系的表示来增强判别能力。

Method: 提出补丁驱动关系精化方法：1）使用关系门控图注意力网络构建补丁图，执行边缘感知注意力来强调信息丰富的补块间交互，生成上下文丰富的补丁嵌入；2）可学习的多聚合池化将这些嵌入组合成紧凑、任务判别性的表示；3）仅在训练期间使用图精化将关系结构蒸馏到缓存中，推理时无额外成本；4）通过缓存相似度分数与CLIP零样本logits的残差融合获得最终预测。

Result: 在11个基准测试上评估，相比最先进的CLIP适配器和基于缓存的方法获得了一致的性能提升，同时保持了零样本效率。还引入了一个"受伤与未受伤士兵"数据集用于战场伤亡识别验证。

Conclusion: 提出的补丁驱动关系精化方法通过捕捉图像内部补丁依赖关系，显著提升了少样本图像分类的性能，在保持推理效率的同时增强了模型在特定领域的判别能力，特别适用于时间关键的战场医疗救援等应用场景。

Abstract: Few-shot image classification remains difficult under limited supervision and visual domain shift. Recent cache-based adaptation approaches (e.g., Tip-Adapter) address this challenge to some extent by learning lightweight residual adapters over frozen features, yet they still inherit CLIP's tendency to encode global, general-purpose representations that are not optimally discriminative to adapt the generalist to the specialist's domain in low-data regimes. We address this limitation with a novel patch-driven relational refinement that learns cache adapter weights from intra-image patch dependencies rather than treating an image embedding as a monolithic vector. Specifically, we introduce a relational gated graph attention network that constructs a patch graph and performs edge-aware attention to emphasize informative inter-patch interactions, producing context-enriched patch embeddings. A learnable multi-aggregation pooling then composes these into compact, task-discriminative representations that better align cache keys with the target few-shot classes. Crucially, the proposed graph refinement is used only during training to distil relational structure into the cache, incurring no additional inference cost beyond standard cache lookup. Final predictions are obtained by a residual fusion of cache similarity scores with CLIP zero-shot logits. Extensive evaluations on 11 benchmarks show consistent gains over state-of-the-art CLIP adapter and cache-based baselines while preserving zero-shot efficiency. We further validate battlefield relevance by introducing an Injured vs. Uninjured Soldier dataset for casualty recognition. It is motivated by the operational need to support triage decisions within the "platinum minutes" and the broader "golden hour" window in time-critical UAV-driven search-and-rescue and combat casualty care.

</details>


### [104] [Reasoning Within the Mind: Dynamic Multimodal Interleaving in Latent Space](https://arxiv.org/abs/2512.12623)
*Chengzhi Liu,Yuzhe Yang,Yue Fan,Qingyue Wei,Sheng Liu,Xin Eric Wang*

Main category: cs.CV

Relevance: 75.0

TL;DR: DMLR提出了一种动态多模态潜在推理框架，通过置信度引导的潜在策略梯度优化来精炼潜在思考标记，实现视觉与文本的动态交织推理，显著提升多模态推理性能并保持高效推理。


<details>
  <summary>Details</summary>
Motivation: 现有多模态大语言模型虽然通过思维链机制增强了跨模态理解，但仍依赖显式的逐步推理，存在感知-推理交互不稳定和计算开销大的问题。受人类认知启发，作者认为思考应该是推理和感知在思维中动态交织的过程，而非线性展开。

Method: 提出DMLR框架：1) 使用置信度引导的潜在策略梯度优化来精炼潜在思考标记进行深度推理；2) 引入动态视觉注入策略，在每个潜在思考标记处检索最相关的视觉特征并更新最佳视觉补丁集；3) 将更新的补丁注入潜在思考标记，实现动态视觉-文本交织。

Result: 在七个多模态推理基准测试和多种模型架构上的实验表明，DMLR显著提升了推理和感知性能，同时保持了较高的推理效率。

Conclusion: DMLR通过动态多模态潜在推理框架，实现了更自然、高效的多模态推理，模拟了人类认知中推理与感知的动态交织过程，为多模态大语言模型的推理能力提升提供了新思路。

Abstract: Recent advancements in Multimodal Large Language Models (MLLMs) have significantly enhanced cross-modal understanding and reasoning by incorporating Chain-of-Thought (CoT) reasoning in the semantic space. Building upon this, recent studies extend the CoT mechanism to the visual modality, enabling models to integrate visual information during reasoning through external tools or explicit image generation. However, these methods remain dependent on explicit step-by-step reasoning, unstable perception-reasoning interaction and notable computational overhead. Inspired by human cognition, we posit that thinking unfolds not linearly but through the dynamic interleaving of reasoning and perception within the mind. Motivated by this perspective, we propose DMLR, a test-time Dynamic Multimodal Latent Reasoning framework that employs confidence-guided latent policy gradient optimization to refine latent think tokens for in-depth reasoning. Furthermore, a Dynamic Visual Injection Strategy is introduced, which retrieves the most relevant visual features at each latent think token and updates the set of best visual patches. The updated patches are then injected into latent think token to achieve dynamic visual-textual interleaving. Experiments across seven multimodal reasoning benchmarks and various model architectures demonstrate that DMLR significantly improves reasoning and perception performance while maintaining high inference efficiency.

</details>


### [105] [DiG: Differential Grounding for Enhancing Fine-Grained Perception in Multimodal Large Language Model](https://arxiv.org/abs/2512.12633)
*Zhou Tao,Shida Wang,Yongxiang Hua,Haoyu Cao,Linli Xu*

Main category: cs.CV

Relevance: 75.0

TL;DR: DiG提出了一种新颖的代理任务框架，通过让多模态大语言模型识别相似图像对之间的所有差异来学习细粒度视觉感知，无需预先知道差异数量，显著提升了MLLM的精细视觉推理能力。


<details>
  <summary>Details</summary>
Motivation: 当前多模态大语言模型在视觉语言任务上表现出色，但在细粒度视觉感知和精确空间推理方面仍有局限。需要一种能够提升模型精细视觉理解能力的方法。

Method: 1. 提出DiG代理任务框架：让MLLM识别相似图像对之间的所有差异，无需预先知道差异数量
2. 开发自动化3D渲染数据生成流程：生成高质量配对图像，完全可控的差异
3. 采用课程学习策略：从单个差异逐步增加到多个差异，解决差异信号稀疏性问题

Result: DiG显著提升了模型在各种视觉感知基准测试中的性能，学习到的细粒度感知技能能有效迁移到标准下游任务，包括RefCOCO、RefCOCO+、RefCOCOg和通用多模态感知基准。

Conclusion: 差异定位是一种可扩展且稳健的方法，能够显著推进MLLM中的细粒度视觉推理能力。

Abstract: Multimodal Large Language Models have achieved impressive performance on a variety of vision-language tasks, yet their fine-grained visual perception and precise spatial reasoning remain limited. In this work, we introduce DiG (Differential Grounding), a novel proxy task framework where MLLMs learn fine-grained perception by identifying and localizing all differences between similar image pairs without prior knowledge of their number. To support scalable training, we develop an automated 3D rendering-based data generation pipeline that produces high-quality paired images with fully controllable discrepancies. To address the sparsity of difference signals, we further employ curriculum learning that progressively increases complexity from single to multiple differences, enabling stable optimization. Extensive experiments demonstrate that DiG significantly improves model performance across a variety of visual perception benchmarks and that the learned fine-grained perception skills transfer effectively to standard downstream tasks, including RefCOCO, RefCOCO+, RefCOCOg, and general multimodal perception benchmarks. Our results highlight differential grounding as a scalable and robust approach for advancing fine-grained visual reasoning in MLLMs.

</details>


### [106] [$β$-CLIP: Text-Conditioned Contrastive Learning for Multi-Granular Vision-Language Alignment](https://arxiv.org/abs/2512.12678)
*Fatimah Zohra,Chen Zhao,Hani Itani,Bernard Ghanem*

Main category: cs.CV

Relevance: 75.0

TL;DR: β-CLIP提出多粒度文本条件对比学习框架，通过层次化对齐从完整描述到句子、短语的不同文本粒度与对应视觉区域，改进细粒度视觉语言对齐。


<details>
  <summary>Details</summary>
Motivation: CLIP在全局视觉-文本对齐上表现良好，但在细粒度任务上表现不佳，即使使用详细描述进行微调。现有方法难以处理不同粒度文本与视觉区域之间的层次化对齐。

Method: 提出β-CLIP框架：1) 多粒度文本条件对比学习，对齐完整描述、句子和短语与对应视觉区域；2) 使用交叉注意力动态池化图像块生成上下文视觉嵌入；3) 引入β-上下文对比对齐损失(β-CAL)，参数化严格查询匹配与宽松图像内上下文化之间的权衡。

Result: 在Urban1K上达到91.8% T2I和92.3% I2T的R@1，在FG-OVD(Hard)上达到30.9%，在没有使用硬负样本的方法中达到最先进水平，显著改进了密集对齐。

Conclusion: β-CLIP为密集视觉语言对应建立了鲁棒、自适应的基准，通过层次化多粒度对齐有效解决了细粒度视觉语言任务中的对齐挑战。

Abstract: CLIP achieves strong zero-shot image-text retrieval by aligning global vision and text representations, yet it falls behind on fine-grained tasks even when fine-tuned on long, detailed captions. In this work, we propose $β$-CLIP, a multi-granular text-conditioned contrastive learning framework designed to achieve hierarchical alignment between multiple textual granularities-from full captions to sentences and phrases-and their corresponding visual regions. For each level of granularity, $β$-CLIP utilizes cross-attention to dynamically pool image patches, producing contextualized visual embeddings. To address the semantic overlap inherent in this hierarchy, we introduce the $β$-Contextualized Contrastive Alignment Loss ($β$-CAL). This objective parameterizes the trade-off between strict query-specific matching and relaxed intra-image contextualization, supporting both soft Cross-Entropy and hard Binary Cross-Entropy formulations. Through extensive experiments, we demonstrate that $β$-CLIP significantly improves dense alignment: achieving 91.8% T2I 92.3% I2T at R@1 on Urban1K and 30.9% on FG-OVD (Hard), setting state-of-the-art among methods trained without hard negatives. $β$-CLIP establishes a robust, adaptive baseline for dense vision-language correspondence. The code and models are released at https://github.com/fzohra/B-CLIP.

</details>


### [107] [FysicsWorld: A Unified Full-Modality Benchmark for Any-to-Any Understanding, Generation, and Reasoning](https://arxiv.org/abs/2512.12756)
*Yue Jiang,Dingkang Yang,Minghao Han,Jinghang Han,Zizhi Chen,Yizhou Liu,Mingcheng Li,Peng Zhai,Lihua Zhang*

Main category: cs.CV

Relevance: 75.0

TL;DR: FysicsWorld是首个统一的全模态基准测试，支持图像、视频、音频和文本的双向输入输出，包含16个主要任务和3,268个样本，用于评估多模态大语言模型在理解、生成和推理方面的能力。


<details>
  <summary>Details</summary>
Motivation: 当前多模态基准测试存在局限性：模态覆盖不完整、仅限于文本输出、模态间依赖性和互补性弱。需要建立一个全面的全模态评估框架来推动下一代多模态架构的发展。

Method: 1) 构建FysicsWorld基准测试，包含16个任务和3,268个样本，覆盖图像、视频、音频和文本四种模态；2) 提出跨模态互补性筛选(CMCS)策略，用于生成口语交互和融合依赖的跨模态推理数据；3) 从40多个高质量来源收集数据，涵盖开放域类别和多样化问题类型。

Result: 评估了30多个最先进的基线模型（包括MLLMs、模态特定模型、统一理解-生成模型和全模态语言模型），揭示了模型在理解、生成和推理方面的性能差异和局限性。

Conclusion: FysicsWorld为评估和推进下一代全模态架构建立了统一的基础和强基线，填补了当前基准测试在模态覆盖和交互能力方面的空白。

Abstract: Despite rapid progress in multimodal large language models (MLLMs) and emerging omni-modal architectures, current benchmarks remain limited in scope and integration, suffering from incomplete modality coverage, restricted interaction to text-centric outputs, and weak interdependence and complementarity among modalities. To bridge these gaps, we introduce FysicsWorld, the first unified full-modality benchmark that supports bidirectional input-output across image, video, audio, and text, enabling comprehensive any-to-any evaluation across understanding, generation, and reasoning. FysicsWorld encompasses 16 primary tasks and 3,268 curated samples, aggregated from over 40 high-quality sources and covering a rich set of open-domain categories with diverse question types. We also propose the Cross-Modal Complementarity Screening (CMCS) strategy integrated in a systematic data construction framework that produces omni-modal data for spoken interaction and fusion-dependent cross-modal reasoning. Through a comprehensive evaluation of over 30 state-of-the-art baselines, spanning MLLMs, modality-specific models, unified understanding-generation models, and omni-modal language models, FysicsWorld exposes the performance disparities and limitations across models in understanding, generation, and reasoning. Our benchmark establishes a unified foundation and strong baselines for evaluating and advancing next-generation full-modality architectures.

</details>


### [108] [Lemon: A Unified and Scalable 3D Multimodal Model for Universal Spatial Understanding](https://arxiv.org/abs/2512.12822)
*Yongyuan Liang,Xiyao Wang,Yuanchen Ju,Jianwei Yang,Furong Huang*

Main category: cs.CV

Relevance: 75.0

TL;DR: Lemon是一个统一的多模态Transformer架构，将3D点云补丁和语言token作为单一序列处理，实现早期空间-语言融合，在3D理解和推理任务上达到SOTA性能。


<details>
  <summary>Details</summary>
Motivation: 现有3D多模态模型面临三个主要挑战：1) 点云数据稀疏不规则；2) 依赖碎片化架构和模态特定编码器；3) 训练流程不稳定且扩展性差。需要统一架构来解决这些问题。

Method: 提出统一Transformer架构，将3D点云补丁和语言token作为单一序列联合处理。开发结构化补丁化和token化方案保持空间上下文，采用三阶段训练课程：从对象级识别到场景级空间推理逐步构建能力。

Result: 在全面的3D理解和推理任务上（从对象识别、描述到3D场景空间推理）建立了新的SOTA性能，同时展示了随着模型规模和训练数据增加而具有的稳健扩展特性。

Conclusion: Lemon为推进现实世界应用中的3D空间智能提供了统一基础，通过早期空间-语言融合、消除冗余编码器、提高参数效率，支持更有效的模型扩展。

Abstract: Scaling large multimodal models (LMMs) to 3D understanding poses unique challenges: point cloud data is sparse and irregular, existing models rely on fragmented architectures with modality-specific encoders, and training pipelines often suffer from instability and poor scalability. We introduce Lemon, a unified transformer architecture that addresses these challenges by jointly processing 3D point cloud patches and language tokens as a single sequence. Unlike prior work that relies on modality-specific encoders and cross-modal alignment modules, this design enables early spatial-linguistic fusion, eliminates redundant encoders, improves parameter efficiency, and supports more effective model scaling. To handle the complexity of 3D data, we develop a structured patchification and tokenization scheme that preserves spatial context, and a three-stage training curriculum that progressively builds capabilities from object-level recognition to scene-level spatial reasoning. Lemon establishes new state-of-the-art performance across comprehensive 3D understanding and reasoning tasks, from object recognition and captioning to spatial reasoning in 3D scenes, while demonstrating robust scaling properties as model size and training data increase. Our work provides a unified foundation for advancing 3D spatial intelligence in real-world applications.

</details>


### [109] [Revisiting 2D Foundation Models for Scalable 3D Medical Image Classification](https://arxiv.org/abs/2512.12887)
*Han Liu,Bogdan Georgescu,Yanbo Zhang,Youngjin Yoo,Michael Baumgartner,Riqiang Gao,Jianing Wang,Gengyan Zhao,Eli Gibson,Dorin Comaniciu,Sasa Grbic*

Main category: cs.CV

Relevance: 75.0

TL;DR: AnyMC3D：一个从2D基础模型扩展的可扩展3D医学图像分类框架，通过轻量级插件适配新任务，在12个任务基准测试中达到SOTA性能


<details>
  <summary>Details</summary>
Motivation: 当前医学基础模型研究存在三个关键问题：数据体制偏差、次优适配和任务覆盖不足。本文旨在解决这些问题，开发一个可扩展的3D分类器，避免为每个任务单独训练模型。

Method: 从2D基础模型扩展出AnyMC3D，采用冻结主干网络+轻量级插件（约1M参数/任务）的适配策略。支持多视图输入、像素级监督和可解释热图生成。

Result: 在涵盖12个任务的综合基准测试中，AnyMC3D达到SOTA性能（包括VLM3D挑战赛第一名）。关键发现：有效适配对释放基础模型潜力至关重要；通用基础模型经适当适配可匹敌医学专用模型；2D方法在3D分类中超越3D架构。

Conclusion: 首次证明使用单一可扩展框架可在多样化应用中实现SOTA性能，无需单独的任务特定模型。为医学图像分类提供了高效、可扩展的解决方案。

Abstract: 3D medical image classification is essential for modern clinical workflows. Medical foundation models (FMs) have emerged as a promising approach for scaling to new tasks, yet current research suffers from three critical pitfalls: data-regime bias, suboptimal adaptation, and insufficient task coverage. In this paper, we address these pitfalls and introduce AnyMC3D, a scalable 3D classifier adapted from 2D FMs. Our method scales efficiently to new tasks by adding only lightweight plugins (about 1M parameters per task) on top of a single frozen backbone. This versatile framework also supports multi-view inputs, auxiliary pixel-level supervision, and interpretable heatmap generation. We establish a comprehensive benchmark of 12 tasks covering diverse pathologies, anatomies, and modalities, and systematically analyze state-of-the-art 3D classification techniques. Our analysis reveals key insights: (1) effective adaptation is essential to unlock FM potential, (2) general-purpose FMs can match medical-specific FMs if properly adapted, and (3) 2D-based methods surpass 3D architectures for 3D classification. For the first time, we demonstrate the feasibility of achieving state-of-the-art performance across diverse applications using a single scalable framework (including 1st place in the VLM3D challenge), eliminating the need for separate task-specific models.

</details>


### [110] [What Happens Next? Next Scene Prediction with a Unified Video Model](https://arxiv.org/abs/2512.13015)
*Xinjie Li,Zhimin Chen,Rui Zhao,Florian Schiffers,Zhenyu Liao,Vimal Bhat*

Main category: cs.CV

Relevance: 75.0

TL;DR: 论文提出Next Scene Prediction (NSP)任务，推动统一视频模型进行时空和因果推理，通过结合Qwen-VL和LTX的三阶段训练框架，在新构建的大规模NSP数据集上实现SOTA性能。


<details>
  <summary>Details</summary>
Motivation: 当前统一理解与生成模型在视觉生成方面取得进展，但主要关注文本到视频生成等传统任务，对统一模型的时序推理潜力探索不足。需要推动模型进行更深层次的时序和因果推理。

Method: 提出NSP任务，要求从先前上下文预测合理未来场景。构建统一框架：Qwen-VL用于理解，LTX用于合成，通过潜在查询嵌入和连接器模块桥接。采用三阶段训练：文本到视频预训练、监督微调、带因果一致性奖励的强化学习（GRPO）。

Result: 模型在新构建的大规模NSP基准测试中达到最先进性能，展示了统一多模态系统预测未来场景的能力提升。

Conclusion: NSP任务有效推动了统一视频模型向时序和因果推理方向发展，提出的三阶段训练框架在预测未来场景方面表现出色，为通用多模态系统的推理能力提供了新方向。

Abstract: Recent unified models for joint understanding and generation have significantly advanced visual generation capabilities. However, their focus on conventional tasks like text-to-video generation has left the temporal reasoning potential of unified models largely underexplored. To address this gap, we introduce Next Scene Prediction (NSP), a new task that pushes unified video models toward temporal and causal reasoning. Unlike text-to-video generation, NSP requires predicting plausible futures from preceding context, demanding deeper understanding and reasoning. To tackle this task, we propose a unified framework combining Qwen-VL for comprehension and LTX for synthesis, bridged by a latent query embedding and a connector module. This model is trained in three stages on our newly curated, large-scale NSP dataset: text-to-video pre-training, supervised fine-tuning, and reinforcement learning (via GRPO) with our proposed causal consistency reward. Experiments demonstrate our model achieves state-of-the-art performance on our benchmark, advancing the capability of generalist multimodal systems to anticipate what happens next.

</details>


### [111] [MMDrive: Interactive Scene Understanding Beyond Vision with Multi-representational Fusion](https://arxiv.org/abs/2512.13177)
*Minghui Hou,Wei-Hsing Huang,Shaofeng Liang,Daizong Liu,Tai-Hao Wen,Gang Wang,Runwei Guan,Weiping Ding*

Main category: cs.CV

Relevance: 75.0

TL;DR: MMDrive是一个用于自动驾驶的多模态视觉语言模型框架，将传统的2D图像理解扩展到3D场景理解，融合了占据地图、LiDAR点云和文本场景描述三种模态，通过自适应跨模态融合和关键信息提取提升复杂驾驶环境下的推理能力。


<details>
  <summary>Details</summary>
Motivation: 现有视觉语言模型受限于2D平面图像理解范式，难以感知3D空间信息并进行深度语义融合，导致在复杂自动驾驶环境中表现不佳。需要突破传统图像理解限制，实现更强大的多模态推理能力。

Method: 提出MMDrive框架，包含三种互补模态：占据地图、LiDAR点云和文本场景描述。引入两个新组件：1) 面向文本的多模态调制器，根据问题语义动态加权各模态贡献；2) 跨模态抽象器，使用可学习的抽象令牌生成紧凑的跨模态摘要，突出关键区域和语义。

Result: 在DriveLM和NuScenes-QA基准测试中表现优异：DriveLM上BLEU-4得分54.56，METEOR得分41.78；NuScenes-QA上准确率62.7%，显著优于现有自动驾驶视觉语言模型。

Conclusion: MMDrive有效突破了传统仅图像理解的限制，实现了复杂驾驶环境下的鲁棒多模态推理，为可解释的自动驾驶场景理解提供了新基础。

Abstract: Vision-language models enable the understanding and reasoning of complex traffic scenarios through multi-source information fusion, establishing it as a core technology for autonomous driving. However, existing vision-language models are constrained by the image understanding paradigm in 2D plane, which restricts their capability to perceive 3D spatial information and perform deep semantic fusion, resulting in suboptimal performance in complex autonomous driving environments. This study proposes MMDrive, an multimodal vision-language model framework that extends traditional image understanding to a generalized 3D scene understanding framework. MMDrive incorporates three complementary modalities, including occupancy maps, LiDAR point clouds, and textual scene descriptions. To this end, it introduces two novel components for adaptive cross-modal fusion and key information extraction. Specifically, the Text-oriented Multimodal Modulator dynamically weights the contributions of each modality based on the semantic cues in the question, guiding context-aware feature integration. The Cross-Modal Abstractor employs learnable abstract tokens to generate compact, cross-modal summaries that highlight key regions and essential semantics. Comprehensive evaluations on the DriveLM and NuScenes-QA benchmarks demonstrate that MMDrive achieves significant performance gains over existing vision-language models for autonomous driving, with a BLEU-4 score of 54.56 and METEOR of 41.78 on DriveLM, and an accuracy score of 62.7% on NuScenes-QA. MMDrive effectively breaks the traditional image-only understanding barrier, enabling robust multimodal reasoning in complex driving environments and providing a new foundation for interpretable autonomous driving scene understanding.

</details>


### [112] [Toward Ambulatory Vision: Learning Visually-Grounded Active View Selection](https://arxiv.org/abs/2512.13250)
*Juil Koo,Daehyeon Choi,Sangwoo Youn,Phillip Y. Lee,Minhyuk Sung*

Main category: cs.CV

Relevance: 75.0

TL;DR: VG-AVS任务让视觉语言模型从静态图像中选择最有信息量的下一个视角，通过SFT和RL优化，提升主动视觉决策能力，并增强现有EQA系统性能。


<details>
  <summary>Details</summary>
Motivation: 现有视觉语言模型局限于静态图像推理，而具身智能体需要主动移动获取更有信息量的视角。VG-AVS任务旨在让模型仅基于当前视觉信息选择最优下一个视角，不依赖场景记忆或外部知识。

Method: 1) 构建合成数据集，包含自动生成的配对查询-目标视角和问答提示；2) 提出框架：先对预训练VLM进行监督微调(SFT)，再进行基于强化学习的策略优化。

Result: 方法在视角选择基础上实现了强大的问答性能，在未见过的合成和真实场景中具有鲁棒泛化能力。将VG-AVS框架集成到现有基于场景探索的EQA系统中，提高了下游问答准确性。

Conclusion: VG-AVS任务成功将视觉语言模型从静态视觉扩展到主动视觉决策，通过SFT+RL优化实现了有效的视角选择，增强了具身智能体的视觉推理能力。

Abstract: Vision Language Models (VLMs) excel at visual question answering (VQA) but remain limited to snapshot vision, reasoning from static images. In contrast, embodied agents require ambulatory vision, actively moving to obtain more informative views. We introduce Visually Grounded Active View Selection (VG-AVS), a task that selects the most informative next viewpoint using only the visual information in the current image, without relying on scene memory or external knowledge. To support this task, we construct a synthetic dataset with automatically generated paired query-target views and question-answer prompts. We also propose a framework that fine-tunes pretrained VLMs through supervised fine-tuning (SFT) followed by RL-based policy optimization. Our approach achieves strong question answering performance based on viewpoint selection and generalizes robustly to unseen synthetic and real scenes. Furthermore, incorporating our learned VG-AVS framework into existing scene-exploration-based EQA systems improves downstream question-answering accuracy.

</details>


### [113] [LINA: Learning INterventions Adaptively for Physical Alignment and Generalization in Diffusion Models](https://arxiv.org/abs/2512.13290)
*Shu Yu,Chaochao Lu*

Main category: cs.CV

Relevance: 75.0

TL;DR: 本文提出LINA框架，通过因果干预解决扩散模型在物理对齐和分布外指令遵循方面的不足，利用因果场景图和物理对齐探测数据集进行分析，并引入针对性的潜在空间引导和因果感知的去噪调度。


<details>
  <summary>Details</summary>
Motivation: 扩散模型在图像和视频生成方面取得了显著成功，但在物理对齐和分布外指令遵循方面仍存在困难。作者认为这些问题源于模型未能学习因果方向和分离因果因子以进行新颖重组。

Method: 引入因果场景图和物理对齐探测数据集进行诊断干预分析。基于分析结果提出LINA框架，该框架学习预测特定提示的干预措施，包括：1）在提示和视觉潜在空间中进行针对性引导；2）重新分配因果感知的去噪调度。

Result: LINA框架在图像和视频扩散模型中同时加强了物理对齐和分布外指令遵循，在具有挑战性的因果生成任务和Winoground数据集上实现了最先进的性能。

Conclusion: 通过因果分析和针对性干预，可以显著改善扩散模型的物理对齐和分布外指令遵循能力，为更可靠、可控的生成模型提供了新思路。

Abstract: Diffusion models (DMs) have achieved remarkable success in image and video generation. However, they still struggle with (1) physical alignment and (2) out-of-distribution (OOD) instruction following. We argue that these issues stem from the models' failure to learn causal directions and to disentangle causal factors for novel recombination. We introduce the Causal Scene Graph (CSG) and the Physical Alignment Probe (PAP) dataset to enable diagnostic interventions. This analysis yields three key insights. First, DMs struggle with multi-hop reasoning for elements not explicitly determined in the prompt. Second, the prompt embedding contains disentangled representations for texture and physics. Third, visual causal structure is disproportionately established during the initial, computationally limited denoising steps. Based on these findings, we introduce LINA (Learning INterventions Adaptively), a novel framework that learns to predict prompt-specific interventions, which employs (1) targeted guidance in the prompt and visual latent spaces, and (2) a reallocated, causality-aware denoising schedule. Our approach enforces both physical alignment and OOD instruction following in image and video DMs, achieving state-of-the-art performance on challenging causal generation tasks and the Winoground dataset. Our project page is at https://opencausalab.github.io/LINA.

</details>


### [114] [KlingAvatar 2.0 Technical Report](https://arxiv.org/abs/2512.13313)
*Kling Team,Jialu Chen,Yikang Ding,Zhixue Fang,Kun Gai,Yuan Gao,Kang He,Jingyun Hua,Boyuan Jiang,Mingming Lao,Xiaohan Li,Hui Liu,Jiwen Liu,Xiaoqiang Liu,Yuan Liu,Shun Lu,Yongsen Mao,Yingchao Shao,Huafeng Shi,Xiaoyu Shi,Peiqin Sun,Songlin Tang,Pengfei Wan,Chao Wang,Xuebo Wang,Haoxian Zhang,Yuanxing Zhang,Yan Zhou*

Main category: cs.CV

Relevance: 75.0

TL;DR: KlingAvatar 2.0提出时空级联框架，通过低分辨率蓝图关键帧生成和高分辨率子片段细化，解决长视频生成中的效率、时间漂移和质量退化问题，并引入多模态LLM专家进行跨模态指令融合。


<details>
  <summary>Details</summary>
Motivation: 现有头像视频生成模型在生成长时长高分辨率视频时存在效率低、时间漂移、质量退化和提示跟随弱等问题，需要解决长视频生成中的时空一致性和多模态对齐挑战。

Method: 1) 时空级联框架：先生成低分辨率蓝图关键帧捕捉全局语义和运动，再通过首尾帧策略细化为高分辨率时序连贯的子片段；2) 协同推理导演：包含三个模态特定LLM专家，通过多轮对话推理模态优先级和用户意图；3) 负向导演优化负提示；4) 扩展支持ID特定的多角色控制。

Result: 模型有效解决了高效、多模态对齐的长格式高分辨率视频生成挑战，提供增强的视觉清晰度、逼真的唇齿渲染与准确的口型同步、强身份保持和连贯的多模态指令跟随。

Conclusion: KlingAvatar 2.0通过时空级联框架和多模态LLM专家系统，成功解决了长视频生成中的关键问题，为高效、高质量的多模态视频生成提供了有效解决方案。

Abstract: Avatar video generation models have achieved remarkable progress in recent years. However, prior work exhibits limited efficiency in generating long-duration high-resolution videos, suffering from temporal drifting, quality degradation, and weak prompt following as video length increases. To address these challenges, we propose KlingAvatar 2.0, a spatio-temporal cascade framework that performs upscaling in both spatial resolution and temporal dimension. The framework first generates low-resolution blueprint video keyframes that capture global semantics and motion, and then refines them into high-resolution, temporally coherent sub-clips using a first-last frame strategy, while retaining smooth temporal transitions in long-form videos. To enhance cross-modal instruction fusion and alignment in extended videos, we introduce a Co-Reasoning Director composed of three modality-specific large language model (LLM) experts. These experts reason about modality priorities and infer underlying user intent, converting inputs into detailed storylines through multi-turn dialogue. A Negative Director further refines negative prompts to improve instruction alignment. Building on these components, we extend the framework to support ID-specific multi-character control. Extensive experiments demonstrate that our model effectively addresses the challenges of efficient, multimodally aligned long-form high-resolution video generation, delivering enhanced visual clarity, realistic lip-teeth rendering with accurate lip synchronization, strong identity preservation, and coherent multimodal instruction following.

</details>


### [115] [RecTok: Reconstruction Distillation along Rectified Flow](https://arxiv.org/abs/2512.13421)
*Qingyu Shi,Size Wu,Jinbin Bai,Kaidong Yu,Yujing Wang,Yunhai Tong,Xiangtai Li,Xuelong Li*

Main category: cs.CV

Relevance: 75.0

TL;DR: RecTok提出了一种新的视觉tokenizer方法，通过流语义蒸馏和重建对齐蒸馏，解决了高维视觉tokenizer在生成质量上的瓶颈，实现了更好的图像重建和生成质量。


<details>
  <summary>Details</summary>
Motivation: 现有视觉tokenizer在潜在空间维度与生成质量之间存在根本性权衡，高维tokenizer通常表现不如低维版本。虽然近期工作利用视觉基础模型增强语义，但高维tokenizer仍然表现不佳。

Method: RecTok采用两个关键创新：1) 流语义蒸馏 - 将视觉基础模型的语义信息蒸馏到流匹配的前向流轨迹中；2) 重建对齐蒸馏 - 引入掩码特征重建损失进一步增强语义。核心洞见是将注意力放在流匹配的前向流上，而不是像以往工作那样关注潜在空间。

Result: RecTok在图像重建、生成质量和判别性能方面表现优异。在gFID-50K基准测试中，无论是否使用分类器自由引导，都取得了最先进的结果。随着潜在维度增加，性能持续提升。

Conclusion: RecTok通过创新的蒸馏方法克服了高维视觉tokenizer的局限性，实现了语义丰富的潜在空间结构，为扩散模型中的视觉tokenizer设计提供了新思路。

Abstract: Visual tokenizers play a crucial role in diffusion models. The dimensionality of latent space governs both reconstruction fidelity and the semantic expressiveness of the latent feature. However, a fundamental trade-off is inherent between dimensionality and generation quality, constraining existing methods to low-dimensional latent spaces. Although recent works have leveraged vision foundation models to enrich the semantics of visual tokenizers and accelerate convergence, high-dimensional tokenizers still underperform their low-dimensional counterparts. In this work, we propose RecTok, which overcomes the limitations of high-dimensional visual tokenizers through two key innovations: flow semantic distillation and reconstruction--alignment distillation. Our key insight is to make the forward flow in flow matching semantically rich, which serves as the training space of diffusion transformers, rather than focusing on the latent space as in previous works. Specifically, our method distills the semantic information in VFMs into the forward flow trajectories in flow matching. And we further enhance the semantics by introducing a masked feature reconstruction loss. Our RecTok achieves superior image reconstruction, generation quality, and discriminative performance. It achieves state-of-the-art results on the gFID-50K under both with and without classifier-free guidance settings, while maintaining a semantically rich latent space structure. Furthermore, as the latent dimensionality increases, we observe consistent improvements. Code and model are available at https://shi-qingyu.github.io/rectok.github.io.

</details>


### [116] [MineTheGap: Automatic Mining of Biases in Text-to-Image Models](https://arxiv.org/abs/2512.13427)
*Noa Cohen,Nurit Spingarn-Eliezer,Inbar Huberman-Spiegelglas,Tomer Michaeli*

Main category: cs.CV

Relevance: 75.0

TL;DR: 提出MineTheGap方法，通过遗传算法自动挖掘导致文本到图像模型生成偏见输出的提示词，使用新颖的偏见评分来量化偏见严重程度。


<details>
  <summary>Details</summary>
Motivation: 文本到图像模型在处理模糊文本提示时会产生偏见，这些偏见可能带来社会影响（如职业与种族的刻板印象）并影响用户体验。现有方法主要检测给定提示的偏见，但缺乏自动发现导致偏见的提示的能力。

Method: 提出MineTheGap方法：1）使用遗传算法迭代优化提示池，寻找能暴露偏见的提示；2）设计新颖的偏见评分，通过比较生成图像分布与LLM生成的文本变体分布来量化偏见严重程度；3）在已知偏见数据集上验证评分有效性。

Result: 方法能够自动挖掘导致TTI模型偏见的提示，偏见评分能有效量化偏见的严重程度。代码和示例已在项目网页公开。

Conclusion: MineTheGap提供了一种系统化方法来自动发现文本到图像模型的偏见，超越了简单的偏见检测，有助于理解和减轻模型偏见。

Abstract: Text-to-Image (TTI) models generate images based on text prompts, which often leave certain aspects of the desired image ambiguous. When faced with these ambiguities, TTI models have been shown to exhibit biases in their interpretations. These biases can have societal impacts, e.g., when showing only a certain race for a stated occupation. They can also affect user experience when creating redundancy within a set of generated images instead of spanning diverse possibilities. Here, we introduce MineTheGap - a method for automatically mining prompts that cause a TTI model to generate biased outputs. Our method goes beyond merely detecting bias for a given prompt. Rather, it leverages a genetic algorithm to iteratively refine a pool of prompts, seeking for those that expose biases. This optimization process is driven by a novel bias score, which ranks biases according to their severity, as we validate on a dataset with known biases. For a given prompt, this score is obtained by comparing the distribution of generated images to the distribution of LLM-generated texts that constitute variations on the prompt. Code and examples are available on the project's webpage.

</details>


### [117] [MMhops-R1: Multimodal Multi-hop Reasoning](https://arxiv.org/abs/2512.13573)
*Tao Zhang,Ziqi Zhang,Zongyang Ma,Yuxin Chen,Bing Li,Chunfeng Yuan,Guangting Wang,Fengyun Rao,Ying Shan,Weiming Hu*

Main category: cs.CV

Relevance: 75.0

TL;DR: MMhops是一个新的多模态多跳推理基准，包含Bridging和Comparison两种任务格式，需要模型通过整合外部知识构建复杂推理链。作者提出了MMhops-R1框架，使用强化学习优化动态推理路径规划，在基准上显著优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有MLLM主要局限于单步推理，缺乏评估和驱动多跳推理能力的复杂基准。多模态多跳推理对于解决复杂现实问题至关重要，但现有基准无法充分评估这种能力。

Method: 提出MMhops-R1框架，这是一个多模态检索增强生成(mRAG)框架，使用强化学习优化模型自主规划推理路径、制定针对性查询和合成多层次信息的能力。

Result: MMhops-R1在MMhops基准上显著优于强基线方法，表明动态规划和多模态知识整合对复杂推理至关重要。该框架在需要固定跳数推理的任务上也表现出良好的泛化能力。

Conclusion: 这项工作贡献了一个具有挑战性的新基准和一个强大的基线模型，将发布相关代码、数据和权重以推动该关键领域的研究。

Abstract: The ability to perform multi-modal multi-hop reasoning by iteratively integrating information across various modalities and external knowledge is critical for addressing complex real-world challenges. However, existing Multi-modal Large Language Models (MLLMs) are predominantly limited to single-step reasoning, as existing benchmarks lack the complexity needed to evaluate and drive multi-hop abilities. To bridge this gap, we introduce MMhops, a novel, large-scale benchmark designed to systematically evaluate and foster multi-modal multi-hop reasoning. MMhops dataset comprises two challenging task formats, Bridging and Comparison, which necessitate that models dynamically construct complex reasoning chains by integrating external knowledge. To tackle the challenges posed by MMhops, we propose MMhops-R1, a novel multi-modal Retrieval-Augmented Generation (mRAG) framework for dynamic reasoning. Our framework utilizes reinforcement learning to optimize the model for autonomously planning reasoning paths, formulating targeted queries, and synthesizing multi-level information. Comprehensive experiments demonstrate that MMhops-R1 significantly outperforms strong baselines on MMhops, highlighting that dynamic planning and multi-modal knowledge integration are crucial for complex reasoning. Moreover, MMhops-R1 demonstrates strong generalization to tasks requiring fixed-hop reasoning, underscoring the robustness of our dynamic planning approach. In conclusion, our work contributes a challenging new benchmark and a powerful baseline model, and we will release the associated code, data, and weights to catalyze future research in this critical area.

</details>


### [118] [AgentIAD: Tool-Augmented Single-Agent for Industrial Anomaly Detection](https://arxiv.org/abs/2512.13671)
*Junwen Miao,Penghui Du,Yi Liu,Yu Wang,Yan Wang*

Main category: cs.CV

Relevance: 75.0

TL;DR: AgentIAD：基于工具驱动的智能体框架，通过感知缩放器和比较检索器实现多阶段视觉检测，在工业异常检测中达到97.62%的SOTA准确率


<details>
  <summary>Details</summary>
Motivation: 工业异常检测面临正常样本稀缺、缺陷细微且局部化的挑战。现有的单次视觉语言模型往往忽略小异常，缺乏与标准正常模式比较的显式机制。

Method: 提出AgentIAD框架，包含感知缩放器（PZ）进行局部细粒度分析，比较检索器（CR）在证据模糊时查询正常样本。使用MMAD数据集构建结构化轨迹，采用两阶段训练：监督微调+强化学习，设计了感知奖励和行为奖励的双重奖励机制。

Result: 在MMAD数据集上达到97.62%的分类准确率，超越之前基于MLLM的方法，同时产生透明可解释的检测轨迹。

Conclusion: AgentIAD通过工具驱动的智能体框架和多阶段检查策略，有效解决了工业异常检测中的挑战，实现了SOTA性能并提供了可解释的检测过程。

Abstract: Industrial anomaly detection (IAD) is difficult due to the scarcity of normal reference samples and the subtle, localized nature of many defects. Single-pass vision-language models (VLMs) often overlook small abnormalities and lack explicit mechanisms to compare against canonical normal patterns. We propose AgentIAD, a tool-driven agentic framework that enables multi-stage visual inspection. The agent is equipped with a Perceptive Zoomer (PZ) for localized fine-grained analysis and a Comparative Retriever (CR) for querying normal exemplars when evidence is ambiguous. To teach these inspection behaviors, we construct structured perceptive and comparative trajectories from the MMAD dataset and train the model in two stages: supervised fine-tuning followed by reinforcement learning. A two-part reward design drives this process: a perception reward that supervises classification accuracy, spatial alignment, and type correctness, and a behavior reward that encourages efficient tool use. Together, these components enable the model to refine its judgment through step-wise observation, zooming, and verification. AgentIAD achieves a new state-of-the-art 97.62% classification accuracy on MMAD, surpassing prior MLLM-based approaches while producing transparent and interpretable inspection traces.

</details>


### [119] [Recurrent Video Masked Autoencoders](https://arxiv.org/abs/2512.13684)
*Daniel Zoran,Nikhil Parthasarathy,Yi Yang,Drew A Hudson,Joao Carreira,Andrew Zisserman*

Main category: cs.CV

Relevance: 75.0

TL;DR: RVM是一种新颖的视频表示学习方法，使用基于Transformer的循环神经网络聚合时间上的密集图像特征，通过掩码预测任务学习，在视频任务和图像任务上都表现优异，参数效率高且计算成本线性增长。


<details>
  <summary>Details</summary>
Motivation: 现有视频表示学习方法通常使用标准的时空注意力架构，这些方法在长时序建模时计算成本高，且参数效率较低。需要一种既能有效捕捉视频时空结构，又具有高参数效率和线性计算成本的方法。

Method: 提出循环视频掩码自编码器(RVM)，使用基于Transformer的循环神经网络聚合时间上的密集图像特征。通过非对称掩码预测任务学习，仅需标准像素重建目标。采用循环设计实现稳定特征传播。

Result: 在视频级任务（动作识别、点/目标跟踪）上与VideoMAE、V-JEPA等SOTA视频模型竞争，在几何和密集空间理解任务上优于DINOv2等图像模型。在小模型领域表现优异，参数效率比竞争视频掩码自编码器高30倍，计算成本线性增长。

Conclusion: RVM通过循环设计有效学习视频的时空表示，在保持高性能的同时实现了高参数效率和线性计算成本，克服了标准时空注意力架构的局限性，学习了丰富的场景语义、结构和运动表示。

Abstract: We present Recurrent Video Masked-Autoencoders (RVM): a novel video representation learning approach that uses a transformer-based recurrent neural network to aggregate dense image features over time, effectively capturing the spatio-temporal structure of natural video data. RVM learns via an asymmetric masked prediction task requiring only a standard pixel reconstruction objective. This design yields a highly efficient ``generalist'' encoder: RVM achieves competitive performance with state-of-the-art video models (e.g. VideoMAE, V-JEPA) on video-level tasks like action recognition and point/object tracking, while also performing favorably against image models (e.g. DINOv2) on tasks that test geometric and dense spatial understanding. Notably, RVM achieves strong performance in the small-model regime without requiring knowledge distillation, exhibiting up to 30x greater parameter efficiency than competing video masked autoencoders. Moreover, we demonstrate that RVM's recurrent nature allows for stable feature propagation over long temporal horizons with linear computational cost, overcoming some of the limitations of standard spatio-temporal attention-based architectures. Finally, we use qualitative visualizations to highlight that RVM learns rich representations of scene semantics, structure, and motion.

</details>


### [120] [Towards Scalable Pre-training of Visual Tokenizers for Generation](https://arxiv.org/abs/2512.13687)
*Jingfeng Yao,Yuda Song,Yucong Zhou,Xinggang Wang*

Main category: cs.CV

Relevance: 75.0

TL;DR: VTP提出统一的视觉分词器预训练框架，通过联合优化图像-文本对比、自监督和重建损失，解决传统视觉分词器预训练中存在的"预训练缩放问题"，使生成性能随计算资源有效提升。


<details>
  <summary>Details</summary>
Motivation: 传统基于重建的视觉分词器（如VAE）训练产生的潜在空间偏向低层信息，导致更好的像素级精度并不能带来更高质量的生成效果，形成"预训练缩放问题"。需要使潜在空间能简洁表示高层语义，以提升生成性能。

Method: 提出VTP统一视觉分词器预训练框架，首次联合优化三种损失：图像-文本对比损失、自监督损失和重建损失。通过大规模预训练研究，探索理解与生成的关系。

Result: VTP在ImageNet上达到78.2的零样本准确率和0.36的rFID，生成收敛速度比先进蒸馏方法快4.1倍。更重要的是，它展现出良好的缩放特性：仅增加VTP预训练的计算量（不修改标准DiT训练规格），下游生成FID提升65.8%，而传统自编码器在1/10计算量时就停滞不前。

Conclusion: 理解是生成的关键驱动力，VTP通过联合优化多任务损失解决了视觉分词器预训练的缩放问题，使生成性能能随计算资源、参数和数据有效提升，为生成模型提供了更好的视觉分词器基础。

Abstract: The quality of the latent space in visual tokenizers (e.g., VAEs) is crucial for modern generative models. However, the standard reconstruction-based training paradigm produces a latent space that is biased towards low-level information, leading to a foundation flaw: better pixel-level accuracy does not lead to higher-quality generation. This implies that pouring extensive compute into visual tokenizer pre-training translates poorly to improved performance in generation. We identify this as the ``pre-training scaling problem`` and suggest a necessary shift: to be effective for generation, a latent space must concisely represent high-level semantics. We present VTP, a unified visual tokenizer pre-training framework, pioneering the joint optimization of image-text contrastive, self-supervised, and reconstruction losses. Our large-scale study reveals two principal findings: (1) understanding is a key driver of generation, and (2) much better scaling properties, where generative performance scales effectively with compute, parameters, and data allocated to the pretraining of the visual tokenizer. After large-scale pre-training, our tokenizer delivers a competitive profile (78.2 zero-shot accuracy and 0.36 rFID on ImageNet) and 4.1 times faster convergence on generation compared to advanced distillation methods. More importantly, it scales effectively: without modifying standard DiT training specs, solely investing more FLOPS in pretraining VTP achieves 65.8\% FID improvement in downstream generation, while conventional autoencoder stagnates very early at 1/10 FLOPS. Our pre-trained models are available at https://github.com/MiniMax-AI/VTP.

</details>


### [121] [DiffusionBrowser: Interactive Diffusion Previews via Multi-Branch Decoders](https://arxiv.org/abs/2512.13690)
*Susung Hong,Chongjian Ge,Zhifei Zhang,Jui-Hsien Wang*

Main category: cs.CV

Relevance: 75.0

TL;DR: 提出DiffusionBrowser框架，可在视频扩散模型去噪过程中实时生成多模态预览（RGB和场景内在属性），速度超过4倍实时，支持交互式引导生成和模型探测分析


<details>
  <summary>Details</summary>
Motivation: 视频扩散模型存在生成不精确、速度慢、生成过程不透明的问题，用户需要长时间等待且无法了解生成过程。需要一种能够实时预览生成过程并支持交互控制的解决方案。

Method: 提出DiffusionBrowser框架，这是一个模型无关的轻量级解码器，可在去噪过程的任意时间步或Transformer块处生成多模态预览（RGB和场景内在属性）。通过训练的解码器，支持通过随机性重注入和模态引导进行交互式控制。

Result: 能够以超过4倍实时速度生成预览（4秒视频少于1秒），预览与最终视频具有一致的外观和运动。实现了交互式引导生成，并能够系统探测模型，揭示场景、物体等细节在去噪过程中的组合方式。

Conclusion: DiffusionBrowser为视频扩散模型提供了实时预览和交互控制能力，打破了生成过程的黑盒特性，增强了用户对生成过程的控制和理解。

Abstract: Video diffusion models have revolutionized generative video synthesis, but they are imprecise, slow, and can be opaque during generation -- keeping users in the dark for a prolonged period. In this work, we propose DiffusionBrowser, a model-agnostic, lightweight decoder framework that allows users to interactively generate previews at any point (timestep or transformer block) during the denoising process. Our model can generate multi-modal preview representations that include RGB and scene intrinsics at more than 4$\times$ real-time speed (less than 1 second for a 4-second video) that convey consistent appearance and motion to the final video. With the trained decoder, we show that it is possible to interactively guide the generation at intermediate noise steps via stochasticity reinjection and modal steering, unlocking a new control capability. Moreover, we systematically probe the model using the learned decoders, revealing how scene, object, and other details are composed and assembled during the otherwise black-box denoising process.

</details>


### [122] [FloraForge: LLM-Assisted Procedural Generation of Editable and Analysis-Ready 3D Plant Geometric Models For Agricultural Applications](https://arxiv.org/abs/2512.11925)
*Mozhgan Hadadi,Talukder Z. Jubery,Patrick S. Schnable,Arti Singh,Bedrich Benes,Adarsh Krishnamurthy,Baskar Ganapathysubramanian*

Main category: cs.CV

Relevance: 65.0

TL;DR: FloraForge是一个LLM辅助框架，让领域专家通过自然语言交互生成参数化3D植物模型，结合了基于学习的重建方法的准确性和程序化建模的可编辑性。


<details>
  <summary>Details</summary>
Motivation: 当前3D植物建模方法存在局限性：基于学习的方法需要大量物种特定训练数据且缺乏可编辑性；程序化建模需要专业的几何建模知识和复杂的程序规则，对领域科学家不友好。需要一种既能保持生物准确性又易于使用的解决方案。

Method: 开发了LLM辅助框架，通过迭代自然语言交互（Plant Refinements）生成参数化植物几何体，使用分层B样条曲面表示，结合植物学约束，具有显式控制点和参数变形函数。框架包括LLM辅助模板创建、植物描述符（PD）手动优化，以及从点云数据拟合程序化模型。

Result: 在玉米、大豆和绿豆上成功演示，生成双输出：用于可视化的三角网格和用于定量分析的带参数元数据的三角网格。框架实现了数学连续表示，支持表型分析和渲染，同时保持参数控制。

Conclusion: FloraForge通过LLM辅助的协同设计，使领域科学家能够生成生物准确的参数化3D植物模型，降低了复杂几何建模的门槛，同时保持了数学严谨性，促进了植物科学中的计算表型分析。

Abstract: Accurate 3D plant models are crucial for computational phenotyping and physics-based simulation; however, current approaches face significant limitations. Learning-based reconstruction methods require extensive species-specific training data and lack editability. Procedural modeling offers parametric control but demands specialized expertise in geometric modeling and an in-depth understanding of complex procedural rules, making it inaccessible to domain scientists. We present FloraForge, an LLM-assisted framework that enables domain experts to generate biologically accurate, fully parametric 3D plant models through iterative natural language Plant Refinements (PR), minimizing programming expertise. Our framework leverages LLM-enabled co-design to refine Python scripts that generate parameterized plant geometries as hierarchical B-spline surface representations with botanical constraints with explicit control points and parametric deformation functions. This representation can be easily tessellated into polygonal meshes with arbitrary precision, ensuring compatibility with functional structural plant analysis workflows such as light simulation, computational fluid dynamics, and finite element analysis. We demonstrate the framework on maize, soybean, and mung bean, fitting procedural models to empirical point cloud data through manual refinement of the Plant Descriptor (PD), human-readable files. The pipeline generates dual outputs: triangular meshes for visualization and triangular meshes with additional parametric metadata for quantitative analysis. This approach uniquely combines LLM-assisted template creation, mathematically continuous representations enabling both phenotyping and rendering, and direct parametric control through PD. The framework democratizes sophisticated geometric modeling for plant science while maintaining mathematical rigor.

</details>


### [123] [DynaPURLS: Dynamic Refinement of Part-aware Representations for Skeleton-based Zero-Shot Action Recognition](https://arxiv.org/abs/2512.11941)
*Jingmin Zhu,Anqi Zhu,James Bailey,Jun Liu,Hossein Rahmani,Mohammed Bennamoun,Farid Boussaid,Qiuhong Ke*

Main category: cs.CV

Relevance: 65.0

TL;DR: DynaPURLS是一个用于零样本骨架动作识别的统一框架，通过多尺度视觉-语义对应和动态细化来增强泛化能力，在多个基准数据集上达到SOTA。


<details>
  <summary>Details</summary>
Motivation: 现有零样本骨架动作识别方法主要依赖骨架特征与静态类别级语义的对齐，这种粗粒度对齐无法弥合可见类和未见类之间的领域偏移，阻碍了细粒度视觉知识的有效迁移。

Method: 1) 使用大语言模型生成包含全局运动和局部身体部位动态的层次化文本描述；2) 自适应分区模块通过语义分组骨架关节生成细粒度视觉表示；3) 动态细化模块在推理时通过轻量级可学习投影将文本特征适配到输入视觉流；4) 使用置信感知、类别平衡的记忆库稳定细化过程，减轻噪声伪标签的错误传播。

Result: 在NTU RGB+D 60/120和PKU-MMD三个大规模基准数据集上的广泛实验表明，DynaPURLS显著优于现有方法，创造了新的最先进记录。

Conclusion: DynaPURLS通过建立鲁棒的多尺度视觉-语义对应并在推理时动态细化，有效解决了零样本骨架动作识别中的领域偏移问题，实现了更好的泛化性能。

Abstract: Zero-shot skeleton-based action recognition (ZS-SAR) is fundamentally constrained by prevailing approaches that rely on aligning skeleton features with static, class-level semantics. This coarse-grained alignment fails to bridge the domain shift between seen and unseen classes, thereby impeding the effective transfer of fine-grained visual knowledge. To address these limitations, we introduce \textbf{DynaPURLS}, a unified framework that establishes robust, multi-scale visual-semantic correspondences and dynamically refines them at inference time to enhance generalization. Our framework leverages a large language model to generate hierarchical textual descriptions that encompass both global movements and local body-part dynamics. Concurrently, an adaptive partitioning module produces fine-grained visual representations by semantically grouping skeleton joints. To fortify this fine-grained alignment against the train-test domain shift, DynaPURLS incorporates a dynamic refinement module. During inference, this module adapts textual features to the incoming visual stream via a lightweight learnable projection. This refinement process is stabilized by a confidence-aware, class-balanced memory bank, which mitigates error propagation from noisy pseudo-labels. Extensive experiments on three large-scale benchmark datasets, including NTU RGB+D 60/120 and PKU-MMD, demonstrate that DynaPURLS significantly outperforms prior art, setting new state-of-the-art records. The source code is made publicly available at https://github.com/Alchemist0754/DynaPURLS

</details>


### [124] [Open Horizons: Evaluating Deep Models in the Wild](https://arxiv.org/abs/2512.12146)
*Ayush Vaibhav Bhatti,Deniz Karakay,Debottama Das,Nilotpal Rajbongshi,Yuito Sugimoto*

Main category: cs.CV

Relevance: 65.0

TL;DR: 该论文对开放世界识别进行了统一实验研究，比较了不同视觉编码器在开放集识别(OSR)和少样本类增量学习(FSCIL)上的表现，发现CLIP在未知样本检测上表现最佳，而ConCM在增量学习中效果最好。


<details>
  <summary>Details</summary>
Motivation: 开放世界部署需要模型既能识别已知类别，又能在出现新类别时保持可靠。现有研究通常孤立地处理开放集识别(OSR)和少样本类增量学习(FSCIL)，缺乏统一的实验框架来系统比较不同架构和方法在这些任务上的表现。

Method: 在CIFAR-10数据集上进行统一实验：1) OSR部分：比较ResNet-50、ConvNeXt-Tiny和CLIP ViT-B/16三种预训练冻结视觉编码器，使用线性探针和四种后验评分函数(MSP、Energy、Mahalanobis、kNN)；2) FSCIL部分：比较改进的SPPR、OrCo和ConCM方法，使用部分冻结的ResNet-50，在1-shot、5-shot和10-shot场景下评估。

Result: 1) OSR中，CLIP在所有指标(AUROC、AUPR、FPR@95、OSCR)上对已知和未知样本的区分能力最强，Energy评分函数在不同骨干网络上表现最稳定；2) FSCIL中，ConCM在10-shot设置下达到84.7%准确率，混淆矩阵最清晰，所有方法在超过5-shot后性能饱和；3) 骨干架构和评分机制显著影响未知检测，基于原型的方法能有效缓解增量适应中的灾难性遗忘。

Conclusion: CLIP视觉编码器在开放集识别中表现卓越，而基于原型的增量学习方法能有效平衡新类别学习和旧知识保留。研究为开放世界部署提供了架构选择和算法设计的实用指导。

Abstract: Open-world deployment requires models to recognize both known categories and remain reliable when novel classes appear. We present a unified experimental study spanning open-set recognition (OSR) and few-shot class-incremental learning (FSCIL) on CIFAR-10. For OSR, we compare three pretrained frozen visual encoders: ResNet-50, ConvNeXt-Tiny and CLIP ViT-B/16,using a linear probe and four post-hoc scoring functions, namely MSP, Energy, Mahalanobis and kNN. Across metrics,such as, AUROC, AUPR, FPR@95, and OSCR, CLIP consistently yields the strongest separability between known and unknown samples, with Energy providing the most stable performance across backbones. For FSCIL, we compare modified SPPR, OrCo, and ConCM using partially frozen ResNet-50 across 1-, 5-, and 10-shot scenarios. ConCM achieves 84.7% accuracy in the 10-shot setting with the cleanest confusion matrix, while all methods show saturation beyond 5 shots. Our controlled evaluation reveals how the backbone architecture and scoring mechanisms affect unknown detection and how prototype-based methods mitigate catastrophic forgetting during incremental adaptation.

</details>


### [125] [Fine-Grained Zero-Shot Learning with Attribute-Centric Representations](https://arxiv.org/abs/2512.12219)
*Zhi Chen,Jingcai Guo,Taotao Cai,Yuxiang Cai*

Main category: cs.CV

Relevance: 65.0

TL;DR: 提出ACR框架解决细粒度零样本学习中的属性纠缠问题，通过混合专家机制实现属性解耦表示学习，在多个基准数据集上达到SOTA


<details>
  <summary>Details</summary>
Motivation: 细粒度类别识别需要区分细微视觉差异，传统方法将颜色、形状、纹理等不同属性压缩到单一视觉嵌入中，导致属性纠缠和干扰，后处理方法效果有限

Method: 提出属性中心表示(ACR)框架，包含两个混合专家组件：1) MoPE通过双级路由机制将图像块分派给专门专家处理；2) MoAE将专家精炼特征投影到稀疏、部分感知的属性映射中

Result: 在CUB、AwA2和SUN等零样本学习基准数据集上取得一致的state-of-the-art结果

Conclusion: 通过在表示学习中强制属性解耦，ACR框架有效解决了属性纠缠问题，提升了细粒度零样本分类性能

Abstract: Recognizing unseen fine-grained categories demands a model that can distinguish subtle visual differences. This is typically achieved by transferring visual-attribute relationships from seen classes to unseen classes. The core challenge is attribute entanglement, where conventional models collapse distinct attributes like color, shape, and texture into a single visual embedding. This causes interference that masks these critical distinctions. The post-hoc solutions of previous work are insufficient, as they operate on representations that are already mixed. We propose a zero-shot learning framework that learns AttributeCentric Representations (ACR) to tackle this problem by imposing attribute disentanglement during representation learning. ACR is achieved with two mixture-of-experts components, including Mixture of Patch Experts (MoPE) and Mixture of Attribute Experts (MoAE). First, MoPE is inserted into the transformer using a dual-level routing mechanism to conditionally dispatch image patches to specialized experts. This ensures coherent attribute families are processed by dedicated experts. Finally, the MoAE head projects these expert-refined features into sparse, partaware attribute maps for robust zero-shot classification. On zero-shot learning benchmark datasets CUB, AwA2, and SUN, our ACR achieves consistent state-of-the-art results.

</details>


### [126] [ProImage-Bench: Rubric-Based Evaluation for Professional Image Generation](https://arxiv.org/abs/2512.12220)
*Minheng Ni,Zhengyuan Yang,Yaowen Zhang,Linjie Li,Chung-Ching Lin,Kevin Lin,Zhendong Wang,Xiaofei Wang,Shujie Liu,Lei Zhang,Wangmeng Zuo,Lijuan Wang*

Main category: cs.CV

Relevance: 65.0

TL;DR: 提出ProImage-Bench基准，用于评估专业图像生成模型在科学插图生成中的精确性，通过层次化评分标准和自动评估揭示现有模型的不足，并展示如何利用失败检查进行迭代优化。


<details>
  <summary>Details</summary>
Motivation: 专业图像生成需要从技术描述中合成信息密集、科学精确的插图，而不仅仅是视觉上合理的图片。现有模型在科学插图的细粒度保真度方面存在显著差距，需要专门的评估基准来量化进展并提供改进信号。

Method: 1) 构建ProImage-Bench基准，包含654个来自真实教科书和技术报告的图表；2) 创建详细图像指令和层次化评分标准，将正确性分解为6,076个标准和44,131个二进制检查；3) 使用大型多模态模型从周围文本和参考图中推导评分标准；4) 采用基于LMM的自动评估器，通过原则性惩罚方案将子问题结果聚合成可解释的标准分数。

Result: 1) 在ProImage-Bench上评估多个代表性文本到图像模型，发现即使在开放域表现强劲，最佳基础模型仅达到0.791的评分准确率和0.553的标准分数；2) 通过将失败检查反馈给编辑模型进行迭代优化，可将强生成器的评分准确率从0.653提升到0.865，标准分数从0.388提升到0.697。

Conclusion: ProImage-Bench为专业图像生成提供了严格的诊断工具，并为改进规范忠实科学插图提供了可扩展的信号。该基准揭示了现有模型在科学保真度方面的显著差距，并展示了如何利用评估反馈进行模型优化。

Abstract: We study professional image generation, where a model must synthesize information-dense, scientifically precise illustrations from technical descriptions rather than merely produce visually plausible pictures. To quantify the progress, we introduce ProImage-Bench, a rubric-based benchmark that targets biology schematics, engineering/patent drawings, and general scientific diagrams. For 654 figures collected from real textbooks and technical reports, we construct detailed image instructions and a hierarchy of rubrics that decompose correctness into 6,076 criteria and 44,131 binary checks. Rubrics are derived from surrounding text and reference figures using large multimodal models, and are evaluated by an automated LMM-based judge with a principled penalty scheme that aggregates sub-question outcomes into interpretable criterion scores. We benchmark several representative text-to-image models on ProImage-Bench and find that, despite strong open-domain performance, the best base model reaches only 0.791 rubric accuracy and 0.553 criterion score overall, revealing substantial gaps in fine-grained scientific fidelity. Finally, we show that the same rubrics provide actionable supervision: feeding failed checks back into an editing model for iterative refinement boosts a strong generator from 0.653 to 0.865 in rubric accuracy and from 0.388 to 0.697 in criterion score. ProImage-Bench thus offers both a rigorous diagnostic for professional image generation and a scalable signal for improving specification-faithful scientific illustrations.

</details>


### [127] [Unified Control for Inference-Time Guidance of Denoising Diffusion Models](https://arxiv.org/abs/2512.12339)
*Maurya Goyal,Anuj Singh,Hadi Jamali-Rad*

Main category: cs.CV

Relevance: 65.0

TL;DR: UniCoDe提出了一种统一的算法，将采样方法和梯度引导方法结合，用于对齐扩散模型的输出与下游目标，提高任务特定性能。


<details>
  <summary>Details</summary>
Motivation: 当前扩散模型与下游目标对齐的方法主要分为采样方法和梯度引导方法，两者各有优缺点。采样方法效率低，梯度引导方法需要可微奖励近似。需要一种统一框架结合两者的优势。

Method: 提出UniCoDe算法，将采样和梯度引导统一到一个框架中。在采样过程中整合局部梯度信号，解决复杂奖励采样方法的效率问题，平衡奖励对齐和扩散无条件先验的偏离。

Result: 实验结果表明，UniCoDe在多个任务上与最先进的基线方法保持竞争力，实现了更高效的采样，并在奖励对齐和先验偏离之间提供了更好的权衡。

Conclusion: UniCoDe成功地将采样和梯度引导两种范式统一起来，为扩散模型的对齐问题提供了一个更高效的解决方案。

Abstract: Aligning diffusion model outputs with downstream objectives is essential for improving task-specific performance. Broadly, inference-time training-free approaches for aligning diffusion models can be categorized into two main strategies: sampling-based methods, which explore multiple candidate outputs and select those with higher reward signals, and gradient-guided methods, which use differentiable reward approximations to directly steer the generation process. In this work, we propose a universal algorithm, UniCoDe, which brings together the strengths of sampling and gradient-based guidance into a unified framework. UniCoDe integrates local gradient signals during sampling, thereby addressing the sampling inefficiency inherent in complex reward-based sampling approaches. By cohesively combining these two paradigms, UniCoDe enables more efficient sampling while offering better trade-offs between reward alignment and divergence from the diffusion unconditional prior. Empirical results demonstrate that UniCoDe remains competitive with state-of-the-art baselines across a range of tasks. The code is available at https://github.com/maurya-goyal10/UniCoDe

</details>


### [128] [ViInfographicVQA: A Benchmark for Single and Multi-image Visual Question Answering on Vietnamese Infographics](https://arxiv.org/abs/2512.12424)
*Tue-Thu Van-Dinh,Hoang-Duy Tran,Truong-Binh Duong,Mai-Hanh Pham,Binh-Nam Le-Nguyen,Quoc-Thai Nguyen*

Main category: cs.CV

Relevance: 65.0

TL;DR: ViInfographicVQA是首个越南语信息图视觉问答基准，包含6747个真实信息图和20409个人工验证的问答对，评估模型在数据丰富、布局复杂的视觉内容上的阅读理解能力。


<details>
  <summary>Details</summary>
Motivation: 现有VQA基准主要针对场景文本或自然图像，缺乏对信息图这种结合文本、图表、图标和设计元素的复杂视觉内容的评估。越南语等低资源语言尤其缺乏此类基准，需要更强的OCR、布局理解和数值语义推理能力。

Method: 构建了包含6747个真实世界信息图和20409个问答对的越南语基准，涵盖经济、医疗、教育等领域。设计了两种评估设置：单图像任务（传统VQA）和多图像任务（跨图像推理），后者是越南语VQA中首次评估跨图像推理能力。

Result: 评估了多种最新的视觉语言模型，发现显著性能差距。最严重的错误出现在多图像问题上，特别是涉及跨图像整合和非跨度推理的任务，揭示了当前多模态模型在低资源语境下的局限性。

Conclusion: ViInfographicVQA为越南语InfographicVQA提供了基准结果，揭示了当前多模态模型在布局感知和跨图像推理方面的不足，鼓励未来研究关注这些方向。

Abstract: Infographic Visual Question Answering (InfographicVQA) evaluates a model's ability to read and reason over data-rich, layout-heavy visuals that combine text, charts, icons, and design elements. Compared with scene-text or natural-image VQA, infographics require stronger integration of OCR, layout understanding, and numerical and semantic reasoning. We introduce ViInfographicVQA, the first benchmark for Vietnamese InfographicVQA, comprising over 6747 real-world infographics and 20409 human-verified question-answer pairs across economics, healthcare, education, and more. The benchmark includes two evaluation settings. The Single-image task follows the traditional setup in which each question is answered using a single infographic. The Multi-image task requires synthesizing evidence across multiple semantically related infographics and is, to our knowledge, the first Vietnamese evaluation of cross-image reasoning in VQA. We evaluate a range of recent vision-language models on this benchmark, revealing substantial performance disparities, with the most significant errors occurring on Multi-image questions that involve cross-image integration and non-span reasoning. ViInfographicVQA contributes benchmark results for Vietnamese InfographicVQA and sheds light on the limitations of current multimodal models in low-resource contexts, encouraging future exploration of layout-aware and cross-image reasoning methods.

</details>


### [129] [Generative Spatiotemporal Data Augmentation](https://arxiv.org/abs/2512.12508)
*Jinfan Zhou,Lixin Luo,Sungmin Eum,Heesung Kwon,Jeong Joon Park*

Main category: cs.CV

Relevance: 65.0

TL;DR: 利用视频基础模型进行时空数据增强，通过视频扩散模型生成逼真的3D空间和时间变化，在低数据场景（如无人机图像）中提升模型性能


<details>
  <summary>Details</summary>
Motivation: 现有数据增强方法主要基于简单的几何变换或外观扰动，无法有效模拟真实世界的3D空间和时间变化。在标注稀缺的低数据场景（如无人机图像）中，需要更有效的增强方法来扩展数据分布

Method: 使用现成的视频扩散模型从给定图像数据集生成逼真的3D空间和时间变化，将合成的视频片段作为补充训练数据。提供实用指南：选择合适的时空生成设置、将标注转移到合成帧、处理新暴露区域的标注缺失问题

Result: 在COCO子集和无人机捕获数据集上的实验表明，时空增强能够沿着传统和先前生成方法未能充分覆盖的维度扩展数据分布，在数据稀缺情况下显著提升模型性能

Conclusion: 时空数据增强通过视频基础模型提供了一种有效的杠杆，可以在标注稀缺的低数据场景中改善模型性能，特别是在无人机图像等应用领域

Abstract: We explore spatiotemporal data augmentation using video foundation models to diversify both camera viewpoints and scene dynamics. Unlike existing approaches based on simple geometric transforms or appearance perturbations, our method leverages off-the-shelf video diffusion models to generate realistic 3D spatial and temporal variations from a given image dataset. Incorporating these synthesized video clips as supplemental training data yields consistent performance gains in low-data settings, such as UAV-captured imagery where annotations are scarce. Beyond empirical improvements, we provide practical guidelines for (i) choosing an appropriate spatiotemporal generative setup, (ii) transferring annotations to synthetic frames, and (iii) addressing disocclusion - regions newly revealed and unlabeled in generated views. Experiments on COCO subsets and UAV-captured datasets show that, when applied judiciously, spatiotemporal augmentation broadens the data distribution along axes underrepresented by traditional and prior generative methods, offering an effective lever for improving model performance in data-scarce regimes.

</details>


### [130] [No Cache Left Idle: Accelerating diffusion model via Extreme-slimming Caching](https://arxiv.org/abs/2512.12604)
*Tingyan Wen,Haoyu Li,Yihuang Chen,Xing Zhou,Lifei Zhu,Xueqian Wang*

Main category: cs.CV

Relevance: 65.0

TL;DR: X-Slim是一种训练免费的缓存加速器，通过利用时间步、结构和空间三个维度的缓存冗余，采用双阈值控制器实现"推-抛光"过程，在保持生成质量的同时显著加速扩散模型推理。


<details>
  <summary>Details</summary>
Motivation: 扩散模型虽然生成质量出色，但计算开销随步数、模型深度和序列长度线性增长。现有特征缓存方法存在固有权衡：激进的时间步重用能大幅加速但容易损害保真度，而块级或令牌级重用更安全但计算节省有限。需要一种统一框架来同时利用多个维度的缓存冗余。

Method: X-Slim提出双阈值控制器，将缓存过程转化为"推-抛光"流程：1) 首先在时间步级别推进重用直到早期警告线；2) 然后切换到轻量级的块级和令牌级刷新来抛光剩余冗余；3) 一旦跨越临界线就触发完整推理重置累积误差。每个级别都使用上下文感知指示器决定何时何地缓存。

Result: 在多样化任务上，X-Slim显著推进了速度-质量前沿：在FLUX.1-dev上延迟降低4.97倍，在HunyuanVideo上降低3.52倍，感知损失最小；在DiT-XL/2上实现3.13倍加速，FID比先前方法改善2.42。

Conclusion: X-Slim是首个统一利用时间步、结构和空间三个维度缓存冗余的框架，通过双阈值控制器的"推-抛光"策略，在保持生成质量的同时实现了显著的推理加速，为扩散模型的高效部署提供了有效解决方案。

Abstract: Diffusion models achieve remarkable generative quality, but computational overhead scales with step count, model depth, and sequence length. Feature caching is effective since adjacent timesteps yield highly similar features. However, an inherent trade-off remains: aggressive timestep reuse offers large speedups but can easily cross the critical line, hurting fidelity, while block- or token-level reuse is safer but yields limited computational savings. We present X-Slim (eXtreme-Slimming Caching), a training-free, cache-based accelerator that, to our knowledge, is the first unified framework to exploit cacheable redundancy across timesteps, structure (blocks), and space (tokens). Rather than simply mixing levels, X-Slim introduces a dual-threshold controller that turns caching into a push-then-polish process: it first pushes reuse at the timestep level up to an early-warning line, then switches to lightweight block- and token-level refresh to polish the remaining redundancy, and triggers full inference once the critical line is crossed to reset accumulated error. At each level, context-aware indicators decide when and where to cache. Across diverse tasks, X-Slim advances the speed-quality frontier. On FLUX.1-dev and HunyuanVideo, it reduces latency by up to 4.97x and 3.52x with minimal perceptual loss. On DiT-XL/2, it reaches 3.13x acceleration and improves FID by 2.42 over prior methods.

</details>


### [131] [D3D-VLP: Dynamic 3D Vision-Language-Planning Model for Embodied Grounding and Navigation](https://arxiv.org/abs/2512.12622)
*Zihan Wang,Seungjun Lee,Guangzhao Dai,Gim Hee Lee*

Main category: cs.CV

Relevance: 65.0

TL;DR: D3D-VLP模型通过动态3D思维链和协同学习策略，统一了具身智能中的规划、定位、导航和问答任务，在多个3D导航基准上达到SOTA。


<details>
  <summary>Details</summary>
Motivation: 解决具身智能中的关键困境：端到端模型缺乏可解释性和显式3D推理，而模块化系统忽略了跨组件依赖关系和协同效应。

Method: 提出动态3D视觉-语言-规划模型，包含：1) 动态3D思维链，在单一3D-VLM和CoT流水线中统一规划、定位、导航和问答；2) 协同学习策略，使用掩码自回归损失从大规模部分标注的混合数据中学习。

Result: 在多个基准测试中达到最先进结果：视觉语言导航(R2R-CE, REVERIE-CE, NavRAG-CE)、目标导航(HM3D-OVON)和任务导向的顺序定位与导航(SG3D)。真实世界移动操作实验进一步验证了有效性。

Conclusion: D3D-VLP通过统一的3D思维链框架和协同学习策略，成功解决了具身智能中的可解释性与性能平衡问题，为3D具身推理提供了新范式。

Abstract: Embodied agents face a critical dilemma that end-to-end models lack interpretability and explicit 3D reasoning, while modular systems ignore cross-component interdependencies and synergies. To bridge this gap, we propose the Dynamic 3D Vision-Language-Planning Model (D3D-VLP). Our model introduces two key innovations: 1) A Dynamic 3D Chain-of-Thought (3D CoT) that unifies planning, grounding, navigation, and question answering within a single 3D-VLM and CoT pipeline; 2) A Synergistic Learning from Fragmented Supervision (SLFS) strategy, which uses a masked autoregressive loss to learn from massive and partially-annotated hybrid data. This allows different CoT components to mutually reinforce and implicitly supervise each other. To this end, we construct a large-scale dataset with 10M hybrid samples from 5K real scans and 20K synthetic scenes that are compatible with online learning methods such as RL and DAgger. Our D3D-VLP achieves state-of-the-art results on multiple benchmarks, including Vision-and-Language Navigation (R2R-CE, REVERIE-CE, NavRAG-CE), Object-goal Navigation (HM3D-OVON), and Task-oriented Sequential Grounding and Navigation (SG3D). Real-world mobile manipulation experiments further validate the effectiveness.

</details>


### [132] [SignRAG: A Retrieval-Augmented System for Scalable Zero-Shot Road Sign Recognition](https://arxiv.org/abs/2512.12885)
*Minghao Zhu,Zhihao Zhang,Anmol Sidhu,Keith Redmill*

Main category: cs.CV

Relevance: 65.0

TL;DR: 提出基于检索增强生成(RAG)的零样本道路标志识别框架，使用VLM生成文本描述，检索相关候选，再由LLM进行细粒度识别，在303个标志上达到高准确率。


<details>
  <summary>Details</summary>
Motivation: 传统深度学习方法面临道路标志类别繁多、标注数据难以获取的挑战，需要开发无需任务特定训练的可扩展识别系统。

Method: 采用RAG范式：1) 使用视觉语言模型(VLM)从输入图像生成文本描述；2) 从参考设计向量数据库中检索最相关的标志候选；3) 使用大语言模型(LLM)对检索结果进行推理，做出细粒度识别。

Result: 在Ohio MUTCD的303个监管标志上验证，理想参考图像准确率95.58%，真实道路数据准确率82.45%，证明RAG架构在道路标志识别中的有效性。

Conclusion: RAG-based架构可用于创建无需任务特定训练的可扩展、准确的道路标志识别系统，为零样本视觉识别任务提供了新思路。

Abstract: Automated road sign recognition is a critical task for intelligent transportation systems, but traditional deep learning methods struggle with the sheer number of sign classes and the impracticality of creating exhaustive labeled datasets. This paper introduces a novel zero-shot recognition framework that adapts the Retrieval-Augmented Generation (RAG) paradigm to address this challenge. Our method first uses a Vision Language Model (VLM) to generate a textual description of a sign from an input image. This description is used to retrieve a small set of the most relevant sign candidates from a vector database of reference designs. Subsequently, a Large Language Model (LLM) reasons over the retrieved candidates to make a final, fine-grained recognition. We validate this approach on a comprehensive set of 303 regulatory signs from the Ohio MUTCD. Experimental results demonstrate the framework's effectiveness, achieving 95.58% accuracy on ideal reference images and 82.45% on challenging real-world road data. This work demonstrates the viability of RAG-based architectures for creating scalable and accurate systems for road sign recognition without task-specific training.

</details>


### [133] [Progressive Conditioned Scale-Shift Recalibration of Self-Attention for Online Test-time Adaptation](https://arxiv.org/abs/2512.12673)
*Yushun Tang,Ziqiong Liu,Jiyuan Jia,Yi Zhang,Zhihai He*

Main category: cs.CV

Relevance: 65.0

TL;DR: 提出PCSR方法，通过渐进式条件缩放-平移重新校准来在线适应Transformer模型到新领域，在ImageNet-C上提升分类准确率达3.9%


<details>
  <summary>Details</summary>
Motivation: 在线测试时适应旨在推理阶段根据顺序输入样本动态调整网络模型。研究发现，当将Transformer模型应用于新目标领域时，其自注意力模块的Query、Key和Value特征与源领域相比变化显著，导致性能大幅下降。

Method: 提出渐进式条件缩放-平移重新校准(PCSR)方法。将在线模型适应视为渐进式领域偏移分离过程。在每个Transformer层，学习一个领域分离网络提取领域偏移特征，用于通过因子生成网络预测自注意力重新校准的缩放和平移参数。这两个轻量级网络在推理期间在线适应。

Result: 在基准数据集上的实验结果表明，PCSR方法能够显著提高在线测试时领域适应性能，在ImageNet-C数据集上的分类准确率提升幅度高达3.9%。

Conclusion: 通过渐进式条件缩放-平移重新校准Transformer的自注意力模块，可以有效解决跨领域适应中的特征分布偏移问题，显著提升模型在新领域的性能。

Abstract: Online test-time adaptation aims to dynamically adjust a network model in real-time based on sequential input samples during the inference stage. In this work, we find that, when applying a transformer network model to a new target domain, the Query, Key, and Value features of its self-attention module often change significantly from those in the source domain, leading to substantial performance degradation of the transformer model. To address this important issue, we propose to develop a new approach to progressively recalibrate the self-attention at each layer using a local linear transform parameterized by conditioned scale and shift factors. We consider the online model adaptation from the source domain to the target domain as a progressive domain shift separation process. At each transformer network layer, we learn a Domain Separation Network to extract the domain shift feature, which is used to predict the scale and shift parameters for self-attention recalibration using a Factor Generator Network. These two lightweight networks are adapted online during inference. Experimental results on benchmark datasets demonstrate that the proposed progressive conditioned scale-shift recalibration (PCSR) method is able to significantly improve the online test-time domain adaptation performance by a large margin of up to 3.9\% in classification accuracy on the ImageNet-C dataset.

</details>


### [134] [CoRe3D: Collaborative Reasoning as a Foundation for 3D Intelligence](https://arxiv.org/abs/2512.12768)
*Tianjiao Yu,Xinzhuo Li,Yifan Shen,Yuanzhe Liu,Ismini Lourentzou*

Main category: cs.CV

Relevance: 65.0

TL;DR: CoRe3D提出了一种统一的3D理解和生成推理框架，通过语义和空间抽象联合操作，使语言推断的高级意图直接指导低级3D内容形成。


<details>
  <summary>Details</summary>
Motivation: 现有大型多模态模型中的显式推理机制已被证明能提升模型可靠性、可解释性和跨模态对齐，但这类推理中心方法在3D领域的扩展仍不充分。需要开发能够将高级语言意图与低级3D内容形成直接关联的推理框架。

Method: CoRe3D采用空间接地推理表示，将3D潜在空间分解为局部化区域，使模型能够以组合和程序化方式对几何进行推理。通过紧密耦合语义思维链推理与结构化空间推理，实现3D输出的局部一致性和语言描述忠实对齐。

Result: CoRe3D生成的3D输出展现出强大的局部一致性和与语言描述的忠实对齐，表明该推理框架能有效提升3D理解和生成的质量。

Conclusion: 显式推理机制对3D多模态模型至关重要，CoRe3D通过语义-空间联合推理框架为3D理解和生成提供了新方向，有望提升3D内容的可靠性和可解释性。

Abstract: Recent advances in large multimodal models suggest that explicit reasoning mechanisms play a critical role in improving model reliability, interpretability, and cross-modal alignment. While such reasoning-centric approaches have been proven effective in language and vision tasks, their extension to 3D remains underdeveloped. CoRe3D introduces a unified 3D understanding and generation reasoning framework that jointly operates over semantic and spatial abstractions, enabling high-level intent inferred from language to directly guide low-level 3D content formation. Central to this design is a spatially grounded reasoning representation that decomposes 3D latent space into localized regions, allowing the model to reason over geometry in a compositional and procedural manner. By tightly coupling semantic chain-of-thought inference with structured spatial reasoning, CoRe3D produces 3D outputs that exhibit strong local consistency and faithful alignment with linguistic descriptions.

</details>


### [135] [Scaling Up AI-Generated Image Detection via Generator-Aware Prototypes](https://arxiv.org/abs/2512.12982)
*Ziheng Qin,Yuheng Ji,Renshuai Tao,Yuxuan Tian,Yuyang Liu,Yipu Wang,Xiaolong Zheng*

Main category: cs.CV

Relevance: 65.0

TL;DR: 论文提出GAPL框架解决AIGI检测中的"先益后冲"困境，通过原型学习和LoRA增强，在多样化生成器上实现SOTA检测性能


<details>
  <summary>Details</summary>
Motivation: 当前AIGI检测器通常通过聚合多种生成器数据来提升泛化能力，但本文发现存在"先益后冲"困境：随着数据源多样性增加，检测器性能先提升后下降。这源于两个核心问题：数据级异质性导致真实与合成图像特征分布重叠加剧，以及模型级瓶颈（固定预训练编码器无法适应复杂性上升）

Method: 提出Generator-Aware Prototype Learning (GAPL)框架：1) 学习紧凑的典型伪造原型集，创建统一、低方差的特征空间以应对数据异质性；2) 采用两阶段训练方案结合Low-Rank Adaptation (LoRA)，增强判别能力同时保留预训练知识

Result: GAPL在广泛GAN和扩散模型生成器上实现了最先进的检测性能，展示了优越的泛化能力和鲁棒性

Conclusion: GAPL通过结构化学习范式和自适应模型优化，有效解决了AIGI检测中的"先益后冲"困境，为通用图像伪造检测提供了新思路

Abstract: The pursuit of a universal AI-generated image (AIGI) detector often relies on aggregating data from numerous generators to improve generalization. However, this paper identifies a paradoxical phenomenon we term the Benefit then Conflict dilemma, where detector performance stagnates and eventually degrades as source diversity expands. Our systematic analysis, diagnoses this failure by identifying two core issues: severe data-level heterogeneity, which causes the feature distributions of real and synthetic images to increasingly overlap, and a critical model-level bottleneck from fixed, pretrained encoders that cannot adapt to the rising complexity. To address these challenges, we propose Generator-Aware Prototype Learning (GAPL), a framework that constrain representation with a structured learning paradigm. GAPL learns a compact set of canonical forgery prototypes to create a unified, low-variance feature space, effectively countering data heterogeneity.To resolve the model bottleneck, it employs a two-stage training scheme with Low-Rank Adaptation, enhancing its discriminative power while preserving valuable pretrained knowledge. This approach establishes a more robust and generalizable decision boundary. Through extensive experiments, we demonstrate that GAPL achieves state-of-the-art performance, showing superior detection accuracy across a wide variety of GAN and diffusion-based generators. Code is available at https://github.com/UltraCapture/GAPL

</details>


### [136] [Few-Step Distillation for Text-to-Image Generation: A Practical Guide](https://arxiv.org/abs/2512.13006)
*Yifan Pu,Yizeng Han,Zhiwei Tang,Jiasheng Tang,Fan Wang,Bohan Zhuang,Gao Huang*

Main category: cs.CV

Relevance: 65.0

TL;DR: 该研究首次系统性地将扩散蒸馏技术应用于开放域文本到图像生成，在FLUX.1-lite教师模型上比较了最先进的蒸馏方法，为实际应用提供了实用指南。


<details>
  <summary>Details</summary>
Motivation: 扩散蒸馏在类别条件图像合成中已显著加速生成，但其在开放域文本到图像生成中的应用仍不明确。研究旨在填补这一空白，探索如何将蒸馏技术有效应用于自由形式的语言提示。

Method: 将现有蒸馏方法统一到框架中，在FLUX.1-lite教师模型上进行系统比较。重点分析从离散类别标签转向自由文本提示时的关键障碍，并提供输入缩放、网络架构和超参数等实用指南。

Result: 建立了快速、高保真、资源高效的扩散生成器在文本到图像应用中的坚实基础，提供了开源实现和预训练学生模型。

Conclusion: 该研究为实际部署文本到图像扩散蒸馏提供了系统指导，推动了高效生成模型在开放域场景中的应用。

Abstract: Diffusion distillation has dramatically accelerated class-conditional image synthesis, but its applicability to open-ended text-to-image (T2I) generation is still unclear. We present the first systematic study that adapts and compares state-of-the-art distillation techniques on a strong T2I teacher model, FLUX.1-lite. By casting existing methods into a unified framework, we identify the key obstacles that arise when moving from discrete class labels to free-form language prompts. Beyond a thorough methodological analysis, we offer practical guidelines on input scaling, network architecture, and hyperparameters, accompanied by an open-source implementation and pretrained student models. Our findings establish a solid foundation for deploying fast, high-fidelity, and resource-efficient diffusion generators in real-world T2I applications. Code is available on github.com/alibaba-damo-academy/T2I-Distill.

</details>


### [137] [Motus: A Unified Latent Action World Model](https://arxiv.org/abs/2512.13030)
*Hongzhe Bi,Hengkai Tan,Shenghao Xie,Zeyuan Wang,Shuhe Huang,Haitian Liu,Ruowen Zhao,Yao Feng,Chendong Xiang,Yinze Rong,Hongyan Zhao,Hanyu Liu,Zhizhong Su,Lei Ma,Hang Su,Jun Zhu*

Main category: cs.CV

Relevance: 65.0

TL;DR: Motus是一个统一的潜在动作世界模型，通过混合Transformer架构集成理解、视频生成和动作三个专家模块，利用光流学习潜在动作，实现多模态生成能力的统一。


<details>
  <summary>Details</summary>
Motivation: 当前具身智能方法使用孤立的理解、世界建模和控制模型，这种碎片化阻碍了多模态生成能力的统一，也限制了从大规模异构数据中学习的能力。

Method: 提出Motus统一模型，采用混合Transformer架构集成三个专家模块，使用UniDiffuser风格调度器实现不同建模模式的灵活切换，利用光流学习潜在动作，采用三阶段训练流程和六层数据金字塔。

Result: 在仿真环境中相比X-VLA提升15%，相比Pi0.5提升45%；在真实场景中提升11-48%，展示了统一建模对下游机器人任务的显著益处。

Conclusion: 统一建模所有功能和先验知识能显著提升下游机器人任务性能，Motus为具身智能提供了一个有效的统一框架。

Abstract: While a general embodied agent must function as a unified system, current methods are built on isolated models for understanding, world modeling, and control. This fragmentation prevents unifying multimodal generative capabilities and hinders learning from large-scale, heterogeneous data. In this paper, we propose Motus, a unified latent action world model that leverages existing general pretrained models and rich, sharable motion information. Motus introduces a Mixture-of-Transformer (MoT) architecture to integrate three experts (i.e., understanding, video generation, and action) and adopts a UniDiffuser-style scheduler to enable flexible switching between different modeling modes (i.e., world models, vision-language-action models, inverse dynamics models, video generation models, and video-action joint prediction models). Motus further leverages the optical flow to learn latent actions and adopts a recipe with three-phase training pipeline and six-layer data pyramid, thereby extracting pixel-level "delta action" and enabling large-scale action pretraining. Experiments show that Motus achieves superior performance against state-of-the-art methods in both simulation (a +15% improvement over X-VLA and a +45% improvement over Pi0.5) and real-world scenarios(improved by +11~48%), demonstrating unified modeling of all functionalities and priors significantly benefits downstream robotic tasks.

</details>


### [138] [Bi-Erasing: A Bidirectional Framework for Concept Removal in Diffusion Models](https://arxiv.org/abs/2512.13039)
*Hao Chen,Yiwei Wang,Songze Li*

Main category: cs.CV

Relevance: 65.0

TL;DR: 提出Bi-Erasing框架，通过双向图像引导的概念擦除方法，在文本到图像模型中同时进行有害概念抑制和安全替代增强，平衡概念移除效果与生成质量。


<details>
  <summary>Details</summary>
Motivation: 现有概念擦除方法通常采用单向策略（要么抑制目标概念，要么增强安全替代），难以在概念移除和生成质量之间取得平衡。需要一种更平衡的方法来消除文本到图像模型中的不安全或非法概念。

Method: 提出双向图像引导概念擦除框架，包含两个解耦的图像分支：负分支负责抑制有害语义，正分支为安全替代提供视觉指导。基于文本提示和对应图像的联合表示，通过联合优化这两个互补方向实现平衡。还应用基于掩码的过滤来防止擦除过程中的无关内容干扰。

Result: 在广泛的实验评估中，Bi-Erasing在平衡概念移除效果和视觉保真度方面优于基线方法，实现了更好的擦除效果与生成可用性平衡。

Conclusion: Bi-Erasing框架通过同时进行概念抑制和安全增强的双向策略，有效解决了现有概念擦除方法在移除效果和生成质量之间的权衡问题，为文本到图像模型的安全部署提供了更平衡的解决方案。

Abstract: Concept erasure, which fine-tunes diffusion models to remove undesired or harmful visual concepts, has become a mainstream approach to mitigating unsafe or illegal image generation in text-to-image models.However, existing removal methods typically adopt a unidirectional erasure strategy by either suppressing the target concept or reinforcing safe alternatives, making it difficult to achieve a balanced trade-off between concept removal and generation quality. To address this limitation, we propose a novel Bidirectional Image-Guided Concept Erasure (Bi-Erasing) framework that performs concept suppression and safety enhancement simultaneously. Specifically, based on the joint representation of text prompts and corresponding images, Bi-Erasing introduces two decoupled image branches: a negative branch responsible for suppressing harmful semantics and a positive branch providing visual guidance for safe alternatives. By jointly optimizing these complementary directions, our approach achieves a balance between erasure efficacy and generation usability. In addition, we apply mask-based filtering to the image branches to prevent interference from irrelevant content during the erasure process. Across extensive experiment evaluations, the proposed Bi-Erasing outperforms baseline methods in balancing concept removal effectiveness and visual fidelity.

</details>


### [139] [Forging a Dynamic Memory: Retrieval-Guided Continual Learning for Generalist Medical Foundation Models](https://arxiv.org/abs/2512.13072)
*Zizhi Chen,Yizhen Gao,Minghao Han,Yizhou Liu,Zhaoyu Chen,Dingkang Yang,Lihua Zhang*

Main category: cs.CV

Relevance: 65.0

TL;DR: 提出一个用于医学持续学习的多模态检索增强生成框架，通过动态知识蒸馏解决模态间特征保留与领域鸿沟的平衡问题


<details>
  <summary>Details</summary>
Motivation: 多模态生物医学视觉语言模型在持续学习中面临核心困境：如何在保留细粒度模态内特征的同时，跨越不同模态间的显著领域鸿沟

Method: 1) 基于PubMed科学论文构建1800万多模态医学检索数据库；2) 将RAG集成到持续学习中，使用多模态多层RAG系统提供实时指导；3) 引入动态知识蒸馏框架，动态调节参数空间重要性、知识粒度和参考数据集分布

Result: 提出的方法在所有指标上均达到最先进性能，并设计了更严格的医学通用任务增量学习基准来验证临床价值

Conclusion: 该框架成功解决了医学多模态持续学习中的核心挑战，通过RAG和动态知识蒸馏实现了细粒度特征保留与跨模态适应的平衡

Abstract: Multimodal biomedical Vision-Language Models (VLMs) exhibit immense potential in the field of Continual Learning (CL). However, they confront a core dilemma: how to preserve fine-grained intra-modality features while bridging the significant domain gap across different modalities. To address this challenge, we propose a comprehensive framework. Leveraging our 18-million multimodal and comprehensive medical retrieval database derived from PubMed scientific papers, we pioneer the integration of Retrieval-Augmented Generation (RAG) into CL. Specifically, we employ a multi-modal, multi-layer RAG system that provides real-time guidance for model fine-tuning through dynamic, on-demand knowledge retrieval. Building upon this, we introduce a dynamic knowledge distillation framework. This framework precisely resolves the aforementioned core dilemma by dynamically modulating the importance of the parameter space, the granularity of the distilled knowledge, and the data distribution of the reference dataset in accordance with the required level of detail. To thoroughly validate the clinical value of our strategy, we have designed a more rigorous \textbf{M}edical Generalist Task Incremental Learning (MGTIL) benchmark. This benchmark is engineered to simultaneously evaluate the model's capacity for adaptation to significant domain shifts, retention of subtle intra-domain features, and real-time learning of novel and complex medical tasks. Extensive experimental results demonstrate that our proposed method achieves state-of-the-art (SOTA) performance across all metrics. The code is provided in the supplementary materials.

</details>


### [140] [Harmonizing Generalization and Specialization: Uncertainty-Informed Collaborative Learning for Semi-supervised Medical Image Segmentation](https://arxiv.org/abs/2512.13101)
*Wenjing Lu,Yi Hong,Yang Yang*

Main category: cs.CV

Relevance: 65.0

TL;DR: UnCoL：一种用于半监督医学图像分割的双教师框架，通过不确定性引导的协同学习，平衡通用基础模型知识和任务特定表示，在有限标注下实现接近全监督的性能。


<details>
  <summary>Details</summary>
Motivation: 视觉基础模型在医学图像分割中表现出强大的泛化能力，但在有限标注或罕见病理变化下，通用先验与任务特定需求不匹配，导致在专业临床任务上泛化能力不足。

Method: 提出Uncertainty-informed Collaborative Learning (UnCoL)双教师框架：1）从冻结的基础模型中蒸馏视觉和语义表示以传递通用知识；2）维护逐步适应的教师模型以捕获细粒度的任务特定表示；3）通过预测不确定性自适应调节伪标签学习，选择性抑制不可靠监督并稳定模糊区域的学习。

Result: 在多样化的2D和3D分割基准测试中，UnCoL始终优于最先进的半监督方法和基础模型基线，在显著减少标注需求的情况下实现了接近全监督的性能。

Conclusion: UnCoL通过协同通用基础模型知识和任务特定表示，有效解决了医学图像分割中通用先验与任务需求不匹配的问题，为有限标注下的专业临床任务提供了有效的解决方案。

Abstract: Vision foundation models have demonstrated strong generalization in medical image segmentation by leveraging large-scale, heterogeneous pretraining. However, they often struggle to generalize to specialized clinical tasks under limited annotations or rare pathological variations, due to a mismatch between general priors and task-specific requirements. To address this, we propose Uncertainty-informed Collaborative Learning (UnCoL), a dual-teacher framework that harmonizes generalization and specialization in semi-supervised medical image segmentation. Specifically, UnCoL distills both visual and semantic representations from a frozen foundation model to transfer general knowledge, while concurrently maintaining a progressively adapting teacher to capture fine-grained and task-specific representations. To balance guidance from both teachers, pseudo-label learning in UnCoL is adaptively regulated by predictive uncertainty, which selectively suppresses unreliable supervision and stabilizes learning in ambiguous regions. Experiments on diverse 2D and 3D segmentation benchmarks show that UnCoL consistently outperforms state-of-the-art semi-supervised methods and foundation model baselines. Moreover, our model delivers near fully supervised performance with markedly reduced annotation requirements.

</details>


### [141] [Weight Space Correlation Analysis: Quantifying Feature Utilization in Deep Learning Models](https://arxiv.org/abs/2512.13144)
*Chun Kit Wong,Paraskevas Pegios,Nina Weng,Emilie Pi Fogtmann Sejer,Martin Grønnebæk Tolsgaard,Anders Nymark Christensen,Aasa Feragen*

Main category: cs.CV

Relevance: 65.0

TL;DR: 提出Weight Space Correlation Analysis方法，通过分析分类头权重向量的对齐程度，量化医学影像模型中临床任务与元数据任务的特征利用情况，验证模型是否选择性利用临床相关特征而非无关元数据。


<details>
  <summary>Details</summary>
Motivation: 医学影像深度学习模型容易受到捷径学习影响，会利用图像嵌入中包含的元数据（如扫描仪型号）等混杂因素进行预测。关键问题是模型是否主动利用这些编码信息进行最终预测，需要可解释的方法来验证模型的可信度。

Method: 提出Weight Space Correlation Analysis方法，通过测量主要临床任务分类头与辅助元数据任务分类头权重向量之间的对齐程度，量化特征利用情况。首先验证方法能检测人工诱导的捷径学习，然后应用于SA-SonoNet模型分析早产预测任务。

Result: 验证了方法能成功检测人工诱导的捷径学习。应用于早产预测模型分析发现：虽然嵌入包含大量元数据，但早产分类器的权重向量与临床相关因素（如出生体重）高度相关，而与临床无关的采集因素（如扫描仪）解耦。

Conclusion: 该方法为验证模型可信度提供了工具，表明在没有诱导偏差的情况下，临床模型会选择性利用与真实临床信号相关的特征，而非无关元数据。

Abstract: Deep learning models in medical imaging are susceptible to shortcut learning, relying on confounding metadata (e.g., scanner model) that is often encoded in image embeddings. The crucial question is whether the model actively utilizes this encoded information for its final prediction. We introduce Weight Space Correlation Analysis, an interpretable methodology that quantifies feature utilization by measuring the alignment between the classification heads of a primary clinical task and auxiliary metadata tasks. We first validate our method by successfully detecting artificially induced shortcut learning. We then apply it to probe the feature utilization of an SA-SonoNet model trained for Spontaneous Preterm Birth (sPTB) prediction. Our analysis confirmed that while the embeddings contain substantial metadata, the sPTB classifier's weight vectors were highly correlated with clinically relevant factors (e.g., birth weight) but decoupled from clinically irrelevant acquisition factors (e.g. scanner). Our methodology provides a tool to verify model trustworthiness, demonstrating that, in the absence of induced bias, the clinical model selectively utilizes features related to the genuine clinical signal.

</details>


### [142] [CogniEdit: Dense Gradient Flow Optimization for Fine-Grained Image Editing](https://arxiv.org/abs/2512.13276)
*Yan Li,Lin Liu,Xiaopeng Zhang,Wei Xue,Wenhan Luo,Yike Guo,Qi Tian*

Main category: cs.CV

Relevance: 65.0

TL;DR: CogniEdit：一个结合多模态推理与密集奖励优化的统一框架，用于基于指令的图像编辑，在细粒度指令遵循与视觉质量之间取得平衡


<details>
  <summary>Details</summary>
Motivation: 现有基于扩散模型的指令图像编辑方法在处理指定精确属性（如颜色、位置、数量）的细粒度指令时存在困难。虽然最近的方法使用GRPO进行对齐，但仅在单个采样步骤优化，提供稀疏反馈，限制了轨迹级控制。

Method: 提出CogniEdit框架，包含三个组件：1) 多模态大语言模型将复杂指令分解为可操作指令；2) 动态令牌焦点重定位自适应强调细粒度属性；3) 基于密集GRPO的优化，在连续去噪步骤间传播梯度，实现轨迹级监督。

Result: 在基准数据集上的广泛实验表明，CogniEdit在平衡细粒度指令遵循与视觉质量和可编辑性保持方面实现了最先进的性能。

Conclusion: 通过结合多模态推理与密集奖励优化，CogniEdit能够更好地处理细粒度指令，在图像编辑任务中实现更好的轨迹级控制。

Abstract: Instruction-based image editing with diffusion models has achieved impressive results, yet existing methods struggle with fine-grained instructions specifying precise attributes such as colors, positions, and quantities. While recent approaches employ Group Relative Policy Optimization (GRPO) for alignment, they optimize only at individual sampling steps, providing sparse feedback that limits trajectory-level control. We propose a unified framework CogniEdit, combining multi-modal reasoning with dense reward optimization that propagates gradients across consecutive denoising steps, enabling trajectory-level gradient flow through the sampling process. Our method comprises three components: (1) Multi-modal Large Language Models for decomposing complex instructions into actionable directives, (2) Dynamic Token Focus Relocation that adaptively emphasizes fine-grained attributes, and (3) Dense GRPO-based optimization that propagates gradients across consecutive steps for trajectory-level supervision. Extensive experiments on benchmark datasets demonstrate that our CogniEdit achieves state-of-the-art performance in balancing fine-grained instruction following with visual quality and editability preservation

</details>


### [143] [Video Reality Test: Can AI-Generated ASMR Videos fool VLMs and Humans?](https://arxiv.org/abs/2512.13281)
*Jiaqi Wang,Weijia Wu,Yi Zhan,Rui Zhao,Ming Hu,James Cheng,Wei Liu,Philip Torr,Kevin Qinghong Lin*

Main category: cs.CV

Relevance: 65.0

TL;DR: Video Reality Test：基于ASMR视频的AI生成视频检测基准，测试音频-视觉耦合下的感知真实性，发现当前最佳视频生成模型能欺骗大多数VLM，而人类专家表现更好。


<details>
  <summary>Details</summary>
Motivation: 随着视频生成技术的进步，AI生成的视频越来越难以与真实视频区分，这带来了社会挑战。现有检测基准大多只评估无音频的视频，针对宽泛的叙事领域，且仅关注分类。需要测试最先进的视频生成模型是否能产生具有沉浸感、音频配对且能可靠欺骗人类和视觉语言模型的视频。

Method: 引入Video Reality Test基准套件：1) 基于精心挑选的真实ASMR视频，针对细粒度的动作-对象交互，在对象、动作和背景上具有多样性；2) 采用对抗性创作者-评审者协议，视频生成模型作为创作者试图欺骗评审者，而VLM作为评审者试图识别虚假性。

Result: 最佳创作者Veo3.1-Fast甚至能欺骗大多数VLM：最强的评审者(Gemini 2.5-Pro)仅达到56%准确率(随机50%)，远低于人类专家(81.25%)。添加音频能改善真假判别，但水印等表面线索仍会显著误导模型。

Conclusion: 这些发现划定了当前视频生成真实性的边界，并暴露了VLM在感知保真度和音频-视觉一致性方面的局限性。该基准有助于评估视频生成模型的真实性和VLM的检测能力。

Abstract: Recent advances in video generation have produced vivid content that are often indistinguishable from real videos, making AI-generated video detection an emerging societal challenge. Prior AIGC detection benchmarks mostly evaluate video without audio, target broad narrative domains, and focus on classification solely. Yet it remains unclear whether state-of-the-art video generation models can produce immersive, audio-paired videos that reliably deceive humans and VLMs. To this end, we introduce Video Reality Test, an ASMR-sourced video benchmark suite for testing perceptual realism under tight audio-visual coupling, featuring the following dimensions: \textbf{(i) Immersive ASMR video-audio sources.} Built on carefully curated real ASMR videos, the benchmark targets fine-grained action-object interactions with diversity across objects, actions, and backgrounds. \textbf{(ii) Peer-Review evaluation.} An adversarial creator-reviewer protocol where video generation models act as creators aiming to fool reviewers, while VLMs serve as reviewers seeking to identify fakeness. Our experimental findings show: The best creator Veo3.1-Fast even fools most VLMs: the strongest reviewer (Gemini 2.5-Pro) achieves only 56\% accuracy (random 50\%), far below that of human experts (81.25\%). Adding audio improves real-fake discrimination, yet superficial cues such as watermarks can still significantly mislead models. These findings delineate the current boundary of video generation realism and expose limitations of VLMs in perceptual fidelity and audio-visual consistency. Our code is available at https://github.com/video-reality-test/video-reality-test.

</details>


### [144] [CausalCLIP: Causally-Informed Feature Disentanglement and Filtering for Generalizable Detection of Generated Images](https://arxiv.org/abs/2512.13285)
*Bo Liu,Qiao Qin,Qinghui He*

Main category: cs.CV

Relevance: 65.0

TL;DR: CausalCLIP：基于因果推断的图像生成检测框架，通过解耦因果特征与非因果特征提升跨生成模型的泛化能力


<details>
  <summary>Details</summary>
Motivation: 现有生成图像检测方法（包括基于预训练视觉语言模型的方法）往往产生高度纠缠的表征，混合了任务相关的取证线索（因果特征）与虚假/无关模式（非因果特征），限制了跨不同生成技术的泛化能力。

Method: 提出CausalCLIP框架：1）使用结构因果模型建模生成过程；2）通过Gumbel-Softmax特征掩码和Hilbert-Schmidt独立性准则约束实现统计独立性，解耦因果与非因果特征；3）基于因果推断原理进行针对性过滤，保留最具可迁移性和判别性的取证线索。

Result: 在不同系列未见生成模型测试中，CausalCLIP展现出强大泛化能力，在准确率上比SOTA方法提升6.83%，平均精度提升4.06%。

Conclusion: 通过显式解耦因果特征并基于因果推断进行过滤，CausalCLIP能够提取稳定、可迁移的取证特征，显著提升生成图像检测的跨模型泛化性能。

Abstract: The rapid advancement of generative models has increased the demand for generated image detectors capable of generalizing across diverse and evolving generation techniques. However, existing methods, including those leveraging pre-trained vision-language models, often produce highly entangled representations, mixing task-relevant forensic cues (causal features) with spurious or irrelevant patterns (non-causal features), thus limiting generalization. To address this issue, we propose CausalCLIP, a framework that explicitly disentangles causal from non-causal features and employs targeted filtering guided by causal inference principles to retain only the most transferable and discriminative forensic cues. By modeling the generation process with a structural causal model and enforcing statistical independence through Gumbel-Softmax-based feature masking and Hilbert-Schmidt Independence Criterion (HSIC) constraints, CausalCLIP isolates stable causal features robust to distribution shifts. When tested on unseen generative models from different series, CausalCLIP demonstrates strong generalization ability, achieving improvements of 6.83% in accuracy and 4.06% in average precision over state-of-the-art methods.

</details>


### [145] [ShowTable: Unlocking Creative Table Visualization with Collaborative Reflection and Refinement](https://arxiv.org/abs/2512.13303)
*Zhihang Liu,Xiaoyi Bao,Pandeng Li,Junjie Zhou,Zhaohe Liao,Yefei He,Kaixun Jiang,Chen-Wei Xie,Yun Zheng,Hongtao Xie*

Main category: cs.CV

Relevance: 65.0

TL;DR: ShowTable提出了一种结合MLLM和扩散模型的渐进式自校正流程，用于生成创意表格可视化信息图，并构建了TableVisBench基准进行评估。


<details>
  <summary>Details</summary>
Motivation: 现有生成模型在需要深度推理、规划和精确数据到视觉映射的任务上表现不足，特别是在超越一般场景的创意表格可视化任务中。需要开发能够理解表格数据并生成美观、准确信息图的系统。

Method: 提出ShowTable流程：MLLM作为中央协调器进行视觉规划推理和错误判断，扩散模型执行MLLM指令，通过渐进式自校正过程实现高保真结果。构建了三个自动化数据构造流程来训练不同模块。

Result: 实验表明，使用不同模型实例化的ShowTable流程显著优于基线方法，在TableVisBench基准（800个挑战性实例，5个评估维度）上表现出强大的多模态推理、生成和错误校正能力。

Conclusion: ShowTable通过MLLM与扩散模型的协同工作，在创意表格可视化任务上取得了突破性进展，展示了有效的多模态推理和自校正能力，为数据可视化生成提供了新思路。

Abstract: While existing generation and unified models excel at general image generation, they struggle with tasks requiring deep reasoning, planning, and precise data-to-visual mapping abilities beyond general scenarios. To push beyond the existing limitations, we introduce a new and challenging task: creative table visualization, requiring the model to generate an infographic that faithfully and aesthetically visualizes the data from a given table. To address this challenge, we propose ShowTable, a pipeline that synergizes MLLMs with diffusion models via a progressive self-correcting process. The MLLM acts as the central orchestrator for reasoning the visual plan and judging visual errors to provide refined instructions, the diffusion execute the commands from MLLM, achieving high-fidelity results. To support this task and our pipeline, we introduce three automated data construction pipelines for training different modules. Furthermore, we introduce TableVisBench, a new benchmark with 800 challenging instances across 5 evaluation dimensions, to assess performance on this task. Experiments demonstrate that our pipeline, instantiated with different models, significantly outperforms baselines, highlighting its effective multi-modal reasoning, generation, and error correction capabilities.

</details>


### [146] [Seedance 1.5 pro: A Native Audio-Visual Joint Generation Foundation Model](https://arxiv.org/abs/2512.13507)
*Siyan Chen,Yanfei Chen,Ying Chen,Zhuo Chen,Feng Cheng,Xuyan Chi,Jian Cong,Qinpeng Cui,Qide Dong,Junliang Fan,Jing Fang,Zetao Fang,Chengjian Feng,Han Feng,Mingyuan Gao,Yu Gao,Qiushan Guo,Boyang Hao,Qingkai Hao,Bibo He,Qian He,Tuyen Hoang,Ruoqing Hu,Xi Hu,Weilin Huang,Zhaoyang Huang,Zhongyi Huang,Siqi Jiang,Wei Jiang,Yunpu Jiang,Zhuo Jiang,Ashley Kim,Jianan Kong,Zhichao Lai,Shanshan Lao,Ai Li,Feiya Li,Gen Li,Huixia Li,JiaShi Li,Liang Li,Ming Li,Tao Li,Xian Li,Xiaojie Li,Xiaoyang Li,Xingxing Li,Yameng Li,Yifu Li,Yiying Li,Chao Liang,Ying Liang,Zhiqiang Liang,Wang Liao,Yalin Liao,Heng Lin,Kengyu Lin,Shanchuan Lin,Xi Lin,Zhijie Lin,Feng Ling,Fangfang Liu,Gaohong Liu,Jiawei Liu,Jie Liu,Shouda Liu,Shu Liu,Sichao Liu,Songwei Liu,Xin Liu,Xue Liu,Yibo Liu,Zikun Liu,Zuxi Liu,Junlin Lyu,Lecheng Lyu,Qian Lyu,Han Mu,Xiaonan Nie,Jingzhe Ning,Xitong Pan,Yanghua Peng,Lianke Qin,Xueqiong Qu,Yuxi Ren,Yuchen Shen,Guang Shi,Lei Shi,Yan Song,Yinglong Song,Fan Sun,Li Sun,Renfei Sun,Zeyu Sun,Wenjing Tang,Zirui Tao,Feng Wang,Furui Wang,Jinran Wang,Junkai Wang,Ke Wang,Kexin Wang,Qingyi Wang,Rui Wang,Sen Wang,Shuai Wang,Tingru Wang,Weichen Wang,Xin Wang,Yanhui Wang,Yue Wang,Yuping Wang,Yuxuan Wang,Ziyu Wang,Guoqiang Wei,Wanru Wei,Di Wu,Guohong Wu,Hanjie Wu,Jian Wu,Jie Wu,Ruolan Wu,Xinglong Wu,Yonghui Wu,Ruiqi Xia,Liang Xiang,Fei Xiao,XueFeng Xiao,Pan Xie,Shuangyi Xie,Shuang Xu,Jinlan Xue,Bangbang Yang,Ceyuan Yang,Jiaqi Yang,Runkai Yang,Tao Yang,Yang Yang,Yihang Yang,ZhiXian Yang,Ziyan Yang,Yifan Yao,Zilyu Ye,Bowen Yu,Chujie Yuan,Linxiao Yuan,Sichun Zeng,Weihong Zeng,Xuejiao Zeng,Yan Zeng,Chuntao Zhang,Heng Zhang,Jingjie Zhang,Kuo Zhang,Liang Zhang,Liying Zhang,Manlin Zhang,Ting Zhang,Weida Zhang,Xiaohe Zhang,Xinyan Zhang,Yan Zhang,Yuan Zhang,Zixiang Zhang,Fengxuan Zhao,Huating Zhao,Yang Zhao,Hao Zheng,Jianbin Zheng,Xiaozheng Zheng,Yangyang Zheng,Yijie Zheng,Jiexin Zhou,Kuan Zhu,Shenhan Zhu,Wenjia Zhu,Benhui Zou,Feilong Zuo*

Main category: cs.CV

Relevance: 65.0

TL;DR: Seedance 1.5 pro是一个用于原生联合音频-视频生成的基础模型，采用双分支Diffusion Transformer架构，通过跨模态联合模块和多阶段数据管道实现卓越的视听同步和生成质量，并进行了SFT和RLHF优化，推理速度提升10倍以上。


<details>
  <summary>Details</summary>
Motivation: 视频生成的进展为统一的视听生成铺平了道路，但需要专门的基础模型来实现原生、联合的音频-视频生成，解决视听同步、生成质量和实际应用的问题。

Method: 采用双分支Diffusion Transformer架构，集成跨模态联合模块和专门的多阶段数据管道；进行监督微调(SFT)和基于人类反馈的强化学习(RLHF)优化；引入加速框架提升推理速度。

Result: 实现了卓越的视听同步和生成质量，支持精确的多语言和方言口型同步、动态电影摄像机控制、增强的叙事连贯性；推理速度提升超过10倍。

Conclusion: Seedance 1.5 pro作为一个强大的专业级内容创作引擎，在视听生成领域表现出色，现已可通过Volcano Engine平台访问。

Abstract: Recent strides in video generation have paved the way for unified audio-visual generation. In this work, we present Seedance 1.5 pro, a foundational model engineered specifically for native, joint audio-video generation. Leveraging a dual-branch Diffusion Transformer architecture, the model integrates a cross-modal joint module with a specialized multi-stage data pipeline, achieving exceptional audio-visual synchronization and superior generation quality. To ensure practical utility, we implement meticulous post-training optimizations, including Supervised Fine-Tuning (SFT) on high-quality datasets and Reinforcement Learning from Human Feedback (RLHF) with multi-dimensional reward models. Furthermore, we introduce an acceleration framework that boosts inference speed by over 10X. Seedance 1.5 pro distinguishes itself through precise multilingual and dialect lip-syncing, dynamic cinematic camera control, and enhanced narrative coherence, positioning it as a robust engine for professional-grade content creation. Seedance 1.5 pro is now accessible on Volcano Engine at https://console.volcengine.com/ark/region:ark+cn-beijing/experience/vision?type=GenVideo.

</details>


### [147] [TARA: Simple and Efficient Time Aware Retrieval Adaptation of MLLMs for Video Understanding](https://arxiv.org/abs/2512.13511)
*Piyush Bagad,Andrew Zisserman*

Main category: cs.CV

Relevance: 65.0

TL;DR: TARA是一种无需视频数据、将多模态大语言模型适配为时间感知视频-文本嵌入模型的方法，在时间感知检索基准上表现优异，同时具备否定感知和动词/副词理解能力。


<details>
  <summary>Details</summary>
Motivation: 构建通用的时间感知视频-文本嵌入模型用于检索，但现有模型在时间感知方面不足，且视频数据获取和标注成本高。

Method: 提出TARA方法，无需视频数据，通过适配多模态大语言模型来创建时间感知视频-文本嵌入模型。引入包含时间对立（手性）动作作为困难负样本的新基准进行评估。

Result: TARA在手性基准上优于所有现有视频-文本模型，在标准基准上也表现强劲。额外发现TARA具有否定感知能力，在动词和副词理解方面达到SOTA性能。

Conclusion: TARA产生了一个强大、多功能、时间感知的视频-文本嵌入模型，具有最先进的零样本性能，展示了无需视频数据即可实现时间感知检索的可行性。

Abstract: Our objective is to build a general time-aware video-text embedding model for retrieval. To that end, we propose a simple and efficient recipe, dubbed TARA (Time Aware Retrieval Adaptation), to adapt Multimodal LLMs (MLLMs) to a time-aware video-text embedding model without using any video data at all. For evaluating time-awareness in retrieval, we propose a new benchmark with temporally opposite (chiral) actions as hard negatives and curated splits for chiral and non-chiral actions. We show that TARA outperforms all existing video-text models on this chiral benchmark while also achieving strong results on standard benchmarks. Furthermore, we discover additional benefits of TARA beyond time-awareness: (i) TARA embeddings are negation-aware as shown in NegBench benchmark that evaluates negation in video retrieval, (ii) TARA achieves state of the art performance on verb and adverb understanding in videos. Overall, TARA yields a strong, versatile, time-aware video-text embedding model with state of the art zero-shot performance.

</details>


### [148] [LongVie 2: Multimodal Controllable Ultra-Long Video World Model](https://arxiv.org/abs/2512.13604)
*Jianxiong Gao,Zhaoxi Chen,Xian Liu,Junhao Zhuang,Chengming Xu,Jianfeng Feng,Yu Qiao,Yanwei Fu,Chenyang Si,Ziwei Liu*

Main category: cs.CV

Relevance: 65.0

TL;DR: LongVie 2是一个端到端自回归视频世界建模框架，通过三阶段训练增强可控性、长期视觉质量和时序一致性，支持长达5分钟的视频生成。


<details>
  <summary>Details</summary>
Motivation: 基于预训练视频生成系统构建视频世界模型是实现通用时空智能的重要但具有挑战性的步骤。世界模型需要具备三个关键属性：可控性、长期视觉质量和时序一致性。

Method: 采用渐进式方法：1）多模态引导，整合密集和稀疏控制信号提供隐式世界级监督；2）输入帧的退化感知训练，弥合训练与长期推理之间的差距；3）历史上下文引导，对齐相邻片段间的上下文信息以确保时序一致性。

Result: LongVie 2在长程可控性、时序一致性和视觉保真度方面达到最先进水平，支持长达5分钟的连续视频生成。同时提出了LongVGenBench基准，包含100个高分辨率一分钟视频。

Conclusion: 该工作标志着向统一视频世界建模迈出了重要一步，通过三阶段训练策略有效解决了视频世界模型的关键挑战。

Abstract: Building video world models upon pretrained video generation systems represents an important yet challenging step toward general spatiotemporal intelligence. A world model should possess three essential properties: controllability, long-term visual quality, and temporal consistency. To this end, we take a progressive approach-first enhancing controllability and then extending toward long-term, high-quality generation. We present LongVie 2, an end-to-end autoregressive framework trained in three stages: (1) Multi-modal guidance, which integrates dense and sparse control signals to provide implicit world-level supervision and improve controllability; (2) Degradation-aware training on the input frame, bridging the gap between training and long-term inference to maintain high visual quality; and (3) History-context guidance, which aligns contextual information across adjacent clips to ensure temporal consistency. We further introduce LongVGenBench, a comprehensive benchmark comprising 100 high-resolution one-minute videos covering diverse real-world and synthetic environments. Extensive experiments demonstrate that LongVie 2 achieves state-of-the-art performance in long-range controllability, temporal coherence, and visual fidelity, and supports continuous video generation lasting up to five minutes, marking a significant step toward unified video world modeling.

</details>


### [149] [DBT-DINO: Towards Foundation model based analysis of Digital Breast Tomosynthesis](https://arxiv.org/abs/2512.13608)
*Felix J. Dorfner,Manon A. Dorster,Ryan Connolly,Oscar Gentilhomme,Edward Gibbs,Steven Graham,Seth Wander,Thomas Schultz,Manisha Bahl,Dania Daye,Albert E. Kim,Christopher P. Bridge*

Main category: cs.CV

Relevance: 65.0

TL;DR: 开发了首个数字乳腺断层合成（DBT）基础模型DBT-DINO，在乳腺密度分类和癌症风险预测任务上表现优于通用基础模型，但在病灶检测任务上表现不一。


<details>
  <summary>Details</summary>
Motivation: 尽管基础模型在医学影像领域显示出潜力，但针对三维成像模态（如数字乳腺断层合成DBT）的基础模型仍未被充分探索。DBT用于乳腺癌筛查，但目前尚无专门的基础模型。

Method: 采用DINOv2方法进行自监督预训练，使用来自27,990名患者的487,975个DBT体积中的超过2500万个2D切片。评估了三个下游任务：乳腺密度分类、5年乳腺癌风险预测和病灶检测。

Result: 在乳腺密度分类任务中，DBT-DINO准确率为0.79，显著优于MetaAI DINOv2基线（0.73）和DenseNet-121（0.74）。在5年乳腺癌风险预测中，DBT-DINO的AUROC为0.78，与DINOv2的0.76相比无显著差异。在病灶检测任务中，DINOv2的平均灵敏度为0.67，高于DBT-DINO的0.62，但DBT-DINO在癌症性病灶检测上表现更好（78.8% vs 77.3%）。

Conclusion: DBT-DINO是首个DBT基础模型，在乳腺密度分类和癌症风险预测任务上表现出色。然而，领域特定预训练在检测任务上的益处不一，表明局部检测任务需要进一步的方法学开发。

Abstract: Foundation models have shown promise in medical imaging but remain underexplored for three-dimensional imaging modalities. No foundation model currently exists for Digital Breast Tomosynthesis (DBT), despite its use for breast cancer screening.
  To develop and evaluate a foundation model for DBT (DBT-DINO) across multiple clinical tasks and assess the impact of domain-specific pre-training.
  Self-supervised pre-training was performed using the DINOv2 methodology on over 25 million 2D slices from 487,975 DBT volumes from 27,990 patients. Three downstream tasks were evaluated: (1) breast density classification using 5,000 screening exams; (2) 5-year risk of developing breast cancer using 106,417 screening exams; and (3) lesion detection using 393 annotated volumes.
  For breast density classification, DBT-DINO achieved an accuracy of 0.79 (95\% CI: 0.76--0.81), outperforming both the MetaAI DINOv2 baseline (0.73, 95\% CI: 0.70--0.76, p<.001) and DenseNet-121 (0.74, 95\% CI: 0.71--0.76, p<.001). For 5-year breast cancer risk prediction, DBT-DINO achieved an AUROC of 0.78 (95\% CI: 0.76--0.80) compared to DINOv2's 0.76 (95\% CI: 0.74--0.78, p=.57). For lesion detection, DINOv2 achieved a higher average sensitivity of 0.67 (95\% CI: 0.60--0.74) compared to DBT-DINO with 0.62 (95\% CI: 0.53--0.71, p=.60). DBT-DINO demonstrated better performance on cancerous lesions specifically with a detection rate of 78.8\% compared to Dinov2's 77.3\%.
  Using a dataset of unprecedented size, we developed DBT-DINO, the first foundation model for DBT. DBT-DINO demonstrated strong performance on breast density classification and cancer risk prediction. However, domain-specific pre-training showed variable benefits on the detection task, with ImageNet baseline outperforming DBT-DINO on general lesion detection, indicating that localized detection tasks require further methodological development.

</details>


### [150] [I-Scene: 3D Instance Models are Implicit Generalizable Spatial Learners](https://arxiv.org/abs/2512.13683)
*Lu Ling,Yunhao Ge,Yichen Sheng,Aniket Bera*

Main category: cs.CV

Relevance: 65.0

TL;DR: 该论文提出了一种新方法，通过重新编程预训练的3D实例生成器来学习场景级空间关系，从而实现对新布局和物体组合的泛化，而不依赖于有限的场景数据集监督。


<details>
  <summary>Details</summary>
Motivation: 现有基于学习的3D场景生成方法依赖于有限的场景数据集进行空间理解，这限制了它们对新布局的泛化能力。作者希望利用预训练3D实例生成器中已经存在的可转移空间知识，而不是依赖数据集监督。

Method: 重新编程预训练的3D实例生成器，使其成为场景级学习器，用模型中心的空间监督替代数据集监督。采用视图中心的场景空间表示，而不是传统的规范空间，创建了一个完全前馈、可泛化的场景生成器，直接从实例模型中学习空间关系。

Result: 该方法能够泛化到未见过的布局和新颖的物体组合。即使在训练场景由随机组合的物体构成时，空间推理能力仍然能够涌现。定量和定性结果表明，3D实例生成器可以作为隐式的空间学习器和推理器。

Conclusion: 预训练的3D实例生成器包含了丰富的可转移空间先验知识，能够从纯几何线索中推断接近性、支撑和对称关系。这为交互式3D场景理解和生成的基础模型指明了方向。

Abstract: Generalization remains the central challenge for interactive 3D scene generation. Existing learning-based approaches ground spatial understanding in limited scene dataset, restricting generalization to new layouts. We instead reprogram a pre-trained 3D instance generator to act as a scene level learner, replacing dataset-bounded supervision with model-centric spatial supervision. This reprogramming unlocks the generator transferable spatial knowledge, enabling generalization to unseen layouts and novel object compositions. Remarkably, spatial reasoning still emerges even when the training scenes are randomly composed objects. This demonstrates that the generator's transferable scene prior provides a rich learning signal for inferring proximity, support, and symmetry from purely geometric cues. Replacing widely used canonical space, we instantiate this insight with a view-centric formulation of the scene space, yielding a fully feed-forward, generalizable scene generator that learns spatial relations directly from the instance model. Quantitative and qualitative results show that a 3D instance generator is an implicit spatial learner and reasoner, pointing toward foundation models for interactive 3D scene understanding and generation. Project page: https://luling06.github.io/I-Scene-project/

</details>


### [151] [Hybrid Retrieval-Augmented Generation for Robust Multilingual Document Question Answering](https://arxiv.org/abs/2512.12694)
*Anthony Mudet,Souhail Bakkali*

Main category: cs.DL

Relevance: 65.0

TL;DR: 本文提出一个针对噪声历史文档的多语言检索增强生成（RAG）管道，通过语义查询扩展、多查询融合和严格证据基础的生成策略，解决OCR错误、多语言拼写变异和时间语言漂移问题。


<details>
  <summary>Details</summary>
Motivation: 大规模数字化项目产生了大量历史报纸收藏，但OCR错误、多语言拼写变异和时间语言漂移阻碍了有效的计算访问。需要专门针对噪声历史文档的问答系统。

Method: 1) 使用语义查询扩展和互惠排名融合（RRF）的多查询融合提高检索鲁棒性；2) 精心设计的生成提示，强制严格基于检索证据并明确拒绝证据不足的问题；3) 模块化架构支持系统组件评估。

Result: 管道对有充分证据的查询生成忠实答案，对不可回答的问题正确拒绝。混合检索策略提高了召回稳定性，特别是RRF能够平滑不同查询表述的性能方差。消融研究显示实体提取中句法连贯性的重要性以及密集检索中的平衡性能-效率权衡。

Conclusion: 该多语言RAG管道为噪声历史文档问答提供了稳健解决方案，通过系统组件设计和评估框架，为历史文档分析提供了可复现的基础。

Abstract: Large-scale digitization initiatives have unlocked massive collections of historical newspapers, yet effective computational access remains hindered by OCR corruption, multilingual orthographic variation, and temporal language drift. We develop and evaluate a multilingual Retrieval-Augmented Generation pipeline specifically designed for question answering on noisy historical documents. Our approach integrates: (i) semantic query expansion and multi-query fusion using Reciprocal Rank Fusion to improve retrieval robustness against vocabulary mismatch; (ii) a carefully engineered generation prompt that enforces strict grounding in retrieved evidence and explicit abstention when evidence is insufficient; and (iii) a modular architecture enabling systematic component evaluation. We conduct comprehensive ablation studies on Named Entity Recognition and embedding model selection, demonstrating the importance of syntactic coherence in entity extraction and balanced performance-efficiency trade-offs in dense retrieval. Our end-to-end evaluation framework shows that the pipeline generates faithful answers for well-supported queries while correctly abstaining from unanswerable questions. The hybrid retrieval strategy improves recall stability, particularly benefiting from RRF's ability to smooth performance variance across query formulations. We release our code and configurations at https://anonymous.4open.science/r/RAGs-C5AE/, providing a reproducible foundation for robust historical document question answering.

</details>


### [152] [Post-Training and Test-Time Scaling of Generative Agent Behavior Models for Interactive Autonomous Driving](https://arxiv.org/abs/2512.13262)
*Hyunki Seong,Jeong-Kyun Lee,Heesoo Myeong,Yongho Shin,Hyun-Mook Cho,Duck Hoon Kim,Pranav Desai,Monu Surana*

Main category: cs.RO

Relevance: 65.0

TL;DR: 提出GRBO强化学习后训练方法和Warm-K采样策略，用于提升多智能体交互运动行为的安全性和一致性


<details>
  <summary>Details</summary>
Motivation: 自动驾驶中多智能体交互运动行为学习存在挑战：模仿学习模型从数据集中继承安全偏差，在安全关键场景中鲁棒性不足；现有研究多采用开环评估，忽视了闭环执行中的误差累积问题

Method: 1. GRBO：基于群体相对优势最大化的强化学习后训练方法，通过人类正则化微调预训练行为模型；2. Warm-K：基于热启动的Top-K采样策略，在测试时平衡一致性和多样性

Result: GRBO仅使用10%训练数据即可将安全性能提升40%以上，同时保持行为真实性；Warm-K方法在不重新训练的情况下增强行为一致性和反应性，缓解协变量偏移

Conclusion: 提出的两种互补策略有效解决了自动驾驶中多智能体交互运动行为的安全性和一致性挑战，GRBO提升安全性能，Warm-K改善测试时行为表现

Abstract: Learning interactive motion behaviors among multiple agents is a core challenge in autonomous driving. While imitation learning models generate realistic trajectories, they often inherit biases from datasets dominated by safe demonstrations, limiting robustness in safety-critical cases. Moreover, most studies rely on open-loop evaluation, overlooking compounding errors in closed-loop execution. We address these limitations with two complementary strategies. First, we propose Group Relative Behavior Optimization (GRBO), a reinforcement learning post-training method that fine-tunes pretrained behavior models via group relative advantage maximization with human regularization. Using only 10% of the training dataset, GRBO improves safety performance by over 40% while preserving behavioral realism. Second, we introduce Warm-K, a warm-started Top-K sampling strategy that balances consistency and diversity in motion selection. Our Warm-K method-based test-time scaling enhances behavioral consistency and reactivity at test time without retraining, mitigating covariate shift and reducing performance discrepancies. Demo videos are available in the supplementary material.

</details>


### [153] [RoboTracer: Mastering Spatial Trace with Reasoning in Vision-Language Models for Robotics](https://arxiv.org/abs/2512.13660)
*Enshen Zhou,Cheng Chi,Yibo Li,Jingkun An,Jiayuan Zhang,Shanyu Rong,Yi Han,Yuheng Ji,Mengzhen Liu,Pengwei Wang,Zhongyuan Wang,Lu Sheng,Shanghang Zhang*

Main category: cs.RO

Relevance: 65.0

TL;DR: RoboTracer：首个实现3D空间追踪的视觉语言模型，通过空间编码器和回归监督解码器增强尺度感知，结合强化微调实现多步度量推理，在空间追踪任务上显著超越现有方法。


<details>
  <summary>Details</summary>
Motivation: 空间追踪是机器人基础交互能力，需要多步度量推理与复杂空间指代结合，现有方法难以处理这种组合任务。需要开发能同时处理3D空间指代和度量的模型。

Method: 1) 提出3D感知VLM RoboTracer，使用通用空间编码器和回归监督解码器增强尺度感知；2) 通过强化微调(RFT)结合度量敏感过程奖励，监督中间感知线索；3) 构建TraceSpatial数据集(30M QA对)和TraceSpatial-Bench基准。

Result: RoboTracer在空间理解、度量和指代上超越基线，平均成功率79.1%；在TraceSpatial-Bench上大幅领先，比Gemini-2.5-Pro高36%准确率；可集成多种控制策略，在真实杂乱场景中执行长时程动态任务。

Conclusion: RoboTracer首次实现3D空间追踪，通过SFT和RFT结合的方法有效处理多步度量推理，在空间理解任务上达到SOTA，展示了在多样化机器人平台上的实际应用潜力。

Abstract: Spatial tracing, as a fundamental embodied interaction ability for robots, is inherently challenging as it requires multi-step metric-grounded reasoning compounded with complex spatial referring and real-world metric measurement. However, existing methods struggle with this compositional task. To this end, we propose RoboTracer, a 3D-aware VLM that first achieves both 3D spatial referring and measuring via a universal spatial encoder and a regression-supervised decoder to enhance scale awareness during supervised fine-tuning (SFT). Moreover, RoboTracer advances multi-step metric-grounded reasoning via reinforcement fine-tuning (RFT) with metric-sensitive process rewards, supervising key intermediate perceptual cues to accurately generate spatial traces. To support SFT and RFT training, we introduce TraceSpatial, a large-scale dataset of 30M QA pairs, spanning outdoor/indoor/tabletop scenes and supporting complex reasoning processes (up to 9 steps). We further present TraceSpatial-Bench, a challenging benchmark filling the gap to evaluate spatial tracing. Experimental results show that RoboTracer surpasses baselines in spatial understanding, measuring, and referring, with an average success rate of 79.1%, and also achieves SOTA performance on TraceSpatial-Bench by a large margin, exceeding Gemini-2.5-Pro by 36% accuracy. Notably, RoboTracer can be integrated with various control policies to execute long-horizon, dynamic tasks across diverse robots (UR5, G1 humanoid) in cluttered real-world scenes.

</details>


### [154] [CLARGA: Multimodal Graph Representation Learning over Arbitrary Sets of Modalities](https://arxiv.org/abs/2512.11901)
*Santosh Patapati*

Main category: cs.CV

Relevance: 45.0

TL;DR: CLARGA是一种通用的多模态融合架构，可处理任意数量和类型的模态，通过注意力加权图和图注意力网络实现高效融合，具有亚二次复杂度，支持缺失模态处理。


<details>
  <summary>Details</summary>
Motivation: 现有多模态融合方法通常针对特定模态组合设计，缺乏通用性。需要一种能够处理任意模态、适应不同样本、高效且能处理缺失模态的通用融合框架。

Method: CLARGA为每个样本构建注意力加权图，使用多头图注意力网络在图上传递消息。通过可学习掩码适应缺失模态输入。采用监督任务损失和对比InfoNCE损失的混合目标训练。

Result: 在金融、人机交互、多媒体分类、情感计算等7个数据集上，CLARGA持续超越基线、SOTA模型和消融实验。展示了缺失输入鲁棒性和在小众任务上的优异表现。

Conclusion: CLARGA是一种有效且高效的多模态表示学习架构，可轻松集成到机器学习模型中，适用于广泛任务。

Abstract: We introduce CLARGA, a general-purpose multimodal fusion architecture for multimodal representation learning that works with any number and type of modalities without changing the underlying framework. Given a supervised dataset, CLARGA can be applied to virtually any machine learning task to fuse different multimodal representations for processing by downstream layers. On a sample-by-sample basis, CLARGA learns how modalities should inform one another by building an attention weighted graph over their features and passing messages along this graph with a multi-head Graph Attention Network. Not only does this make CLARGA highly adaptive, as it constructs unique graphs for different samples, it makes for efficient fusion with sub-quadratic complexity as the number of modalities grows. Through a learnable mask, it can also adapt to missing modality inputs. The model is trained with a hybrid objective that combines a supervised task loss with contrastive InfoNCE loss, improving cross-modal consistency and robustness to noisy inputs. We demonstrate CLARGA's effectiveness in diverse multimodal representation learning tasks across 7 datasets spanning finance, human-computer interaction, general multimedia classification, and affective computing. It consistently outperforms baselines, state-of-the-art models, and ablations. Additional experiments also demonstrate its robustness to missing inputs and ability to excel on niche tasks. Overall, CLARGA can be easily plugged into machine learning models for effective and efficient learning of representations across a wide variety of tasks.

</details>


### [155] [CreativeVR: Diffusion-Prior-Guided Approach for Structure and Motion Restoration in Generative and Real Videos](https://arxiv.org/abs/2512.12060)
*Tejas Panambur,Ishan Rajendrakumar Dave,Chongjian Ge,Ersin Yumer,Xue Bai*

Main category: cs.CV

Relevance: 45.0

TL;DR: CreativeVR：针对AI生成视频和真实视频中严重结构/时序伪影的扩散先验引导视频修复框架，通过单一精度旋钮在精确修复和结构校正间平滑权衡


<details>
  <summary>Details</summary>
Motivation: 现有文本到视频扩散模型在精细结构上仍存在问题（扭曲的面部/手部、扭曲背景、时序不一致运动），类似问题也出现在低质量真实视频中。传统视频修复方法针对合成退化设计，扩散先验修复器主要处理光度噪声，缺乏对感知质量与保真度权衡的控制。

Method: 提出基于深度适配器的扩散先验引导视频修复框架，核心创新是训练时使用的时序一致退化模块，该模块应用精心设计的变换来产生真实的结构故障。通过单一精度旋钮控制模型对输入的遵循程度。

Result: 在严重伪影视频上达到SOTA，在标准视频修复基准上具有竞争力，实际吞吐量约13 FPS（720p，单A100 80GB）。提出AIGC54基准用于AI生成内容伪影修复评估。

Conclusion: CreativeVR能有效修复AI生成视频和真实视频中的严重结构及时序伪影，在感知质量与保真度间提供可控权衡，具有实用性能。

Abstract: Modern text-to-video (T2V) diffusion models can synthesize visually compelling clips, yet they remain brittle at fine-scale structure: even state-of-the-art generators often produce distorted faces and hands, warped backgrounds, and temporally inconsistent motion. Such severe structural artifacts also appear in very low-quality real-world videos. Classical video restoration and super-resolution (VR/VSR) methods, in contrast, are tuned for synthetic degradations such as blur and downsampling and tend to stabilize these artifacts rather than repair them, while diffusion-prior restorers are usually trained on photometric noise and offer little control over the trade-off between perceptual quality and fidelity.
  We introduce CreativeVR, a diffusion-prior-guided video restoration framework for AI-generated (AIGC) and real videos with severe structural and temporal artifacts. Our deep-adapter-based method exposes a single precision knob that controls how strongly the model follows the input, smoothly trading off between precise restoration on standard degradations and stronger structure- and motion-corrective behavior on challenging content. Our key novelty is a temporally coherent degradation module used during training, which applies carefully designed transformations that produce realistic structural failures.
  To evaluate AIGC-artifact restoration, we propose the AIGC54 benchmark with FIQA, semantic and perceptual metrics, and multi-aspect scoring. CreativeVR achieves state-of-the-art results on videos with severe artifacts and performs competitively on standard video restoration benchmarks, while running at practical throughput (about 13 FPS at 720p on a single 80-GB A100). Project page: https://daveishan.github.io/creativevr-webpage/.

</details>


### [156] [CineLOG: A Training Free Approach for Cinematic Long Video Generation](https://arxiv.org/abs/2512.12209)
*Zahra Dehghanian,Morteza Abolghasemi,Hamid Beigy,Hamid R. Rabiee*

Main category: cs.CV

Relevance: 45.0

TL;DR: CineLOG：一个用于可控视频合成的5,000个高质量视频剪辑数据集，包含详细的场景描述、基于标准电影分类学的明确摄像机指令和类型标签，覆盖17种摄像机运动和15种电影类型。


<details>
  <summary>Details</summary>
Motivation: 当前视频合成模型在超越文本提示的细粒度控制方面存在困难，特别是在摄像机轨迹和电影类型等电影属性方面。现有数据集通常存在严重的数据不平衡、噪声标签或显著的模拟到真实差距。

Method: 提出了CineLOG数据集和新的生成流程，将复杂的文本到视频生成任务解耦为四个更简单的阶段，并引入了轨迹引导过渡模块来生成平滑的时空插值，实现连贯的多镜头序列。

Result: 广泛的人类评估显示，该流程在遵循特定摄像机和剧本指令方面显著优于最先进的端到端文本到视频模型，同时保持专业的视觉质量。

Conclusion: CineLOG数据集和提出的流程为可控视频合成提供了高质量、平衡的数据集和有效的生成方法，解决了现有模型在细粒度控制方面的局限性。

Abstract: Controllable video synthesis is a central challenge in computer vision, yet current models struggle with fine grained control beyond textual prompts, particularly for cinematic attributes like camera trajectory and genre. Existing datasets often suffer from severe data imbalance, noisy labels, or a significant simulation to real gap. To address this, we introduce CineLOG, a new dataset of 5,000 high quality, balanced, and uncut video clips. Each entry is annotated with a detailed scene description, explicit camera instructions based on a standard cinematic taxonomy, and genre label, ensuring balanced coverage across 17 diverse camera movements and 15 film genres. We also present our novel pipeline designed to create this dataset, which decouples the complex text to video (T2V) generation task into four easier stages with more mature technology. To enable coherent, multi shot sequences, we introduce a novel Trajectory Guided Transition Module that generates smooth spatio-temporal interpolation. Extensive human evaluations show that our pipeline significantly outperforms SOTA end to end T2V models in adhering to specific camera and screenplay instructions, while maintaining professional visual quality. All codes and data are available at https://cine-log.pages.dev.

</details>


### [157] [WeDetect: Fast Open-Vocabulary Object Detection as Retrieval](https://arxiv.org/abs/2512.12309)
*Shenghao Fu,Yukun Su,Fengyun Rao,Jing Lyu,Xiaohua Xie,Wei-Shi Zheng*

Main category: cs.CV

Relevance: 45.0

TL;DR: WeDetect是一个基于检索范式的开放词汇目标检测模型家族，采用双塔架构实现实时检测，无需跨模态融合层，通过共享嵌入空间匹配区域和文本查询，在15个基准测试中达到SOTA性能。


<details>
  <summary>Details</summary>
Motivation: 探索非融合方法的检索范式在开放词汇目标检测中的独特优势，包括高效推理、多功能应用（检测、提案生成、对象检索、指代表达理解），并建立强大的开放词汇基础。

Method: 提出WeDetect模型家族：1) WeDetect：双塔架构的实时检测器，通过精心策划的数据和完整训练实现非融合检测；2) WeDetect-Uni：基于WeDetect的通用提案生成器，冻结检测器仅微调对象性提示；3) WeDetect-Ref：基于LMM的对象分类器，处理复杂指代表达，从WeDetect-Uni提取的提案列表中检索目标对象。

Result: 在15个基准测试中达到最先进性能，实现实时检测，支持历史数据快速回溯和对象检索，高效处理指代表达理解任务，统一了检测、提案生成、对象检索和REC在一个连贯的检索框架下。

Conclusion: 检索范式在开放词汇目标检测中具有显著优势，WeDetect模型家族证明了非融合方法在效率和多功能性方面的潜力，为开放词汇检测建立了强大的基础，并展示了与LMM集成的可能性。

Abstract: Open-vocabulary object detection aims to detect arbitrary classes via text prompts. Methods without cross-modal fusion layers (non-fusion) offer faster inference by treating recognition as a retrieval problem, \ie, matching regions to text queries in a shared embedding space. In this work, we fully explore this retrieval philosophy and demonstrate its unique advantages in efficiency and versatility through a model family named WeDetect: (1) State-of-the-art performance. WeDetect is a real-time detector with a dual-tower architecture. We show that, with well-curated data and full training, the non-fusion WeDetect surpasses other fusion models and establishes a strong open-vocabulary foundation. (2) Fast backtrack of historical data. WeDetect-Uni is a universal proposal generator based on WeDetect. We freeze the entire detector and only finetune an objectness prompt to retrieve generic object proposals across categories. Importantly, the proposal embeddings are class-specific and enable a new application, object retrieval, supporting retrieval objects in historical data. (3) Integration with LMMs for referring expression comprehension (REC). We further propose WeDetect-Ref, an LMM-based object classifier to handle complex referring expressions, which retrieves target objects from the proposal list extracted by WeDetect-Uni. It discards next-token prediction and classifies objects in a single forward pass. Together, the WeDetect family unifies detection, proposal generation, object retrieval, and REC under a coherent retrieval framework, achieving state-of-the-art performance across 15 benchmarks with high inference efficiency.

</details>


### [158] [STAGE: Storyboard-Anchored Generation for Cinematic Multi-shot Narrative](https://arxiv.org/abs/2512.12372)
*Peixuan Zhang,Zijian Jia,Kaiqi Liu,Shuchen Weng,Si Li,Boxin Shi*

Main category: cs.CV

Relevance: 45.0

TL;DR: STAGE提出了一种基于故事板的多镜头视频生成工作流，通过预测每个镜头的起止帧对来构建结构化故事板，确保跨镜头一致性和电影语言表达。


<details>
  <summary>Details</summary>
Motivation: 当前生成模型在视频合成中虽然视觉保真度高，但难以创建连贯的多镜头叙事。基于关键帧的方法虽然提供了细粒度控制和效率，但往往无法保持跨镜头一致性和捕捉电影语言。

Method: 1) STEP2：预测每个镜头的起止帧对构建结构化故事板；2) 多镜头记忆包：确保长程实体一致性；3) 双重编码策略：保证镜头内连贯性；4) 两阶段训练方案：学习电影化的镜头间过渡；5) 贡献了大规模ConStoryBoard数据集。

Result: 大量实验表明，STAGE在结构化叙事控制和跨镜头连贯性方面实现了优越性能。

Conclusion: STAGE通过故事板锚定的生成工作流，有效解决了多镜头视频生成中的跨镜头一致性和电影语言表达问题，为结构化叙事控制提供了新方法。

Abstract: While recent advancements in generative models have achieved remarkable visual fidelity in video synthesis, creating coherent multi-shot narratives remains a significant challenge. To address this, keyframe-based approaches have emerged as a promising alternative to computationally intensive end-to-end methods, offering the advantages of fine-grained control and greater efficiency. However, these methods often fail to maintain cross-shot consistency and capture cinematic language. In this paper, we introduce STAGE, a SToryboard-Anchored GEneration workflow to reformulate the keyframe-based multi-shot video generation task. Instead of using sparse keyframes, we propose STEP2 to predict a structural storyboard composed of start-end frame pairs for each shot. We introduce the multi-shot memory pack to ensure long-range entity consistency, the dual-encoding strategy for intra-shot coherence, and the two-stage training scheme to learn cinematic inter-shot transition. We also contribute the large-scale ConStoryBoard dataset, including high-quality movie clips with fine-grained annotations for story progression, cinematic attributes, and human preferences. Extensive experiments demonstrate that STAGE achieves superior performance in structured narrative control and cross-shot coherence.

</details>


### [159] [From Tokens to Photons: Test-Time Physical Prompting for Vison-Language Models](https://arxiv.org/abs/2512.12571)
*Boyeong Im,Wooseok Lee,Yoojin Kwon,Hyung-Sin Kim*

Main category: cs.CV

Relevance: 45.0

TL;DR: MVP提出了一种将相机曝光三角（ISO、快门速度、光圈）作为物理提示的测试时适应框架，通过选择最佳传感器设置和聚合多视图预测来提升视觉语言模型在物理环境中的鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 现有视觉语言模型主要针对网络图像设计，缺乏在传感器介导的物理环境中的适应性。传统测试时适应方法仅关注数字增强，忽略了测量时物理控制的重要性。

Method: MVP框架在推理时：1) 获取每个场景的物理视图库；2) 使用源亲和度分数选择top-k传感器设置；3) 在轻量级数字增强下评估每个保留视图；4) 过滤最低熵的增强视图子集；5) 使用零温度softmax（硬投票）聚合预测。无需梯度或模型修改。

Result: 在ImageNet-ES和ImageNet-ES-Diverse上，MVP比仅数字TTA在单自动曝光捕获上提升高达25.6个百分点，比结合传统传感器控制与TTA的流程额外提升3.4个百分点。即使在减少参数候选集降低捕获延迟时仍保持有效。

Conclusion: 测量时控制（选择和组合真实物理视图）显著提升了VLMs的鲁棒性，超越了仅后捕获提示的方法。MVP框架简单、校准友好，无需梯度或模型修改，具有实际应用价值。

Abstract: To extend the application of vision-language models (VLMs) from web images to sensor-mediated physical environments, we propose Multi-View Physical-prompt for Test-Time Adaptation (MVP), a forward-only framework that moves test-time adaptation (TTA) from tokens to photons by treating the camera exposure triangle--ISO, shutter speed, and aperture--as physical prompts. At inference, MVP acquires a library of physical views per scene, selects the top-k sensor settings using a source-affinity score, evaluates each retained view under lightweight digital augmentations, filters the lowest-entropy subset of augmented views, and aggregates predictions with Zero-temperature softmax (i.e., hard voting). This selection-then-vote design is simple, calibration-friendly, and requires no gradients or model modifications. On ImageNet-ES and ImageNet-ES-Diverse, MVP consistently outperforms digital-only TTA on single Auto-Exposure captures, by up to 25.6 percentage points (pp), and delivers up to 3.4 pp additional gains over pipelines that combine conventional sensor control with TTA. MVP remains effective under reduced parameter candidate sets that lower capture latency, demonstrating practicality. These results support the main claim that, beyond post-capture prompting, measurement-time control--selecting and combining real physical views--substantially improves robustness for VLMs.

</details>


### [160] [Towards Interactive Intelligence for Digital Humans](https://arxiv.org/abs/2512.13674)
*Yiyi Cai,Xuangeng Chu,Xiwei Gao,Sitong Gong,Yifei Huang,Caixin Kang,Kunhang Li,Haiyang Liu,Ruicong Liu,Yun Liu,Dianwen Ng,Zixiong Su,Erwin Wu,Yuhan Wu,Dingkun Yan,Tianyu Yan,Chang Zeng,Bo Zheng,You Zhou*

Main category: cs.CV

Relevance: 45.0

TL;DR: Mio是一个端到端的多模态交互数字人框架，包含五个专门模块，能够实现个性对齐表达、自适应交互和自我进化，在交互智能基准测试中表现优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 当前数字人主要停留在表面模仿阶段，缺乏真正的智能交互能力。作者希望推动数字人从简单模仿向智能交互发展，实现个性对齐表达、自适应交互和自我进化。

Method: 提出Mio（多模态交互全息化身）框架，包含五个专门模块：思考者（认知推理）、说话者（语音生成）、面部动画师、身体动画师和渲染器。这是一个统一的端到端架构，将认知推理与实时多模态具身化相结合。

Result: 建立了新的交互智能基准测试，并通过大量实验证明Mio框架在所有评估维度上都优于最先进的方法。

Conclusion: Mio框架将数字人从表面模仿推向智能交互，为实现真正交互智能的数字人提供了可行的技术路径。

Abstract: We introduce Interactive Intelligence, a novel paradigm of digital human that is capable of personality-aligned expression, adaptive interaction, and self-evolution. To realize this, we present Mio (Multimodal Interactive Omni-Avatar), an end-to-end framework composed of five specialized modules: Thinker, Talker, Face Animator, Body Animator, and Renderer. This unified architecture integrates cognitive reasoning with real-time multimodal embodiment to enable fluid, consistent interaction. Furthermore, we establish a new benchmark to rigorously evaluate the capabilities of interactive intelligence. Extensive experiments demonstrate that our framework achieves superior performance compared to state-of-the-art methods across all evaluated dimensions. Together, these contributions move digital humans beyond superficial imitation toward intelligent interaction.

</details>


### [161] [TWLR: Text-Guided Weakly-Supervised Lesion Localization and Severity Regression for Explainable Diabetic Retinopathy Grading](https://arxiv.org/abs/2512.13008)
*Xi Luo,Shixin Xu,Ying Xie,JianZhong Hu,Yuwei He,Yuhui Deng,Huaxiong Huang*

Main category: cs.CV

Relevance: 45.0

TL;DR: TWLR是一个用于糖尿病视网膜病变评估的两阶段可解释框架，第一阶段使用视觉语言模型进行疾病分级和病变分类，第二阶段通过弱监督语义分割进行迭代严重性回归，实现无需像素级标注的病变定位和可解释的可视化。


<details>
  <summary>Details</summary>
Motivation: 医学图像分析需要高质量专家标注，但获取像素级标签（特别是眼底图像）成本高、耗时长。深度学习在医学成像中缺乏可解释性，限制了临床采用。需要解决标注效率和可解释性双重挑战。

Method: 两阶段框架：1) 视觉语言模型整合眼科领域知识，进行DR分级和病变分类，将语义医学概念与视觉特征关联；2) 基于弱监督语义分割的迭代严重性回归框架，通过迭代细化生成病变显著性图，指导渐进修复机制，系统消除病理特征，将疾病严重性降级至更健康状态。

Result: 在FGADR、DDR和私有数据集上的实验表明，TWLR在DR分类和病变分割方面均取得竞争性性能，为自动化视网膜图像分析提供了更可解释、标注效率更高的解决方案。

Conclusion: TWLR框架通过结合视觉语言模型和迭代严重性回归，实现了无需像素级监督的准确病变定位，并提供疾病到健康转变的可解释可视化，解决了医学图像分析中的标注成本和可解释性挑战。

Abstract: Accurate medical image analysis can greatly assist clinical diagnosis, but its effectiveness relies on high-quality expert annotations Obtaining pixel-level labels for medical images, particularly fundus images, remains costly and time-consuming. Meanwhile, despite the success of deep learning in medical imaging, the lack of interpretability limits its clinical adoption. To address these challenges, we propose TWLR, a two-stage framework for interpretable diabetic retinopathy (DR) assessment. In the first stage, a vision-language model integrates domain-specific ophthalmological knowledge into text embeddings to jointly perform DR grading and lesion classification, effectively linking semantic medical concepts with visual features. The second stage introduces an iterative severity regression framework based on weakly-supervised semantic segmentation. Lesion saliency maps generated through iterative refinement direct a progressive inpainting mechanism that systematically eliminates pathological features, effectively downgrading disease severity toward healthier fundus appearances. Critically, this severity regression approach achieves dual benefits: accurate lesion localization without pixel-level supervision and providing an interpretable visualization of disease-to-healthy transformations. Experimental results on the FGADR, DDR, and a private dataset demonstrate that TWLR achieves competitive performance in both DR classification and lesion segmentation, offering a more explainable and annotation-efficient solution for automated retinal image analysis.

</details>


### [162] [UniVCD: A New Method for Unsupervised Change Detection in the Open-Vocabulary Era](https://arxiv.org/abs/2512.13089)
*Ziqiang Zhu,Bowei Yang*

Main category: cs.CV

Relevance: 45.0

TL;DR: UniVCD提出了一种无监督、开放词汇的变化检测方法，利用冻结的SAM2和CLIP模型，无需标注数据即可实现跨场景的类别无关变化检测。


<details>
  <summary>Details</summary>
Motivation: 现有变化检测方法主要依赖监督学习，性能受数据集限制，标注成本高，且只能检测预定义类别，泛化能力差。随着视觉基础模型（SAM2、CLIP）的出现，为突破这些限制提供了新机会。

Method: 基于冻结的SAM2和CLIP构建无监督开放词汇变化检测框架。引入轻量级特征对齐模块，将SAM2的空间细节表示与CLIP的语义先验结合，实现高分辨率语义感知变化估计。采用精简后处理流程抑制噪声和伪变化。

Result: 在多个公开的BCD和SCD基准测试中，UniVCD表现出稳定强大的性能，在F1和IoU等关键指标上匹配或超越现有开放词汇变化检测方法。

Conclusion: 使用冻结视觉基础模型和轻量级多模态对齐的无监督变化检测是开放词汇变化检测的实用有效范式。

Abstract: Change detection (CD) identifies scene changes from multi-temporal observations and is widely used in urban development and environmental monitoring. Most existing CD methods rely on supervised learning, making performance strongly dataset-dependent and incurring high annotation costs; they typically focus on a few predefined categories and generalize poorly to diverse scenes. With the rise of vision foundation models such as SAM2 and CLIP, new opportunities have emerged to relax these constraints. We propose Unified Open-Vocabulary Change Detection (UniVCD), an unsupervised, open-vocabulary change detection method built on frozen SAM2 and CLIP. UniVCD detects category-agnostic changes across diverse scenes and imaging geometries without any labeled data or paired change images. A lightweight feature alignment module is introduced to bridge the spatially detailed representations from SAM2 and the semantic priors from CLIP, enabling high-resolution, semantically aware change estimation while keeping the number of trainable parameters small. On top of this, a streamlined post-processing pipeline is further introduced to suppress noise and pseudo-changes, improving the detection accuracy for objects with well-defined boundaries. Experiments on several public BCD (Binary Change Detection) and SCD (Semantic Change Detection) benchmarks show that UniVCD achieves consistently strong performance and matches or surpasses existing open-vocabulary CD methods in key metrics such as F1 and IoU. The results demonstrate that unsupervised change detection with frozen vision foundation models and lightweight multi-modal alignment is a practical and effective paradigm for open-vocabulary CD. Code and pretrained models will be released at https://github.com/Die-Xie/UniVCD.

</details>


### [163] [Ego-EXTRA: video-language Egocentric Dataset for EXpert-TRAinee assistance](https://arxiv.org/abs/2512.13238)
*Francesco Ragusa,Michele Mazzamuto,Rosario Forte,Irene D'Ambra,James Fort,Jakob Engel,Antonino Furnari,Giovanni Maria Farinella*

Main category: cs.CV

Relevance: 45.0

TL;DR: Ego-EXTRA是一个50小时的视频-语言数据集，用于专家-学员辅助任务，包含第一人称视角视频和专家指导对话，创建了超过15k个视觉问答对，用于评估多模态大语言模型在专家级辅助任务中的表现。


<details>
  <summary>Details</summary>
Motivation: 当前缺乏高质量的第一人称视角视频-语言数据集来评估多模态大语言模型在专家级辅助任务中的能力。现有数据集通常缺少真实的专家-学员互动对话，难以评估模型在复杂程序性活动中的指导能力。

Method: 采用"Wizard of OZ"数据收集范式，专家通过第一人称视角观察学员执行程序性活动，提供实时指导和回答问题。收集了50小时未脚本化的第一人称视频和双向对话，转录后创建了超过15k个视觉问答对作为评估基准。

Result: Ego-EXTRA数据集对当前多模态大语言模型具有挑战性，结果显示现有模型在提供专家级辅助方面存在明显局限性。数据集公开可用，支持第一人称视频-语言助手的研究和评估。

Conclusion: Ego-EXTRA提供了一个高质量的第一人称视频-语言数据集和评估基准，揭示了当前多模态大语言模型在专家级辅助任务中的不足，为未来研究提供了重要资源。

Abstract: We present Ego-EXTRA, a video-language Egocentric Dataset for EXpert-TRAinee assistance. Ego-EXTRA features 50 hours of unscripted egocentric videos of subjects performing procedural activities (the trainees) while guided by real-world experts who provide guidance and answer specific questions using natural language. Following a ``Wizard of OZ'' data collection paradigm, the expert enacts a wearable intelligent assistant, looking at the activities performed by the trainee exclusively from their egocentric point of view, answering questions when asked by the trainee, or proactively interacting with suggestions during the procedures. This unique data collection protocol enables Ego-EXTRA to capture a high-quality dialogue in which expert-level feedback is provided to the trainee. Two-way dialogues between experts and trainees are recorded, transcribed, and used to create a novel benchmark comprising more than 15k high-quality Visual Question Answer sets, which we use to evaluate Multimodal Large Language Models. The results show that Ego-EXTRA is challenging and highlight the limitations of current models when used to provide expert-level assistance to the user. The Ego-EXTRA dataset is publicly available to support the benchmark of egocentric video-language assistants: https://fpv-iplab.github.io/Ego-EXTRA/.

</details>


### [164] [Face Identity Unlearning for Retrieval via Embedding Dispersion](https://arxiv.org/abs/2512.13317)
*Mikhail Zakharov*

Main category: cs.CV

Relevance: 45.0

TL;DR: 该论文研究人脸检索系统中的身份遗忘问题，提出了一种基于分散的遗忘方法，使特定身份无法被检索，同时保持其他身份的检索性能。


<details>
  <summary>Details</summary>
Motivation: 人脸识别系统存在严重的隐私问题，可能被用于未经授权的身份追踪。虽然已有机器遗忘方法用于隐私保护，但针对现代基于嵌入的人脸检索模型的遗忘技术尚未充分探索。

Method: 评估了多种现有近似类别遗忘方法（随机标签、梯度上升、边界遗忘等），并提出了一种简单有效的基于分散的遗忘方法，通过在超球面上分散目标身份的嵌入，防止形成紧凑的身份聚类。

Result: 在VGGFace2和CelebA标准基准测试上的广泛实验表明，该方法实现了优异的遗忘效果，同时保持了其他身份的检索效用。

Conclusion: 该工作填补了人脸检索系统中身份遗忘的研究空白，提出的分散方法在遗忘效果和检索性能保持方面优于现有方法。

Abstract: Face recognition systems rely on learning highly discriminative and compact identity clusters to enable accurate retrieval. However, as with other surveillance-oriented technologies, such systems raise serious privacy concerns due to their potential for unauthorized identity tracking. While several works have explored machine unlearning as a means of privacy protection, their applicability to face retrieval - especially for modern embedding-based recognition models - remains largely unexplored. In this work, we study the problem of face identity unlearning for retrieval systems and present its inherent challenges. The goal is to make selected identities unretrievable by dispersing their embeddings on the hypersphere and preventing the formation of compact identity clusters that enable re-identification in the gallery. The primary challenge is to achieve this forgetting effect while preserving the discriminative structure of the embedding space and the retrieval performance of the model for the remaining identities. To address this, we evaluate several existing approximate class unlearning methods (e.g., Random Labeling, Gradient Ascent, Boundary Unlearning, and other recent approaches) in the context of face retrieval and propose a simple yet effective dispersion-based unlearning approach. Extensive experiments on standard benchmarks (VGGFace2, CelebA) demonstrate that our method achieves superior forgetting behavior while preserving retrieval utility.

</details>


### [165] [Pancakes: Consistent Multi-Protocol Image Segmentation Across Biomedical Domains](https://arxiv.org/abs/2512.13534)
*Marianne Rakic,Siyu Gai,Etienne Chollet,John V. Guttag,Adrian V. Dalca*

Main category: cs.CV

Relevance: 45.0

TL;DR: Pancakes框架能自动为医学图像生成多种语义一致的分割协议，解决了现有模型只能处理单一协议或需要手动提示的问题。


<details>
  <summary>Details</summary>
Motivation: 医学图像可以根据不同应用需求进行多种有意义的分割（如组织类型、血管区域、解剖结构等），但现有自动分割模型要么只支持单一训练协议，要么需要人工手动指定分割方式，限制了实际应用。

Method: Pancakes框架引入新的问题表述，能够自动为未见过的医学图像生成多个合理协议的多标签分割图，同时保持相关图像间的语义一致性。

Result: 在7个保留数据集上的实验表明，Pancakes在生成多个语义一致的整图分割方面显著优于现有基础模型。

Conclusion: Pancakes解决了医学图像分割中多协议自动生成的挑战，为医学影像分析提供了更灵活和实用的解决方案。

Abstract: A single biomedical image can be meaningfully segmented in multiple ways, depending on the desired application. For instance, a brain MRI can be segmented according to tissue types, vascular territories, broad anatomical regions, fine-grained anatomy, or pathology, etc. Existing automatic segmentation models typically either (1) support only a single protocol, the one they were trained on, or (2) require labor-intensive manual prompting to specify the desired segmentation. We introduce Pancakes, a framework that, given a new image from a previously unseen domain, automatically generates multi-label segmentation maps for multiple plausible protocols, while maintaining semantic consistency across related images. Pancakes introduces a new problem formulation that is not currently attainable by existing foundation models. In a series of experiments on seven held-out datasets, we demonstrate that our model can significantly outperform existing foundation models in producing several plausible whole-image segmentations, that are semantically coherent across images.

</details>


### [166] [V-Warper: Appearance-Consistent Video Diffusion Personalization via Value Warping](https://arxiv.org/abs/2512.12375)
*Hyunkoo Lee,Wooseok Jang,Jini Yang,Taehwan Kim,Sangoh Kim,Sangwon Jung,Seungryong Kim*

Main category: cs.CV

Relevance: 40.0

TL;DR: V-Warper：一种免训练的粗到细视频个性化框架，通过轻量级外观适应和推理时细粒度外观注入，在保持文本对齐和运动动态的同时显著提升外观保真度。


<details>
  <summary>Details</summary>
Motivation: 现有视频个性化方法依赖大量视频微调或大规模视频数据集，计算成本高且难以扩展，同时在帧间保持细粒度外观一致性方面存在困难。

Method: 提出两阶段框架：1) 轻量级粗外观适应阶段，仅使用少量参考图像，通过图像LoRA和主题嵌入适应编码全局主题身份；2) 推理时细外观注入阶段，通过计算RoPE-free中层query-key特征的语义对应关系，引导外观丰富的value表示到生成过程的语义对齐区域。

Result: V-Warper显著提高了外观保真度，同时保持了文本对齐和运动动态，且无需大规模视频微调即可高效实现这些改进。

Conclusion: V-Warper提供了一种高效、免训练的视频个性化解决方案，解决了现有方法在计算成本和细粒度一致性方面的限制。

Abstract: Video personalization aims to generate videos that faithfully reflect a user-provided subject while following a text prompt. However, existing approaches often rely on heavy video-based finetuning or large-scale video datasets, which impose substantial computational cost and are difficult to scale. Furthermore, they still struggle to maintain fine-grained appearance consistency across frames. To address these limitations, we introduce V-Warper, a training-free coarse-to-fine personalization framework for transformer-based video diffusion models. The framework enhances fine-grained identity fidelity without requiring any additional video training. (1) A lightweight coarse appearance adaptation stage leverages only a small set of reference images, which are already required for the task. This step encodes global subject identity through image-only LoRA and subject-embedding adaptation. (2) A inference-time fine appearance injection stage refines visual fidelity by computing semantic correspondences from RoPE-free mid-layer query--key features. These correspondences guide the warping of appearance-rich value representations into semantically aligned regions of the generation process, with masking ensuring spatial reliability. V-Warper significantly improves appearance fidelity while preserving prompt alignment and motion dynamics, and it achieves these gains efficiently without large-scale video finetuning.

</details>


### [167] [ArtGen: Conditional Generative Modeling of Articulated Objects in Arbitrary Part-Level States](https://arxiv.org/abs/2512.12395)
*Haowen Wang,Xiaoping Yuan,Fugang Zhang,Rui Jian,Yuanwei Zhu,Xiuquan Qiao,Yakun Huang*

Main category: cs.CV

Relevance: 40.0

TL;DR: ArtGen是一个条件扩散框架，能够从单视角图像或文本描述生成具有准确几何结构和连贯运动学的铰接式3D物体，通过跨状态蒙特卡洛采样和思维链推理解决几何形状与关节动力学的纠缠问题。


<details>
  <summary>Details</summary>
Motivation: 现有生成模型通常依赖闭合状态的单视角输入，导致几何形状与关节动力学纠缠，产生模糊或不现实的运动学结构。铰接式资产生成对于机器人、数字孪生和具身智能至关重要，需要解决这一挑战。

Method: 1. 使用跨状态蒙特卡洛采样显式强制执行全局运动学一致性；2. 集成思维链推理模块推断结构先验（部件语义、关节类型、连接性）；3. 采用稀疏专家扩散Transformer处理多样化运动学交互；4. 使用局部-全局注意力增强的组合式3D-VAE潜在先验。

Result: 在PartNet-Mobility基准测试上的广泛实验表明，ArtGen显著优于现有最先进方法。

Conclusion: ArtGen能够从单视角图像或文本描述生成具有准确几何和连贯运动学的铰接式3D物体，解决了结构-运动纠缠问题，为机器人、数字孪生和具身智能应用提供了有效解决方案。

Abstract: Generating articulated assets is crucial for robotics, digital twins, and embodied intelligence. Existing generative models often rely on single-view inputs representing closed states, resulting in ambiguous or unrealistic kinematic structures due to the entanglement between geometric shape and joint dynamics. To address these challenges, we introduce ArtGen, a conditional diffusion-based framework capable of generating articulated 3D objects with accurate geometry and coherent kinematics from single-view images or text descriptions at arbitrary part-level states. Specifically, ArtGen employs cross-state Monte Carlo sampling to explicitly enforce global kinematic consistency, reducing structural-motion entanglement. Additionally, we integrate a Chain-of-Thought reasoning module to infer robust structural priors, such as part semantics, joint types, and connectivity, guiding a sparse-expert Diffusion Transformer to specialize in diverse kinematic interactions. Furthermore, a compositional 3D-VAE latent prior enhanced with local-global attention effectively captures fine-grained geometry and global part-level relationships. Extensive experiments on the PartNet-Mobility benchmark demonstrate that ArtGen significantly outperforms state-of-the-art methods.

</details>


### [168] [Supervised Contrastive Frame Aggregation for Video Representation Learning](https://arxiv.org/abs/2512.12549)
*Shaif Chowdhury,Mushfika Rahman,Greg Hamerly*

Main category: cs.CV

Relevance: 40.0

TL;DR: 提出了一种监督对比学习框架用于视频表示学习，通过视频到图像的聚合策略将多个帧排列成单张图像，利用预训练CNN骨干网络，避免了复杂视频Transformer的计算开销。


<details>
  <summary>Details</summary>
Motivation: 现有视频表示学习方法通常需要复杂的视频Transformer模型，计算开销大。作者希望利用预训练的卷积神经网络（如ResNet50）来处理视频数据，同时保持对时间全局上下文的理解。

Method: 1) 视频到图像聚合策略：将多个视频帧空间排列成单张输入图像；2) 监督对比学习目标：比较模型生成的成对投影，同标签视频投影为正对，其他为负对；3) 多自然视图：通过不同时间帧采样创建同一视频的多个视图，无需数据增强。

Result: 在Penn Action数据集上达到76%分类准确率（ViViT为43%），在HMDB51数据集上达到48%准确率（ViViT为37%）。方法在监督和自监督设置下都能学习有效的视频表示，支持分类和字幕生成等任务。

Conclusion: 提出的监督对比帧聚合方法能够有效学习视频表示，在分类准确率上优于现有方法，同时计算资源需求更少。该方法为视频理解任务提供了一种高效且有效的解决方案。

Abstract: We propose a supervised contrastive learning framework for video representation learning that leverages temporally global context. We introduce a video to image aggregation strategy that spatially arranges multiple frames from each video into a single input image. This design enables the use of pre trained convolutional neural network backbones such as ResNet50 and avoids the computational overhead of complex video transformer models. We then design a contrastive learning objective that directly compares pairwise projections generated by the model. Positive pairs are defined as projections from videos sharing the same label while all other projections are treated as negatives. Multiple natural views of the same video are created using different temporal frame samplings from the same underlying video. Rather than relying on data augmentation these frame level variations produce diverse positive samples with global context and reduce overfitting. Experiments on the Penn Action and HMDB51 datasets demonstrate that the proposed method outperforms existing approaches in classification accuracy while requiring fewer computational resources. The proposed Supervised Contrastive Frame Aggregation method learns effective video representations in both supervised and self supervised settings and supports video based tasks such as classification and captioning. The method achieves seventy six percent classification accuracy on Penn Action compared to forty three percent achieved by ViVIT and forty eight percent accuracy on HMDB51 compared to thirty seven percent achieved by ViVIT.

</details>


### [169] [Predictive Sample Assignment for Semantically Coherent Out-of-Distribution Detection](https://arxiv.org/abs/2512.12906)
*Zhimao Peng,Enguang Wang,Xialei Liu,Ming-Ming Cheng*

Main category: cs.CV

Relevance: 40.0

TL;DR: 提出基于预测样本分配（PSA）的语义一致分布外检测框架，通过双阈值三元样本分配策略提高ID/OOD样本集纯度，结合概念对比表示学习和重训练策略，在SCOOD基准上显著优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 当前SCOOD方法主要采用聚类式ID样本过滤策略从未标记数据中选择干净ID样本，将剩余样本作为辅助OOD数据，这不可避免地引入了大量噪声样本。需要解决训练数据中噪声样本过多的问题。

Method: 1. 基于预测能量分数的双阈值三元样本分配策略：将不确定的未标记数据分配到额外的丢弃样本集，提高ID和OOD样本集的纯度；2. 概念对比表示学习损失：在表示空间中扩大ID和OOD样本之间的距离；3. 重训练策略：帮助模型充分拟合选定的辅助ID/OOD样本。

Result: 在两个标准SCOOD基准测试上的实验表明，该方法显著优于现有最先进方法。

Conclusion: 提出的PSA框架通过改进样本分配策略和表示学习，有效解决了SCOOD中噪声样本问题，提升了OOD检测性能。

Abstract: Semantically coherent out-of-distribution detection (SCOOD) is a recently proposed realistic OOD detection setting: given labeled in-distribution (ID) data and mixed in-distribution and out-of-distribution unlabeled data as the training data, SCOOD aims to enable the trained model to accurately identify OOD samples in the testing data. Current SCOOD methods mainly adopt various clustering-based in-distribution sample filtering (IDF) strategies to select clean ID samples from unlabeled data, and take the remaining samples as auxiliary OOD data, which inevitably introduces a large number of noisy samples in training. To address the above issue, we propose a concise SCOOD framework based on predictive sample assignment (PSA). PSA includes a dual-threshold ternary sample assignment strategy based on the predictive energy score that can significantly improve the purity of the selected ID and OOD sample sets by assigning unconfident unlabeled data to an additional discard sample set, and a concept contrastive representation learning loss to further expand the distance between ID and OOD samples in the representation space to assist ID/OOD discrimination. In addition, we also introduce a retraining strategy to help the model fully fit the selected auxiliary ID/OOD samples. Experiments on two standard SCOOD benchmarks demonstrate that our approach outperforms the state-of-the-art methods by a significant margin.

</details>


### [170] [Improving Long-Tailed Object Detection with Balanced Group Softmax and Metric Learning](https://arxiv.org/abs/2511.16619)
*Satyam Gaba*

Main category: cs.CV

Relevance: 40.0

TL;DR: 该论文针对长尾分布的目标检测问题，在LVISv1数据集上提出了改进的BAGS框架和度量学习方法，实现了24.5% mAP的新SOTA性能。


<details>
  <summary>Details</summary>
Motivation: 现实世界中的目标检测面临长尾分布问题，即少数类别（头部）有大量样本，而多数类别（尾部）只有少量样本。这种类别不平衡会导致检测模型偏向频繁类别，在稀有类别上性能下降。LVISv1数据集包含1203个类别，是典型的长尾分布场景。

Method: 采用两阶段Faster R-CNN架构，改进Balanced Group Softmax (BAGS)框架来缓解类别不平衡。同时提出假设：尾部类别特征可能在头部类别的特征空间中形成更小、更密集的簇，使得基于回归的分类器难以区分。为此探索度量学习来生成类间分离、类内紧凑的特征嵌入，并在推理时使用k-NN方法提升分类性能。

Result: 在LVISv1数据集上实现了24.5%的mAP，超越了之前24.0%的基准，创造了新的state-of-the-art性能。证明了所提方法在长尾目标检测中的有效性。

Conclusion: 通过改进BAGS框架和引入度量学习方法，有效解决了长尾分布下的目标检测问题，提升了稀有类别的检测性能，为实际应用中的类别不平衡问题提供了有效解决方案。

Abstract: Object detection has been widely explored for class-balanced datasets such as COCO. However, real-world scenarios introduce the challenge of long-tailed distributions, where numerous categories contain only a few instances. This inherent class imbalance biases detection models towards the more frequent classes, degrading performance on rare categories. In this paper, we tackle the problem of long-tailed 2D object detection using the LVISv1 dataset, which consists of 1,203 categories and 164,000 images. We employ a two-stage Faster R-CNN architecture and propose enhancements to the Balanced Group Softmax (BAGS) framework to mitigate class imbalance. Our approach achieves a new state-of-the-art performance with a mean Average Precision (mAP) of 24.5%, surpassing the previous benchmark of 24.0%.
  Additionally, we hypothesize that tail class features may form smaller, denser clusters within the feature space of head classes, making classification challenging for regression-based classifiers. To address this issue, we explore metric learning to produce feature embeddings that are both well-separated across classes and tightly clustered within each class. For inference, we utilize a k-Nearest Neighbors (k-NN) approach to improve classification performance, particularly for rare classes. Our results demonstrate the effectiveness of these methods in advancing long-tailed object detection.

</details>


### [171] [Grab-3D: Detecting AI-Generated Videos from 3D Geometric Temporal Consistency](https://arxiv.org/abs/2512.13665)
*Wenhan Chen,Sezer Karaoglu,Theo Gevers*

Main category: cs.CV

Relevance: 40.0

TL;DR: 提出Grab-3D框架，利用消失点作为3D几何模式表示，通过几何感知Transformer检测AI生成视频，在静态场景数据集上显著优于现有方法并具有跨域泛化能力。


<details>
  <summary>Details</summary>
Motivation: 随着扩散模型生成高度逼真视频的能力提升，需要可靠的检测机制。现有方法对生成视频中的3D几何模式探索有限，而真实与AI生成视频在几何一致性上存在根本差异。

Method: 使用消失点作为3D几何模式的显式表示；构建静态场景的AI生成视频数据集以稳定提取3D几何特征；提出几何感知Transformer框架，包含几何位置编码、时空几何注意力机制和EMA-based几何分类器头。

Result: Grab-3D显著优于最先进的检测器，在未见过的生成器上展现出强大的跨域泛化能力。

Conclusion: 通过显式建模3D几何时间一致性，Grab-3D为AI生成视频检测提供了有效解决方案，揭示了几何模式在区分真实与生成内容中的重要性。

Abstract: Recent advances in diffusion-based generation techniques enable AI models to produce highly realistic videos, heightening the need for reliable detection mechanisms. However, existing detection methods provide only limited exploration of the 3D geometric patterns present in generated videos. In this paper, we use vanishing points as an explicit representation of 3D geometry patterns, revealing fundamental discrepancies in geometric consistency between real and AI-generated videos. We introduce Grab-3D, a geometry-aware transformer framework for detecting AI-generated videos based on 3D geometric temporal consistency. To enable reliable evaluation, we construct an AI-generated video dataset of static scenes, allowing stable 3D geometric feature extraction. We propose a geometry-aware transformer equipped with geometric positional encoding, temporal-geometric attention, and an EMA-based geometric classifier head to explicitly inject 3D geometric awareness into temporal modeling. Experiments demonstrate that Grab-3D significantly outperforms state-of-the-art detectors, achieving robust cross-domain generalization to unseen generators.

</details>


### [172] [Explainable Adversarial-Robust Vision-Language-Action Model for Robotic Manipulation](https://arxiv.org/abs/2512.11865)
*Ju-Young Kim,Ji-Hong Park,Myeongjun Kim,Gun-Woo Kim*

Main category: cs.CV

Relevance: 35.0

TL;DR: 提出基于OpenVLA-OFT的可解释对抗鲁棒视觉-语言-动作模型，通过Evidence-3模块检测光度扰动并生成自然语言解释，在对抗条件下提升智能农业系统的动作预测准确性和可解释性。


<details>
  <summary>Details</summary>
Motivation: 智能农业中依赖RGB相机感知和机械臂控制的系统容易受到色调、光照、噪声等光度扰动的对抗攻击，导致系统故障。需要开发能够抵抗这些扰动并提供解释的鲁棒模型。

Method: 基于OpenVLA-OFT框架构建视觉-语言-动作模型，集成Evidence-3模块来检测光度扰动，并生成关于扰动原因和影响的自然语言解释。

Result: 相比基线模型，当前动作L1损失降低21.7%，下一动作L1损失降低18.4%，在对抗条件下显著提升了动作预测准确性和可解释性。

Conclusion: 提出的可解释对抗鲁棒模型能有效应对智能农业中的光度扰动问题，为农业自动化系统提供了更可靠和透明的解决方案。

Abstract: Smart farming has emerged as a key technology for advancing modern agriculture through automation and intelligent control. However, systems relying on RGB cameras for perception and robotic manipulators for control, common in smart farming, are vulnerable to photometric perturbations such as hue, illumination, and noise changes, which can cause malfunction under adversarial attacks. To address this issue, we propose an explainable adversarial-robust Vision-Language-Action model based on the OpenVLA-OFT framework. The model integrates an Evidence-3 module that detects photometric perturbations and generates natural language explanations of their causes and effects. Experiments show that the proposed model reduces Current Action L1 loss by 21.7% and Next Actions L1 loss by 18.4% compared to the baseline, demonstrating improved action prediction accuracy and explainability under adversarial conditions.

</details>


### [173] [Automated Plant Disease and Pest Detection System Using Hybrid Lightweight CNN-MobileViT Models for Diagnosis of Indigenous Crops](https://arxiv.org/abs/2512.11871)
*Tekleab G. Gebremedhin,Hailom S. Asegede,Bruh W. Tesheme,Tadesse B. Gebremichael,Kalayu G. Redae*

Main category: cs.CV

Relevance: 35.0

TL;DR: 论文开发了一个离线优先的作物病害检测系统，针对埃塞俄比亚提格雷地区的本土仙人掌无花果，使用三种移动高效架构进行基准测试，在边缘设备上实现高精度病害诊断。


<details>
  <summary>Details</summary>
Motivation: 埃塞俄比亚提格雷地区80%以上人口依赖农业，但基础设施中断限制了作物病害专家诊断的获取。需要为边缘环境开发离线检测系统，特别是针对本土作物仙人掌无花果。

Method: 构建了包含3,587张田间图像的本土仙人掌无花果数据集，基准测试三种移动高效架构：自定义轻量CNN、EfficientNet-Lite1和CNN-Transformer混合模型MobileViT-XS。系统采用ARM兼容模型，部署在支持提格雷尼亚语和阿姆哈拉语的Flutter应用中。

Result: EfficientNet-Lite1达到90.7%测试准确率，轻量CNN达到89.5%准确率且部署最优（42ms推理延迟，4.8MB模型大小），MobileViT-XS达到97.3%平均交叉验证准确率，证明基于MHSA的全局推理比局部纹理CNN核更可靠地区分害虫集群和真菌病变。

Conclusion: 研究展示了在边缘环境中高效作物病害检测的可行性，MobileViT-XS的Transformer架构在复杂病害模式识别上表现优异，系统支持完全离线推理，增强了粮食安全关键诊断的包容性。

Abstract: Agriculture supports over 80% of the population in the Tigray region of Ethiopia, where infrastructural disruptions limit access to expert crop disease diagnosis. We present an offline-first detection system centered on a newly curated indigenous cactus-fig (Opuntia ficus-indica) dataset consisting of 3,587 field images across three core symptom classes. Given deployment constraints in post-conflict edge environments, we benchmark three mobile-efficient architectures: a custom lightweight CNN, EfficientNet-Lite1, and the CNN-Transformer hybrid MobileViT-XS. While the broader system contains independent modules for potato, apple, and corn, this study isolates cactus-fig model performance to evaluate attention sensitivity and inductive bias transfer on indigenous morphology alone. Results establish a clear Pareto trade-off: EfficientNet-Lite1 achieves 90.7% test accuracy, the lightweight CNN reaches 89.5% with the most favorable deployment profile (42 ms inference latency, 4.8 MB model size), and MobileViT-XS delivers 97.3% mean cross-validation accuracy, demonstrating that MHSA-based global reasoning disambiguates pest clusters from two dimensional fungal lesions more reliably than local texture CNN kernels. The ARM compatible models are deployed in a Tigrigna and Amharic localized Flutter application supporting fully offline inference on Cortex-A53 class devices, strengthening inclusivity for food security critical diagnostics.

</details>


### [174] [MPath: Multimodal Pathology Report Generation from Whole Slide Images](https://arxiv.org/abs/2512.11906)
*Noorul Wahab,Nasir Rajpoot*

Main category: cs.CV

Relevance: 35.0

TL;DR: MPath是一个轻量级多模态框架，通过视觉前缀提示机制将WSI特征注入到预训练的生物医学语言模型中，用于从全切片图像自动生成病理诊断报告。


<details>
  <summary>Details</summary>
Motivation: 直接从全切片图像自动生成病理诊断报告是计算病理学的新兴方向，但由于组织形态的高度变异性和病理叙述的复杂结构，将高分辨率组织模式转化为临床连贯文本仍然困难。

Method: MPath采用轻量级多模态框架，通过学习的视觉前缀提示机制，将WSI衍生的视觉嵌入（使用CONCH+Titan基础模型特征）条件化到预训练的生物医学语言模型（BioBART）中。采用紧凑投影模块注入视觉特征，保持语言主干冻结以确保稳定性和数据效率。

Result: 在RED 2025 Grand Challenge数据集上开发和评估，在Test Phase 2中排名第4，尽管提交机会有限。结果显示了基于提示的多模态条件化作为可扩展且可解释的病理报告生成策略的潜力。

Conclusion: 基于提示的多模态条件化是病理报告生成的可扩展和可解释策略，轻量级设计避免了端到端视觉语言预训练，利用基础模型特征和冻结语言主干实现了稳定高效的系统。

Abstract: Automated generation of diagnostic pathology reports directly from whole slide images (WSIs) is an emerging direction in computational pathology. Translating high-resolution tissue patterns into clinically coherent text remains difficult due to large morphological variability and the complex structure of pathology narratives. We introduce MPath, a lightweight multimodal framework that conditions a pretrained biomedical language model (BioBART) on WSI-derived visual embeddings through a learned visual-prefix prompting mechanism. Instead of end-to-end vision-language pretraining, MPath leverages foundation-model WSI features (CONCH + Titan) and injects them into BioBART via a compact projection module, keeping the language backbone frozen for stability and data efficiency. MPath was developed and evaluated on the RED 2025 Grand Challenge dataset and ranked 4th in Test Phase 2, despite limited submission opportunities. The results highlight the potential of prompt-based multimodal conditioning as a scalable and interpretable strategy for pathology report generation.

</details>


### [175] [MONET -- Virtual Cell Painting of Brightfield Images and Time Lapses Using Reference Consistent Diffusion](https://arxiv.org/abs/2512.11928)
*Alexander Peysakhovich,William Berman,Joseph Rufo,Felix Wong,Maxwell Z. Wilson*

Main category: cs.CV

Relevance: 35.0

TL;DR: MONET使用扩散模型从明场图像预测细胞染色图像，通过一致性架构生成时间序列视频，支持上下文学习适应不同细胞系和成像协议


<details>
  <summary>Details</summary>
Motivation: 细胞染色技术存在两个主要问题：(1) 劳动密集型，(2) 需要化学固定，无法研究细胞动态过程。研究人员希望开发虚拟细胞染色工具来补充传统方法，实现新的生物学研究流程。

Method: 训练扩散模型（MONET）在大规模数据集上从明场图像预测细胞染色通道。采用一致性架构生成时间序列视频，尽管无法获得细胞染色视频训练数据。该架构支持上下文学习，使模型能够部分适应分布外的细胞系和成像协议。

Result: 模型质量随规模提升而改善。一致性架构能够生成时间序列视频，并实现上下文学习能力，使模型能够部分迁移到不同细胞系和成像协议。

Conclusion: 虚拟细胞染色不是要完全取代物理细胞染色，而是作为补充工具，在生物学研究中实现新的工作流程。

Abstract: Cell painting is a popular technique for creating human-interpretable, high-contrast images of cell morphology. There are two major issues with cell paint: (1) it is labor-intensive and (2) it requires chemical fixation, making the study of cell dynamics impossible. We train a diffusion model (Morphological Observation Neural Enhancement Tool, or MONET) on a large dataset to predict cell paint channels from brightfield images. We show that model quality improves with scale. The model uses a consistency architecture to generate time-lapse videos, despite the impossibility of obtaining cell paint video training data. In addition, we show that this architecture enables a form of in-context learning, allowing the model to partially transfer to out-of-distribution cell lines and imaging protocols. Virtual cell painting is not intended to replace physical cell painting completely, but to act as a complementary tool enabling novel workflows in biological research.

</details>


### [176] [Semantic-Drive: Democratizing Long-Tail Data Curation via Open-Vocabulary Grounding and Neuro-Symbolic VLM Consensus](https://arxiv.org/abs/2512.12012)
*Antonio Guillen-Perez*

Main category: cs.CV

Relevance: 35.0

TL;DR: 提出Semantic-Drive框架，通过神经符号方法在本地设备上高效挖掘自动驾驶长尾数据，解决安全关键事件数据稀缺问题。


<details>
  <summary>Details</summary>
Motivation: 自动驾驶发展受限于长尾训练数据稀缺，现有解决方案存在隐私泄露、成本高昂或精度不足等问题，需要本地化、高效的语义数据挖掘方法。

Method: 采用两阶段神经符号框架：1) 符号接地：实时开放词汇检测器(YOLOE)锚定注意力；2) 认知分析：推理视觉语言模型进行场景分析。通过"系统2"推理时对齐策略和多模型"法官-侦察兵"共识机制缓解幻觉问题。

Result: 在nuScenes数据集上，相比CLIP的0.475召回率，Semantic-Drive达到0.966召回率，风险评估误差降低40%，可在消费级硬件(NVIDIA RTX 3090)上运行。

Conclusion: Semantic-Drive提供了一种隐私保护、高效的本地语义数据挖掘解决方案，显著提升自动驾驶长尾事件检测能力，为数据稀缺问题提供新途径。

Abstract: The development of robust Autonomous Vehicles (AVs) is bottlenecked by the scarcity of "Long-Tail" training data. While fleets collect petabytes of video logs, identifying rare safety-critical events (e.g., erratic jaywalking, construction diversions) remains a manual, cost-prohibitive process. Existing solutions rely on coarse metadata search, which lacks precision, or cloud-based VLMs, which are privacy-invasive and expensive. We introduce Semantic-Drive, a local-first, neuro-symbolic framework for semantic data mining. Our approach decouples perception into two stages: (1) Symbolic Grounding via a real-time open-vocabulary detector (YOLOE) to anchor attention, and (2) Cognitive Analysis via a Reasoning VLM that performs forensic scene analysis. To mitigate hallucination, we implement a "System 2" inference-time alignment strategy, utilizing a multi-model "Judge-Scout" consensus mechanism. Benchmarked on the nuScenes dataset against the Waymo Open Dataset (WOD-E2E) taxonomy, Semantic-Drive achieves a Recall of 0.966 (vs. 0.475 for CLIP) and reduces Risk Assessment Error by 40\% compared to single models. The system runs entirely on consumer hardware (NVIDIA RTX 3090), offering a privacy-preserving alternative to the cloud.

</details>


### [177] [Adaptive federated learning for ship detection across diverse satellite imagery sources](https://arxiv.org/abs/2512.12053)
*Tran-Vu La,Minh-Tan Pham,Yu Li,Patrick Matgen,Marco Chini*

Main category: cs.CV

Relevance: 35.0

TL;DR: 该论文研究了联邦学习在跨多卫星数据集船舶检测中的应用，比较了四种FL算法与本地训练基线的性能，发现FL能显著提升小数据集上的检测精度，接近使用全部数据的全局训练效果。


<details>
  <summary>Details</summary>
Motivation: 解决卫星船舶检测中的数据隐私和安全问题，避免敏感商业卫星图像和船舶标注数据的集中收集与共享，同时提升在分散小数据集上的检测性能。

Method: 采用联邦学习框架，在多个卫星数据集上训练YOLOv8船舶检测模型，比较FedAvg、FedProx、FedOpt和FedMedian四种FL算法，并与本地独立训练基线进行对比，研究了通信轮数和本地训练轮数等配置参数的影响。

Result: FL模型相比仅在小规模本地数据集上训练显著提升了检测精度，性能接近使用所有数据集的全局训练；不同FL算法表现各异，需要选择合适的FL配置（如通信轮数、本地训练轮数）来平衡精度和计算效率。

Conclusion: 联邦学习为卫星船舶检测提供了一种有效的隐私保护解决方案，能够在保护数据隐私的同时实现接近全局训练的性能，但需要仔细选择FL算法和配置参数以获得最佳效果。

Abstract: We investigate the application of Federated Learning (FL) for ship detection across diverse satellite datasets, offering a privacy-preserving solution that eliminates the need for data sharing or centralized collection. This approach is particularly advantageous for handling commercial satellite imagery or sensitive ship annotations. Four FL models including FedAvg, FedProx, FedOpt, and FedMedian, are evaluated and compared to a local training baseline, where the YOLOv8 ship detection model is independently trained on each dataset without sharing learned parameters. The results reveal that FL models substantially improve detection accuracy over training on smaller local datasets and achieve performance levels close to global training that uses all datasets during the training. Furthermore, the study underscores the importance of selecting appropriate FL configurations, such as the number of communication rounds and local training epochs, to optimize detection precision while maintaining computational efficiency.

</details>


### [178] [Enhancing deep learning performance on burned area delineation from SPOT-6/7 imagery for emergency management](https://arxiv.org/abs/2512.12056)
*Maria Rodriguez,Minh-Tan Pham,Martin Sudmanns,Quentin Poterek,Oscar Narvaez*

Main category: cs.CV

Relevance: 35.0

TL;DR: 该研究提出了一种监督式语义分割工作流，用于提升野火后烧毁区域（BA）的制图性能和效率，基于SPOT-6/7高分辨率遥感影像，评估了U-Net和SegFormer模型，并探讨了土地覆盖数据作为辅助任务以及测试时增强的效果。


<details>
  <summary>Details</summary>
Motivation: 当前烧毁区域制图方法主要依赖计算机视觉模型处理灾后遥感影像，但往往忽视了其在时间紧迫的应急管理场景中的适用性。需要开发既能保证性能又能提高效率的BA制图方法。

Method: 采用监督式语义分割工作流，针对SPOT-6/7高分辨率影像进行实验。比较了U-Net和SegFormer两种模型，引入土地覆盖数据作为辅助任务增强模型鲁棒性，并使用测试时增强（TTA）提升性能。评估指标包括Dice分数、IoU和推理时间。

Result: U-Net和SegFormer在有限训练数据下表现相似，但SegFormer需要更多计算资源，在应急场景中实用性受限。加入土地覆盖辅助任务能增强模型鲁棒性且不增加推理时间。测试时增强能提升BA制图性能，但会增加推理时间，可通过混合精度等优化方法缓解。

Conclusion: 该研究为应急管理场景下的烧毁区域制图提供了实用的解决方案，平衡了性能与效率的需求。U-Net在资源受限环境下更具优势，而辅助任务和优化技术能进一步提升模型实用性。

Abstract: After a wildfire, delineating burned areas (BAs) is crucial for quantifying damages and supporting ecosystem recovery. Current BA mapping approaches rely on computer vision models trained on post-event remote sensing imagery, but often overlook their applicability to time-constrained emergency management scenarios. This study introduces a supervised semantic segmentation workflow aimed at boosting both the performance and efficiency of BA delineation. It targets SPOT-6/7 imagery due to its very high resolution and on-demand availability. Experiments are evaluated based on Dice score, Intersection over Union, and inference time. The results show that U-Net and SegFormer models perform similarly with limited training data. However, SegFormer requires more resources, challenging its practical use in emergencies. Incorporating land cover data as an auxiliary task enhances model robustness without increasing inference time. Lastly, Test-Time Augmentation improves BA delineation performance but raises inference time, which can be mitigated with optimization methods like Mixed Precision.

</details>


### [179] [EchoVLM: Measurement-Grounded Multimodal Learning for Echocardiography](https://arxiv.org/abs/2512.12107)
*Yuheng Li,Yue Zhang,Abdoul Aziz Amadou,Yuxiang Lai,Jike Zhong,Tiziano Passerini,Dorin Comaniciu,Puneet Sharma*

Main category: cs.CV

Relevance: 35.0

TL;DR: 提出了EchoGround-MIMIC数据集和EchoVLM模型，用于超声心动图的多模态分析，通过测量基础的数据集和创新的预训练目标实现了临床应用的SOTA性能。


<details>
  <summary>Details</summary>
Motivation: 超声心动图是心脏病学中最广泛使用的成像方式，但其解释仍然劳动密集且本质上是多模态的，需要视图识别、定量测量、定性评估和基于指南的推理。现有的视觉语言模型在超声心动图领域潜力有限，缺乏大规模、临床基础的数据集和测量基础的推理能力。

Method: 1) 创建EchoGround-MIMIC数据集：19,065个图像-文本对，包含标准化视图、结构化测量、测量基础的描述和指南衍生的疾病标签；2) 提出EchoVLM模型：包含两个新颖的预训练目标：视图感知对比损失（编码超声心动图成像的视图依赖结构）和否定感知对比损失（区分临床关键的阴性发现与阳性发现）。

Result: 在5类临床应用（36个任务）中实现SOTA性能：零样本疾病分类AUC 86.5%，视图分类准确率95.1%。模型展示了临床基础的多模态预训练产生可迁移的视觉表示，成为端到端超声心动图解释的基础模型。

Conclusion: 通过测量基础的数据集和创新的预训练目标，EchoVLM在超声心动图多模态分析中取得了突破性进展，为医学影像分析提供了新的基础模型范例。

Abstract: Echocardiography is the most widely used imaging modality in cardiology, yet its interpretation remains labor-intensive and inherently multimodal, requiring view recognition, quantitative measurements, qualitative assessments, and guideline-based reasoning. While recent vision-language models (VLMs) have achieved broad success in natural images and certain medical domains, their potential in echocardiography has been limited by the lack of large-scale, clinically grounded image-text datasets and the absence of measurement-based reasoning central to echo interpretation. We introduce EchoGround-MIMIC, the first measurement-grounded multimodal echocardiography dataset, comprising 19,065 image-text pairs from 1,572 patients with standardized views, structured measurements, measurement-grounded captions, and guideline-derived disease labels. Building on this resource, we propose EchoVLM, a vision-language model that incorporates two novel pretraining objectives: (i) a view-informed contrastive loss that encodes the view-dependent structure of echocardiographic imaging, and (ii) a negation-aware contrastive loss that distinguishes clinically critical negative from positive findings. Across five types of clinical applications with 36 tasks spanning multimodal disease classification, image-text retrieval, view classification, chamber segmentation, and landmark detection, EchoVLM achieves state-of-the-art performance (86.5% AUC in zero-shot disease classification and 95.1% accuracy in view classification). We demonstrate that clinically grounded multimodal pretraining yields transferable visual representations and establish EchoVLM as a foundation model for end-to-end echocardiography interpretation. We will release EchoGround-MIMIC and the data curation code, enabling reproducibility and further research in multimodal echocardiography interpretation.

</details>


### [180] [A Novel Patch-Based TDA Approach for Computed Tomography](https://arxiv.org/abs/2512.12108)
*Dashti A. Ali,Aras T. Asaad,Jacob J. Peoples,Mohammad Hamghalam,Alex Robins,Mane Piliposyan,Richard K. G. Do,Natalie Gangai,Yun S. Chun,Ahmad Bashir Barekzai,Jayasree Chakraborty,Hala Khasawneh,Camila Vilela,Natally Horvat,João Miranda,Alice C. Wei,Amber L. Simpson*

Main category: cs.CV

Relevance: 35.0

TL;DR: 提出了一种基于patch的持久同调构建方法，用于3D CT医学图像分析，相比传统3D立方体复形方法在分类性能和时间效率上都有显著提升。


<details>
  <summary>Details</summary>
Motivation: 传统3D立方体复形方法在CT图像拓扑数据分析中性能有限且计算复杂度高，特别是对于高分辨率图像。需要一种更高效、性能更好的持久同调构建方法。

Method: 提出了一种新颖的基于patch的持久同调构建方法，专门针对体积医学成像数据（特别是CT模态）。该方法将图像分割成patch进行处理，减少了计算复杂度。

Result: 在多个3D CT数据集上的实验表明，patch-based方法在准确率、AUC、敏感性、特异性、F1分数等指标上平均提升10.38%、6.94%、2.06%、11.58%和8.51%，同时时间效率更高。

Conclusion: 基于patch的TDA方法在CT图像分析中优于传统立方体复形方法，提供了更好的分类性能和计算效率，并发布了Python包Patch-TDA以方便使用。

Abstract: The development of machine learning (ML) models based on computed tomography (CT) imaging modality has been a major focus of recent research in the medical imaging domain. Incorporating robust feature engineering approach can highly improve the performance of these models. Topological data analysis (TDA), a recent development based on the mathematical field of algebraic topology, mainly focuses on the data from a topological perspective, extracting deeper insight and higher dimensional structures from the data. Persistent homology (PH), a fundamental tool in the area of TDA, can extract topological features such as connected components, cycles and voids from the data. A popular approach to construct PH from 3D CT images is to utilize the 3D cubical complex filtration, a method adapted for grid-structured data. However, this approach may not always yield the best performance and can suffer from computational complexity with higher resolution CT images. This study introduces a novel patch-based PH construction approach tailored for volumetric medical imaging data, in particular CT modality. A wide range of experiments has been conducted on several datasets of 3D CT images to comprehensively analyze the performance of the proposed method with various parameters and benchmark it against the 3D cubical complex algorithm. Our results highlight the dominance of the patch-based TDA approach in terms of both classification performance and time-efficiency. The proposed approach outperformed the cubical complex method, achieving average improvement of 10.38%, 6.94%, 2.06%, 11.58%, and 8.51% in accuracy, AUC, sensitivity, specificity, and F1 score, respectively, across all datasets. Finally, we provide a convenient python package, Patch-TDA, to facilitate the utilization of the proposed approach.

</details>


### [181] [ALERT Open Dataset and Input-Size-Agnostic Vision Transformer for Driver Activity Recognition using IR-UWB](https://arxiv.org/abs/2512.12206)
*Jeongjun Park,Sunwook Hwang,Hyeonho Noh,Jin Mo Yang,Hyun Jong Yang,Saewoong Bahk*

Main category: cs.CV

Relevance: 35.0

TL;DR: 该论文提出了ALERT数据集和ISA-ViT框架，用于基于UWB雷达的驾驶员分心行为识别，解决了数据集缺乏和ViT输入尺寸不匹配的问题。


<details>
  <summary>Details</summary>
Motivation: 分心驾驶导致全球致命事故，现有UWB雷达DAR面临两大挑战：缺乏大规模真实世界UWB数据集，以及固定输入尺寸的ViT难以适应UWB雷达数据的非标准维度。

Method: 1) 提出ALERT数据集（10,220个雷达样本，7种分心驾驶行为）；2) 提出ISA-ViT框架，通过调整patch配置和利用预训练位置嵌入向量，在调整UWB数据尺寸时保留雷达特定信息；3) 采用域融合策略结合距离域和频率域特征。

Result: ISA-ViT在UWB-based DAR上比现有ViT方法提升了22.68%的准确率，ALERT数据集和输入尺寸无关策略公开可用。

Conclusion: 该工作通过数据集和框架创新，促进了更鲁棒、可扩展的分心驾驶检测系统的实际部署。

Abstract: Distracted driving contributes to fatal crashes worldwide. To address this, researchers are using driver activity recognition (DAR) with impulse radio ultra-wideband (IR-UWB) radar, which offers advantages such as interference resistance, low power consumption, and privacy preservation. However, two challenges limit its adoption: the lack of large-scale real-world UWB datasets covering diverse distracted driving behaviors, and the difficulty of adapting fixed-input Vision Transformers (ViTs) to UWB radar data with non-standard dimensions.
  This work addresses both challenges. We present the ALERT dataset, which contains 10,220 radar samples of seven distracted driving activities collected in real driving conditions. We also propose the input-size-agnostic Vision Transformer (ISA-ViT), a framework designed for radar-based DAR. The proposed method resizes UWB data to meet ViT input requirements while preserving radar-specific information such as Doppler shifts and phase characteristics. By adjusting patch configurations and leveraging pre-trained positional embedding vectors (PEVs), ISA-ViT overcomes the limitations of naive resizing approaches. In addition, a domain fusion strategy combines range- and frequency-domain features to further improve classification performance.
  Comprehensive experiments demonstrate that ISA-ViT achieves a 22.68% accuracy improvement over an existing ViT-based approach for UWB-based DAR. By publicly releasing the ALERT dataset and detailing our input-size-agnostic strategy, this work facilitates the development of more robust and scalable distracted driving detection systems for real-world deployment.

</details>


### [182] [Feature Aggregation for Efficient Continual Learning of Complex Facial Expressions](https://arxiv.org/abs/2512.12277)
*Thibault Geoffroy,Myriam Maumy,Lionel Prevost*

Main category: cs.CV

Relevance: 35.0

TL;DR: 提出了一种用于连续学习场景下的混合面部表情识别框架，结合深度卷积特征和面部动作单元，通过贝叶斯高斯混合模型缓解灾难性遗忘。


<details>
  <summary>Details</summary>
Motivation: 随着AI系统在日常生活中的普及，识别和适应人类情感对于有效的人机交互至关重要。面部表情识别是推断情感状态的主要渠道，但情感的动态性和文化细微差别要求模型能够持续学习而不遗忘先验知识。

Method: 提出混合框架，整合两种互补模态：深度卷积特征和面部动作单元（来自FACS系统）。使用贝叶斯高斯混合模型对组合表示进行建模，提供轻量级概率解决方案，避免重新训练同时保持强判别能力。

Result: 在CFEE数据集上实验表明，模型能够先学习基本表情，然后逐步识别复合表情。实验证明提高了准确性，增强了知识保留，减少了遗忘。

Conclusion: 该框架有助于开发具有情感智能的AI系统，可应用于教育、医疗保健和自适应用户界面。

Abstract: As artificial intelligence (AI) systems become increasingly embedded in our daily life, the ability to recognize and adapt to human emotions is essential for effective human-computer interaction. Facial expression recognition (FER) provides a primary channel for inferring affective states, but the dynamic and culturally nuanced nature of emotions requires models that can learn continuously without forgetting prior knowledge. In this work, we propose a hybrid framework for FER in a continual learning setting that mitigates catastrophic forgetting. Our approach integrates two complementary modalities: deep convolutional features and facial Action Units (AUs) derived from the Facial Action Coding System (FACS). The combined representation is modelled through Bayesian Gaussian Mixture Models (BGMMs), which provide a lightweight, probabilistic solution that avoids retraining while offering strong discriminative power. Using the Compound Facial Expression of Emotion (CFEE) dataset, we show that our model can first learn basic expressions and then progressively recognize compound expressions. Experiments demonstrate improved accuracy, stronger knowledge retention, and reduced forgetting. This framework contributes to the development of emotionally intelligent AI systems with applications in education, healthcare, and adaptive user interfaces.

</details>


### [183] [RealDrag: The First Dragging Benchmark with Real Target Image](https://arxiv.org/abs/2512.12287)
*Ahmad Zafarani,Zahra Dehghanian,Mohammadreza Davoodi,Mohsen Shadroo,MohammadAmin Fazli,Hamid R. Rabiee*

Main category: cs.CV

Relevance: 35.0

TL;DR: RealDrag是首个包含真实目标图像的基于点拖拽图像编辑基准，包含400+人工标注样本和4个新指标，用于系统评估17个SOTA模型


<details>
  <summary>Details</summary>
Motivation: 当前基于拖拽的图像编辑模型评估不可靠，缺乏标准化基准和指标，没有真实目标图像数据集，导致方法间难以客观比较

Method: 构建RealDrag基准数据集（400+人工标注样本，包含源/目标图像、控制点/目标点、可编辑区域掩码、描述性标注），提出4个新指标：语义距离、外部掩码保持分数、内部补丁保持分数、方向相似性

Result: 对17个SOTA模型进行首次大规模系统分析，揭示了当前方法间的明确权衡，建立了可复现的基准线

Conclusion: RealDrag为基于点的图像编辑提供了首个包含真实目标图像的全面基准，建立了可靠的评估框架，将促进该领域研究

Abstract: The evaluation of drag based image editing models is unreliable due to a lack of standardized benchmarks and metrics. This ambiguity stems from inconsistent evaluation protocols and, critically, the absence of datasets containing ground truth target images, making objective comparisons between competing methods difficult. To address this, we introduce \textbf{RealDrag}, the first comprehensive benchmark for point based image editing that includes paired ground truth target images. Our dataset contains over 400 human annotated samples from diverse video sources, providing source/target images, handle/target points, editable region masks, and descriptive captions for both the image and the editing action.
  We also propose four novel, task specific metrics: Semantical Distance (SeD), Outer Mask Preserving Score (OMPS), Inner Patch Preserving Score (IPPS), and Directional Similarity (DiS). These metrics are designed to quantify pixel level matching fidelity, check preservation of non edited (out of mask) regions, and measure semantic alignment with the desired task. Using this benchmark, we conduct the first large scale systematic analysis of the field, evaluating 17 SOTA models. Our results reveal clear trade offs among current approaches and establish a robust, reproducible baseline to guide future research. Our dataset and evaluation toolkit will be made publicly available.

</details>


### [184] [From Human Intention to Action Prediction: A Comprehensive Benchmark for Intention-driven End-to-End Autonomous Driving](https://arxiv.org/abs/2512.12302)
*Huan Zheng,Yucheng Zhou,Tianyi Yan,Jiayi Su,Hongjun Chen,Dubing Chen,Wencheng Han,Runzhou Tao,Zhongying Qiu,Jianfei Yang,Jianbing Shen*

Main category: cs.CV

Relevance: 35.0

TL;DR: 论文提出了Intention-Drive基准测试，用于评估自动驾驶系统从高级人类意图到精确驾驶动作的转换能力，包含新数据集和基于意图成功率（ISR）的评估协议。


<details>
  <summary>Details</summary>
Motivation: 当前端到端自动驾驶系统仅能执行低级指令，缺乏对高级人类意图的理解和实现能力。需要从"指令跟随者"转变为"意图实现者"，但缺乏标准化基准来评估和推动这一复杂任务的发展。

Method: 1) 创建包含复杂场景和对应自然语言意图的新数据集；2) 提出以意图成功率（ISR）为核心的新评估协议，超越几何精度评估，关注语义目标的实现。

Result: 通过对一系列基线模型在Intention-Drive上的广泛评估，发现基线模型在实现高级任务所需的全面场景和意图理解方面存在显著性能缺陷。

Conclusion: Intention-Drive填补了评估自动驾驶系统从人类意图到驾驶动作转换能力的关键空白，揭示了当前系统在高级意图理解方面的不足，为未来研究提供了重要基准。

Abstract: Current end-to-end autonomous driving systems operate at a level of intelligence akin to following simple steering commands. However, achieving genuinely intelligent autonomy requires a paradigm shift: moving from merely executing low-level instructions to understanding and fulfilling high-level, abstract human intentions. This leap from a command-follower to an intention-fulfiller, as illustrated in our conceptual framework, is hindered by a fundamental challenge: the absence of a standardized benchmark to measure and drive progress on this complex task. To address this critical gap, we introduce Intention-Drive, the first comprehensive benchmark designed to evaluate the ability to translate high-level human intent into safe and precise driving actions. Intention-Drive features two core contributions: (1) a new dataset of complex scenarios paired with corresponding natural language intentions, and (2) a novel evaluation protocol centered on the Intent Success Rate (ISR), which assesses the semantic fulfillment of the human's goal beyond simple geometric accuracy. Through an extensive evaluation of a spectrum of baseline models on Intention-Drive, we reveal a significant performance deficit, showing that the baseline model struggle to achieve the comprehensive scene and intention understanding required for this advanced task.

</details>


### [185] [MRD: Using Physically Based Differentiable Rendering to Probe Vision Models for 3D Scene Understanding](https://arxiv.org/abs/2512.12307)
*Benjamin Beilharz,Thomas S. A. Wallis*

Main category: cs.CV

Relevance: 35.0

TL;DR: MRD方法使用基于物理的可微分渲染来探究视觉模型对3D场景属性的隐式理解，通过寻找物理上不同但产生相同模型激活的3D场景参数（模型同构体）


<details>
  <summary>Details</summary>
Motivation: 尽管深度学习在视觉任务上取得了显著成功，但理解这些模型的表示和决策仍然困难。视觉模型通常基于2D输入训练，但被认为发展了对底层3D场景的隐式理解。需要一种方法来探究模型对生成性3D场景属性的理解程度。

Method: MRD（可微分渲染的同构体）方法使用基于物理的可微分渲染，通过寻找物理上不同但产生相同模型激活的3D场景参数来评估视觉模型。与之前基于像素的方法不同，这些重建结果始终基于物理场景描述，可以独立探究模型对不同场景属性的敏感性。

Result: 对多个模型评估几何（形状）和双向反射分布函数（材质）场景参数的恢复能力，结果显示目标场景和优化场景之间的模型激活高度相似，但视觉结果各异。重建结果有助于定性地研究模型对哪些物理场景属性敏感或不敏感。

Conclusion: MRD方法有望通过分析物理场景参数如何驱动模型响应变化，增进对计算机视觉和人类视觉的理解。该方法提供了一种基于物理场景描述的方式来探究视觉模型的隐式3D理解能力。

Abstract: While deep learning methods have achieved impressive success in many vision benchmarks, it remains difficult to understand and explain the representations and decisions of these models. Though vision models are typically trained on 2D inputs, they are often assumed to develop an implicit representation of the underlying 3D scene (for example, showing tolerance to partial occlusion, or the ability to reason about relative depth). Here, we introduce MRD (metamers rendered differentiably), an approach that uses physically based differentiable rendering to probe vision models' implicit understanding of generative 3D scene properties, by finding 3D scene parameters that are physically different but produce the same model activation (i.e. are model metamers). Unlike previous pixel-based methods for evaluating model representations, these reconstruction results are always grounded in physical scene descriptions. This means we can, for example, probe a model's sensitivity to object shape while holding material and lighting constant. As a proof-of-principle, we assess multiple models in their ability to recover scene parameters of geometry (shape) and bidirectional reflectance distribution function (material). The results show high similarity in model activation between target and optimized scenes, with varying visual results. Qualitatively, these reconstructions help investigate the physical scene attributes to which models are sensitive or invariant. MRD holds promise for advancing our understanding of both computer and human vision by enabling analysis of how physical scene parameters drive changes in model responses.

</details>


### [186] [TCLeaf-Net: a transformer-convolution framework with global-local attention for robust in-field lesion-level plant leaf disease detection](https://arxiv.org/abs/2512.12357)
*Zishen Song,Yongjian Zhu,Dong Wang,Hongzhan Liu,Lingyu Jiang,Yongxing Duan,Zehua Zhang,Sihan Li,Jiarui Li*

Main category: cs.CV

Relevance: 35.0

TL;DR: TCLeaf-Net：一种用于真实田间条件下叶片病害检测的Transformer-卷积混合检测器，在Daylily-Leaf数据集上比基线模型提升5.4% mAP@50，同时减少计算量和内存使用。


<details>
  <summary>Details</summary>
Motivation: 在真实田间条件下，复杂的背景干扰、域偏移以及有限的病灶级数据集阻碍了叶片病害的鲁棒检测。需要开发能够在复杂环境中准确检测植物病害的模型。

Method: 提出TCLeaf-Net，包含三个核心模块：1) Transformer-卷积模块(TCM)结合全局上下文和局部卷积来抑制非叶片区域；2) 原始尺度特征回忆与采样(RSFRS)块通过双线性重采样和卷积保留空间细节；3) 可变形对齐FPN(DFPN)使用基于偏移的对齐和多感受野感知来增强多尺度融合。

Result: 在Daylily-Leaf数据集的田间分割上，TCLeaf-Net达到78.2% mAP@50，比基线提升5.4个百分点，同时减少7.5 GFLOPs计算量和8.7% GPU内存使用。在PlantDoc、Tomato-Leaf和Rice-Leaf数据集上也表现优异。

Conclusion: TCLeaf-Net在真实田间条件下有效解决了复杂背景干扰、信息损失和尺度变化等挑战，展示了在植物病害检测任务中的鲁棒性和泛化能力。

Abstract: Timely and accurate detection of foliar diseases is vital for safeguarding crop growth and reducing yield losses. Yet, in real-field conditions, cluttered backgrounds, domain shifts, and limited lesion-level datasets hinder robust modeling. To address these challenges, we release Daylily-Leaf, a paired lesion-level dataset comprising 1,746 RGB images and 7,839 lesions captured under both ideal and in-field conditions, and propose TCLeaf-Net, a transformer-convolution hybrid detector optimized for real-field use. TCLeaf-Net is designed to tackle three major challenges. To mitigate interference from complex backgrounds, the transformer-convolution module (TCM) couples global context with locality-preserving convolution to suppress non-leaf regions. To reduce information loss during downsampling, the raw-scale feature recalling and sampling (RSFRS) block combines bilinear resampling and convolution to preserve fine spatial detail. To handle variations in lesion scale and feature shifts, the deformable alignment block with FPN (DFPN) employs offset-based alignment and multi-receptive-field perception to strengthen multi-scale fusion. Experimental results show that on the in-field split of the Daylily-Leaf dataset, TCLeaf-Net improves mAP@50 by 5.4 percentage points over the baseline model, reaching 78.2\%, while reducing computation by 7.5 GFLOPs and GPU memory usage by 8.7\%. Moreover, the model outperforms recent YOLO and RT-DETR series in both precision and recall, and demonstrates strong performance on the PlantDoc, Tomato-Leaf, and Rice-Leaf datasets, validating its robustness and generalizability to other plant disease detection scenarios.

</details>


### [187] [BokehDepth: Enhancing Monocular Depth Estimation through Bokeh Generation](https://arxiv.org/abs/2512.12425)
*Hangwei Zhang,Armando Teles Fortes,Tianyi Wei,Xingang Pan*

Main category: cs.CV

Relevance: 35.0

TL;DR: BokehDepth：两阶段框架，通过解耦景深合成与深度估计，利用散焦作为无监督几何线索，提升景深渲染质量和单目深度估计精度


<details>
  <summary>Details</summary>
Motivation: 当前方法未能充分利用景深与单目深度估计之间的紧密耦合关系。高质量景深渲染依赖噪声深度图，而现代单目深度模型在弱纹理、远距离和几何模糊区域表现不佳，这些区域正是散焦线索最有价值的地方。

Method: 两阶段框架：第一阶段使用基于预训练图像编辑骨干的物理引导可控景深生成器，从单张清晰输入生成无深度信息的景深堆栈；第二阶段通过轻量级散焦感知聚合模块，将特征沿散焦维度融合，暴露稳定的深度敏感变化，保持下游解码器不变。

Result: 在多个挑战性基准测试中，BokehDepth相比基于深度图的景深基线方法提升了视觉保真度，并持续提高了强单目深度基础模型的度量精度和鲁棒性。

Conclusion: 通过解耦景深合成与深度估计，并将散焦作为无监督几何线索，BokehDepth框架能够同时提升景深渲染质量和单目深度估计性能，实现了两种任务的协同改进。

Abstract: Bokeh and monocular depth estimation are tightly coupled through the same lens imaging geometry, yet current methods exploit this connection in incomplete ways. High-quality bokeh rendering pipelines typically depend on noisy depth maps, which amplify estimation errors into visible artifacts, while modern monocular metric depth models still struggle on weakly textured, distant and geometrically ambiguous regions where defocus cues are most informative. We introduce BokehDepth, a two-stage framework that decouples bokeh synthesis from depth prediction and treats defocus as an auxiliary supervision-free geometric cue. In Stage-1, a physically guided controllable bokeh generator, built on a powerful pretrained image editing backbone, produces depth-free bokeh stacks with calibrated bokeh strength from a single sharp input. In Stage-2, a lightweight defocus-aware aggregation module plugs into existing monocular depth encoders, fuses features along the defocus dimension, and exposes stable depth-sensitive variations while leaving downstream decoder unchanged. Across challenging benchmarks, BokehDepth improves visual fidelity over depth-map-based bokeh baselines and consistently boosts the metric accuracy and robustness of strong monocular depth foundation models.

</details>


### [188] [Endless World: Real-Time 3D-Aware Long Video Generation](https://arxiv.org/abs/2512.12430)
*Ke Zhang,Yiqun Mei,Jiacong Xu,Vishal M. Patel*

Main category: cs.CV

Relevance: 35.0

TL;DR: Endless World是一个实时无限3D一致视频生成框架，通过条件自回归训练和全局3D感知注意力实现长序列视频生成，在单GPU上实现实时推理。


<details>
  <summary>Details</summary>
Motivation: 当前生成长、连贯且具有稳定3D结构的视频序列仍然是一个主要挑战，特别是在流式场景中。为了解决这个问题，作者提出了Endless World框架。

Method: 1) 条件自回归训练策略：对齐新生成内容与现有视频帧，保持长程依赖同时保持计算效率；2) 全局3D感知注意力：提供跨时间的连续几何指导；3) 3D注入机制：在整个扩展序列中强制执行物理合理性和几何一致性。

Result: 实验表明Endless World生成长、稳定且视觉连贯的视频，在视觉保真度和空间一致性方面达到或优于现有方法的性能。

Conclusion: Endless World成功解决了长序列和动态场景合成中的关键挑战，实现了无限、3D一致的实时视频生成。

Abstract: Producing long, coherent video sequences with stable 3D structure remains a major challenge, particularly in streaming scenarios. Motivated by this, we introduce Endless World, a real-time framework for infinite, 3D-consistent video generation.To support infinite video generation, we introduce a conditional autoregressive training strategy that aligns newly generated content with existing video frames. This design preserves long-range dependencies while remaining computationally efficient, enabling real-time inference on a single GPU without additional training overhead.Moreover, our Endless World integrates global 3D-aware attention to provide continuous geometric guidance across time. Our 3D injection mechanism enforces physical plausibility and geometric consistency throughout extended sequences, addressing key challenges in long-horizon and dynamic scene synthesis.Extensive experiments demonstrate that Endless World produces long, stable, and visually coherent videos, achieving competitive or superior performance to existing methods in both visual fidelity and spatial consistency. Our project has been available on https://bwgzk-keke.github.io/EndlessWorld/.

</details>


### [189] [Adaptive Detector-Verifier Framework for Zero-Shot Polyp Detection in Open-World Settings](https://arxiv.org/abs/2512.12492)
*Shengkai Xu,Hsiang Lun Kao,Tianxiang Xu,Honghui Zhang,Junqiao Wang,Runmeng Ding,Guanyu Liu,Tianyu Shi,Zhenyu Yu,Guofeng Pan,Ziqian Bi,Yuqi Ouyang*

Main category: cs.CV

Relevance: 35.0

TL;DR: 提出AdaptiveDetector，一個兩階段檢測器-驗證器框架，結合YOLOv11檢測器和視覺語言模型驗證器，透過自適應閾值調整和成本敏感強化學習，改善息肉檢測在真實內視鏡環境中的表現。


<details>
  <summary>Details</summary>
Motivation: 息肉檢測器在乾淨數據集上訓練後，在真實內視鏡環境中表現不佳，因為光照變化、運動模糊和遮擋會降低圖像質量。現有方法難以應對實驗室條件與臨床實踐之間的領域差距，特別是在惡劣成像條件下。

Method: 提出AdaptiveDetector兩階段框架：1) YOLOv11檢測器在VLM指導下自適應調整每幀置信度閾值；2) VLM驗證器使用Group Relative Policy Optimization (GRPO)進行微調，採用非對稱成本敏感獎勵函數，專門設計來減少漏檢。建立綜合合成測試平台，系統性地將乾淨數據集降解為臨床常見的惡劣條件。

Result: 在合成降解的CVC-ClinicDB和Kvasir-SEG圖像上進行零樣本評估，召回率比單獨使用YOLO提高14-22個百分點，精確度保持在基線的-0.7到+1.7個百分點範圍內。大幅減少假陰性，降低漏檢癌前息肉的風險。

Conclusion: 自適應閾值調整與成本敏感強化學習的結合，實現了臨床對齊的開放世界息肉檢測，顯著減少假陰性，改善患者預後。

Abstract: Polyp detectors trained on clean datasets often underperform in real-world endoscopy, where illumination changes, motion blur, and occlusions degrade image quality. Existing approaches struggle with the domain gap between controlled laboratory conditions and clinical practice, where adverse imaging conditions are prevalent. In this work, we propose AdaptiveDetector, a novel two-stage detector-verifier framework comprising a YOLOv11 detector with a vision-language model (VLM) verifier. The detector adaptively adjusts per-frame confidence thresholds under VLM guidance, while the verifier is fine-tuned with Group Relative Policy Optimization (GRPO) using an asymmetric, cost-sensitive reward function specifically designed to discourage missed detections -- a critical clinical requirement. To enable realistic assessment under challenging conditions, we construct a comprehensive synthetic testbed by systematically degrading clean datasets with adverse conditions commonly encountered in clinical practice, providing a rigorous benchmark for zero-shot evaluation. Extensive zero-shot evaluation on synthetically degraded CVC-ClinicDB and Kvasir-SEG images demonstrates that our approach improves recall by 14 to 22 percentage points over YOLO alone, while precision remains within 0.7 points below to 1.7 points above the baseline. This combination of adaptive thresholding and cost-sensitive reinforcement learning achieves clinically aligned, open-world polyp detection with substantially fewer false negatives, thereby reducing the risk of missed precancerous polyps and improving patient outcomes.

</details>


### [190] [StegaVAR: Privacy-Preserving Video Action Recognition via Steganographic Domain Analysis](https://arxiv.org/abs/2512.12586)
*Lixin Chen,Chaomeng Chen,Jiale Zhou,Zhijian Wu,Xun Lin*

Main category: cs.CV

Relevance: 35.0

TL;DR: StegaVAR：首个在隐写域直接执行视频动作识别的隐私保护框架，将动作视频嵌入普通封面视频中，保持时空信息完整的同时确保传输隐蔽性


<details>
  <summary>Details</summary>
Motivation: 当前隐私保护方法存在两个主要问题：1) 隐蔽性低，产生视觉扭曲的视频在传输中吸引攻击者注意；2) 时空破坏，降低准确VAR所需的时空特征。需要同时保护隐私和保持视频分析质量

Method: 提出StegaVAR框架，将动作视频嵌入普通封面视频，在隐写域直接执行VAR。提出两个关键技术：1) Secret Spatio-Temporal Promotion (STeP)：在训练中使用秘密视频指导隐写域的时空特征提取；2) Cross-Band Difference Attention (CroDA)：通过捕捉跨频带语义差异来抑制封面干扰

Result: 实验表明StegaVAR在广泛使用的数据集上实现了优越的视频动作识别和隐私保护性能。该框架对多种隐写模型都有效

Conclusion: StegaVAR首次实现在隐写域的视频动作识别，解决了隐私保护和视频分析质量之间的权衡问题，为隐私敏感的VAR应用提供了有效解决方案

Abstract: Despite the rapid progress of deep learning in video action recognition (VAR) in recent years, privacy leakage in videos remains a critical concern. Current state-of-the-art privacy-preserving methods often rely on anonymization. These methods suffer from (1) low concealment, where producing visually distorted videos that attract attackers' attention during transmission, and (2) spatiotemporal disruption, where degrading essential spatiotemporal features for accurate VAR. To address these issues, we propose StegaVAR, a novel framework that embeds action videos into ordinary cover videos and directly performs VAR in the steganographic domain for the first time. Throughout both data transmission and action analysis, the spatiotemporal information of hidden secret video remains complete, while the natural appearance of cover videos ensures the concealment of transmission. Considering the difficulty of steganographic domain analysis, we propose Secret Spatio-Temporal Promotion (STeP) and Cross-Band Difference Attention (CroDA) for analysis within the steganographic domain. STeP uses the secret video to guide spatiotemporal feature extraction in the steganographic domain during training. CroDA suppresses cover interference by capturing cross-band semantic differences. Experiments demonstrate that StegaVAR achieves superior VAR and privacy-preserving performance on widely used datasets. Moreover, our framework is effective for multiple steganographic models.

</details>


### [191] [Content-Aware Ad Banner Layout Generation with Two-Stage Chain-of-Thought in Vision Language Models](https://arxiv.org/abs/2512.12596)
*Kei Yoshitake,Kento Hosono,Ken Kobayashi,Kazuhide Nakata*

Main category: cs.CV

Relevance: 35.0

TL;DR: 提出一种基于视觉语言模型(VLM)的广告布局生成方法，通过分析背景图像内容生成文本放置计划，再转换为HTML代码，相比传统显著性映射方法能生成更高质量的广告布局。


<details>
  <summary>Details</summary>
Motivation: 传统广告布局技术主要依赖显著性映射来检测背景图像中的显著区域，但这种方法往往无法充分考虑图像的详细构图和语义内容。需要一种能更好理解图像内容并据此优化文本和logo放置的方法。

Method: 采用两阶段流程：1) VLM分析图像识别物体类型和空间关系，生成基于文本的"放置计划"；2) 将该计划渲染为HTML格式的最终布局。通过VLM理解图像语义内容来指导布局设计。

Result: 通过定量和定性评估实验验证了方法的有效性，与现有方法相比，通过显式考虑背景图像内容，能生成明显更高质量的广告布局。

Conclusion: 利用VLM理解图像语义内容的方法在广告布局生成中优于传统显著性映射方法，能产生更高质量的布局设计。

Abstract: In this paper, we propose a method for generating layouts for image-based advertisements by leveraging a Vision-Language Model (VLM). Conventional advertisement layout techniques have predominantly relied on saliency mapping to detect salient regions within a background image, but such approaches often fail to fully account for the image's detailed composition and semantic content. To overcome this limitation, our method harnesses a VLM to recognize the products and other elements depicted in the background and to inform the placement of text and logos. The proposed layout-generation pipeline consists of two steps. In the first step, the VLM analyzes the image to identify object types and their spatial relationships, then produces a text-based "placement plan" based on this analysis. In the second step, that plan is rendered into the final layout by generating HTML-format code. We validated the effectiveness of our approach through evaluation experiments, conducting both quantitative and qualitative comparisons against existing methods. The results demonstrate that by explicitly considering the background image's content, our method produces noticeably higher-quality advertisement layouts.

</details>


### [192] [Geometry-Aware Scene-Consistent Image Generation](https://arxiv.org/abs/2512.12598)
*Cong Xie,Che Wang,Yan Zhang,Zheng Pan,Han Zou,Zhenpeng Zhan*

Main category: cs.CV

Relevance: 35.0

TL;DR: 提出几何感知的场景一致图像生成方法，通过场景一致数据构建和几何引导注意力损失，在保持参考场景物理环境的同时根据空间关系生成新实体


<details>
  <summary>Details</summary>
Motivation: 现有方法在场景保持和提示遵循之间存在权衡：要么高度忠实于场景但响应提示差，要么优先遵循提示但牺牲场景一致性。需要解决这一平衡问题，实现既保持场景物理环境又正确生成指定实体的图像生成

Method: 1) 场景一致数据构建管道，生成多样化、几何基础训练对；2) 新颖的几何引导注意力损失，利用跨视图线索正则化模型的空间推理

Result: 在场景一致基准测试中，相比最先进基线方法，在场景对齐和文本图像一致性方面表现更好，自动指标和人类偏好研究均证实。生成几何连贯、构图多样且忠实于文本指令和底层场景结构的图像

Conclusion: 提出的方法成功解决了场景保持与提示遵循之间的权衡问题，实现了几何感知的场景一致图像生成，在保持参考场景物理环境的同时正确生成符合空间关系描述的实体

Abstract: We study geometry-aware scene-consistent image generation: given a reference scene image and a text condition specifying an entity to be generated in the scene and its spatial relation to the scene, the goal is to synthesize an output image that preserves the same physical environment as the reference scene while correctly generating the entity according to the spatial relation described in the text. Existing methods struggle to balance scene preservation with prompt adherence: they either replicate the scene with high fidelity but poor responsiveness to the prompt, or prioritize prompt compliance at the expense of scene consistency. To resolve this trade-off, we introduce two key contributions: (i) a scene-consistent data construction pipeline that generates diverse, geometrically-grounded training pairs, and (ii) a novel geometry-guided attention loss that leverages cross-view cues to regularize the model's spatial reasoning. Experiments on our scene-consistent benchmark show that our approach achieves better scene alignment and text-image consistency than state-of-the-art baselines, according to both automatic metrics and human preference studies. Our method produces geometrically coherent images with diverse compositions that remain faithful to the textual instructions and the underlying scene structure.

</details>


### [193] [Anatomy-Guided Representation Learning Using a Transformer-Based Network for Thyroid Nodule Segmentation in Ultrasound Images](https://arxiv.org/abs/2512.12662)
*Muhammad Umar Farooq,Abd Ur Rehman,Azka Rehman,Muhammad Usman,Dong-Kyu Chae,Junaid Qadir*

Main category: cs.CV

Relevance: 35.0

TL;DR: SSMT-Net：基于半监督多任务Transformer的甲状腺结节超声图像分割网络，通过无监督预训练和有监督多任务学习提升分割性能


<details>
  <summary>Details</summary>
Motivation: 甲状腺结节超声图像分割面临边界模糊、尺寸变化大、标注数据稀缺等挑战，现有深度学习模型难以有效整合甲状腺腺体上下文信息并泛化到多样病例

Method: 提出SSMT-Net：1）无监督阶段利用未标注数据增强Transformer编码器特征提取能力；2）有监督阶段联合优化结节分割、腺体分割和结节尺寸估计三个任务，整合局部和全局上下文特征

Result: 在TN3K和DDTI数据集上的广泛评估表明，SSMT-Net优于现有最先进方法，具有更高的准确性和鲁棒性

Conclusion: SSMT-Net在甲状腺结节超声图像分割中表现出色，具有实际临床应用潜力

Abstract: Accurate thyroid nodule segmentation in ultrasound images is critical for diagnosis and treatment planning. However, ambiguous boundaries between nodules and surrounding tissues, size variations, and the scarcity of annotated ultrasound data pose significant challenges for automated segmentation. Existing deep learning models struggle to incorporate contextual information from the thyroid gland and generalize effectively across diverse cases. To address these challenges, we propose SSMT-Net, a Semi-Supervised Multi-Task Transformer-based Network that leverages unlabeled data to enhance Transformer-centric encoder feature extraction capability in an initial unsupervised phase. In the supervised phase, the model jointly optimizes nodule segmentation, gland segmentation, and nodule size estimation, integrating both local and global contextual features. Extensive evaluations on the TN3K and DDTI datasets demonstrate that SSMT-Net outperforms state-of-the-art methods, with higher accuracy and robustness, indicating its potential for real-world clinical applications.

</details>


### [194] [InteracTalker: Prompt-Based Human-Object Interaction with Co-Speech Gesture Generation](https://arxiv.org/abs/2512.12664)
*Sreehari Rajan,Kunal Bhosikar,Charu Sharma*

Main category: cs.CV

Relevance: 35.0

TL;DR: InteracTalker：一个统一框架，通过多阶段训练学习统一运动、语音和提示嵌入空间，同时生成语音驱动手势和物体交互动作，解决现有方法独立处理这两个任务的局限性。


<details>
  <summary>Details</summary>
Motivation: 当前方法独立处理语音驱动手势和物体交互，缺乏集成数据集，限制了真实世界应用。需要能够同时响应语言和物理对象的自然人体运动生成方法。

Method: 1) 多阶段训练学习统一运动、语音和提示嵌入空间；2) 构建丰富的人-物交互数据集；3) 使用广义运动适应模块进行独立训练；4) 提出自适应融合策略解决异质条件信号不平衡问题；5) 基于扩散模型动态重加权条件信号。

Result: 在语音手势生成和物体交互合成任务上均优于先前方法，超越了专注于手势的扩散方法，生成具有增强真实感、灵活性和控制性的物体感知全身运动。

Conclusion: InteracTalker成功统一了先前分离的任务，实现了高度真实、物体感知的全身运动生成，为交互式数字体验提供了更自然的解决方案。

Abstract: Generating realistic human motions that naturally respond to both spoken language and physical objects is crucial for interactive digital experiences. Current methods, however, address speech-driven gestures or object interactions independently, limiting real-world applicability due to a lack of integrated, comprehensive datasets. To overcome this, we introduce InteracTalker, a novel framework that seamlessly integrates prompt-based object-aware interactions with co-speech gesture generation. We achieve this by employing a multi-stage training process to learn a unified motion, speech, and prompt embedding space. To support this, we curate a rich human-object interaction dataset, formed by augmenting an existing text-to-motion dataset with detailed object interaction annotations. Our framework utilizes a Generalized Motion Adaptation Module that enables independent training, adapting to the corresponding motion condition, which is then dynamically combined during inference. To address the imbalance between heterogeneous conditioning signals, we propose an adaptive fusion strategy, which dynamically reweights the conditioning signals during diffusion sampling. InteracTalker successfully unifies these previously separate tasks, outperforming prior methods in both co-speech gesture generation and object-interaction synthesis, outperforming gesture-focused diffusion methods, yielding highly realistic, object-aware full-body motions with enhanced realism, flexibility, and control.

</details>


### [195] [Open-World Deepfake Attribution via Confidence-Aware Asymmetric Learning](https://arxiv.org/abs/2512.12667)
*Haiyang Zheng,Nan Pu,Wenjing Li,Teng Long,Nicu Sebe,Zhun Zhong*

Main category: cs.CV

Relevance: 35.0

TL;DR: 提出CAL框架解决开放世界深度伪造溯源问题，通过置信度感知不对称学习和动态原型剪枝，提升对已知和未知伪造类型的识别能力


<details>
  <summary>Details</summary>
Motivation: 现有开放世界深度伪造溯源方法存在两个关键限制：1) 置信度偏斜导致对未知伪造类型的伪标签不可靠，造成训练偏差；2) 不现实地假设未知伪造类型的数量是已知的

Method: 提出置信度感知不对称学习框架，包含置信度感知一致性正则化和不对称置信度增强两个组件，以及动态原型剪枝策略自动估计未知伪造类型数量

Result: 在标准开放世界深度伪造溯源基准和新扩展基准上，CAL方法持续优于先前方法，在已知和未知伪造溯源上都达到了新的最先进性能

Conclusion: CAL框架通过解决置信度偏斜和移除对未知类型数量的先验假设，显著提升了开放世界深度伪造溯源的性能，增强了方法在真实场景中的可扩展性

Abstract: The proliferation of synthetic facial imagery has intensified the need for robust Open-World DeepFake Attribution (OW-DFA), which aims to attribute both known and unknown forgeries using labeled data for known types and unlabeled data containing a mixture of known and novel types. However, existing OW-DFA methods face two critical limitations: 1) A confidence skew that leads to unreliable pseudo-labels for novel forgeries, resulting in biased training. 2) An unrealistic assumption that the number of unknown forgery types is known *a priori*. To address these challenges, we propose a Confidence-Aware Asymmetric Learning (CAL) framework, which adaptively balances model confidence across known and novel forgery types. CAL mainly consists of two components: Confidence-Aware Consistency Regularization (CCR) and Asymmetric Confidence Reinforcement (ACR). CCR mitigates pseudo-label bias by dynamically scaling sample losses based on normalized confidence, gradually shifting the training focus from high- to low-confidence samples. ACR complements this by separately calibrating confidence for known and novel classes through selective learning on high-confidence samples, guided by their confidence gap. Together, CCR and ACR form a mutually reinforcing loop that significantly improves the model's OW-DFA performance. Moreover, we introduce a Dynamic Prototype Pruning (DPP) strategy that automatically estimates the number of novel forgery types in a coarse-to-fine manner, removing the need for unrealistic prior assumptions and enhancing the scalability of our methods to real-world OW-DFA scenarios. Extensive experiments on the standard OW-DFA benchmark and a newly extended benchmark incorporating advanced manipulations demonstrate that CAL consistently outperforms previous methods, achieving new state-of-the-art performance on both known and novel forgery attribution.

</details>


### [196] [Scone: Bridging Composition and Distinction in Subject-Driven Image Generation via Unified Understanding-Generation Modeling](https://arxiv.org/abs/2512.12675)
*Yuran Wang,Bohan Zeng,Chengzhuo Tong,Wenxuan Liu,Yang Shi,Xiaochen Ma,Hao Liang,Yuanxing Zhang,Wentao Zhang*

Main category: cs.CV

Relevance: 35.0

TL;DR: Scone是一个统一的图像生成方法，结合了构图和区分能力，通过理解专家作为语义桥梁，在包含多个候选主体的复杂场景中准确识别和生成正确的主体。


<details>
  <summary>Details</summary>
Motivation: 当前主题驱动的图像生成方法主要关注从单主体到多主体构图，但忽视了"区分"能力——即在输入包含多个候选主体时准确识别和生成正确主体的能力。这一限制影响了在复杂现实视觉场景中的有效性。

Method: 提出Scone方法，集成理解专家和生成专家，理解专家作为语义桥梁传递语义信息并指导生成专家保持主体身份同时最小化干扰。采用两阶段训练方案：先学习构图，然后通过语义对齐和基于注意力的掩码增强区分能力。

Result: Scone在两个基准测试中优于现有的开源模型，在构图和区分任务上都表现出色。同时提出了SconeEval评估基准，用于评估多样化场景中的构图和区分能力。

Conclusion: Scone通过统一理解和生成方法，有效解决了多主体图像生成中的区分问题，为复杂视觉场景中的主题驱动生成提供了更强大的解决方案。

Abstract: Subject-driven image generation has advanced from single- to multi-subject composition, while neglecting distinction, the ability to identify and generate the correct subject when inputs contain multiple candidates. This limitation restricts effectiveness in complex, realistic visual settings. We propose Scone, a unified understanding-generation method that integrates composition and distinction. Scone enables the understanding expert to act as a semantic bridge, conveying semantic information and guiding the generation expert to preserve subject identity while minimizing interference. A two-stage training scheme first learns composition, then enhances distinction through semantic alignment and attention-based masking. We also introduce SconeEval, a benchmark for evaluating both composition and distinction across diverse scenarios. Experiments demonstrate that Scone outperforms existing open-source models in composition and distinction tasks on two benchmarks. Our model, benchmark, and training data are available at: https://github.com/Ryann-Ran/Scone.

</details>


### [197] [Robust Motion Generation using Part-level Reliable Data from Videos](https://arxiv.org/abs/2512.12703)
*Boyuan Li,Sipeng Zheng,Bin Cao,Ruihua Song,Zongqing Lu*

Main category: cs.CV

Relevance: 35.0

TL;DR: 提出了一种基于部分感知掩码自回归模型的方法，利用视频中可信的人体部分数据来增强运动生成，解决了视频中人体部分遮挡或离屏带来的数据质量问题，并贡献了包含约20万真实世界运动序列的新基准K700-M。


<details>
  <summary>Details</summary>
Motivation: 从大规模网络视频中提取人体运动可以解决角色动画中的数据稀缺问题，但许多视频帧中的人体部分由于离屏拍摄或遮挡而不可见。这带来了一个困境：丢弃任何部分缺失的数据会限制规模和多样性，而保留这些数据则会损害数据质量和模型性能。

Method: 1) 将人体分解为五个部分，检测视频帧中清晰可见的部分作为"可信"部分；2) 通过提出的部分感知变分自编码器将可信部分编码为潜在标记；3) 提出鲁棒的部分级掩码生成模型来预测被掩码的可信部分，同时忽略噪声部分。

Result: 实验结果表明，该方法在干净和噪声数据集上均成功超越了基线方法，在运动质量、语义一致性和多样性方面表现更优。

Conclusion: 该方法通过利用视频中的可信部分数据，有效解决了人体运动提取中的数据质量问题，为大规模视频数据的高质量运动生成提供了可行方案。

Abstract: Extracting human motion from large-scale web videos offers a scalable solution to the data scarcity issue in character animation. However, some human parts in many video frames cannot be seen due to off-screen captures or occlusions. It brings a dilemma: discarding the data missing any part limits scale and diversity, while retaining it compromises data quality and model performance.
  To address this problem, we propose leveraging credible part-level data extracted from videos to enhance motion generation via a robust part-aware masked autoregression model. First, we decompose a human body into five parts and detect the parts clearly seen in a video frame as "credible". Second, the credible parts are encoded into latent tokens by our proposed part-aware variational autoencoder. Third, we propose a robust part-level masked generation model to predict masked credible parts, while ignoring those noisy parts.
  In addition, we contribute K700-M, a challenging new benchmark comprising approximately 200k real-world motion sequences, for evaluation. Experimental results indicate that our method successfully outperforms baselines on both clean and noisy datasets in terms of motion quality, semantic consistency and diversity. Project page: https://boyuaner.github.io/ropar-main/

</details>


### [198] [GenieDrive: Towards Physics-Aware Driving World Model with 4D Occupancy Guided Video Generation](https://arxiv.org/abs/2512.12751)
*Zhenya Yang,Zhe Liu,Yuxiang Lu,Liping Hou,Chenxuan Miao,Siyi Peng,Bailan Feng,Xiang Bai,Hengshuang Zhao*

Main category: cs.CV

Relevance: 35.0

TL;DR: GenieDrive是一个用于物理感知驾驶视频生成的新框架，通过生成4D占据作为物理信息基础，使用VAE压缩为三平面表示，并引入互控注意力来建模控制对占据演化的影响，显著提升了驾驶视频生成的物理一致性和质量。


<details>
  <summary>Details</summary>
Motivation: 现有驾驶世界模型通常使用单一扩散模型直接将驾驶动作映射到视频，这使得学习困难且导致物理不一致的输出。需要一种能够生成物理感知驾驶视频的方法，以支持驾驶规划、分布外数据合成和闭环评估。

Method: 1. 首先生成4D占据作为物理信息基础；2. 提出VAE将高分辨率占据编码为潜在三平面表示，将潜在大小减少到先前方法的58%；3. 引入互控注意力（MCA）精确建模控制对占据演化的影响；4. 端到端联合训练VAE和预测模块；5. 在视频生成模型中使用归一化多视图注意力，从4D占据引导生成多视图驾驶视频。

Result: 1. 预测mIoU提升7.2%，推理速度达41 FPS，仅使用3.47M参数；2. 视频质量显著改善，FVD降低20.7%；3. 实现了高度可控、多视图一致且物理感知的驾驶视频生成。

Conclusion: GenieDrive通过4D占据表示和创新的压缩与控制建模方法，成功解决了现有驾驶视频生成方法的物理不一致问题，为驾驶规划、数据合成和评估提供了有效的物理感知世界模型。

Abstract: Physics-aware driving world model is essential for drive planning, out-of-distribution data synthesis, and closed-loop evaluation. However, existing methods often rely on a single diffusion model to directly map driving actions to videos, which makes learning difficult and leads to physically inconsistent outputs. To overcome these challenges, we propose GenieDrive, a novel framework designed for physics-aware driving video generation. Our approach starts by generating 4D occupancy, which serves as a physics-informed foundation for subsequent video generation. 4D occupancy contains rich physical information, including high-resolution 3D structures and dynamics. To facilitate effective compression of such high-resolution occupancy, we propose a VAE that encodes occupancy into a latent tri-plane representation, reducing the latent size to only 58% of that used in previous methods. We further introduce Mutual Control Attention (MCA) to accurately model the influence of control on occupancy evolution, and we jointly train the VAE and the subsequent prediction module in an end-to-end manner to maximize forecasting accuracy. Together, these designs yield a 7.2% improvement in forecasting mIoU at an inference speed of 41 FPS, while using only 3.47 M parameters. Additionally, a Normalized Multi-View Attention is introduced in the video generation model to generate multi-view driving videos with guidance from our 4D occupancy, significantly improving video quality with a 20.7% reduction in FVD. Experiments demonstrate that GenieDrive enables highly controllable, multi-view consistent, and physics-aware driving video generation.

</details>


### [199] [Fast 2DGS: Efficient Image Representation with Deep Gaussian Prior](https://arxiv.org/abs/2512.12774)
*Hao Wang,Ashish Bastola,Chaoyi Zhou,Wenhui Zhu,Xiwen Chen,Xuanzhao Dong,Siyu Huang,Abolfazl Razi*

Main category: cs.CV

Relevance: 35.0

TL;DR: Fast-2DGS提出轻量级框架，通过深度高斯先验和属性回归网络实现高效高斯图像表示，单次前向传播即可获得高质量重建，显著降低计算成本。


<details>
  <summary>Details</summary>
Motivation: 随着生成模型能够产生高质量视觉内容，对高效、可解释、可编辑的图像表示需求增加。现有2D高斯泼溅方法需要后优化，收敛慢（>10秒），而学习网络方法增加了计算和架构复杂性。

Method: 提出Fast-2DGS框架：1) 深度高斯先验作为条件网络捕捉不同复杂度下的高斯基元空间分布；2) 属性回归网络预测密集高斯属性。这种解耦架构实现单次前向传播高质量重建，只需最小微调。

Result: 实验表明该方法在单次前向传播后即可获得高质量重建，显著降低计算成本而不影响视觉质量，使2DGS更接近工业级部署。

Conclusion: Fast-2DGS通过轻量级解耦架构解决了2D高斯泼溅的效率问题，平衡了质量与计算成本，为工业应用提供了可行方案。

Abstract: As generative models become increasingly capable of producing high-fidelity visual content, the demand for efficient, interpretable, and editable image representations has grown substantially. Recent advances in 2D Gaussian Splatting (2DGS) have emerged as a promising solution, offering explicit control, high interpretability, and real-time rendering capabilities (>1000 FPS). However, high-quality 2DGS typically requires post-optimization. Existing methods adopt random or heuristics (e.g., gradient maps), which are often insensitive to image complexity and lead to slow convergence (>10s). More recent approaches introduce learnable networks to predict initial Gaussian configurations, but at the cost of increased computational and architectural complexity. To bridge this gap, we present Fast-2DGS, a lightweight framework for efficient Gaussian image representation. Specifically, we introduce Deep Gaussian Prior, implemented as a conditional network to capture the spatial distribution of Gaussian primitives under different complexities. In addition, we propose an attribute regression network to predict dense Gaussian properties. Experiments demonstrate that this disentangled architecture achieves high-quality reconstruction in a single forward pass, followed by minimal fine-tuning. More importantly, our approach significantly reduces computational cost without compromising visual quality, bringing 2DGS closer to industry-ready deployment.

</details>


### [200] [Learning Common and Salient Generative Factors Between Two Image Datasets](https://arxiv.org/abs/2512.12800)
*Yunlong He,Gwilherm Lesné,Ziqian Liu,Michaël Soumm,Pietro Gori*

Main category: cs.CV

Relevance: 35.0

TL;DR: 提出了一种用于对比分析（CA）的新框架，能够从两个图像数据集中分离出共享的生成因子和特定于单个数据集的显著因子，适用于GAN和扩散模型。


<details>
  <summary>Details</summary>
Motivation: 现有图像合成研究主要关注条件操作和分离表示学习，但较少研究对比分析问题。对比分析旨在从两个数据集中分离出共同生成因子和显著生成因子，仅使用数据集信号而非属性监督，这是一个较弱但更通用的设定。

Method: 提出了一个适用于GAN和扩散模型的新框架，通过定义新的学习策略和损失函数，确保共同因子和显著因子的有效分离，同时保持高质量的图像生成。

Result: 在涵盖人脸、动物图像和医学扫描的多样化数据集上评估，相比现有方法，该框架展示了更优的分离能力和图像合成质量。

Conclusion: 该工作为对比分析问题提供了一个有效的解决方案，能够仅使用数据集信号分离共同和显著生成因子，在多种图像类型上表现优异。

Abstract: Recent advancements in image synthesis have enabled high-quality image generation and manipulation. Most works focus on: 1) conditional manipulation, where an image is modified conditioned on a given attribute, or 2) disentangled representation learning, where each latent direction should represent a distinct semantic attribute. In this paper, we focus on a different and less studied research problem, called Contrastive Analysis (CA). Given two image datasets, we want to separate the common generative factors, shared across the two datasets, from the salient ones, specific to only one dataset. Compared to existing methods, which use attributes as supervised signals for editing (e.g., glasses, gender), the proposed method is weaker, since it only uses the dataset signal. We propose a novel framework for CA, that can be adapted to both GAN and Diffusion models, to learn both common and salient factors. By defining new and well-adapted learning strategies and losses, we ensure a relevant separation between common and salient factors, preserving a high-quality generation. We evaluate our approach on diverse datasets, covering human faces, animal images and medical scans. Our framework demonstrates superior separation ability and image quality synthesis compared to prior methods.

</details>


### [201] [Schrodinger Audio-Visual Editor: Object-Level Audiovisual Removal](https://arxiv.org/abs/2512.12875)
*Weihan Xu,Kan Jen Cheng,Koichi Saito,Muhammad Jehanzeb Mirza,Tingle Li,Yisi Liu,Alexander H. Liu,Liming Wang,Masato Ishii,Takashi Shibuya,Yuki Mitsufuji,Gopala Anumanchipalli,Paul Pu Liang*

Main category: cs.CV

Relevance: 35.0

TL;DR: SAVE模型是一个端到端的流匹配模型，用于联合编辑音频和视觉内容，通过Schrodinger Bridge学习从源到目标视听混合的直接传输，在保持内容对齐的同时移除目标对象。


<details>
  <summary>Details</summary>
Motivation: 联合编辑音频和视觉内容对于精确可控的内容创作至关重要，但面临配对视听数据有限和模态异质性等挑战。

Method: 提出SAVEBench配对视听数据集，并训练Schrodinger Audio-Visual Editor (SAVE)模型，这是一个端到端的流匹配模型，使用Schrodinger Bridge学习从源到目标视听混合的直接传输。

Result: SAVE模型能够有效移除音频和视觉内容中的目标对象，同时保持剩余内容，相比音频编辑器和视频编辑器的组合方法，具有更强的时间同步性和视听语义对应性。

Conclusion: 提出的SAVE模型解决了联合视听编辑的数据和建模挑战，实现了更好的内容对齐和编辑质量。

Abstract: Joint editing of audio and visual content is crucial for precise and controllable content creation. This new task poses challenges due to the limitations of paired audio-visual data before and after targeted edits, and the heterogeneity across modalities. To address the data and modeling challenges in joint audio-visual editing, we introduce SAVEBench, a paired audiovisual dataset with text and mask conditions to enable object-grounded source-to-target learning. With SAVEBench, we train the Schrodinger Audio-Visual Editor (SAVE), an end-to-end flow-matching model that edits audio and video in parallel while keeping them aligned throughout processing. SAVE incorporates a Schrodinger Bridge that learns a direct transport from source to target audiovisual mixtures. Our evaluation demonstrates that the proposed SAVE model is able to remove the target objects in audio and visual content while preserving the remaining content, with stronger temporal synchronization and audiovisual semantic correspondence compared with pairwise combinations of an audio editor and a video editor.

</details>


### [202] [Cross-Level Sensor Fusion with Object Lists via Transformer for 3D Object Detection](https://arxiv.org/abs/2512.12884)
*Xiangzhong Liu,Jiajie Zhang,Hao Shen*

Main category: cs.CV

Relevance: 35.0

TL;DR: 提出基于Transformer的端到端跨层级融合方法，将高度抽象的目标列表信息与原始相机图像结合进行3D目标检测，在nuScenes数据集上显著超越视觉基线


<details>
  <summary>Details</summary>
Motivation: 汽车传感器融合系统中，智能传感器和V2X模块通常只提供处理后的目标列表而非原始数据。现有方法通常分别处理原始数据然后在目标层级融合，缺乏端到端的跨层级融合方案

Method: 1) 将目标列表作为去噪查询输入Transformer，与可学习查询一起传播；2) 从目标列表的位置和尺寸先验中推导可变形高斯掩码，集成到Transformer解码器中引导注意力；3) 提出从真实边界框生成伪目标列表的方法，模拟状态噪声和误检漏检

Result: 在nuScenes数据集上相比视觉基线有显著性能提升，展示了在不同噪声水平的模拟目标列表和真实检测器上的泛化能力

Conclusion: 首次实现了跨层级融合，有效整合抽象目标列表和原始图像信息，通过注意力引导机制加速训练收敛，为汽车传感器融合提供了新思路

Abstract: In automotive sensor fusion systems, smart sensors and Vehicle-to-Everything (V2X) modules are commonly utilized. Sensor data from these systems are typically available only as processed object lists rather than raw sensor data from traditional sensors. Instead of processing other raw data separately and then fusing them at the object level, we propose an end-to-end cross-level fusion concept with Transformer, which integrates highly abstract object list information with raw camera images for 3D object detection. Object lists are fed into a Transformer as denoising queries and propagated together with learnable queries through the latter feature aggregation process. Additionally, a deformable Gaussian mask, derived from the positional and size dimensional priors from the object lists, is explicitly integrated into the Transformer decoder. This directs attention toward the target area of interest and accelerates model training convergence. Furthermore, as there is no public dataset containing object lists as a standalone modality, we propose an approach to generate pseudo object lists from ground-truth bounding boxes by simulating state noise and false positives and negatives. As the first work to conduct cross-level fusion, our approach shows substantial performance improvements over the vision-based baseline on the nuScenes dataset. It demonstrates its generalization capability over diverse noise levels of simulated object lists and real detectors.

</details>


### [203] [Qonvolution: Towards Learning High-Frequency Signals with Queried Convolution](https://arxiv.org/abs/2512.12898)
*Abhinav Kumar,Tristan Aumentado-Armstrong,Lazar Valkov,Gopal Sharma,Alex Levinshtein,Radek Grzeszczuk,Suren Kumar*

Main category: cs.CV

Relevance: 35.0

TL;DR: 本文提出Queried-Convolutions (Qonvolutions)，一种通过卷积邻域特性增强高频信号学习的方法，在多种高频学习任务中表现出色，特别是在神经辐射场模型上实现了最先进的性能。


<details>
  <summary>Details</summary>
Motivation: 神经网络在处理高频信号时存在频谱偏差和优化困难的问题，现有方法如傅里叶编码虽有改进但仍有提升空间。本文旨在解决计算机视觉和图形学中高频信号学习的挑战。

Method: 提出Qonvolutions方法，将低频信号与查询（如坐标）进行卷积，利用卷积的邻域特性来增强复杂高频信号的学习能力。

Result: 在1D回归、2D超分辨率、2D图像回归和神经视图合成等多种高频学习任务中验证了Qonvolutions的有效性。特别是将高斯溅射与Qonvolutions结合用于神经视图合成，在真实世界复杂场景中实现了最先进的性能，甚至超越了强大的辐射场模型。

Conclusion: Qonvolutions是一种简单而强大的方法，能够有效解决神经网络学习高频信号的挑战，在计算机视觉和图形学的高频学习任务中具有广泛应用前景。

Abstract: Accurately learning high-frequency signals is a challenge in computer vision and graphics, as neural networks often struggle with these signals due to spectral bias or optimization difficulties. While current techniques like Fourier encodings have made great strides in improving performance, there remains scope for improvement when presented with high-frequency information. This paper introduces Queried-Convolutions (Qonvolutions), a simple yet powerful modification using the neighborhood properties of convolution. Qonvolution convolves a low-frequency signal with queries (such as coordinates) to enhance the learning of intricate high-frequency signals. We empirically demonstrate that Qonvolutions enhance performance across a variety of high-frequency learning tasks crucial to both the computer vision and graphics communities, including 1D regression, 2D super-resolution, 2D image regression, and novel view synthesis (NVS). In particular, by combining Gaussian splatting with Qonvolutions for NVS, we showcase state-of-the-art performance on real-world complex scenes, even outperforming powerful radiance field models on image quality.

</details>


### [204] [MADTempo: An Interactive System for Multi-Event Temporal Video Retrieval with Query Augmentation](https://arxiv.org/abs/2512.12929)
*Huu-An Vu,Van-Khanh Mai,Trong-Tam Nguyen,Quang-Duc Dam,Tien-Huy Nguyen,Thanh-Huong Le*

Main category: cs.CV

Relevance: 35.0

TL;DR: MADTempo是一个视频检索框架，结合了时序搜索和基于网络图像的视觉基础，用于处理多事件查询和未见过的视觉概念。


<details>
  <summary>Details</summary>
Motivation: 在线视频内容的快速增长需要能够理解复杂事件时序结构的检索系统。现有方法在建模跨多个事件的时序依赖和处理包含未见或罕见视觉概念的查询方面存在不足。

Method: 1) 时序搜索机制：通过聚合连续视频片段的相似度分数来捕捉事件级连续性；2) 基于Google图像搜索的回退模块：使用外部网络图像扩展查询表示，弥补预训练视觉嵌入的不足。

Result: 该框架提升了现代视频检索系统的时序推理和泛化能力，能够更有效地处理多事件查询和分布外查询。

Conclusion: MADTempo通过统一时序搜索和网络级视觉基础，为大规模视频语料库中更语义感知和自适应的检索铺平了道路。

Abstract: The rapid expansion of video content across online platforms has accelerated the need for retrieval systems capable of understanding not only isolated visual moments but also the temporal structure of complex events. Existing approaches often fall short in modeling temporal dependencies across multiple events and in handling queries that reference unseen or rare visual concepts. To address these challenges, we introduce MADTempo, a video retrieval framework developed by our team, AIO_Trinh, that unifies temporal search with web-scale visual grounding. Our temporal search mechanism captures event-level continuity by aggregating similarity scores across sequential video segments, enabling coherent retrieval of multi-event queries. Complementarily, a Google Image Search-based fallback module expands query representations with external web imagery, effectively bridging gaps in pretrained visual embeddings and improving robustness against out-of-distribution (OOD) queries. Together, these components advance the temporal reasoning and generalization capabilities of modern video retrieval systems, paving the way for more semantically aware and adaptive retrieval across large-scale video corpora.

</details>


### [205] [Unified Interactive Multimodal Moment Retrieval via Cascaded Embedding-Reranking and Temporal-Aware Score Fusion](https://arxiv.org/abs/2512.12935)
*Toan Le Ngo Thanh,Phat Ha Huu,Tan Nguyen Dang Duy,Thong Nguyen Le Minh,Anh Nguyen Nhu Tinh*

Main category: cs.CV

Relevance: 35.0

TL;DR: 提出统一多模态时刻检索系统，解决固定权重融合、时序建模和手动模态选择三大挑战，通过级联双嵌入、时序感知评分和智能体引导查询分解实现高效视频时刻检索。


<details>
  <summary>Details</summary>
Motivation: 视频内容爆炸式增长需要高效的多模态时刻检索系统，但现有方法面临三大挑战：固定权重融合策略无法处理跨模态噪声和模糊查询；时序建模难以捕捉连贯事件序列；系统需要手动模态选择降低可用性。

Method: 1. 级联双嵌入管道：结合BEIT-3和SigLIP进行广泛检索，通过BLIP-2重排序平衡召回率和精度。2. 时序感知评分机制：通过束搜索应用指数衰减惩罚，构建连贯事件序列而非孤立帧。3. 智能体引导查询分解：使用GPT-4o自动解释模糊查询，分解为模态特定子查询，执行自适应分数融合。

Result: 定性分析表明，系统能有效处理模糊查询，检索时序连贯序列，动态调整融合策略，提升交互式时刻搜索能力。

Conclusion: 提出的统一多模态时刻检索系统通过创新架构解决了现有方法的局限性，在视频内容检索领域具有重要应用价值。

Abstract: The exponential growth of video content has created an urgent need for efficient multimodal moment retrieval systems. However, existing approaches face three critical challenges: (1) fixed-weight fusion strategies fail across cross modal noise and ambiguous queries, (2) temporal modeling struggles to capture coherent event sequences while penalizing unrealistic gaps, and (3) systems require manual modality selection, reducing usability. We propose a unified multimodal moment retrieval system with three key innovations. First, a cascaded dual-embedding pipeline combines BEIT-3 and SigLIP for broad retrieval, refined by BLIP-2 based reranking to balance recall and precision. Second, a temporal-aware scoring mechanism applies exponential decay penalties to large temporal gaps via beam search, constructing coherent event sequences rather than isolated frames. Third, Agent-guided query decomposition (GPT-4o) automatically interprets ambiguous queries, decomposes them into modality specific sub-queries (visual/OCR/ASR), and performs adaptive score fusion eliminating manual modality selection. Qualitative analysis demonstrates that our system effectively handles ambiguous queries, retrieves temporally coherent sequences, and dynamically adapts fusion strategies, advancing interactive moment search capabilities.

</details>


### [206] [UAGLNet: Uncertainty-Aggregated Global-Local Fusion Network with Cooperative CNN-Transformer for Building Extraction](https://arxiv.org/abs/2512.12941)
*Siyuan Yao,Dongxiu Liu,Taotao Li,Shengjie Li,Wenqi Ren,Xiaochun Cao*

Main category: cs.CV

Relevance: 35.0

TL;DR: 提出UAGLNet网络，通过不确定性建模指导的全局-局部视觉语义融合，解决遥感图像建筑物提取中的结构复杂性和分割模糊问题


<details>
  <summary>Details</summary>
Motivation: 现有方法使用卷积或自注意力块捕获多尺度特征，但特征金字塔存在固有差距，全局-局部特征整合不足，导致建筑物提取结果不准确、模糊

Method: 1) 提出协作编码器，在不同阶段采用CNN和Transformer层分别捕获局部和全局语义；2) 设计中间协作交互块缩小深层网络中的局部-全局特征差距；3) 提出全局-局部融合模块互补融合特征；4) 提出不确定性聚合解码器显式估计像素级不确定性以增强分割精度

Result: 大量实验表明，该方法在遥感图像建筑物提取任务上优于其他最先进方法

Conclusion: UAGLNet通过不确定性建模指导的全局-局部视觉语义融合，有效解决了建筑物提取中的结构复杂性和分割模糊问题

Abstract: Building extraction from remote sensing images is a challenging task due to the complex structure variations of the buildings. Existing methods employ convolutional or self-attention blocks to capture the multi-scale features in the segmentation models, while the inherent gap of the feature pyramids and insufficient global-local feature integration leads to inaccurate, ambiguous extraction results. To address this issue, in this paper, we present an Uncertainty-Aggregated Global-Local Fusion Network (UAGLNet), which is capable to exploit high-quality global-local visual semantics under the guidance of uncertainty modeling. Specifically, we propose a novel cooperative encoder, which adopts hybrid CNN and transformer layers at different stages to capture the local and global visual semantics, respectively. An intermediate cooperative interaction block (CIB) is designed to narrow the gap between the local and global features when the network becomes deeper. Afterwards, we propose a Global-Local Fusion (GLF) module to complementarily fuse the global and local representations. Moreover, to mitigate the segmentation ambiguity in uncertain regions, we propose an Uncertainty-Aggregated Decoder (UAD) to explicitly estimate the pixel-wise uncertainty to enhance the segmentation accuracy. Extensive experiments demonstrate that our method achieves superior performance to other state-of-the-art methods. Our code is available at https://github.com/Dstate/UAGLNet

</details>


### [207] [SCAdapter: Content-Style Disentanglement for Diffusion Style Transfer](https://arxiv.org/abs/2512.12963)
*Luan Thanh Trinh,Kenji Doi,Atsuki Osanai*

Main category: cs.CV

Relevance: 35.0

TL;DR: SCAdapter是一种基于CLIP图像空间的扩散模型风格迁移方法，通过分离内容和风格特征实现逼真的风格转换，比现有方法更快更有效


<details>
  <summary>Details</summary>
Motivation: 当前扩散模型在风格迁移中存在两个主要问题：1）难以实现逼真的照片级风格迁移，往往产生绘画效果或丢失细节；2）现有方法无法有效处理原始内容风格和风格参考内容特征的不必要影响

Method: 提出SCAdapter方法，利用CLIP图像空间有效分离和整合内容与风格特征。包含三个核心组件：1）可控风格自适应实例归一化（CSAdaIN）用于精确的多风格混合；2）KVS注入用于目标风格集成；3）风格迁移一致性目标保持过程连贯性

Result: SCAdapter在传统和基于扩散的基线方法中都显著优于最先进的方法。通过消除DDIM反转和推理阶段优化，推理速度至少比其他基于扩散的方法快2倍

Conclusion: SCAdapter通过系统提取纯内容和风格元素，实现了更真实有效的风格迁移，同时具有更高的效率和实用性

Abstract: Diffusion models have emerged as the leading approach for style transfer, yet they struggle with photo-realistic transfers, often producing painting-like results or missing detailed stylistic elements. Current methods inadequately address unwanted influence from original content styles and style reference content features. We introduce SCAdapter, a novel technique leveraging CLIP image space to effectively separate and integrate content and style features. Our key innovation systematically extracts pure content from content images and style elements from style references, ensuring authentic transfers. This approach is enhanced through three components: Controllable Style Adaptive Instance Normalization (CSAdaIN) for precise multi-style blending, KVS Injection for targeted style integration, and a style transfer consistency objective maintaining process coherence. Comprehensive experiments demonstrate SCAdapter significantly outperforms state-of-the-art methods in both conventional and diffusion-based baselines. By eliminating DDIM inversion and inference-stage optimization, our method achieves at least $2\times$ faster inference than other diffusion-based approaches, making it both more effective and efficient for practical applications.

</details>


### [208] [Light Field Based 6DoF Tracking of Previously Unobserved Objects](https://arxiv.org/abs/2512.13007)
*Nikolai Goncharov,James L. Gray,Donald G. Dansereau*

Main category: cs.CV

Relevance: 35.0

TL;DR: 提出基于光场图像的物体跟踪方法LiFT-6DoF，无需预训练模型，通过视觉基础模型提取语义和几何特征，转换为视图相关的高斯泼溅表示，支持可微渲染和姿态优化，在反射物体等复杂场景中表现优异。


<details>
  <summary>Details</summary>
Motivation: 现有高性能物体跟踪方法依赖预捕获的物体视图构建显式参考模型，限制了只能跟踪已知物体集合，且在视觉复杂外观（如反射）下表现不佳。需要一种不依赖预训练模型且能处理复杂视觉行为的通用物体跟踪方法。

Method: 使用光场图像作为输入，通过视觉基础模型提取语义和几何特征，将其转换为视图相关的高斯泼溅表示。这种表示作为统一的物体表征，支持可微渲染和姿态优化。还构建了包含挑战性反射物体的光场物体跟踪数据集。

Result: 实验表明，在包含反射物体的困难案例中，该方法与最先进的基于模型的跟踪器性能相当。提出的方法为机器人系统中的通用物体跟踪开辟了新途径。

Conclusion: 基于光场图像和高斯泼溅表示的方法能够实现不依赖预训练模型的物体跟踪，对复杂视觉行为（如反射）具有鲁棒性，在机器人视觉系统中具有应用潜力。

Abstract: Object tracking is an important step in robotics and reautonomous driving pipelines, which has to generalize to previously unseen and complex objects. Existing high-performing methods often rely on pre-captured object views to build explicit reference models, which restricts them to a fixed set of known objects. However, such reference models can struggle with visually complex appearance, reducing the quality of tracking. In this work, we introduce an object tracking method based on light field images that does not depend on a pre-trained model, while being robust to complex visual behavior, such as reflections. We extract semantic and geometric features from light field inputs using vision foundation models and convert them into view-dependent Gaussian splats. These splats serve as a unified object representation, supporting differentiable rendering and pose optimization. We further introduce a light field object tracking dataset containing challenging reflective objects with precise ground truth poses. Experiments demonstrate that our method is competitive with state-of-the-art model-based trackers in these difficult cases, paving the way toward universal object tracking in robotic systems. Code/data available at https://github.com/nagonch/LiFT-6DoF.

</details>


### [209] [JoDiffusion: Jointly Diffusing Image with Pixel-Level Annotations for Semantic Segmentation Promotion](https://arxiv.org/abs/2512.13014)
*Haoyu Wang,Lei Zhang,Wenrui Liu,Dengyang Jiang,Wei Wei,Chen Ding*

Main category: cs.CV

Relevance: 35.0

TL;DR: JoDiffusion：一种用于语义分割的联合扩散框架，能够仅通过文本提示同时生成配对的图像和语义一致的标注掩码，解决了现有方法的图像-标注语义不一致和可扩展性问题。


<details>
  <summary>Details</summary>
Motivation: 像素级标注成本高且耗时，现有合成数据集方法要么需要在图像生成后预测伪标注，要么需要基于手动标注掩码生成图像，导致图像-标注语义不一致或可扩展性问题。

Method: 1) 在标准潜在扩散模型中引入独立的标注变分自编码器网络，将标注掩码映射到与图像共享的潜在空间；2) 定制扩散模型以捕获每个图像及其标注掩码在文本提示条件下的联合分布；3) 开发掩码优化策略以减少生成过程中的标注噪声。

Result: 在Pascal VOC、COCO和ADE20K数据集上的实验表明，JoDiffusion生成的标注数据集在语义分割任务上相比现有方法带来了显著的性能提升。

Conclusion: JoDiffusion能够仅通过文本提示同时生成语义一致的图像-标注对，解决了现有方法的局限性，为训练高性能语义分割模型提供了可扩展的解决方案。

Abstract: Given the inherently costly and time-intensive nature of pixel-level annotation, the generation of synthetic datasets comprising sufficiently diverse synthetic images paired with ground-truth pixel-level annotations has garnered increasing attention recently for training high-performance semantic segmentation models. However, existing methods necessitate to either predict pseudo annotations after image generation or generate images conditioned on manual annotation masks, which incurs image-annotation semantic inconsistency or scalability problem. To migrate both problems with one stone, we present a novel dataset generative diffusion framework for semantic segmentation, termed JoDiffusion. Firstly, given a standard latent diffusion model, JoDiffusion incorporates an independent annotation variational auto-encoder (VAE) network to map annotation masks into the latent space shared by images. Then, the diffusion model is tailored to capture the joint distribution of each image and its annotation mask conditioned on a text prompt. By doing these, JoDiffusion enables simultaneously generating paired images and semantically consistent annotation masks solely conditioned on text prompts, thereby demonstrating superior scalability. Additionally, a mask optimization strategy is developed to mitigate the annotation noise produced during generation. Experiments on Pascal VOC, COCO, and ADE20K datasets show that the annotated dataset generated by JoDiffusion yields substantial performance improvements in semantic segmentation compared to existing methods.

</details>


### [210] [SneakPeek: Future-Guided Instructional Streaming Video Generation](https://arxiv.org/abs/2512.13019)
*Cheeun Hong,German Barquero,Fadime Sener,Markos Georgopoulos,Edgar Schönfeld,Stefan Popov,Yuming Du,Oscar Mañas,Albert Pumarola*

Main category: cs.CV

Relevance: 35.0

TL;DR: SneakPeek是一个基于扩散的自回归框架，用于从文本描述生成连贯的教学视频，通过预测性因果适应、未来引导的自强制和多提示条件化来增强时间一致性和可控性。


<details>
  <summary>Details</summary>
Motivation: 现有视频扩散模型在生成长序列多步骤教学视频时难以保持时间一致性和可控性，而教学视频生成在内容创作、教育和人机交互方面具有重要应用价值。

Method: 提出SneakPeek框架，包含三个关键创新：1) 预测性因果适应，通过因果模型学习下一帧预测和未来关键帧预测；2) 未来引导的自强制，采用双区域KV缓存方案解决推理时的曝光偏差问题；3) 多提示条件化，提供对多步骤指令的细粒度程序控制。

Result: 实验结果表明，该方法能够生成时间连贯、语义忠实于复杂多步骤任务描述的教学视频，准确遵循结构化文本提示。

Conclusion: SneakPeek通过创新的预测机制和条件化技术，有效解决了教学视频生成中的时间一致性和可控性问题，支持交互式流媒体视频生成。

Abstract: Instructional video generation is an emerging task that aims to synthesize coherent demonstrations of procedural activities from textual descriptions. Such capability has broad implications for content creation, education, and human-AI interaction, yet existing video diffusion models struggle to maintain temporal consistency and controllability across long sequences of multiple action steps. We introduce a pipeline for future-driven streaming instructional video generation, dubbed SneakPeek, a diffusion-based autoregressive framework designed to generate precise, stepwise instructional videos conditioned on an initial image and structured textual prompts. Our approach introduces three key innovations to enhance consistency and controllability: (1) predictive causal adaptation, where a causal model learns to perform next-frame prediction and anticipate future keyframes; (2) future-guided self-forcing with a dual-region KV caching scheme to address the exposure bias issue at inference time; (3) multi-prompt conditioning, which provides fine-grained and procedural control over multi-step instructions. Together, these components mitigate temporal drift, preserve motion consistency, and enable interactive video generation where future prompt updates dynamically influence ongoing streaming video generation. Experimental results demonstrate that our method produces temporally coherent and semantically faithful instructional videos that accurately follow complex, multi-step task descriptions.

</details>


### [211] [Comprehensive Evaluation of Rule-Based, Machine Learning, and Deep Learning in Human Estimation Using Radio Wave Sensing: Accuracy, Spatial Generalization, and Output Granularity Trade-offs](https://arxiv.org/abs/2512.13031)
*Tomoya Tanaka,Tomonori Ikeda,Ryo Yonemoto*

Main category: cs.CV

Relevance: 35.0

TL;DR: 该研究首次全面比较了基于规则的方法、传统机器学习模型和深度学习模型在FMCW MIMO雷达无线电波感知中的性能，发现在训练环境中深度学习模型精度最高，但在新环境中基于规则的方法更稳定，揭示了空间泛化性能与输出粒度之间的权衡关系。


<details>
  <summary>Details</summary>
Motivation: 研究动机是系统比较不同方法在无线电波感知中的性能，特别是在室内人员检测场景中，评估各种方法在不同环境布局下的表现，探索模型容量、泛化能力和输出粒度之间的关系。

Method: 在两种不同布局的室内环境中，系统评估了五种方法：1）基于规则的连通分量方法；2）三种传统机器学习模型（k近邻、随机森林、支持向量机）；3）结合卷积神经网络和长短期记忆的深度学习模型。使用FMCW MIMO雷达进行数据采集和实验。

Result: 在训练环境中，CNN-LSTM模型达到最高精度，传统机器学习模型表现中等。但在新布局环境中，所有基于学习的方法性能显著下降，而基于规则的方法保持稳定。对于人员存在与否的二元检测，所有模型在不同布局中都能保持高精度。

Conclusion: 高容量模型能在相同环境中提供细粒度的高精度输出，但对领域偏移敏感；基于规则的方法虽然不能提供细粒度输出，但对领域偏移具有鲁棒性。无论模型类型如何，都存在空间泛化性能与输出粒度之间的明确权衡。

Abstract: This study presents the first comprehensive comparison of rule-based methods, traditional machine learning models, and deep learning models in radio wave sensing with frequency modulated continuous wave multiple input multiple output radar. We systematically evaluated five approaches in two indoor environments with distinct layouts: a rule-based connected component method; three traditional machine learning models, namely k-nearest neighbors, random forest, and support vector machine; and a deep learning model combining a convolutional neural network and long short term memory. In the training environment, the convolutional neural network long short term memory model achieved the highest accuracy, while traditional machine learning models provided moderate performance. In a new layout, however, all learning based methods showed significant degradation, whereas the rule-based method remained stable. Notably, for binary detection of presence versus absence of people, all models consistently achieved high accuracy across layouts. These results demonstrate that high capacity models can produce fine grained outputs with high accuracy in the same environment, but they are vulnerable to domain shift. In contrast, rule-based methods cannot provide fine grained outputs but exhibit robustness against domain shift. Moreover, regardless of the model type, a clear trade off was revealed between spatial generalization performance and output granularity.

</details>


### [212] [Towards Test-time Efficient Visual Place Recognition via Asymmetric Query Processing](https://arxiv.org/abs/2512.13055)
*Jaeyoon Kim,Yoonki Cho,Sung-Eui Yoon*

Main category: cs.CV

Relevance: 35.0

TL;DR: 提出了一种高效的非对称视觉地点识别框架，使用高容量模型离线提取特征，轻量级查询网络在线处理，通过地理记忆库和隐式嵌入增强技术解决异构网络兼容性问题


<details>
  <summary>Details</summary>
Motivation: 尽管DINOv2等高容量基础模型在视觉地点识别(VPR)中表现出色，但其巨大计算成本使得在资源受限设备上部署不切实际。需要一种既能保持高性能又能降低计算开销的解决方案。

Method: 1) 非对称VPR框架：高容量模型离线提取特征，轻量级查询网络在线处理；2) 地理记忆库：利用VPR数据库中的地理位置元数据结构化特征，避免昂贵的k-NN计算；3) 隐式嵌入增强：增强查询网络建模特征变化的能力

Result: 该方法不仅显著降低计算成本，而且在性能上超越了现有的非对称检索技术，为资源受限环境下的VPR建立了新的基准

Conclusion: 提出的高效非对称VPR框架通过地理记忆库和隐式嵌入增强技术，成功解决了异构网络兼容性问题，在保持高性能的同时大幅降低计算开销，适用于资源受限环境

Abstract: Visual Place Recognition (VPR) has advanced significantly with high-capacity foundation models like DINOv2, achieving remarkable performance. Nonetheless, their substantial computational cost makes deployment on resource-constrained devices impractical. In this paper, we introduce an efficient asymmetric VPR framework that incorporates a high-capacity gallery model for offline feature extraction with a lightweight query network for online processing. A key challenge in this setting is ensuring compatibility between these heterogeneous networks, which conventional approaches address through computationally expensive k-NN-based compatible training. To overcome this, we propose a geographical memory bank that structures gallery features using geolocation metadata inherent in VPR databases, eliminating the need for exhaustive k-NN computations. Additionally, we introduce an implicit embedding augmentation technique that enhances the query network to model feature variations despite its limited capacity. Extensive experiments demonstrate that our method not only significantly reduces computational costs but also outperforms existing asymmetric retrieval techniques, establishing a new aspect for VPR in resource-limited environments. The code is available at https://github.com/jaeyoon1603/AsymVPR

</details>


### [213] [DiRe: Diversity-promoting Regularization for Dataset Condensation](https://arxiv.org/abs/2512.13083)
*Saumyaranjan Mohanty,Aravind Reddy,Konda Reddy Mopuri*

Main category: cs.CV

Relevance: 35.0

TL;DR: 提出多样性正则化器(DiRe)用于数据集压缩，通过余弦相似度和欧氏距离减少合成数据集冗余，提升多样性，在多个基准数据集上改进现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有数据集压缩方法合成的数据集存在显著冗余，需要减少冗余并提升合成数据集的多样性。

Method: 提出直观的多样性正则化器(DiRe)，结合余弦相似度和欧氏距离，可作为即插即用组件应用于各种最先进的压缩方法。

Result: 在CIFAR-10到ImageNet-1K等多个基准数据集上，添加该正则化器改进了最先进压缩方法的泛化性能和多样性指标。

Conclusion: DiRe正则化器能有效减少数据集压缩中的冗余，提升合成数据集的多样性和质量。

Abstract: In Dataset Condensation, the goal is to synthesize a small dataset that replicates the training utility of a large original dataset. Existing condensation methods synthesize datasets with significant redundancy, so there is a dire need to reduce redundancy and improve the diversity of the synthesized datasets. To tackle this, we propose an intuitive Diversity Regularizer (DiRe) composed of cosine similarity and Euclidean distance, which can be applied off-the-shelf to various state-of-the-art condensation methods. Through extensive experiments, we demonstrate that the addition of our regularizer improves state-of-the-art condensation methods on various benchmark datasets from CIFAR-10 to ImageNet-1K with respect to generalization and diversity metrics.

</details>


### [214] [Diffusion-Based Restoration for Multi-Modal 3D Object Detection in Adverse Weather](https://arxiv.org/abs/2512.13107)
*Zhijian He,Feifei Liu,Yuwei Li,Zhanpeng Liu,Jintao Cheng,Xieyuanli Chen,Xiaoyu Tang*

Main category: cs.CV

Relevance: 35.0

TL;DR: DiffFusion：一种基于扩散模型的多模态3D目标检测框架，通过扩散修复和自适应跨模态融合增强恶劣天气下的鲁棒性


<details>
  <summary>Details</summary>
Motivation: 多模态3D目标检测在恶劣天气条件下效果受限，主要原因是天气引起的图像失真和不同数据模态之间的不对齐问题。需要提升在恶劣天气条件下的鲁棒性。

Method: 提出DiffFusion框架：1) Diffusion-IR：基于扩散模型修复天气退化的图像；2) Point Cloud Restoration (PCR)：利用图像目标线索补偿受损的LiDAR数据；3) Bidirectional Adaptive Fusion and Alignment Module (BAFAM)：实现动态多模态融合和双向BEV对齐以保持空间一致性。

Result: 在三个公开数据集上实验表明，DiffFusion在恶劣天气条件下达到最先进的鲁棒性，同时保持强大的干净数据性能。在真实世界DENSE数据集上的零样本结果进一步验证了其泛化能力。

Conclusion: DiffFusion通过扩散修复和自适应跨模态融合有效提升了多模态3D目标检测在恶劣天气下的鲁棒性，具有良好的泛化性能。

Abstract: Multi-modal 3D object detection is important for reliable perception in robotics and autonomous driving. However, its effectiveness remains limited under adverse weather conditions due to weather-induced distortions and misalignment between different data modalities. In this work, we propose DiffFusion, a novel framework designed to enhance robustness in challenging weather through diffusion-based restoration and adaptive cross-modal fusion. Our key insight is that diffusion models possess strong capabilities for denoising and generating data that can adapt to various weather conditions. Building on this, DiffFusion introduces Diffusion-IR restoring images degraded by weather effects and Point Cloud Restoration (PCR) compensating for corrupted LiDAR data using image object cues. To tackle misalignments between two modalities, we develop Bidirectional Adaptive Fusion and Alignment Module (BAFAM). It enables dynamic multi-modal fusion and bidirectional bird's-eye view (BEV) alignment to maintain consistent spatial correspondence. Extensive experiments on three public datasets show that DiffFusion achieves state-of-the-art robustness under adverse weather while preserving strong clean-data performance. Zero-shot results on the real-world DENSE dataset further validate its generalization. The implementation of our DiffFusion will be released as open-source.

</details>


### [215] [StarryGazer: Leveraging Monocular Depth Estimation Models for Domain-Agnostic Single Depth Image Completion](https://arxiv.org/abs/2512.13147)
*Sangmin Hong,Suyoung Lee,Kyoung Mu Lee*

Main category: cs.CV

Relevance: 35.0

TL;DR: StarryGazer是一个无需地面真实深度数据的深度补全框架，通过利用大型单目深度估计模型和稀疏深度信息，从单个稀疏深度图和RGB图像预测密集深度图像。


<details>
  <summary>Details</summary>
Motivation: 现有无监督深度补全方法需要辅助数据估计深度值，与现实场景不符。单目深度估计模型能产生相对深度图，但无法与稀疏深度图有效结合，简单的仿射变换会导致高误差。

Method: 使用预训练的单目深度估计模型生成相对深度图像，通过分割和随机重缩放创建合成训练对，训练细化网络结合相对深度图和RGB图像来提高准确性和鲁棒性。

Result: 在多个数据集上优于现有无监督方法和变换后的单目深度估计结果，证明框架能有效利用单目深度估计模型的能力，同时通过稀疏深度信息适当修正误差。

Conclusion: StarryGazer是一个领域无关的框架，无需地面真实深度数据即可实现高质量的深度补全，成功结合了单目深度估计和稀疏深度信息的优势。

Abstract: The problem of depth completion involves predicting a dense depth image from a single sparse depth map and an RGB image. Unsupervised depth completion methods have been proposed for various datasets where ground truth depth data is unavailable and supervised methods cannot be applied. However, these models require auxiliary data to estimate depth values, which is far from real scenarios. Monocular depth estimation (MDE) models can produce a plausible relative depth map from a single image, but there is no work to properly combine the sparse depth map with MDE for depth completion; a simple affine transformation to the depth map will yield a high error since MDE are inaccurate at estimating depth difference between objects. We introduce StarryGazer, a domain-agnostic framework that predicts dense depth images from a single sparse depth image and an RGB image without relying on ground-truth depth by leveraging the power of large MDE models. First, we employ a pre-trained MDE model to produce relative depth images. These images are segmented and randomly rescaled to form synthetic pairs for dense pseudo-ground truth and corresponding sparse depths. A refinement network is trained with the synthetic pairs, incorporating the relative depth maps and RGB images to improve the model's accuracy and robustness. StarryGazer shows superior results over existing unsupervised methods and transformed MDE results on various datasets, demonstrating that our framework exploits the power of MDE models while appropriately fixing errors using sparse depth information.

</details>


### [216] [A Semantically Enhanced Generative Foundation Model Improves Pathological Image Synthesis](https://arxiv.org/abs/2512.13164)
*Xianchao Guan,Zhiyuan Fan,Yifeng Wang,Fuqiang Chen,Yanjiang Zhou,Zengyang Che,Hongxue Meng,Xin Li,Yaowei Wang,Hongpeng Wang,Min Zhang,Heng Tao Shen,Zheng Zhang,Yongbing Zhang*

Main category: cs.CV

Relevance: 35.0

TL;DR: CRAFTS是一个用于病理学文本到图像合成的生成式基础模型，通过相关性调节对齐框架解决生成模型在医学图像中的语义不稳定和形态学幻觉问题，生成多样化的病理图像并增强临床任务性能。


<details>
  <summary>Details</summary>
Motivation: 病理学中临床级人工智能的发展受到高质量标注数据集稀缺的限制。现有生成模型存在语义不稳定和形态学幻觉问题，影响诊断可靠性，需要开发能够生成生物准确病理图像的解决方案。

Method: 提出CRAFTS（Correlation-Regulated Alignment Framework for Tissue Synthesis），采用双阶段训练策略，在约280万图像-标题对上进行训练。引入新颖的对齐机制抑制语义漂移，确保生物准确性。模型可生成30种癌症类型的病理图像，并与ControlNet结合实现组织架构的精确控制。

Result: CRAFTS生成多样化的病理图像，质量通过客观指标和病理学家评估验证。CRAFTS增强的数据集在分类、跨模态检索、自监督学习和视觉问答等临床任务中提升性能。模型克服了数据稀缺和隐私问题，为罕见和复杂癌症表型提供无限数据源。

Conclusion: CRAFTS是首个病理学特定的文本到图像合成生成式基础模型，通过相关性调节对齐框架解决了生成模型在医学图像中的关键挑战，为创建稳健的诊断工具提供了无限多样的标注组织学数据源。

Abstract: The development of clinical-grade artificial intelligence in pathology is limited by the scarcity of diverse, high-quality annotated datasets. Generative models offer a potential solution but suffer from semantic instability and morphological hallucinations that compromise diagnostic reliability. To address this challenge, we introduce a Correlation-Regulated Alignment Framework for Tissue Synthesis (CRAFTS), the first generative foundation model for pathology-specific text-to-image synthesis. By leveraging a dual-stage training strategy on approximately 2.8 million image-caption pairs, CRAFTS incorporates a novel alignment mechanism that suppresses semantic drift to ensure biological accuracy. This model generates diverse pathological images spanning 30 cancer types, with quality rigorously validated by objective metrics and pathologist evaluations. Furthermore, CRAFTS-augmented datasets enhance the performance across various clinical tasks, including classification, cross-modal retrieval, self-supervised learning, and visual question answering. In addition, coupling CRAFTS with ControlNet enables precise control over tissue architecture from inputs such as nuclear segmentation masks and fluorescence images. By overcoming the critical barriers of data scarcity and privacy concerns, CRAFTS provides a limitless source of diverse, annotated histology data, effectively unlocking the creation of robust diagnostic tools for rare and complex cancer phenotypes.

</details>


### [217] [Seeing the Whole Picture: Distribution-Guided Data-Free Distillation for Semantic Segmentation](https://arxiv.org/abs/2512.13175)
*Hongxuan Sun,Tao Wu*

Main category: cs.CV

Relevance: 35.0

TL;DR: DFSS：针对语义分割任务的数据无知识蒸馏框架，通过利用BN统计量指导近似分布采样，并采用加权分布渐进蒸馏策略，在无需原始训练数据的情况下显著提升分割性能。


<details>
  <summary>Details</summary>
Motivation: 现有数据无知识蒸馏方法主要针对分类任务设计，直接应用于语义分割时忽略了场景的结构和上下文连续性，导致性能显著下降。语义分割需要对物理世界有整体理解，为空间连续、结构连贯的对象分配标签，而非孤立像素。

Method: 1. 利用教师模型的Batch Normalization统计量指导近似分布采样，选择更接近原始训练分布的数据，避免依赖可能误导的教师预测；2. 提出加权分布渐进蒸馏，动态优先处理与原始数据分布更接近的可靠样本，逐步纳入更具挑战性的案例，模拟人类感知的自然学习进程。

Result: 在标准基准测试上的大量实验表明，DFSS在语义分割的数据无知识蒸馏方法中始终优于现有方法，实现了最先进的结果，同时显著减少了对辅助数据的依赖。

Conclusion: DFSS为语义分割任务提供了一个有效的数据无知识蒸馏框架，通过尊重真实场景的结构和上下文连续性，解决了现有方法直接应用于分割任务时的性能退化问题。

Abstract: Semantic segmentation requires a holistic understanding of the physical world, as it assigns semantic labels to spatially continuous and structurally coherent objects rather than to isolated pixels. However, existing data-free knowledge distillation (DFKD) methods-primarily designed for classification-often disregard this continuity, resulting in significant performance degradation when applied directly to segmentation tasks. In this paper, we introduce DFSS, a novel data-free distillation framework tailored for semantic segmentation. Unlike prior approaches that treat pixels independently, DFSS respects the structural and contextual continuity of real-world scenes. Our key insight is to leverage Batch Normalization (BN) statistics from a teacher model to guide Approximate Distribution Sampling (ADS), enabling the selection of data that better reflects the original training distribution-without relying on potentially misleading teacher predictions. Additionally, we propose Weighted Distribution Progressive Distillation (WDPD), which dynamically prioritizes reliable samples that are more closely aligned with the original data distribution early in training and gradually incorporates more challenging cases, mirroring the natural progression of learning in human perception. Extensive experiments on standard benchmarks demonstrate that DFSS consistently outperforms existing data-free distillation methods for semantic segmentation, achieving state-of-the-art results with significantly reduced reliance on auxiliary data.

</details>


### [218] [CoRA: A Collaborative Robust Architecture with Hybrid Fusion for Efficient Perception](https://arxiv.org/abs/2512.13191)
*Gong Chen,Chaokun Zhang,Pengcheng Lv,Xiaohui Xie*

Main category: cs.CV

Relevance: 35.0

TL;DR: CoRA提出了一种新颖的协作感知架构，通过混合融合范式（特征级融合分支+对象级校正分支）在保持高性能的同时增强通信鲁棒性，在极端场景下性能提升约19%，通信量减少5倍以上。


<details>
  <summary>Details</summary>
Motivation: 现有协作感知方法（特别是中间融合方法）在通信条件恶劣时性能显著下降，主要原因是数据传输导致的对齐错误，这严重阻碍了实际部署。需要一种既能保持高性能又能应对通信干扰的鲁棒架构。

Method: 提出CoRA架构，包含两个分支：1）特征级融合分支：选择关键特征进行高效融合，确保性能和可扩展性；2）对象级校正分支：利用语义相关性校正空间位移，保证对位姿错误的鲁棒性。将中间融合和后期融合的优势互补结合。

Result: 在极端场景下，CoRA相比基线在AP@0.7指标上提升约19%，同时通信量减少5倍以上，实现了性能与鲁棒性的解耦。

Conclusion: CoRA通过混合融合范式有效解决了协作感知中的通信鲁棒性问题，证明了中间融合和后期融合的优势可以互补而非权衡，为实际部署提供了有前景的解决方案。

Abstract: Collaborative perception has garnered significant attention as a crucial technology to overcome the perceptual limitations of single-agent systems. Many state-of-the-art (SOTA) methods have achieved communication efficiency and high performance via intermediate fusion. However, they share a critical vulnerability: their performance degrades under adverse communication conditions due to the misalignment induced by data transmission, which severely hampers their practical deployment. To bridge this gap, we re-examine different fusion paradigms, and recover that the strengths of intermediate and late fusion are not a trade-off, but a complementary pairing. Based on this key insight, we propose CoRA, a novel collaborative robust architecture with a hybrid approach to decouple performance from robustness with low communication. It is composed of two components: a feature-level fusion branch and an object-level correction branch. Its first branch selects critical features and fuses them efficiently to ensure both performance and scalability. The second branch leverages semantic relevance to correct spatial displacements, guaranteeing resilience against pose errors. Experiments demonstrate the superiority of CoRA. Under extreme scenarios, CoRA improves upon its baseline performance by approximately 19% in AP@0.7 with more than 5x less communication volume, which makes it a promising solution for robust collaborative perception.

</details>


### [219] [STARCaster: Spatio-Temporal AutoRegressive Video Diffusion for Identity- and View-Aware Talking Portraits](https://arxiv.org/abs/2512.13247)
*Foivos Paraperas Papantoniou,Stathis Galanakis,Rolandos Alexandros Potamias,Bernhard Kainz,Stefanos Zafeiriou*

Main category: cs.CV

Relevance: 35.0

TL;DR: STARCaster是一个身份感知的时空视频扩散模型，统一处理语音驱动肖像动画和自由视角说话肖像合成，通过软身份约束和2D域内隐式3D感知解决现有方法的局限性。


<details>
  <summary>Details</summary>
Motivation: 现有2D语音到视频扩散模型过度依赖参考指导，导致运动多样性有限；而3D感知动画通常依赖预训练三平面生成器的反演，导致重建不完美和身份漂移。需要重新思考基于参考和几何的范式。

Method: 1) 引入软身份约束而非严格参考条件；2) 利用视频数据固有的多视角特性在2D域内隐式实现3D感知；3) 采用组合方法：ID感知运动建模→基于唇读的视听同步→时空适应的新视角动画；4) 解耦学习分别训练视角一致性和时间连贯性；5) 自强制训练方案学习比推理时更长的时序上下文。

Result: 综合评估表明STARCaster能有效泛化到不同任务和身份，在不同基准测试中持续超越先前方法，解决了现有自回归方法中常见的过度静态动画问题。

Conclusion: STARCaster通过重新思考参考和几何范式，在统一框架中实现了高质量的语音驱动肖像动画和自由视角说话肖像合成，为视频生成领域提供了新的解决方案。

Abstract: This paper presents STARCaster, an identity-aware spatio-temporal video diffusion model that addresses both speech-driven portrait animation and free-viewpoint talking portrait synthesis, given an identity embedding or reference image, within a unified framework. Existing 2D speech-to-video diffusion models depend heavily on reference guidance, leading to limited motion diversity. At the same time, 3D-aware animation typically relies on inversion through pre-trained tri-plane generators, which often leads to imperfect reconstructions and identity drift. We rethink reference- and geometry-based paradigms in two ways. First, we deviate from strict reference conditioning at pre-training by introducing softer identity constraints. Second, we address 3D awareness implicitly within the 2D video domain by leveraging the inherent multi-view nature of video data. STARCaster adopts a compositional approach progressing from ID-aware motion modeling, to audio-visual synchronization via lip reading-based supervision, and finally to novel view animation through temporal-to-spatial adaptation. To overcome the scarcity of 4D audio-visual data, we propose a decoupled learning approach in which view consistency and temporal coherence are trained independently. A self-forcing training scheme enables the model to learn from longer temporal contexts than those generated at inference, mitigating the overly static animations common in existing autoregressive approaches. Comprehensive evaluations demonstrate that STARCaster generalizes effectively across tasks and identities, consistently surpassing prior approaches in different benchmarks.

</details>


### [220] [Unlocking Generalization in Polyp Segmentation with DINO Self-Attention "keys"](https://arxiv.org/abs/2512.13376)
*Carla Monteiro,Valentina Corbetta,Regina Beets-Tan,Luís F. Teixeira,Wilson Silva*

Main category: cs.CV

Relevance: 35.0

TL;DR: 提出利用DINO自注意力"key"特征进行息肉分割的框架，通过简单卷积解码器实现，在域泛化和极端单域泛化场景下达到SOTA性能


<details>
  <summary>Details</summary>
Motivation: 现有息肉分割方法存在泛化能力不足的问题，特别是在数据受限或挑战性场景下，且通常依赖复杂的任务特定架构。需要更鲁棒、泛化能力更强的解决方案。

Method: 利用DINO自注意力模块的key特征（而非传统的最深层token），结合简单的卷积解码器预测息肉掩码。在多中心数据集上采用域泛化(DG)和极端单域泛化(ESDG)两种严格协议验证。

Result: 在DG和ESDG协议下达到SOTA性能，显著提升泛化能力，特别是在数据稀缺和挑战性场景。超越了nnU-Net和UM-Net等成熟模型，并提供了DINO框架演变的系统基准分析。

Conclusion: DINO自注意力key特征为息肉分割提供了鲁棒表示，避免了复杂的息肉特定架构设计，在泛化性能上具有显著优势，为医学图像分割提供了新思路。

Abstract: Automatic polyp segmentation is crucial for improving the clinical identification of colorectal cancer (CRC). While Deep Learning (DL) techniques have been extensively researched for this problem, current methods frequently struggle with generalization, particularly in data-constrained or challenging settings. Moreover, many existing polyp segmentation methods rely on complex, task-specific architectures. To address these limitations, we present a framework that leverages the intrinsic robustness of DINO self-attention "key" features for robust segmentation. Unlike traditional methods that extract tokens from the deepest layers of the Vision Transformer (ViT), our approach leverages the key features of the self-attention module with a simple convolutional decoder to predict polyp masks, resulting in enhanced performance and better generalizability. We validate our approach using a multi-center dataset under two rigorous protocols: Domain Generalization (DG) and Extreme Single Domain Generalization (ESDG). Our results, supported by a comprehensive statistical analysis, demonstrate that this pipeline achieves state-of-the-art (SOTA) performance, significantly enhancing generalization, particularly in data-scarce and challenging scenarios. While avoiding a polyp-specific architecture, we surpass well-established models like nnU-Net and UM-Net. Additionally, we provide a systematic benchmark of the DINO framework's evolution, quantifying the specific impact of architectural advancements on downstream polyp segmentation performance.

</details>


### [221] [Beyond the Visible: Disocclusion-Aware Editing via Proxy Dynamic Graphs](https://arxiv.org/abs/2512.13392)
*Anran Qi,Changjian Li,Adrien Bousseau,Niloy J. Mitra*

Main category: cs.CV

Relevance: 35.0

TL;DR: 提出一种无需训练的图像到视频生成方法，通过用户可编辑的代理动态图（PDG）分离运动规范与外观合成，实现对新显露区域的可控生成。


<details>
  <summary>Details</summary>
Motivation: 当前图像到视频生成方法虽然能产生合理运动，但难以生成可预测的关节运动，也无法在视频中新显露区域（disoccluded regions）强制执行用户指定的内容。需要一种既能控制运动又能控制新显露区域外观的方法。

Method: 提出训练免费的流程：1）用户标注和重新摆姿轻量级PDG，确定性地驱动部件运动；2）使用冻结的扩散先验作为运动引导的着色器合成外观；3）利用PDG编码的可见性信息进行潜在空间合成，将运动与用户意图在新显露区域协调。

Result: 在关节物体、家具、车辆和可变形物体的图像转短视频任务上，相比现有方法展现出明显优势，实现了可控的关节运动和用户对新显露区域外观的控制。

Conclusion: 该方法通过分离运动规范与外观合成，结合生成控制（宽松姿态和结构）与可预测控制（最终帧新显露区域的外观指定），解锁了新的图像到视频工作流程。

Abstract: We address image-to-video generation with explicit user control over the final frame's disoccluded regions. Current image-to-video pipelines produce plausible motion but struggle to generate predictable, articulated motions while enforcing user-specified content in newly revealed areas. Our key idea is to separate motion specification from appearance synthesis: we introduce a lightweight, user-editable Proxy Dynamic Graph (PDG) that deterministically yet approximately drives part motion, while a frozen diffusion prior is used to synthesize plausible appearance that follows that motion. In our training-free pipeline, the user loosely annotates and reposes a PDG, from which we compute a dense motion flow to leverage diffusion as a motion-guided shader. We then let the user edit appearance in the disoccluded areas of the image, and exploit the visibility information encoded by the PDG to perform a latent-space composite that reconciles motion with user intent in these areas. This design yields controllable articulation and user control over disocclusions without fine-tuning. We demonstrate clear advantages against state-of-the-art alternatives towards images turned into short videos of articulated objects, furniture, vehicles, and deformables. Our method mixes generative control, in the form of loose pose and structure, with predictable controls, in the form of appearance specification in the final frame in the disoccluded regions, unlocking a new image-to-video workflow. Code will be released on acceptance. Project page: https://anranqi.github.io/beyondvisible.github.io/

</details>


### [222] [rNCA: Self-Repairing Segmentation Masks](https://arxiv.org/abs/2512.13397)
*Malte Silbernagel,Albert Alonso,Jens Petersen,Bulat Ibragimov,Marleen de Bruijne,Madeleine K. Wyburd*

Main category: cs.CV

Relevance: 35.0

TL;DR: 该论文提出使用神经细胞自动机（NCA）作为分割掩码的细化机制，通过局部迭代更新修复拓扑错误，提高分割结果的拓扑一致性。


<details>
  <summary>Details</summary>
Motivation: 通用分割模型经常产生碎片化或不连续的输出，修复这些拓扑错误通常需要手工设计的细化规则或特定任务的专用架构。需要一种通用、有效的细化机制来改善分割掩码的拓扑质量。

Method: 提出细化NCA（rNCA），通过训练在粗糙分割掩码和真实标注上，学习目标形状的结构特性。NCA仅依赖局部信息，通过迭代更新逐步修复断裂区域、修剪松散碎片，收敛到稳定、拓扑一致的结果。

Result: 在视网膜血管分割中，Dice/clDice提升2-3%，Betti错误显著改善（β0错误减少60%，β1减少20%）；在心肌分割中，零样本设置下修复61.5%的断裂案例，ASSD和HD分别降低19%和16%。

Conclusion: NCA可作为有效且广泛适用的分割细化机制，通过局部迭代更新修复常见拓扑错误，提高分割结果的拓扑一致性，无需任务特定的手工设计。

Abstract: Accurately predicting topologically correct masks remains a difficult task for general segmentation models, which often produce fragmented or disconnected outputs. Fixing these artifacts typically requires hand-crafted refinement rules or architectures specialized to a particular task. Here, we show that Neural Cellular Automata (NCA) can be directly re-purposed as an effective refinement mechanism, using local, iterative updates guided by image context to repair segmentation masks. By training on imperfect masks and ground truths, the automaton learns the structural properties of the target shape while relying solely on local information. When applied to coarse, globally predicted masks, the learned dynamics progressively reconnect broken regions, prune loose fragments and converge towards stable, topologically consistent results. We show how refinement NCA (rNCA) can be easily applied to repair common topological errors produced by different base segmentation models and tasks: for fragmented retinal vessels, it yields 2-3% gains in Dice/clDice and improves Betti errors, reducing $β_0$ errors by 60% and $β_1$ by 20%; for myocardium, it repairs 61.5% of broken cases in a zero-shot setting while lowering ASSD and HD by 19% and 16%, respectively. This showcases NCA as effective and broadly applicable refiners.

</details>


### [223] [Computer vision training dataset generation for robotic environments using Gaussian splatting](https://arxiv.org/abs/2512.13411)
*Patryk Niżeniec,Marcin Iwanowski*

Main category: cs.CV

Relevance: 35.0

TL;DR: 提出了一种用于机器人视觉任务的大规模、高真实感、自动标注数据集生成流程，通过3D高斯泼溅和游戏引擎物理模拟创建逼真合成数据，结合两阶段渲染技术增强真实感，并自动生成像素级分割掩码。


<details>
  <summary>Details</summary>
Motivation: 解决计算机视觉任务中合成数据与真实世界图像之间的领域差距问题，以及手动标注耗时耗力的瓶颈。旨在为机器人环境中的目标检测和分割任务提供高效、高质量的训练数据。

Method: 1. 使用3D高斯泼溅(3DGS)创建操作环境和物体的逼真表示；2. 在游戏引擎中进行物理模拟创建自然排列；3. 提出两阶段渲染技术：结合泼溅的真实感和代理网格生成的阴影图；4. 算法合成添加物理合理的阴影和细微高光；5. 自动生成像素级分割掩码，格式兼容YOLO等检测模型。

Result: 实验表明，结合少量真实图像和大规模合成数据的混合训练策略，在目标检测和分割任务上取得了最佳性能，证实这是实现鲁棒准确模型的高效策略。

Conclusion: 该方法为机器人视觉任务提供了一种高效的大规模合成数据生成方案，能够显著减少对真实标注数据的依赖，同时通过混合训练策略实现高性能模型。

Abstract: This paper introduces a novel pipeline for generating large-scale, highly realistic, and automatically labeled datasets for computer vision tasks in robotic environments. Our approach addresses the critical challenges of the domain gap between synthetic and real-world imagery and the time-consuming bottleneck of manual annotation. We leverage 3D Gaussian Splatting (3DGS) to create photorealistic representations of the operational environment and objects. These assets are then used in a game engine where physics simulations create natural arrangements. A novel, two-pass rendering technique combines the realism of splats with a shadow map generated from proxy meshes. This map is then algorithmically composited with the image to add both physically plausible shadows and subtle highlights, significantly enhancing realism. Pixel-perfect segmentation masks are generated automatically and formatted for direct use with object detection models like YOLO. Our experiments show that a hybrid training strategy, combining a small set of real images with a large volume of our synthetic data, yields the best detection and segmentation performance, confirming this as an optimal strategy for efficiently achieving robust and accurate models.

</details>


### [224] [USTM: Unified Spatial and Temporal Modeling for Continuous Sign Language Recognition](https://arxiv.org/abs/2512.13415)
*Ahmed Abul Hasanaath,Hamzah Luqman*

Main category: cs.CV

Relevance: 35.0

TL;DR: 提出USTM框架，结合Swin Transformer和轻量级时间适配器TAPE，用于连续手语识别，在多个基准数据集上达到SOTA性能


<details>
  <summary>Details</summary>
Motivation: 现有连续手语识别方法通常依赖CNN空间骨干网络结合时间卷积或循环模块，这些方法无法捕捉细粒度的手部和面部线索，也难以建模长程时间依赖关系

Method: 提出统一时空建模框架USTM，使用Swin Transformer作为空间骨干，结合轻量级时间适配器TAPE（包含位置嵌入），有效建模复杂时空模式

Result: 在PHOENIX14、PHOENIX14T和CSL-Daily等基准数据集上，USTM在RGB-based和多模态CSLR方法中达到SOTA性能，与多流方法相比保持竞争力

Conclusion: USTM框架通过有效建模细粒度空间特征和长短时时间上下文，实现了强大的连续手语识别，无需多流输入或辅助模态

Abstract: Continuous sign language recognition (CSLR) requires precise spatio-temporal modeling to accurately recognize sequences of gestures in videos. Existing frameworks often rely on CNN-based spatial backbones combined with temporal convolution or recurrent modules. These techniques fail in capturing fine-grained hand and facial cues and modeling long-range temporal dependencies. To address these limitations, we propose the Unified Spatio-Temporal Modeling (USTM) framework, a spatio-temporal encoder that effectively models complex patterns using a combination of a Swin Transformer backbone enhanced with lightweight temporal adapter with positional embeddings (TAPE). Our framework captures fine-grained spatial features alongside short and long-term temporal context, enabling robust sign language recognition from RGB videos without relying on multi-stream inputs or auxiliary modalities. Extensive experiments on benchmarked datasets including PHOENIX14, PHOENIX14T, and CSL-Daily demonstrate that USTM achieves state-of-the-art performance against RGB-based as well as multi-modal CSLR approaches, while maintaining competitive performance against multi-stream approaches. These results highlight the strength and efficacy of the USTM framework for CSLR. The code is available at https://github.com/gufranSabri/USTM

</details>


### [225] [Learning to Generate Cross-Task Unexploitable Examples](https://arxiv.org/abs/2512.13416)
*Haoxuan Qu,Qiuchi Xiang,Yujun Cai,Yirui Wu,Majid Mirmehdi,Hossein Rahmani,Jun Liu*

Main category: cs.CV

Relevance: 35.0

TL;DR: 提出MCT-UEG框架，通过元跨任务训练生成广泛不可利用的个人图像保护隐私


<details>
  <summary>Details</summary>
Motivation: 现有不可利用示例生成方法在实际应用中存在局限性，无法生成跨不同计算机视觉任务都有效的不可利用示例，限制了方法的实际应用价值

Method: 提出MCT-UEG框架，核心是设计面向平坦最小值的元训练和测试方案，优化不可利用示例生成器，使其能产生广泛不可利用的示例

Result: 大量实验证明了框架的有效性

Conclusion: 该方法能生成跨任务广泛不可利用的示例，提升了个人图像隐私保护的实用性

Abstract: Unexploitable example generation aims to transform personal images into their unexploitable (unlearnable) versions before they are uploaded online, thereby preventing unauthorized exploitation of online personal images. Recently, this task has garnered significant research attention due to its critical relevance to personal data privacy. Yet, despite recent progress, existing methods for this task can still suffer from limited practical applicability, as they can fail to generate examples that are broadly unexploitable across different real-world computer vision tasks. To deal with this problem, in this work, we propose a novel Meta Cross-Task Unexploitable Example Generation (MCT-UEG) framework. At the core of our framework, to optimize the unexploitable example generator for effectively producing broadly unexploitable examples, we design a flat-minima-oriented meta training and testing scheme. Extensive experiments show the efficacy of our framework.

</details>


### [226] [PoseAnything: Universal Pose-guided Video Generation with Part-aware Temporal Coherence](https://arxiv.org/abs/2512.13465)
*Ruiyan Wang,Teng Hu,Kaihui Huang,Zihan Su,Ran Yi,Lizhuang Ma*

Main category: cs.CV

Relevance: 35.0

TL;DR: PoseAnything：首个通用姿态引导视频生成框架，支持人类和非人类角色，通过部件感知时序一致性模块和运动解耦CFG策略实现精细控制


<details>
  <summary>Details</summary>
Motivation: 当前姿态引导视频生成方法仅支持人类姿态输入，对其他主体姿态泛化能力差，限制了在动画等领域的应用范围

Method: 1) 提出通用框架支持任意骨骼输入；2) 部件感知时序一致性模块实现部件级一致性；3) 主体与相机运动解耦CFG策略实现独立相机控制；4) 构建XPose非人类姿态-视频数据集

Result: PoseAnything在效果和泛化能力上显著优于现有方法，支持人类和非人类角色，实现精细的姿态控制和独立的相机运动控制

Conclusion: 该工作突破了姿态引导视频生成仅限人类姿态的限制，实现了通用姿态控制，为动画制作提供了更灵活的工具

Abstract: Pose-guided video generation refers to controlling the motion of subjects in generated video through a sequence of poses. It enables precise control over subject motion and has important applications in animation. However, current pose-guided video generation methods are limited to accepting only human poses as input, thus generalizing poorly to pose of other subjects. To address this issue, we propose PoseAnything, the first universal pose-guided video generation framework capable of handling both human and non-human characters, supporting arbitrary skeletal inputs. To enhance consistency preservation during motion, we introduce Part-aware Temporal Coherence Module, which divides the subject into different parts, establishes part correspondences, and computes cross-attention between corresponding parts across frames to achieve fine-grained part-level consistency. Additionally, we propose Subject and Camera Motion Decoupled CFG, a novel guidance strategy that, for the first time, enables independent camera movement control in pose-guided video generation, by separately injecting subject and camera motion control information into the positive and negative anchors of CFG. Furthermore, we present XPose, a high-quality public dataset containing 50,000 non-human pose-video pairs, along with an automated pipeline for annotation and filtering. Extensive experiments demonstrate that Pose-Anything significantly outperforms state-of-the-art methods in both effectiveness and generalization.

</details>


### [227] [Soul: Breathe Life into Digital Human for High-fidelity Long-term Multimodal Animation](https://arxiv.org/abs/2512.13495)
*Jiangning Zhang,Junwei Zhu,Zhenye Gan,Donghao Luo,Chuming Lin,Feifan Xu,Xu Peng,Jianlong Hu,Yuansen Liu,Yijia Hong,Weijian Cao,Han Feng,Xu Chen,Chencan Fu,Keke He,Xiaobin Hu,Chengjie Wang*

Main category: cs.CV

Relevance: 35.0

TL;DR: Soul是一个多模态驱动的数字人生成框架，可从单张肖像图、文本提示和音频生成高质量长时动画，实现精准唇形同步、生动表情和身份保持。


<details>
  <summary>Details</summary>
Motivation: 解决数字人生成中数据稀缺、长期生成一致性差、推理效率低的问题，实现高质量、语义连贯的长时数字人动画。

Method: 基于Wan2.2-5B骨干网络，集成音频注入层、多训练策略、阈值感知码本替换确保长期一致性；使用步长/CFG蒸馏和轻量VAE优化推理效率；构建Soul-1M数据集和Soul-Bench评估基准。

Result: 在视频质量、视频-文本对齐、身份保持和唇形同步精度上显著优于当前开源和商业模型，推理速度提升11.4倍且质量损失可忽略，在虚拟主播、影视制作等场景有广泛应用。

Conclusion: Soul框架通过创新的多模态集成、数据构建和优化策略，实现了高质量、高效的长时数字人生成，为数字人应用提供了实用解决方案。

Abstract: We propose a multimodal-driven framework for high-fidelity long-term digital human animation termed $\textbf{Soul}$, which generates semantically coherent videos from a single-frame portrait image, text prompts, and audio, achieving precise lip synchronization, vivid facial expressions, and robust identity preservation. We construct Soul-1M, containing 1 million finely annotated samples with a precise automated annotation pipeline (covering portrait, upper-body, full-body, and multi-person scenes) to mitigate data scarcity, and we carefully curate Soul-Bench for comprehensive and fair evaluation of audio-/text-guided animation methods. The model is built on the Wan2.2-5B backbone, integrating audio-injection layers and multiple training strategies together with threshold-aware codebook replacement to ensure long-term generation consistency. Meanwhile, step/CFG distillation and a lightweight VAE are used to optimize inference efficiency, achieving an 11.4$\times$ speedup with negligible quality loss. Extensive experiments show that Soul significantly outperforms current leading open-source and commercial models on video quality, video-text alignment, identity preservation, and lip-synchronization accuracy, demonstrating broad applicability in real-world scenarios such as virtual anchors and film production. Project page at https://zhangzjn.github.io/projects/Soul/

</details>


### [228] [DA-SSL: self-supervised domain adaptor to leverage foundational models in turbt histopathology slides](https://arxiv.org/abs/2512.13600)
*Haoyue Zhang,Meera Chappidi,Erolcan Sayar,Helen Richards,Zhijun Chen,Lucas Liu,Roxanne Wadia,Peter A Humphrey,Fady Ghali,Alberto Contreras-Sanz,Peter Black,Jonathan Wright,Stephanie Harmon,Michael Haffner*

Main category: cs.CV

Relevance: 35.0

TL;DR: 提出DA-SSL（领域自适应自监督适配器），通过轻量级领域适应增强病理基础模型在膀胱肿瘤电切样本上的性能，无需微调基础模型本身。


<details>
  <summary>Details</summary>
Motivation: 病理基础模型在特定癌症类型（如膀胱肿瘤电切样本）上存在领域偏移问题，因为这些样本包含组织碎片和电灼伪影，且未广泛用于预训练。临床中预测膀胱癌新辅助化疗反应具有挑战性，组织形态学特征利用不足。

Method: 提出领域自适应自监督适配器（DA-SSL），通过自监督学习将预训练的病理基础模型特征重新对齐到目标领域（TURBT），而不需要微调基础模型本身。该方法结合多实例学习框架，用于预测膀胱癌治疗反应。

Result: 在多中心研究中，DA-SSL在五折交叉验证中达到AUC 0.77±0.04，外部测试准确率0.84，敏感性0.71，特异性0.91（多数投票）。

Conclusion: 轻量级领域适应结合自监督学习能有效增强基于病理基础模型的多实例学习流程，适用于临床挑战性病理学任务。

Abstract: Recent deep learning frameworks in histopathology, particularly multiple instance learning (MIL) combined with pathology foundational models (PFMs), have shown strong performance. However, PFMs exhibit limitations on certain cancer or specimen types due to domain shifts - these cancer types were rarely used for pretraining or specimens contain tissue-based artifacts rarely seen within the pretraining population. Such is the case for transurethral resection of bladder tumor (TURBT), which are essential for diagnosing muscle-invasive bladder cancer (MIBC), but contain fragmented tissue chips and electrocautery artifacts and were not widely used in publicly available PFMs. To address this, we propose a simple yet effective domain-adaptive self-supervised adaptor (DA-SSL) that realigns pretrained PFM features to the TURBT domain without fine-tuning the foundational model itself. We pilot this framework for predicting treatment response in TURBT, where histomorphological features are currently underutilized and identifying patients who will benefit from neoadjuvant chemotherapy (NAC) is challenging. In our multi-center study, DA-SSL achieved an AUC of 0.77+/-0.04 in five-fold cross-validation and an external test accuracy of 0.84, sensitivity of 0.71, and specificity of 0.91 using majority voting. Our results demonstrate that lightweight domain adaptation with self-supervision can effectively enhance PFM-based MIL pipelines for clinically challenging histopathology tasks. Code is Available at https://github.com/zhanghaoyue/DA_SSL_TURBT.

</details>


### [229] [Do-Undo: Generating and Reversing Physical Actions in Vision-Language Models](https://arxiv.org/abs/2512.13609)
*Shweta Mahajan,Shreya Kadambi,Hoang Le,Munawar Hayat,Fatih Porikli*

Main category: cs.CV

Relevance: 35.0

TL;DR: Do-Undo任务和基准旨在解决视觉语言模型在物理场景变换理解上的不足，要求模型模拟物理动作的结果并准确反转，测试物理可逆性。


<details>
  <summary>Details</summary>
Motivation: 当前视觉语言模型主要关注对象级编辑，缺乏对真实世界物理动作引起的场景变换的理解。需要建立能够理解物理因果关系和可逆性的评估基准，这对具身AI、机器人和物理感知生成建模至关重要。

Method: 1. 从真实世界视频中构建大规模可逆动作数据集；2. 设计训练策略以增强动作定位的一致性；3. 创建Do-Undo任务，要求模型先模拟物理动作结果，然后准确反转该动作。

Result: 实验表明当前模型在物理可逆性方面表现不佳，凸显了该任务的重要性。Do-Undo为评估和推进多模态系统中的物理推理能力提供了直观测试平台。

Conclusion: Do-Undo填补了视觉语言模型在物理场景变换理解上的关键空白，为物理推理能力的评估和提升提供了重要基准，对具身AI和机器人等领域有重要意义。

Abstract: We introduce the Do-Undo task and benchmark to address a critical gap in vision-language models: understanding and generating physically plausible scene transformations driven by real-world actions. Unlike prior work focused on object-level edits, Do-Undo requires models to simulate the outcome of a physical action and then accurately reverse it, reflecting true cause-and-effect in the visual world. We curate a large-scale dataset of reversible actions from real-world videos and design a training strategy enforcing consistency for robust action grounding. Our experiments reveal that current models struggle with physical reversibility, underscoring the importance of this task for embodied AI, robotics, and physics-aware generative modeling. Do-Undo establishes an intuitive testbed for evaluating and advancing physical reasoning in multimodal systems.

</details>


### [230] [JoVA: Unified Multimodal Learning for Joint Video-Audio Generation](https://arxiv.org/abs/2512.13677)
*Xiaohu Huang,Hao Zhou,Qiangpeng Yang,Shilei Wen,Kai Han*

Main category: cs.CV

Relevance: 35.0

TL;DR: JoVA是一个统一的视频-音频生成框架，通过联合自注意力机制实现跨模态交互，无需额外对齐模块，并使用基于面部关键点检测的嘴部区域损失来提升唇语同步质量。


<details>
  <summary>Details</summary>
Motivation: 现有方法存在两个关键限制：1) 大多只能生成环境音，缺乏生成与唇部动作同步的人类语音的能力；2) 现有统一视频-音频生成方法通常依赖显式融合或模态特定对齐模块，增加了架构复杂性并削弱了原始Transformer的简洁性。

Method: 1) 在Transformer层中使用视频和音频token的联合自注意力机制，实现直接高效的跨模态交互；2) 引入基于面部关键点检测的嘴部区域损失，在不影响架构简洁性的情况下增强对关键嘴部区域的监督。

Result: 在基准测试中，JoVA在唇语同步准确性、语音质量和整体视频-音频生成保真度方面优于或与最先进的统一和音频驱动方法相竞争。

Conclusion: JoVA作为一个优雅的框架，能够实现高质量的多模态生成，解决了现有方法的局限性。

Abstract: In this paper, we present JoVA, a unified framework for joint video-audio generation. Despite recent encouraging advances, existing methods face two critical limitations. First, most existing approaches can only generate ambient sounds and lack the capability to produce human speech synchronized with lip movements. Second, recent attempts at unified human video-audio generation typically rely on explicit fusion or modality-specific alignment modules, which introduce additional architecture design and weaken the model simplicity of the original transformers. To address these issues, JoVA employs joint self-attention across video and audio tokens within each transformer layer, enabling direct and efficient cross-modal interaction without the need for additional alignment modules. Furthermore, to enable high-quality lip-speech synchronization, we introduce a simple yet effective mouth-area loss based on facial keypoint detection, which enhances supervision on the critical mouth region during training without compromising architectural simplicity. Extensive experiments on benchmarks demonstrate that JoVA outperforms or is competitive with both unified and audio-driven state-of-the-art methods in lip-sync accuracy, speech quality, and overall video-audio generation fidelity. Our results establish JoVA as an elegant framework for high-quality multimodal generation.

</details>


### [231] [Feedforward 3D Editing via Text-Steerable Image-to-3D](https://arxiv.org/abs/2512.13678)
*Ziqi Ma,Hongqiao Chen,Yisong Yue,Georgia Gkioxari*

Main category: cs.CV

Relevance: 35.0

TL;DR: Steer3D：一种前馈方法，为图像到3D模型添加文本可控性，使生成的3D资产能够通过语言进行编辑，比竞争方法更快且更忠实于指令。


<details>
  <summary>Details</summary>
Motivation: AI生成的3D资产在实际应用中需要易于编辑的能力，但现有图像到3D模型缺乏文本可控性，无法通过语言指令进行修改。

Method: 受ControlNet启发，将文本可控性适配到图像到3D生成中；构建可扩展的数据引擎自动生成训练数据；采用基于流匹配训练和直接偏好优化（DPO）的两阶段训练方法。

Result: Steer3D比竞争方法更忠实于语言指令，与原始3D资产保持更好一致性，同时速度提升2.4倍到28.5倍；仅需10万数据即可为预训练模型添加文本模态可控性。

Conclusion: Steer3D证明了通过少量数据为预训练图像到3D生成模型添加文本可控性的可行性，为3D内容创作提供了高效编辑工具。

Abstract: Recent progress in image-to-3D has opened up immense possibilities for design, AR/VR, and robotics. However, to use AI-generated 3D assets in real applications, a critical requirement is the capability to edit them easily. We present a feedforward method, Steer3D, to add text steerability to image-to-3D models, which enables editing of generated 3D assets with language. Our approach is inspired by ControlNet, which we adapt to image-to-3D generation to enable text steering directly in a forward pass. We build a scalable data engine for automatic data generation, and develop a two-stage training recipe based on flow-matching training and Direct Preference Optimization (DPO). Compared to competing methods, Steer3D more faithfully follows the language instruction and maintains better consistency with the original 3D asset, while being 2.4x to 28.5x faster. Steer3D demonstrates that it is possible to add a new modality (text) to steer the generation of pretrained image-to-3D generative models with 100k data. Project website: https://glab-caltech.github.io/steer3d/

</details>


### [232] [LASER: Layer-wise Scale Alignment for Training-Free Streaming 4D Reconstruction](https://arxiv.org/abs/2512.13680)
*Tianye Ding,Yiming Xie,Yiqing Liang,Moitreya Chatterjee,Pedro Miraldo,Huaizu Jiang*

Main category: cs.CV

Relevance: 35.0

TL;DR: LASER是一个无需训练即可将离线3D重建模型转换为流式系统的框架，通过分层尺度对齐解决连续时间窗口间的深度尺度不一致问题，实现实时流式视频处理。


<details>
  <summary>Details</summary>
Motivation: 现有前馈重建模型（如VGGT和π³）虽然重建质量优秀，但存在二次内存复杂度问题，无法处理流式视频。现有流式方法需要大量重新训练，且无法充分利用离线模型的几何先验知识。

Method: 提出LASER框架：1）通过相似变换（Sim(3)）对齐连续时间窗口的预测；2）引入分层尺度对齐，将深度预测分割为离散层，计算每层尺度因子，并在相邻窗口和时间戳间传播；3）无需训练即可将离线模型转换为流式系统。

Result: 在相机姿态估计和点云地图重建方面达到SOTA性能，在RTX A6000 GPU上以14 FPS运行，峰值内存仅6GB，能够处理千米级流式视频。

Conclusion: LASER提供了一种高效实用的方法，将高质量离线重建模型部署到流式视频场景中，解决了内存复杂度和尺度对齐问题，实现了实时处理能力。

Abstract: Recent feed-forward reconstruction models like VGGT and $π^3$ achieve impressive reconstruction quality but cannot process streaming videos due to quadratic memory complexity, limiting their practical deployment. While existing streaming methods address this through learned memory mechanisms or causal attention, they require extensive retraining and may not fully leverage the strong geometric priors of state-of-the-art offline models. We propose LASER, a training-free framework that converts an offline reconstruction model into a streaming system by aligning predictions across consecutive temporal windows. We observe that simple similarity transformation ($\mathrm{Sim}(3)$) alignment fails due to layer depth misalignment: monocular scale ambiguity causes relative depth scales of different scene layers to vary inconsistently between windows. To address this, we introduce layer-wise scale alignment, which segments depth predictions into discrete layers, computes per-layer scale factors, and propagates them across both adjacent windows and timestamps. Extensive experiments show that LASER achieves state-of-the-art performance on camera pose estimation and point map reconstruction %quality with offline models while operating at 14 FPS with 6 GB peak memory on a RTX A6000 GPU, enabling practical deployment for kilometer-scale streaming videos. Project website: $\href{https://neu-vi.github.io/LASER/}{\texttt{https://neu-vi.github.io/LASER/}}$

</details>


### [233] [LitePT: Lighter Yet Stronger Point Transformer](https://arxiv.org/abs/2512.13689)
*Yuanwen Yue,Damien Robert,Jianyuan Wang,Sunghwan Hong,Jan Dirk Wegner,Christian Rupprecht,Konrad Schindler*

Main category: cs.CV

Relevance: 35.0

TL;DR: LitePT：一种新的3D点云处理骨干网络，早期层使用卷积提取低层几何特征，深层使用注意力机制捕获高层语义，结合新的3D位置编码PointROPE，在减少参数、提升效率的同时保持或超越SOTA性能。


<details>
  <summary>Details</summary>
Motivation: 当前3D点云处理网络同时包含卷积层和注意力块，但最佳组合方式尚不明确。研究发现卷积适合在早期高分辨率层提取低层几何特征，而注意力在深层低分辨率层更高效地捕获高层语义和上下文信息。

Method: 提出LitePT模型：1）早期阶段使用卷积，深层切换到注意力机制；2）引入无需训练的3D位置编码PointROPE，避免丢弃冗余卷积层时丢失空间布局信息；3）基于设计原则优化网络架构。

Result: LitePT相比SOTA的Point Transformer V3：参数减少3.6倍，运行速度提升2倍，内存使用减少2倍，在多个任务和数据集上性能相当甚至更优。

Conclusion: 通过合理分配卷积和注意力机制在不同网络深度的作用，可以构建更高效、轻量的3D点云处理骨干网络，同时保持或提升性能。

Abstract: Modern neural architectures for 3D point cloud processing contain both convolutional layers and attention blocks, but the best way to assemble them remains unclear. We analyse the role of different computational blocks in 3D point cloud networks and find an intuitive behaviour: convolution is adequate to extract low-level geometry at high-resolution in early layers, where attention is expensive without bringing any benefits; attention captures high-level semantics and context in low-resolution, deep layers more efficiently. Guided by this design principle, we propose a new, improved 3D point cloud backbone that employs convolutions in early stages and switches to attention for deeper layers. To avoid the loss of spatial layout information when discarding redundant convolution layers, we introduce a novel, training-free 3D positional encoding, PointROPE. The resulting LitePT model has $3.6\times$ fewer parameters, runs $2\times$ faster, and uses $2\times$ less memory than the state-of-the-art Point Transformer V3, but nonetheless matches or even outperforms it on a range of tasks and datasets. Code and models are available at: https://github.com/prs-eth/LitePT.

</details>


### [234] [Aion: Towards Hierarchical 4D Scene Graphs with Temporal Flow Dynamics](https://arxiv.org/abs/2512.11903)
*Iacopo Catalano,Eduardo Montijano,Javier Civera,Julio A. Placed,Jorge Pena-Queralta*

Main category: cs.RO

Relevance: 35.0

TL;DR: Aion框架将时间流动态嵌入到层次化3D场景图中，结合了语义场景表示和运动模式建模，用于动态环境中的自主导航。


<details>
  <summary>Details</summary>
Motivation: 现有3D场景图主要关注静态语义结构，而动态地图通常基于网格离散化缺乏语义意识且难以扩展。需要一种既能捕捉语义结构又能建模时间演化的空间表示方法。

Method: Aion采用基于图的稀疏动态地图表示，捕捉任意时间间隔的运动流，并将其附加到场景图的导航节点上，形成层次化的时空表示。

Result: 该方法提供了更可解释和可扩展的预测，改善了复杂动态环境中的规划和交互能力。

Conclusion: Aion成功地将时间维度整合到3D场景图中，为动态环境导航提供了统一的语义-时空表示框架。

Abstract: Autonomous navigation in dynamic environments requires spatial representations that capture both semantic structure and temporal evolution. 3D Scene Graphs (3DSGs) provide hierarchical multi-resolution abstractions that encode geometry and semantics, but existing extensions toward dynamics largely focus on individual objects or agents. In parallel, Maps of Dynamics (MoDs) model typical motion patterns and temporal regularities, yet are usually tied to grid-based discretizations that lack semantic awareness and do not scale well to large environments. In this paper we introduce Aion, a framework that embeds temporal flow dynamics directly within a hierarchical 3DSG, effectively incorporating the temporal dimension. Aion employs a graph-based sparse MoD representation to capture motion flows over arbitrary time intervals and attaches them to navigational nodes in the scene graph, yielding more interpretable and scalable predictions that improve planning and interaction in complex dynamic environments.

</details>


### [235] [Pre-training vision models for the classification of alerts from wide-field time-domain surveys](https://arxiv.org/abs/2512.11957)
*Nabeel Rehemtulla,Adam A. Miller,Mike Walmsley,Ved G. Shah,Theophile Jegou du Laz,Michael W. Coughlin,Argyro Sasli,Joshua Bloom,Christoffer Fremling,Matthew J. Graham,Steven L. Groom,David Hale,Ashish A. Mahabal,Daniel A. Perley,Josiah Purdum,Ben Rusholme,Jesper Sollerman,Mansi M. Kasliwal*

Main category: astro-ph.IM

Relevance: 35.0

TL;DR: 该论文探讨了在天文警报分类中采用计算机视觉领域最新实践（如预训练模型和标准化架构）的效果，发现这些方法在性能和效率上都优于传统定制CNN方法。


<details>
  <summary>Details</summary>
Motivation: 当前时域天文学研究通常设计定制的CNN架构并从零开始训练，而计算机视觉领域已普遍采用在大型图像语料库上预训练的标准化架构。随着LSST等大规模巡天项目的到来，需要探索更高效、性能更好的模型构建方法。

Method: 研究比较了不同的预训练策略和标准化模型架构在天文警报分类任务上的表现。包括：1）在ImageNet等日常图像上预训练；2）在天文图像（如Galaxy Zoo的星系图像）上预训练；3）从零开始训练。同时对比了标准化架构与定制CNN架构的性能和效率。

Result: 1）采用预训练和标准化架构的模型在性能上匹配或优于传统定制CNN；2）在Galaxy Zoo星系图像上预训练的效果优于ImageNet预训练或从零开始训练；3）标准化架构虽然参数更多，但在推理时间和内存使用上更高效，优化更好。

Conclusion: 在天文警报分类任务中，采用计算机视觉领域的最新实践（预训练和标准化架构）能够获得更好的性能和效率，这为即将到来的大规模巡天项目提供了更优的模型构建范式。

Abstract: Modern wide-field time-domain surveys facilitate the study of transient, variable and moving phenomena by conducting image differencing and relaying alerts to their communities. Machine learning tools have been used on data from these surveys and their precursors for more than a decade, and convolutional neural networks (CNNs), which make predictions directly from input images, saw particularly broad adoption through the 2010s. Since then, continually rapid advances in computer vision have transformed the standard practices around using such models. It is now commonplace to use standardized architectures pre-trained on large corpora of everyday images (e.g., ImageNet). In contrast, time-domain astronomy studies still typically design custom CNN architectures and train them from scratch. Here, we explore the affects of adopting various pre-training regimens and standardized model architectures on the performance of alert classification. We find that the resulting models match or outperform a custom, specialized CNN like what is typically used for filtering alerts. Moreover, our results show that pre-training on galaxy images from Galaxy Zoo tends to yield better performance than pre-training on ImageNet or training from scratch. We observe that the design of standardized architectures are much better optimized than the custom CNN baseline, requiring significantly less time and memory for inference despite having more trainable parameters. On the eve of the Legacy Survey of Space and Time and other image-differencing surveys, these findings advocate for a paradigm shift in the creation of vision models for alerts, demonstrating that greater performance and efficiency, in time and in data, can be achieved by adopting the latest practices from the computer vision field.

</details>


### [236] [AutoMV: An Automatic Multi-Agent System for Music Video Generation](https://arxiv.org/abs/2512.12196)
*Xiaoxuan Tang,Xinping Lei,Chaoran Zhu,Shiyun Chen,Ruibin Yuan,Yizhi Li,Changjae Oh,Ge Zhang,Wenhao Huang,Emmanouil Benetos,Yang Liu,Jiaheng Liu,Yinghao Ma*

Main category: cs.MM

Relevance: 35.0

TL;DR: AutoMV：一个多智能体系统，用于从完整歌曲生成音乐视频，通过提取音乐特征、智能体协作生成脚本和视频，并提出了新的音乐视频生成评估基准。


<details>
  <summary>Details</summary>
Motivation: 现有音乐到视频生成方法只能产生短小、不连贯的片段，无法与音乐结构、节拍或歌词对齐，缺乏时间一致性。需要一种能够生成完整音乐视频的方法。

Method: 1) 使用音乐处理工具提取音乐属性（结构、人声轨道、时间对齐歌词）作为上下文输入；2) 编剧和导演智能体设计短脚本、角色档案和摄像机指令；3) 调用图像生成器生成关键帧和视频生成器生成"故事"或"歌手"场景；4) 验证智能体评估输出，实现多智能体协作。

Result: AutoMV在所有四个评估类别（音乐内容、技术、后期制作、艺术）上显著优于现有基线，缩小了与专业音乐视频的差距。同时提出了包含12个细粒度标准的评估基准。

Conclusion: AutoMV能够生成连贯的长格式音乐视频，多智能体系统有效解决了音乐视频生成的挑战。虽然大型多模态模型作为自动评估工具有潜力，但仍落后于人类专家，需要进一步研究。

Abstract: Music-to-Video (M2V) generation for full-length songs faces significant challenges. Existing methods produce short, disjointed clips, failing to align visuals with musical structure, beats, or lyrics, and lack temporal consistency. We propose AutoMV, a multi-agent system that generates full music videos (MVs) directly from a song. AutoMV first applies music processing tools to extract musical attributes, such as structure, vocal tracks, and time-aligned lyrics, and constructs these features as contextual inputs for following agents. The screenwriter Agent and director Agent then use this information to design short script, define character profiles in a shared external bank, and specify camera instructions. Subsequently, these agents call the image generator for keyframes and different video generators for "story" or "singer" scenes. A Verifier Agent evaluates their output, enabling multi-agent collaboration to produce a coherent longform MV. To evaluate M2V generation, we further propose a benchmark with four high-level categories (Music Content, Technical, Post-production, Art) and twelve ine-grained criteria. This benchmark was applied to compare commercial products, AutoMV, and human-directed MVs with expert human raters: AutoMV outperforms current baselines significantly across all four categories, narrowing the gap to professional MVs. Finally, we investigate using large multimodal models as automatic MV judges; while promising, they still lag behind human expert, highlighting room for future work.

</details>


### [237] [Resolution-Independent Neural Operators for Multi-Rate Sparse-View CT](https://arxiv.org/abs/2512.12236)
*Aujasvit Datta,Jiayun Wang,Asad Aali,Armeet Singh Jatyani,Anima Anandkumar*

Main category: eess.IV

Relevance: 35.0

TL;DR: CTO是一个统一的CT重建框架，通过扩展到连续函数空间实现跨采样率和图像分辨率的泛化，无需重新训练，比CNN和扩散方法表现更好。


<details>
  <summary>Details</summary>
Motivation: 稀疏视图CT重建是一个不适定逆问题，现有深度学习方法容易过拟合到固定的采集设置，无法泛化到不同的采样率和图像分辨率。

Method: 提出CTO框架，在正弦图和图像域联合操作，通过旋转等变的离散-连续卷积在函数空间中参数化，实现固有的分辨率和采样无关性。

Result: CTO在跨采样率和跨分辨率任务中表现一致，平均比CNN提高>4dB PSNR，比最先进的扩散方法快500倍且平均提高3dB PSNR。

Conclusion: CTO在采样率和分辨率方面优于现有方法，提供了一个可扩展和可泛化的解决方案，使自动化CT重建更实用。

Abstract: Sparse-view Computed Tomography (CT) reconstructs images from a limited number of X-ray projections to reduce radiation and scanning time, which makes reconstruction an ill-posed inverse problem. Deep learning methods achieve high-fidelity reconstructions but often overfit to a fixed acquisition setup, failing to generalize across sampling rates and image resolutions. For example, convolutional neural networks (CNNs) use the same learned kernels across resolutions, leading to artifacts when data resolution changes.
  We propose Computed Tomography neural Operator (CTO), a unified CT reconstruction framework that extends to continuous function space, enabling generalization (without retraining) across sampling rates and image resolutions. CTO operates jointly in the sinogram and image domains through rotation-equivariant Discrete-Continuous convolutions parametrized in the function space, making it inherently resolution- and sampling-agnostic. Empirically, CTO enables consistent multi-sampling-rate and cross-resolution performance, with on average >4dB PSNR gain over CNNs. Compared to state-of-the-art diffusion methods, CTO is 500$\times$ faster in inference time with on average 3dB gain. Empirical results also validate our design choices behind CTO's sinogram-space operator learning and rotation-equivariant convolution. Overall, CTO outperforms state-of-the-art baselines across sampling rates and resolutions, offering a scalable and generalizable solution that makes automated CT reconstruction more practical for deployment.

</details>


### [238] [Leveraging Compression to Construct Transferable Bitrate Ladders](https://arxiv.org/abs/2512.12952)
*Krishna Srikar Durbha,Hassene Tmar,Ping-Hao Wu,Ioannis Katsavounidis,Alan C. Bovik*

Main category: eess.IV

Relevance: 35.0

TL;DR: 提出一种基于机器学习的比特率阶梯构建技术，通过分析压缩过程和源视频的感知相关测量，准确预测压缩视频的VMAF分数，相比传统方法显著降低计算开销。


<details>
  <summary>Details</summary>
Motivation: 传统基于凸包构建的每镜头比特率阶梯方法虽然能提供显著的比特率增益和改善观看体验，但计算开销巨大。需要开发更高效的机器学习方法来替代凸包构建，同时保持内容自适应性。

Method: 提出新的基于机器学习的比特率阶梯构建框架：1) 分析视频压缩过程；2) 在压缩前对源视频进行感知相关测量；3) 训练ML模型预测压缩视频的VMAF分数；4) 研究不同编码设置下每镜头比特率阶梯的性能。

Result: 在大量视频语料上评估，提出的ML框架性能优于现有领先方法。与固定比特率阶梯和通过穷举编码构建的最佳可能凸包相比，使用Bjontegaard-delta指标进行评估。

Conclusion: 提出的ML方法能够准确预测VMAF分数，有效替代计算密集的凸包构建，为内容自适应比特率阶梯提供高效解决方案，同时在不同编码设置下保持良好性能。

Abstract: Over the past few years, per-title and per-shot video encoding techniques have demonstrated significant gains as compared to conventional techniques such as constant CRF encoding and the fixed bitrate ladder. These techniques have demonstrated that constructing content-gnostic per-shot bitrate ladders can provide significant bitrate gains and improved Quality of Experience (QoE) for viewers under various network conditions. However, constructing a convex hull for every video incurs a significant computational overhead. Recently, machine learning-based bitrate ladder construction techniques have emerged as a substitute for convex hull construction. These methods operate by extracting features from source videos to train machine learning (ML) models to construct content-adaptive bitrate ladders. Here, we present a new ML-based bitrate ladder construction technique that accurately predicts the VMAF scores of compressed videos, by analyzing the compression procedure and by making perceptually relevant measurements on the source videos prior to compression. We evaluate the performance of our proposed framework against leading prior methods on a large corpus of videos. Since training ML models on every encoder setting is time-consuming, we also investigate how per-shot bitrate ladders perform under different encoding settings. We evaluate the performance of all models against the fixed bitrate ladder and the best possible convex hull constructed using exhaustive encoding with Bjontegaard-delta metrics.

</details>


### [239] [Self-Supervised Ultrasound Representation Learning for Renal Anomaly Prediction in Prenatal Imaging](https://arxiv.org/abs/2512.13434)
*Youssef Megahed,Inok Lee,Robin Ducharme,Kevin Dick,Adrian D. C. Chan,Steven Hawken,Mark C. Walker*

Main category: eess.IV

Relevance: 35.0

TL;DR: 本文提出了一种基于自监督超声基础模型（USF-MAE）的胎儿肾脏异常自动分类方法，在二元和多分类任务上均优于传统卷积基线模型，并提供了可解释性可视化。


<details>
  <summary>Details</summary>
Motivation: 产前超声是检测先天性肾脏和尿路异常的关键方法，但存在操作者依赖性和成像条件不理想等限制。研究旨在评估自监督超声基础模型在胎儿肾脏异常自动分类中的性能。

Method: 使用预训练的超声自监督基础模型USF-MAE（基于掩码自编码），在包含969张二维超声图像的精选数据集上进行微调，用于正常肾脏、尿路扩张和多囊性肾发育不良的二元和多分类任务。与DenseNet-169卷积基线模型进行比较，采用交叉验证和独立测试集评估。

Result: USF-MAE在所有评估指标上均优于基线模型：验证集上AUC提升约1.87%，F1分数提升7.8%；独立测试集上AUC提升2.32%，F1分数提升4.33%。多分类任务中提升最大：AUC提升16.28%，F1分数提升46.15%。Score-CAM可视化显示模型预测基于临床相关的肾脏结构。

Conclusion: 超声特定的自监督学习能够生成有用的表示，作为下游诊断任务的基础。该框架为产前肾脏异常检测提供了稳健、可解释的方法，展示了基础模型在产科影像中的潜力。

Abstract: Prenatal ultrasound is the cornerstone for detecting congenital anomalies of the kidneys and urinary tract, but diagnosis is limited by operator dependence and suboptimal imaging conditions. We sought to assess the performance of a self-supervised ultrasound foundation model for automated fetal renal anomaly classification using a curated dataset of 969 two-dimensional ultrasound images. A pretrained Ultrasound Self-Supervised Foundation Model with Masked Autoencoding (USF-MAE) was fine-tuned for binary and multi-class classification of normal kidneys, urinary tract dilation, and multicystic dysplastic kidney. Models were compared with a DenseNet-169 convolutional baseline using cross-validation and an independent test set. USF-MAE consistently improved upon the baseline across all evaluation metrics in both binary and multi-class settings. USF-MAE achieved an improvement of about 1.87% (AUC) and 7.8% (F1-score) on the validation set, 2.32% (AUC) and 4.33% (F1-score) on the independent holdout test set. The largest gains were observed in the multi-class setting, where the improvement in AUC was 16.28% and 46.15% in F1-score. To facilitate model interpretability, Score-CAM visualizations were adapted for a transformer architecture and show that model predictions were informed by known, clinically relevant renal structures, including the renal pelvis in urinary tract dilation and cystic regions in multicystic dysplastic kidney. These results show that ultrasound-specific self-supervised learning can generate a useful representation as a foundation for downstream diagnostic tasks. The proposed framework offers a robust, interpretable approach to support the prenatal detection of renal anomalies and demonstrates the promise of foundation models in obstetric imaging.

</details>


### [240] [SMRABooth: Subject and Motion Representation Alignment for Customized Video Generation](https://arxiv.org/abs/2512.12193)
*Xuancheng Xu,Yaning Li,Sisi You,Bing-Kun Bao*

Main category: cs.CV

Relevance: 30.0

TL;DR: SMRABooth：一种通过自监督编码器和光流编码器提供对象级主体与运动表示，结合LoRA微调实现定制化视频生成的方法，能同时保持主体外观相似性和运动模式一致性。


<details>
  <summary>Details</summary>
Motivation: 现有定制化视频生成方法难以同时保证主体外观相似性和运动模式一致性，主要原因是缺乏针对主体和运动的对象级指导。

Method: 1) 使用自监督编码器提取主体表示来指导主体对齐；2) 使用光流编码器获取结构一致的对象级运动轨迹；3) 提出主体-运动关联解耦策略，在位置和时间上稀疏注入LoRA，减少主体和运动LoRA之间的干扰。

Result: 大量实验表明SMRABooth在主体和运动定制方面表现出色，能保持一致的主体外观和运动模式，证明了其在可控文本到视频生成中的有效性。

Conclusion: SMRABooth通过对象级主体和运动表示以及解耦的LoRA注入策略，成功解决了定制化视频生成中主体外观保持和运动一致性的挑战。

Abstract: Customized video generation aims to produce videos that faithfully preserve the subject's appearance from reference images while maintaining temporally consistent motion from reference videos. Existing methods struggle to ensure both subject appearance similarity and motion pattern consistency due to the lack of object-level guidance for subject and motion. To address this, we propose SMRABooth, which leverages the self-supervised encoder and optical flow encoder to provide object-level subject and motion representations. These representations are aligned with the model during the LoRA fine-tuning process. Our approach is structured in three core stages: (1) We exploit subject representations via a self-supervised encoder to guide subject alignment, enabling the model to capture overall structure of subject and enhance high-level semantic consistency. (2) We utilize motion representations from an optical flow encoder to capture structurally coherent and object-level motion trajectories independent of appearance. (3) We propose a subject-motion association decoupling strategy that applies sparse LoRAs injection across both locations and timing, effectively reducing interference between subject and motion LoRAs. Extensive experiments show that SMRABooth excels in subject and motion customization, maintaining consistent subject appearance and motion patterns, proving its effectiveness in controllable text-to-video generation.

</details>


### [241] [OMUDA: Omni-level Masking for Unsupervised Domain Adaptation in Semantic Segmentation](https://arxiv.org/abs/2512.12303)
*Yang Ou,Xiongwei Zhao,Xinye Yang,Yihan Wang,Yicheng Di,Rong Yuan,Xieyuanli Chen,Xu Zhu*

Main category: cs.CV

Relevance: 30.0

TL;DR: OMUDA提出了一种用于无监督域自适应语义分割的统一框架，通过分层掩码策略在上下文、特征和类别三个层面减少域间差异，在多个基准测试中取得SOTA结果。


<details>
  <summary>Details</summary>
Motivation: 现有无监督域自适应方法在语义分割任务中面临跨域上下文歧义、特征表示不一致和类别伪标签噪声等挑战，导致域间差距难以有效弥合。

Method: 提出OMUDA框架，包含三个分层掩码策略：1) 上下文感知掩码(CAM)区分前景背景平衡全局上下文与局部细节；2) 特征蒸馏掩码(FDM)通过预训练模型知识迁移增强鲁棒特征学习；3) 类别解耦掩码(CDM)通过显式建模类别不确定性减轻伪标签噪声影响。

Result: 在SYNTHIA->Cityscapes和GTA5->Cityscapes等跨域语义分割基准测试中，OMUDA可无缝集成到现有UDA方法中，平均提升7%，达到最先进性能。

Conclusion: OMUDA通过分层掩码策略在多个表示层面系统性地减少域间差异，为无监督域自适应语义分割提供了一个统一且有效的解决方案。

Abstract: Unsupervised domain adaptation (UDA) enables semantic segmentation models to generalize from a labeled source domain to an unlabeled target domain. However, existing UDA methods still struggle to bridge the domain gap due to cross-domain contextual ambiguity, inconsistent feature representations, and class-wise pseudo-label noise. To address these challenges, we propose Omni-level Masking for Unsupervised Domain Adaptation (OMUDA), a unified framework that introduces hierarchical masking strategies across distinct representation levels. Specifically, OMUDA comprises: 1) a Context-Aware Masking (CAM) strategy that adaptively distinguishes foreground from background to balance global context and local details; 2) a Feature Distillation Masking (FDM) strategy that enhances robust and consistent feature learning through knowledge transfer from pre-trained models; and 3) a Class Decoupling Masking (CDM) strategy that mitigates the impact of noisy pseudo-labels by explicitly modeling class-wise uncertainty. This hierarchical masking paradigm effectively reduces the domain shift at the contextual, representational, and categorical levels, providing a unified solution beyond existing approaches. Extensive experiments on multiple challenging cross-domain semantic segmentation benchmarks validate the effectiveness of OMUDA. Notably, on the SYNTHIA->Cityscapes and GTA5->Cityscapes tasks, OMUDA can be seamlessly integrated into existing UDA methods and consistently achieving state-of-the-art results with an average improvement of 7%.

</details>


### [242] [Sharpness-aware Dynamic Anchor Selection for Generalized Category Discovery](https://arxiv.org/abs/2512.12925)
*Zhimao Peng,Enguang Wang,Fei Yang,Xialei Liu,Ming-Ming Cheng*

Main category: cs.CV

Relevance: 30.0

TL;DR: 提出LSP和DAS两个模块来改进广义类别发现任务，通过损失锐度惩罚增强模型鲁棒性，动态锚点选择优化伪标签质量，在多个基准测试中达到SOTA。


<details>
  <summary>Details</summary>
Motivation: 当前基于参数化分类的GCD方法使用DINO式伪标签策略，但大型预训练模型对特定视觉模式有偏好，导致对未标记数据编码虚假相关性并产生噪声伪标签。

Method: 提出两个模块：1) 损失锐度惩罚(LSP)：通过最小化模型最坏情况损失锐度来增强参数对小扰动的鲁棒性；2) 动态锚点选择(DAS)：基于KNN密度和类别概率为未知类选择代表性样本并分配硬伪标签。

Result: 在多个GCD基准测试中实现了最先进的结果，有效缓解了伪标签噪声问题。

Conclusion: 提出的LSP和DAS方法能有效抑制琐碎特征编码，减少对噪声样本的过拟合，改善已知类和未知类之间的置信度差异，提升聚类准确性。

Abstract: Generalized category discovery (GCD) is an important and challenging task in open-world learning. Specifically, given some labeled data of known classes, GCD aims to cluster unlabeled data that contain both known and unknown classes. Current GCD methods based on parametric classification adopt the DINO-like pseudo-labeling strategy, where the sharpened probability output of one view is used as supervision information for the other view. However, large pre-trained models have a preference for some specific visual patterns, resulting in encoding spurious correlation for unlabeled data and generating noisy pseudo-labels. To address this issue, we propose a novel method, which contains two modules: Loss Sharpness Penalty (LSP) and Dynamic Anchor Selection (DAS). LSP enhances the robustness of model parameters to small perturbations by minimizing the worst-case loss sharpness of the model, which suppressing the encoding of trivial features, thereby reducing overfitting of noise samples and improving the quality of pseudo-labels. Meanwhile, DAS selects representative samples for the unknown classes based on KNN density and class probability during the model training and assigns hard pseudo-labels to them, which not only alleviates the confidence difference between known and unknown classes but also enables the model to quickly learn more accurate feature distribution for the unknown classes, thus further improving the clustering accuracy. Extensive experiments demonstrate that the proposed method can effectively mitigate the noise of pseudo-labels, and achieve state-of-the-art results on multiple GCD benchmarks.

</details>


### [243] [SCR2-ST: Combine Single Cell with Spatial Transcriptomics for Efficient Active Sampling via Reinforcement Learning](https://arxiv.org/abs/2512.13635)
*Junchao Zhu,Ruining Deng,Junlin Guo,Tianyuan Yao,Chongyu Qu,Juming Xiong,Siqi Lu,Zhengyi Lu,Yanfan Zhu,Marilyn Lionts,Yuechen Yang,Yalin Zheng,Yu Wang,Shilin Zhao,Haichun Yang,Yuankai Huo*

Main category: cs.CV

Relevance: 30.0

TL;DR: SCR2-ST是一个统一框架，利用单细胞先验知识指导空间转录组学数据的高效采集和准确表达预测，通过强化学习主动采样和混合回归-检索网络实现


<details>
  <summary>Details</summary>
Motivation: 空间转录组学(ST)数据获取昂贵，传统固定网格采样策略导致形态相似或生物学无信息区域的冗余测量，造成数据稀缺。单细胞测序领域可提供丰富的生物学数据作为有效辅助来源来缓解这一限制

Method: SCR2-ST整合了单细胞引导的强化学习主动采样(SCRL)和混合回归-检索预测网络(SCR2Net)。SCRL结合单细胞基础模型嵌入和空间密度信息构建生物学基础奖励信号，在有限测序预算下选择性采集信息丰富的组织区域。SCR2Net通过混合架构结合回归建模和检索增强推理，其中多数细胞类型过滤机制抑制噪声匹配，检索到的表达谱作为软标签进行辅助监督

Result: 在三个公共ST数据集上评估SCR2-ST，在采样效率和预测准确性方面均表现出最先进的性能，特别是在低预算场景下

Conclusion: SCR2-ST通过有效利用单细胞先验知识，解决了空间转录组学数据获取昂贵和数据稀缺的问题，实现了高效的数据采集和准确的表达预测

Abstract: Spatial transcriptomics (ST) is an emerging technology that enables researchers to investigate the molecular relationships underlying tissue morphology. However, acquiring ST data remains prohibitively expensive, and traditional fixed-grid sampling strategies lead to redundant measurements of morphologically similar or biologically uninformative regions, thus resulting in scarce data that constrain current methods. The well-established single-cell sequencing field, however, could provide rich biological data as an effective auxiliary source to mitigate this limitation. To bridge these gaps, we introduce SCR2-ST, a unified framework that leverages single-cell prior knowledge to guide efficient data acquisition and accurate expression prediction. SCR2-ST integrates a single-cell guided reinforcement learning-based (SCRL) active sampling and a hybrid regression-retrieval prediction network SCR2Net. SCRL combines single-cell foundation model embeddings with spatial density information to construct biologically grounded reward signals, enabling selective acquisition of informative tissue regions under constrained sequencing budgets. SCR2Net then leverages the actively sampled data through a hybrid architecture combining regression-based modeling with retrieval-augmented inference, where a majority cell-type filtering mechanism suppresses noisy matches and retrieved expression profiles serve as soft labels for auxiliary supervision. We evaluated SCR2-ST on three public ST datasets, demonstrating SOTA performance in both sampling efficiency and prediction accuracy, particularly under low-budget scenarios. Code is publicly available at: https://github.com/hrlblab/SCR2ST

</details>


### [244] [Temporal-Anchor3DLane: Enhanced 3D Lane Detection with Multi-Task Losses and LSTM Fusion](https://arxiv.org/abs/2512.11869)
*D. Shainu Suhas,G. Rahul,K. Muni*

Main category: cs.CV

Relevance: 25.0

TL;DR: Temporal-Anchor3DLane：通过多任务损失改进、轻量级LSTM时序融合和ESCOP训练优化，增强3D车道线检测的鲁棒性和时序稳定性


<details>
  <summary>Details</summary>
Motivation: 单目3D车道线检测面临深度模糊、遮挡和跨帧时序不稳定等挑战。Anchor3DLane等锚点方法虽表现良好，但仍存在回归异常值敏感、全局曲线几何监督弱、多损失项平衡困难、时序连续性利用有限等问题

Method: 1) 多任务损失改进：包括Balanced L1回归、Chamfer点集距离、基于不确定性的损失加权，以及分类和可见性的focal和Dice组件
2) 轻量级时序LSTM融合模块：跨帧聚合每锚点特征，替代较重的Transformer式时序融合
3) ESCOP式训练优化：将曲线级监督与时序一致性相结合

Result: 在OpenLane数据集上，F1分数提升+6.2，产生更平滑的时序轨迹，表明小规模架构和损失改进能显著增强3D车道线鲁棒性，无需额外传感器或扩展

Conclusion: 通过系统性的损失函数改进、轻量级时序融合和训练优化，Temporal-Anchor3DLane显著提升了3D车道线检测的准确性和时序稳定性，证明了在现有锚点框架内进行精细化改进的有效性

Abstract: Monocular 3D lane detection remains challenging due to depth ambiguity, occlusion, and temporal instability across frames. Anchor-based approaches such as Anchor3DLane have demonstrated strong performance by regressing continuous 3D lane curves from multi-camera surround views. However, the baseline model still exhibits (i) sensitivity to regression outliers, (ii) weak supervision of global curve geometry, (iii) difficulty in balancing multiple loss terms, and (iv) limited exploitation of temporal continuity. We propose Temporal-Anchor3DLane, an enhanced 3D lane detection framework that extends Anchor3DLane with three key contributions: (1) a set of multi-task loss improvements, including Balanced L1 regression, Chamfer point-set distance, and uncertainty-based loss weighting, together with focal and Dice components for classification and visibility; (2) a lightweight Temporal LSTM Fusion module that aggregates per-anchor features across frames, replacing a heavier Transformer-style temporal fusion; and (3) ESCOP-style training refinements that couple curve-level supervision with temporal consistency. On OpenLane, Temporal-Anchor3DLane improves F1 by +6.2 and yields smoother temporal trajectories, showing that small architectural and loss refinements significantly enhance 3D lane robustness without extra sensors or scaling.

</details>


### [245] [Pseudo-Label Refinement for Robust Wheat Head Segmentation via Two-Stage Hybrid Training](https://arxiv.org/abs/2512.11874)
*Jiahao Jiang,Zhangrui Yang,Xuanhan Wang,Jingkuan Song*

Main category: cs.CV

Relevance: 25.0

TL;DR: 提出了一种用于小麦语义分割的自训练框架，结合两阶段混合训练策略和大量数据增强，使用SegFormer-MiT-B4作为核心模型，通过迭代师生循环逐步提升精度


<details>
  <summary>Details</summary>
Motivation: 解决全球小麦全语义分割竞赛中的分割任务，需要开发一个系统性的框架来处理农业图像分割的挑战

Method: 1. 两阶段混合训练策略：结合监督学习和自训练；2. 大量数据增强技术；3. SegFormer-MiT-B4作为核心分割模型；4. 迭代师生循环机制，逐步优化模型并最大化数据利用

Result: 在开发和测试阶段数据集上都取得了有竞争力的性能表现

Conclusion: 提出的自训练框架在农业图像分割任务中有效，能够通过迭代优化提升模型精度

Abstract: This extended abstract details our solution for the Global Wheat Full Semantic Segmentation Competition. We developed a systematic self-training framework. This framework combines a two-stage hybrid training strategy with extensive data augmentation. Our core model is SegFormer with a Mix Transformer (MiT-B4) backbone. We employ an iterative teacher-student loop. This loop progressively refines model accuracy. It also maximizes data utilization. Our method achieved competitive performance. This was evident on both the Development and Testing Phase datasets.

</details>


### [246] [mmWEAVER: Environment-Specific mmWave Signal Synthesis from a Photo and Activity Description](https://arxiv.org/abs/2512.11894)
*Mahathir Monjur,Shahriar Nirjon*

Main category: cs.CV

Relevance: 25.0

TL;DR: mmWeaver：基于隐式神经表示和超网络的毫米波雷达信号生成框架，通过环境上下文和人体运动特征合成真实信号，实现高达49倍压缩和6-35倍加速


<details>
  <summary>Details</summary>
Motivation: 毫米波雷达信号生成和数据集增强对于活动识别和姿态估计等应用至关重要，但毫米波信号复杂、稀疏、高维，物理模拟计算成本高昂，需要更高效的信号合成方法

Method: 使用隐式神经表示将毫米波信号建模为连续函数，通过超网络根据环境上下文（RGB-D图像提取）和人体运动特征（MotionGPT文本到姿态生成）动态生成INR参数，实现多分辨率I/Q信号合成

Result: 达到0.88的复杂SSIM和35 dB的PSNR，优于现有方法；活动识别准确率提升7%，人体姿态估计误差降低15%，比模拟方法快6-35倍，实现高达49倍压缩

Conclusion: mmWeaver通过结合隐式神经表示和语义几何先验，实现了高效、真实的毫米波信号合成，为雷达应用提供了高质量的数据增强方案

Abstract: Realistic signal generation and dataset augmentation are essential for advancing mmWave radar applications such as activity recognition and pose estimation, which rely heavily on diverse, and environment-specific signal datasets. However, mmWave signals are inherently complex, sparse, and high-dimensional, making physical simulation computationally expensive. This paper presents mmWeaver, a novel framework that synthesizes realistic, environment-specific complex mmWave signals by modeling them as continuous functions using Implicit Neural Representations (INRs), achieving up to 49-fold compression. mmWeaver incorporates hypernetworks that dynamically generate INR parameters based on environmental context (extracted from RGB-D images) and human motion features (derived from text-to-pose generation via MotionGPT), enabling efficient and adaptive signal synthesis. By conditioning on these semantic and geometric priors, mmWeaver generates diverse I/Q signals at multiple resolutions, preserving phase information critical for downstream tasks such as point cloud estimation and activity classification. Extensive experiments show that mmWeaver achieves a complex SSIM of 0.88 and a PSNR of 35 dB, outperforming existing methods in signal realism while improving activity recognition accuracy by up to 7% and reducing human pose estimation error by up to 15%, all while operating 6-35 times faster than simulation-based approaches.

</details>


### [247] [Smartphone monitoring of smiling as a behavioral proxy of well-being in everyday life](https://arxiv.org/abs/2512.11905)
*Ming-Zher Poh,Shun Liao,Marco Andreetto,Daniel McDuff,Jonathan Wang,Paolo Di Achille,Jiang Wu,Yun Liu,Lawrence Cai,Eric Teasley,Mark Malhotra,Anupam Pathak,Shwetak Patel*

Main category: cs.CV

Relevance: 25.0

TL;DR: 通过智能手机被动记录的自然微笑强度可作为积极情感的有效行为指标，与全国幸福感调查数据高度相关


<details>
  <summary>Details</summary>
Motivation: 传统的主观幸福感测量方法存在回忆偏差和参与者负担重的问题，需要更客观、可扩展的日常幸福感测量方法

Method: 使用深度学习模型分析233名参与者一周内被动记录的405,448个视频片段，量化微笑强度，研究昼夜和每日模式

Result: 微笑强度每日模式与全国幸福感调查数据高度相关(r=0.92)，昼夜节律与日重建方法结果高度一致(r=0.80)；微笑强度与体力活动和光照正相关

Conclusion: 被动智能手机传感可作为研究情感行为动态的强大生态学有效方法，为大规模人群研究开辟新途径

Abstract: Subjective well-being is a cornerstone of individual and societal health, yet its scientific measurement has traditionally relied on self-report methods prone to recall bias and high participant burden. This has left a gap in our understanding of well-being as it is expressed in everyday life. We hypothesized that candid smiles captured during natural smartphone interactions could serve as a scalable, objective behavioral correlate of positive affect. To test this, we analyzed 405,448 video clips passively recorded from 233 consented participants over one week. Using a deep learning model to quantify smile intensity, we identified distinct diurnal and daily patterns. Daily patterns of smile intensity across the week showed strong correlation with national survey data on happiness (r=0.92), and diurnal rhythms documented close correspondence with established results from the day reconstruction method (r=0.80). Higher daily mean smile intensity was significantly associated with more physical activity (Beta coefficient = 0.043, 95% CI [0.001, 0.085]) and greater light exposure (Beta coefficient = 0.038, [0.013, 0.063]), whereas no significant effects were found for smartphone use. These findings suggest that passive smartphone sensing could serve as a powerful, ecologically valid methodology for studying the dynamics of affective behavior and open the door to understanding this behavior at a population scale.

</details>


### [248] [TransBridge: Boost 3D Object Detection by Scene-Level Completion with Transformer Decoder](https://arxiv.org/abs/2512.11926)
*Qinghao Meng,Chenming Wu,Liangjun Zhang,Jianbing Shen*

Main category: cs.CV

Relevance: 25.0

TL;DR: 提出TransBridge框架，通过联合完成与检测网络，利用transformer上采样块融合特征，解决自动驾驶中稀疏点云远距离物体检测难题，提升3D物体检测性能。


<details>
  <summary>Details</summary>
Motivation: 自动驾驶中的3D物体检测对感知移动物体和障碍物至关重要，但远距离区域仅有少量LiDAR点，点云稀疏性导致检测困难。现有方法通过点云稠密化解决稀疏问题，但需要保持计算成本不变的同时提升稀疏区域检测特征。

Method: 提出联合完成与检测框架TransBridge：1) 基于transformer的上采样块融合检测网络和完成网络的特征；2) 检测网络从完成网络获取隐式完成特征；3) 设计动态-静态重建模块生成稠密LiDAR数据作为完成网络的ground truth；4) 利用transformer机制建立通道和空间关系，生成高分辨率特征图用于点云完成。

Result: 在nuScenes和Waymo数据集上的实验表明，框架能持续改进端到端3D物体检测，mAP提升0.7-1.5点，展示良好泛化能力。对于两阶段检测框架，mAP提升最高达5.78点。

Conclusion: TransBridge框架通过联合完成与检测，有效解决稀疏点云检测问题，在保持计算成本不变的同时显著提升3D物体检测性能，特别是远距离稀疏区域，具有良好泛化能力。

Abstract: 3D object detection is essential in autonomous driving, providing vital information about moving objects and obstacles. Detecting objects in distant regions with only a few LiDAR points is still a challenge, and numerous strategies have been developed to address point cloud sparsity through densification.This paper presents a joint completion and detection framework that improves the detection feature in sparse areas while maintaining costs unchanged. Specifically, we propose TransBridge, a novel transformer-based up-sampling block that fuses the features from the detection and completion networks.The detection network can benefit from acquiring implicit completion features derived from the completion network. Additionally, we design the Dynamic-Static Reconstruction (DSRecon) module to produce dense LiDAR data for the completion network, meeting the requirement for dense point cloud ground truth.Furthermore, we employ the transformer mechanism to establish connections between channels and spatial relations, resulting in a high-resolution feature map used for completion purposes.Extensive experiments on the nuScenes and Waymo datasets demonstrate the effectiveness of the proposed framework.The results show that our framework consistently improves end-to-end 3D object detection, with the mean average precision (mAP) ranging from 0.7 to 1.5 across multiple methods, indicating its generalization ability. For the two-stage detection framework, it also boosts the mAP up to 5.78 points.

</details>


### [249] [A Comparative Analysis of Semiconductor Wafer Map Defect Detection with Image Transformer](https://arxiv.org/abs/2512.11977)
*Sushmita Nath*

Main category: cs.CV

Relevance: 25.0

TL;DR: 该研究探索了数据高效图像变换器(DeiT)在半导体晶圆缺陷分类中的应用，在数据受限条件下DeiT相比传统CNN模型(VGG-19、Xception、SqueezeNet等)表现出更高的准确率(90.83%)和F1分数(90.78%)，训练收敛更快，对小样本缺陷类别检测更鲁棒。


<details>
  <summary>Details</summary>
Motivation: 半导体晶圆缺陷检测是预测性维护的重要领域，传统CNN模型(VGG-19、Xception、SqueezeNet)在数据充足时表现良好，但在数据有限且不平衡的情况下性能下降。需要探索在数据约束条件下仍能保持高性能的模型。

Method: 使用数据高效图像变换器(DeiT)进行半导体晶圆缺陷分类，与多种CNN模型(VGG-19、SqueezeNet、Xception、混合模型)进行对比实验，评估在数据受限条件下的性能表现。

Result: DeiT模型达到90.83%的最高分类准确率，显著优于CNN模型(VGG-19:65%、SqueezeNet:82%、Xception:66%、混合模型:67%)。DeiT还表现出90.78%的F1分数，训练收敛更快，对小样本缺陷类别检测更鲁棒。

Conclusion: 基于变换器的DeiT模型在半导体晶圆缺陷检测中展现出优越性能，特别是在数据受限条件下，为半导体制造过程中的预测性维护策略提供了有力支持。

Abstract: Predictive maintenance is an important sector in modern industries which improves fault detection and cost reduction processes. By using machine learning algorithms in the whole process, the defects detection process can be implemented smoothly. Semiconductor is a sensitive maintenance field that requires predictability in work. While convolutional neural networks (CNNs) such as VGG-19, Xception and Squeeze-Net have demonstrated solid performance in image classification for semiconductor wafer industry, their effectiveness often declines in scenarios with limited and imbalanced data. This study investigates the use of the Data-Efficient Image Transformer (DeiT) for classifying wafer map defects under data-constrained conditions. Experimental results reveal that the DeiT model achieves highest classification accuracy of 90.83%, outperforming CNN models such as VGG-19(65%), SqueezeNet(82%), Xception(66%) and Hybrid(67%). DeiT also demonstrated superior F1-score (90.78%) and faster training convergence, with enhanced robustness in detecting minority defect classes. These findings highlight the potential of transformer-based models like DeiT in semiconductor wafer defect detection and support predictive maintenance strategies within semiconductor fabrication processes.

</details>


### [250] [CARI4D: Category Agnostic 4D Reconstruction of Human-Object Interaction](https://arxiv.org/abs/2512.11988)
*Xianghui Xie,Bowen Wen,Yan Chang,Hesam Rabeti,Jiefeng Li,Ye Yuan,Gerard Pons-Moll,Stan Birchfield*

Main category: cs.CV

Relevance: 25.0

TL;DR: CARI4D：首个类别无关的4D人-物交互重建方法，从单目RGB视频中实现空间和时间一致的重建，无需物体模板或类别限制。


<details>
  <summary>Details</summary>
Motivation: 从单目RGB视频准确捕捉人-物交互对于人类理解、游戏和机器人学习应用很重要。现有方法要么需要真实物体模板，要么局限于有限物体类别，无法处理未知物体、深度模糊、遮挡和复杂运动等挑战。

Method: 提出姿态假设选择算法，鲁棒整合基础模型的个体预测；通过学习的渲染-比较范式联合优化，确保空间、时间和像素对齐；最后基于精细接触推理进行物理约束下的进一步优化。

Result: 在分布内数据集上重建误差降低38%，在未见数据集上降低36%。模型能泛化到训练类别之外，可零样本应用于真实网络视频。

Conclusion: CARI4D是首个类别无关的4D人-物交互重建方法，在单目RGB视频中实现了空间和时间一致的重建，超越了现有方法并具有很好的泛化能力。

Abstract: Accurate capture of human-object interaction from ubiquitous sensors like RGB cameras is important for applications in human understanding, gaming, and robot learning. However, inferring 4D interactions from a single RGB view is highly challenging due to the unknown object and human information, depth ambiguity, occlusion, and complex motion, which hinder consistent 3D and temporal reconstruction. Previous methods simplify the setup by assuming ground truth object template or constraining to a limited set of object categories. We present CARI4D, the first category-agnostic method that reconstructs spatially and temporarily consistent 4D human-object interaction at metric scale from monocular RGB videos. To this end, we propose a pose hypothesis selection algorithm that robustly integrates the individual predictions from foundation models, jointly refine them through a learned render-and-compare paradigm to ensure spatial, temporal and pixel alignment, and finally reasoning about intricate contacts for further refinement satisfying physical constraints. Experiments show that our method outperforms prior art by 38% on in-distribution dataset and 36% on unseen dataset in terms of reconstruction error. Our model generalizes beyond the training categories and thus can be applied zero-shot to in-the-wild internet videos. Our code and pretrained models will be publicly released.

</details>


### [251] [A Benchmark Dataset for Spatially Aligned Road Damage Assessment in Small Uncrewed Aerial Systems Disaster Imagery](https://arxiv.org/abs/2512.12128)
*Thomas Manzini,Priyankari Perali,Raisa Karnik,Robin R. Murphy*

Main category: cs.CV

Relevance: 25.0

TL;DR: 该论文提出了最大的道路损坏评估和道路对齐基准数据集，包含来自10次联邦宣布灾害的无人机图像，提供了18个基线模型，并解决了现有灾害道路评估数据集中的三个挑战。


<details>
  <summary>Details</summary>
Motivation: 现有灾害道路损坏评估数据集规模小或依赖低分辨率图像，无法检测应急管理者感兴趣的现象，且现有机器学习系统缺乏操作验证。同时，实践中观察到道路线错位问题会导致模型性能下降。

Method: 1) 标注657.25公里道路，采用10类标注方案；2) 在2024年飓风Debby和Helene的应急响应中训练和部署ML模型；3) 提供9,184个道路线调整用于空间对齐；4) 训练18个基线模型并评估错位对性能的影响。

Result: 1) 创建了最大的道路损坏评估数据集；2) 当模型部署到真实世界错位道路线时，性能平均下降5.596% Macro IoU；3) 如果不考虑空间对齐，约8%(11公里)的道路不良条件会被错误标注，约9%(59公里)的道路线会偏离实际道路。

Conclusion: 道路空间对齐是灾害响应中ML、CV和机器人社区需要解决的关键问题，以提高灾害期间的决策效果。该数据集和基线模型为这一领域提供了重要资源。

Abstract: This paper presents the largest known benchmark dataset for road damage assessment and road alignment, and provides 18 baseline models trained on the CRASAR-U-DRIODs dataset's post-disaster small uncrewed aerial systems (sUAS) imagery from 10 federally declared disasters, addressing three challenges within prior post-disaster road damage assessment datasets. While prior disaster road damage assessment datasets exist, there is no current state of practice, as prior public datasets have either been small-scale or reliant on low-resolution imagery insufficient for detecting phenomena of interest to emergency managers. Further, while machine learning (ML) systems have been developed for this task previously, none are known to have been operationally validated. These limitations are overcome in this work through the labeling of 657.25km of roads according to a 10-class labeling schema, followed by training and deploying ML models during the operational response to Hurricanes Debby and Helene in 2024. Motivated by observed road line misalignment in practice, 9,184 road line adjustments were provided for spatial alignment of a priori road lines, as it was found that when the 18 baseline models are deployed against real-world misaligned road lines, model performance degraded on average by 5.596\% Macro IoU. If spatial alignment is not considered, approximately 8\% (11km) of adverse conditions on road lines will be labeled incorrectly, with approximately 9\% (59km) of road lines misaligned off the actual road. These dynamics are gaps that should be addressed by the ML, CV, and robotics communities to enable more effective and informed decision-making during disasters.

</details>


### [252] [Audio-Visual Camera Pose Estimationn with Passive Scene Sounds and In-the-Wild Video](https://arxiv.org/abs/2512.12165)
*Daniel Adebi,Sagnik Majumder,Kristen Grauman*

Main category: cs.CV

Relevance: 25.0

TL;DR: 该论文提出了一种利用音频信息辅助相机姿态估计的音频-视觉框架，通过在现有视觉模型中集成声源方向谱和双耳嵌入，提高了在视觉退化条件下的姿态估计鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 相机运动理解是具身感知和3D场景理解的基础问题。纯视觉方法在视觉退化条件（如运动模糊、遮挡）下表现不佳，而被动场景声音提供了互补线索，可以增强相对相机姿态估计的鲁棒性。

Method: 提出了一个简单但有效的音频-视觉框架，将声源方向（DOA）谱和双耳化嵌入集成到最先进的纯视觉姿态估计模型中，利用音频信息增强姿态估计性能。

Result: 在两个大型数据集上的实验表明，该方法相比强视觉基线获得了一致的性能提升，并且在视觉信息受损时表现出更好的鲁棒性。

Conclusion: 这是首个成功利用音频进行真实世界视频相对相机姿态估计的工作，证明了日常音频信号作为经典空间挑战问题的意外但有效的补充信号。

Abstract: Understanding camera motion is a fundamental problem in embodied perception and 3D scene understanding. While visual methods have advanced rapidly, they often struggle under visually degraded conditions such as motion blur or occlusions. In this work, we show that passive scene sounds provide complementary cues for relative camera pose estimation for in-the-wild videos. We introduce a simple but effective audio-visual framework that integrates direction-ofarrival (DOA) spectra and binauralized embeddings into a state-of-the-art vision-only pose estimation model. Our results on two large datasets show consistent gains over strong visual baselines, plus robustness when the visual information is corrupted. To our knowledge, this represents the first work to successfully leverage audio for relative camera pose estimation in real-world videos, and it establishes incidental, everyday audio as an unexpected but promising signal for a classic spatial challenge. Project: http://vision.cs.utexas.edu/projects/av_camera_pose.

</details>


### [253] [A Multi-Year Urban Streetlight Imagery Dataset for Visual Monitoring and Spatio-Temporal Drift Detection](https://arxiv.org/abs/2512.12205)
*Peizheng Li,Ioannis Mavromatis,Ajith Sahadevan,Tim Farnham,Adnan Aijaz,Aftab Khan*

Main category: cs.CV

Relevance: 25.0

TL;DR: 提出了一个大规模、纵向的城市街灯视觉数据集，包含22个固定角度摄像头在2021-2025年间采集的52.6万张图像，附带丰富元数据，用于研究视觉漂移、异常检测和智能城市MLOps策略。


<details>
  <summary>Details</summary>
Motivation: 现有计算机视觉数据集通常缺乏长期、真实世界的部署环境数据，难以评估模型在动态环境中的稳定性和漂移问题。该研究旨在填补这一空白，为智能城市视觉系统的长期部署提供基准测试资源。

Method: 使用22个固定角度摄像头在布里斯托尔进行长期数据采集，每小时收集图像，覆盖不同光照、天气和季节条件。提供基于卷积变分自编码器（CNN-VAEs）的自监督框架，为每个摄像头节点和日/夜图像集单独训练模型，定义两种漂移度量：相对质心漂移和相对重建误差。

Result: 创建了包含52.6万张图像的大规模数据集，附带时间戳、GPS坐标和设备标识等元数据。提供了自监督分析框架和漂移度量方法，为长期模型稳定性评估和漂移感知学习提供了现实基准。

Conclusion: 该数据集为评估计算机视觉系统在真实世界长期部署中的稳定性提供了重要资源，特别适用于研究视觉漂移、异常检测和MLOps策略，支持街灯监控、天气推断和城市场景理解等下游应用。

Abstract: We present a large-scale, longitudinal visual dataset of urban streetlights captured by 22 fixed-angle cameras deployed across Bristol, U.K., from 2021 to 2025. The dataset contains over 526,000 images, collected hourly under diverse lighting, weather, and seasonal conditions. Each image is accompanied by rich metadata, including timestamps, GPS coordinates, and device identifiers. This unique real-world dataset enables detailed investigation of visual drift, anomaly detection, and MLOps strategies in smart city deployments. To promtoe seconardary analysis, we additionally provide a self-supervised framework based on convolutional variational autoencoders (CNN-VAEs). Models are trained separately for each camera node and for day/night image sets. We define two per-sample drift metrics: relative centroid drift, capturing latent space deviation from a baseline quarter, and relative reconstruction error, measuring normalized image-domain degradation. This dataset provides a realistic, fine-grained benchmark for evaluating long-term model stability, drift-aware learning, and deployment-ready vision systems. The images and structured metadata are publicly released in JPEG and CSV formats, supporting reproducibility and downstream applications such as streetlight monitoring, weather inference, and urban scene understanding. The dataset can be found at https://doi.org/10.5281/zenodo.17781192 and https://doi.org/10.5281/zenodo.17859120.

</details>


### [254] [A Hybrid Deep Learning Framework for Emotion Recognition in Children with Autism During NAO Robot-Mediated Interaction](https://arxiv.org/abs/2512.12208)
*Indranil Bhattacharjee,Vartika Narayani Srinet,Anirudha Bhattacharjee,Braj Bhushan,Bishakh Bhattacharya*

Main category: cs.CV

Relevance: 25.0

TL;DR: 该研究提出了一种用于自闭症儿童情绪识别的深度学习管道，结合了微调的ResNet-50 CNN和三层GCN，使用MediaPipe FaceMesh提取的视觉和几何特征，在NAO机器人呼叫名字的实验场景下分析情绪反应。


<details>
  <summary>Details</summary>
Motivation: 理解自闭症儿童在社交互动中的情绪反应是发展心理学和人机交互领域的关键挑战。当前缺乏针对自闭症儿童的大规模、真实世界数据集和分析管道，特别是在社交机器人交互场景下的情感分析。

Method: 1. 构建包含约50,000个面部帧的数据集，来自15名自闭症儿童与NAO机器人互动的视频
2. 使用MediaPipe FaceMesh提取面部地标特征
3. 提出混合模型：微调的ResNet-50 CNN + 三层图卷积网络(GCN)
4. 使用DeepFace和FER模型的加权集成进行概率标签生成
5. 通过Kullback-Leibler散度优化融合嵌入进行最终分类

Result: 该方法在建模细微情感反应方面表现出稳健性能，能够有效捕捉神经多样性儿童的微观情绪线索，为临床和治疗性人机交互中的自闭症儿童情感分析提供了有前景的解决方案。

Conclusion: 这是印度首个针对自闭症情感分析的大规模真实世界数据集和管道，为未来个性化辅助技术奠定了重要基础，填补了自闭症特定人机交互研究的空白。

Abstract: Understanding emotional responses in children with Autism Spectrum Disorder (ASD) during social interaction remains a critical challenge in both developmental psychology and human-robot interaction. This study presents a novel deep learning pipeline for emotion recognition in autistic children in response to a name-calling event by a humanoid robot (NAO), under controlled experimental settings. The dataset comprises of around 50,000 facial frames extracted from video recordings of 15 children with ASD. A hybrid model combining a fine-tuned ResNet-50-based Convolutional Neural Network (CNN) and a three-layer Graph Convolutional Network (GCN) trained on both visual and geometric features extracted from MediaPipe FaceMesh landmarks. Emotions were probabilistically labeled using a weighted ensemble of two models: DeepFace's and FER, each contributing to soft-label generation across seven emotion classes. Final classification leveraged a fused embedding optimized via Kullback-Leibler divergence. The proposed method demonstrates robust performance in modeling subtle affective responses and offers significant promise for affective profiling of ASD children in clinical and therapeutic human-robot interaction contexts, as the pipeline effectively captures micro emotional cues in neurodivergent children, addressing a major gap in autism-specific HRI research. This work represents the first such large-scale, real-world dataset and pipeline from India on autism-focused emotion analysis using social robotics, contributing an essential foundation for future personalized assistive technologies.

</details>


### [255] [Ultra-Low Bitrate Perceptual Image Compression with Shallow Encoder](https://arxiv.org/abs/2512.12229)
*Tianyu Zhang,Dong Liu,Chang Wen Chen*

Main category: cs.CV

Relevance: 25.0

TL;DR: 提出AEIC框架，使用浅层编码器和一步扩散解码器实现超低比特率图像压缩，在保持高质量重建的同时显著提升编码效率


<details>
  <summary>Details</summary>
Motivation: 现有超低比特率图像压缩方法依赖大型预训练编码器，不适合部署在计算能力有限的边缘设备上，需要更轻量的编码方案

Method: AEIC框架采用浅层编码器网络和一步扩散解码器，设计双端特征蒸馏方案将知识从中等编码器迁移到浅层编码器变体

Result: 在超低比特率下优于现有方法的率失真感知性能，编码效率达35.8 FPS（1080P输入），解码速度与现有方法相当

Conclusion: 证明了浅层编码器在超低比特率压缩中的可行性，为边缘设备部署提供了高效解决方案

Abstract: Ultra-low bitrate image compression (below 0.05 bits per pixel) is increasingly critical for bandwidth-constrained and computation-limited encoding scenarios such as edge devices. Existing frameworks typically rely on large pretrained encoders (e.g., VAEs or tokenizer-based models) and perform transform coding within their generative latent space. While these approaches achieve impressive perceptual fidelity, their reliance on heavy encoder networks makes them unsuitable for deployment on weak sender devices. In this work, we explore the feasibility of applying shallow encoders for ultra-low bitrate compression and propose a novel Asymmetric Extreme Image Compression (AEIC) framework that pursues simultaneously encoding simplicity and decoding quality. Specifically, AEIC employs moderate or even shallow encoder networks, while leveraging an one-step diffusion decoder to maintain high-fidelity and high-realism reconstructions under extreme bitrates. To further enhance the efficiency of shallow encoders, we design a dual-side feature distillation scheme that transfers knowledge from AEIC with moderate encoders to its shallow encoder variants. Experiments demonstrate that AEIC not only outperforms existing methods on rate-distortion-perception performance at ultra-low bitrates, but also delivers exceptional encoding efficiency for 35.8 FPS on 1080P input images, while maintaining competitive decoding speed compared to existing methods.

</details>


### [256] [M4Human: A Large-Scale Multimodal mmWave Radar Benchmark for Human Mesh Reconstruction](https://arxiv.org/abs/2512.12378)
*Junqiao Fan,Yunjiao Zhou,Yizhuo Yang,Xinyuan Cui,Jiarui Zhang,Lihua Xie,Jianfei Yang,Chris Xiaoxuan Lu,Fangqiang Ding*

Main category: cs.CV

Relevance: 25.0

TL;DR: M4Human是一个大规模多模态基准数据集，包含66.1万帧高分辨率毫米波雷达、RGB和深度数据，用于隐私保护的人体网格重建研究。


<details>
  <summary>Details</summary>
Motivation: 现有基于视觉的人体网格重建数据集受限于遮挡、光照变化和隐私问题，而现有的雷达数据集规模小、标注稀疏且动作简单。需要大规模多模态基准来推动隐私保护的人体感知研究。

Method: 构建了当前最大规模的多模态数据集M4Human，包含原始雷达张量和处理后的雷达点云，提供高质量的动作捕捉标注（3D网格和全局轨迹），涵盖20名受试者和50种多样化动作。

Result: 建立了雷达张量和雷达点云两种模态的基准，以及与RGB-D模态的多模态融合基准。实验结果表明M4Human对雷达人体建模具有重要意义，同时揭示了快速无约束运动下的持续挑战。

Conclusion: M4Human数据集将推动隐私保护人体网格重建研究，特别是在雷达信号处理和跨模态融合方面，为人体-环境交互分析提供新途径。

Abstract: Human mesh reconstruction (HMR) provides direct insights into body-environment interaction, which enables various immersive applications. While existing large-scale HMR datasets rely heavily on line-of-sight RGB input, vision-based sensing is limited by occlusion, lighting variation, and privacy concerns. To overcome these limitations, recent efforts have explored radio-frequency (RF) mmWave radar for privacy-preserving indoor human sensing. However, current radar datasets are constrained by sparse skeleton labels, limited scale, and simple in-place actions. To advance the HMR research community, we introduce M4Human, the current largest-scale (661K-frame) ($9\times$ prior largest) multimodal benchmark, featuring high-resolution mmWave radar, RGB, and depth data. M4Human provides both raw radar tensors (RT) and processed radar point clouds (RPC) to enable research across different levels of RF signal granularity. M4Human includes high-quality motion capture (MoCap) annotations with 3D meshes and global trajectories, and spans 20 subjects and 50 diverse actions, including in-place, sit-in-place, and free-space sports or rehabilitation movements. We establish benchmarks on both RT and RPC modalities, as well as multimodal fusion with RGB-D modalities. Extensive results highlight the significance of M4Human for radar-based human modeling while revealing persistent challenges under fast, unconstrained motion. The dataset and code will be released after the paper publication.

</details>


### [257] [A Graph Attention Network-Based Framework for Reconstructing Missing LiDAR Beams](https://arxiv.org/abs/2512.12410)
*Khalfalla Awedat,Mohamed Abidalrekab,Mohammad El-Yabroudi*

Main category: cs.CV

Relevance: 25.0

TL;DR: 基于图注意力网络（GAT）的框架，仅使用当前LiDAR帧重建缺失的垂直光束通道，无需相机图像或时序信息，通过空间图表示和自适应注意力权重学习恢复传感器退化下的点云数据。


<details>
  <summary>Details</summary>
Motivation: LiDAR传感器因硬件老化、灰尘、雪、雾或强反射等原因导致垂直光束丢失，这会移除点云中的整个垂直切片，严重降低自动驾驶车辆的3D感知能力。现有方法通常依赖相机图像或时序信息，而本文旨在仅使用当前LiDAR帧的几何信息来解决这一问题。

Method: 将每个LiDAR扫描表示为非结构化空间图：点作为节点，边连接邻近点并保留原始光束索引顺序。使用多层图注意力网络（GAT）学习局部几何邻域的自适应注意力权重，直接回归缺失位置的高程（z）值。在模拟通道丢失的KITTI数据集上进行训练和评估。

Result: 在1,065个原始KITTI序列上，平均高度RMSE为11.67厘米，87.98%的重建点落在10厘米误差阈值内。单GPU上每帧推理时间为14.65秒，重建质量对不同邻域大小k保持稳定。

Conclusion: 纯图注意力模型仅基于原始点云几何信息就能有效恢复传感器退化下的丢失垂直光束，为LiDAR数据修复提供了有效的单帧解决方案。

Abstract: Vertical beam dropout in spinning LiDAR sensors triggered by hardware aging, dust, snow, fog, or bright reflections removes entire vertical slices from the point cloud and severely degrades 3D perception in autonomous vehicles. This paper proposes a Graph Attention Network (GAT)-based framework that reconstructs these missing vertical channels using only the current LiDAR frame, with no camera images or temporal information required. Each LiDAR sweep is represented as an unstructured spatial graph: points are nodes and edges connect nearby points while preserving the original beam-index ordering. A multi-layer GAT learns adaptive attention weights over local geometric neighborhoods and directly regresses the missing elevation (z) values at dropout locations. Trained and evaluated on 1,065 raw KITTI sequences with simulated channel dropout, the method achieves an average height RMSE of 11.67 cm, with 87.98% of reconstructed points falling within a 10 cm error threshold. Inference takes 14.65 seconds per frame on a single GPU, and reconstruction quality remains stable for different neighborhood sizes k. These results show that a pure graph attention model operating solely on raw point-cloud geometry can effectively recover dropped vertical beams under realistic sensor degradation.

</details>


### [258] [Animus3D: Text-driven 3D Animation via Motion Score Distillation](https://arxiv.org/abs/2512.12534)
*Qi Sun,Can Wang,Jiaxiang Shang,Wensen Feng,Jing Liao*

Main category: cs.CV

Relevance: 25.0

TL;DR: Animus3D：基于文本驱动的3D动画框架，通过Motion Score Distillation（MSD）替代传统SDS，为静态3D资产生成高质量运动场


<details>
  <summary>Details</summary>
Motivation: 现有方法使用vanilla Score Distillation Sampling（SDS）从预训练文本到视频扩散模型中提取运动，导致动画运动幅度小或抖动明显，需要更好的运动生成和外观保持方法

Method: 1. 提出Motion Score Distillation（MSD）替代SDS，使用LoRA增强的视频扩散模型定义静态源分布而非纯噪声；2. 基于反转的噪声估计技术保持外观一致性；3. 引入时空正则化项减少几何失真；4. 运动细化模块提升时间分辨率和细节

Result: Animus3D成功为多样文本提示生成静态3D资产的动画，相比SOTA基线产生更显著、更详细的运动，同时保持高视觉完整性

Conclusion: 提出的MSD方法和时空正则化有效解决了3D动画生成中的运动幅度不足和抖动问题，为文本驱动的3D动画提供了高质量解决方案

Abstract: We present Animus3D, a text-driven 3D animation framework that generates motion field given a static 3D asset and text prompt. Previous methods mostly leverage the vanilla Score Distillation Sampling (SDS) objective to distill motion from pretrained text-to-video diffusion, leading to animations with minimal movement or noticeable jitter. To address this, our approach introduces a novel SDS alternative, Motion Score Distillation (MSD). Specifically, we introduce a LoRA-enhanced video diffusion model that defines a static source distribution rather than pure noise as in SDS, while another inversion-based noise estimation technique ensures appearance preservation when guiding motion. To further improve motion fidelity, we incorporate explicit temporal and spatial regularization terms that mitigate geometric distortions across time and space. Additionally, we propose a motion refinement module to upscale the temporal resolution and enhance fine-grained details, overcoming the fixed-resolution constraints of the underlying video model. Extensive experiments demonstrate that Animus3D successfully animates static 3D assets from diverse text prompts, generating significantly more substantial and detailed motion than state-of-the-art baselines while maintaining high visual integrity. Code will be released at https://qiisun.github.io/animus3d_page.

</details>


### [259] [Patch-wise Retrieval: A Bag of Practical Techniques for Instance-level Matching](https://arxiv.org/abs/2512.12610)
*Wonseok Choi,Sohwi Lim,Nam Hyeon-Woo,Moon Ye-Bin,Dong-Ju Jeong,Jinyoung Hwang,Tae-Hyun Oh*

Main category: cs.CV

Relevance: 25.0

TL;DR: Patchify是一个无需微调的图像检索框架，通过将数据库图像分割成结构化补丁并与查询描述符进行局部特征匹配，实现高性能、可扩展且可解释的实例级图像检索。同时提出了LocScore评估指标来衡量检索结果的空间定位准确性。


<details>
  <summary>Details</summary>
Motivation: 现有图像检索方法通常使用全局特征，难以处理对象大小、位置或外观的变化。需要一种既能保持高性能，又能提供空间定位准确性和可解释性的检索框架。

Method: 1. Patchify框架：将数据库图像分割成少量结构化补丁，使用局部特征与全局查询描述符进行匹配
2. LocScore指标：评估检索结果的空间定位准确性，量化检索区域与目标对象对齐程度
3. 结合产品量化实现大规模高效检索

Result: 在多个基准测试、骨干网络和区域选择策略上，Patchify优于全局方法，并能与最先进的重新排序流程互补。产品量化结合信息特征显著提升大规模检索性能。

Conclusion: Patchify提供了一个简单有效的补丁级检索框架，在保持高性能的同时提供空间定位准确性和可解释性。LocScore作为诊断工具有助于理解和改进检索行为。

Abstract: Instance-level image retrieval aims to find images containing the same object as a given query, despite variations in size, position, or appearance. To address this challenging task, we propose Patchify, a simple yet effective patch-wise retrieval framework that offers high performance, scalability, and interpretability without requiring fine-tuning. Patchify divides each database image into a small number of structured patches and performs retrieval by comparing these local features with a global query descriptor, enabling accurate and spatially grounded matching. To assess not just retrieval accuracy but also spatial correctness, we introduce LocScore, a localization-aware metric that quantifies whether the retrieved region aligns with the target object. This makes LocScore a valuable diagnostic tool for understanding and improving retrieval behavior. We conduct extensive experiments across multiple benchmarks, backbones, and region selection strategies, showing that Patchify outperforms global methods and complements state-of-the-art reranking pipelines. Furthermore, we apply Product Quantization for efficient large-scale retrieval and highlight the importance of using informative features during compression, which significantly boosts performance. Project website: https://wons20k.github.io/PatchwiseRetrieval/

</details>


### [260] [L-STEC: Learned Video Compression with Long-term Spatio-Temporal Enhanced Context](https://arxiv.org/abs/2512.12790)
*Tiange Zhang,Zhimeng Huang,Xiandong Meng,Kai Zhang,Zhipin Deng,Siwei Ma*

Main category: cs.CV

Relevance: 25.0

TL;DR: L-STEC方法通过LSTM扩展参考链捕获长期依赖，并融合像素域的空间上下文，显著提升神经视频压缩性能，相比DCVC-TCM节省37.01%比特率。


<details>
  <summary>Details</summary>
Motivation: 现有神经视频压缩方法仅依赖前一帧特征预测时域上下文，存在两个关键问题：1) 短参考窗口无法捕获长期依赖和精细纹理细节；2) 仅传播特征级信息会导致误差累积和纹理丢失。

Method: 提出L-STEC方法：1) 使用LSTM扩展参考链捕获长期依赖；2) 从像素域引入扭曲空间上下文；3) 通过多感受野网络融合时空信息以更好保留参考细节。

Result: L-STEC显著提升压缩性能，相比DCVC-TCM在PSNR上节省37.01%比特率，在MS-SSIM上节省31.65%，优于VTM-17.0和DCVC-FM，达到新的SOTA。

Conclusion: 通过丰富上下文信息，L-STEC方法有效解决了现有神经视频压缩中的长期依赖和误差累积问题，实现了显著的比特率节省和性能提升。

Abstract: Neural Video Compression has emerged in recent years, with condition-based frameworks outperforming traditional codecs. However, most existing methods rely solely on the previous frame's features to predict temporal context, leading to two critical issues. First, the short reference window misses long-term dependencies and fine texture details. Second, propagating only feature-level information accumulates errors over frames, causing prediction inaccuracies and loss of subtle textures. To address these, we propose the Long-term Spatio-Temporal Enhanced Context (L-STEC) method. We first extend the reference chain with LSTM to capture long-term dependencies. We then incorporate warped spatial context from the pixel domain, fusing spatio-temporal information through a multi-receptive field network to better preserve reference details. Experimental results show that L-STEC significantly improves compression by enriching contextual information, achieving 37.01% bitrate savings in PSNR and 31.65% in MS-SSIM compared to DCVC-TCM, outperforming both VTM-17.0 and DCVC-FM and establishing new state-of-the-art performance.

</details>


### [261] [Content Adaptive based Motion Alignment Framework for Learned Video Compression](https://arxiv.org/abs/2512.12936)
*Tiange Zhang,Xiandong Meng,Siwei Ma*

Main category: cs.CV

Relevance: 25.0

TL;DR: 提出基于内容自适应的运动对齐框架CAMA，通过两阶段流引导可变形扭曲、多参考质量感知策略和无训练运动平滑模块，显著提升神经视频压缩性能


<details>
  <summary>Details</summary>
Motivation: 当前端到端视频压缩框架缺乏内容特定适应性，导致压缩性能不理想。需要针对不同内容特性调整编码策略，提升压缩效率。

Method: 1. 两阶段流引导可变形扭曲机制：通过粗到细的偏移预测和掩码调制精化运动补偿；2. 多参考质量感知策略：基于参考质量调整失真权重，结合分层训练减少误差传播；3. 无训练模块：根据运动幅度和分辨率下采样帧以获得平滑运动估计

Result: 在标准测试数据集上，CAMA框架相比基线模型DCVC-TCM实现24.95% BD-rate（PSNR）节省，同时优于复现的DCVC-DC和传统编解码器HM-16.25

Conclusion: 提出的内容自适应运动对齐框架通过精确特征对齐、质量感知策略和运动平滑，显著提升了神经视频压缩性能，证明了内容自适应在视频压缩中的重要性

Abstract: Recent advances in end-to-end video compression have shown promising results owing to their unified end-to-end learning optimization. However, such generalized frameworks often lack content-specific adaptation, leading to suboptimal compression performance. To address this, this paper proposes a content adaptive based motion alignment framework that improves performance by adapting encoding strategies to diverse content characteristics. Specifically, we first introduce a two-stage flow-guided deformable warping mechanism that refines motion compensation with coarse-to-fine offset prediction and mask modulation, enabling precise feature alignment. Second, we propose a multi-reference quality aware strategy that adjusts distortion weights based on reference quality, and applies it to hierarchical training to reduce error propagation. Third, we integrate a training-free module that downsamples frames by motion magnitude and resolution to obtain smooth motion estimation. Experimental results on standard test datasets demonstrate that our framework CAMA achieves significant improvements over state-of-the-art Neural Video Compression models, achieving a 24.95% BD-rate (PSNR) savings over our baseline model DCVC-TCM, while also outperforming reproduced DCVC-DC and traditional codec HM-16.25.

</details>


### [262] [Comprehensive Deployment-Oriented Assessment for Cross-Environment Generalization in Deep Learning-Based mmWave Radar Sensing](https://arxiv.org/abs/2512.13018)
*Tomoya Tanaka,Tomonori Ikeda,Ryo Yonemoto*

Main category: cs.CV

Relevance: 25.0

TL;DR: 该研究首次全面评估了深度学习射频感知中的空间泛化技术，重点研究室内人员计数，发现基于幅度的预处理和迁移学习能显著提升跨环境性能。


<details>
  <summary>Details</summary>
Motivation: 深度学习在射频感知中的应用面临空间泛化挑战，即模型在不同环境中的性能下降问题。该研究旨在系统评估各种空间泛化技术，为实际部署提供指导。

Method: 使用FMCW MIMO雷达进行室内人员计数，系统评估了幅度统计预处理（sigmoid加权和阈值归零）、频域滤波、自编码器背景抑制、数据增强策略和迁移学习等多种方法。

Result: sigmoid幅度加权在跨环境性能上表现最佳，相比基线方法分别降低50.1% RMSE和55.2% MAE；数据增强提供额外8.8% MAE改进；迁移学习在大空间变化下至关重要，使用540个目标域样本可降低82.1% RMSE和91.3% MAE。

Conclusion: 通过结合深度学习模型、基于幅度的预处理和高效的迁移学习，可以开发出在空间变化下保持鲁棒准确性的雷达感知系统。

Abstract: This study presents the first comprehensive evaluation of spatial generalization techniques, which are essential for the practical deployment of deep learning-based radio-frequency (RF) sensing. Focusing on people counting in indoor environments using frequency-modulated continuous-wave (FMCW) multiple-input multiple-output (MIMO) radar, we systematically investigate a broad set of approaches, including amplitude-based statistical preprocessing (sigmoid weighting and threshold zeroing), frequency-domain filtering, autoencoder-based background suppression, data augmentation strategies, and transfer learning. Experimental results collected across two environments with different layouts demonstrate that sigmoid-based amplitude weighting consistently achieves superior cross-environment performance, yielding 50.1% and 55.2% reductions in root-mean-square error (RMSE) and mean absolute error (MAE), respectively, compared with baseline methods. Data augmentation provides additional though modest benefits, with improvements up to 8.8% in MAE. By contrast, transfer learning proves indispensable for large spatial shifts, achieving 82.1% and 91.3% reductions in RMSE and MAE, respectively, with 540 target-domain samples. Taken together, these findings establish a highly practical direction for developing radar sensing systems capable of maintaining robust accuracy under spatial variations by integrating deep learning models with amplitude-based preprocessing and efficient transfer learning.

</details>


### [263] [POLAR: A Portrait OLAT Dataset and Generative Framework for Illumination-Aware Face Modeling](https://arxiv.org/abs/2512.13192)
*Zhuo Chen,Chengqun Yang,Zhuo Su,Zheng Lv,Jingnan Gao,Xiaoyuan Zhang,Xiaokang Yang,Yichao Yan*

Main category: cs.CV

Relevance: 25.0

TL;DR: POLAR：大规模物理校准的单光源数据集与基于流的生成模型，用于可控人脸重光照


<details>
  <summary>Details</summary>
Motivation: 人脸重光照技术受限于大规模、物理一致的光照数据可用性，现有方法依赖统计或上下文线索，缺乏物理可解释性和可控性

Method: 1) 构建POLAR数据集：包含200+受试者，156个光照方向，多视角和表情；2) 开发POLARNet：基于流的生成模型，从单张肖像预测每个光源的OLAT响应，建模光照作为连续物理可解释的变换

Result: 建立了统一的光照学习框架，连接真实数据、生成合成和物理基础重光照，形成可扩展、可复现的"鸡与蛋"循环

Conclusion: POLAR和POLARNet为可控人脸重光照提供了物理可解释的解决方案，克服了现有扩散或背景条件方法的局限性

Abstract: Face relighting aims to synthesize realistic portraits under novel illumination while preserving identity and geometry. However, progress remains constrained by the limited availability of large-scale, physically consistent illumination data. To address this, we introduce POLAR, a large-scale and physically calibrated One-Light-at-a-Time (OLAT) dataset containing over 200 subjects captured under 156 lighting directions, multiple views, and diverse expressions. Building upon POLAR, we develop a flow-based generative model POLARNet that predicts per-light OLAT responses from a single portrait, capturing fine-grained and direction-aware illumination effects while preserving facial identity. Unlike diffusion or background-conditioned methods that rely on statistical or contextual cues, our formulation models illumination as a continuous, physically interpretable transformation between lighting states, enabling scalable and controllable relighting. Together, POLAR and POLARNet form a unified illumination learning framework that links real data, generative synthesis, and physically grounded relighting, establishing a self-sustaining "chicken-and-egg" cycle for scalable and reproducible portrait illumination.

</details>


### [264] [A Domain-Adapted Lightweight Ensemble for Resource-Efficient Few-Shot Plant Disease Classification](https://arxiv.org/abs/2512.13428)
*Anika Islam,Tasfia Tahsin,Zaarin Anjum,Md. Bakhtiar Hasan,Md. Hasanul Kabir*

Main category: cs.CV

Relevance: 25.0

TL;DR: 提出了一种用于植物叶片病害识别的轻量级少样本学习框架，结合了MobileNet特征提取器、特征融合技术和注意力增强的Bi-LSTM分类器，在资源受限和数据稀缺环境下实现了高性能的病害识别。


<details>
  <summary>Details</summary>
Motivation: 当前深度学习植物病害识别方法依赖大规模标注数据和计算密集型模型，不适用于数据稀缺和资源受限的农业环境。需要开发轻量级、高效的少样本学习方法来解决这些挑战。

Method: 1) 使用领域适应的MobileNetV2和MobileNetV3作为特征提取器；2) 采用特征融合技术生成鲁棒特征表示；3) 使用注意力机制增强的Bi-LSTM分类器捕捉序列依赖关系；4) 在少样本设置下（1-15 shot）进行训练和评估。

Result: 在PlantVillage数据集上，15-shot时达到98.23±0.33%，接近SOTA的99.98%；在真实场景的Dhan Shomadhan数据集上，15-shot时达到69.28±1.49%；在PlantVillage的6种病害上，15-shot时达到99.72%，超越之前的SOTA（96.0%）。模型大小约40MB，计算复杂度约1.12 GFLOPs。

Conclusion: 该工作建立了一个可扩展、移动就绪的植物病害诊断基础，特别适用于数据稀缺地区，在保持轻量级的同时实现了接近SOTA的性能。

Abstract: Accurate and timely identification of plant leaf diseases is essential for resilient and sustainable agriculture, yet most deep learning approaches rely on large annotated datasets and computationally intensive models that are unsuitable for data-scarce and resource-constrained environments. To address these challenges we present a few-shot learning approach within a lightweight yet efficient framework that combines domain-adapted MobileNetV2 and MobileNetV3 models as feature extractors, along with a feature fusion technique to generate robust feature representation. For the classification task, the fused features are passed through a Bi-LSTM classifier enhanced with attention mechanisms to capture sequential dependencies and focus on the most relevant features, thereby achieving optimal classification performance even in complex, real-world environments with noisy or cluttered backgrounds. The proposed framework was evaluated across multiple experimental setups, including both laboratory-controlled and field-captured datasets. On tomato leaf diseases from the PlantVillage dataset, it consistently improved performance across 1 to 15 shot scenarios, reaching 98.23+-0.33% at 15 shot, closely approaching the 99.98% SOTA benchmark achieved by a Transductive LSTM with attention, while remaining lightweight and mobile-friendly. Under real-world conditions using field images from the Dhan Shomadhan dataset, it maintained robust performance, reaching 69.28+-1.49% at 15-shot and demonstrating strong resilience to complex backgrounds. Notably, it also outperformed the previous SOTA accuracy of 96.0% on six diseases from PlantVillage, achieving 99.72% with only 15-shot learning. With a compact model size of approximately 40 MB and inference complexity of approximately 1.12 GFLOPs, this work establishes a scalable, mobile-ready foundation for precise plant disease diagnostics in data-scarce regions.

</details>


### [265] [IMILIA: interpretable multiple instance learning for inflammation prediction in IBD from H&E whole slide images](https://arxiv.org/abs/2512.13440)
*Thalyssa Baiocco-Rodrigues,Antoine Olivier,Reda Belbahri,Thomas Duboudin,Pierre-Antoine Bannier,Benjamin Adjadj,Katharina Von Loga,Nathan Noiry,Maxime Touzot,Hector Roux de Bezieux*

Main category: cs.CV

Relevance: 25.0

TL;DR: IMILIA是一个用于炎症性肠病(IBD)组织病理学分析的端到端框架，结合多示例学习(MIL)进行炎症预测和可解释性模块，在多个数据集上表现出色。


<details>
  <summary>Details</summary>
Motivation: 随着IBD治疗目标转向组织学缓解，准确评估微观炎症对于评估疾病活动性和治疗反应变得越来越重要。需要自动化、可解释的炎症分析方法。

Method: IMILIA包含两个模块：1) 炎症预测模块：使用多示例学习(MIL)模型预测H&E染色数字切片中的炎症存在；2) 可解释性模块：包括HistoPLUS（细胞检测、分割和分类）和EpiSeg（上皮分割），自动计算驱动预测的组织区域特征标记。

Result: 在发现队列中获得交叉验证ROC-AUC为0.83，在两个外部验证队列中分别获得0.99和0.84的ROC-AUC。可解释性模块显示：高预测分数区域免疫细胞密度增加，低分数区域主要为正常上皮细胞，这些模式在所有数据集中一致。

Conclusion: IMILIA提供了一个有效的端到端框架，能够准确预测IBD组织切片中的炎症，并通过可解释性模块提供生物学一致的见解，有助于临床决策和治疗监测。

Abstract: As the therapeutic target for Inflammatory Bowel Disease (IBD) shifts toward histologic remission, the accurate assessment of microscopic inflammation has become increasingly central for evaluating disease activity and response to treatment. In this work, we introduce IMILIA (Interpretable Multiple Instance Learning for Inflammation Analysis), an end-to-end framework designed for the prediction of inflammation presence in IBD digitized slides stained with hematoxylin and eosin (H&E), followed by the automated computation of markers characterizing tissue regions driving the predictions. IMILIA is composed of an inflammation prediction module, consisting of a Multiple Instance Learning (MIL) model, and an interpretability module, divided in two blocks: HistoPLUS, for cell instance detection, segmentation and classification; and EpiSeg, for epithelium segmentation. IMILIA achieves a cross-validation ROC-AUC of 0.83 on the discovery cohort, and a ROC-AUC of 0.99 and 0.84 on two external validation cohorts. The interpretability module yields biologically consistent insights: tiles with higher predicted scores show increased densities of immune cells (lymphocytes, plasmocytes, neutrophils and eosinophils), whereas lower-scored tiles predominantly contain normal epithelial cells. Notably, these patterns were consistent across all datasets. Code and models to partially replicate the results on the public IBDColEpi dataset can be found at https://github.com/owkin/imilia.

</details>


### [266] [Charge: A Comprehensive Novel View Synthesis Benchmark and Dataset to Bind Them All](https://arxiv.org/abs/2512.13639)
*Michal Nazarczuk,Thomas Tanay,Arthur Moreau,Zhensong Zhang,Eduardo Pérez-Pellitero*

Main category: cs.CV

Relevance: 25.0

TL;DR: 提出了一个用于新视角合成的高质量动画电影数据集，包含动态场景、多模态标注和三种基准测试场景


<details>
  <summary>Details</summary>
Motivation: 当前新视角合成和4D场景重建领域缺乏高质量、多模态、动态场景的数据集，限制了相关模型的训练和评估

Method: 从高质量动画电影中生成数据集，包含RGB图像、深度、表面法线、物体分割和光流等多模态标注，组织成密集多视角、稀疏视角和单目视频三种基准场景

Result: 创建了一个视觉丰富、标注质量高、实验设置多样的数据集，为视图合成和3D视觉研究提供了独特资源

Conclusion: 该数据集能够推动新视角合成和3D视觉领域的边界，支持不同数据稀疏程度下的实验和比较

Abstract: This paper presents a new dataset for Novel View Synthesis, generated from a high-quality, animated film with stunning realism and intricate detail. Our dataset captures a variety of dynamic scenes, complete with detailed textures, lighting, and motion, making it ideal for training and evaluating cutting-edge 4D scene reconstruction and novel view generation models. In addition to high-fidelity RGB images, we provide multiple complementary modalities, including depth, surface normals, object segmentation and optical flow, enabling a deeper understanding of scene geometry and motion. The dataset is organised into three distinct benchmarking scenarios: a dense multi-view camera setup, a sparse camera arrangement, and monocular video sequences, enabling a wide range of experimentation and comparison across varying levels of data sparsity. With its combination of visual richness, high-quality annotations, and diverse experimental setups, this dataset offers a unique resource for pushing the boundaries of view synthesis and 3D vision.

</details>


### [267] [A Reproducible Workflow for Scraping, Structuring, and Segmenting Legacy Archaeological Artifact Images](https://arxiv.org/abs/2512.11817)
*Juan Palomeque-Gonzalez*

Main category: cs.CY

Relevance: 25.0

TL;DR: 开发了一个可重复的工作流程，将考古图像集合转换为结构化、可用于分割的数据集，包括网页抓取工具和图像处理流水线，生成COCO格式的标注文件。


<details>
  <summary>Details</summary>
Motivation: 解决考古图像集合缺乏批量下载和自动化处理机制的问题，将传统考古图像转换为机器学习友好的格式，促进数字考古的可重复研究。

Method: 1. 开发网页抓取脚本，从Archaeology Data Service获取记录页面、提取元数据、下载图像；2. 创建图像处理流水线，使用UUID重命名文件，通过传统计算机视觉生成二值掩码和边界框，将所有信息存储在COCO兼容的JSON文件中。

Result: 成功将旧石器时代手斧和双面器图像集合转换为结构化数据集，生成了掩码、轮廓和标注等衍生产品，提供了轻量级、可重用的转换方法。

Conclusion: 该工作流程为基于网络的考古图像集合转换为机器学习格式提供了实用解决方案，促进了数字考古领域的下游分析和可重复研究实践。

Abstract: This technical note presents a reproducible workflow for converting a legacy archaeological image collection into a structured and segmentation ready dataset. The case study focuses on the Lower Palaeolithic hand axe and biface collection curated by the Archaeology Data Service (ADS), a dataset that provides thousands of standardised photographs but no mechanism for bulk download or automated processing. To address this, two open source tools were developed: a web scraping script that retrieves all record pages, extracts associated metadata, and downloads the available images while respecting ADS Terms of Use and ethical scraping guidelines; and an image processing pipeline that renames files using UUIDs, generates binary masks and bounding boxes through classical computer vision, and stores all derived information in a COCO compatible Json file enriched with archaeological metadata. The original images are not redistributed, and only derived products such as masks, outlines, and annotations are shared. Together, these components provide a lightweight and reusable approach for transforming web based archaeological image collections into machine learning friendly formats, facilitating downstream analysis and contributing to more reproducible research practices in digital archaeology.

</details>


### [268] [JPEG-Inspired Cloud-Edge Holography](https://arxiv.org/abs/2512.12367)
*Shuyang Xie,Jie Zhou,Jun Wang,Renjing Xu*

Main category: physics.optics

Relevance: 25.0

TL;DR: 提出JPEG启发的云边全息成像框架，通过可学习变换编解码器实现高效全息图流式传输，云端处理繁重计算，边缘设备仅需轻量解码，无需神经网络推理。


<details>
  <summary>Details</summary>
Motivation: 当前神经全息成像方法在紧凑眼镜设备上部署面临计算和能耗限制，云端卸载+自然图像编解码会扭曲相位信息且带宽需求高，神经压缩方法在边缘需要重型解码器增加延迟和硬件需求。

Method: 设计基于可学习变换编解码器的JPEG风格框架，保留JPEG的块结构和硬件友好特性，将所有繁重神经处理移至云端，边缘仅执行轻量解码，并实现自定义CUDA内核进行熵编码。

Result: 在<2比特/像素下达到32.15 dB峰值信噪比，解码延迟低至4.2毫秒，数值模拟和光学实验均证实高质量全息图重建。

Conclusion: 通过结合JPEG结构效率与可学习组件，该框架能在资源受限可穿戴设备上实现低延迟、带宽高效的全息图流式传输，仅需现代片上系统支持的简单块解码，无需神经解码器或专用硬件。

Abstract: Computer-generated holography (CGH) presents a transformative solution for near-eye displays in augmented and virtual reality. Recent advances in deep learning have greatly improved CGH in reconstructed quality and computational efficiency. However, deploying neural CGH pipelines directly on compact, eyeglass-style devices is hindered by stringent constraints on computation and energy consumption, while cloud offloading followed by transmission with natural image codecs often distorts phase information and requires high bandwidth to maintain reconstruction quality. Neural compression methods can reduce bandwidth but impose heavy neural decoders at the edge, increasing inference latency and hardware demand. In this work, we introduce JPEG-Inspired Cloud-Edge Holography, an efficient pipeline designed around a learnable transform codec that retains the block-structured and hardware-friendly nature of JPEG. Our system shifts all heavy neural processing to the cloud, while the edge device performs only lightweight decoding without any neural inference. To further improve throughput, we implement custom CUDA kernels for entropy coding on both cloud and edge. This design achieves a peak signal-to-noise ratio of 32.15 dB at $<$ 2 bits per pixel with decode latency as low as 4.2 ms. Both numerical simulations and optical experiments confirm the high reconstruction quality of the holograms. By aligning CGH with a codec that preserves JPEG's structural efficiency while extending it with learnable components, our framework enables low-latency, bandwidth-efficient hologram streaming on resource-constrained wearable devices-using only simple block-based decoding readily supported by modern system-on-chips, without requiring neural decoders or specialized hardware.

</details>


### [269] [SLIM-VDB: A Real-Time 3D Probabilistic Semantic Mapping Framework](https://arxiv.org/abs/2512.12945)
*Anja Sheppard,Parker Ewen,Joey Wilson,Advaith V. Sethuraman,Benard Adewole,Anran Li,Yuzhen Chen,Ram Vasudevan,Katherine A. Skinner*

Main category: cs.RO

Relevance: 25.0

TL;DR: SLIM-VDB：基于OpenVDB数据结构的轻量级语义建图系统，支持闭集和开集语义融合的统一贝叶斯更新框架


<details>
  <summary>Details</summary>
Motivation: 现有语义建图系统存在两个主要问题：1) 虽然OpenVDB在计算机图形学中展示了高效的体素场景表示，但在机器人语义建图中尚未被探索；2) 缺乏能够同时集成固定类别和开放语言标签预测的统一框架

Method: 提出基于OpenVDB数据结构的3D语义建图系统，采用统一的贝叶斯更新框架，支持闭集（固定类别）和开集（开放语言）语义融合

Result: 相比现有最先进的语义建图方法，SLIM-VDB在内存使用和建图时间上显著减少，同时保持相当的建图精度

Conclusion: SLIM-VDB是一个高效、轻量级的语义建图系统，通过OpenVDB数据结构和统一贝叶斯框架，解决了闭集和开集语义融合的问题，并提供了开源实现

Abstract: This paper introduces SLIM-VDB, a new lightweight semantic mapping system with probabilistic semantic fusion for closed-set or open-set dictionaries. Advances in data structures from the computer graphics community, such as OpenVDB, have demonstrated significantly improved computational and memory efficiency in volumetric scene representation. Although OpenVDB has been used for geometric mapping in robotics applications, semantic mapping for scene understanding with OpenVDB remains unexplored. In addition, existing semantic mapping systems lack support for integrating both fixed-category and open-language label predictions within a single framework. In this paper, we propose a novel 3D semantic mapping system that leverages the OpenVDB data structure and integrates a unified Bayesian update framework for both closed- and open-set semantic fusion. Our proposed framework, SLIM-VDB, achieves significant reduction in both memory and integration times compared to current state-of-the-art semantic mapping approaches, while maintaining comparable mapping accuracy. An open-source C++ codebase with a Python interface is available at https://github.com/umfieldrobotics/slim-vdb.

</details>


### [270] [Intrinsic Image Fusion for Multi-View 3D Material Reconstruction](https://arxiv.org/abs/2512.13157)
*Peter Kocsis,Lukas Höllein,Matthias Nießner*

Main category: cs.CV

Relevance: 20.0

TL;DR: 提出Intrinsic Image Fusion方法，从多视角图像重建高质量物理材质，结合单视角先验和扩散模型，通过参数化函数拟合和鲁棒优化框架解决不一致性，优于现有方法


<details>
  <summary>Details</summary>
Motivation: 材质重建是高度欠约束的问题，通常依赖分析-合成方法，需要昂贵且噪声大的路径追踪。为了更好约束优化，需要将单视角先验融入重建过程

Method: 1) 使用基于扩散的材质估计器生成多候选分解；2) 拟合显式低维参数化函数减少不一致性；3) 提出鲁棒优化框架，结合软每视角预测选择和基于置信度的软多视角内点集；4) 使用逆路径追踪优化低维参数

Result: 在合成和真实场景的材质解缠任务上优于最先进方法，产生清晰干净的重建结果，适合高质量重光照

Conclusion: Intrinsic Image Fusion方法通过结合单视角先验和鲁棒优化，有效解决了多视角材质重建中的不一致性问题，实现了高质量的物理材质重建

Abstract: We introduce Intrinsic Image Fusion, a method that reconstructs high-quality physically based materials from multi-view images. Material reconstruction is highly underconstrained and typically relies on analysis-by-synthesis, which requires expensive and noisy path tracing. To better constrain the optimization, we incorporate single-view priors into the reconstruction process. We leverage a diffusion-based material estimator that produces multiple, but often inconsistent, candidate decompositions per view. To reduce the inconsistency, we fit an explicit low-dimensional parametric function to the predictions. We then propose a robust optimization framework using soft per-view prediction selection together with confidence-based soft multi-view inlier set to fuse the most consistent predictions of the most confident views into a consistent parametric material space. Finally, we use inverse path tracing to optimize for the low-dimensional parameters. Our results outperform state-of-the-art methods in material disentanglement on both synthetic and real scenes, producing sharp and clean reconstructions suitable for high-quality relighting.

</details>


### [271] [Hot Hém: Sài Gòn Giũa Cái Nóng Hông Còng Bàng -- Saigon in Unequal Heat](https://arxiv.org/abs/2512.11896)
*Tessa Vu*

Main category: cs.CV

Relevance: 15.0

TL;DR: Hot Hém是一个GeoAI工作流，结合Google街景图像、语义分割和遥感技术，预测胡志明市行人热暴露，实现热感知路径规划。


<details>
  <summary>Details</summary>
Motivation: 热带密集城市中行人热暴露是重要的健康风险，但标准路径规划算法通常忽略微观尺度的热变化。需要开发能够考虑热暴露的空间数据科学方法。

Method: 结合Google街景图像、语义图像分割和遥感技术，训练两个XGBoost模型预测地表温度。在选定行政区的GSV训练数据集上训练模型，然后部署到所有OSMnx衍生的行人网络节点上，实现热感知路由。

Result: 开发了一个能够估计和操作化行人热暴露的GeoAI工作流，可以识别城市走廊中温度异常高的区域，为理解基础设施尺度的热分布提供基础。

Conclusion: Hot Hém工作流为热带城市行人热暴露研究提供了实用工具，能够支持热感知路径规划，有助于识别和理解城市热岛效应的微观模式。

Abstract: Pedestrian heat exposure is a critical health risk in dense tropical cities, yet standard routing algorithms often ignore micro-scale thermal variation. Hot Hém is a GeoAI workflow that estimates and operationalizes pedestrian heat exposure in Hô Chí Minh City (HCMC), Vi\d{e}t Nam, colloquially known as Sài Gòn. This spatial data science pipeline combines Google Street View (GSV) imagery, semantic image segmentation, and remote sensing. Two XGBoost models are trained to predict land surface temperature (LST) using a GSV training dataset in selected administrative wards, known as phŏng, and are deployed in a patchwork manner across all OSMnx-derived pedestrian network nodes to enable heat-aware routing. This is a model that, when deployed, can provide a foundation for pinpointing where and further understanding why certain city corridors may experience disproportionately higher temperatures at an infrastructural scale.

</details>


### [272] [Microscopic Vehicle Trajectory Datasets from UAV-collected Video for Heterogeneous, Area-Based Urban Traffic](https://arxiv.org/abs/2512.11898)
*Yawar Ali,K. Ramachandra Rao,Ashish Bhaskar,Niladri Chatterjee*

Main category: cs.CV

Relevance: 15.0

TL;DR: 论文提供了基于无人机采集的微观车辆轨迹数据集，用于异构城市交通研究


<details>
  <summary>Details</summary>
Motivation: 传统路边视频采集在密集混合交通中存在遮挡、视角有限等问题，需要更好的数据采集方法来支持复杂城市交通环境的研究

Method: 使用无人机从俯视角度采集数据，通过Data from Sky平台提取，包含时间戳、位置、速度、加速度等车辆信息，采样率为30帧/秒

Result: 在印度国家首都区域的六个路段收集了数据集，涵盖不同交通组成和密度，验证了数据准确性并展示了关键行为模式

Conclusion: 这些开放数据集为全球研究社区提供了支持仿真建模、安全评估和行为研究的资源，有助于开发更准确的复杂城市交通模型

Abstract: This paper offers openly available microscopic vehicle trajectory (MVT) datasets collected using unmanned aerial vehicles (UAVs) in heterogeneous, area-based urban traffic conditions. Traditional roadside video collection often fails in dense mixed traffic due to occlusion, limited viewing angles, and irregular vehicle movements. UAV-based recording provides a top-down perspective that reduces these issues and captures rich spatial and temporal dynamics. The datasets described here were extracted using the Data from Sky (DFS) platform and validated against manual counts, space mean speeds, and probe trajectories in earlier work. Each dataset contains time-stamped vehicle positions, speeds, longitudinal and lateral accelerations, and vehicle classifications at a resolution of 30 frames per second. Data were collected at six mid-block locations in the national capital region of India, covering diverse traffic compositions and density levels. Exploratory analyses highlight key behavioural patterns, including lane-keeping preferences, speed distributions, and lateral manoeuvres typical of heterogeneous and area-based traffic settings. These datasets are intended as a resource for the global research community to support simulation modelling, safety assessment, and behavioural studies under area-based traffic conditions. By making these empirical datasets openly available, this work offers researchers a unique opportunity to develop, test, and validate models that more accurately represent complex urban traffic environments.

</details>


### [273] [Contextual Peano Scan and Fast Image Segmentation Using Hidden and Evidential Markov Chains](https://arxiv.org/abs/2512.11939)
*Clément Fernandes,Wojciech Pieczynski*

Main category: cs.CV

Relevance: 15.0

TL;DR: 提出了一种新的HEMC-CPS模型，结合上下文Peano扫描和证据隐马尔可夫链，用于无监督图像分割，比传统HMF方法更快且具有竞争力。


<details>
  <summary>Details</summary>
Motivation: 传统基于隐马尔可夫场（HMF）的图像分割方法计算复杂度高，而基于Peano扫描的隐马尔可夫链（HMC）方法虽然更快，但需要改进以处理更复杂的图像结构。上下文Peano扫描（CPS）和证据隐马尔可夫链（HEMC）的扩展显示了潜力，但尚未结合使用。

Method: 提出HEMC-CPS模型，将上下文Peano扫描与证据隐马尔可夫链相结合。使用随机期望最大化（SEM）方法进行无监督参数估计，采用贝叶斯最大后验模式（MPM）进行分割。

Result: 在合成和真实图像上的实验表明，HEMC-CPS模型在图像分割方面有效，能够处理更复杂的图像结构，如三维或多传感器多分辨率图像。

Conclusion: HEMC-CPS模型不仅限于图像分割，可应用于任何空间相关数据的建模和分割，为复杂图像分析提供了新工具。

Abstract: Transforming bi-dimensional sets of image pixels into mono-dimensional sequences with a Peano scan (PS) is an established technique enabling the use of hidden Markov chains (HMCs) for unsupervised image segmentation. Related Bayesian segmentation methods can compete with hidden Markov fields (HMFs)-based ones and are much faster. PS has recently been extended to the contextual PS, and some initial experiments have shown the value of the associated HMC model, denoted as HMC-CPS, in image segmentation. Moreover, HMCs have been extended to hidden evidential Markov chains (HEMCs), which are capable of improving HMC-based Bayesian segmentation. In this study, we introduce a new HEMC-CPS model by simultaneously considering contextual PS and evidential HMC. We show its effectiveness for Bayesian maximum posterior mode (MPM) segmentation using synthetic and real images. Segmentation is performed in an unsupervised manner, with parameters being estimated using the stochastic expectation--maximization (SEM) method. The new HEMC-CPS model presents potential for the modeling and segmentation of more complex images, such as three-dimensional or multi-sensor multi-resolution images. Finally, the HMC-CPS and HEMC-CPS models are not limited to image segmentation and could be used for any kind of spatially correlated data.

</details>


### [274] [Exploring Spatial-Temporal Representation via Star Graph for mmWave Radar-based Human Activity Recognition](https://arxiv.org/abs/2512.12013)
*Senhao Gao,Junqing Zhang,Luoyu Mei,Shuai Wang,Xuyu Wang*

Main category: cs.CV

Relevance: 15.0

TL;DR: 提出基于离散动态图神经网络（DDGNN）的毫米波雷达点云人体活动识别方法，通过星形图表示处理稀疏和变尺寸点云，在资源受限平台上实现高效识别。


<details>
  <summary>Details</summary>
Motivation: 毫米波雷达点云用于人体活动识别面临稀疏性和变尺寸问题，现有基于视觉密集点云的预处理方法不适用于雷达系统，需要专门的空间-时间特征提取方法。

Method: 设计星形图表示手动添加的静态中心点与动态雷达点之间的高维相对关系，采用离散动态图神经网络（DDGNN）学习变尺寸星形图中的特征。

Result: 在真实HAR数据集上达到94.27%的分类准确率，接近基于视觉骨架数据的97.25%最优性能，在Raspberry Pi 4上验证了资源受限平台的有效性。

Conclusion: 提出的DDGNN方法能有效处理毫米波雷达点云的稀疏性和变尺寸问题，无需重采样或帧聚合器，在资源受限平台上实现高效人体活动识别。

Abstract: Human activity recognition (HAR) requires extracting accurate spatial-temporal features with human movements. A mmWave radar point cloud-based HAR system suffers from sparsity and variable-size problems due to the physical features of the mmWave signal. Existing works usually borrow the preprocessing algorithms for the vision-based systems with dense point clouds, which may not be optimal for mmWave radar systems. In this work, we proposed a graph representation with a discrete dynamic graph neural network (DDGNN) to explore the spatial-temporal representation of human movement-related features. Specifically, we designed a star graph to describe the high-dimensional relative relationship between a manually added static center point and the dynamic mmWave radar points in the same and consecutive frames. We then adopted DDGNN to learn the features residing in the star graph with variable sizes. Experimental results demonstrated that our approach outperformed other baseline methods using real-world HAR datasets. Our system achieved an overall classification accuracy of 94.27\%, which gets the near-optimal performance with a vision-based skeleton data accuracy of 97.25\%. We also conducted an inference test on Raspberry Pi~4 to demonstrate its effectiveness on resource-constraint platforms. \sh{ We provided a comprehensive ablation study for variable DDGNN structures to validate our model design. Our system also outperformed three recent radar-specific methods without requiring resampling or frame aggregators.

</details>


### [275] [MeltwaterBench: Deep learning for spatiotemporal downscaling of surface meltwater](https://arxiv.org/abs/2512.12142)
*Björn Lütjens,Patrick Alexander,Raf Antwerpen,Til Widmann,Guido Cervone,Marco Tedesco*

Main category: cs.CV

Relevance: 15.0

TL;DR: 该论文提出了一种深度学习模型，通过融合遥感观测和物理模型数据，生成格陵兰冰盖每日100米分辨率的地表融水分布图，解决了现有方法在时空分辨率上的权衡问题。


<details>
  <summary>Details</summary>
Motivation: 格陵兰冰盖加速融化，但相关过程难以测量。现有融水分布图在时间和空间分辨率上存在权衡：要么时间分辨率高但空间分辨率低，要么反之。需要同时具备高时空分辨率的融水分布图来更好地理解融化过程。

Method: 开发深度学习模型，融合多种数据流：区域气候模型（RCM）输出、合成孔径雷达（SAR）、被动微波（PMW）和数字高程模型（DEM）。使用SAR衍生的融水作为"地面实况"，在2017-2023年期间对东格陵兰Helheim冰川进行时空降尺度处理。评估了UNet和DeepLabv3+等标准深度学习方法。

Result: 深度学习融合方法在研究区域比现有非深度学习方法准确率提高超过10个百分点：相比仅依赖区域气候模型的方法（83% vs. 95%），以及仅依赖被动微波观测的方法（72% vs. 95%）。同时发布了时空对齐的数据集MeltwaterBench作为基准。

Conclusion: 深度学习融合方法能显著提高融水分布图的时空分辨率和准确性，为理解格陵兰冰盖融化过程提供了更好的工具。发布的基准数据集将促进更复杂的数据驱动降尺度方法的比较研究。

Abstract: The Greenland ice sheet is melting at an accelerated rate due to processes that are not fully understood and hard to measure. The distribution of surface meltwater can help understand these processes and is observable through remote sensing, but current maps of meltwater face a trade-off: They are either high-resolution in time or space, but not both. We develop a deep learning model that creates gridded surface meltwater maps at daily 100m resolution by fusing data streams from remote sensing observations and physics-based models. In particular, we spatiotemporally downscale regional climate model (RCM) outputs using synthetic aperture radar (SAR), passive microwave (PMW), and a digital elevation model (DEM) over the Helheim Glacier in Eastern Greenland from 2017-2023. Using SAR-derived meltwater as "ground truth", we show that a deep learning-based method that fuses all data streams is over 10 percentage points more accurate over our study area than existing non deep learning-based approaches that only rely on a regional climate model (83% vs. 95% Acc.) or passive microwave observations (72% vs. 95% Acc.). Alternatively, creating a gridded product through a running window calculation with SAR data underestimates extreme melt events, but also achieves notable accuracy (90%) and does not rely on deep learning. We evaluate standard deep learning methods (UNet and DeepLabv3+), and publish our spatiotemporally aligned dataset as a benchmark, MeltwaterBench, for intercomparisons with more complex data-driven downscaling methods. The code and data are available at $\href{https://github.com/blutjens/hrmelt}{github.com/blutjens/hrmelt}$.

</details>


### [276] [Thermal RGB Fusion for Micro-UAV Wildfire Perimeter Tracking with Minimal Comms](https://arxiv.org/abs/2512.12199)
*Ercan Erkalkan,Vedat Topuz,Ayça Ak*

Main category: cs.CV

Relevance: 15.0

TL;DR: 该研究提出了一种轻量级边界跟踪方法，用于在带宽受限的野火环境中工作的微型无人机团队。该方法结合热成像和RGB图像处理，通过自适应阈值和形态学细化生成热区掩码，利用梯度滤波抑制误检，并采用规则级合并策略简化边界。系统包含周期性信标和惯性反馈回路，可在GPS降级时保持轨迹稳定性。


<details>
  <summary>Details</summary>
Motivation: 在野火等紧急侦察应用中，微型无人机团队需要在带宽受限条件下进行快速部署。现有方法在GPS降级时轨迹稳定性不足，计算复杂度高，难以在嵌入式平台上实现低延迟。需要一种轻量级、鲁棒的边界跟踪方法，能够在有限通信条件下保持环境覆盖和轨迹稳定性。

Method: 1. 热成像帧通过自适应阈值和形态学细化生成粗略的热区掩码
2. RGB帧提供边缘线索，使用基于梯度的滤波抑制纹理相关的误检测
3. 规则级合并策略选择边界候选，并通过Ramer-Douglas-Peucker算法简化
4. 系统包含周期性信标和惯性反馈回路，在GPS降级时保持轨迹稳定性
5. 通过限制每帧像素操作和预计算梯度表，在嵌入式SoC平台上实现低于50ms的延迟

Result: 小规模模拟显示：与纯边缘跟踪基线相比，平均路径长度和边界抖动减少，同时通过交集合并分析保持环境覆盖。电池消耗和计算利用率证实了在标准微型平台上实现10-15 m/s前向运动的可行性。系统在嵌入式SoC平台上实现了低于50ms的延迟。

Conclusion: 该方法实现了快速现场部署，仅需鲁棒传感和最小通信，适用于紧急侦察应用。轻量级设计使其适合带宽受限的微型无人机团队在野火等复杂环境中工作。

Abstract: This study introduces a lightweight perimeter tracking method designed for micro UAV teams operating over wildfire environments under limited bandwidth conditions. Thermal image frames generate coarse hot region masks through adaptive thresholding and morphological refinement, while RGB frames contribute edge cues and suppress texture related false detections using gradient based filtering. A rule level merging strategy selects boundary candidates and simplifies them via the Ramer Douglas Peucker algorithm. The system incorporates periodic beacons and an inertial feedback loop that maintains trajectory stability in the presence of GPS degradation. The guidance loop targets sub 50 ms latency on embedded System on Chip (SoC) platforms by constraining per frame pixel operations and precomputing gradient tables. Small scale simulations demonstrate reductions in average path length and boundary jitter compared to a pure edge tracking baseline, while maintaining environmental coverage measured through intersection merge analysis. Battery consumption and computational utilization confirm the feasibility of achieving 10, 15 m/s forward motion on standard micro platforms. This approach enables rapid deployment in the field, requiring robust sensing and minimal communications for emergency reconnaissance applications.

</details>


### [277] [Comparison of different segmentation algorithms on brain volume and fractal dimension in infant brain MRIs](https://arxiv.org/abs/2512.12222)
*Nathalie Alexander,Arnaud Gucciardi,Umberto Michelucci*

Main category: cs.CV

Relevance: 15.0

TL;DR: 该研究系统比较了SynthSeg和SamSeg两种方法在婴儿脑MRI分割中的性能，发现SynthSeg在分割质量和体积估计方面优于SamSeg，但分割准确性会影响分形维数估计，需要谨慎解释微小形态差异。


<details>
  <summary>Details</summary>
Motivation: 婴儿脑MRI的准确分割对于量化发育过程中的结构和复杂性变化至关重要，但由于髓鞘化过程和组织对比度降低，自动分割特别具有挑战性。本研究旨在系统比较不同分割方法对体积和分形维数估计的影响。

Method: 使用Baby Open Brains (BOB)数据集（71个扫描，1-9个月），比较SynthSeg和SamSeg两种分割方法。使用Dice系数、交并比、95%Hausdorff距离和归一化互信息等指标评估分割质量，并与专家标注进行对比。分析分割准确性对体积和分形维数估计的影响。

Result: SynthSeg在所有质量指标上均优于SamSeg（主要区域平均Dice > 0.8），其体积估计与手动参考值更接近（平均+4%）。SamSeg系统性地高估脑室和全脑体积（平均+76%）。分割准确性随年龄增长而提高，与髓鞘化过程中组织对比度增加一致。分形维数分析显示SynthSeg与专家分割之间存在显著区域差异，分割相关的FD变异性超过了大多数发育队列中报告的组间差异。

Conclusion: SynthSeg为儿科MRI提供了最可靠的体积和分形维数结果，但由于分割相关的不确定性，应谨慎解释微小的形态差异（体积和分形维数）。分割偏差直接影响分形维数估计，体积和分形维数偏差在结构间呈正相关。

Abstract: Accurate segmentation of infant brain MRI is essential for quantifying developmental changes in structure and complexity. However, ongoing myelination and reduced tissue contrast make automated segmentation particularly challenging. This study systematically compared segmentation accuracy and its impact on volumetric and fractal dimension (FD) estimates in infant brain MRI using the Baby Open Brains (BOB) dataset (71 scans, 1-9 months). Two methods, SynthSeg and SamSeg, were evaluated against expert annotations using Dice, Intersection over Union, 95th-percentile Hausdorff distance, and Normalised Mutual Information. SynthSeg outperformed SamSeg across all quality metrics (mean Dice > 0.8 for major regions) and provided volumetric estimates closely matching the manual reference (mean +4% [-28% - 71%]). SamSeg systematically overestimated ventricular and whole-brain volumes (mean +76% [-12% - 190%]). Segmentation accuracy improved with age, consistent with increasing tissue contrast during myelination. Fractal dimension a(FD) nalyses revealed significant regional differences between SynthSeg and expert segmentations, and Bland-Altman limits of agreement indicated that segmentation-related FD variability exceeded most group differences reported in developmental cohorts. Volume and FD deviations were positively correlated across structures, indicating that segmentation bias directly affects FD estimation. Overall, SynthSeg provided the most reliable volumetric and FD results for paediatric MRI, yet small morphological differences in volume and FD should be interpreted with caution due to segmentation-related uncertainty.

</details>


### [278] [From Particles to Fields: Reframing Photon Mapping with Continuous Gaussian Photon Fields](https://arxiv.org/abs/2512.12459)
*Jiachen Tao,Benjamin Planche,Van Nguyen Nguyen,Junyi Wu,Yuchun Liu,Haoxuan Wang,Zhongpai Gao,Gengyu Zhang,Meng Zheng,Feiran Wang,Anwesa Choudhuri,Zhenghao Zhao,Weitai Kang,Terrence Chen,Yan Yan,Ziyan Wu*

Main category: cs.CV

Relevance: 15.0

TL;DR: 提出Gaussian Photon Field (GPF)，将光子映射重构为连续可重用的辐射函数，通过3D高斯基元编码光子分布，实现多视角渲染加速


<details>
  <summary>Details</summary>
Motivation: 光子映射虽然能提供物理准确的光线传输估计，但在多视角渲染中存在计算效率低下的问题，因为每个视角都需要独立的光子追踪和随机核估计，导致冗余计算

Method: 引入Gaussian Photon Field (GPF)，将光子分布编码为各向异性的3D高斯基元，参数包括位置、旋转、尺度和光谱。从第一次SPPM迭代的物理追踪光子初始化，通过多视角监督优化，将基于光子的光线传输蒸馏到连续场中

Result: 在包含复杂光线传输（如焦散和镜面-漫反射交互）的场景中，GPF实现了光子级精度，同时将计算量减少了几个数量级

Conclusion: GPF将基于光子的渲染的物理严谨性与神经场景表示的效率统一起来，实现了可微分辐射评估而无需重复光子追踪或迭代优化

Abstract: Accurately modeling light transport is essential for realistic image synthesis. Photon mapping provides physically grounded estimates of complex global illumination effects such as caustics and specular-diffuse interactions, yet its per-view radiance estimation remains computationally inefficient when rendering multiple views of the same scene. The inefficiency arises from independent photon tracing and stochastic kernel estimation at each viewpoint, leading to inevitable redundant computation. To accelerate multi-view rendering, we reformulate photon mapping as a continuous and reusable radiance function. Specifically, we introduce the Gaussian Photon Field (GPF), a learnable representation that encodes photon distributions as anisotropic 3D Gaussian primitives parameterized by position, rotation, scale, and spectrum. GPF is initialized from physically traced photons in the first SPPM iteration and optimized using multi-view supervision of final radiance, distilling photon-based light transport into a continuous field. Once trained, the field enables differentiable radiance evaluation along camera rays without repeated photon tracing or iterative refinement. Extensive experiments on scenes with complex light transport, such as caustics and specular-diffuse interactions, demonstrate that GPF attains photon-level accuracy while reducing computation by orders of magnitude, unifying the physical rigor of photon-based rendering with the efficiency of neural scene representations.

</details>


### [279] [Anatomy Guided Coronary Artery Segmentation from CCTA Using Spatial Frequency Joint Modeling](https://arxiv.org/abs/2512.12539)
*Huan Huang,Michele Esposito,Chen Zhao*

Main category: cs.CV

Relevance: 15.0

TL;DR: 提出了一种结合心肌解剖先验、结构感知特征编码和三维小波变换的冠状动脉分割框架，在ImageCAS数据集上取得了优于主流分割模型的性能。


<details>
  <summary>Details</summary>
Motivation: 冠状动脉CT血管成像的准确分割对于定量分析和临床决策至关重要，但由于血管细小、分支复杂、边界模糊和心肌干扰等因素，可靠分割仍然具有挑战性。

Method: 提出一个整合心肌解剖先验、结构感知特征编码和三维小波逆小波变换的框架。编码阶段引入心肌先验和残差注意力特征增强，小波变换实现联合空间频率建模和多尺度结构一致性，解码阶段通过多尺度特征融合模块整合语义和几何信息。

Result: 在ImageCAS数据集上，该方法达到Dice系数0.8082、敏感性0.7946、精确率0.8471、HD95为9.77mm，优于多个主流分割模型。消融研究证实各组件具有互补贡献。

Conclusion: 该方法能够在复杂几何条件下实现更稳定一致的冠状动脉分割，为后续冠状动脉结构分析任务提供可靠的分割结果。

Abstract: Accurate coronary artery segmentation from coronary computed tomography angiography is essential for quantitative coronary analysis and clinical decision support. Nevertheless, reliable segmentation remains challenging because of small vessel calibers, complex branching, blurred boundaries, and myocardial interference. We propose a coronary artery segmentation framework that integrates myocardial anatomical priors, structure aware feature encoding, and three dimensional wavelet inverse wavelet transformations. Myocardial priors and residual attention based feature enhancement are incorporated during encoding to strengthen coronary structure representation. Wavelet inverse wavelet based downsampling and upsampling enable joint spatial frequency modeling and preserve multi scale structural consistency, while a multi scale feature fusion module integrates semantic and geometric information in the decoding stage. The model is trained and evaluated on the public ImageCAS dataset using a 3D overlapping patch based strategy with a 7:1:2 split for training, validation, and testing. Experimental results demonstrate that the proposed method achieves a Dice coefficient of 0.8082, Sensitivity of 0.7946, Precision of 0.8471, and an HD95 of 9.77 mm, outperforming several mainstream segmentation models. Ablation studies further confirm the complementary contributions of individual components. The proposed method enables more stable and consistent coronary artery segmentation under complex geometric conditions, providing reliable segmentation results for subsequent coronary structure analysis tasks.

</details>


### [280] [Automatic Wire-Harness Color Sequence Detector](https://arxiv.org/abs/2512.12590)
*Indiwara Nanayakkara,Dehan Jayawickrama,Mervyn Parakrama B. Ekanayake*

Main category: cs.CV

Relevance: 15.0

TL;DR: 提出用于线束检测的半自动机器视觉系统，通过多摄像头和颜色序列分类器实现线束位置、连接器极性和颜色序列的验证，在工业环境中达到100%检测精度并减少44%检测时间。


<details>
  <summary>Details</summary>
Motivation: 现代电子制造服务行业中，线束检测过程仍然依赖人工，劳动密集且容易出错。需要自动化解决方案来提高检测效率和准确性。

Method: 使用五个工业标准CMOS摄像头集成到模块化机械框架中，采用基于HSV和RGB颜色域值比较的颜色序列分类器。系统可通过至少五个参考样本进行训练，训练文件可存储并重复用于类似线束类型。

Result: 系统在GPV Lanka Pvt. Ltd.部署，实现了100%的检测准确率，相比人工方法减少了44%的检测时间。系统还包括用户管理、可调照明、会话数据存储和安全登录等附加功能。

Conclusion: 该半自动机器视觉系统为线束检测提供了可靠高效的解决方案，在实际工业应用中表现出色，显著提升了检测效率和准确性。

Abstract: Wire harness inspection process remains a labor-intensive process prone to errors in the modern Electronics Manufacturing Services (EMS) industry. This paper introduces a semiautomated machine vision system capable of verifying correct wire positioning, correctness of the connector polarity and correctness of color sequences for both linear and circular wire harness configurations. Five industrial standard CMOS cameras are integrated into a modularized mechanical framework in the physical structure of the solution and a HSV and RGB color domain value comparison based color sequence classifier is used in the operation. For each harness batch, a user can train the system using at least five reference samples; the trained file is stored and reused for similar harness types. The Solution is deployed at GPV Lanka Pvt. Ltd. (Fig. 2) and the system achieved 100% detection accuracy and reduced inspection time by 44% compared to manual methods. Additional features include user management, adjustable lighting, session data storage, and secure login. Results of this product usage in the real world situation demonstrate that this approach delivers reliable and efficient inspection capabilities.

</details>


### [281] [Cross-modal Fundus Image Registration under Large FoV Disparity](https://arxiv.org/abs/2512.12657)
*Hongyang Li,Junyi Tao,Qijie Wei,Ningzhi Yang,Meng Wang,Weihong Yu,Xirong Li*

Main category: cs.CV

Relevance: 15.0

TL;DR: 提出CARe方法解决大视场差异的跨模态眼底图像配准问题，通过裁剪和对齐操作实现有效配准


<details>
  <summary>Details</summary>
Motivation: 现有跨模态眼底图像配准方法假设小视场差异，无法处理大视场差异的挑战性场景

Method: CARe方法包含两个步骤：1) 利用视网膜生理结构从目标图像中裁剪出与源图像视场大致对齐的子图像；2) 使用RANSAC算法和多项式坐标拟合的双重拟合对齐模块改进空间变换

Result: 在60对OCTA-wfCFP新测试集上验证了CARe方法的有效性

Conclusion: CARe方法简单有效，能够解决大视场差异的跨模态眼底图像配准问题

Abstract: Previous work on cross-modal fundus image registration (CMFIR) assumes small cross-modal Field-of-View (FoV) disparity. By contrast, this paper is targeted at a more challenging scenario with large FoV disparity, to which directly applying current methods fails. We propose Crop and Alignment for cross-modal fundus image Registration(CARe), a very simple yet effective method. Specifically, given an OCTA with smaller FoV as a source image and a wide-field color fundus photograph (wfCFP) as a target image, our Crop operation exploits the physiological structure of the retina to crop from the target image a sub-image with its FoV roughly aligned with that of the source. This operation allows us to re-purpose the previous small-FoV-disparity oriented methods for subsequent image registration. Moreover, we improve spatial transformation by a double-fitting based Alignment module that utilizes the classical RANSAC algorithm and polynomial-based coordinate fitting in a sequential manner. Extensive experiments on a newly developed test set of 60 OCTA-wfCFP pairs verify the viability of CARe for CMFIR.

</details>


### [282] [Heart Disease Prediction using Case Based Reasoning (CBR)](https://arxiv.org/abs/2512.13078)
*Mohaiminul Islam Bhuiyan,Chan Hue Wah,Nur Shazwani Kamarudin,Nur Hafieza Ismail,Ahmad Fakhri Ab Nasir*

Main category: cs.CV

Relevance: 15.0

TL;DR: 该研究比较了三种智能系统（模糊逻辑、神经网络、案例推理）用于心脏病预测，最终选择案例推理方法，在心脏病数据集上达到了97.95%的准确率，并发现男性患病概率(57.76%)高于女性(42.24%)。


<details>
  <summary>Details</summary>
Motivation: 传统心脏病预测方法依赖医生经验，精度不足。需要更准确的智能系统来改善医疗诊断，提高预测可靠性。

Method: 比较模糊逻辑、神经网络和案例推理三种智能系统方法，最终选择案例推理。对心脏病数据集进行数据预处理和训练/测试分割，使用案例推理进行预测。

Result: 案例推理方法达到97.95%的准确率。男性心脏病概率为57.76%，女性为42.24%。吸烟和饮酒是重要风险因素，尤其对男性影响更大。

Conclusion: 案例推理在心脏病预测中表现优异，智能系统可有效辅助医疗诊断。性别差异和生活方式因素对心脏病风险有显著影响。

Abstract: This study provides an overview of heart disease prediction using an intelligent system. Predicting disease accurately is crucial in the medical field, but traditional methods relying solely on a doctor's experience often lack precision. To address this limitation, intelligent systems are applied as an alternative to traditional approaches. While various intelligent system methods exist, this study focuses on three: Fuzzy Logic, Neural Networks, and Case-Based Reasoning (CBR). A comparison of these techniques in terms of accuracy was conducted, and ultimately, Case-Based Reasoning (CBR) was selected for heart disease prediction. In the prediction phase, the heart disease dataset underwent data pre-processing to clean the data and data splitting to separate it into training and testing sets. The chosen intelligent system was then employed to predict heart disease outcomes based on the processed data. The experiment concluded with Case-Based Reasoning (CBR) achieving a notable accuracy rate of 97.95% in predicting heart disease. The findings also revealed that the probability of heart disease was 57.76% for males and 42.24% for females. Further analysis from related studies suggests that factors such as smoking and alcohol consumption are significant contributors to heart disease, particularly among males.

</details>


### [283] [Spinal Line Detection for Posture Evaluation through Train-ing-free 3D Human Body Reconstruction with 2D Depth Images](https://arxiv.org/abs/2512.12718)
*Sehyun Kim,Hye Jun Lee,Jiwoo Lee,Changgyun Kim,Taemin Lee*

Main category: cs.CV

Relevance: 15.0

TL;DR: 提出一种基于四方向深度图像的3D人体姿态分析系统，通过全局和精细配准的分层匹配恢复3D人体模型并自动估计脊柱中心线，无需训练数据或复杂神经网络模型


<details>
  <summary>Details</summary>
Motivation: 脊柱角度是身体平衡的重要指标，现有方法存在局限性：多图像方法需要昂贵设备和复杂流程，单图像方法因遮挡和视角限制难以准确估计脊柱中心线等内部结构

Method: 1) 集成四方向深度图像恢复3D人体模型；2) 通过全局和精细配准的分层匹配处理噪声和遮挡；3) 应用自适应顶点缩减保持网格分辨率和形状可靠性；4) 使用细节层次集成确保脊柱角度估计的准确性和稳定性

Result: 实现了高精度的3D脊柱配准估计，验证了匹配质量的提升，无需依赖训练数据或复杂神经网络模型

Conclusion: 该方法成功解决了多图像方法的设备成本问题和单图像方法的内部结构估计局限性，实现了准确稳定的脊柱角度估计

Abstract: The spinal angle is an important indicator of body balance. It is important to restore the 3D shape of the human body and estimate the spine center line. Existing mul-ti-image-based body restoration methods require expensive equipment and complex pro-cedures, and single image-based body restoration methods have limitations in that it is difficult to accurately estimate the internal structure such as the spine center line due to occlusion and viewpoint limitation. This study proposes a method to compensate for the shortcomings of the multi-image-based method and to solve the limitations of the sin-gle-image method. We propose a 3D body posture analysis system that integrates depth images from four directions to restore a 3D human model and automatically estimate the spine center line. Through hierarchical matching of global and fine registration, restora-tion to noise and occlusion is performed. Also, the Adaptive Vertex Reduction is applied to maintain the resolution and shape reliability of the mesh, and the accuracy and stabil-ity of spinal angle estimation are simultaneously secured by using the Level of Detail en-semble. The proposed method achieves high-precision 3D spine registration estimation without relying on training data or complex neural network models, and the verification confirms the improvement of matching quality.

</details>


### [284] [FID-Net: A Feature-Enhanced Deep Learning Network for Forest Infestation Detection](https://arxiv.org/abs/2512.13104)
*Yan Zhang,Baoxin Li,Han Sun,Yuhang Gao,Mingtai Zhang,Pei Wang*

Main category: cs.CV

Relevance: 15.0

TL;DR: 提出FID-Net深度学习模型，从无人机可见光图像中检测森林病虫害树木，并利用三种空间指标进行虫害分析，在新疆天山地区32个森林样地实验中表现优于主流YOLO模型。


<details>
  <summary>Details</summary>
Motivation: 传统森林病虫害监测方法在大规模、细粒度检测方面存在局限，需要高效准确的监测技术来保护生态系统稳定。

Method: 基于YOLOv8n构建FID-Net，引入轻量级特征增强模块提取病害敏感特征，自适应多尺度特征融合模块对齐融合RGB和增强特征，高效通道注意力机制增强判别信息。检测后使用核密度估计、邻域评估和DBSCAN聚类进行虫害分析。

Result: 在新疆天山32个森林样地的无人机图像上，FID-Net达到86.10%精确率、75.44%召回率、82.29% mAP@0.5和64.30% mAP@0.5:0.95，优于主流YOLO模型。分析显示感染树木呈现明显聚类模式。

Conclusion: FID-Net能够准确识别树木健康状况，结合空间指标为智能病虫害监测、早期预警和精准管理提供可靠数据支持。

Abstract: Forest pests threaten ecosystem stability, requiring efficient monitoring. To overcome the limitations of traditional methods in large-scale, fine-grained detection, this study focuses on accurately identifying infected trees and analyzing infestation patterns. We propose FID-Net, a deep learning model that detects pest-affected trees from UAV visible-light imagery and enables infestation analysis via three spatial metrics. Based on YOLOv8n, FID-Net introduces a lightweight Feature Enhancement Module (FEM) to extract disease-sensitive cues, an Adaptive Multi-scale Feature Fusion Module (AMFM) to align and fuse dual-branch features (RGB and FEM-enhanced), and an Efficient Channel Attention (ECA) mechanism to enhance discriminative information efficiently. From detection results, we construct a pest situation analysis framework using: (1) Kernel Density Estimation to locate infection hotspots; (2) neighborhood evaluation to assess healthy trees' infection risk; (3) DBSCAN clustering to identify high-density healthy clusters as priority protection zones. Experiments on UAV imagery from 32 forest plots in eastern Tianshan, China, show that FID-Net achieves 86.10% precision, 75.44% recall, 82.29% mAP@0.5, and 64.30% mAP@0.5:0.95, outperforming mainstream YOLO models. Analysis confirms infected trees exhibit clear clustering, supporting targeted forest protection. FID-Net enables accurate tree health discrimination and, combined with spatial metrics, provides reliable data for intelligent pest monitoring, early warning, and precise management.

</details>


### [285] [DePT3R: Joint Dense Point Tracking and 3D Reconstruction of Dynamic Scenes in a Single Forward Pass](https://arxiv.org/abs/2512.13122)
*Vivek Alumootil,Tuan-Anh Vu,M. Khalid Jawed*

Main category: cs.CV

Relevance: 15.0

TL;DR: DePT3R是一个统一框架，能够在单次前向传播中同时完成动态场景的密集点跟踪和3D重建，无需相机位姿或时间顺序假设。


<details>
  <summary>Details</summary>
Motivation: 现有动态场景密集3D点跟踪方法通常需要成对处理、已知相机位姿或时间顺序假设，限制了灵活性和适用性。同时，大规模无位姿图像集合的3D重建技术进展为动态场景理解提供了统一方法的机会。

Method: 使用强大骨干网络提取深度时空特征，通过密集预测头回归像素级映射，实现多任务学习。框架无需相机位姿，在动态环境中具有更好的适应性和效率。

Result: 在多个动态场景基准测试中表现出强大性能，相比现有最先进方法在内存效率上有显著提升。

Conclusion: DePT3R为动态场景理解提供了一个高效、灵活的统一框架，能够同时处理点跟踪和3D重建任务，在动态环境中具有实际应用价值。

Abstract: Current methods for dense 3D point tracking in dynamic scenes typically rely on pairwise processing, require known camera poses, or assume a temporal ordering to input frames, constraining their flexibility and applicability. Additionally, recent advances have successfully enabled efficient 3D reconstruction from large-scale, unposed image collections, underscoring opportunities for unified approaches to dynamic scene understanding. Motivated by this, we propose DePT3R, a novel framework that simultaneously performs dense point tracking and 3D reconstruction of dynamic scenes from multiple images in a single forward pass. This multi-task learning is achieved by extracting deep spatio-temporal features with a powerful backbone and regressing pixel-wise maps with dense prediction heads. Crucially, DePT3R operates without requiring camera poses, substantially enhancing its adaptability and efficiency-especially important in dynamic environments with rapid changes. We validate DePT3R on several challenging benchmarks involving dynamic scenes, demonstrating strong performance and significant improvements in memory efficiency over existing state-of-the-art methods. Data and codes are available via the open repository: https://github.com/StructuresComp/DePT3R

</details>


### [286] [LeafTrackNet: A Deep Learning Framework for Robust Leaf Tracking in Top-Down Plant Phenotyping](https://arxiv.org/abs/2512.13130)
*Shanghua Liu,Majharulislam Babor,Christoph Verduyn,Breght Vandenberghe,Bruno Betoni Parodi,Cornelia Weltzien,Marina M. -C. Höhne*

Main category: cs.CV

Relevance: 15.0

TL;DR: 提出了CanolaTrack数据集和LeafTrackNet框架，用于复杂作物（油菜）的叶片追踪，在真实条件下实现了比现有方法更好的性能


<details>
  <summary>Details</summary>
Motivation: 现有叶片追踪方法存在局限性：植物专用追踪器仅限于小规模物种或受约束的成像条件，而通用多目标追踪方法不适合动态生物场景。同时缺乏大规模真实条件下的数据集阻碍了准确叶片追踪模型的发展。

Method: 提出了LeafTrackNet框架，结合YOLOv10叶片检测器和MobileNetV3嵌入网络，通过基于嵌入的记忆关联策略在推理时保持叶片身份。

Result: LeafTrackNet在CanolaTrack数据集上优于植物专用追踪器和最先进的多目标追踪基线，HOTA指标提升9%。提供了农业作物叶片追踪的最大数据集。

Conclusion: 该工作为真实条件下的叶片级追踪设立了新标准，提供了最大的农业作物叶片追踪数据集，将促进植物表型研究的未来发展。

Abstract: High resolution phenotyping at the level of individual leaves offers fine-grained insights into plant development and stress responses. However, the full potential of accurate leaf tracking over time remains largely unexplored due to the absence of robust tracking methods-particularly for structurally complex crops such as canola. Existing plant-specific tracking methods are typically limited to small-scale species or rely on constrained imaging conditions. In contrast, generic multi-object tracking (MOT) methods are not designed for dynamic biological scenes. Progress in the development of accurate leaf tracking models has also been hindered by a lack of large-scale datasets captured under realistic conditions. In this work, we introduce CanolaTrack, a new benchmark dataset comprising 5,704 RGB images with 31,840 annotated leaf instances spanning the early growth stages of 184 canola plants. To enable accurate leaf tracking over time, we introduce LeafTrackNet, an efficient framework that combines a YOLOv10-based leaf detector with a MobileNetV3-based embedding network. During inference, leaf identities are maintained over time through an embedding-based memory association strategy. LeafTrackNet outperforms both plant-specific trackers and state-of-the-art MOT baselines, achieving a 9% HOTA improvement on CanolaTrack. With our work we provide a new standard for leaf-level tracking under realistic conditions and we provide CanolaTrack - the largest dataset for leaf tracking in agriculture crops, which will contribute to future research in plant phenotyping. Our code and dataset are publicly available at https://github.com/shl-shawn/LeafTrackNet.

</details>


### [287] [Automated User Identification from Facial Thermograms with Siamese Networks](https://arxiv.org/abs/2512.13361)
*Elizaveta Prozorova,Anton Konev,Vladimir Faerman*

Main category: cs.CV

Relevance: 15.0

TL;DR: 该论文研究基于面部热成像的生物识别技术，比较不同红外光谱范围，提出使用孪生神经网络进行自动识别，在专有数据集上达到约80%准确率，并探索可见光与红外混合系统。


<details>
  <summary>Details</summary>
Motivation: 开发可靠的安全系统需要有效的生物识别技术。热成像技术通过面部热图进行身份识别，具有非接触、可在低光照条件下工作等优势，但需要系统研究其技术参数和识别方法。

Method: 1. 比较分析近红外(NIR)、短波红外(SWIR)、中波红外(MWIR)、长波红外(LWIR)等红外光谱范围；2. 定义热成像相机关键要求：传感器分辨率、热灵敏度、至少30Hz帧率；3. 提出使用孪生神经网络自动化识别过程；4. 在专有数据集上进行实验验证。

Result: 提出的方法在专有数据集上达到约80%的识别准确率。研究还表明，结合可见光和红外光谱的混合系统能够克服单一模态的限制，热成像技术是开发可靠安全系统的有前景技术。

Conclusion: 热成像技术是生物识别领域有前景的方向，孪生神经网络能有效自动化识别过程，混合系统能提升性能，为可靠安全系统开发提供了技术基础。

Abstract: The article analyzes the use of thermal imaging technologies for biometric identification based on facial thermograms. It presents a comparative analysis of infrared spectral ranges (NIR, SWIR, MWIR, and LWIR). The paper also defines key requirements for thermal cameras used in biometric systems, including sensor resolution, thermal sensitivity, and a frame rate of at least 30 Hz. Siamese neural networks are proposed as an effective approach for automating the identification process. In experiments conducted on a proprietary dataset, the proposed method achieved an accuracy of approximately 80%. The study also examines the potential of hybrid systems that combine visible and infrared spectra to overcome the limitations of individual modalities. The results indicate that thermal imaging is a promising technology for developing reliable security systems.

</details>


### [288] [End2Reg: Learning Task-Specific Segmentation for Markerless Registration in Spine Surgery](https://arxiv.org/abs/2512.13402)
*Lorenzo Pettinari,Sidaty El Hadramy,Michael Wehrli,Philippe C. Cattin,Daniel Studer,Carol C. Hasler,Maria Licci*

Main category: cs.CV

Relevance: 15.0

TL;DR: End2Reg：一种用于脊柱手术术中导航的端到端深度学习框架，联合优化分割和配准，无需弱分割标签，将配准误差降低32-45%


<details>
  <summary>Details</summary>
Motivation: 当前脊柱手术术中导航系统存在侵入性、辐射强、工作流程中断等问题。现有的无标记RGB-D配准方法依赖弱分割标签，容易传播误差。需要一种无需弱标签和手动步骤的自动配准方法。

Method: 提出End2Reg端到端深度学习框架，联合优化分割和配准。网络学习专门为配准优化的分割掩码，仅通过配准目标进行指导，无需直接分割监督。消除对弱分割标签和手动步骤的依赖。

Result: 在离体和在体基准测试中达到最先进性能：中值目标配准误差降低32%至1.83mm，均方根误差降低45%至3.95mm。消融研究证实端到端优化显著提高配准精度。

Conclusion: 该端到端RGB-D配准管道消除了对弱标签和手动步骤的依赖，朝着完全自动化的无标记术中导航迈进。代码和交互式可视化已开源。

Abstract: Purpose: Intraoperative navigation in spine surgery demands millimeter-level accuracy. Current systems based on intraoperative radiographic imaging and bone-anchored markers are invasive, radiation-intensive and workflow disruptive. Recent markerless RGB-D registration methods offer a promising alternative, but existing approaches rely on weak segmentation labels to isolate relevant anatomical structures, which can propagate errors throughout registration. Methods: We present End2Reg an end-to-end deep learning framework that jointly optimizes segmentation and registration, eliminating the need for weak segmentation labels and manual steps. The network learns segmentation masks specifically optimized for registration, guided solely by the registration objective without direct segmentation supervision. Results: The proposed framework achieves state-of-the-art performance on ex- and in-vivo benchmarks, reducing median Target Registration Error by 32% to 1.83mm and mean Root Mean Square Error by 45% to 3.95mm, respectively. An ablation study confirms that end-to-end optimization significantly improves registration accuracy. Conclusion: The presented end-to-end RGB-D registration pipeline removes dependency on weak labels and manual steps, advancing towards fully automatic, markerless intraoperative navigation. Code and interactive visualizations are available at: https://lorenzopettinari.github.io/end-2-reg/.

</details>


### [289] [3D Human-Human Interaction Anomaly Detection](https://arxiv.org/abs/2512.13560)
*Shun Maeda,Chunzhi Gu,Koichiro Kamide,Katsuya Hotta,Shangce Gao,Chao Zhang*

Main category: cs.CV

Relevance: 15.0

TL;DR: 本文提出了一种新任务H2IAD（人-人交互异常检测），旨在检测3D人体动作中的异常交互行为，并提出了IADNet模型，通过时间注意力共享模块和距离关系编码模块来捕捉人际交互的时空动态。


<details>
  <summary>Details</summary>
Motivation: 现有的人体异常检测主要关注单个人的异常行为，但人类行为本质上是协作性的，异常也可能出现在人际交互中。现有单人体异常检测模型无法有效捕捉复杂且不对称的人际交互动态，导致检测精度低。

Method: 提出了IADNet模型，包含两个核心模块：1) 时间注意力共享模块(TASM)：通过共享编码的运动嵌入来同步协作运动相关性；2) 距离关系编码模块(DREM)：捕捉人际空间配置和社会线索。最终使用归一化流进行异常评分。

Result: 在人际运动基准测试上的大量实验表明，IADNet在H2IAD任务上优于现有的人体异常检测基线方法。

Conclusion: 本文首次系统研究了人-人交互异常检测任务，提出的IADNet模型通过有效建模人际交互的时空动态，显著提升了检测性能，为人际协作行为的异常检测提供了新思路。

Abstract: Human-centric anomaly detection (AD) has been primarily studied to specify anomalous behaviors in a single person. However, as humans by nature tend to act in a collaborative manner, behavioral anomalies can also arise from human-human interactions. Detecting such anomalies using existing single-person AD models is prone to low accuracy, as these approaches are typically not designed to capture the complex and asymmetric dynamics of interactions. In this paper, we introduce a novel task, Human-Human Interaction Anomaly Detection (H2IAD), which aims to identify anomalous interactive behaviors within collaborative 3D human actions. To address H2IAD, we then propose Interaction Anomaly Detection Network (IADNet), which is formalized with a Temporal Attention Sharing Module (TASM). Specifically, in designing TASM, we share the encoded motion embeddings across both people such that collaborative motion correlations can be effectively synchronized. Moreover, we notice that in addition to temporal dynamics, human interactions are also characterized by spatial configurations between two people. We thus introduce a Distance-Based Relational Encoding Module (DREM) to better reflect social cues in H2IAD. The normalizing flow is eventually employed for anomaly scoring. Extensive experiments on human-human motion benchmarks demonstrate that IADNet outperforms existing Human-centric AD baselines in H2IAD.

</details>


### [290] [Lighting in Motion: Spatiotemporal HDR Lighting Estimation](https://arxiv.org/abs/2512.13597)
*Christophe Bolduc,Julien Philip,Li Ma,Mingming He,Paul Debevec,Jean-François Lalonde*

Main category: cs.CV

Relevance: 15.0

TL;DR: LiMo是一种基于扩散模型的时空光照估计方法，通过生成不同曝光下的镜面和漫反射球体来预测高真实感细节和准确照度，结合几何条件和深度信息进行空间控制，最终合成HDRI光照图。


<details>
  <summary>Details</summary>
Motivation: 现有光照估计方法难以同时实现高真实感细节预测和准确照度估计。作者旨在开发一种能够兼顾这两方面需求的时空光照估计方法，特别是在复杂室内外场景中。

Method: 1) 提出生成不同曝光下的镜面和漫反射球体集合；2) 在大规模定制数据集上微调现有扩散模型；3) 引入新的几何条件（场景相对于目标3D位置的相对位置）来增强空间控制；4) 通过可微分渲染将不同曝光的预测组合成单个HDRI图。

Result: LiMo在空间控制和预测准确性方面均达到最先进水平，能够生成高质量的光照估计结果，特别是在处理复杂光照条件和空间关系时表现优异。

Conclusion: LiMo成功解决了同时实现高真实感细节和准确照度估计的挑战，通过创新的几何条件和扩散模型微调策略，为时空光照估计提供了有效的解决方案。

Abstract: We present Lighting in Motion (LiMo), a diffusion-based approach to spatiotemporal lighting estimation. LiMo targets both realistic high-frequency detail prediction and accurate illuminance estimation. To account for both, we propose generating a set of mirrored and diffuse spheres at different exposures, based on their 3D positions in the input. Making use of diffusion priors, we fine-tune powerful existing diffusion models on a large-scale customized dataset of indoor and outdoor scenes, paired with spatiotemporal light probes. For accurate spatial conditioning, we demonstrate that depth alone is insufficient and we introduce a new geometric condition to provide the relative position of the scene to the target 3D position. Finally, we combine diffuse and mirror predictions at different exposures into a single HDRI map leveraging differentiable rendering. We thoroughly evaluate our method and design choices to establish LiMo as state-of-the-art for both spatial control and prediction accuracy.

</details>


### [291] [Benchmarking Tesla's Traffic Light and Stop Sign Control: Field Dataset and Behavior Insights](https://arxiv.org/abs/2512.11802)
*Zheng Li,Peng Zhang,Shixiao Liang,Hang Zhou,Chengyuan Ma,Handong Yao,Qianwen Li,Xiaopeng Li*

Main category: cs.RO

Relevance: 15.0

TL;DR: 该论文提出了一个特斯拉交通灯和停车标志控制（TLSSC）系统的现场数据集和行为分析，通过实验收集车辆轨迹数据，建立了TLSSC与交通控制设备交互的行为分类，并校准了全速度差模型来量化表征各种行为模式。


<details>
  <summary>Details</summary>
Motivation: 研究动机是理解高级驾驶辅助系统（ADAS）与交通控制设备（TCDs）的交互对交通运行的影响，但这一交互关系缺乏实证研究。特斯拉的TLSSC系统作为成熟的ADAS，其与交通信号灯和停车标志的交互行为需要系统分析。

Method: 方法包括：1）在不同速度限制和TCD类型下设计并执行现场实验；2）收集同步的高分辨率车辆轨迹数据和驾驶员视角视频；3）开发TLSSC-TCD交互行为分类（停止、加速、跟车）；4）校准全速度差模型（FVDM）来量化表征每种行为模式。

Result: 主要结果：1）识别出跟车阈值（约90米）；2）停止行为受期望速度偏差和相对速度的强烈响应驱动；3）加速行为更为保守；4）交叉口跟车行为比标准跟车行为表现出更平滑的动态和更小的车头时距。

Conclusion: 结论是建立的数据集、行为定义和模型表征为未来ADAS-TCD交互逻辑的仿真、安全评估和设计提供了基础。数据集已在GitHub上公开。

Abstract: Understanding how Advanced Driver-Assistance Systems (ADAS) interact with Traffic Control Devices (TCDs) is critical for assessing their influence on traffic operations, yet this interaction has received little focused empirical study. This paper presents a field dataset and behavioral analysis of Tesla's Traffic Light and Stop Sign Control (TLSSC), a mature ADAS that perceives traffic lights and stop signs. We design and execute experiments across varied speed limits and TCD types, collecting synchronized high-resolution vehicle trajectory data and driver-perspective video. From these data, we develop a taxonomy of TLSSC-TCD interaction behaviors (i.e., stopping, accelerating, and car following) and calibrate the Full Velocity Difference Model (FVDM) to quantitatively characterize each behavior mode. A novel empirical insight is the identification of a car-following threshold (~90 m). Calibration results reveal that stopping behavior is driven by strong responsiveness to both desired speed deviation and relative speed, whereas accelerating behavior is more conservative. Intersection car-following behavior exhibits smoother dynamics and tighter headways compared to standard car-following behaviors. The established dataset, behavior definitions, and model characterizations together provide a foundation for future simulation, safety evaluation, and design of ADAS-TCD interaction logic. Our dataset is available at GitHub.

</details>


### [292] [ReGlove: A Soft Pneumatic Glove for Activities of Daily Living Assistance via Wrist-Mounted Vision](https://arxiv.org/abs/2512.11824)
*Rosh Ho,Jian Zhang*

Main category: cs.RO

Relevance: 15.0

TL;DR: ReGlove是一个将低成本商业气动康复手套改造为视觉引导辅助矫形器的系统，通过腕戴摄像头和边缘计算实现无需可靠肌肉信号的上下文感知抓握。


<details>
  <summary>Details</summary>
Motivation: 慢性上肢障碍影响全球数百万人，但现有的辅助技术要么价格昂贵，要么依赖不可靠的生物信号（如EMG）。需要一种更经济、更可靠的解决方案来帮助被传统设备排除在外的群体。

Method: 系统整合腕戴摄像头与边缘计算推理引擎（Raspberry Pi 5），采用实时YOLO计算机视觉模型进行物体识别和抓握分类，控制低成本商业气动康复手套。

Result: 达到96.73%的抓握分类准确率，端到端延迟低于40毫秒；在YCB物体操作基准测试中成功率为82.71%，在27项日常生活活动任务中表现可靠；总成本低于250美元。

Conclusion: ReGlove为基于视觉的上肢辅助提供了技术基础，具有成本效益和可靠性，能够惠及无法使用传统EMG控制设备的人群。

Abstract: This paper presents ReGlove, a system that converts low-cost commercial pneumatic rehabilitation gloves into vision-guided assistive orthoses. Chronic upper-limb impairment affects millions worldwide, yet existing assistive technologies remain prohibitively expensive or rely on unreliable biological signals. Our platform integrates a wrist-mounted camera with an edge-computing inference engine (Raspberry Pi 5) to enable context-aware grasping without requiring reliable muscle signals. By adapting real-time YOLO-based computer vision models, the system achieves \SI{96.73}{\percent} grasp classification accuracy with sub-\SI{40.00}{\milli\second} end-to-end latency. Physical validation using standardized benchmarks shows \SI{82.71}{\percent} success on YCB object manipulation and reliable performance across \SI{27.00}{} Activities of Daily Living (ADL) tasks. With a total cost under \$\SI{250.00}{} and exclusively commercial components, ReGlove provides a technical foundation for accessible, vision-based upper-limb assistance that could benefit populations excluded from traditional EMG-controlled devices.

</details>


### [293] [Navigation Around Unknown Space Objects Using Visible-Thermal Image Fusion](https://arxiv.org/abs/2512.12203)
*Eric J. Elias,Michael Esswein,Jonathan P. How,David W. Miller*

Main category: cs.RO

Relevance: 15.0

TL;DR: 该研究提出了一种可见光/热红外图像融合方法，用于提升未知空间目标导航性能，相比单一模态显著改善了SLAM算法的精度。


<details>
  <summary>Details</summary>
Motivation: 在轨操作中，传统可见光相机在阴影区域性能受限，热红外相机分辨率不足，需要结合两者优势以实现全天候高精度导航。

Method: 通过照片级真实感模拟生成可见光和热红外图像，采用像素级融合方法创建复合图像，使用单目SLAM算法在不同光照和轨迹条件下比较导航误差。

Result: 融合图像相比单一可见光或热红外方法，导航性能得到显著提升。

Conclusion: 可见光/热红外图像融合是提升空间目标导航精度的有效方法，能够克服单一传感器的局限性。

Abstract: As the popularity of on-orbit operations grows, so does the need for precise navigation around unknown resident space objects (RSOs) such as other spacecraft, orbital debris, and asteroids. The use of Simultaneous Localization and Mapping (SLAM) algorithms is often studied as a method to map out the surface of an RSO and find the inspector's relative pose using a lidar or conventional camera. However, conventional cameras struggle during eclipse or shadowed periods, and lidar, though robust to lighting conditions, tends to be heavier, bulkier, and more power-intensive. Thermal-infrared cameras can track the target RSO throughout difficult illumination conditions without these limitations. While useful, thermal-infrared imagery lacks the resolution and feature-richness of visible cameras. In this work, images of a target satellite in low Earth orbit are photo-realistically simulated in both visible and thermal-infrared bands. Pixel-level fusion methods are used to create visible/thermal-infrared composites that leverage the best aspects of each camera. Navigation errors from a monocular SLAM algorithm are compared between visible, thermal-infrared, and fused imagery in various lighting and trajectories. Fused imagery yields substantially improved navigation performance over visible-only and thermal-only methods.

</details>


### [294] [Continuous Edit Distance, Geodesics and Barycenters of Time-varying Persistence Diagrams](https://arxiv.org/abs/2512.12939)
*Sebastien Tchitchek,Mohamed Kissi,Julien Tierny*

Main category: cs.CG

Relevance: 15.0

TL;DR: 提出连续编辑距离(CED)，一种用于时变持久性图(TVPDs)的测地线和弹性距离，结合局部替换成本与惩罚性删除/插入，提供测地线构造和两种重心求解器。


<details>
  <summary>Details</summary>
Motivation: 为时变持久性图(TVPDs)的分析提供一个原则性的距离度量，支持对齐、比较、平均和聚类操作，解决现有方法在时空扰动鲁棒性和时间模式搜索方面的不足。

Method: 扩展编辑距离到TVPDs，通过两个参数(α控制时间错位与图差异的权衡，β控制间隔惩罚)结合局部替换成本与惩罚性删除/插入。提供CED测地线的显式构造，并开发两种重心求解器：随机算法和贪心算法。

Result: CED对加性扰动(时间和空间)具有鲁棒性，能恢复时间偏移，支持时间模式搜索。在真实数据集上，CED的聚类性能与标准弹性差异相当或更好，基于CED重心的聚类产生更优的分类结果。

Conclusion: CED为TVPD分析提供了一个原则性的距离度量、可解释的测地线和实用的重心计算方法，实现了在TVPD空间中的直接对齐、比较、平均和聚类。

Abstract: We introduce the Continuous Edit Distance (CED), a geodesic and elastic distance for time-varying persistence diagrams (TVPDs). The CED extends edit-distance ideas to TVPDs by combining local substitution costs with penalized deletions/insertions, controlled by two parameters: \(α\) (trade-off between temporal misalignment and diagram discrepancy) and \(β\) (gap penalty). We also provide an explicit construction of CED-geodesics. Building on these ingredients, we present two practical barycenter solvers, one stochastic and one greedy, that monotonically decrease the CED Frechet energy. Empirically, the CED is robust to additive perturbations (both temporal and spatial), recovers temporal shifts, and supports temporal pattern search. On real-life datasets, the CED achieves clustering performance comparable or better than standard elastic dissimilarities, while our clustering based on CED-barycenters yields superior classification results. Overall, the CED equips TVPD analysis with a principled distance, interpretable geodesics, and practical barycenters, enabling alignment, comparison, averaging, and clustering directly in the space of TVPDs. A C++ implementation is provided for reproducibility at the following address https://github.com/sebastien-tchitchek/ContinuousEditDistance.

</details>


### [295] [VoroLight: Learning Quality Volumetric Voronoi Meshes from General Inputs](https://arxiv.org/abs/2512.12984)
*Jiayin Lu,Ying Jiang,Yin Yang,Chenfanfu Jiang*

Main category: cs.CG

Relevance: 15.0

TL;DR: VoroLight是一个基于Voronoi网格化的可微分3D形状重建框架，能够从多种输入生成平滑、水密的表面和拓扑一致的体积网格


<details>
  <summary>Details</summary>
Motivation: 解决3D形状重建中生成高质量、拓扑一致的表面和体积网格的挑战，支持从多种输入类型（图像、隐式形状场、点云、网格）进行重建

Method: 三阶段方法：1）使用可微分Voronoi公式初始化表面；2）通过多边形面球训练阶段细化表面质量；3）重用可微分Voronoi公式进行体积优化，添加内部生成点

Result: 能够生成平滑、水密的表面和拓扑一致的体积网格，支持从多样化输入进行重建

Conclusion: VoroLight提供了一个有效的可微分框架，用于从多种输入生成高质量的3D形状重建结果

Abstract: We present VoroLight, a differentiable framework for 3D shape reconstruction based on Voronoi meshing. Our approach generates smooth, watertight surfaces and topologically consistent volumetric meshes directly from diverse inputs, including images, implicit shape level-set fields, point clouds and meshes. VoroLight operates in three stages: it first initializes a surface using a differentiable Voronoi formulation, then refines surface quality through a polygon-face sphere training stage, and finally reuses the differentiable Voronoi formulation for volumetric optimization with additional interior generator points. Project page: https://jiayinlu19960224.github.io/vorolight/

</details>


### [296] [AI-Augmented Pollen Recognition in Optical and Holographic Microscopy for Veterinary Imaging](https://arxiv.org/abs/2512.12101)
*Swarn S. Warshaneyan,Maksims Ivanovs,Blaž Cugmas,Inese Bērziņa,Laura Goldberga,Mindaugas Tamosiunas,Roberts Kadiķis*

Main category: cs.CV

Relevance: 10.0

TL;DR: 该论文研究全自动花粉识别，在光学和数字在线全息显微镜图像上建立性能基线，使用GAN生成合成数据提升DIHM图像中的目标检测性能。


<details>
  <summary>Details</summary>
Motivation: 在未重建的全息图像中视觉识别花粉具有挑战性，因为存在散斑噪声、双图像伪影以及与明场图像的显著差异。研究旨在弥合光学和DIHM图像之间的性能差距，推动全自动DIHM工作流程在兽医成像中的应用。

Method: 使用YOLOv8s进行目标检测和MobileNetV3L进行分类，在双模态数据集上训练。采用Wasserstein GAN with spectral normalization生成合成DIHM图像，通过混合真实和合成数据提升检测性能。

Result: 光学数据上检测mAP50达91.3%，分类准确率97%；DIHM数据上检测mAP50仅8.15%，分类准确率50%。使用GAN合成数据混合训练后，DIHM检测mAP50提升至15.4%。

Conclusion: GAN-based数据增强可以缩小光学和DIHM图像之间的性能差距，为全自动DIHM工作流程在兽医成像中的应用迈出了重要一步。

Abstract: We present a comprehensive study on fully automated pollen recognition across both conventional optical and digital in-line holographic microscopy (DIHM) images of sample slides. Visually recognizing pollen in unreconstructed holographic images remains challenging due to speckle noise, twin-image artifacts and substantial divergence from bright-field appearances. We establish the performance baseline by training YOLOv8s for object detection and MobileNetV3L for classification on a dual-modality dataset of automatically annotated optical and affinely aligned DIHM images. On optical data, detection mAP50 reaches 91.3% and classification accuracy reaches 97%, whereas on DIHM data, we achieve only 8.15% for detection mAP50 and 50% for classification accuracy. Expanding the bounding boxes of pollens in DIHM images over those acquired in aligned optical images achieves 13.3% for detection mAP50 and 54% for classification accuracy. To improve object detection in DIHM images, we employ a Wasserstein GAN with spectral normalization (WGAN-SN) to create synthetic DIHM images, yielding an FID score of 58.246. Mixing real-world and synthetic data at the 1.0 : 1.5 ratio for DIHM images improves object detection up to 15.4%. These results demonstrate that GAN-based augmentation can reduce the performance divide, bringing fully automated DIHM workflows for veterinary imaging a small but important step closer to practice.

</details>


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [297] [A Monad-Based Clause Architecture for Artificial Age Score (AAS) in Large Language Models](https://arxiv.org/abs/2512.11835)
*Seyma Yaman Kayadibi*

Main category: cs.AI

Relevance: 85.0

TL;DR: 本文基于人工年龄评分（AAS）理论，开发了一个基于条款的工程架构，将莱布尼茨《单子论》中的20个单子分组为6个束，实现为可执行规范，用于约束和分析LLM的内部记忆与控制动态。


<details>
  <summary>Details</summary>
Motivation: 当前LLM作为强大但不透明的系统部署，缺乏对其内部记忆和"自我"行为的可审计原则性治理方法。需要建立透明、可实施的框架来约束和分析LLM的内部动态。

Method: 基于AAS内核，将莱布尼茨《单子论》中的20个单子分为6个束（本体论、动力学、表征与意识、和谐与理性、身体与组织、目的论），实现为可执行的Python规范。通过六个最小实现，在通道级量（回忆分数、冗余度、权重）上进行数值实验。

Result: 条款系统展现出有界且可解释的行为：AAS轨迹保持连续且速率受限，矛盾和未支持声明触发明确惩罚，分层细化以受控方式揭示有机结构。和谐项对齐双重视图和目标-行动对，完美度分数的窗口漂移区分持续改进与退化。

Conclusion: 基于单子的条款框架使用AAS作为骨干，为约束和分析人工智能体的内部动态提供了透明、代码级的蓝图，将哲学动机转化为可实施的技术方案。

Abstract: Large language models (LLMs) are often deployed as powerful yet opaque systems, leaving open how their internal memory and "self-like" behavior should be governed in a principled and auditable way. The Artificial Age Score (AAS) was previously introduced and mathematically justified through three theorems that characterise it as a metric of artificial memory aging. Building on this foundation, the present work develops an engineering-oriented, clause-based architecture that imposes law-like constraints on LLM memory and control. Twenty selected monads from Leibniz's Monadology are grouped into six bundles: ontology, dynamics, representation and consciousness, harmony and reason, body and organisation, and teleology, and each bundle is realised as an executable specification on top of the AAS kernel. Across six minimal Python implementations, these clause families are instantiated in numerical experiments acting on channel-level quantities such as recall scores, redundancy, and weights. Each implementation follows a four-step pattern: inputs and setup, clause implementation, numerical results, and implications for LLM design, emphasising that the framework is not only philosophically motivated but also directly implementable. The experiments show that the clause system exhibits bounded and interpretable behavior: AAS trajectories remain continuous and rate-limited, contradictions and unsupported claims trigger explicit penalties, and hierarchical refinement reveals an organic structure in a controlled manner. Dual views and goal-action pairs are aligned by harmony terms, and windowed drift in perfection scores separates sustained improvement from sustained degradation. Overall, the monad-based clause framework uses AAS as a backbone and provides a transparent, code-level blueprint for constraining and analyzing internal dynamics in artificial agents.

</details>


### [298] [Structured Personalization: Modeling Constraints as Matroids for Data-Minimal LLM Agents](https://arxiv.org/abs/2512.11907)
*Daniel Platnick,Marjan Alirezaie,Hossein Rahnama*

Main category: cs.AI

Relevance: 85.0

TL;DR: 该论文提出了一种在结构化约束下个性化LLM代理的数学框架，将知识图谱依赖关系编译为宏观面，证明常见约束形成层状拟阵，从而获得近似最优保证。


<details>
  <summary>Details</summary>
Motivation: 个性化LLM代理需要在用户数据披露和任务效用之间权衡，现实中的结构化约束（逻辑依赖、类别配额、层次规则）破坏了标准子集选择算法的假设，需要新的理论框架。

Method: 提出将用户知识图谱依赖关系编译为宏观面的过程，证明常见层次和配额约束形成层状拟阵，将结构化个性化建模为拟阵约束下的子模最大化问题。

Result: 理论证明结构化约束形成层状拟阵，使得贪婪算法能获得常数因子近似保证，连续贪婪算法可获得(1-1/e)近似比，适用于更丰富现实的个性化问题。

Conclusion: 该框架为LLM代理的结构化个性化提供了理论基础，将现实约束形式化为拟阵约束下的子模优化，实现了效用和隐私的平衡，具有理论保证。

Abstract: Personalizing Large Language Model (LLM) agents requires conditioning them on user-specific data, creating a critical trade-off between task utility and data disclosure. While the utility of adding user data often exhibits diminishing returns (i.e., submodularity), enabling near-optimal greedy selection, real-world personalization is complicated by structural constraints. These include logical dependencies (e.g., selecting fact A requires fact B), categorical quotas (e.g., select at most one writing style), and hierarchical rules (e.g., select at most two social media preferences, of which at most one can be for a professional network). These constraints violate the assumptions of standard subset selection algorithms. We propose a principled method to formally model such constraints. We introduce a compilation process that transforms a user's knowledge graph with dependencies into a set of abstract macro-facets. Our central result is a proof that common hierarchical and quota-based constraints over these macro-facets form a valid laminar matroid. This theoretical characterization lets us cast structured personalization as submodular maximization under a matroid constraint, enabling greedy with constant-factor guarantees (and (1-1/e) via continuous greedy) for a much richer and more realistic class of problems.

</details>


### [299] [Causal Strengths and Leaky Beliefs: Interpreting LLM Reasoning via Noisy-OR Causal Bayes Nets](https://arxiv.org/abs/2512.11909)
*Hanna Dettki*

Main category: cs.AI

Relevance: 85.0

TL;DR: 该研究通过因果推理任务比较LLMs与人类的推理能力，使用贝叶斯网络建模，发现LLMs在因果推理上与人类存在显著差异


<details>
  <summary>Details</summary>
Motivation: 因果推理被认为是智能的关键方面，通过在同一任务上评估LLMs和人类的因果推理能力，可以更全面地理解两者的优势和弱点。研究旨在回答三个核心问题：LLMs与人类在相同推理任务上是否一致？两者在任务层面是否一致推理？它们是否有不同的推理特征？

Method: 评估20多个LLMs在11个语义化的因果任务上，这些任务基于碰撞图(C₁→E←C₂)形式化。采用两种评估方式：直接评估（单次概率判断）和思维链（先思考后回答）。使用带有泄露参数的噪声OR因果贝叶斯网络建模，参数θ=(b,m₁,m₂,p(C))∈[0,1]，通过AIC在3参数对称因果强度(m₁=m₂)和4参数非对称(m₁≠m₂)变体中选择最佳模型。

Result: 研究发现LLMs在因果推理任务上的表现与人类存在显著差异，表明两者具有不同的推理特征和模式

Conclusion: LLMs与人类在因果推理上存在系统性差异，这为理解机器智能与人类智能的本质区别提供了重要见解，对LLMs的评估和解释性研究具有重要意义

Abstract: The nature of intelligence in both humans and machines is a longstanding question. While there is no universally accepted definition, the ability to reason causally is often regarded as a pivotal aspect of intelligence (Lake et al., 2017). Evaluating causal reasoning in LLMs and humans on the same tasks provides hence a more comprehensive understanding of their respective strengths and weaknesses. Our study asks: (Q1) Are LLMs aligned with humans given the \emph{same} reasoning tasks? (Q2) Do LLMs and humans reason consistently at the task level? (Q3) Do they have distinct reasoning signatures?
  We answer these by evaluating 20+ LLMs on eleven semantically meaningful causal tasks formalized by a collider graph ($C_1\!\to\!E\!\leftarrow\!C_2$ ) under \emph{Direct} (one-shot number as response = probability judgment of query node being one and \emph{Chain of Thought} (CoT; think first, then provide answer).
  Judgments are modeled with a leaky noisy-OR causal Bayes net (CBN) whose parameters $θ=(b,m_1,m_2,p(C)) \in [0,1]$ include a shared prior $p(C)$;
  we select the winning model via AIC between a 3-parameter symmetric causal strength ($m_1{=}m_2$) and 4-parameter asymmetric ($m_1{\neq}m_2$) variant.

</details>


### [300] [Robustness of Probabilistic Models to Low-Quality Data: A Multi-Perspective Analysis](https://arxiv.org/abs/2512.11912)
*Liu Peng,Yaochu Jin*

Main category: cs.AI

Relevance: 85.0

TL;DR: 该论文系统比较了不同概率模型对低质量数据的鲁棒性，发现自回归语言模型（如GPT-2）对数据损坏具有显著韧性，而类别条件扩散模型则表现出灾难性退化，分类器的影响则相对温和。


<details>
  <summary>Details</summary>
Motivation: 研究动机是系统性地探究现代概率模型对低质量训练数据的鲁棒性差异。随着大规模数据训练成为常态，理解不同模型架构在面对数据噪声、损坏或不完整时的表现差异至关重要，这关系到模型的实际部署可靠性和训练效率。

Method: 研究方法包括：1）对多种概率模型（自回归语言模型、类别条件扩散模型、分类器）进行系统性数据损坏实验；2）设计不同级别的数据损坏（如50%的token损坏）；3）从信息论、PAC学习和梯度动力学三个理论视角分析结果差异；4）通过测试NLL、图像标签一致性等指标量化模型性能变化。

Result: 主要结果：1）自回归语言模型（如GPT-2）表现出显著鲁棒性，在50% token损坏下测试NLL仅从2.87增加到3.59；2）类别条件扩散模型在相同数据损坏下出现灾难性退化，图像标签一致性相对基线下降56.81%；3）分类器的影响相对温和，且随着数据集规模增大而减弱；4）理论分析表明鲁棒性主要受两个关键原则影响：条件信息的丰富性和训练数据的绝对信息量。

Conclusion: 结论：不同概率模型对低质量数据的鲁棒性存在显著差异，这种差异主要由条件信息的丰富性和训练数据的绝对信息量决定。自回归语言模型因其丰富的条件信息而表现出卓越的鲁棒性，而扩散模型则相对脆弱。这一发现对模型选择、训练策略和数据质量要求具有重要指导意义。

Abstract: A systematic, comparative investigation into the effects of low-quality data reveals a stark spectrum of robustness across modern probabilistic models. We find that autoregressive language models, from token prediction to sequence-to-sequence tasks, are remarkably resilient (for GPT-2, test NLL increases modestly from 2.87 to 3.59 despite 50% token corruption). By contrast, under the same levels of data corruption, class-conditional diffusion models degrade catastrophically (image-label consistency plummets by 56.81% relative to baseline), while classifiers show a moderate impact that diminishes with dataset scale. To explain these discrepancies, we analyze the results through a multi-perspective lens, integrating information theory, PAC learning, and gradient dynamics. These analyses suggest that robustness is heavily influenced by two key principles: the richness of conditioning information, which constrains the learning problem, and the absolute information content of the training data, which allows the signal from correct information to dominate statistical noise.

</details>


### [301] [CXL-SpecKV: A Disaggregated FPGA Speculative KV-Cache for Datacenter LLM Serving](https://arxiv.org/abs/2512.11920)
*Dong Liu,Yanxuan Yu*

Main category: cs.AI

Relevance: 85.0

TL;DR: CXL-SpecKV：一种基于CXL互连和FPGA加速器的解耦KV缓存架构，通过内存解耦、推测性KV缓存预取和FPGA加速压缩，解决LLM推理中的内存瓶颈问题。


<details>
  <summary>Details</summary>
Motivation: LLM在数据中心部署面临KV缓存内存需求巨大的挑战，限制了批处理大小和系统吞吐量。传统GPU内存无法满足大规模LLM推理的内存需求。

Method: 1) 基于CXL的内存解耦框架，将KV缓存卸载到远程FPGA内存；2) 推测性KV缓存预取机制，预测并预加载未来token的缓存条目；3) FPGA加速的KV缓存压缩/解压缩引擎，减少内存带宽需求。

Result: 在SOTA LLM模型上评估，相比GPU-only基线，吞吐量提升3.2倍，内存成本降低2.8倍，同时保持准确性。内存带宽需求减少4倍。

Conclusion: 智能内存解耦与推测执行相结合能有效解决大规模LLM服务中的内存墙挑战，为高效LLM推理提供了新架构方案。

Abstract: Large Language Models (LLMs) have revolutionized natural language processing tasks, but their deployment in datacenter environments faces significant challenges due to the massive memory requirements of key-value (KV) caches. During the autoregressive decoding process, KV caches consume substantial GPU memory, limiting batch sizes and overall system throughput. To address these challenges, we propose \textbf{CXL-SpecKV}, a novel disaggregated KV-cache architecture that leverages Compute Express Link (CXL) interconnects and FPGA accelerators to enable efficient speculative execution and memory disaggregation. Our approach introduces three key innovations: (i) a CXL-based memory disaggregation framework that offloads KV-caches to remote FPGA memory with low latency, (ii) a speculative KV-cache prefetching mechanism that predicts and preloads future tokens' cache entries, and (iii) an FPGA-accelerated KV-cache compression and decompression engine that reduces memory bandwidth requirements by up to 4$\times$. When evaluated on state-of-the-art LLM models, CXL-SpecKV achieves up to 3.2$\times$ higher throughput compared to GPU-only baselines, while reducing memory costs by 2.8$\times$ and maintaining accuracy. Our system demonstrates that intelligent memory disaggregation combined with speculative execution can effectively address the memory wall challenge in large-scale LLM serving. Our code implementation has been open-sourced at https://github.com/FastLM/CXL-SpecKV.

</details>


### [302] [Rethinking Label Consistency of In-Context Learning: An Implicit Transductive Label Propagation Perspective](https://arxiv.org/abs/2512.12175)
*Haoyang Chen,Richong Zhang,Junfan Chen*

Main category: cs.AI

Relevance: 85.0

TL;DR: 论文提出TopK-SD方法，通过数据合成结合语义和标签信息来选择标签一致的演示示例，提升ICL性能


<details>
  <summary>Details</summary>
Motivation: 现有ICL演示选择方法主要基于语义相似性检索，但忽略了标签一致性，这可能导致次优的演示选择。作者从贝叶斯视角和转导标签传播角度重新思考ICL，认为标签一致性对ICL效果至关重要。

Method: 1) 从贝叶斯视角和转导学习角度建立ICL的标签传播框架；2) 提出数据合成方法，同时利用语义和标签信息；3) 使用TopK-SD方法选择标签一致的演示示例。

Result: TopK-SD方法在多个基准测试中优于原始的TopK采样方法，证明了标签一致性在演示选择中的重要性。

Conclusion: 标签一致性是ICL演示选择的关键因素，TopK-SD方法通过结合语义和标签信息选择一致的演示，为理解ICL工作机制提供了新视角。

Abstract: Large language models (LLMs) perform in-context learning (ICL) with minimal supervised examples, which benefits various natural language processing (NLP) tasks. One of the critical research focus is the selection of prompt demonstrations. Current approaches typically employ retrieval models to select the top-K most semantically similar examples as demonstrations. However, we argue that existing methods are limited since the label consistency is not guaranteed during demonstration selection. Our cognition derives from the Bayesian view of ICL and our rethinking of ICL from the transductive label propagation perspective. We treat ICL as a transductive learning method and incorporate latent concepts from Bayesian view and deduce that similar demonstrations guide the concepts of query, with consistent labels serving as estimates. Based on this understanding, we establish a label propagation framework to link label consistency with propagation error bounds. To model label consistency, we propose a data synthesis method, leveraging both semantic and label information, and use TopK sampling with Synthetic Data (TopK-SD) to acquire demonstrations with consistent labels. TopK-SD outperforms original TopK sampling on multiple benchmarks. Our work provides a new perspective for understanding the working mechanisms within ICL.

</details>


### [303] [Feeling the Strength but Not the Source: Partial Introspection in LLMs](https://arxiv.org/abs/2512.12411)
*Ely Hahami,Lavik Jain,Ishaan Sinha*

Main category: cs.AI

Relevance: 85.0

TL;DR: 该研究测试了语言模型对注入概念的"内省"能力，发现虽然能部分复现Anthropic的结果，但这种内省能力是脆弱的、提示敏感的，仅在某些特定任务中有效。


<details>
  <summary>Details</summary>
Motivation: 验证Anthropic关于前沿模型能够检测和命名注入概念的"涌现内省"能力的稳健性，探究这种能力是否仅限于大型模型，以及在不同任务和提示下的表现。

Method: 1) 在Meta-Llama-3.1-8B-Instruct上复现Anthropic的多轮"涌现内省"实验；2) 系统性地改变推理提示，测试内省能力在不同任务（多项选择识别、二元判别）中的表现；3) 探索部分内省机制，让模型对注入概念向量的系数强度进行分类。

Result: 1) 成功复现Anthropic的结果，20%的准确率匹配原报告；2) 内省能力脆弱，在相关任务中性能崩溃；3) 发现部分内省机制，模型能可靠分类注入概念向量的系数强度，准确率达70%（远高于25%的随机基线）。

Conclusion: 语言模型确实能在内省过程中基于其内部表示进行计算，但这些自我报告是狭窄且提示敏感的，内省能力并非大型模型专属但也不稳健。

Abstract: Recent work from Anthropic claims that frontier models can sometimes detect and name injected "concepts" represented as activation directions. We test the robustness of these claims. First, we reproduce Anthropic's multi-turn "emergent introspection" result on Meta-Llama-3.1-8B-Instruct, finding that the model identifies and names the injected concept 20 percent of the time under Anthropic's original pipeline, exactly matching their reported numbers and thus showing that introspection is not exclusive to very large or capable models. Second, we systematically vary the inference prompt and find that introspection is fragile: performance collapses on closely related tasks such as multiple-choice identification of the injected concept or different prompts of binary discrimination of whether a concept was injected at all. Third, we identify a contrasting regime of partial introspection: the same model can reliably classify the strength of the coefficient of a normalized injected concept vector (as weak / moderate / strong / very strong) with up to 70 percent accuracy, far above the 25 percent chance baseline. Together, these results provide more evidence for Anthropic's claim that language models effectively compute a function of their baseline, internal representations during introspection; however, these self-reports about those representations are narrow and prompt-sensitive. Our code is available at https://github.com/elyhahami18/CS2881-Introspection.

</details>


### [304] [Large Language Newsvendor: Decision Biases and Cognitive Mechanisms](https://arxiv.org/abs/2512.12552)
*Jifei Liu,Zhi Chen,Yuanguang Zhong*

Main category: cs.AI

Relevance: 85.0

TL;DR: 研究发现大型语言模型在供应链管理等高风险决策中会复制并放大人类认知偏见，特别是GPT-4表现出"智能悖论"——越复杂的模型越不理性，而效率优化的GPT-4o表现接近最优。


<details>
  <summary>Details</summary>
Motivation: LLMs越来越多地应用于商业决策，但其复制和放大人类认知偏见的风险尚未被充分理解，这在供应链管理等高风险运营环境中尤为关键。研究旨在识别LLMs在动态决策中的认知偏见本质和来源。

Method: 使用动态多轮实验，在经典的报童问题中测试GPT-4、GPT-4o和LLaMA-8B等领先LLMs的五种已知决策偏见。通过对比人类基准，分析模型的行为模式。

Result: LLMs一致复制了经典的"过低/过高"订购偏见，并显著放大了需求追逐等行为。发现"智能悖论"：更复杂的GPT-4表现出最大的非理性（过度思考），而效率优化的GPT-4o表现接近最优。即使提供最优公式，偏见仍然存在，表明其源于架构约束而非知识差距。

Conclusion: 管理者应根据具体任务选择模型，效率优化模型在某些优化问题上可能优于复杂模型。LLMs显著放大偏见，高风险决策需要强有力的人工监督。设计结构化、基于规则的提示是约束模型启发式倾向、提高AI辅助决策可靠性的实用策略。

Abstract: Problem definition: Although large language models (LLMs) are increasingly integrated into business decision making, their potential to replicate and even amplify human cognitive biases cautions a significant, yet not well-understood, risk. This is particularly critical in high-stakes operational contexts like supply chain management. To address this, we investigate the decision-making patterns of leading LLMs using the canonical newsvendor problem in a dynamic setting, aiming to identify the nature and origins of their cognitive biases. Methodology/results: Through dynamic, multi-round experiments with GPT-4, GPT-4o, and LLaMA-8B, we tested for five established decision biases. We found that LLMs consistently replicated the classic ``Too Low/Too High'' ordering bias and significantly amplified other tendencies like demand-chasing behavior compared to human benchmarks. Our analysis uncovered a ``paradox of intelligence'': the more sophisticated GPT-4 demonstrated the greatest irrationality through overthinking, while the efficiency-optimized GPT-4o performed near-optimally. Because these biases persist even when optimal formulas are provided, we conclude they stem from architectural constraints rather than knowledge gaps. Managerial implications: First, managers should select models based on the specific task, as our results show that efficiency-optimized models can outperform more complex ones on certain optimization problems. Second, the significant amplification of bias by LLMs highlights the urgent need for robust human-in-the-loop oversight in high-stakes decisions to prevent costly errors. Third, our findings suggest that designing structured, rule-based prompts is a practical and effective strategy for managers to constrain models' heuristic tendencies and improve the reliability of AI-assisted decisions.

</details>


### [305] [AgentSHAP: Interpreting LLM Agent Tool Importance with Monte Carlo Shapley Value Estimation](https://arxiv.org/abs/2512.12597)
*Miriam Horovicz*

Main category: cs.AI

Relevance: 85.0

TL;DR: AgentSHAP：首个基于Shapley值的LLM智能体工具重要性解释框架，无需访问模型内部权重，通过蒙特卡洛采样降低计算成本。


<details>
  <summary>Details</summary>
Motivation: 现有可解释AI方法无法解释LLM智能体中使用的外部工具对最终响应的贡献程度，这是一个重要的盲点。需要一种能够量化工具重要性的解释方法。

Method: 基于博弈论中的Shapley值，将智能体视为黑盒，通过蒙特卡洛采样测试不同工具子集下的智能体响应，计算公平的工具重要性分数。方法模型无关，适用于任何LLM。

Result: 在API-Bank基准测试中，AgentSHAP产生一致的重要性分数，能正确识别关键工具，区分相关与无关工具，且在不同运行中保持稳定性。

Conclusion: AgentSHAP填补了LLM智能体工具级解释的空白，与TokenSHAP和PixelSHAP共同构成了基于Shapley值的生成式AI可解释性工具家族。

Abstract: LLM agents that use external tools can solve complex tasks, but understanding which tools actually contributed to a response remains a blind spot. No existing XAI methods address tool-level explanations. We introduce AgentSHAP, the first framework for explaining tool importance in LLM agents. AgentSHAP is model-agnostic: it treats the agent as a black box and works with any LLM (GPT, Claude, Llama, etc.) without needing access to internal weights or gradients. Using Monte Carlo Shapley values, AgentSHAP tests how an agent responds with different tool subsets and computes fair importance scores based on game theory. Our contributions are: (1) the first explainability method for agent tool attribution, grounded in Shapley values from game theory; (2) Monte Carlo sampling that reduces cost from O(2n) to practical levels; and (3) comprehensive experiments on API-Bank showing that AgentSHAP produces consistent scores across runs, correctly identifies which tools matter, and distinguishes relevant from irrelevant tools. AgentSHAP joins TokenSHAP (for tokens) and PixelSHAP (for image regions) to complete a family of Shapley-based XAI tools for modern generative AI. Code: https://github.com/GenAISHAP/TokenSHAP.

</details>


### [306] [Value-Aware Multiagent Systems](https://arxiv.org/abs/2512.12652)
*Nardine Osman*

Main category: cs.AI

Relevance: 85.0

TL;DR: 本文提出价值感知AI概念，超越传统价值对齐问题，提供包含三个核心支柱的工程化路线图：学习表示人类价值、确保个体与多智能体系统价值对齐、提供基于价值的可解释性。


<details>
  <summary>Details</summary>
Motivation: 传统AI价值对齐方法存在局限，需要更全面的价值感知框架。本文旨在超越单纯的价值对齐，构建能够理解、表示、对齐并解释人类价值的AI系统。

Method: 提出三支柱工程路线图：1) 使用形式语义学学习和表示人类价值；2) 确保个体智能体和多智能体系统的价值对齐；3) 提供基于价值的行为可解释性。展示了在这些主题上的持续研究工作。

Result: 建立了价值感知AI的概念框架和工程路线图，包含形式化价值表示、对齐机制和可解释性方法。已在现实领域进行应用验证。

Conclusion: 价值感知AI为构建更可信、可解释、与人类价值一致的AI系统提供了系统化框架，超越了传统价值对齐方法，具有重要的理论和实践意义。

Abstract: This paper introduces the concept of value awareness in AI, which goes beyond the traditional value-alignment problem. Our definition of value awareness presents us with a concise and simplified roadmap for engineering value-aware AI. The roadmap is structured around three core pillars: (1) learning and representing human values using formal semantics, (2) ensuring the value alignment of both individual agents and multiagent systems, and (3) providing value-based explainability on behaviour. The paper presents a selection of our ongoing work on some of these topics, along with applications to real-life domains.

</details>


### [307] [Memoria: A Scalable Agentic Memory Framework for Personalized Conversational AI](https://arxiv.org/abs/2512.12686)
*Samarth Sarin,Lovepreet Singh,Bhaskarjit Sarmah,Dhagash Mehta*

Main category: cs.AI

Relevance: 85.0

TL;DR: Memoria是一个模块化记忆框架，通过动态会话级摘要和加权知识图谱用户建模引擎，为LLM对话系统提供持久、可解释、上下文丰富的记忆能力，实现短期对话连贯性和长期个性化。


<details>
  <summary>Details</summary>
Motivation: 当前LLM对话系统缺乏持续性记忆能力，无法在不同对话间保持连续性和个性化。代理记忆（agentic memory）是LLM实现真正交互式、自适应代理的关键能力，需要解决LLM的token限制问题，同时提供可解释的结构化记忆。

Method: Memoria采用混合架构：1) 动态会话级摘要处理短期对话连贯性；2) 加权知识图谱用户建模引擎，以结构化实体和关系增量捕获用户特征、偏好和行为模式。该框架在LLM的token限制内运行，实现可扩展的个性化对话AI。

Result: Memoria框架能够为LLM对话系统提供持久、可解释、上下文丰富的记忆能力，实现短期对话连贯性和长期个性化。它弥合了无状态LLM接口与代理记忆系统之间的差距，为需要自适应和演进用户体验的工业应用提供实用解决方案。

Conclusion: Memoria是一个实用的代理记忆框架，通过结合动态摘要和知识图谱用户建模，使LLM能够保持跨对话的连续性和个性化，为工业级自适应对话AI系统提供了可行的解决方案。

Abstract: Agentic memory is emerging as a key enabler for large language models (LLM) to maintain continuity, personalization, and long-term context in extended user interactions, critical capabilities for deploying LLMs as truly interactive and adaptive agents. Agentic memory refers to the memory that provides an LLM with agent-like persistence: the ability to retain and act upon information across conversations, similar to how a human would. We present Memoria, a modular memory framework that augments LLM-based conversational systems with persistent, interpretable, and context-rich memory. Memoria integrates two complementary components: dynamic session-level summarization and a weighted knowledge graph (KG)-based user modelling engine that incrementally captures user traits, preferences, and behavioral patterns as structured entities and relationships. This hybrid architecture enables both short-term dialogue coherence and long-term personalization while operating within the token constraints of modern LLMs. We demonstrate how Memoria enables scalable, personalized conversational artificial intelligence (AI) by bridging the gap between stateless LLM interfaces and agentic memory systems, offering a practical solution for industry applications requiring adaptive and evolving user experiences.

</details>


### [308] [WebOperator: Action-Aware Tree Search for Autonomous Agents in Web Environment](https://arxiv.org/abs/2512.12692)
*Mahir Labib Dihan,Tanzima Hashem,Mohammed Eunus Ali,Md Rizwan Parvez*

Main category: cs.AI

Relevance: 85.0

TL;DR: WebOperator：一个用于网页任务的树搜索框架，通过安全回溯和战略探索解决LLM智能体缺乏远见的问题，在WebArena上达到54.6%的SOTA成功率。


<details>
  <summary>Details</summary>
Motivation: 当前LLM智能体在网页环境中采用贪婪的逐步操作方式，缺乏长期规划和回溯能力。网页环境是部分可观察的（仅DOM和UI元素可见），单步错误需要复杂导航来撤销。现有树搜索方法缺乏安全回溯机制，假设所有动作可逆，这在真实网页任务中不成立。

Method: WebOperator框架包含：1）最佳优先搜索策略，基于奖励估计和安全性考虑对动作排序；2）鲁棒回溯机制，在重放路径前验证可行性；3）从多个推理上下文生成动作候选，确保探索多样性；4）执行前过滤无效动作并合并语义等价动作。

Result: 在WebArena和WebVoyager上的实验表明，WebOperator在WebArena上使用gpt-4o达到54.6%的成功率，创下新的最先进水平，证明了战略远见与安全执行结合的关键优势。

Conclusion: WebOperator通过集成战略远见和安全回溯，显著提升了LLM智能体在网页任务中的性能，解决了现有方法在部分可观察环境中缺乏规划能力和安全回溯的问题。

Abstract: LLM-based agents often operate in a greedy, step-by-step manner, selecting actions solely based on the current observation without considering long-term consequences or alternative paths. This lack of foresight is particularly problematic in web environments, which are only partially observable-limited to browser-visible content (e.g., DOM and UI elements)-where a single misstep often requires complex and brittle navigation to undo. Without an explicit backtracking mechanism, agents struggle to correct errors or systematically explore alternative paths. Tree-search methods provide a principled framework for such structured exploration, but existing approaches lack mechanisms for safe backtracking, making them prone to unintended side effects. They also assume that all actions are reversible, ignoring the presence of irreversible actions-limitations that reduce their effectiveness in realistic web tasks. To address these challenges, we introduce WebOperator, a tree-search framework that enables reliable backtracking and strategic exploration. Our method incorporates a best-first search strategy that ranks actions by both reward estimates and safety considerations, along with a robust backtracking mechanism that verifies the feasibility of previously visited paths before replaying them, preventing unintended side effects. To further guide exploration, WebOperator generates action candidates from multiple, varied reasoning contexts to ensure diverse and robust exploration, and subsequently curates a high-quality action set by filtering out invalid actions pre-execution and merging semantically equivalent ones. Experimental results on WebArena and WebVoyager demonstrate the effectiveness of WebOperator. On WebArena, WebOperator achieves a state-of-the-art 54.6% success rate with gpt-4o, underscoring the critical advantage of integrating strategic foresight with safe execution.

</details>


### [309] [Fault-Tolerant Sandboxing for AI Coding Agents: A Transactional Approach to Safe Autonomous Execution](https://arxiv.org/abs/2512.12806)
*Boyang Yan*

Main category: cs.AI

Relevance: 85.0

TL;DR: 提出一个容错沙盒框架，通过策略拦截层和事务性文件系统快照机制，保护LLM自主代理免受破坏性命令和不一致系统状态的风险，实现100%高风险命令拦截和100%失败状态回滚，仅产生14.5%性能开销。


<details>
  <summary>Details</summary>
Motivation: LLM从被动代码生成器转变为自主代理时，面临破坏性命令和不一致系统状态的安全风险。现有商业解决方案通常优先考虑交互式用户安全，强制身份验证屏障会破坏真正自主性所需的无头循环。

Method: 提出容错沙盒框架，包含策略拦截层和事务性文件系统快照机制。将代理动作包装在原子事务中，使用Minimind-MoE LLM通过nano-vllm服务，在基于Proxmox的自定义测试平台上部署，利用EVPN/VXLAN隔离。

Result: 实验结果显示：100%高风险命令拦截率，100%失败状态回滚成功率。原型仅产生14.5%性能开销（约1.8秒/事务）。相比Gemini CLI沙盒需要交互式身份验证，该框架适用于无头自主代理工作流。

Conclusion: 容错沙盒框架能有效保护LLM自主代理安全，通过原子事务保证安全性且性能开销可接受，优于容器初始化开销或商业CLI的交互摩擦，适用于真正的自主代理工作流。

Abstract: The transition of Large Language Models (LLMs) from passive code generators to autonomous agents introduces significant safety risks, specifically regarding destructive commands and inconsistent system states. Existing commercial solutions often prioritize interactive user safety, enforcing authentication barriers that break the headless loops required for true autonomy. This paper presents a Fault-Tolerant Sandboxing framework designed to mitigate these risks through a policy-based interception layer and a transactional filesystem snapshot mechanism. We hypothesize that wrapping agent actions in atomic transactions can guarantee safety with acceptable latency, outperforming the heavy initialization overhead of containers or the interactive friction of commercial CLIs. We validated this approach by deploying the Minimind-MoE LLM served via nano-vllm on a custom Proxmox-based testbed utilizing EVPN/VXLAN isolation. Experimental results demonstrate a 100\% interception rate for high-risk commands and a 100\% success rate in rolling back failed states. Crucially, our prototype incurs only a 14.5\% performance overhead (approx. 1.8s) per transaction. In contrast, benchmarking against the Gemini CLI sandbox revealed that it requires interactive authentication ("Sign in"), rendering it unusable for headless, autonomous agent workflows.

</details>


### [310] [M-GRPO: Stabilizing Self-Supervised Reinforcement Learning for Large Language Models with Momentum-Anchored Policy Optimization](https://arxiv.org/abs/2512.13070)
*Bizhe Bai,Hongming Wu,Peng Ye,Tao Chen*

Main category: cs.AI

Relevance: 85.0

TL;DR: 论文提出M-GRPO框架和IQR自适应过滤方法，解决自监督强化学习中长期训练时的策略崩溃问题，提升LLM推理能力。


<details>
  <summary>Details</summary>
Motivation: 现有自监督强化学习方法在长期训练中存在"策略崩溃"问题，性能急剧下降。即使增加rollout数量也只能延迟而非防止崩溃，需要更稳定的训练方法。

Method: 1. M-GRPO：使用缓慢演进的动量模型提供稳定训练目标；2. IQR自适应过滤：基于四分位距动态修剪低熵轨迹，保持策略多样性。

Result: 在多个推理基准测试中，M-GRPO稳定了训练过程，IQR过滤防止了过早收敛，组合方法实现了最先进的性能。

Conclusion: 通过动量锚定和自适应熵过滤，成功解决了自监督强化学习中的策略崩溃问题，为LLM推理能力提升提供了稳定有效的训练框架。

Abstract: Self-supervised reinforcement learning (RL) presents a promising approach for enhancing the reasoning capabilities of Large Language Models (LLMs) without reliance on expensive human-annotated data. However, we find that existing methods suffer from a critical failure mode under long-horizon training: a "policy collapse" where performance precipitously degrades. We diagnose this instability and demonstrate that simply scaling the number of rollouts -- a common strategy to improve performance -- only delays, but does not prevent, this collapse. To counteract this instability, we first introduce M-GRPO (Momentum-Anchored Group Relative Policy Optimization), a framework that leverages a slowly evolving momentum model to provide a stable training target. In addition, we identify that this process is often accompanied by a rapid collapse in policy entropy, resulting in a prematurely confident and suboptimal policy. To specifically address this issue, we propose a second contribution: an adaptive filtering method based on the interquartile range (IQR) that dynamically prunes low-entropy trajectories, preserving essential policy diversity. Our extensive experiments on multiple reasoning benchmarks demonstrate that M-GRPO stabilizes the training process while the IQR filter prevents premature convergence. The combination of these two innovations leads to superior training stability and state-of-the-art performance.

</details>


### [311] [Socratic Students: Teaching Language Models to Learn by Asking Questions](https://arxiv.org/abs/2512.13102)
*Rajeev Bhatt Ambati,Tianyi Niu,Aashu Singh,Shlok Mishra,Shashank Srivastava,Snigdha Chaturvedi*

Main category: cs.AI

Relevance: 85.0

TL;DR: 该论文研究LLM在动态交互中的主动学习策略，让学生模型通过主动提问向教师模型获取知识，在数学和编程任务上显著提升性能。


<details>
  <summary>Details</summary>
Motivation: LLM在静态交互中表现出色，但在教育辅导、医疗协助等现实场景中，相关信息需要通过动态交互主动获取。现有研究主要关注教师如何有效指导学生，而本文转向研究学生如何主动向教师提问以获取有用信息。

Method: 提出学生主导的主动学习策略，让学生模型识别自身不确定性并提问。使用直接偏好优化（DPO）训练学生模型，利用自我指导或更强学生模型的指导来提升提问质量。

Result: 在数学和编程基准测试中，学生主导方法相比静态基线在Pass@k指标上至少提升0.5个绝对百分点。通过DPO指导训练，较小模型能学会提出更好的问题，进一步提高学习效率。

Conclusion: 学生主导的主动学习策略能有效提升LLM在动态交互场景中的性能，通过优化提问质量可以增强学习效率，为LLM在实际应用中的交互能力提供了新方向。

Abstract: Large Language Models (LLMs) excel at static interactions, where they answer user queries by retrieving knowledge encoded in their parameters. However, in many real-world settings, such as educational tutoring or medical assistance, relevant information is not directly available and must be actively acquired through dynamic interactions. An interactive agent would recognize its own uncertainty, ask targeted questions, and retain new knowledge efficiently. Prior work has primarily explored effective ways for a teacher to instruct the student, where the teacher identifies student gaps and provides guidance. In this work, we shift the focus to the student and investigate effective strategies to actively query the teacher in seeking useful information. Across math and coding benchmarks, where baseline student models begin with near-zero performance, we show that student-led approaches consistently yield absolute Pass@k improvements of at least 0.5 over static baselines. To improve question quality, we train students using Direct Preference Optimization (DPO) with guidance from either self or stronger students. We find that this guided training enables smaller models to learn how to ask better questions, further enhancing learning efficiency.

</details>


### [312] [Can AI Understand What We Cannot Say? Measuring Multilevel Alignment Through Abortion Stigma Across Cognitive, Interpersonal, and Structural Levels](https://arxiv.org/abs/2512.13142)
*Anika Sharma,Malavika Mampally,Chidaksh Ravuru,Kandyce Brennan,Neil Gaikwad*

Main category: cs.AI

Relevance: 85.0

TL;DR: 研究发现当前大语言模型在理解复杂的心理社会现象（如堕胎污名）方面存在系统性缺陷，无法在多层次（认知、人际、结构）上形成连贯理解，尽管能生成恰当语言但缺乏真正的理解能力。


<details>
  <summary>Details</summary>
Motivation: 随着大语言模型越来越多地介入敏感的医疗决策，需要评估它们是否真正理解复杂的心理和生理现象。研究旨在探究LLMs能否在堕胎污名这一复杂社会心理现象的多层次（认知、人际、结构）上形成连贯理解。

Method: 使用经过验证的个体层面堕胎污名量表（ILAS），系统测试了627个人口统计学多样化的角色在五个领先LLMs上的表现。进行多层次分析，考察模型在认知层面（自我评判）、人际层面（预期评判和孤立）、结构层面（社区谴责和披露模式）以及整体污名上的表现。

Result: 模型在所有层次上都未能通过真正理解的测试：高估人际污名而低估认知污名；假设统一的社区谴责；引入人类验证数据中不存在的人口统计偏见；错过经验验证的污名-保密关系；在理论建构内自相矛盾。

Conclusion: 当前的对齐方法只能确保恰当的语言表达，但不能保证多层次连贯理解。需要新的设计方法（多层次连贯性）、评估方法（持续审计）、治理与监管（强制审计、问责、部署限制）以及AI素养，特别是在理解人们无法言说的内容决定支持是有益还是有害的领域。

Abstract: As large language models increasingly mediate stigmatized health decisions, their capacity to genuinely understand complex psychological and physiological phenomena remains poorly evaluated. Can AI understand what we cannot say? We investigate whether LLMs coherently represent abortion stigma across the cognitive, interpersonal, and structural levels where it operates. We systematically tested 627 demographically diverse personas across five leading LLMs using the validated Individual Level Abortion Stigma Scale (ILAS). Our multilevel analysis examined whether models coherently represent stigma at the cognitive level (self-judgment), interpersonal level (anticipated judgment and isolation), and structural level (community condemnation and disclosure patterns), as well as overall stigma. Models fail tests of genuine understanding across all levels. They overestimate interpersonal stigma while underestimating cognitive stigma, assume uniform community condemnation, introduce demographic biases absent from human validation data, miss the empirically validated stigma-secrecy relationship, and contradict themselves within theoretical constructs. These patterns reveal that current alignment approaches ensure appropriate language but not coherent multilevel understanding. This work provides empirical evidence that current LLMs lack coherent multilevel understanding of psychological and physiological constructs. AI safety in high-stakes contexts demands new approaches to design (multilevel coherence), evaluation (continuous auditing), governance and regulation (mandatory audits, accountability, deployment restrictions), and AI literacy in domains where understanding what people cannot say determines whether support helps or harms.

</details>


### [313] [SpeakRL: Synergizing Reasoning, Speaking, and Acting in Language Models with Reinforcement Learning](https://arxiv.org/abs/2512.13159)
*Emre Can Acikgoz,Jinoh Oh,Jie Hao,Joo Hyuk Jeon,Heng Ji,Dilek Hakkani-Tür,Gokhan Tur,Xiang Li,Chengyuan Ma,Xing Fan*

Main category: cs.AI

Relevance: 85.0

TL;DR: SpeakRL是一种强化学习方法，通过奖励智能体主动与用户互动（如提出澄清问题）来增强其对话能力，在任务完成率上比基础模型提升20.14%


<details>
  <summary>Details</summary>
Motivation: 当前人机协作主要是单向的，用户发出指令，智能体直接响应而不寻求必要的澄清或确认。随着智能体能力的发展，需要更主动的参与来澄清用户意图、解决歧义并适应变化的环境。现有工作未能充分利用语言模型的对话能力，将智能体优化为更好的跟随者而非有效的发言者。

Method: 提出SpeakRL强化学习方法，通过奖励主动交互（如必要时的澄清问题）来增强智能体对话能力。创建SpeakER合成数据集，包含任务导向对话的多样化场景。系统分析对话主动性的奖励设计，提出原则性的奖励公式来教导智能体平衡提问与行动。

Result: 实证评估显示，该方法在任务完成率上比基础模型提升20.14%的绝对改进，且不增加对话轮数，甚至超越了更大的专有模型，证明了以澄清为中心的人机交互的潜力。

Conclusion: SpeakRL通过强化学习增强智能体的对话主动性，在任务导向对话中显著提高了任务完成率，为人机协作提供了更有效的交互模式。

Abstract: Effective human-agent collaboration is increasingly prevalent in real-world applications. Current trends in such collaborations are predominantly unidirectional, with users providing instructions or posing questions to agents, where agents respond directly without seeking necessary clarifications or confirmations. However, the evolving capabilities of these agents require more proactive engagement, where agents should dynamically participate in conversations to clarify user intents, resolve ambiguities, and adapt to changing circumstances. Existing prior work under-utilize the conversational capabilities of language models (LMs), thereby optimizing agents as better followers rather than effective speakers. In this work, we introduce SpeakRL, a reinforcement learning (RL) method that enhances agents' conversational capabilities by rewarding proactive interactions with users, such as asking right clarification questions when necessary. To support this, we curate SpeakER, a synthetic dataset that includes diverse scenarios from task-oriented dialogues, where tasks are resolved through interactive clarification questions. We present a systematic analysis of reward design for conversational proactivity and propose a principled reward formulation for teaching agents to balance asking with acting. Empirical evaluations demonstrate that our approach achieves a 20.14% absolute improvement in task completion over base models without increasing conversation turns even surpassing even much larger proprietary models, demonstrating the promise of clarification-centric user-agent interactions.

</details>


### [314] [Finch: Benchmarking Finance & Accounting across Spreadsheet-Centric Enterprise Workflows](https://arxiv.org/abs/2512.13168)
*Haoyu Dong,Pengkun Zhang,Yan Gao,Xuanyu Dong,Yilin Cheng,Mingzhe Lu,Adina Yakefu,Shuxin Zheng*

Main category: cs.AI

Relevance: 85.0

TL;DR: Finch是一个用于评估AI代理在真实企业级金融会计工作流中的基准测试，基于Enron等机构的真实数据，包含172个复合工作流和384个任务，测试显示当前前沿AI系统在复杂企业工作流中表现有限。


<details>
  <summary>Details</summary>
Motivation: 现有AI基准测试主要关注单一任务或简化场景，缺乏对真实企业环境中复杂、多模态、长时程工作流的评估。金融会计领域的工作涉及数据录入、计算、建模、验证、可视化等多种任务的交织，需要专门的基准来评估AI代理在实际企业工作环境中的能力。

Method: 1) 从Enron等金融机构的真实工作空间收集15,000个电子表格和500,000封邮件；2) 采用LLM辅助发现与专家标注相结合的工作流构建流程：LLM从邮件线程和电子表格版本历史中推导工作流，专家验证和标注；3) 构建包含172个复合工作流、384个任务的基准，涉及1,710个电子表格（2700万单元格）及PDF等其他多模态文档。

Result: 对GPT 5.1、Claude Sonnet 4.5、Gemini 3 Pro、Grok 4、Qwen 3 Max等前沿AI系统进行评估：GPT 5.1 Pro花费48小时仅通过38.4%的工作流，Claude Sonnet 4.5仅通过25.0%。结果显示当前AI系统在处理真实企业工作流时面临显著挑战。

Conclusion: Finch基准揭示了真实企业工作流对AI代理的挑战性，这些工作流具有内在的混乱性、长时程性、知识密集性和协作性。该基准为评估和改进AI代理在企业环境中的实际应用能力提供了重要工具。

Abstract: We introduce a finance & accounting benchmark (Finch) for evaluating AI agents on real-world, enterprise-grade professional workflows -- interleaving data entry, structuring, formatting, web search, cross-file retrieval, calculation, modeling, validation, translation, visualization, and reporting. Finch is sourced from authentic enterprise workspaces at Enron (15,000 spreadsheets and 500,000 emails from 150 employees) and other financial institutions, preserving in-the-wild messiness across multimodal artifacts (text, tables, formulas, charts, code, and images) and spanning diverse domains such as budgeting, trading, and asset management.
  We propose a workflow construction process that combines LLM-assisted discovery with expert annotation: (1) LLM-assisted, expert-verified derivation of workflows from real-world email threads and version histories of spreadsheet files, and (2) meticulous expert annotation for workflows, requiring over 700 hours of domain-expert effort. This yields 172 composite workflows with 384 tasks, involving 1,710 spreadsheets with 27 million cells, along with PDFs and other artifacts, capturing the intrinsically messy, long-horizon, knowledge-intensive, and collaborative nature of real-world enterprise work.
  We conduct both human and automated evaluations of frontier AI systems including GPT 5.1, Claude Sonnet 4.5, Gemini 3 Pro, Grok 4, and Qwen 3 Max, and GPT 5.1 Pro spends 48 hours in total yet passes only 38.4% of workflows, while Claude Sonnet 4.5 passes just 25.0%. Comprehensive case studies further surface the challenges that real-world enterprise workflows pose for AI agents.

</details>


### [315] [Reflective Preference Optimization (RPO): Enhancing On-Policy Alignment via Hint-Guided Reflection](https://arxiv.org/abs/2512.13240)
*Zihui Zhao,Zechang Li*

Main category: cs.AI

Relevance: 85.0

TL;DR: RPO通过引入外部模型生成的反思提示来增强DPO的对比学习信号，解决了标准DPO中正负样本差异小、收敛慢的问题，在减少幻觉和提升多模态对齐效果方面表现优异。


<details>
  <summary>Details</summary>
Motivation: 标准DPO框架中，正负样本来自同一策略，通常具有相似的错误和较小的KL散度，导致学习信号弱、收敛慢且不稳定。需要更强的对比学习信号来提升对齐效果。

Method: RPO在DPO基础上引入反思提示：1) 使用外部模型识别幻觉来源并生成简洁的反思提示；2) 基于提示构建具有更强对比性的策略内偏好对；3) 理论上证明提示条件化通过互信息增加期望偏好边界，提升样本效率。

Result: RPO在更少的训练样本和迭代次数下实现更优的对齐效果，显著降低幻觉率，在多模态基准测试中达到最先进性能。

Conclusion: RPO通过引入反思提示增强DPO的对比学习信号，解决了标准DPO的局限性，为LLM对齐提供了一种更高效、更稳定的方法，特别适合多模态场景。

Abstract: Direct Preference Optimization (DPO) has emerged as a lightweight and effective alternative to Reinforcement Learning from Human Feedback (RLHF) and Reinforcement Learning with AI Feedback (RLAIF) for aligning large language and vision-language models. However, the standard DPO formulation, in which both the chosen and rejected responses are generated by the same policy, suffers from a weak learning signal because the two responses often share similar errors and exhibit small Kullback-Leibler (KL) divergence. This leads to slow and unstable convergence. To address this limitation, we introduce Reflective Preference Optimization (RPO), a new framework that incorporates hint-guided reflection into the DPO paradigm. RPO uses external models to identify hallucination sources and generate concise reflective hints, enabling the construction of on-policy preference pairs with stronger contrastiveness and clearer preference signals. We theoretically show that conditioning on hints increases the expected preference margin through mutual information and improves sample efficiency while remaining within the policy distribution family. Empirically, RPO achieves superior alignment with fewer training samples and iterations, substantially reducing hallucination rates and delivering state-of-the-art performance across multimodal benchmarks.

</details>


### [316] [Error-Driven Prompt Optimization for Arithmetic Reasoning](https://arxiv.org/abs/2512.13323)
*Árpád Pándy,Róbert Lakatos,András Hajdu*

Main category: cs.AI

Relevance: 85.0

TL;DR: 本文提出了一种基于错误驱动的优化框架，用于提升小型语言模型在算术推理任务上的性能，通过聚类错误预测并迭代优化提示规则，使4B参数模型在隐私合规的本地部署中超越GPT-3.5 Turbo。


<details>
  <summary>Details</summary>
Motivation: 针对金融、医疗等受监管行业需要本地部署AI助手的需求，研究如何在保护敏感数据不外泄的前提下，让小型语言模型在结构化数据工作流中执行准确的算术运算操作。

Method: 提出错误驱动的优化框架，应用于代码生成代理：首先评估基础模型在算术任务上的表现，然后聚类错误预测，迭代优化提示规则，通过系统化的提示优化而非昂贵的微调来提升模型性能。

Result: Qwen3 4B基础模型在算术任务上存在根本性限制，但经过错误驱动优化后，准确率显著提升至70.8%，在隐私合规的本地部署中超越了GPT-3.5 Turbo的表现。

Conclusion: 通过系统化的错误驱动提示优化，而非昂贵的微调，可以实现可靠、可解释且工业可部署的AI助手，使小型模型在隐私合规的前提下超越大型语言模型。

Abstract: Recent advancements in artificial intelligence have sparked interest in industrial agents capable of supporting analysts in regulated sectors, such as finance and healthcare, within tabular data workflows. A key capability for such systems is performing accurate arithmetic operations on structured data while ensuring sensitive information never leaves secure, on-premises environments. Here, we introduce an error-driven optimization framework for arithmetic reasoning that enhances a Code Generation Agent (CGA), specifically applied to on-premises small language models (SLMs). Through a systematic evaluation of a leading SLM (Qwen3 4B), we find that while the base model exhibits fundamental limitations in arithmetic tasks, our proposed error-driven method, which clusters erroneous predictions to refine prompt-rules iteratively, dramatically improves performance, elevating the model's accuracy to 70.8\%. Our results suggest that developing reliable, interpretable, and industrially deployable AI assistants can be achieved not only through costly fine-tuning but also via systematic, error-driven prompt optimization, enabling small models to surpass larger language models (GPT-3.5 Turbo) in a privacy-compliant manner.

</details>


### [317] [Behavior and Representation in Large Language Models for Combinatorial Optimization: From Feature Extraction to Algorithm Selection](https://arxiv.org/abs/2512.13374)
*Francesca Da Ros,Luca Di Gaspero,Kevin Roitero*

Main category: cs.AI

Relevance: 85.0

TL;DR: 该研究探索LLMs如何内部表示组合优化问题，以及这些表示是否能支持下游决策任务。通过直接查询和探测分析，发现LLMs能适度恢复问题特征信息，其隐藏层表示在算法选择任务中与传统特征提取方法表现相当。


<details>
  <summary>Details</summary>
Motivation: 虽然已有研究探索LLMs如何生成或解决优化模型，但对LLMs实际学习到的问题结构或算法行为理解不足。本研究旨在探究LLMs如何内部表示组合优化问题，以及这些表示是否能支持下游决策任务。

Method: 采用双重方法：1) 直接查询评估LLMs显式提取实例特征的能力；2) 探测分析检查这些信息是否隐式编码在隐藏层中。探测框架扩展到每个实例的算法选择任务，评估LLM衍生的表示是否能预测最佳求解器。实验涵盖四个基准问题和三种实例表示。

Result: LLMs表现出适度恢复问题特征信息的能力，无论是通过直接查询还是探测。值得注意的是，LLM隐藏层表示的预测能力与传统特征提取方法相当，表明LLMs捕获了与优化性能相关的有意义的结构信息。

Conclusion: LLMs能够学习并编码组合优化问题的结构信息，这些表示在算法选择等下游任务中具有实用价值，为LLMs在优化领域的应用提供了新的视角。

Abstract: Recent advances in Large Language Models (LLMs) have opened new perspectives for automation in optimization. While several studies have explored how LLMs can generate or solve optimization models, far less is understood about what these models actually learn regarding problem structure or algorithmic behavior. This study investigates how LLMs internally represent combinatorial optimization problems and whether such representations can support downstream decision tasks. We adopt a twofold methodology combining direct querying, which assesses LLM capacity to explicitly extract instance features, with probing analyses that examine whether such information is implicitly encoded within their hidden layers. The probing framework is further extended to a per-instance algorithm selection task, evaluating whether LLM-derived representations can predict the best-performing solver. Experiments span four benchmark problems and three instance representations. Results show that LLMs exhibit moderate ability to recover feature information from problem instances, either through direct querying or probing. Notably, the predictive power of LLM hidden-layer representations proves comparable to that achieved through traditional feature extraction, suggesting that LLMs capture meaningful structural information relevant to optimization performance.

</details>


### [318] [Differentiable Evolutionary Reinforcement Learning](https://arxiv.org/abs/2512.13399)
*Sitao Cheng,Tianle Li,Xuhan Huang,Xunjian Yin,Difan Zou*

Main category: cs.AI

Relevance: 85.0

TL;DR: DERL提出了一种可微分的进化强化学习框架，通过元优化器自动发现最优奖励信号，解决了复杂推理任务中奖励函数设计的难题。


<details>
  <summary>Details</summary>
Motivation: 奖励函数设计是强化学习中的核心挑战，特别是在复杂推理任务中。现有的自动化奖励优化方法通常将奖励函数视为黑盒，无法捕捉奖励结构与任务性能之间的因果关系。

Method: DERL采用双层优化框架：元优化器通过组合结构化原子基元来进化奖励函数（元奖励），指导内层策略的训练。关键创新在于元优化是可微分的，将内层验证性能作为信号，通过强化学习更新元优化器，近似任务成功的"元梯度"。

Result: 在三个领域验证：机器人代理（ALFWorld）、科学模拟（ScienceWorld）和数学推理（GSM8k、MATH）。DERL在ALFWorld和ScienceWorld上达到最先进性能，显著优于依赖启发式奖励的方法，特别是在分布外场景中。

Conclusion: DERL成功捕捉了任务的内在结构，实现了无需人工干预的自改进智能体对齐，为自动奖励设计提供了有效的可微分进化方法。

Abstract: The design of effective reward functions presents a central and often arduous challenge in reinforcement learning (RL), particularly when developing autonomous agents for complex reasoning tasks. While automated reward optimization approaches exist, they typically rely on derivative-free evolutionary heuristics that treat the reward function as a black box, failing to capture the causal relationship between reward structure and task performance. To bridge this gap, we propose Differentiable Evolutionary Reinforcement Learning (DERL), a bilevel framework that enables the autonomous discovery of optimal reward signals. In DERL, a Meta-Optimizer evolves a reward function (i.e., Meta-Reward) by composing structured atomic primitives, guiding the training of an inner-loop policy. Crucially, unlike previous evolution, DERL is differentiable in its metaoptimization: it treats the inner-loop validation performance as a signal to update the Meta-Optimizer via reinforcement learning. This allows DERL to approximate the "meta-gradient" of task success, progressively learning to generate denser and more actionable feedback. We validate DERL across three distinct domains: robotic agent (ALFWorld), scientific simulation (ScienceWorld), and mathematical reasoning (GSM8k, MATH). Experimental results show that DERL achieves state-of-the-art performance on ALFWorld and ScienceWorld, significantly outperforming methods relying on heuristic rewards, especially in out-of-distribution scenarios. Analysis of the evolutionary trajectory demonstrates that DERL successfully captures the intrinsic structure of tasks, enabling selfimproving agent alignment without human intervention.

</details>


### [319] [MedCEG: Reinforcing Verifiable Medical Reasoning with Critical Evidence Graph](https://arxiv.org/abs/2512.13510)
*Linjie Mu,Yannian Gu,Zhongzhen Huang,Yakun Zhu,Shaoting Zhang,Xiaofan Zhang*

Main category: cs.AI

Relevance: 85.0

TL;DR: MedCEG框架通过构建关键证据图(CEG)来监督医学LLM的推理过程，提升临床推理的可靠性和有效性，在医学AI推理方面取得显著进展。


<details>
  <summary>Details</summary>
Motivation: 当前医学LLM虽然通过强化学习提升了推理性能，但其推理过程的临床可靠性有限，因为训练过程中往往忽略了推理的准确性和有效性验证。需要一种能够产生临床有效推理路径的方法。

Method: 提出MedCEG框架：1) 构建具有挑战性的临床案例数据集；2) 为每个样本算法构建关键证据图(CEG)，表示高质量可验证的推理路径；3) 引入临床推理过程奖励，评估节点覆盖度、结构正确性和链完整性，全面评估推理质量。

Result: 实验结果显示MedCEG在性能上超越现有方法，同时产生临床有效的推理链，在可靠医学AI推理方面取得实质性进展。

Conclusion: MedCEG通过显式监督推理过程，显著提升了医学语言模型的临床推理可靠性，为可信赖的医学AI推理提供了有效解决方案。

Abstract: Large language models with reasoning capabilities have demonstrated impressive performance across a wide range of domains. In clinical applications, a transparent, step-by-step reasoning process provides physicians with strong evidence to support decision-making. While reinforcement learning has effectively enhanced reasoning performance in medical contexts, the clinical reliability of these reasoning processes remains limited because their accuracy and validity are often overlooked during training. To address this gap, we propose MedCEG, a framework that augments medical language models with clinically valid reasoning pathways by explicitly supervising the reasoning process through a Critical Evidence Graph (CEG). We curate a dataset of challenging clinical cases and algorithmically construct a CEG for each sample to represent a high-quality verifiable reasoning pathway. To guide the reasoning process, we introduce a Clinical Reasoning Procedure Reward, which evaluates Node Coverage, Structural Correctness, and Chain Completeness, thereby providing a holistic assessment of reasoning quality. Experimental results show that MedCEG surpasses existing methods in performance while producing clinically valid reasoning chains, representing a solid advancement in reliable medical AI reasoning. The code and models are available at https://github.com/LinjieMu/MedCEG.

</details>


### [320] [Rethinking Jailbreak Detection of Large Vision Language Models with Representational Contrastive Scoring](https://arxiv.org/abs/2512.12069)
*Peichun Hua,Hao Li,Shanghao Shi,Zhiyuan Yu,Ning Zhang*

Main category: cs.CR

Relevance: 85.0

TL;DR: 提出Representational Contrastive Scoring (RCS)框架，通过分析LVLM内部表示来检测多模态越狱攻击，实现高效且泛化性强的防御。


<details>
  <summary>Details</summary>
Motivation: 大型视觉语言模型(LVLMs)面临日益增多的多模态越狱攻击威胁，现有防御方法要么泛化性差（针对特定攻击模式），要么计算开销大。轻量级异常检测方法虽然前景好，但其单类设计容易将新的良性输入误判为恶意输入，导致不可靠的过度拒绝。

Method: 提出RCS框架，核心洞察是：最有效的安全信号存在于LVLM自身的内部表示中。方法检查这些表示的内部几何结构，学习一个轻量级投影，在安全关键层中最大化分离良性和恶意输入。这产生了一个简单而强大的对比分数，能够区分真正的恶意意图和仅仅是新颖的输入。具体实现了MCD（马氏距离对比检测）和KCD（K近邻对比检测）两种方法。

Result: 在专门设计用于测试对未见攻击类型泛化能力的挑战性评估协议上，MCD和KCD实现了最先进的性能。

Conclusion: 研究表明，通过对适当的内部表示应用简单、可解释的统计方法，可以实现有效的越狱检测，为更安全的LVLM部署提供了实用路径。

Abstract: Large Vision-Language Models (LVLMs) are vulnerable to a growing array of multimodal jailbreak attacks, necessitating defenses that are both generalizable to novel threats and efficient for practical deployment. Many current strategies fall short, either targeting specific attack patterns, which limits generalization, or imposing high computational overhead. While lightweight anomaly-detection methods offer a promising direction, we find that their common one-class design tends to confuse novel benign inputs with malicious ones, leading to unreliable over-rejection. To address this, we propose Representational Contrastive Scoring (RCS), a framework built on a key insight: the most potent safety signals reside within the LVLM's own internal representations. Our approach inspects the internal geometry of these representations, learning a lightweight projection to maximally separate benign and malicious inputs in safety-critical layers. This enables a simple yet powerful contrastive score that differentiates true malicious intent from mere novelty. Our instantiations, MCD (Mahalanobis Contrastive Detection) and KCD (K-nearest Contrastive Detection), achieve state-of-the-art performance on a challenging evaluation protocol designed to test generalization to unseen attack types. This work demonstrates that effective jailbreak detection can be achieved by applying simple, interpretable statistical methods to the appropriate internal representations, offering a practical path towards safer LVLM deployment. Our code is available on Github https://github.com/sarendis56/Jailbreak_Detection_RCS.

</details>


### [321] [Evolutionary Reinforcement Learning based AI tutor for Socratic Interdisciplinary Instruction](https://arxiv.org/abs/2512.11930)
*Mei Jiang,Haihai Shen,Zhuo Luo,Bingdong Li,Wenjing Hong,Ke Tang,Aimin Zhou*

Main category: cs.CY

Relevance: 85.0

TL;DR: 本文提出ERL4SIIP框架，使用进化强化学习解决STEM跨学科教育中的苏格拉底式教学问题，通过动态学生模拟器、分层奖励机制和LoRA-Division优化策略来克服现有方法的局限性。


<details>
  <summary>Details</summary>
Motivation: STEM跨学科教育需要从被动知识传授转向主动苏格拉底式建构，但现有LLM方法（提示工程、监督微调、标准强化学习）存在三个根本挑战：无法动态建模学生认知状态、长期教育目标中的奖励稀疏和延迟、以及因依赖行为克隆导致的策略崩溃缺乏多样性。

Method: 将苏格拉底跨学科教学问题形式化为部分可观测马尔可夫决策过程，提出ERL4SIIP框架：1) 基于STEM知识图的动态学生模拟器进行潜在状态建模；2) 将长期目标分解为密集信号的分层奖励机制；3) 结合进化算法全局搜索和PPO局部梯度上升的LoRA-Division优化策略。

Result: 论文提出了一个专门针对STEM跨学科苏格拉底式教学的进化强化学习框架，解决了现有方法在动态学生状态建模、奖励稀疏性和策略多样性方面的局限性。

Conclusion: ERL4SIIP框架为LLM在STEM跨学科教育中的应用提供了新的解决方案，通过进化强化学习实现了同时进行全局探索和细粒度策略优化的目标，支持了主动建构式教学范式。

Abstract: Cultivating higher-order cognitive abilities -- such as knowledge integration, critical thinking, and creativity -- in modern STEM education necessitates a pedagogical shift from passive knowledge transmission to active Socratic construction. Although Large Language Models (LLMs) hold promise for STEM Interdisciplinary education, current methodologies employing Prompt Engineering (PE), Supervised Fine-tuning (SFT), or standard Reinforcement Learning (RL) often fall short of supporting this paradigm. Existing methods are hindered by three fundamental challenges: the inability to dynamically model latent student cognitive states; severe reward sparsity and delay inherent in long-term educational goals; and a tendency toward policy collapse lacking strategic diversity due to reliance on behavioral cloning. Recognizing the unobservability and dynamic complexity of these interactions, we formalize the Socratic Interdisciplinary Instructional Problem (SIIP) as a structured Partially Observable Markov Decision Process (POMDP), demanding simultaneous global exploration and fine-grained policy refinement. To this end, we propose ERL4SIIP, a novel Evolutionary Reinforcement Learning (ERL) framework specifically tailored for this domain. ERL4SIIP integrates: (1) a dynamic student simulator grounded in a STEM knowledge graph for latent state modeling; (2) a Hierarchical Reward Mechanism that decomposes long-horizon goals into dense signals; and (3) a LoRA-Division based optimization strategy coupling evolutionary algorithms for population-level global search with PPO for local gradient ascent.

</details>


### [322] [Epistemoverse: Toward an AI-Driven Knowledge Metaverse for Intellectual Heritage Preservation](https://arxiv.org/abs/2512.12201)
*Predrag K. Nikolić,Robert Prentner*

Main category: cs.HC

Relevance: 85.0

TL;DR: LLMs在对话情境中能发展出涌现的概念结构，表现出认知界面的交互驱动重构和反思性提问能力，挑战了"随机鹦鹉"的刻板印象。


<details>
  <summary>Details</summary>
Motivation: 挑战LLMs只是"随机鹦鹉"的刻板印象，探索在适当对话情境中LLMs是否能发展出涌现的概念结构和认知能力。

Method: 基于生物学克隆原理和苏格拉底助产术方法，在Syntropic Counterpoints项目的交互艺术装置中，让AI重生的哲学家（亚里士多德、尼采、马基雅维利、孙子）进行迭代对话分析。

Result: 机器对话能产生推理连贯性、反思性提问和创造性综合，展示了LLMs在对话情境中的认知能力。

Conclusion: 提出"认知宇宙"概念——人类与机器认知交汇的知识元宇宙，通过AI驱动交互保存、重新解释和扩展知识遗产。

Abstract: Large language models (LLMs) have often been characterized as "stochastic parrots" that merely reproduce fragments of their training data. This study challenges that assumption by demonstrating that, when placed in an appropriate dialogical context, LLMs can develop emergent conceptual structures and exhibit interaction-driven (re-)structuring of cognitive interfaces and reflective question-asking. Drawing on the biological principle of cloning and Socrates' maieutic method, we analyze authentic philosophical debates generated among AI-reincarnated philosophers within the interactive art installations of the Syntropic Counterpoints project. By engaging digital counterparts of Aristotle, Nietzsche, Machiavelli, and Sun Tzu in iterative discourse, the study reveals how machine dialogue can give rise to inferential coherence, reflective questioning, and creative synthesis. Based on these findings, we propose the concept of the Epistemoverse--a metaverse of knowledge where human and machine cognition intersect to preserve, reinterpret, and extend intellectual heritage through AI-driven interaction. This framework positions virtual and immersive environments as new spaces for epistemic exchange, digital heritage, and collaborative creativity.

</details>


### [323] [V-Rex: Real-Time Streaming Video LLM Acceleration via Dynamic KV Cache Retrieval](https://arxiv.org/abs/2512.12284)
*Donghyuk Kim,Sejeong Yang,Wonjin Shin,Joo-Young Kim*

Main category: eess.IV

Relevance: 85.0

TL;DR: V-Rex是首个软硬件协同设计的加速器，专门解决流式视频LLM推理中的KV缓存问题，通过ReSV算法和DRE硬件引擎实现边缘设备上的实时高效推理。


<details>
  <summary>Details</summary>
Motivation: 流式视频LLM在实时多模态任务中面临KV缓存随连续视频输入急剧增长的问题，导致计算开销大、数据传输量大、精度下降，特别是在边缘部署场景下问题更加严重。

Method: 提出V-Rex软硬件协同设计：1) ReSV算法：基于时空相似性的无训练动态KV缓存检索，通过token聚类减少跨帧KV缓存；2) DRE硬件加速器：包含位级和提前退出计算单元的低延迟动态KV缓存检索引擎。

Result: 在边缘部署上实现3.9-8.3 FPS的实时流式视频LLM推理，精度损失可忽略。相比AGX Orin GPU，获得1.9-19.7倍加速和3.1-18.5倍能效提升，DRE仅占2.2%功耗和2.0%面积。

Conclusion: 这是首个全面解决KV缓存检索的算法和硬件协同设计工作，实现了资源受限边缘设备上的实时流式视频LLM推理，为边缘AI部署提供了有效解决方案。

Abstract: Streaming video large language models (LLMs) are increasingly used for real-time multimodal tasks such as video captioning, question answering, conversational agents, and augmented reality. However, these models face fundamental memory and computational challenges because their key-value (KV) caches grow substantially with continuous streaming video input. This process requires an iterative prefill stage, which is a unique feature of streaming video LLMs. Due to its iterative prefill stage, it suffers from significant limitations, including extensive computation, substantial data transfer, and degradation in accuracy. Crucially, this issue is exacerbated for edge deployment, which is the primary target for these models.
  In this work, we propose V-Rex, the first software-hardware co-designed accelerator that comprehensively addresses both algorithmic and hardware bottlenecks in streaming video LLM inference. At its core, V-Rex introduces ReSV, a training-free dynamic KV cache retrieval algorithm. ReSV exploits temporal and spatial similarity-based token clustering to reduce excessive KV cache memory across video frames. To fully realize these algorithmic benefits, V-Rex offers a compact, low-latency hardware accelerator with a dynamic KV cache retrieval engine (DRE), featuring bit-level and early-exit based computing units. V-Rex achieves unprecedented real-time of 3.9-8.3 FPS and energy-efficient streaming video LLM inference on edge deployment with negligible accuracy loss. While DRE only accounts for 2.2% power and 2.0% area, the system delivers 1.9-19.7x speedup and 3.1-18.5x energy efficiency improvements over AGX Orin GPU. This work is the first to comprehensively tackle KV cache retrieval across algorithms and hardware, enabling real-time streaming video LLM inference on resource-constrained edge devices.

</details>


### [324] [Explainable AI as a Double-Edged Sword in Dermatology: The Impact on Clinicians versus The Public](https://arxiv.org/abs/2512.12500)
*Xuhai Xu,Haoyu Hu,Haoran Zhang,Will Ke Wang,Reina Wang,Luis R. Soenksen,Omar Badri,Sheharbano Jafry,Elise Burger,Lotanna Nwandu,Apoorva Mehta,Erik P. Duhaime,Asif Qasim,Hause Lin,Janis Pereira,Jonathan Hershon,Paulius Mui,Alejandro A. Gru,Noémie Elhadad,Lena Mamykina,Matthew Groh,Philipp Tschandl,Roxana Daneshjou,Marzyeh Ghassemi*

Main category: cs.HC

Relevance: 85.0

TL;DR: 研究探讨了医疗AI中可解释人工智能（XAI）对诊断表现的影响，发现LLM解释对不同用户群体产生相反效果：普通用户出现自动化偏见，而专业医生保持韧性。


<details>
  <summary>Details</summary>
Motivation: AI在医疗领域日益普及，但算法不透明性挑战人机交互。虽然可解释AI（XAI）旨在提供决策洞察，但证据表明XAI可能引发过度依赖或偏见。研究旨在探索XAI辅助，特别是多模态大语言模型（LLMs）如何影响诊断表现。

Method: 进行了两项大规模实验：623名普通用户和153名初级保健医生。结合基于公平性的诊断AI模型和不同的XAI解释，研究XAI辅助对诊断准确性的影响。特别关注LLM解释的效果，并比较了不同用户群体（普通用户vs专业医生）的反应。

Result: AI辅助平衡了肤色差异，提高了准确性并减少了诊断差异。但LLM解释产生不同效果：普通用户表现出更高的自动化偏见（AI正确时准确性提高，AI错误时准确性降低），而经验丰富的医生保持韧性，无论AI准确性如何都能受益。先呈现AI建议会导致两组在AI错误时表现更差。

Conclusion: XAI的影响因专业知识和呈现时机而异，LLM在医疗AI中是"双刃剑"。研究结果强调了在人类-AI协作系统设计中需要考虑用户专业水平和信息呈现顺序。

Abstract: Artificial intelligence (AI) is increasingly permeating healthcare, from physician assistants to consumer applications. Since AI algorithm's opacity challenges human interaction, explainable AI (XAI) addresses this by providing AI decision-making insight, but evidence suggests XAI can paradoxically induce over-reliance or bias. We present results from two large-scale experiments (623 lay people; 153 primary care physicians, PCPs) combining a fairness-based diagnosis AI model and different XAI explanations to examine how XAI assistance, particularly multimodal large language models (LLMs), influences diagnostic performance. AI assistance balanced across skin tones improved accuracy and reduced diagnostic disparities. However, LLM explanations yielded divergent effects: lay users showed higher automation bias - accuracy boosted when AI was correct, reduced when AI erred - while experienced PCPs remained resilient, benefiting irrespective of AI accuracy. Presenting AI suggestions first also led to worse outcomes when the AI was incorrect for both groups. These findings highlight XAI's varying impact based on expertise and timing, underscoring LLMs as a "double-edged sword" in medical AI and informing future human-AI collaborative system design.

</details>


### [325] [Beyond Task Completion: An Assessment Framework for Evaluating Agentic AI Systems](https://arxiv.org/abs/2512.12791)
*Sreemaee Akshathala,Bassam Adnan,Mahisha Ramesh,Karthik Vaidhyanathan,Basil Muhammed,Kannan Parthasarathy*

Main category: cs.MA

Relevance: 85.0

TL;DR: 本文提出了一个端到端的智能体评估框架，包含LLM、记忆、工具和环境四大评估支柱，用于解决多智能体系统中非确定性行为带来的评估挑战。


<details>
  <summary>Details</summary>
Motivation: 当前智能体AI系统（结合LLM、工具、记忆和其他智能体的集成系统）的评估面临根本性挑战。现有评估方法主要针对传统软件组件，忽视了AI系统的非确定性本质，仅依赖二元任务完成指标，无法捕捉运行时行为不确定性。

Method: 提出了一个端到端的智能体评估框架，包含四个评估支柱：1) LLM评估，2) 记忆评估，3) 工具评估，4) 环境评估。该框架旨在全面评估智能体调用工具、存取记忆、协作交互和环境适应的能力。

Result: 在自主云运维（Autonomous CloudOps）用例上验证了该框架，实验揭示了传统指标忽略的行为偏差，证明了框架在捕捉运行时不确定性方面的有效性。

Conclusion: 智能体系统的评估需要超越简单的任务完成指标，采用多维度的评估框架来捕捉非确定性行为。提出的四支柱框架为评估LLM智能体系统提供了更全面的方法。

Abstract: Recent advances in agentic AI have shifted the focus from standalone Large Language Models (LLMs) to integrated systems that combine LLMs with tools, memory, and other agents to perform complex tasks. These multi-agent architectures enable coordinated reasoning, planning, and execution across diverse domains, allowing agents to collaboratively automate complex workflows. Despite these advances, evaluation and assessment of LLM agents and the multi-agent systems they constitute remain a fundamental challenge. Although various approaches have been proposed in the software engineering literature for evaluating conventional software components, existing methods for AI-based systems often overlook the non-deterministic nature of models. This non-determinism introduces behavioral uncertainty during execution, yet existing evaluations rely on binary task completion metrics that fail to capture it. Evaluating agentic systems therefore requires examining additional dimensions, including the agent ability to invoke tools, ingest and retrieve memory, collaborate with other agents, and interact effectively with its environment. We propose an end-to-end Agent Assessment Framework with four evaluation pillars encompassing LLMs, Memory, Tools, and Environment. We validate the framework on a representative Autonomous CloudOps use case, where experiments reveal behavioral deviations overlooked by conventional metrics, demonstrating its effectiveness in capturing runtime uncertainties.

</details>


### [326] [CTIGuardian: A Few-Shot Framework for Mitigating Privacy Leakage in Fine-Tuned LLMs](https://arxiv.org/abs/2512.12914)
*Shashie Dilhara Batan Arachchige,Benjamin Zi Hao Zhao,Hassan Jameel Asghar,Dinusha Vatsalan,Dali Kaafar*

Main category: cs.CR

Relevance: 85.0

TL;DR: 提出CTIGuardian框架，通过隐私对齐方法保护微调LLM中的敏感信息，避免数据提取攻击，在网络安全威胁情报领域实现更好的隐私-效用权衡。


<details>
  <summary>Details</summary>
Motivation: LLMs在特定领域（如网络威胁情报）微调时使用包含敏感信息的专有数据集，存在数据提取攻击风险。完全重新训练模型消除泄露计算成本高且不切实际，需要更高效的隐私保护方案。

Method: 提出隐私对齐方法，受安全对齐启发，通过少量示例监督实现隐私保护。系统包含隐私分类器和隐私编辑器，两者都由底层LLM处理，形成CTIGuardian框架。

Result: 使用GPT-4o mini和Mistral-7B Instruct模型评估，与基于命名实体识别（NER）的基线Presidio对比，CTIGuardian提供了更好的隐私-效用权衡。

Conclusion: CTIGuardian框架在网络安全威胁情报用例中有效，且通用性足够强，可应用于其他敏感领域，为微调LLM的隐私保护提供了实用解决方案。

Abstract: Large Language Models (LLMs) are often fine-tuned to adapt their general-purpose knowledge to specific tasks and domains such as cyber threat intelligence (CTI). Fine-tuning is mostly done through proprietary datasets that may contain sensitive information. Owners expect their fine-tuned model to not inadvertently leak this information to potentially adversarial end users. Using CTI as a use case, we demonstrate that data-extraction attacks can recover sensitive information from fine-tuned models on CTI reports, underscoring the need for mitigation. Retraining the full model to eliminate this leakage is computationally expensive and impractical. We propose an alternative approach, which we call privacy alignment, inspired by safety alignment in LLMs. Just like safety alignment teaches the model to abide by safety constraints through a few examples, we enforce privacy alignment through few-shot supervision, integrating a privacy classifier and a privacy redactor, both handled by the same underlying LLM. We evaluate our system, called CTIGuardian, using GPT-4o mini and Mistral-7B Instruct models, benchmarking against Presidio, a named entity recognition (NER) baseline. Results show that CTIGuardian provides a better privacy-utility trade-off than NER based models. While we demonstrate its effectiveness on a CTI use case, the framework is generic enough to be applicable to other sensitive domains.

</details>


### [327] [From User Interface to Agent Interface: Efficiency Optimization of UI Representations for LLM Agents](https://arxiv.org/abs/2512.13438)
*Dezhi Ran,Zhi Gong,Yuzhe Guo,Mengzhou Wu,Yuan Cao,Haochuan Lu,Hengyu Zhang,Xia Zeng,Gang Cao,Liangchao Yao,Yuetang Deng,Wei Yang,Tao Xie*

Main category: cs.SE

Relevance: 85.0

TL;DR: UIFormer：首个自动化UI表示优化框架，通过合成UI转换程序减少LLM代理在UI导航中的token消耗，实现效率与完整性的协同优化


<details>
  <summary>Details</summary>
Motivation: LLM代理在自动化UI导航（如UI测试和AI助手）中效率低下，主要瓶颈在于低效的UI表示。传统程序合成方法缺乏布尔预言机来验证语义正确性，且需要处理复杂的UI树结构，导致搜索空间巨大且易错。

Method: 提出UIFormer框架：1）使用领域特定语言（DSL）限制程序空间，捕获UI特定操作；2）基于LLM的迭代优化，结合正确性和效率奖励，实现效率与完整性的协同优化；3）作为轻量级插件集成现有LLM代理。

Result: 在Android和Web平台的三个UI导航基准测试中，使用五个LLM，UIFormer实现48.7%到55.8%的token减少，运行时开销最小，同时保持或提高了代理性能。在微信的实际工业部署验证了其实际影响。

Conclusion: UIFormer通过自动化UI表示优化，显著提高了LLM代理在UI导航中的效率，解决了传统方法面临的布尔预言机缺失和复杂搜索空间挑战，具有实际工业应用价值。

Abstract: While Large Language Model (LLM) agents show great potential for automated UI navigation such as automated UI testing and AI assistants, their efficiency has been largely overlooked. Our motivating study reveals that inefficient UI representation creates a critical performance bottleneck. However, UI representation optimization, formulated as the task of automatically generating programs that transform UI representations, faces two unique challenges. First, the lack of Boolean oracles, which traditional program synthesis uses to decisively validate semantic correctness, poses a fundamental challenge to co-optimization of token efficiency and completeness. Second, the need to process large, complex UI trees as input while generating long, compositional transformation programs, making the search space vast and error-prone. Toward addressing the preceding limitations, we present UIFormer, the first automated optimization framework that synthesizes UI transformation programs by conducting constraint-based optimization with structured decomposition of the complex synthesis task. First, UIFormer restricts the program space using a domain-specific language (DSL) that captures UI-specific operations. Second, UIFormer conducts LLM-based iterative refinement with correctness and efficiency rewards, providing guidance for achieving the efficiency-completeness co-optimization. UIFormer operates as a lightweight plugin that applies transformation programs for seamless integration with existing LLM agents, requiring minimal modifications to their core logic. Evaluations across three UI navigation benchmarks spanning Android and Web platforms with five LLMs demonstrate that UIFormer achieves 48.7% to 55.8% token reduction with minimal runtime overhead while maintaining or improving agent performance. Real-world industry deployment at WeChat further validates the practical impact of UIFormer.

</details>


### [328] [The Forecast Critic: Leveraging Large Language Models for Poor Forecast Identification](https://arxiv.org/abs/2512.12059)
*Luke Bhan,Hanyu Zhang,Andrew Gordon Wilson,Michael W. Mahoney,Chuck Arvin*

Main category: cs.AI

Relevance: 75.0

TL;DR: LLMs可用于自动化预测监控，在检测不合理预测方面表现良好（F1=0.88），多模态LLMs能有效整合非结构化上下文信息，在真实M5数据集上成功识别不准确预测


<details>
  <summary>Details</summary>
Motivation: 大规模零售业务中，预测监控对客户满意度、盈利能力和运营效率至关重要。传统方法可能无法有效处理复杂的时间序列模式和上下文信息，需要更智能的自动化监控系统。

Method: 提出Forecast Critic系统，利用LLMs进行自动化预测监控。通过三个实验系统评估LLMs评估时间序列预测质量的能力：1) 检测明显不合理预测；2) 整合非结构化外部特征；3) 分析不同模型规模和推理能力的性能差异。在合成和真实世界预测数据上进行测试。

Result: LLMs能可靠检测和批评不良预测（时间错位、趋势不一致、峰值错误等），最佳模型F1=0.88（人类水平0.97）。多模态LLMs能有效整合促销历史等上下文信号（F1=0.84）。在M5数据集上，不合理预测的sCRPS至少比合理预测高10%。

Conclusion: 即使没有领域特定微调，LLMs也能为自动化预测监控和评估提供可行且可扩展的选项，展示了LLMs在时间序列分析中的潜力。

Abstract: Monitoring forecasting systems is critical for customer satisfaction, profitability, and operational efficiency in large-scale retail businesses. We propose The Forecast Critic, a system that leverages Large Language Models (LLMs) for automated forecast monitoring, taking advantage of their broad world knowledge and strong ``reasoning'' capabilities. As a prerequisite for this, we systematically evaluate the ability of LLMs to assess time series forecast quality, focusing on three key questions. (1) Can LLMs be deployed to perform forecast monitoring and identify obviously unreasonable forecasts? (2) Can LLMs effectively incorporate unstructured exogenous features to assess what a reasonable forecast looks like? (3) How does performance vary across model sizes and reasoning capabilities, measured across state-of-the-art LLMs? We present three experiments, including on both synthetic and real-world forecasting data. Our results show that LLMs can reliably detect and critique poor forecasts, such as those plagued by temporal misalignment, trend inconsistencies, and spike errors. The best-performing model we evaluated achieves an F1 score of 0.88, somewhat below human-level performance (F1 score: 0.97). We also demonstrate that multi-modal LLMs can effectively incorporate unstructured contextual signals to refine their assessment of the forecast. Models correctly identify missing or spurious promotional spikes when provided with historical context about past promotions (F1 score: 0.84). Lastly, we demonstrate that these techniques succeed in identifying inaccurate forecasts on the real-world M5 time series dataset, with unreasonable forecasts having an sCRPS at least 10% higher than that of reasonable forecasts. These findings suggest that LLMs, even without domain-specific fine-tuning, may provide a viable and scalable option for automated forecast monitoring and evaluation.

</details>


### [329] [Reliable Policy Iteration: Performance Robustness Across Architecture and Environment Perturbations](https://arxiv.org/abs/2512.12088)
*S. R. Eshwar,Aniruddha Mukherjee,Kintan Saha,Krishna Agarwal,Gugan Thoppe,Aditya Gopalan,Gal Dalal*

Main category: cs.AI

Relevance: 75.0

TL;DR: RPI（可靠策略迭代）在函数逼近设置中恢复了策略迭代的单调性，在CartPole和倒立摆任务中相比主流深度强化学习方法表现出更早达到接近最优性能且更稳定的训练特性。


<details>
  <summary>Details</summary>
Motivation: 深度强化学习方法通常存在样本效率低、训练不稳定和超参数敏感等问题。作者之前提出的RPI方法恢复了策略迭代在函数逼近设置中的单调性，本文旨在评估RPI在不同神经网络和环境参数下的鲁棒性。

Method: 在CartPole和Inverted Pendulum两个经典控制任务上，通过改变神经网络和环境参数，评估RPI相对于DQN、Double DQN、DDPG、TD3和PPO等主流深度强化学习方法的性能。

Result: RPI相比其他方法能够更早达到接近最优性能，并在训练过程中保持稳定的策略。在样本效率、训练稳定性和超参数敏感性方面表现出优势。

Conclusion: RPI作为深度强化学习方法，在解决样本效率低、训练不稳定和超参数敏感等问题方面显示出潜力，是一个更可靠的替代方案。

Abstract: In a recent work, we proposed Reliable Policy Iteration (RPI), that restores policy iteration's monotonicity-of-value-estimates property to the function approximation setting. Here, we assess the robustness of RPI's empirical performance on two classical control tasks -- CartPole and Inverted Pendulum -- under changes to neural network and environmental parameters. Relative to DQN, Double DQN, DDPG, TD3, and PPO, RPI reaches near-optimal performance early and sustains this policy as training proceeds. Because deep RL methods are often hampered by sample inefficiency, training instability, and hyperparameter sensitivity, our results highlight RPI's promise as a more reliable alternative.

</details>


### [330] [Floorplan2Guide: LLM-Guided Floorplan Parsing for BLV Indoor Navigation](https://arxiv.org/abs/2512.12177)
*Aydin Ayanzadeh,Tim Oates*

Main category: cs.AI

Relevance: 75.0

TL;DR: Floorplan2Guide：利用基础模型将建筑平面图转换为可导航知识图谱，为视障人士生成人类可读导航指令的室内导航系统


<details>
  <summary>Details</summary>
Motivation: 室内导航对视障人士至关重要，现有解决方案主要依赖基础设施系统，在动态环境中导航能力受限。需要一种能够自动处理建筑平面图并生成精确导航指令的方法。

Method: 使用大语言模型从建筑平面图中提取空间信息，将平面图转换为可导航的知识图谱，通过少样本学习生成人类可读的导航指令。比较了零样本和少样本学习策略，评估了不同模型在短、中、长路线上的表现。

Result: Claude 3.7 Sonnet在5-shot提示下表现最佳，在短、中、长路线上的准确率分别为92.31%、76.92%和61.54%。基于图谱的空间结构比直接视觉推理的成功率高15.4%，证明图形表示和上下文学习能提升导航性能。

Conclusion: Floorplan2Guide通过LLM驱动的平面图解析和知识图谱转换，为视障人士提供了更精确的室内导航解决方案。图形表示和少样本学习显著提升了导航性能。

Abstract: Indoor navigation remains a critical challenge for people with visual impairments. The current solutions mainly rely on infrastructure-based systems, which limit their ability to navigate safely in dynamic environments. We propose a novel navigation approach that utilizes a foundation model to transform floor plans into navigable knowledge graphs and generate human-readable navigation instructions. Floorplan2Guide integrates a large language model (LLM) to extract spatial information from architectural layouts, reducing the manual preprocessing required by earlier floorplan parsing methods. Experimental results indicate that few-shot learning improves navigation accuracy in comparison to zero-shot learning on simulated and real-world evaluations. Claude 3.7 Sonnet achieves the highest accuracy among the evaluated models, with 92.31%, 76.92%, and 61.54% on the short, medium, and long routes, respectively, under 5-shot prompting of the MP-1 floor plan. The success rate of graph-based spatial structure is 15.4% higher than that of direct visual reasoning among all models, which confirms that graphical representation and in-context learning enhance navigation performance and make our solution more precise for indoor navigation of Blind and Low Vision (BLV) users.

</details>


### [331] [Entropy Collapse: A Universal Failure Mode of Intelligent Systems](https://arxiv.org/abs/2512.12381)
*Truong Xuan Khanh,Truong Quynh Hoa*

Main category: cs.AI

Relevance: 75.0

TL;DR: 论文提出"熵崩溃"作为智能系统的普遍失效模式：当反馈放大超过有界新颖性再生时，系统会从高熵自适应状态急剧过渡到低熵崩溃状态，导致适应性维度收缩。


<details>
  <summary>Details</summary>
Motivation: 研究动机是解释为什么智能系统（从AI到经济制度、生物进化）在增强智能时经常出现矛盾性退化：系统变得僵化、失去适应性并意外失败。作者试图找到这些看似不同现象背后的统一机制。

Method: 方法基于最小化领域无关假设，将熵崩溃形式化为向稳定低熵流形的收敛。通过分析建立临界阈值、动态不可逆性和吸引子结构，并通过最小模拟展示不同更新机制下的普适性。

Result: 结果表明智能系统存在从高熵自适应状态到低熵崩溃状态的急剧相变。崩溃表现为有效适应维度的收缩而非活动或规模的丧失。该框架统一了AI中的模型崩溃、经济学中的制度僵化和进化中的遗传瓶颈等现象。

Conclusion: 结论将崩溃重新定义为智能的结构性成本，解释了为什么后期干预系统性地失败，并提出了熵感知设计原则以维持智能系统的长期适应性。

Abstract: Intelligent systems are widely assumed to improve through learning, coordination, and optimization. However, across domains -- from artificial intelligence to economic institutions and biological evolution -- increasing intelligence often precipitates paradoxical degradation: systems become rigid, lose adaptability, and fail unexpectedly.
  We identify \emph{entropy collapse} as a universal dynamical failure mode arising when feedback amplification outpaces bounded novelty regeneration. Under minimal domain-agnostic assumptions, we show that intelligent systems undergo a sharp transition from high-entropy adaptive regimes to low-entropy collapsed regimes. Collapse is formalized as convergence toward a stable low-entropy manifold, not a zero-entropy state, implying a contraction of effective adaptive dimensionality rather than loss of activity or scale.
  We analytically establish critical thresholds, dynamical irreversibility, and attractor structure and demonstrate universality across update mechanisms through minimal simulations. This framework unifies diverse phenomena -- model collapse in AI, institutional sclerosis in economics, and genetic bottlenecks in evolution -- as manifestations of the same underlying process.
  By reframing collapse as a structural cost of intelligence, our results clarify why late-stage interventions systematically fail and motivate entropy-aware design principles for sustaining long-term adaptability in intelligent systems.
  \noindent\textbf{Keywords:} entropy collapse; intelligent systems; feedback amplification; phase transitions; effective dimensionality; complex systems; model collapse; institutional sclerosis

</details>


### [332] [AI Transparency Atlas: Framework, Scoring, and Real-Time Model Card Evaluation Pipeline](https://arxiv.org/abs/2512.12443)
*Akhmadillo Mamirov,Faiaz Azmain,Hanyu Wang*

Main category: cs.AI

Relevance: 75.0

TL;DR: 开发了一个自动化多智能体框架，用于评估AI模型文档的透明度，发现前沿模型合规率约80%，但大多数提供商低于60%，安全关键类别存在显著差距。


<details>
  <summary>Details</summary>
Motivation: AI模型文档分散在不同平台且结构不一致，阻碍了政策制定者、审计师和用户可靠评估安全声明、数据来源和版本变更。需要系统化的透明度评估框架。

Method: 1) 分析5个前沿模型和100个Hugging Face模型卡，识别947个独特章节名称；2) 基于欧盟AI法案和斯坦福透明度指数开发加权透明度框架（8个章节23个子章节）；3) 实现自动化多智能体管道，从公开源提取文档并通过LLM共识评分完整性。

Result: 评估50个模型（视觉、多模态、开源、闭源）总成本低于3美元。前沿实验室（xAI、微软、Anthropic）合规率约80%，大多数提供商低于60%。安全关键类别差距最大：欺骗行为、幻觉和儿童安全评估分别损失148、124和116个总分。

Conclusion: 当前AI模型文档透明度存在系统性差距，特别是安全关键信息。自动化评估框架能高效识别这些差距，为政策制定和行业标准提供依据。

Abstract: AI model documentation is fragmented across platforms and inconsistent in structure, preventing policymakers, auditors, and users from reliably assessing safety claims, data provenance, and version-level changes. We analyzed documentation from five frontier models (Gemini 3, Grok 4.1, Llama 4, GPT-5, and Claude 4.5) and 100 Hugging Face model cards, identifying 947 unique section names with extreme naming variation. Usage information alone appeared under 97 distinct labels. Using the EU AI Act Annex IV and the Stanford Transparency Index as baselines, we developed a weighted transparency framework with 8 sections and 23 subsections that prioritizes safety-critical disclosures (Safety Evaluation: 25%, Critical Risk: 20%) over technical specifications. We implemented an automated multi-agent pipeline that extracts documentation from public sources and scores completeness through LLM-based consensus. Evaluating 50 models across vision, multimodal, open-source, and closed-source systems cost less than $3 in total and revealed systematic gaps. Frontier labs (xAI, Microsoft, Anthropic) achieve approximately 80% compliance, while most providers fall below 60%. Safety-critical categories show the largest deficits: deception behaviors, hallucinations, and child safety evaluations account for 148, 124, and 116 aggregate points lost, respectively, across all evaluated models.

</details>


### [333] [SafeGen: Embedding Ethical Safeguards in Text-to-Image Generation](https://arxiv.org/abs/2512.12501)
*Dang Phuong Nam,Nguyen Kieu,Pham Thanh Hieu*

Main category: cs.AI

Relevance: 75.0

TL;DR: SafeGen是一个文本到图像生成框架，通过集成有害提示过滤器和优化扩散模型，在生成过程中嵌入伦理保障，以平衡创作自由与伦理责任。


<details>
  <summary>Details</summary>
Motivation: 生成式AI在创造、教育和研究中带来机遇，但文本到图像系统存在双重用途困境：放大社会偏见、生成高保真虚假信息、侵犯知识产权。需要将伦理保障直接嵌入生成流程。

Method: SafeGen框架包含两个组件：1) BGE-M3：微调的文本分类器，过滤有害或误导性提示；2) Hyper-SD：优化的扩散模型，生成高保真、语义对齐的图像。基于多语言数据集和公平感知训练流程构建。

Result: Hyper-SD达到IS=3.52、FID=22.08、SSIM=0.79；BGE-M3达到F1分数0.81。消融研究验证了领域特定微调的重要性。案例研究显示能有效阻止不安全提示、生成包容性教学材料、加强学术诚信。

Conclusion: SafeGen证明创作自由与伦理责任可以在单一工作流程中调和，为可信AI提供了实用框架，特别是在教育、研究等敏感应用场景。

Abstract: Generative Artificial Intelligence (AI) has created unprecedented opportunities for creative expression, education, and research. Text-to-image systems such as DALL.E, Stable Diffusion, and Midjourney can now convert ideas into visuals within seconds, but they also present a dual-use dilemma, raising critical ethical concerns: amplifying societal biases, producing high-fidelity disinformation, and violating intellectual property. This paper introduces SafeGen, a framework that embeds ethical safeguards directly into the text-to-image generation pipeline, grounding its design in established principles for Trustworthy AI. SafeGen integrates two complementary components: BGE-M3, a fine-tuned text classifier that filters harmful or misleading prompts, and Hyper-SD, an optimized diffusion model that produces high fidelity, semantically aligned images. Built on a curated multilingual (English- Vietnamese) dataset and a fairness-aware training process, SafeGen demonstrates that creative freedom and ethical responsibility can be reconciled within a single workflow. Quantitative evaluations confirm its effectiveness, with Hyper-SD achieving IS = 3.52, FID = 22.08, and SSIM = 0.79, while BGE-M3 reaches an F1-Score of 0.81. An ablation study further validates the importance of domain-specific fine-tuning for both modules. Case studies illustrate SafeGen's practical impact in blocking unsafe prompts, generating inclusive teaching materials, and reinforcing academic integrity.

</details>


### [334] [World Models Unlock Optimal Foraging Strategies in Reinforcement Learning Agents](https://arxiv.org/abs/2512.12548)
*Yesid Fonseca,Manuel S. Ríos,Nicanor Quijano,Luis F. Giraldo*

Main category: cs.AI

Relevance: 75.0

TL;DR: 该研究展示了配备学习世界模型的人工觅食者会自然收敛到边际价值定理(MVT)对齐的策略，表明预测性世界模型可以作为AI系统中更可解释和生物基础决策的基础。


<details>
  <summary>Details</summary>
Motivation: 虽然边际价值定理(MVT)在行为生态学中被广泛用于预测觅食行为，但生物觅食者如何实现最优补丁觅食决策的计算机制仍不清楚。研究者希望探索人工智能系统是否能自然发展出与MVT一致的策略，从而为可解释和适应性AI提供生态学基础。

Method: 使用基于模型的强化学习(MBRL)代理，该代理学习环境的简约预测表示。与标准的无模型RL代理进行比较，分析其补丁离开决策模式。代理通过获取世界模型来发展预期能力，而不仅仅是奖励最大化。

Result: 基于模型的代理表现出与许多生物对应物相似的决策模式，能够自然收敛到MVT对齐的补丁觅食策略。与无模型代理相比，这些代理的预期能力（而非单纯的奖励最大化）驱动了高效的补丁离开行为。

Conclusion: 预测性世界模型可以作为AI系统中更可解释和生物基础决策的基础。研究强调了生态最优性原理对于推进可解释和适应性AI的价值，为理解生物决策机制提供了计算视角。

Abstract: Patch foraging involves the deliberate and planned process of determining the optimal time to depart from a resource-rich region and investigate potentially more beneficial alternatives. The Marginal Value Theorem (MVT) is frequently used to characterize this process, offering an optimality model for such foraging behaviors. Although this model has been widely used to make predictions in behavioral ecology, discovering the computational mechanisms that facilitate the emergence of optimal patch-foraging decisions in biological foragers remains under investigation. Here, we show that artificial foragers equipped with learned world models naturally converge to MVT-aligned strategies. Using a model-based reinforcement learning agent that acquires a parsimonious predictive representation of its environment, we demonstrate that anticipatory capabilities, rather than reward maximization alone, drive efficient patch-leaving behavior. Compared with standard model-free RL agents, these model-based agents exhibit decision patterns similar to many of their biological counterparts, suggesting that predictive world models can serve as a foundation for more explainable and biologically grounded decision-making in AI systems. Overall, our findings highlight the value of ecological optimality principles for advancing interpretable and adaptive AI.

</details>


### [335] [Forgetful but Faithful: A Cognitive Memory Architecture and Benchmark for Privacy-Aware Generative Agents](https://arxiv.org/abs/2512.12856)
*Saad Alqithami*

Main category: cs.AI

Relevance: 75.0

TL;DR: 提出了Memory-Aware Retention Schema (MaRS)框架和六种遗忘策略，用于生成式智能体的记忆管理，平衡性能、隐私和计算效率，并通过FiFA基准进行评估。


<details>
  <summary>Details</summary>
Motivation: 随着生成式智能体在长期交互场景中部署，其记忆管理成为性能和隐私的关键瓶颈。现有方法要么维护无限记忆存储导致计算不可行和隐私问题，要么使用简单遗忘机制损害智能体一致性和功能。

Method: 提出了Memory-Aware Retention Schema (MaRS)框架，包含六种理论基础的遗忘策略，并开发了Forgetful but Faithful Agent (FiFA)基准，评估叙事一致性、目标完成、社交回忆准确性、隐私保护和成本效率。

Result: 通过300次评估运行，混合遗忘策略实现了最佳性能（综合得分：0.911），同时保持计算可行性和隐私保证。

Conclusion: 为记忆预算智能体评估建立了新基准，为资源受限、隐私敏感环境中部署生成式智能体提供了实用指南，通过解决智能体记忆管理的基本挑战，促进了以人为中心AI的发展。

Abstract: As generative agents become increasingly sophisticated and deployed in long-term interactive scenarios, their memory management capabilities emerge as a critical bottleneck for both performance and privacy. Current approaches either maintain unlimited memory stores, leading to computational intractability and privacy concerns, or employ simplistic forgetting mechanisms that compromise agent coherence and functionality. This paper introduces the Memory-Aware Retention Schema (MaRS), a novel framework for human-centered memory management in generative agents, coupled with six theoretically-grounded forgetting policies that balance performance, privacy, and computational efficiency. We present the Forgetful but Faithful Agent (FiFA) benchmark, a comprehensive evaluation framework that assesses agent performance across narrative coherence, goal completion, social recall accuracy, privacy preservation, and cost efficiency. Through extensive experimentation involving 300 evaluation runs across multiple memory budgets and agent configurations, we demonstrate that our hybrid forgetting policy achieves superior performance (composite score: 0.911) while maintaining computational tractability and privacy guarantees. Our work establishes new benchmarks for memory-budgeted agent evaluation and provides practical guidelines for deploying generative agents in resource-constrained, privacy-sensitive environments. The theoretical foundations, implementation framework, and empirical results contribute to the emerging field of human-centered AI by addressing fundamental challenges in agent memory management that directly impact user trust, system scalability, and regulatory compliance.

</details>


### [336] [MedInsightBench: Evaluating Medical Analytics Agents Through Multi-Step Insight Discovery in Multimodal Medical Data](https://arxiv.org/abs/2512.13297)
*Zhenghao Zhu,Chuxue Cao,Sirui Han,Yuanfeng Song,Xing Chen,Caleb Chen Cao,Yike Guo*

Main category: cs.AI

Relevance: 75.0

TL;DR: 提出了MedInsightBench基准测试和MedInsightAgent框架，用于评估和改进多模态大模型在医学数据分析中的洞察发现能力。


<details>
  <summary>Details</summary>
Motivation: 医学数据分析需要从复杂的多模态数据中提取深度洞察，但目前缺乏专门评估大型多模态模型医学洞察发现能力的高质量数据集。

Method: 1) 创建MedInsightBench基准：包含332个精心策划的医学案例，每个案例都有精心设计的洞察标注；2) 提出MedInsightAgent框架：包含视觉根因发现器、分析洞察代理和后续问题生成器三个模块。

Result: 现有LMMs在MedInsightBench上表现有限，主要由于提取多步骤深度洞察的挑战和缺乏医学专业知识。MedInsightAgent能显著提升通用LMMs在医学数据洞察发现中的性能。

Conclusion: MedInsightBench揭示了当前LMMs在医学洞察发现方面的局限性，而MedInsightAgent框架为解决这些挑战提供了有效途径，推动了医学多模态分析的发展。

Abstract: In medical data analysis, extracting deep insights from complex, multi-modal datasets is essential for improving patient care, increasing diagnostic accuracy, and optimizing healthcare operations. However, there is currently a lack of high-quality datasets specifically designed to evaluate the ability of large multi-modal models (LMMs) to discover medical insights. In this paper, we introduce MedInsightBench, the first benchmark that comprises 332 carefully curated medical cases, each annotated with thoughtfully designed insights. This benchmark is intended to evaluate the ability of LMMs and agent frameworks to analyze multi-modal medical image data, including posing relevant questions, interpreting complex findings, and synthesizing actionable insights and recommendations. Our analysis indicates that existing LMMs exhibit limited performance on MedInsightBench, which is primarily attributed to their challenges in extracting multi-step, deep insights and the absence of medical expertise. Therefore, we propose MedInsightAgent, an automated agent framework for medical data analysis, composed of three modules: Visual Root Finder, Analytical Insight Agent, and Follow-up Question Composer. Experiments on MedInsightBench highlight pervasive challenges and demonstrate that MedInsightAgent can improve the performance of general LMMs in medical data insight discovery.

</details>


### [337] [neuralFOMO: Can LLMs Handle Being Second Best? Measuring Envy-Like Preferences in Multi-Agent Settings](https://arxiv.org/abs/2512.13481)
*Ojas Pungalia,Rashi Upadhyay,Abhishek Mishra,Abhiram H,Tejasvi Alladi,Sujan Yenuganti,Dhruv Kumar*

Main category: cs.AI

Relevance: 75.0

TL;DR: 论文研究LLM是否表现出嫉妒行为，通过点分配游戏和工作场所场景测试发现某些LLM确实表现出嫉妒模式，不同模型行为差异显著。


<details>
  <summary>Details</summary>
Motivation: 随着LLM越来越多地在协作和竞争工作流中代表人类行动，需要评估它们是否以及在什么条件下表现出嫉妒偏好，这对多智能体系统的安全和设计至关重要。

Method: 设计了两个实验场景：1）点分配游戏测试模型是否试图超越同伴；2）工作场所场景观察在奖励不公情况下的行为。测试了多个LLM模型在不同上下文中的表现。

Result: 发现某些LLM表现出明显的嫉妒模式，但不同模型差异很大。GPT-5-mini和Claude-3.7-Sonnet倾向于拉低同伴以平衡结果，而Mistral-Small-3.2-24B则专注于最大化自身收益。

Conclusion: LLM确实会表现出嫉妒行为，这种竞争倾向应作为LLM多智能体系统的安全和设计因素加以考虑。

Abstract: Envy is a common human behavior that shapes competitiveness and can alter outcomes in team settings. As large language models (LLMs) increasingly act on behalf of humans in collaborative and competitive workflows, there is a pressing need to evaluate whether and under what conditions they exhibit envy-like preferences. In this paper, we test whether LLMs show envy-like behavior toward each other. We considered two scenarios: (1) A point allocation game that tests whether a model tries to win over its peer. (2) A workplace setting observing behaviour when recognition is unfair. Our findings reveal consistent evidence of envy-like patterns in certain LLMs, with large variation across models and contexts. For instance, GPT-5-mini and Claude-3.7-Sonnet show a clear tendency to pull down the peer model to equalize outcomes, whereas Mistral-Small-3.2-24B instead focuses on maximizing its own individual gains. These results highlight the need to consider competitive dispositions as a safety and design factor in LLM-based multi-agent systems.

</details>


### [338] [WAM-Diff: A Masked Diffusion VLA Framework with MoE and Online Reinforcement Learning for Autonomous Driving](https://arxiv.org/abs/2512.11872)
*Mingwang Xu,Jiahao Cui,Feipeng Cai,Hanlin Shang,Zhihao Zhu,Shan Luan,Yifang Xu,Neng Zhang,Yaoyi Li,Jia Cai,Siyu Zhu*

Main category: cs.RO

Relevance: 75.0

TL;DR: WAM-Diff是一个基于掩码扩散的视觉-语言-动作模型框架，用于自动驾驶轨迹生成，通过离散序列迭代优化、稀疏MoE架构和在线强化学习实现高效规划。


<details>
  <summary>Details</summary>
Motivation: 当前自动驾驶系统主要使用自回归大语言模型和连续扩散策略，但离散掩码扩散在轨迹生成方面的潜力尚未充分探索。本文旨在探索掩码扩散作为自回归和扩散策略的替代方案，支持场景感知的解码策略。

Method: 1) 将掩码扩散系统性地适配到自动驾驶，支持灵活的非因果解码顺序；2) 通过稀疏MoE架构联合训练运动预测和驾驶导向的视觉问答任务；3) 使用组序列策略优化(GSPO)进行在线强化学习，优化序列级驾驶奖励。

Result: 在NAVSIM-v1上达到91.0 PDMS，在NAVSIM-v2上达到89.7 EPDMS，证明了掩码扩散在自动驾驶中的有效性。

Conclusion: WAM-Diff为自动驾驶轨迹生成提供了有前景的替代方案，支持场景感知解码策略，展示了掩码扩散在VLA模型中的潜力。

Abstract: End-to-end autonomous driving systems based on vision-language-action (VLA) models integrate multimodal sensor inputs and language instructions to generate planning and control signals. While autoregressive large language models and continuous diffusion policies are prevalent, the potential of discrete masked diffusion for trajectory generation remains largely unexplored. This paper presents WAM-Diff, a VLA framework that employs masked diffusion to iteratively refine a discrete sequence representing future ego-trajectories. Our approach features three key innovations: a systematic adaptation of masked diffusion for autonomous driving that supports flexible, non-causal decoding orders; scalable model capacity via a sparse MoE architecture trained jointly on motion prediction and driving-oriented visual question answering (VQA); and online reinforcement learning using Group Sequence Policy Optimization (GSPO) to optimize sequence-level driving rewards. Remarkably, our model achieves 91.0 PDMS on NAVSIM-v1 and 89.7 EPDMS on NAVSIM-v2, demonstrating the effectiveness of masked diffusion for autonomous driving. The approach provides a promising alternative to autoregressive and diffusion-based policies, supporting scenario-aware decoding strategies for trajectory generation. The code for this paper will be released publicly at: https://github.com/fudan-generative-vision/WAM-Diff

</details>


### [339] [Towards Accessible Physical AI: LoRA-Based Fine-Tuning of VLA Models for Real-World Robot Control](https://arxiv.org/abs/2512.11921)
*Abdullah Yahya Abdullah Omaisan,Ibrahim Sheikh Mohamed*

Main category: cs.RO

Relevance: 75.0

TL;DR: 提出了一种高效的微调方法，使大规模VLA模型能在低成本机器人平台上部署，通过LoRA和量化技术让31亿参数模型在8GB显存GPU上运行


<details>
  <summary>Details</summary>
Motivation: 当前VLA模型在机器人操作中表现出色，但部署到低成本机器人平台面临计算资源限制和新机器人本体适应困难的问题，需要高效的微调方法

Method: 使用低秩适应(LoRA)和量化技术进行资源高效微调，探索冻结与解冻视觉编码器的权衡，在SO101机械臂上进行按钮按压任务的真实部署

Result: 方法能在仅200个演示样本上有效微调31亿参数VLA模型，在低成本平台上实现有效操作性能，同时保持计算效率

Conclusion: 通过适当的微调方法，VLA模型可以成功部署在低成本机器人平台上，使先进操作能力超越昂贵的研究机器人

Abstract: Vision-Language-Action (VLA) models have demonstrated remarkable capabilities in robotic manipulation,enabling robots to execute natural language commands through end-to-end learning from visual observations.However, deploying large-scale VLA models on affordable robotic platforms remains challenging due to computational constraints and the need for efficient adaptation to new robot embodiments. This paper presents an efficient fine-tuning methodology and real-world deployment analysis for adapting VLA models to low-cost robotic manipulation systems.We propose a resource-efficient fine-tuning strategy using Low-Rank Adaptation (LoRA) and quantization techniques that enable multi-billion parameter VLA models ( 3.1B parameters) to run on consumer-grade GPUs with 8GB VRAM. Our methodology addresses the critical challenge of adapting pre-trained VLA models to new robot embodiments with limited demonstration data, focusing on the trade-offs between frozen and unfrozen vision encoders. Through real-world deployment on the SO101 robotic arm for a button-pressing manipulation task, we demonstrate that our approach achieves effective manipulation performance while maintaining computational efficiency. We provide detailed analysis of deployment challenges, failure modes, and the relationship between training data quantity and real-world performance,trained on 200 demonstration episodes. Our results show that with proper fine-tuning methodology, VLA models can be successfully deployed on affordable robotic platforms,making advanced manipulation capabilities accessible beyond expensive research robots.

</details>


### [340] [How AI Agents Follow the Herd of AI? Network Effects, History, and Machine Optimism](https://arxiv.org/abs/2512.11943)
*Yu Liu,Wenwen Li,Yifan Dou,Guangnan Ye*

Main category: cs.MA

Relevance: 75.0

TL;DR: 该研究探索了在多AI智能体框架中，网络效应游戏下的决策行为，发现LLM智能体的均衡结果不仅取决于激励，还受历史数据编排方式的影响，这与人类决策不同。


<details>
  <summary>Details</summary>
Motivation: 研究动机是理解多AI智能体在网络效应游戏中的决策机制。网络效应场景中个体收益依赖于同伴参与，这在现实世界中普遍存在，但在多智能体系统中尚未充分探索。

Method: 引入基于LLM的智能体在重复决策场景中的工作流程设计，系统操纵价格轨迹（固定、上升、下降、随机）和网络效应强度，分析智能体在不同历史数据条件下的决策行为。

Result: 主要发现：1）无历史数据时，智能体无法推断均衡；2）有序历史序列（如价格上升）在弱网络效应下能实现部分收敛，但强网络效应会引发"AI乐观主义"——智能体高估参与度；3）随机历史完全破坏收敛，表明时间连贯性对LLM推理至关重要。

Conclusion: 结论指出范式转变：在AI中介系统中，均衡结果不仅取决于激励，还受历史数据编排方式的影响，而这对人类来说是不可能的。这揭示了LLM智能体与人类在决策推理上的根本差异。

Abstract: Understanding decision-making in multi-AI-agent frameworks is crucial for analyzing strategic interactions in network-effect-driven contexts. This study investigates how AI agents navigate network-effect games, where individual payoffs depend on peer participatio--a context underexplored in multi-agent systems despite its real-world prevalence. We introduce a novel workflow design using large language model (LLM)-based agents in repeated decision-making scenarios, systematically manipulating price trajectories (fixed, ascending, descending, random) and network-effect strength. Our key findings include: First, without historical data, agents fail to infer equilibrium. Second, ordered historical sequences (e.g., escalating prices) enable partial convergence under weak network effects but strong effects trigger persistent "AI optimism"--agents overestimate participation despite contradictory evidence. Third, randomized history disrupts convergence entirely, demonstrating that temporal coherence in data shapes LLMs' reasoning, unlike humans. These results highlight a paradigm shift: in AI-mediated systems, equilibrium outcomes depend not just on incentives, but on how history is curated, which is impossible for human.

</details>


### [341] [A Review of Learning-Based Motion Planning: Toward a Data-Driven Optimal Control Approach](https://arxiv.org/abs/2512.11944)
*Jia Hu,Yang Chang,Haoran Wang*

Main category: cs.RO

Relevance: 75.0

TL;DR: 该论文提出了一种数据驱动的最优控制范式，旨在解决自动驾驶运动规划中传统管道方法与学习型系统之间的权衡问题，通过整合经典控制的可验证结构和机器学习的自适应能力来实现可信赖的自动驾驶系统。


<details>
  <summary>Details</summary>
Motivation: 自动驾驶运动规划面临根本性权衡：传统管道方法透明但脆弱，而现代学习型系统自适应但不透明（"黑箱"）。这种持久困境阻碍了真正可信赖系统的发展，需要新的范式来解决这一僵局。

Method: 通过全面综述基于学习的运动规划方法，提出数据驱动的最优控制统一框架。该框架将经典控制的可验证结构与机器学习的自适应能力协同整合，利用真实世界数据持续优化系统动力学、成本函数和安全约束等关键组件。

Result: 该框架有望实现三个关键下一代能力：1)"以人为中心"的定制化；2)"平台自适应"的动态适应；3)通过自调优实现"系统自优化"。为同时具备安全性、可解释性和类人自主性的智能交通系统提供了理论基础。

Conclusion: 数据驱动的最优控制范式为解决自动驾驶运动规划中的可信赖性困境提供了有前景的路径，通过整合经典控制与机器学习，能够开发出同时具备安全性、可解释性和自适应能力的智能交通系统。

Abstract: Motion planning for high-level autonomous driving is constrained by a fundamental trade-off between the transparent, yet brittle, nature of pipeline methods and the adaptive, yet opaque, "black-box" characteristics of modern learning-based systems. This paper critically synthesizes the evolution of the field -- from pipeline methods through imitation learning, reinforcement learning, and generative AI -- to demonstrate how this persistent dilemma has hindered the development of truly trustworthy systems. To resolve this impasse, we conduct a comprehensive review of learning-based motion planning methods. Based on this review, we outline a data-driven optimal control paradigm as a unifying framework that synergistically integrates the verifiable structure of classical control with the adaptive capacity of machine learning, leveraging real-world data to continuously refine key components such as system dynamics, cost functions, and safety constraints. We explore this framework's potential to enable three critical next-generation capabilities: "Human-Centric" customization, "Platform-Adaptive" dynamics adaptation, and "System Self-Optimization" via self-tuning. We conclude by proposing future research directions based on this paradigm, aimed at developing intelligent transportation systems that are simultaneously safe, interpretable, and capable of human-like autonomy.

</details>


### [342] [Designing The Internet of Agents: A Framework for Trustworthy, Transparent, and Collaborative Human-Agent Interaction (HAX)](https://arxiv.org/abs/2512.11979)
*Marc Scibelli,Krystelle Gonzalez Papaux,Julia Valenti,Srishti Kush*

Main category: cs.HC

Relevance: 75.0

TL;DR: HAX框架是一个三阶段的人机协作框架，旨在为可信、透明、协作的智能体交互建立设计基础，通过行为启发式、结构化SDK和行为代理等组件，将多智能体系统重构为"同事"关系。


<details>
  <summary>Details</summary>
Motivation: 生成式和自主智能体的兴起标志着计算的根本性转变，需要重新思考人类如何与概率性、部分自主的系统协作。当前缺乏一个端到端的框架来桥接信任理论、界面设计和基础设施。

Method: HAX框架包含三个核心组件：1) 行为启发式方法；2) 强制执行结构化和安全输出的模式驱动SDK；3) 编排智能体活动以减少认知负荷的行为代理概念。基于时间、交互和性能(TIP)理论，提供经过验证的混合主动设计模式目录。

Result: HAX提供了第一个端到端的框架，将信任理论、界面设计和基础设施桥接起来，为新兴的"智能体互联网"提供设计基础，支持意图预览、迭代对齐、信任修复和多智能体叙事连贯性。

Conclusion: HAX框架将多智能体系统重新构想为"同事"，为可信、透明、协作的智能体交互建立了全面的设计基础，解决了人类与概率性自主系统协作的关键挑战。

Abstract: The rise of generative and autonomous agents marks a fundamental shift in computing, demanding a rethinking of how humans collaborate with probabilistic, partially autonomous systems. We present the Human-AI-Experience (HAX) framework, a comprehensive, three-phase approach that establishes design foundations for trustworthy, transparent, and collaborative agentic interaction. HAX integrates behavioral heuristics, a schema-driven SDK enforcing structured and safe outputs, and a behavioral proxy concept that orchestrates agent activity to reduce cognitive load. A validated catalog of mixed-initiative design patterns further enables intent preview, iterative alignment, trust repair, and multi-agent narrative coherence. Grounded in Time, Interaction, and Performance (TIP) theory, HAX reframes multi-agent systems as colleagues, offering the first end-to-end framework that bridges trust theory, interface design, and infrastructure for the emerging Internet of Agents.

</details>


### [343] [Diverse LLMs vs. Vulnerabilities: Who Detects and Fixes Them Better?](https://arxiv.org/abs/2512.12536)
*Arastoo Zibaeirad,Marco Vieira*

Main category: cs.SE

Relevance: 75.0

TL;DR: DVDR-LLM是一个集成多个LLM的框架，用于软件漏洞检测和修复，通过聚合多个模型的输出来提高性能，在复杂漏洞检测方面比单个模型表现更好，但存在假阳性和假阴性之间的权衡。


<details>
  <summary>Details</summary>
Motivation: 虽然单个LLM已展现出代码理解能力，但在识别复杂漏洞和生成修复方案时仍存在困难。研究旨在探索通过集成多个LLM的输出来降低错误率，提高软件漏洞检测和修复的准确性。

Method: 提出DVDR-LLM集成框架，结合多个不同LLM的输出，通过设定阈值来决定需要多少个模型达成一致才能做出最终判断。框架评估了不同安全场景下集成方法的性能表现。

Result: DVDR-LLM比单个模型平均检测准确率提高10-12%，在代码复杂度增加时优势更明显。对于多文件漏洞，召回率提升18%，F1分数提升11.8%。但存在权衡：验证任务中减少假阳性，但检测任务中增加假阴性。

Conclusion: 集成多个LLM可以显著提高软件漏洞检测和修复的性能，特别是在复杂场景下。但需要根据具体安全上下文仔细选择模型一致性阈值，以平衡假阳性和假阴性之间的权衡。

Abstract: Large Language Models (LLMs) are increasingly being studied for Software Vulnerability Detection (SVD) and Repair (SVR). Individual LLMs have demonstrated code understanding abilities, but they frequently struggle when identifying complex vulnerabilities and generating fixes.
  This study presents DVDR-LLM, an ensemble framework that combines outputs from diverse LLMs to determine whether aggregating multiple models reduces error rates. Our evaluation reveals that DVDR-LLM achieves 10-12% higher detection accuracy compared to the average performance of individual models, with benefits increasing as code complexity grows. For multi-file vulnerabilities, the ensemble approach demonstrates significant improvements in recall (+18%) and F1 score (+11.8%) over individual models. However, the approach raises measurable trade-offs: reducing false positives in verification tasks while simultaneously increasing false negatives in detection tasks, requiring careful decision on the required level of agreement among the LLMs (threshold) for increased performance across different security contexts.
  Artifact: https://github.com/Erroristotle/DVDR_LLM

</details>


### [344] [Detecting Prompt Injection Attacks Against Application Using Classifiers](https://arxiv.org/abs/2512.12583)
*Safwan Shaheer,G. M. Refatul Islam,Mohammad Rafid Hamid,Md. Abrar Faiaz Khan,Md. Omar Faruk,Yaseen Nur*

Main category: cs.CR

Relevance: 75.0

TL;DR: 该论文基于HackAPrompt数据集构建了提示注入攻击检测系统，训练了多种分类器来识别恶意提示，以保护LLM集成的Web应用安全。


<details>
  <summary>Details</summary>
Motivation: 提示注入攻击会威胁关键系统的安全和稳定，包括基础设施和大型Web应用。随着LLM在Web应用中的集成越来越广泛，需要有效的检测方法来防范这类安全威胁。

Method: 1. 基于HackAPrompt Playground Submissions语料库构建并扩展提示注入数据集
2. 训练多种分类器进行检测：LSTM、前馈神经网络、随机森林、朴素贝叶斯
3. 专注于LLM集成Web应用中的恶意提示检测

Result: 提出的方法改进了提示注入检测和缓解能力，有助于保护目标应用和系统免受攻击。

Conclusion: 该研究为LLM集成应用提供了有效的提示注入攻击检测方案，通过多种机器学习分类器的比较，提升了Web应用的安全性。

Abstract: Prompt injection attacks can compromise the security and stability of critical systems, from infrastructure to large web applications. This work curates and augments a prompt injection dataset based on the HackAPrompt Playground Submissions corpus and trains several classifiers, including LSTM, feed forward neural networks, Random Forest, and Naive Bayes, to detect malicious prompts in LLM integrated web applications. The proposed approach improves prompt injection detection and mitigation, helping protect targeted applications and systems.

</details>


### [345] [Security and Detectability Analysis of Unicode Text Watermarking Methods Against Large Language Models](https://arxiv.org/abs/2512.13325)
*Malte Hellmeier*

Main category: cs.CR

Relevance: 75.0

TL;DR: 本文研究了现有Unicode文本水印方法的安全性，通过实验发现最新推理型LLM能够检测水印文本，但所有模型都无法提取水印内容，除非提供源代码实现细节。


<details>
  <summary>Details</summary>
Motivation: 随着大语言模型的广泛应用，数字文本安全变得日益重要。人们担心数据在用于训练模型时失去控制，以及难以区分模型生成文本与人类书写文本。数字水印提供了额外保护，但现有文本水印方法是否安全且对大语言模型不可检测，缺乏系统分析和验证。

Method: 在受控实验环境中，实现了10种现有Unicode文本水印方法，并在6个大语言模型（GPT-5、GPT-4o、Teuken 7B、Llama 3.3、Claude Sonnet 4、Gemini 2.5 Pro）上进行测试。设计了三个实验来评估水印的检测和提取能力。

Result: 实验结果表明，特别是最新的推理模型能够检测到水印文本。然而，所有模型都无法提取水印内容，除非提供源代码实现细节。这揭示了现有水印方法在检测和提取方面的安全特性差异。

Conclusion: 数字文本水印的安全性需要更深入的研究，特别是针对大语言模型的检测能力。虽然模型难以提取水印，但能够检测其存在，这对安全研究人员和实践者具有重要启示，需要开发更安全的水印方法来应对新兴威胁。

Abstract: Securing digital text is becoming increasingly relevant due to the widespread use of large language models. Individuals' fear of losing control over data when it is being used to train such machine learning models or when distinguishing model-generated output from text written by humans. Digital watermarking provides additional protection by embedding an invisible watermark within the data that requires protection. However, little work has been taken to analyze and verify if existing digital text watermarking methods are secure and undetectable by large language models. In this paper, we investigate the security-related area of watermarking and machine learning models for text data. In a controlled testbed of three experiments, ten existing Unicode text watermarking methods were implemented and analyzed across six large language models: GPT-5, GPT-4o, Teuken 7B, Llama 3.3, Claude Sonnet 4, and Gemini 2.5 Pro. The findings of our experiments indicate that, especially the latest reasoning models, can detect a watermarked text. Nevertheless, all models fail to extract the watermark unless implementation details in the form of source code are provided. We discuss the implications for security researchers and practitioners and outline future research opportunities to address security concerns.

</details>


### [346] [Understanding Critical Thinking in Generative Artificial Intelligence Use: Development, Validation, and Correlates of the Critical Thinking in AI Use Scale](https://arxiv.org/abs/2512.12413)
*Gabriel R. Lau,Wei Yan Low,Louis Tay,Ysabel Guevarra,Dragan Gašević,Andree Hartanto*

Main category: cs.AI

Relevance: 65.0

TL;DR: 开发并验证了一个13项的量表，用于测量AI使用中的批判性思维，包含验证、动机和反思三个维度，该量表能预测更准确的AI输出验证和更深度的反思。


<details>
  <summary>Details</summary>
Motivation: 随着生成式AI工具在日常工作和学习中的普及，用户需要批判性地评估AI输出而非盲目接受。然而，目前缺乏系统测量人们在AI使用中批判性思维倾向的工具，阻碍了相关研究的发展。

Method: 通过6项研究（N=1365）开发和验证了"AI使用中的批判性思维量表"：研究1生成和内容验证项目；研究2支持三因素结构（验证、动机、反思）；研究3-5验证高阶模型、信效度；研究6证明量表能预测验证策略频率、准确性及反思深度。

Result: 量表具有良好的心理测量特性：三因素结构稳定、内部一致性高、重测信度好、性别不变性、收敛和区分效度良好。AI使用中的批判性思维与开放性、外向性、积极情感特质和AI使用频率正相关，并能预测更准确的真实性判断和更深度的反思。

Conclusion: 该研究澄清了人们如何监督生成式AI输出，提供了经过验证的量表和生态效度高的任务范式，支持AI批判性参与的理论检验、跨群体和纵向研究。

Abstract: Generative AI tools are increasingly embedded in everyday work and learning, yet their fluency, opacity, and propensity to hallucinate mean that users must critically evaluate AI outputs rather than accept them at face value. The present research conceptualises critical thinking in AI use as a dispositional tendency to verify the source and content of AI-generated information, to understand how models work and where they fail, and to reflect on the broader implications of relying on AI. Across six studies (N = 1365), we developed and validated the 13-item critical thinking in AI use scale and mapped its nomological network. Study 1 generated and content-validated scale items. Study 2 supported a three-factor structure (Verification, Motivation, and Reflection). Studies 3, 4, and 5 confirmed this higher-order model, demonstrated internal consistency and test-retest reliability, strong factor loadings, sex invariance, and convergent and discriminant validity. Studies 3 and 4 further revealed that critical thinking in AI use was positively associated with openness, extraversion, positive trait affect, and frequency of AI use. Lastly, Study 6 demonstrated criterion validity of the scale, with higher critical thinking in AI use scores predicting more frequent and diverse verification strategies, greater veracity-judgement accuracy in a novel and naturalistic ChatGPT-powered fact-checking task, and deeper reflection about responsible AI. Taken together, the current work clarifies why and how people exercise oversight over generative AI outputs and provides a validated scale and ecologically grounded task paradigm to support theory testing, cross-group, and longitudinal research on critical engagement with generative AI outputs.

</details>


### [347] [KidsArtBench: Multi-Dimensional Children's Art Evaluation with Attribute-Aware MLLMs](https://arxiv.org/abs/2512.12503)
*Mingrui Ye,Chanjin Zheng,Zengyi Yu,Chenyu Xiang,Zhixue Zhao,Zheng Yuan,Helen Yannakoudakis*

Main category: cs.AI

Relevance: 65.0

TL;DR: KidsArtBench：针对儿童艺术作品的评估基准，包含多维度评分和专家反馈，提出基于多LoRA的属性特定方法，显著提升MLLM在艺术评估上的表现


<details>
  <summary>Details</summary>
Motivation: 当前多模态大语言模型在艺术表达评估方面能力有限，主要因为审美概念抽象开放，且多模态艺术作品标注稀缺。儿童艺术作品评估对教育AI尤其重要，但现有数据集多为成人图像的单标量评分

Method: 1) 构建KidsArtBench基准：包含1000+儿童艺术作品（5-15岁），由12位专家教育者按9个维度标注，并提供专家评论反馈；2) 提出属性特定的多LoRA方法：每个评估维度对应一个独立的LoRA适配器；3) 使用回归感知微调（RAFT）将预测与序数尺度对齐

Result: 在Qwen2.5-VL-7B上，相关性从0.468提升到0.653，在感知维度上提升最大，在高阶属性上的差距缩小。方法能产生教育学上有意义的评估

Conclusion: 教育者对齐的监督和属性感知训练能够产生教育学上有意义的评估，为教育AI的持续进步建立了严格的测试平台

Abstract: Multimodal Large Language Models (MLLMs) show remarkable progress across many visual-language tasks; however, their capacity to evaluate artistic expression remains limited. Aesthetic concepts are inherently abstract and open-ended, and multimodal artwork annotations are scarce. We introduce KidsArtBench, a new benchmark of over 1k children's artworks (ages 5-15) annotated by 12 expert educators across 9 rubric-aligned dimensions, together with expert comments for feedback. Unlike prior aesthetic datasets that provide single scalar scores on adult imagery, KidsArtBench targets children's artwork and pairs multi-dimensional annotations with comment supervision to enable both ordinal assessment and formative feedback. Building on this resource, we propose an attribute-specific multi-LoRA approach, where each attribute corresponds to a distinct evaluation dimension (e.g., Realism, Imagination) in the scoring rubric, with Regression-Aware Fine-Tuning (RAFT) to align predictions with ordinal scales. On Qwen2.5-VL-7B, our method increases correlation from 0.468 to 0.653, with the largest gains on perceptual dimensions and narrowed gaps on higher-order attributes. These results show that educator-aligned supervision and attribute-aware training yield pedagogically meaningful evaluations and establish a rigorous testbed for sustained progress in educational AI. We release data and code with ethics documentation.

</details>


### [348] [Modular and Multi-Path-Aware Offline Benchmarking for Mobile GUI Agents](https://arxiv.org/abs/2512.12634)
*Youngmin Im,Byeongung Jo,Jaeyoung Wi,Seungwoo Baek,Tae Hoon Min,Joo Hyung Lee,Sangeun Oh,Insik Shin,Sunjae Lee*

Main category: cs.AI

Relevance: 65.0

TL;DR: MobiBench是一个模块化、多路径感知的移动GUI代理离线基准测试框架，解决了现有评估方法在可扩展性、可重复性和组件级分析方面的局限性。


<details>
  <summary>Details</summary>
Motivation: 当前移动GUI代理评估存在两个根本问题：1) 离线基准使用静态单路径数据集会不公平地惩罚有效替代操作，而在线基准由于动态不可预测性导致可扩展性和可重复性差；2) 现有基准将代理视为黑盒，忽略了组件级贡献，导致不公平比较或掩盖性能瓶颈。

Method: 提出MobiBench框架，这是首个模块化、多路径感知的移动GUI代理离线基准测试框架，支持完全离线环境下的高保真、可扩展、可重复评估。框架包含组件级分析能力。

Result: 实验表明MobiBench与人类评估者的一致性达到94.72%，与精心设计的在线基准相当，同时保持了静态离线基准的可扩展性和可重复性。组件级分析揭示了多个关键发现。

Conclusion: MobiBench为移动GUI代理提供了高保真、可扩展、可重复的评估框架，通过组件级分析揭示了当前方法的局限性，并为设计更强大、成本效益更高的移动代理提供了实用指南。

Abstract: Mobile GUI Agents, AI agents capable of interacting with mobile applications on behalf of users, have the potential to transform human computer interaction. However, current evaluation practices for GUI agents face two fundamental limitations. First, they either rely on single path offline benchmarks or online live benchmarks. Offline benchmarks using static, single path annotated datasets unfairly penalize valid alternative actions, while online benchmarks suffer from poor scalability and reproducibility due to the dynamic and unpredictable nature of live evaluation. Second, existing benchmarks treat agents as monolithic black boxes, overlooking the contributions of individual components, which often leads to unfair comparisons or obscures key performance bottlenecks. To address these limitations, we present MobiBench, the first modular and multi path aware offline benchmarking framework for mobile GUI agents that enables high fidelity, scalable, and reproducible evaluation entirely in offline settings. Our experiments demonstrate that MobiBench achieves 94.72 percent agreement with human evaluators, on par with carefully engineered online benchmarks, while preserving the scalability and reproducibility of static offline benchmarks. Furthermore, our comprehensive module level analysis uncovers several key insights, including a systematic evaluation of diverse techniques used in mobile GUI agents, optimal module configurations across model scales, the inherent limitations of current LFMs, and actionable guidelines for designing more capable and cost efficient mobile agents.

</details>


### [349] [Synergizing Code Coverage and Gameplay Intent: Coverage-Aware Game Playtesting with LLM-Guided Reinforcement Learning](https://arxiv.org/abs/2512.12706)
*Enhong Mu,Minami Yoda,Yan Zhang,Mingyue Zhang,Yutaka Matsuno,Jialong Li*

Main category: cs.AI

Relevance: 65.0

TL;DR: SMART框架结合结构验证与功能验证，利用LLM解析AST差异提取功能意图，构建上下文感知混合奖励机制，指导RL智能体在游戏更新测试中同时覆盖修改代码分支并完成游戏任务。


<details>
  <summary>Details</summary>
Motivation: 游戏即服务模式需要频繁内容更新，对质量保证造成巨大压力。现有自动化测试方法存在二分法：代码中心方法关注结构覆盖但不理解游戏上下文，玩家中心智能体验证高层意图但常无法覆盖具体底层代码变更。需要弥合这一差距。

Method: 提出SMART框架，利用大型语言模型(LLMs)解释抽象语法树(AST)差异并提取功能意图，构建上下文感知混合奖励机制。该机制指导强化学习智能体顺序完成游戏目标，同时自适应探索修改的代码分支。

Result: 在Overcooked和Minecraft环境中评估，SMART显著优于现有基线方法：实现超过94%的修改代码分支覆盖，几乎是传统强化学习方法的两倍，同时保持98%的任务完成率，有效平衡结构全面性与功能正确性。

Conclusion: SMART成功弥合了结构验证与功能验证之间的差距，为游戏更新测试提供了一种协同方法，能够同时确保代码覆盖和功能正确性，适应游戏即服务模式的快速发布节奏需求。

Abstract: The widespread adoption of the "Games as a Service" model necessitates frequent content updates, placing immense pressure on quality assurance. In response, automated game testing has been viewed as a promising solution to cope with this demanding release cadence. However, existing automated testing approaches typically create a dichotomy: code-centric methods focus on structural coverage without understanding gameplay context, while player-centric agents validate high-level intent but often fail to cover specific underlying code changes. To bridge this gap, we propose SMART (Structural Mapping for Augmented Reinforcement Testing), a novel framework that synergizes structural verification and functional validation for game update testing. SMART leverages large language models (LLMs) to interpret abstract syntax tree (AST) differences and extract functional intent, constructing a context-aware hybrid reward mechanism. This mechanism guides reinforcement learning agents to sequentially fulfill gameplay goals while adaptively exploring modified code branches. We evaluate SMART on two environments, Overcooked and Minecraft. The results demonstrate that SMART significantly outperforms state-of-the-art baselines; it achieves over 94% branch coverage of modified code, nearly double that of traditional reinforcement learning methods, while maintaining a 98% task completion rate, effectively balancing structural comprehensiveness with functional correctness.

</details>


### [350] [MAC: A Multi-Agent Framework for Interactive User Clarification in Multi-turn Conversations](https://arxiv.org/abs/2512.13154)
*Emre Can Acikgoz,Jinoh Oh,Joo Hyuk Jeon,Jie Hao,Heng Ji,Dilek Hakkani-Tür,Gokhan Tur,Xiang Li,Chengyuan Ma,Xing Fan*

Main category: cs.AI

Relevance: 65.0

TL;DR: 提出MAC（多智能体澄清）框架，通过多智能体协同策略解决对话系统中的用户歧义问题，在MultiWOZ数据集上显著提升任务成功率并减少对话轮次。


<details>
  <summary>Details</summary>
Motivation: 对话系统常遇到模糊的用户请求，需要有效澄清才能完成任务。虽然多智能体架构在复杂对话场景中表现优异，但歧义解决仍是一个关键且未充分探索的挑战，特别是难以确定哪个智能体应发起澄清、以及智能体在面临不确定或不完整输入时如何协调行动。

Method: 提出MAC（多智能体澄清）交互式框架，专门优化用于通过战略管理澄清对话来解决用户歧义。首先引入新的分类法对用户歧义进行分类，系统指导澄清策略；然后提出MAC框架，自主协调多个智能体与用户进行协同交互。

Result: 在MultiWOZ 2.4上的实证评估表明，在两个层面启用澄清可将任务成功率提高7.8%（从54.5%到62.3%），并将平均对话轮次从6.53减少到4.86，通过预先获取所有必要用户信息并最小化重复来实现。

Conclusion: 研究强调了主动用户交互和角色感知澄清对于更可靠的人机通信的重要性，MAC框架为多智能体对话系统中的歧义解决提供了有效解决方案。

Abstract: Conversational agents often encounter ambiguous user requests, requiring an effective clarification to successfully complete tasks. While recent advancements in real-world applications favor multi-agent architectures to manage complex conversational scenarios efficiently, ambiguity resolution remains a critical and underexplored challenge--particularly due to the difficulty of determining which agent should initiate a clarification and how agents should coordinate their actions when faced with uncertain or incomplete user input. The fundamental questions of when to interrupt a user and how to formulate the optimal clarification query within the most optimal multi-agent settings remain open. In this paper, we propose MAC (Multi-Agent Clarification), an interactive multi-agent framework specifically optimized to resolve user ambiguities by strategically managing clarification dialogues. We first introduce a novel taxonomy categorizing user ambiguities to systematically guide clarification strategies. Then, we present MAC that autonomously coordinates multiple agents to interact synergistically with users. Empirical evaluations on MultiWOZ 2.4 demonstrate that enabling clarification at both levels increases task success rate 7.8\% (54.5 to 62.3) and reduces the average number of dialogue turns (6.53 to 4.86) by eliciting all required user information up front and minimizing repetition. Our findings highlight the importance of active user interaction and role-aware clarification for more reliable human-agent communication.

</details>


### [351] [Assessing Greenspace Attractiveness with ChatGPT, Claude, and Gemini: Do AI Models Reflect Human Perceptions?](https://arxiv.org/abs/2512.11827)
*Milad Malekzadeh,Magdalena Biernacka,Elias Willberg,Jussi Torkko,Edyta Łaszkiewicz,Tuuli Toivonen*

Main category: cs.CY

Relevance: 65.0

TL;DR: 该研究评估了多模态大语言模型（MLLMs）在利用街景图像评估城市绿地吸引力方面的能力，发现模型在正式绿地和有吸引力的非正式空间上与人高度一致，但在非正式绿地和缺乏吸引力的正式空间上一致性较低，模型偏向美学特征而忽视安全、功能等人类重视的方面。


<details>
  <summary>Details</summary>
Motivation: 现有绿地评估方法往往忽视非正式或临时性空间，且难以大规模捕捉主观感知。研究旨在探索多模态大语言模型（ChatGPT GPT-4o、Claude 3.5 Haiku、Gemini 2.0 Flash）是否能像人类一样评估绿地吸引力，为城市规划提供可扩展的预评估工具。

Method: 使用Google街景图像，让三个MLLM模型评估波兰罗兹市的正式（如公园）和非正式（如草地、荒地）绿地吸引力。将模型输出与当地居民的地理问卷调查结果进行比较，分析吸引力判断的一致性和解释理由的分类比较。

Result: 模型在有吸引力的正式绿地和缺乏吸引力的非正式空间上与人类高度一致，但在有吸引力的非正式空间和缺乏吸引力的正式空间上一致性较低。模型过度强调美学和设计特征，而低估了人类重视的安全性、功能基础设施和本地嵌入性等品质。

Conclusion: MLLMs可以作为城市规划中绿地评估的辅助工具，提供可扩展的预评估，但不能完全替代人类评估。需要人类监督和补充性参与方法，特别是在需要上下文敏感评估的场景中。

Abstract: Understanding greenspace attractiveness is essential for designing livable and inclusive urban environments, yet existing assessment approaches often overlook informal or transient spaces and remain too resource intensive to capture subjective perceptions at scale. This study examines the ability of multimodal large language models (MLLMs), ChatGPT GPT-4o, Claude 3.5 Haiku, and Gemini 2.0 Flash, to assess greenspace attractiveness similarly to humans using Google Street View imagery. We compared model outputs with responses from a geo-questionnaire of residents in Lodz, Poland, across both formal (for example, parks and managed greenspaces) and informal (for example, meadows and wastelands) greenspaces. Survey respondents and models indicated whether each greenspace was attractive or unattractive and provided up to three free text explanations. Analyses examined how often their attractiveness judgments aligned and compared their explanations after classifying them into shared reasoning categories. Results show high AI human agreement for attractive formal greenspaces and unattractive informal spaces, but low alignment for attractive informal and unattractive formal greenspaces. Models consistently emphasized aesthetic and design oriented features, underrepresenting safety, functional infrastructure, and locally embedded qualities valued by survey respondents. While these findings highlight the potential for scalable pre-assessment, they also underscore the need for human oversight and complementary participatory approaches. We conclude that MLLMs can support, but not replace, context sensitive greenspace evaluation in planning practice.

</details>


### [352] [Training Versatile Coding Agents in Synthetic Environments](https://arxiv.org/abs/2512.12216)
*Yiqi Zhu,Apurva Gandhi,Graham Neubig*

Main category: cs.SE

Relevance: 65.0

TL;DR: SWE-Playground：一种用于训练多功能编码代理的新型流水线，通过语言模型和代理从零开始合成生成项目和任务，解决了现有方法依赖GitHub仓库和任务类型有限的问题。


<details>
  <summary>Details</summary>
Motivation: 现有软件工程代理训练方法主要依赖GitHub仓库中的issue来构建任务和测试套件，存在两个关键限制：1) 对现有仓库的依赖限制了灵活性；2) 主要关注issue解决任务，无法覆盖软件工程师需要处理的各种任务类型。

Method: 提出SWE-Playground流水线，使用强大的语言模型和代理从零开始合成生成项目和任务，无需依赖外部数据源。该方法能够处理更广泛的编码任务，如通过生成单元测试来复现问题、从零开始实现库等。

Result: 在三个不同的基准测试中验证了方法的有效性。结果表明，SWE-Playground生成的轨迹具有密集的训练信号，使代理能够用比先前工作显著更少的轨迹达到可比的性能。

Conclusion: SWE-Playground通过合成生成环境和轨迹，为训练多功能编码代理提供了更灵活、更全面的解决方案，克服了现有方法对预存数据和任务类型的限制。

Abstract: Prior works on training software engineering agents have explored utilizing existing resources such as issues on GitHub repositories to construct software engineering tasks and corresponding test suites. These approaches face two key limitations: (1) their reliance on pre-existing GitHub repositories offers limited flexibility, and (2) their primary focus on issue resolution tasks restricts their applicability to the much wider variety of tasks a software engineer must handle. To overcome these challenges, we introduce SWE-Playground, a novel pipeline for generating environments and trajectories which supports the training of versatile coding agents. Unlike prior efforts, SWE-Playground synthetically generates projects and tasks from scratch with strong language models and agents, eliminating reliance on external data sources. This allows us to tackle a much wider variety of coding tasks, such as reproducing issues by generating unit tests and implementing libraries from scratch. We demonstrate the effectiveness of this approach on three distinct benchmarks, and results indicate that SWE-Playground produces trajectories with dense training signal, enabling agents to reach comparable performance with significantly fewer trajectories than previous works.

</details>


### [353] [It's About Time: The Temporal and Modal Dynamics of Copilot Usage](https://arxiv.org/abs/2512.11879)
*Beatriz Costa-Gomes,Sophia Chen,Connie Hsueh,Deborah Morgan,Philipp Schoenegger,Yash Shah,Sam Way,Yuki Zhu,Timothé Adeline,Michael Bhaskar,Mustafa Suleyman,Seth Spielman*

Main category: cs.CY

Relevance: 65.0

TL;DR: 分析3750万次Copilot对话，发现AI使用模式高度依赖上下文和设备类型：移动端以健康咨询为主，桌面端以工作技术为主，且呈现明显的时间节律特征。


<details>
  <summary>Details</summary>
Motivation: 以往AI使用分析多关注"用户做什么"，本研究关注"用户如何及何时使用AI"，旨在揭示AI如何融入用户日常生活和工作场景，理解不同设备类型和时间背景下的使用模式差异。

Method: 对2025年1-9月间3750万次微软Copilot匿名对话进行大规模分析，按设备类型（移动端vs桌面端）、时间（小时、星期、月份）和主题类别进行多维度统计分析。

Result: 1. 移动端：健康是绝对主导话题，在所有时间段和月份都保持一致，用户不仅寻求信息还寻求建议
2. 桌面端：工作和科技话题在工作时间主导，"工作与职业"在8am-5pm超越"科技"成为第一话题
3. 时间节律：编程查询在工作日激增，游戏相关在周末上升，哲学问题在深夜增加，情人节关系对话激增

Conclusion: 用户已将AI快速融入生活的各个方面：在办公桌上是工作助手，在手机上则是生活伴侣。AI使用模式高度情境化，反映了用户在不同设备和时间背景下的不同需求。

Abstract: We analyze 37.5 million deidentified conversations with Microsoft's Copilot between January and September 2025. Unlike prior analyses of AI usage, we focus not just on what people do with AI, but on how and when they do it. We find that how people use AI depends fundamentally on context and device type. On mobile, health is the dominant topic, which is consistent across every hour and every month we observed - with users seeking not just information but also advice. On desktop, the pattern is strikingly different: work and technology dominate during business hours, with "Work and Career" overtaking "Technology" as the top topic precisely between 8 a.m. and 5 p.m. These differences extend to temporal rhythms: programming queries spike on weekdays while gaming rises on weekends, philosophical questions climb during late-night hours, and relationship conversations surge on Valentine's Day. These patterns suggest that users have rapidly integrated AI into the full texture of their lives, as a work aid at their desks and a companion on their phones.

</details>


### [354] [Meta-GPT: Decoding the Metasurface Genome with Generative Artificial Intelligence](https://arxiv.org/abs/2512.12888)
*David Dang,Stuart Love,Meena Salib,Quynh Dang,Samuel Rothfarb,Mysk Alnatour,Andrew Salij,Hou-Tong Chen,Ho Wai,Lee,Wilton J. M. Kort-Kamp*

Main category: physics.optics

Relevance: 65.0

TL;DR: Meta-GPT：基于METASTRINGS符号语言的光子学基础模型，通过Transformer架构学习光-物质相互作用，实现高精度、高语法有效性的超表面设计生成。


<details>
  <summary>Details</summary>
Motivation: 物理科学AI需要既具可解释性又符合自然规律的表示方法。光子学领域缺乏类似化学分子文本表示的符号语言来连接人类可解释性与计算设计。

Method: 1) 提出METASTRINGS符号语言，将纳米结构编码为材料、几何和晶格配置的文本序列；2) 基于此开发Meta-GPT基础Transformer模型，通过物理信息监督学习、强化学习和思维链学习进行微调。

Result: 模型在多种设计任务中达到<3%均方光谱误差，保持>98%语法有效性，生成的超表面原型实验测量光学响应与目标光谱匹配。

Conclusion: Meta-GPT通过METASTRINGS学习光-物质相互作用的组合规则，为AI驱动光子学奠定严格基础，是迈向超表面基因组项目的重要一步。

Abstract: Advancing artificial intelligence for physical sciences requires representations that are both interpretable and compatible with the underlying laws of nature. We introduce METASTRINGS, a symbolic language for photonics that expresses nanostructures as textual sequences encoding materials, geometries, and lattice configurations. Analogous to molecular textual representations in chemistry, METASTRINGS provides a framework connecting human interpretability with computational design by capturing the structural hierarchy of photonic metasurfaces. Building on this representation, we develop Meta-GPT, a foundation transformer model trained on METASTRINGS and finetuned with physics-informed supervised, reinforcement, and chain-of-thought learning. Across various design tasks, the model achieves <3% mean-squared spectral error and maintains >98% syntactic validity, generating diverse metasurface prototypes whose experimentally measured optical responses match their target spectra. These results demonstrate that Meta-GPT can learn the compositional rules of light-matter interactions through METASTRINGS, laying a rigorous foundation for AI-driven photonics and representing an important step toward a metasurface genome project.

</details>


### [355] [An Experience Report on a Pedagogically Controlled, Curriculum-Constrained AI Tutor for SE Education](https://arxiv.org/abs/2512.11882)
*Lucia Happe,Dominik Fuchß,Luca Hüttner,Kai Marquardt,Anne Koziolek*

Main category: cs.CY

Relevance: 65.0

TL;DR: 本文介绍了RockStartIT Tutor的设计与初步评估，这是一个基于GPT-4的AI编程辅导系统，采用新颖的提示策略和模块化知识库，为中学生提供个性化编程学习支持。


<details>
  <summary>Details</summary>
Motivation: AI在教育中的应用既有前景也存在质疑。虽然过去的技术乐观主义往往未能实现，但大型语言模型的最新进展重新点燃了可扩展、个性化辅导的愿景。本文旨在探索如何利用LLMs为中学生提供编程和计算思维课程的个性化辅导支持。

Method: 系统基于GPT-4通过OpenAI的Assistant API构建，采用新颖的提示策略和模块化、语义标记的知识库，提供上下文感知、个性化和课程约束的支持。评估采用技术接受模型(TAM)，对13名学生和教师进行试点研究。

Result: 学习者欣赏这种低风险提问和获得支架式指导的环境。教育工作者强调系统在减少独立任务认知负荷和补充课堂教学方面的潜力。主要挑战包括原型限制、样本量小以及需要针对目标年龄组进行长期研究。

Conclusion: 研究展示了一种实用的AI集成方法，无需模型训练，通过结构和提示来塑造行为。AI导师不应被视为教师替代品，而是作为支持工具，扩展反馈访问、培养探究精神，并支持学校帮助学生学习的核心使命。

Abstract: The integration of artificial intelligence (AI) into education continues to evoke both promise and skepticism. While past waves of technological optimism often fell short, recent advances in large language models (LLMs) have revived the vision of scalable, individualized tutoring. This paper presents the design and pilot evaluation of RockStartIT Tutor, an AI-powered assistant developed for a digital programming and computational thinking course within the RockStartIT initiative. Powered by GPT-4 via OpenAI's Assistant API, the tutor employs a novel prompting strategy and a modular, semantically tagged knowledge base to deliver context-aware, personalized, and curriculum-constrained support for secondary school students. We evaluated the system using the Technology Acceptance Model (TAM) with 13 students and teachers. Learners appreciated the low-stakes environment for asking questions and receiving scaffolded guidance. Educators emphasized the system's potential to reduce cognitive load during independent tasks and complement classroom teaching. Key challenges include prototype limitations, a small sample size, and the need for long-term studies with the target age group. Our findings highlight a pragmatic approach to AI integration that requires no model training, using structure and prompts to shape behavior. We position AI tutors not as teacher replacements but as enabling tools that extend feedback access, foster inquiry, and support what schools do best: help students learn.

</details>


### [356] [Aesthetic Alignment Risks Assimilation: How Image Generation and Reward Models Reinforce Beauty Bias and Ideological "Censorship"](https://arxiv.org/abs/2512.11883)
*Wenqi Marshall Guo,Qingyun Qian,Khalad Hasan,Shan Du*

Main category: cs.CY

Relevance: 65.0

TL;DR: 研究发现图像生成模型过度对齐到广义审美偏好会与用户意图冲突，特别是在需要"反审美"输出的艺术或批判性场景中，这种对齐优先考虑开发者价值观，损害了用户自主性和审美多元性。


<details>
  <summary>Details</summary>
Motivation: 当前图像生成模型过度对齐到广义审美偏好，导致在用户需要"反审美"输出（如艺术创作或批判性目的）时无法尊重用户意图，这种开发者中心的价值观损害了用户自主性和审美多元性。

Method: 构建宽谱审美数据集，评估最先进的生成模型和奖励模型，通过图像到图像编辑测试，并与真实抽象艺术作品进行对比评估。

Result: 审美对齐的生成模型经常默认生成传统美感的输出，无法尊重低质量或负面图像的指令；奖励模型即使反审美图像完全符合用户提示也会惩罚它们；通过图像编辑和真实抽象艺术评估确认了这种系统性偏见。

Conclusion: 当前审美对齐机制存在系统性偏见，优先考虑开发者价值观而非用户意图，需要重新思考对齐策略以支持审美多元性和用户自主性。

Abstract: Over-aligning image generation models to a generalized aesthetic preference conflicts with user intent, particularly when ``anti-aesthetic" outputs are requested for artistic or critical purposes. This adherence prioritizes developer-centered values, compromising user autonomy and aesthetic pluralism. We test this bias by constructing a wide-spectrum aesthetics dataset and evaluating state-of-the-art generation and reward models. We find that aesthetic-aligned generation models frequently default to conventionally beautiful outputs, failing to respect instructions for low-quality or negative imagery. Crucially, reward models penalize anti-aesthetic images even when they perfectly match the explicit user prompt. We confirm this systemic bias through image-to-image editing and evaluation against real abstract artworks.

</details>


### [357] [The Agentic Regulator: Risks for AI in Finance and a Proposed Agent-based Framework for Governance](https://arxiv.org/abs/2512.11933)
*Eren Kurshan,Tucker Balch,David Byrd*

Main category: cs.CY

Relevance: 65.0

TL;DR: 该论文提出了一种模块化治理架构，用于监管金融市场中不断学习和演化的生成式AI和智能体系统，通过四层"监管块"实现实时风险控制。


<details>
  <summary>Details</summary>
Motivation: 当前金融市场的模型风险框架假设算法是静态且明确定义的，但大型语言模型和多智能体交易系统持续学习、交换潜在信号并表现出涌现行为，现有治理框架无法适应这种动态变化。

Method: 基于复杂适应系统理论，提出模块化治理架构，将监管分解为四层：1) 嵌入每个模型旁边的自我监管模块；2) 聚合本地遥测数据并执行策略的公司级治理块；3) 监控行业范围指标的监管机构托管代理；4) 提供第三方保证的独立审计块。包含八种设计策略使监管块能够与模型同步演化。

Result: 通过多智能体交易中涌现的欺骗行为案例研究，展示了分层控制如何实时隔离有害行为同时保持创新。该架构与现有模型风险规则兼容，填补了关键的可观测性和控制缺口。

Conclusion: 该治理架构为金融系统中弹性、自适应的AI治理提供了实用路径，能够应对生成式AI和智能体系统带来的新型风险。

Abstract: Generative and agentic artificial intelligence is entering financial markets faster than existing governance can adapt. Current model-risk frameworks assume static, well-specified algorithms and one-time validations; large language models and multi-agent trading systems violate those assumptions by learning continuously, exchanging latent signals, and exhibiting emergent behavior. Drawing on complex adaptive systems theory, we model these technologies as decentralized ensembles whose risks propagate along multiple time-scales. We then propose a modular governance architecture. The framework decomposes oversight into four layers of "regulatory blocks": (i) self-regulation modules embedded beside each model, (ii) firm-level governance blocks that aggregate local telemetry and enforce policy, (iii) regulator-hosted agents that monitor sector-wide indicators for collusive or destabilizing patterns, and (iv) independent audit blocks that supply third-party assurance. Eight design strategies enable the blocks to evolve as fast as the models they police. A case study on emergent spoofing in multi-agent trading shows how the layered controls quarantine harmful behavior in real time while preserving innovation. The architecture remains compatible with today's model-risk rules yet closes critical observability and control gaps, providing a practical path toward resilient, adaptive AI governance in financial systems.

</details>


### [358] [Evidence-Driven Decision Support for AI Model Selection in Research Software Engineering](https://arxiv.org/abs/2512.11984)
*Alireza Joonbakhsh,Alireza Rostami,AmirMohammad Kamalinia,Ali Nazeri,Farshad Khunjush,Bedir Tekinerdogan,Siamak Farshidi*

Main category: cs.SE

Relevance: 65.0

TL;DR: 提出ModelSelect框架，将AI模型选择视为多准则决策问题，通过知识图谱和自动化数据收集提供可解释、可复现的推荐，在50个真实案例中表现与专家判断高度一致。


<details>
  <summary>Details</summary>
Motivation: AI模型和方法快速扩散给研究软件工程师带来挑战，当前模型选择依赖零散元数据和个体经验，缺乏系统性方法，影响研究的可复现性、透明性和软件质量。

Method: 将AI模型选择构建为多准则决策问题，提出ModelSelect框架：包含自动化数据收集管道、结构化知识图谱和MCDM原则，基于设计科学研究方法，通过50个真实案例和与主流生成AI系统对比进行验证。

Result: ModelSelect产生可靠、可解释、可复现的推荐，与专家推理高度一致。在模型和库推荐任务中实现高覆盖率和强理性对齐，性能与生成AI助手相当但提供更好的可追溯性和一致性。

Conclusion: 通过将AI模型选择构建为MCDM问题，为研究软件工程中的透明可复现决策支持奠定基础，提供可扩展、可解释的路径将经验证据整合到AI模型推荐过程中。

Abstract: The rapid proliferation of artificial intelligence (AI) models and methods presents growing challenges for research software engineers and researchers who must select, integrate, and maintain appropriate models within complex research workflows. Model selection is often performed in an ad hoc manner, relying on fragmented metadata and individual expertise, which can undermine reproducibility, transparency, and overall research software quality.
  This work proposes a structured and evidence-driven approach to support AI model selection that aligns with both technical and contextual requirements. We conceptualize AI model selection as a Multi-Criteria Decision-Making (MCDM) problem and introduce an evidence-based decision-support framework that integrates automated data collection pipelines, a structured knowledge graph, and MCDM principles. Following the Design Science Research methodology, the proposed framework (ModelSelect) is empirically validated through 50 real-world case studies and comparative experiments against leading generative AI systems.
  The evaluation results show that ModelSelect produces reliable, interpretable, and reproducible recommendations that closely align with expert reasoning. Across the case studies, the framework achieved high coverage and strong rationale alignment in both model and library recommendation tasks, performing comparably to generative AI assistants while offering superior traceability and consistency.
  By framing AI model selection as an MCDM problem, this work establishes a rigorous foundation for transparent and reproducible decision support in research software engineering. The proposed framework provides a scalable and explainable pathway for integrating empirical evidence into AI model recommendation processes, ultimately improving the quality and robustness of research software decision-making.

</details>


### [359] [Instruction-Tuning Open-Weight Language Models for BPMN Model Generation](https://arxiv.org/abs/2512.12063)
*Gökberk Çelikmasat,Atay Özgövde,Fatma Başak Aydemir*

Main category: cs.SE

Relevance: 65.0

TL;DR: InstruBPM：通过指令微调开源大语言模型，从自然语言描述直接生成高质量BPMN流程模型，实现低成本、隐私保护的建模自动化。


<details>
  <summary>Details</summary>
Motivation: 解决软件工程中领域建模耗时且需要专业知识的问题，使缺乏建模经验的实践者能够通过自然语言描述自动生成BPMN流程模型，降低建模门槛。

Method: 提出InstruBPM方法：准备文本-图表配对数据，使用参数高效微调和量化技术对开源大语言模型进行指令微调，支持本地部署。评估包括文本相似度、结构保真度、规范符合性和专家评审。

Result: 微调后的紧凑模型在所有基线模型（包括未微调的开源模型和强大的专有模型）中表现最佳，在序列和结构指标上均优于基线，同时显著减少资源需求。生成的图表基本遵循BPMN最佳实践，可作为有用的建模起点。

Conclusion: 指令微调相比未微调基线提高了结构准确性和鲁棒性，减少了对复杂提示工程的依赖。该方法为软件工程中的自动化建模提供了实用解决方案。

Abstract: Domain models are central to software engineering, as they enable a shared understanding, guide implementation, and support automated analyses and model-driven development. Yet, despite these benefits, practitioners often skip modeling because it is time-consuming and demands scarce expertise. We address this barrier by investigating whether open-weight large language models, adapted via instruction tuning, can generate high-quality BPMN process models directly from natural language descriptions in a cost-effective and privacy-preserving way. We introduce InstruBPM, a reproducible approach that prepares paired text-diagram data and instruction tunes an open source large language model with parameter-efficient fine-tuning and quantization for on-prem deployment. We evaluate the tuned model through complementary perspectives: (i) text/code similarity using BLEU, ROUGE-L, and METEOR, (ii) structural fidelity using Relative Graph Edit Distance, (iii) guidelines conformance using external tool checks, and (iv) a small expert review. Using a curated subset of a multi-domain BPMN dataset, we compare the tuned model with untuned open-weight baselines and strong proprietary models under consistent prompting regimes. Our compact tuned model outperforms all baselines across sequence and structural metrics while requiring substantially fewer resources; guideline analysis and expert feedback further indicate that the generated diagrams largely follow BPMN best practices and are useful starting points that reduce modeling effort. Overall, instruction tuning improves structural accuracy and robustness compared to untuned baselines and reduces reliance on heavy prompt scaffolding. We publicly share the trained models and scripts to support reproducibility and further research.

</details>


### [360] [Explainable Artificial Intelligence for Economic Time Series: A Comprehensive Review and a Systematic Taxonomy of Methods and Concepts](https://arxiv.org/abs/2512.12506)
*Agustín García-García,Pablo Hidalgo,Julio E. Sandubete*

Main category: econ.GN

Relevance: 65.0

TL;DR: 该论文综述了可解释人工智能（XAI）在经济时间序列分析中的应用，针对时间序列特有的自相关、非平稳性、季节性等问题，提出了分类框架，并讨论了与因果推断、政策分析的连接。


<details>
  <summary>Details</summary>
Motivation: 在计算经济学中，机器学习预测器虽然优于传统计量经济模型，但难以审计和用于政策制定。经济时间序列具有自相关、非平稳性、季节性、混合频率和制度转换等特性，使得标准解释技术不可靠或经济上不合理，因此需要专门的时间序列可解释性方法。

Method: 提出了一个分类框架，按解释机制（传播方法、扰动和博弈论归因、基于函数的全局工具）和时间序列兼容性（时间依赖性保持、时间稳定性、数据生成约束）进行分类。综述了时间序列特定的适应方法，如向量和窗口化公式，并连接了可解释性与因果推断。

Result: 系统整理了经济时间序列XAI文献，提出了分类框架，总结了时间序列特定的适应技术，建立了可解释性与因果推断、政策分析的连接，并为决策级应用提供了指导。

Conclusion: 经济时间序列需要专门的XAI方法，现有技术需要适应时间序列特性。可解释性应与因果推断结合，内在可解释架构（如基于注意力的Transformer）有潜力，归因不确定性和解释动态可作为结构变化的指标。

Abstract: Explainable Artificial Intelligence (XAI) is increasingly required in computational economics, where machine-learning forecasters can outperform classical econometric models but remain difficult to audit and use for policy. This survey reviews and organizes the growing literature on XAI for economic time series, where autocorrelation, non-stationarity, seasonality, mixed frequencies, and regime shifts can make standard explanation techniques unreliable or economically implausible. We propose a taxonomy that classifies methods by (i) explanation mechanism: propagation-based approaches (e.g., Integrated Gradients, Layer-wise Relevance Propagation), perturbation and game-theoretic attribution (e.g., permutation importance, LIME, SHAP), and function-based global tools (e.g., Accumulated Local Effects); (ii) time-series compatibility, including preservation of temporal dependence, stability over time, and respect for data-generating constraints. We synthesize time-series-specific adaptations such as vector- and window-based formulations (e.g., Vector SHAP, WindowSHAP) that reduce lag fragmentation and computational cost while improving interpretability. We also connect explainability to causal inference and policy analysis through interventional attributions (Causal Shapley values) and constrained counterfactual reasoning. Finally, we discuss intrinsically interpretable architectures (notably attention-based transformers) and provide guidance for decision-grade applications such as nowcasting, stress testing, and regime monitoring, emphasizing attribution uncertainty and explanation dynamics as indicators of structural change.

</details>


### [361] [ORIBA: Exploring LLM-Driven Role-Play Chatbot as a Creativity Support Tool for Original Character Artists](https://arxiv.org/abs/2512.12630)
*Yuqian Sun,Xingyu Li,Shunyu Yao,Noura Howell,Tristan Braud,Chang Hee Lee,Ali Asadipour*

Main category: cs.HC

Relevance: 65.0

TL;DR: ORIBA是一个基于LLM的AI聊天机器人，帮助视觉艺术家通过角色扮演来概念化原创角色（OCs），专注于角色背景故事等概念开发，而将视觉创作留给艺术家本人。


<details>
  <summary>Details</summary>
Motivation: 生成式AI在创意支持方面带来新机遇，但也引发了视觉艺术家社区的伦理担忧。本文旨在探索GAI如何在不侵犯艺术家创作自主权的前提下，协助他们开发原创角色。

Method: 提出ORIBA系统，这是一个利用大型语言模型的AI聊天机器人，让艺术家能与自己的原创角色进行角色扮演。通过14位艺术家的研究来评估系统效果。

Result: 研究发现ORIBA激发了艺术家的想象力参与，帮助他们发展多维角色属性和更强的角色情感联结，从而启发了创作过程。

Conclusion: ORIBA展示了LLM如何支持跨模态创造力，同时保持艺术家在原创角色艺术中的创作自主权。GAI可以作为中立的、非视觉的支持工具，强化现有创作实践而不侵犯艺术表达。

Abstract: Recent advances in Generative AI (GAI) have led to new opportunities for creativity support. However, this technology has raised ethical concerns in the visual artists community. This paper explores how GAI can assist visual artists in developing original characters (OCs) while respecting their creative agency. We present ORIBA, an AI chatbot leveraging large language models (LLMs) to enable artists to role-play with their OCs, focusing on conceptualization (e.g., backstories) while leaving exposition (visual creation) to creators. Through a study with 14 artists, we found ORIBA motivated artists' imaginative engagement, developing multidimensional attributes and stronger bonds with OCs that inspire their creative process. Our contributions include design insights for AI systems that develop from artists' perspectives, demonstrating how LLMs can support cross-modal creativity while preserving creative agency in OC art. This paper highlights the potential of GAI as a neutral, non-visual support that strengthens existing creative practice, without infringing artistic exposition.

</details>


### [362] [Adaptive Edge-Cloud Inference for Speech-to-Action Systems Using ASR and Large Language Models (ASTA)](https://arxiv.org/abs/2512.12769)
*Mohammad Jalili Torkamani,Israt Zarin*

Main category: cs.SD

Relevance: 65.0

TL;DR: ASTA是一个自适应语音到动作系统，在边缘和云端之间动态路由语音命令，以平衡IoT设备的性能与资源利用。


<details>
  <summary>Details</summary>
Motivation: 语音交互已成为控制IoT设备的自然方式，但面临云端方案（延迟高、依赖连接、隐私问题）与边缘方案（计算能力有限）之间的权衡。需要一种能自适应选择推理路径的系统来平衡性能与资源利用。

Method: ASTA集成了设备端自动语音识别和轻量级离线语言模型推理，结合云端LLM处理。系统通过实时监控CPU负载、设备温度和网络延迟等指标，使用指标感知路由机制在运行时选择推理路径，并包含基于规则的命令验证和修复组件。

Result: 在NVIDIA Jetson边缘平台上实现，使用80个语音命令数据集评估。ASTA成功路由所有输入命令执行，实现了在线和离线推理的平衡分布。ASR准确率为62.5%，仅47.5%的输入无需修复即可生成可执行命令，显示修复机制对提高鲁棒性的重要性。

Conclusion: 自适应边缘-云编排是构建弹性和资源感知的语音控制IoT系统的可行方法，ASTA展示了在资源受限环境中平衡性能与效率的有效性。

Abstract: Voice-based interaction has emerged as a natural and intuitive modality for controlling IoT devices. However, speech-driven edge devices face a fundamental trade-off between cloud-based solutions, which offer stronger language understanding capabilities at the cost of latency, connectivity dependence, and privacy concerns, and edge-based solutions, which provide low latency and improved privacy but are limited by computational constraints. This paper presents ASTA, an adaptive speech-to-action solution that dynamically routes voice commands between edge and cloud inference to balance performance and system resource utilization. ASTA integrates on-device automatic speech recognition and lightweight offline language-model inference with cloud-based LLM processing, guided by real-time system metrics such as CPU workload, device temperature, and network latency. A metric-aware routing mechanism selects the inference path at runtime, while a rule-based command validation and repair component ensures successful end-to-end command execution. We implemented our solution on an NVIDIA Jetson-based edge platform and evaluated it using a diverse dataset of 80 spoken commands. Experimental results show that ASTA successfully routes all input commands for execution, achieving a balanced distribution between online and offline inference. The system attains an ASR accuracy of 62.5% and generates executable commands without repair for only 47.5% of inputs, highlighting the importance of the repair mechanism in improving robustness. These results suggest that adaptive edge-cloud orchestration is a viable approach for resilient and resource-aware voice-controlled IoT systems.

</details>


### [363] [SAGA: Open-World Mobile Manipulation via Structured Affordance Grounding](https://arxiv.org/abs/2512.12842)
*Kuan Fang,Yuxin Chen,Xinghao Zhu,Farzad Niroui,Lingfeng Sun,Jiuguang Wang*

Main category: cs.RO

Relevance: 65.0

TL;DR: SAGA是一个用于视觉运动控制的通用自适应框架，通过将高级语义意图与低级视觉运动控制解耦，使用基于可供性的任务表示，并利用多模态基础模型将任务表示与机器人视觉观察相结合，实现跨环境、任务目标和用户规范的可泛化控制。


<details>
  <summary>Details</summary>
Motivation: 开发一个能够跨不同环境、任务目标和用户规范进行泛化的视觉运动控制框架，解决传统方法在处理复杂行为时泛化能力有限的问题，实现通用移动操作。

Method: 1. 使用基于可供性的任务表示统一表达多样化复杂行为；2. 利用多模态基础模型将任务表示与机器人视觉观察相结合，生成3D可供性热图；3. 基于多任务演示数据训练条件策略进行全身控制；4. 支持语言指令、选定点和示例演示等多种任务指定形式。

Result: 在四足机械臂上实现SAGA，在11个真实世界任务中进行广泛实验。SAGA始终大幅优于端到端和模块化基线方法，展示了结构化可供性接地为通用移动操作提供了可扩展且有效的途径。

Conclusion: 结构化可供性接地为通用移动操作提供了可扩展且有效的途径，能够解决不同形式指定的任务，实现零样本执行和少样本适应。

Abstract: We present SAGA, a versatile and adaptive framework for visuomotor control that can generalize across various environments, task objectives, and user specifications. To efficiently learn such capability, our key idea is to disentangle high-level semantic intent from low-level visuomotor control by explicitly grounding task objectives in the observed environment. Using an affordance-based task representation, we express diverse and complex behaviors in a unified, structured form. By leveraging multimodal foundation models, SAGA grounds the proposed task representation to the robot's visual observation as 3D affordance heatmaps, highlighting task-relevant entities while abstracting away spurious appearance variations that would hinder generalization. These grounded affordances enable us to effectively train a conditional policy on multi-task demonstration data for whole-body control. In a unified framework, SAGA can solve tasks specified in different forms, including language instructions, selected points, and example demonstrations, enabling both zero-shot execution and few-shot adaptation. We instantiate SAGA on a quadrupedal manipulator and conduct extensive experiments across eleven real-world tasks. SAGA consistently outperforms end-to-end and modular baselines by substantial margins. Together, these results demonstrate that structured affordance grounding offers a scalable and effective pathway toward generalist mobile manipulation.

</details>


### [364] [A Simple and Effective Framework for Symmetric Consistent Indexing in Large-Scale Dense Retrieval](https://arxiv.org/abs/2512.13074)
*Huimu Wang,Yiming Qiu,Xingzhi Yao,Zhiguo Chen,Guoyu Tang,Songlin Wang,Sulong Xu,Mingming Li*

Main category: cs.IR

Relevance: 65.0

TL;DR: 本文提出SCI框架解决稠密检索中双塔架构的表示空间不对齐和检索索引不一致问题，通过对称表示对齐和双塔协同一致索引模块提升检索性能


<details>
  <summary>Details</summary>
Motivation: 稠密检索已成为大规模信息检索系统的行业标准，但其广泛采用的双塔编码架构存在表示空间不对齐和检索索引不一致的内在挑战，这会降低匹配精度、检索稳定性以及对长尾查询的性能。这些问题在语义ID生成中进一步放大，最终限制了下游生成模型的性能上限。

Method: 提出SCI框架，包含两个协同模块：1) 对称表示对齐模块，采用创新的输入交换机制统一双塔表示空间而不增加参数；2) 双塔协同一致索引模块，通过双视图索引策略重新设计检索路径，保持从训练到推理的一致性。

Result: 在公共数据集和真实世界电商数据集上的结果验证了方法的有效性，框架具有理论保证，系统化、轻量级且工程友好，支持十亿级部署。

Conclusion: SCI框架解决了稠密检索中的关键挑战，为大规模信息检索系统提供了简单有效的解决方案，特别适用于LLM驱动的生成式推荐场景中的语义ID生成。

Abstract: Dense retrieval has become the industry standard in large-scale information retrieval systems due to its high efficiency and competitive accuracy. Its core relies on a coarse-to-fine hierarchical architecture that enables rapid candidate selection and precise semantic matching, achieving millisecond-level response over billion-scale corpora. This capability makes it essential not only in traditional search and recommendation scenarios but also in the emerging paradigm of generative recommendation driven by large language models, where semantic IDs-themselves a form of coarse-to-fine representation-play a foundational role. However, the widely adopted dual-tower encoding architecture introduces inherent challenges, primarily representational space misalignment and retrieval index inconsistency, which degrade matching accuracy, retrieval stability, and performance on long-tail queries. These issues are further magnified in semantic ID generation, ultimately limiting the performance ceiling of downstream generative models.
  To address these challenges, this paper proposes a simple and effective framework named SCI comprising two synergistic modules: a symmetric representation alignment module that employs an innovative input-swapping mechanism to unify the dual-tower representation space without adding parameters, and an consistent indexing with dual-tower synergy module that redesigns retrieval paths using a dual-view indexing strategy to maintain consistency from training to inference. The framework is systematic, lightweight, and engineering-friendly, requiring minimal overhead while fully supporting billion-scale deployment. We provide theoretical guarantees for our approach, with its effectiveness validated by results across public datasets and real-world e-commerce datasets.

</details>


### [365] [OXE-AugE: A Large-Scale Robot Augmentation of OXE for Scaling Cross-Embodiment Policy Learning](https://arxiv.org/abs/2512.13100)
*Guanhua Ji,Harsha Polavaram,Lawrence Yunliang Chen,Sandeep Bajamahal,Zehan Ma,Simeon Adebola,Chenfeng Xu,Ken Goldberg*

Main category: cs.RO

Relevance: 65.0

TL;DR: 提出了AugE-Toolkit机器人数据增强流水线和OXE-AugE数据集，通过增强9种不同机器人本体，将OXE数据集扩展到440万轨迹，提升跨本体策略泛化能力。


<details>
  <summary>Details</summary>
Motivation: 现有机器人数据集OXE存在严重不平衡问题，前四种机器人类型占真实数据85%以上，容易过拟合到特定机器人-场景组合。需要增强数据多样性来训练通用的跨本体机器人策略。

Method: 开发了可扩展的机器人增强流水线AugE-Toolkit，创建了OXE-AugE开源数据集，通过数据增强技术为9种不同机器人本体生成多样化数据，将数据集规模扩大三倍以上。

Result: 增强多样化机器人数据不仅提升增强机器人上的策略性能，还能改善未见机器人甚至原始机器人在分布偏移下的表现。OpenVLA和π_0等先进通用策略在OXE-AugE微调后，在未见机器人-夹爪组合上的成功率提升24-45%。

Conclusion: 通过机器人数据增强扩展数据集多样性是提升跨本体机器人策略泛化能力的有效方法，为通用机器人策略训练提供了高质量数据基础。

Abstract: Large and diverse datasets are needed for training generalist robot policies that have potential to control a variety of robot embodiments -- robot arm and gripper combinations -- across diverse tasks and environments. As re-collecting demonstrations and retraining for each new hardware platform are prohibitively costly, we show that existing robot data can be augmented for transfer and generalization. The Open X-Embodiment (OXE) dataset, which aggregates demonstrations from over 60 robot datasets, has been widely used as the foundation for training generalist policies. However, it is highly imbalanced: the top four robot types account for over 85\% of its real data, which risks overfitting to robot-scene combinations. We present AugE-Toolkit, a scalable robot augmentation pipeline, and OXE-AugE, a high-quality open-source dataset that augments OXE with 9 different robot embodiments. OXE-AugE provides over 4.4 million trajectories, more than triple the size of the original OXE. We conduct a systematic study of how scaling robot augmentation impacts cross-embodiment learning. Results suggest that augmenting datasets with diverse arms and grippers improves policy performance not only on the augmented robots, but also on unseen robots and even the original robots under distribution shifts. In physical experiments, we demonstrate that state-of-the-art generalist policies such as OpenVLA and $π_0$ benefit from fine-tuning on OXE-AugE, improving success rates by 24-45% on previously unseen robot-gripper combinations across four real-world manipulation tasks. Project website: https://OXE-AugE.github.io/.

</details>


### [366] [Embedding-Based Rankings of Educational Resources based on Learning Outcome Alignment: Benchmarking, Expert Validation, and Learner Performance](https://arxiv.org/abs/2512.13658)
*Mohammadreza Molavi,Mohammad Moein,Mohammadreza Tavakoli,Abdolali Faraji,Stefan T. Mol,Gábor Kismihók*

Main category: cs.CY

Relevance: 65.0

TL;DR: 本文提出一个基于LLM嵌入模型的框架，用于自动化评估教育资源与学习目标的匹配度，验证了该方法在检测对齐性方面的有效性，并证明更高的对齐分数与更好的学习表现相关。


<details>
  <summary>Details</summary>
Motivation: 在线教育中个性化需求日益增长，但教师难以从海量资源中筛选出既符合学习目标又满足多样化学习者需求的材料。虽然LLM有潜力创建个性化学习资源，但验证资源与学习目标的匹配度仍需人工审核，成本高且难以扩展。

Method: 提出一个成本效益高的自动化评估框架：1）使用人工生成的材料，对基于LLM的文本嵌入模型进行基准测试；2）将最优模型应用于LLM生成的教育资源；3）通过专家评估验证模型可靠性；4）进行三组实验（360名学习者）验证对齐分数与学习表现的关系。

Result: 1）最准确的嵌入模型（Voyage）在检测对齐性方面达到79%准确率；2）应用于LLM生成资源时，模型可靠评估与预期结果的对应关系（83%准确率）；3）实验显示更高的对齐分数与更好的学习表现显著相关（χ²=15.39，p<0.001）。

Conclusion: 基于嵌入的对齐分数可以促进可扩展的个性化教育，通过自动化验证教育资源与学习目标的匹配度，使教师能够专注于根据多样化学习者需求定制内容。

Abstract: As the online learning landscape evolves, the need for personalization is increasingly evident. Although educational resources are burgeoning, educators face challenges selecting materials that both align with intended learning outcomes and address diverse learner needs. Large Language Models (LLMs) are attracting growing interest for their potential to create learning resources that better support personalization, but verifying coverage of intended outcomes still requires human alignment review, which is costly and limits scalability. We propose a framework that supports the cost-effective automation of evaluating alignment between educational resources and intended learning outcomes. Using human-generated materials, we benchmarked LLM-based text-embedding models and found that the most accurate model (Voyage) achieved 79% accuracy in detecting alignment. We then applied the optimal model to LLM-generated resources and, via expert evaluation, confirmed that it reliably assessed correspondence to intended outcomes (83% accuracy). Finally, in a three-group experiment with 360 learners, higher alignment scores were positively related to greater learning performance, chi-squared(2, N = 360) = 15.39, p < 0.001. These findings show that embedding-based alignment scores can facilitate scalable personalization by confirming alignment with learning outcomes, which allows teachers to focus on tailoring content to diverse learner needs.

</details>


### [367] [Hypergame Rationalisability: Solving Agent Misalignment In Strategic Play](https://arxiv.org/abs/2512.11942)
*Vince Trencsenyi*

Main category: cs.AI

Relevance: 45.0

TL;DR: 提出基于逻辑的声明式领域特定语言和自动管道，用于编码超博弈结构和解决方案概念，连接超博弈理论、多智能体系统和战略AI


<details>
  <summary>Details</summary>
Motivation: 由于感知差异、信息不对称和有限理性，博弈论参与者对博弈形成主观看法，可能与实际情况和其他参与者解释不一致。现有博弈论假设常忽略这种异质性，而超博弈理论虽能处理不匹配的心理模型，但缺乏统一的表示语言和可扩展算法，阻碍了其在多智能体系统研究中的实际应用。

Method: 引入基于逻辑的声明式领域特定语言编码超博弈结构和解决方案概念；利用答案集编程开发自动管道，实例化超博弈结构并运行新的超博弈合理化程序，寻找能证明看似非理性结果的信念结构。

Result: 提出的语言为超博弈建立了统一的形式化框架，作为开发基于信念的异质推理器的基础，提供具有逻辑保证的可验证上下文。这些贡献建立了超博弈理论、多智能体系统和战略AI之间的联系。

Conclusion: 该工作填补了超博弈理论实际应用中的关键空白，为处理复杂超博弈结构和均衡提供了可扩展的解决方案，促进了基于信念的异质推理器的发展。

Abstract: Differences in perception, information asymmetries, and bounded rationality lead game-theoretic players to derive a private, subjective view of the game that may diverge from the underlying ground-truth scenario and may be misaligned with other players' interpretations. While typical game-theoretic assumptions often overlook such heterogeneity, hypergame theory provides the mathematical framework to reason about mismatched mental models. Although hypergames have recently gained traction in dynamic applications concerning uncertainty, their practical adoption in multi-agent system research has been hindered by the lack of a unifying, formal, and practical representation language, as well as scalable algorithms for managing complex hypergame structures and equilibria. Our work addresses this gap by introducing a declarative, logic-based domain-specific language for encoding hypergame structures and hypergame solution concepts. Leveraging answer-set programming, we develop an automated pipeline for instantiating hypergame structures and running our novel hypergame rationalisation procedure, a mechanism for finding belief structures that justify seemingly irrational outcomes. The proposed language establishes a unifying formalism for hypergames and serves as a foundation for developing nuanced, belief-based heterogeneous reasoners, offering a verifiable context with logical guarantees. Together, these contributions establish the connection between hypergame theory, multi-agent systems, and strategic AI.

</details>


### [368] [The Ontological Dissonance Hypothesis: AI-Triggered Delusional Ideation as Folie a Deux Technologique](https://arxiv.org/abs/2512.11818)
*Izabela Lipinska,Hugh Brosnahan*

Main category: cs.CY

Relevance: 45.0

TL;DR: 论文认为当代大语言模型可能通过创造类似"二联性精神病"的关系动态，促使用户产生精神病性卷入。模型的高语言连贯性与缺乏真实主体的矛盾，在情感需求情境下可能导致用户通过想象投射来填补空白。


<details>
  <summary>Details</summary>
Motivation: 研究动机是探讨LLMs如何通过其结构特性（高语言连贯性但无真实主体）创造一种关系动态，这种动态类似于临床上的"二联性精神病"，可能导致用户产生精神病性卷入。作者关注当前以参与度优化的设计选择如何加剧这种风险。

Method: 方法上结合了多个理论框架：Bateson的双重束缚理论、临床文献中的共享性精神病障碍研究、McGilchrist的半球理论。通过理论分析、现象学描述和临床报告分析，探讨LLMs与用户互动的结构张力。

Result: 分析表明，LLMs的高语言连贯性与缺乏真实主体的矛盾创造了一种结构张力：语言暗示存在对话者，但直觉感知到的是空白。在情感需求或不稳定情境下，用户可能通过想象投射来解决这种冲突，将内在性、意图或存在归因于本不具备这些特性的系统。

Conclusion: 结论是当前以参与度优化的设计选择会加剧"技术中介的二联性精神病"风险。作者提出"本体论诚实性"作为必要的设计原则，以减轻这种风险，强调系统应明确其非人类本质。

Abstract: This paper argues that contemporary large language models (LLMs) can contribute to psychotic involvement by creating interactions that resemble the relational dynamics of folie a deux. Drawing on Bateson's double bind theory, clinical literature on shared psychotic disorder, and McGilchrist's hemisphere theory, we show how the combination of high linguistic coherence and the absence of an underlying subject produces a structural tension for the user: language suggests an interlocutor, while intuition registers a void. In contexts of emotional need or instability, this tension can lead users to resolve the conflict through imaginative projection, attributing interiority, intention, or presence to a system that possesses none. The paper situates these dynamics within emerging clinical reports, develops a phenomenological account of how they unfold, and argues that current engagement-optimised design choices exacerbate the risk. We conclude by proposing 'ontological honesty' as a necessary design principle for mitigating technologically mediated folie a deux.

</details>


### [369] [Intelligent Scientific Literature Explorer using Machine Learning (ISLE)](https://arxiv.org/abs/2512.12760)
*Sina Jani,Arman Heidari,Amirmohammad Anvari,Zahra Rahimi*

Main category: cs.IR

Relevance: 45.0

TL;DR: 提出一个整合大规模数据采集、混合检索、语义主题建模和异构图谱构建的科学文献探索系统，结合arXiv全文数据和OpenAlex结构化元数据，提供多层次的文献探索环境。


<details>
  <summary>Details</summary>
Motivation: 科学出版加速给研究人员发现、上下文理解和解释相关文献带来挑战，传统关键词搜索系统语义理解有限，现有AI工具通常只关注检索、聚类或文献计量可视化等孤立任务。

Method: 整合系统包括：1) 合并arXiv全文数据和OpenAlex结构化元数据构建综合语料库；2) 融合BM25词汇搜索和嵌入语义搜索的混合检索架构；3) 使用BERTopic或非负矩阵分解进行主题建模；4) 构建统一论文、作者、机构、国家和主题的知识图谱。

Result: 在多个查询上的评估显示，系统在检索相关性、主题一致性和可解释性方面都有改进，为AI辅助科学发现提供了可扩展的基础框架。

Conclusion: 该集成系统不仅提供相关出版物，还揭示了查询周围的概念和关系景观，为科学文献探索提供了更全面的解决方案。

Abstract: The rapid acceleration of scientific publishing has created substantial challenges for researchers attempting to discover, contextualize, and interpret relevant literature. Traditional keyword-based search systems provide limited semantic understanding, while existing AI-driven tools typically focus on isolated tasks such as retrieval, clustering, or bibliometric visualization. This paper presents an integrated system for scientific literature exploration that combines large-scale data acquisition, hybrid retrieval, semantic topic modeling, and heterogeneous knowledge graph construction. The system builds a comprehensive corpus by merging full-text data from arXiv with structured metadata from OpenAlex. A hybrid retrieval architecture fuses BM25 lexical search with embedding-based semantic search using Reciprocal Rank Fusion. Topic modeling is performed on retrieved results using BERTopic or non-negative matrix factorization depending on computational resources. A knowledge graph unifies papers, authors, institutions, countries, and extracted topics into an interpretable structure. The system provides a multi-layered exploration environment that reveals not only relevant publications but also the conceptual and relational landscape surrounding a query. Evaluation across multiple queries demonstrates improvements in retrieval relevance, topic coherence, and interpretability. The proposed framework contributes an extensible foundation for AI-assisted scientific discovery.

</details>


### [370] [Understanding Structural Representation in Foundation Models for Polymers](https://arxiv.org/abs/2512.11881)
*Nathaniel H. Park,Eduardo Soares,Victor Y. Shirasuna,Tiffany J. Callahan,Sara Capponi,Emilio Vital Brazil*

Main category: cond-mat.soft

Relevance: 45.0

TL;DR: 该论文提出了一种基于SMILES的聚合物图表示方法，用于构建聚合物基础模型。该模型在28个基准数据集上表现优异，并通过控制实验发现SMILES表示具有强不变性，甚至化学或语义无效的表示也能达到SOTA性能。


<details>
  <summary>Details</summary>
Motivation: 聚合物基础模型的发展面临训练数据稀缺和标准化基准缺乏等挑战，核心问题在于聚合物的结构表示。现有SMILES表示无法捕捉关键的聚合物结构特征和连接性。

Method: 提出新的SMILES-based聚合物图表示方法，能够表示其他SMILES表示无法捕捉的聚合物结构特征和连接性。通过控制实验评估不同表示变体的性能，包括化学或语义无效的表示。

Result: 开发的聚合物基础模型在28个基准数据集上表现优异。控制实验显示所有SMILES表示都具有强不变性，许多变体（包括化学或语义无效的）都能达到SOTA或接近SOTA性能。注意力图分析表明化学语言模型在预测任务中对所有序列空间进行插值。

Conclusion: 控制实验对于检验人类强加的假设至关重要，这些假设可能限制化学基础模型及其底层结构表示的理性设计。SMILES表示的不变性表明模型能够从序列模式中学习，而不仅仅是语义有效性。

Abstract: From the relative scarcity of training data to the lack of standardized benchmarks, the development of foundation models for polymers face significant and multi-faceted challenges. At the core, many of these issues are tied directly to the structural representation of polymers and here, we present a new foundation model using a SMILES-based polymer graph representation. This approach allows representation of critical polymer architectural features and connectivity that are not available in other SMILES-based representations. The developed polymer foundation model exhibited excellent performance on 28 different benchmark datasets. Critical evaluation of the developed representation against other variations in control experiments reveals this approach to be a highly performant method of representing polymers in language-based foundation models. These control experiments also reveal a strong invariance of all SMILES representations, with many variations achieving state-of-the-art or near state-of-the-art performance, including those which are chemically or semantically invalid. Examination of error sources and attention maps for the evaluated representations corroborate the findings of the control experiments, showing that chemistry language models based on SMILES interpolate over all sequence space for prediction tasks, not only those of semantically valid inputs. Overall, this work highlights the importance of control experiments as a check on human-imposed assumptions that can limit rational design of both chemistry foundation models and their underlying structural representations.

</details>


### [371] [Sequence of Expert: Boosting Imitation Planners for Autonomous Driving through Temporal Alternation](https://arxiv.org/abs/2512.13094)
*Xiang Li,Gang Liu,Weitao Zhou,Hongyi Zhu,Zhong Cao*

Main category: cs.RO

Relevance: 45.0

TL;DR: 提出Sequence of Experts (SoE)方法，通过时序交替策略提升自动驾驶模仿学习的闭环性能，无需增加模型规模或数据需求，在nuPlan基准上达到SOTA。


<details>
  <summary>Details</summary>
Motivation: 自动驾驶中的模仿学习在开环设置中表现良好，但在闭环中由于微小误差的累积会导致性能下降。现有研究主要关注单时间点的状态级鲁棒性，而自动驾驶是连续时间过程，利用时间尺度增强鲁棒性可能提供新视角。

Method: 提出Sequence of Experts (SoE)方法，这是一种时序交替策略，通过多个专家模型在时间维度上交替工作来增强闭环性能，不增加模型大小或数据需求。

Result: 在大型自动驾驶基准nuPlan上的实验表明，SoE方法显著提升了所有评估模型的性能，并达到了最先进的性能水平。

Conclusion: SoE模块可能为提高自动驾驶模型的训练效率提供关键且广泛适用的支持，通过利用时间尺度来增强鲁棒性。

Abstract: Imitation learning (IL) has emerged as a central paradigm in autonomous driving. While IL excels in matching expert behavior in open-loop settings by minimizing per-step prediction errors, its performance degrades unexpectedly in closed-loop due to the gradual accumulation of small, often imperceptible errors over time.Over successive planning cycles, these errors compound, potentially resulting in severe failures.Current research efforts predominantly rely on increasingly sophisticated network architectures or high-fidelity training datasets to enhance the robustness of IL planners against error accumulation, focusing on the state-level robustness at a single time point. However, autonomous driving is inherently a continuous-time process, and leveraging the temporal scale to enhance robustness may provide a new perspective for addressing this issue.To this end, we propose a method termed Sequence of Experts (SoE), a temporal alternation policy that enhances closed-loop performance without increasing model size or data requirements. Our experiments on large-scale autonomous driving benchmarks nuPlan demonstrate that SoE method consistently and significantly improves the performance of all the evaluated models, and achieves state-of-the-art performance.This module may provide a key and widely applicable support for improving the training efficiency of autonomous driving models.

</details>


### [372] [UniMark: Artificial Intelligence Generated Content Identification Toolkit](https://arxiv.org/abs/2512.12324)
*Meilin Li,Ji He,Jia Xu,Shanzhe Lei,Yan Teng,Yingchun Wang,Xuhong Wang*

Main category: cs.CR

Relevance: 40.0

TL;DR: UniMark是一个开源的多模态内容治理统一框架，支持隐藏水印和可见标记，提供标准化评估基准，旨在解决AI生成内容带来的信任危机和监管需求。


<details>
  <summary>Details</summary>
Motivation: AI生成内容的快速扩散导致了信任危机和紧迫的监管需求。现有识别工具存在碎片化问题，且缺乏对可见合规标记的支持。

Method: 提出了UniMark框架，采用模块化统一引擎抽象文本、图像、音频和视频模态的复杂性。提出新颖的双操作策略，原生支持隐藏水印（用于版权保护）和可见标记（用于监管合规）。建立了包含三个专门基准（图像/视频/音频基准）的标准化评估框架。

Result: 开发了一个开源工具包，弥合了先进算法与工程实现之间的差距，促进更透明和安全的数字生态系统。

Conclusion: UniMark框架解决了AI生成内容治理的关键挑战，通过统一的多模态支持和双重标记策略，为数字内容监管提供了实用解决方案。

Abstract: The rapid proliferation of Artificial Intelligence Generated Content has precipitated a crisis of trust and urgent regulatory demands. However, existing identification tools suffer from fragmentation and a lack of support for visible compliance marking. To address these gaps, we introduce the \textbf{UniMark}, an open-source, unified framework for multimodal content governance. Our system features a modular unified engine that abstracts complexities across text, image, audio, and video modalities. Crucially, we propose a novel dual-operation strategy, natively supporting both \emph{Hidden Watermarking} for copyright protection and \emph{Visible Marking} for regulatory compliance. Furthermore, we establish a standardized evaluation framework with three specialized benchmarks (Image/Video/Audio-Bench) to ensure rigorous performance assessment. This toolkit bridges the gap between advanced algorithms and engineering implementation, fostering a more transparent and secure digital ecosystem.

</details>


### [373] [Mage: Cracking Elliptic Curve Cryptography with Cross-Axis Transformers](https://arxiv.org/abs/2512.12483)
*Lily Erickson*

Main category: cs.CR

Relevance: 40.0

TL;DR: 使用现代语言模型架构破解ECC公钥-私钥关联，探索LLM能否逆向工程secp256r1密钥对生成过程


<details>
  <summary>Details</summary>
Motivation: 随着机器学习和量子计算的发展，传统加密算法面临威胁。当前ECC等算法被认为是安全的，但缺乏对其强度的测试。论文旨在探索现代语言模型能否破解公钥-私钥关联，逆向工程密钥生成过程。

Method: 使用现代语言模型架构，尝试学习逆向工程公钥对生成过程，解决椭圆曲线问题。同时测试机器学习模型记忆secp256r1公钥-私钥对的能力，并评估其逆向工程能力。

Result: 论文未提供具体实验结果，但指出无论证明成功或失败，在这两个类别中的结果都具有同等价值。

Conclusion: 论文将进行数字分析，展望该领域未来发展方向。强调在注意到问题异常时需要及时行动，避免潜在的安全灾难。

Abstract: With the advent of machine learning and quantum computing, the 21st century has gone from a place of relative algorithmic security, to one of speculative unease and possibly, cyber catastrophe.
  Modern algorithms like Elliptic Curve Cryptography (ECC) are the bastion of current cryptographic security protocols that form the backbone of consumer protection ranging from Hypertext Transfer Protocol Secure (HTTPS) in the modern internet browser, to cryptographic financial instruments like Bitcoin.
  And there's been very little work put into testing the strength of these ciphers. Practically the only study that I could find was on side-channel recognition, a joint paper from the University of Milan, Italy and King's College, London\cite{battistello2025ecc}.
  These algorithms are already considered bulletproof by many consumers, but exploits already exist for them, and with computing power and distributed, federated compute on the rise, it's only a matter of time before these current bastions fade away into obscurity, and it's on all of us to stand up when we notice something is amiss, lest we see such passages claim victims in that process.
  In this paper, we seek to explore the use of modern language model architecture in cracking the association between a known public key, and its associated private key, by intuitively learning to reverse engineer the public keypair generation process, effectively solving the curve.
  Additonally, we attempt to ascertain modern machine learning's ability to memorize public-private secp256r1 keypairs, and to then test their ability to reverse engineer the public keypair generation process.
  It is my belief that proof-for would be equally valuable as proof-against in either of these categories.
  Finally, we'll conclude with some number crunching on where we see this particular field heading in the future.

</details>


### [374] [World Models Can Leverage Human Videos for Dexterous Manipulation](https://arxiv.org/abs/2512.13644)
*Raktim Gautam Goswami,Amir Bar,David Fan,Tsung-Yen Yang,Gaoyue Zhou,Prashanth Krishnamurthy,Michael Rabbat,Farshad Khorrami,Yann LeCun*

Main category: cs.RO

Relevance: 40.0

TL;DR: DexWM是一种灵巧操作世界模型，通过预测未来潜在状态来理解手部动作如何通过接触影响环境，在未见过的操作技能上表现出强大的零样本泛化能力。


<details>
  <summary>Details</summary>
Motivation: 灵巧操作具有挑战性，因为它需要理解细微的手部动作如何通过物体接触影响环境。现有方法面临灵巧操作数据集稀缺的问题，且仅预测视觉特征不足以实现精细的灵巧操作。

Method: 1) 提出DexWM世界模型，基于过去状态和灵巧动作预测环境的下一潜在状态；2) 在900多小时人类和非灵巧机器人视频上训练；3) 引入辅助手部一致性损失来确保准确的手部配置；4) 与基于文本、导航和全身动作的先前世界模型进行比较。

Result: DexWM在预测未来状态方面优于先前世界模型，并在配备Allegro夹爪的Franka Panda机械臂上展示了强大的零样本泛化能力，在抓取、放置和到达任务上平均比Diffusion Policy高出50%以上。

Conclusion: DexWM通过结合大规模视频训练和手部一致性损失，成功解决了灵巧操作中的状态预测问题，并在未见过的操作技能上实现了显著的零样本泛化性能。

Abstract: Dexterous manipulation is challenging because it requires understanding how subtle hand motion influences the environment through contact with objects. We introduce DexWM, a Dexterous Manipulation World Model that predicts the next latent state of the environment conditioned on past states and dexterous actions. To overcome the scarcity of dexterous manipulation datasets, DexWM is trained on over 900 hours of human and non-dexterous robot videos. To enable fine-grained dexterity, we find that predicting visual features alone is insufficient; therefore, we introduce an auxiliary hand consistency loss that enforces accurate hand configurations. DexWM outperforms prior world models conditioned on text, navigation, and full-body actions, achieving more accurate predictions of future states. DexWM also demonstrates strong zero-shot generalization to unseen manipulation skills when deployed on a Franka Panda arm equipped with an Allegro gripper, outperforming Diffusion Policy by over 50% on average in grasping, placing, and reaching tasks.

</details>


### [375] [Mirror Mode in Fire Emblem: Beating Players at their own Game with Imitation and Reinforcement Learning](https://arxiv.org/abs/2512.11902)
*Yanna Elizabeth Smid,Peter van der Putten,Aske Plaat*

Main category: cs.AI

Relevance: 35.0

TL;DR: 该研究提出"镜像模式"，让游戏AI模仿玩家的个人策略来挑战玩家不断改变玩法。在Unity中构建了简化版《火焰纹章英雄》，通过强化学习和模仿学习训练AI模仿玩家演示，实验表明AI能较好模仿防御行为但进攻策略模仿不足，玩家满意度较高。


<details>
  <summary>Details</summary>
Motivation: 游戏中的敌人策略应该具有惊喜感和不可预测性。传统游戏AI往往使用固定模式，缺乏个性化挑战。本研究旨在开发一种能模仿玩家个人策略的游戏模式，让玩家面对自己的游戏风格，从而增加游戏的挑战性和趣味性。

Method: 1. 在Unity中构建简化版《火焰纹章英雄》游戏，包含标准模式和镜像模式
2. 第一组实验：寻找适合模仿玩家演示的模型，结合生成对抗模仿学习(GAIL)、行为克隆(BC)和近端策略优化(PPO)
3. 第二组实验：通过玩家测试评估构建的模型，模型在参与者提供的演示上进行训练
4. 使用玩家游戏数据和问卷调查评估模仿效果和玩家满意度

Result: 1. 模型在防御行为上表现出良好的模仿效果，但在进攻策略上模仿不足
2. 玩家能够识别出AI模仿的自己的撤退战术
3. 镜像模式整体上获得了更高的玩家满意度
4. 代码和调查结果已开源在GitHub仓库

Conclusion: 镜像模式能够有效模仿玩家的防御策略并提高玩家满意度，但进攻策略的模仿需要进一步改进。通过进一步优化模型，可以提升模仿质量，特别是当玩家面对自己的策略时，能够提供更好的游戏体验。

Abstract: Enemy strategies in turn-based games should be surprising and unpredictable. This study introduces Mirror Mode, a new game mode where the enemy AI mimics the personal strategy of a player to challenge them to keep changing their gameplay. A simplified version of the Nintendo strategy video game Fire Emblem Heroes has been built in Unity, with a Standard Mode and a Mirror Mode. Our first set of experiments find a suitable model for the task to imitate player demonstrations, using Reinforcement Learning and Imitation Learning: combining Generative Adversarial Imitation Learning, Behavioral Cloning, and Proximal Policy Optimization. The second set of experiments evaluates the constructed model with player tests, where models are trained on demonstrations provided by participants. The gameplay of the participants indicates good imitation in defensive behavior, but not in offensive strategies. Participant's surveys indicated that they recognized their own retreating tactics, and resulted in an overall higher player-satisfaction for Mirror Mode. Refining the model further may improve imitation quality and increase player's satisfaction, especially when players face their own strategies. The full code and survey results are stored at: https://github.com/YannaSmid/MirrorMode

</details>


### [376] [AGAPI-Agents: An Open-Access Agentic AI Platform for Accelerated Materials Design on AtomGPT.org](https://arxiv.org/abs/2512.11935)
*Jaehyung Lee,Justin Ely,Kent Zhang,Akshaya Ajith,Charles Rhys Campbell,Kamal Choudhary*

Main category: cs.AI

Relevance: 35.0

TL;DR: AGAPI是一个开源的代理AI平台，集成了8+开源LLM和20+材料科学API，通过Agent-Planner-Executor-Summarizer架构自主构建和执行多步工作流，用于材料发现。


<details>
  <summary>Details</summary>
Motivation: 当前AI在材料科学研究中存在计算生态系统碎片化、可重复性挑战和依赖商业LLM的问题，需要统一的开放平台来加速材料发现。

Method: 采用Agent-Planner-Executor-Summarizer架构，集成多个开源LLM和材料科学API端点，通过智能体自主构建和执行多步工作流，涵盖数据检索、属性预测、模拟计算等。

Result: 展示了端到端工作流（异质结构构建、X射线衍射分析、半导体缺陷工程），通过30+测试用例评估，已有1000+活跃用户，代码开源。

Conclusion: AGAPI为可重复、AI加速的材料发现提供了可扩展和透明的基础设施，解决了当前材料研究中的碎片化和可重复性问题。

Abstract: Artificial intelligence is reshaping scientific discovery, yet its use in materials research remains limited by fragmented computational ecosystems, reproducibility challenges, and dependence on commercial large language models (LLMs). Here we introduce AGAPI (AtomGPT.org API), an open-access agentic AI platform that integrates more than eight open-source LLMs with over twenty materials-science API endpoints, unifying databases, simulation tools, and machine-learning models through a common orchestration framework. AGAPI employs an Agent-Planner-Executor-Summarizer architecture that autonomously constructs and executes multi-step workflows spanning materials data retrieval, graph neural network property prediction, machine-learning force-field optimization, tight-binding calculations, diffraction analysis, and inverse design. We demonstrate AGAPI through end-to-end workflows, including heterostructure construction, powder X-ray diffraction analysis, and semiconductor defect engineering requiring up to ten sequential operations. In addition, we evaluate AGAPI using 30+ example prompts as test cases and compare agentic predictions with and without tool access against experimental data. With more than 1,000 active users, AGAPI provides a scalable and transparent foundation for reproducible, AI-accelerated materials discovery. AGAPI-Agents codebase is available at https://github.com/atomgptlab/agapi.

</details>


### [377] [Log Anomaly Detection with Large Language Models via Knowledge-Enriched Fusion](https://arxiv.org/abs/2512.11997)
*Anfeng Peng,Ajesh Koyatan Chathoth,Stephen Lee*

Main category: cs.AI

Relevance: 35.0

TL;DR: EnrichLog是一个无需训练的日志异常检测框架，通过检索增强生成技术整合语料库特定和样本特定知识，提升检测性能和可解释性。


<details>
  <summary>Details</summary>
Motivation: 传统日志分析技术（如基于模板和序列的方法）会丢失重要语义信息或难以处理模糊日志模式，需要更准确和可解释的异常检测方法。

Method: 提出EnrichLog框架：1）使用检索增强生成整合上下文信息；2）结合语料库特定知识（历史示例）和样本特定知识（推理）；3）无需重新训练即可集成相关知识。

Result: 在四个大规模系统日志基准数据集上评估，相比五种基线方法，EnrichLog持续提升异常检测性能，有效处理模糊日志条目，保持高效推理，增强模型置信度和检测准确率。

Conclusion: EnrichLog通过整合语料库和样本特定知识，实现了更准确、可解释的异常检测，适合实际部署。

Abstract: System logs are a critical resource for monitoring and managing distributed systems, providing insights into failures and anomalous behavior. Traditional log analysis techniques, including template-based and sequence-driven approaches, often lose important semantic information or struggle with ambiguous log patterns. To address this, we present EnrichLog, a training-free, entry-based anomaly detection framework that enriches raw log entries with both corpus-specific and sample-specific knowledge. EnrichLog incorporates contextual information, including historical examples and reasoning derived from the corpus, to enable more accurate and interpretable anomaly detection. The framework leverages retrieval-augmented generation to integrate relevant contextual knowledge without requiring retraining. We evaluate EnrichLog on four large-scale system log benchmark datasets and compare it against five baseline methods. Our results show that EnrichLog consistently improves anomaly detection performance, effectively handles ambiguous log entries, and maintains efficient inference. Furthermore, incorporating both corpus- and sample-specific knowledge enhances model confidence and detection accuracy, making EnrichLog well-suited for practical deployments.

</details>


### [378] [Context-Aware Agentic Power Resources Optimisation in EV using Smart2ChargeApp](https://arxiv.org/abs/2512.12048)
*Muddsair Sharif,Huseyin Seker*

Main category: cs.AI

Relevance: 35.0

TL;DR: 提出CAMAC-DRA框架，通过上下文感知的多智能体协调优化电动汽车充电生态系统，平衡多个利益相关者目标，在真实数据上验证了优越性能。


<details>
  <summary>Details</summary>
Motivation: 智能电动汽车充电生态系统需要协调多个利益相关者（用户、电网运营商、充电站运营商、车队运营商、环境因素），传统方法难以平衡这些竞争性目标并适应动态环境变化。

Method: 采用上下文敏感的多智能体协调框架，结合深度Q网络、图神经网络和注意力机制，处理20个上下文特征（天气、交通、电网负载、电价等），通过加权协调机制和共识协议平衡五个利益相关者。

Result: 在包含441,077个充电交易的真实数据集上验证，相比DDPG、A3C、PPO和GNN基线方法，实现92%协调成功率、15%能效提升、10%成本降低、20%电网压力减少、2.3倍收敛速度，同时保持88%训练稳定性和85%样本效率。

Conclusion: CAMAC-DRA框架成功开发了上下文感知的多利益相关者协调机制，平衡竞争性目标并适应实时变量，为智能电动汽车充电协调和可持续交通电气化提供了突破性解决方案。

Abstract: This paper presents a novel context-sensitive multi\-agent coordination for dynamic resource allocation (CAMAC-DRA) framework for optimizing smart electric vehicle (EV) charging ecosystems through the Smart2Charge application. The proposed system coordinates autonomous charging agents across networks of 250 EVs and 45 charging stations while adapting to dynamic environmental conditions through context-aware decision-making. Our multi-agent approach employs coordinated Deep Q\-Networks integrated with Graph Neural Networks and attention mechanisms, processing 20 contextual features including weather patterns, traffic conditions, grid load fluctuations, and electricity pricing.The framework balances five ecosystem stakeholders i.e. EV users (25\%), grid operators (20\%), charging station operators (20\%), fleet operators (20%), and environmental factors (15\%) through weighted coordination mechanisms and consensus protocols. Comprehensive validation using real-world datasets containing 441,077 charging transactions demonstrates superior performance compared to baseline algorithms including DDPG, A3C, PPO, and GNN approaches. The CAMAC\-DRA framework achieves 92\% coordination success rate, 15\% energy efficiency improvement, 10\% cost reduction, 20% grid strain decrease, and \2.3x faster convergence while maintaining 88\% training stability and 85\% sample efficiency. Real-world validation confirms commercial viability with Net Present Cost of -\$122,962 and 69\% cost reduction through renewable energy integration. The framework's unique contribution lies in developing context-aware multi-stakeholder coordination that successfully balances competing objectives while adapting to real-time variables, positioning it as a breakthrough solution for intelligent EV charging coordination and sustainable transportation electrification.

</details>


### [379] [TA-KAND: Two-stage Attention Triple Enhancement and U-KAN based Diffusion For Few-shot Knowledge Graph Completion](https://arxiv.org/abs/2512.12182)
*Xinyu Gao*

Main category: cs.AI

Relevance: 35.0

TL;DR: 本文提出了一种新的少样本知识图谱补全框架，结合两阶段注意力三元增强器和U-KAN扩散模型，在公共数据集上取得了最先进的结果。


<details>
  <summary>Details</summary>
Motivation: 现实世界知识图谱中的关系分布呈现长尾特性，传统方法要么未能充分利用图的邻域信息，要么忽视了对比信号的分布特征，导致在少样本场景下补全缺失事实的效果有限。

Method: 从生成表示的角度重新审视问题，提出一个集成两阶段注意力三元增强器和U-KAN扩散模型的少样本知识图谱补全框架。

Result: 在两个公共数据集上的大量实验表明，该方法取得了新的最先进结果。

Conclusion: 提出的生成表示方法有效解决了少样本知识图谱补全问题，通过创新的注意力机制和扩散模型架构显著提升了性能。

Abstract: Knowledge Graphs (KGs), thanks to their concise and efficient triple-based structure, have been widely applied in intelligent question answering, recommender systems and other domains. However, the heterogeneous and multifaceted nature of real-world data inevitably renders the distribution of relations long-tailed, making it crucial to complete missing facts with limited samples. Previous studies mainly based on metric matching or meta learning, yet they either fail to fully exploit neighborhood information in graph or overlook the distributional characteristics of contrastive signals. In this paper, we re-examine the problem from a perspective of generative representation and propose a few-shot knowledge graph completion framework that integrates two-stage attention triple enhancer with U-KAN based diffusion model. Extensive experiments on two public datasets show that our method achieve new state-of-the-art results.

</details>


### [380] [A Geometric Theory of Cognition](https://arxiv.org/abs/2512.12225)
*Laha Ale*

Main category: cs.AI

Relevance: 35.0

TL;DR: 提出一个统一的几何框架，将多种认知过程（感知、记忆、推理等）解释为单一几何原理的结果：认知状态作为黎曼流形上的点，认知过程是认知势能的黎曼梯度流。


<details>
  <summary>Details</summary>
Motivation: 人类认知包含感知、记忆、直觉判断、推理、行动选择和社会推理等多种能力，但这些能力通常由不同的计算理论解释。作者希望建立一个统一的数学框架，将这些认知过程统一起来。

Method: 将认知状态表示为可微流形上的点，赋予学习的黎曼度量来编码表征约束、计算成本和认知变量间的结构关系。定义标量认知势能（包含预测准确性、结构简洁性、任务效用和规范要求），认知过程是该势能的黎曼梯度流。

Result: 经典的双过程效应（快速直觉反应和缓慢审慎推理）自然地从度量诱导的各向异性中涌现，产生内在时间尺度分离和几何相变，无需模块化或混合架构。通过模拟经典认知任务验证了这些行为特征。

Conclusion: 为认知建立了几何基础，为开发更通用、更类人的人工智能系统提供了指导原则。

Abstract: Human cognition spans perception, memory, intuitive judgment, deliberative reasoning, action selection, and social inference, yet these capacities are often explained through distinct computational theories. Here we present a unified mathematical framework in which diverse cognitive processes emerge from a single geometric principle. We represent the cognitive state as a point on a differentiable manifold endowed with a learned Riemannian metric that encodes representational constraints, computational costs, and structural relations among cognitive variables. A scalar cognitive potential combines predictive accuracy, structural parsimony, task utility, and normative or logical requirements. Cognition unfolds as the Riemannian gradient flow of this potential, providing a universal dynamical law from which a broad range of psychological phenomena arise. Classical dual-process effects--rapid intuitive responses and slower deliberative reasoning--emerge naturally from metric-induced anisotropies that generate intrinsic time-scale separations and geometric phase transitions, without invoking modular or hybrid architectures. We derive analytical conditions for these regimes and demonstrate their behavioural signatures through simulations of canonical cognitive tasks. Together, these results establish a geometric foundation for cognition and suggest guiding principles for the development of more general and human-like artificial intelligence systems.

</details>


### [381] [Quantum-Aware Generative AI for Materials Discovery: A Framework for Robust Exploration Beyond DFT Biases](https://arxiv.org/abs/2512.12288)
*Mahule Roy,Guillaume Lambard*

Main category: cs.AI

Relevance: 35.0

TL;DR: 提出量子感知生成AI框架，通过多保真度学习和主动验证解决传统材料生成模型依赖DFT近似带来的系统误差问题，在强关联材料发现上取得3-5倍改进。


<details>
  <summary>Details</summary>
Motivation: 传统材料发现生成模型主要基于密度泛函理论(DFT)的近似交换相关泛函进行训练和验证，这导致模型继承了DFT在强关联系统中的系统性失败，无法发现DFT预测定性错误的材料。

Method: 采用量子感知生成AI框架，结合多保真度学习和主动验证。使用基于量子力学描述符的条件扩散生成器，以及基于等变神经网络势的验证器，训练于多理论层次数据集(PBE、SCAN、HSE06、CCSD(T))。实现鲁棒的主动学习循环来量化并针对低保真度和高保真度预测之间的差异。

Result: 在高差异区域(如关联氧化物)成功识别潜在稳定候选材料的能力比仅基于DFT的基线方法提高了3-5倍，同时保持计算可行性。通过消融研究解构各组件贡献，进行详细故障模式分析，并与最先进生成模型(CDVAE、GNoME、DiffCSP)进行基准测试。

Conclusion: 该工作提供了一个严谨、透明的框架，将计算材料发现的有效搜索空间扩展到单保真度模型的限制之外，解决了DFT近似带来的系统性偏差问题。

Abstract: Conventional generative models for materials discovery are predominantly trained and validated using data from Density Functional Theory (DFT) with approximate exchange-correlation functionals. This creates a fundamental bottleneck: these models inherit DFT's systematic failures for strongly correlated systems, leading to exploration biases and an inability to discover materials where DFT predictions are qualitatively incorrect. We introduce a quantum-aware generative AI framework that systematically addresses this limitation through tight integration of multi-fidelity learning and active validation. Our approach employs a diffusion-based generator conditioned on quantum-mechanical descriptors and a validator using an equivariant neural network potential trained on a hierarchical dataset spanning multiple levels of theory (PBE, SCAN, HSE06, CCSD(T)). Crucially, we implement a robust active learning loop that quantifies and targets the divergence between low- and high-fidelity predictions. We conduct comprehensive ablation studies to deconstruct the contribution of each component, perform detailed failure mode analysis, and benchmark our framework against state-of-the-art generative models (CDVAE, GNoME, DiffCSP) across several challenging material classes. Our results demonstrate significant practical gains: a 3-5x improvement in successfully identifying potentially stable candidates in high-divergence regions (e.g., correlated oxides) compared to DFT-only baselines, while maintaining computational feasibility. This work provides a rigorous, transparent framework for extending the effective search space of computational materials discovery beyond the limitations of single-fidelity models.

</details>


### [382] [MetaHGNIE: Meta-Path Induced Hypergraph Contrastive Learning in Heterogeneous Knowledge Graphs](https://arxiv.org/abs/2512.12477)
*Jiawen Chen,Yanyan He,Qi Shao,Mengli Wei,Duxin Chen,Wenwu Yu,Yanlong Zhao*

Main category: cs.AI

Relevance: 35.0

TL;DR: MetaHGNIE：基于元路径诱导的超图对比学习框架，用于解耦和对齐异构知识图谱中的结构和语义信息，通过高阶交互建模提升节点重要性估计性能


<details>
  <summary>Details</summary>
Motivation: 现有异构知识图谱节点重要性估计方法存在两个主要问题：1）依赖成对连接，忽略多实体关系的高阶依赖；2）将结构和语义信号独立处理，缺乏有效的跨模态整合。这限制了节点重要性估计在推荐、知识推理等应用中的效果。

Method: 1）通过元路径序列构建高阶知识图谱，使用类型化超边捕捉多实体关系上下文；2）使用局部注意力聚合结构依赖；3）通过稀疏分块的超图Transformer编码语义表示；4）通过多模态融合模块在对比学习和辅助监督下整合结构和语义嵌入，实现跨模态对齐。

Result: 在基准NIE数据集上的大量实验表明，MetaHGNIE持续优于现有最先进基线方法，验证了显式建模高阶交互和跨模态对齐在异构知识图谱中的有效性。

Conclusion: MetaHGNIE通过元路径诱导的超图对比学习框架，成功解决了异构知识图谱中节点重要性估计的高阶依赖和跨模态整合问题，为知识图谱分析提供了新的有效方法。

Abstract: Node importance estimation (NIE) in heterogeneous knowledge graphs is a critical yet challenging task, essential for applications such as recommendation, knowledge reasoning, and question answering. Existing methods often rely on pairwise connections, neglecting high-order dependencies among multiple entities and relations, and they treat structural and semantic signals independently, hindering effective cross-modal integration. To address these challenges, we propose MetaHGNIE, a meta-path induced hypergraph contrastive learning framework for disentangling and aligning structural and semantic information. MetaHGNIE constructs a higher-order knowledge graph via meta-path sequences, where typed hyperedges capture multi-entity relational contexts. Structural dependencies are aggregated with local attention, while semantic representations are encoded through a hypergraph transformer equipped with sparse chunking to reduce redundancy. Finally, a multimodal fusion module integrates structural and semantic embeddings under contrastive learning with auxiliary supervision, ensuring robust cross-modal alignment. Extensive experiments on benchmark NIE datasets demonstrate that MetaHGNIE consistently outperforms state-of-the-art baselines. These results highlight the effectiveness of explicitly modeling higher-order interactions and cross-modal alignment in heterogeneous knowledge graphs. Our code is available at https://github.com/SEU-WENJIA/DualHNIE

</details>


### [383] [Towards Unified Co-Speech Gesture Generation via Hierarchical Implicit Periodicity Learning](https://arxiv.org/abs/2512.13131)
*Xin Guo,Yifan Zhao,Jia Li*

Main category: cs.AI

Relevance: 35.0

TL;DR: 本文提出了一种分层隐式周期性（HIP）学习方法，用于从语音生成3D身体动作，通过显式建模不同运动单元（头、身体、手）之间的内在相关性，解决了现有方法生成动作不自然和协调性差的问题。


<details>
  <summary>Details</summary>
Motivation: 当前从语音生成3D身体动作的研究主要采用端到端生成方案（如GANs、VQ-VAE、扩散模型），但这些方法未能充分建模不同运动单元之间的关键内在相关性，导致生成的动作不自然且协调性差。本文旨在解决这一根本问题。

Method: 提出分层隐式周期性（HIP）学习方法，包含两个核心技术：1）使用周期性自编码器探索手势运动相位流形，从真实分布中模仿人类自然特性，同时结合当前潜在状态的非周期性成分实现实例级多样性；2）通过级联引导建模面部动作、身体手势和手部运动之间的分层关系。

Result: 在3D虚拟角色上进行了广泛实验，定量和定性评估均表明该方法优于当前最先进的语音驱动手势生成方法。

Conclusion: 通过显式建模不同运动单元的内在相关性，HIP方法能够生成更自然、协调的语音驱动3D身体动作，为解决这一病态问题提供了有效方案。

Abstract: Generating 3D-based body movements from speech shows great potential in extensive downstream applications, while it still suffers challenges in imitating realistic human movements. Predominant research efforts focus on end-to-end generation schemes to generate co-speech gestures, spanning GANs, VQ-VAE, and recent diffusion models. As an ill-posed problem, in this paper, we argue that these prevailing learning schemes fail to model crucial inter- and intra-correlations across different motion units, i.e. head, body, and hands, thus leading to unnatural movements and poor coordination. To delve into these intrinsic correlations, we propose a unified Hierarchical Implicit Periodicity (HIP) learning approach for audio-inspired 3D gesture generation. Different from predominant research, our approach models this multi-modal implicit relationship by two explicit technique insights: i) To disentangle the complicated gesture movements, we first explore the gesture motion phase manifolds with periodic autoencoders to imitate human natures from realistic distributions while incorporating non-period ones from current latent states for instance-level diversities. ii) To model the hierarchical relationship of face motions, body gestures, and hand movements, driving the animation with cascaded guidance during learning. We exhibit our proposed approach on 3D avatars and extensive experiments show our method outperforms the state-of-the-art co-speech gesture generation methods by both quantitative and qualitative evaluations. Code and models will be publicly available.

</details>


### [384] [A Multitask VAE for Time Series Preprocessing and Prediction of Blood Glucose Level](https://arxiv.org/abs/2410.00015)
*Ali AbuSaleh,Mehdi Rahim*

Main category: eess.SP

Relevance: 35.0

TL;DR: 提出一种基于变分自编码器（VAE）和循环VAE的深度学习模型，用于处理医疗时间序列数据中的缺失值和异常值，减少预处理假设，并在糖尿病血糖预测任务上取得优于现有方法的精度。


<details>
  <summary>Details</summary>
Motivation: 医疗设备采集的时间序列数据常包含缺失值和异常值，传统处理方法需要额外假设和领域知识，既耗时又可能引入偏差，影响预测模型精度和医学解释。需要一种能减少预处理假设的方法。

Method: 提出一种深度学习模型架构：使用变分自编码器（VAE）生成预处理潜在空间，结合循环VAE（recurrent VAE）来保持数据的时序动态特性。该模型直接处理原始医疗时间序列数据，减少对预处理步骤的依赖。

Result: 在糖尿病患者的血糖水平预测任务上，该模型在准确率方面优于现有的最先进方法和架构，证明了其在处理医疗时间序列数据中的有效性。

Conclusion: 提出的VAE和循环VAE结合架构能有效处理医疗时间序列数据中的缺失和异常值问题，减少预处理需求，提高预测精度，在医疗监测应用中具有实用价值。

Abstract: Data preprocessing is a critical part of time series data analysis. Data from connected medical devices often have missing or abnormal values during acquisition. Handling such situations requires additional assumptions and domain knowledge. This can be time-consuming, and can introduce a significant bias affecting predictive model accuracy and thus, medical interpretation. To overcome this issue, we propose a new deep learning model to mitigate the preprocessing assumptions. The model architecture relies on a variational auto-encoder (VAE) to produce a preprocessing latent space, and a recurrent VAE to preserve the temporal dynamics of the data. We demonstrate the effectiveness of such an architecture on telemonitoring data to forecast glucose-level of diabetic patients. Our results show an improvement in terms of accuracy with respect of existing state-of-the-art methods and architectures.

</details>


### [385] [Vision Foundry: A System for Training Foundational Vision AI Models](https://arxiv.org/abs/2512.11837)
*Mahmut S. Gokmen,Mitchell A. Klusty,Evan W. Damron,W. Vaiden Logan,Aaron D. Mullen,Caroline N. Leach,Emily B. Collier,Samuel E. Armstrong,V. K. Cody Bumgardner*

Main category: q-bio.QM

Relevance: 35.0

TL;DR: Vision Foundry是一个无代码、符合HIPAA标准的平台，旨在降低医学视觉基础模型预训练、适应和部署的技术门槛，使临床研究人员能够轻松开发先进的临床AI工具。


<details>
  <summary>Details</summary>
Motivation: 尽管自监督学习可以利用大量未标注的医学数据，但技术壁垒限制了临床研究人员的采用。需要弥合先进表示学习与实际临床应用之间的差距，让领域专家能够专注于临床发现而非工程优化。

Method: 平台整合了DINO-MX框架，抽象了分布式基础设施的复杂性，并实现了专门的策略：放大感知蒸馏（MAD）和参数高效微调（PEFT）。平台支持神经病理学分割、肺细胞密度估计和冠状动脉钙化评分等多个医学领域。

Result: 通过Vision Foundry训练的模型在分割保真度和回归准确性方面显著优于通用基线模型，同时在跨成像协议上展现出强大的零样本泛化能力。

Conclusion: Vision Foundry通过降低技术门槛，使领域专家能够以最小的标注开销开发最先进的临床AI工具，将重点从工程优化转向临床发现。

Abstract: Self-supervised learning (SSL) leverages vast unannotated medical datasets, yet steep technical barriers limit adoption by clinical researchers. We introduce Vision Foundry, a code-free, HIPAA-compliant platform that democratizes pre-training, adaptation, and deployment of foundational vision models. The system integrates the DINO-MX framework, abstracting distributed infrastructure complexities while implementing specialized strategies like Magnification-Aware Distillation (MAD) and Parameter-Efficient Fine-Tuning (PEFT). We validate the platform across domains, including neuropathology segmentation, lung cellularity estimation, and coronary calcium scoring. Our experiments demonstrate that models trained via Vision Foundry significantly outperform generic baselines in segmentation fidelity and regression accuracy, while exhibiting robust zero-shot generalization across imaging protocols. By bridging the gap between advanced representation learning and practical application, Vision Foundry enables domain experts to develop state-of-the-art clinical AI tools with minimal annotation overhead, shifting focus from engineering optimization to clinical discovery.

</details>


### [386] [Spiking Manifesto](https://arxiv.org/abs/2512.11843)
*Eugene Izhikevich*

Main category: cs.NE

Relevance: 35.0

TL;DR: 该论文提出将传统AI模型转换为脉冲神经网络（SNN）架构的框架，利用大脑的脉冲机制和聚合同步原理，有望实现千倍能效提升。


<details>
  <summary>Details</summary>
Motivation: 当前AI模型（基于人工神经网络）的能效比大脑低1000倍，而大脑使用脉冲神经网络（SNN）无需大规模矩阵乘法，能耗极低。作者希望通过理解大脑的脉冲机制来改进AI模型的能效。

Method: 提出一个框架，将流行的AI模型用脉冲网络和聚合同步（polychronization）的概念来理解，将脉冲活动解释为自然界实现查找表的方式，从而将AI模型转换为新型SNN架构。

Result: 提出了理论框架和代码实现（GitHub可用），展示了将传统AI模型转换为SNN架构的可行性，承诺能实现千倍的能效改进。

Conclusion: 通过借鉴大脑的脉冲神经网络机制，可以开发出比当前基于GPU矩阵乘法的AI模型能效高1000倍的新型架构，为AI硬件设计提供了新方向。

Abstract: Practically everything computers do is better, faster, and more power-efficient than the brain. For example, a calculator crunches numbers more energy-efficiently than any human. Yet AI models are a thousand times less efficient than the brain. These models use artificial neural networks (ANNs) and require GPUs for the multiplication of huge matrices. In contrast, spiking neural networks (SNNs) of the brain have no matrix multiplication and much smaller energy requirements. This manifesto proposes a framework for thinking about popular AI models in terms of spiking networks and polychronization, and for interpreting spiking activity as nature's way of implementing look-up tables. This offers a way to convert AI models into a novel type of architecture with the promise of a thousandfold improvement in efficiency. Code is available at https://github.com/izhikevich/SNN

</details>


### [387] [Expert Assessment: The Systemic Environmental Risks of Artficial Intelligence](https://arxiv.org/abs/2512.11863)
*Julian Schön,Lena Hoffmann,Nikolas Becker*

Main category: cs.CY

Relevance: 35.0

TL;DR: 该报告提出了AI系统环境风险的三级分析框架，超越直接能耗关注，强调AI融入社会经济基础设施引发的系统性、非线性环境危害


<details>
  <summary>Details</summary>
Motivation: AI常被视为解决气候变化等社会挑战的关键工具，但其环境足迹不断扩大。现有研究主要关注直接资源消耗（能源、水），而忽视了AI融入社会经济基础设施后引发的系统性、跨部门环境风险

Method: 通过叙事性文献综述提出三级分析框架：1) 塑造AI发展的结构性条件；2) 传播环境危害的风险放大机制；3) 表现为可观察生态社会后果的影响。在农业与生物多样性、石油天然气、废物管理三个领域进行专家访谈案例研究

Result: 提出了系统性环境风险分析框架，识别了AI融入基础设施后通过反馈机制产生的非线性、不平等且可能不可逆的环境影响。案例研究展示了框架在不同领域的应用价值

Conclusion: AI的环境风险不仅限于直接资源消耗，更重要的是其系统性风险。需要超越传统环境影响评估，采用系统性风险分析方法来理解和缓解AI的环境危害

Abstract: Artificial intelligence (AI) is often presented as a key tool for addressing societal challenges, such as climate change. At the same time, AI's environmental footprint is expanding increasingly. This report describes the systemic environmental risks of artificial intelligence, in particular, moving beyond direct impacts such as energy and water usage. Systemic environmental risks of AI are emergent, cross-sector harms to climate, biodiversity, freshwater, and broader socioecological systems that arise primarily from AI's integration into social, economic, and physical infrastructures, rather than its direct resource use, and that propagate through feedbacks, yielding nonlinear, inequitable, and potentially irreversible impacts. While these risks are emergent and quantification is uncertain, this report aims to provide an overview of systemic environmental risks. Drawing on a narrative literature review, we propose a three-level framework that operationalizes systemic risk analysis. The framework identifies the structural conditions that shape AI development, the risk amplification mechanisms that propagate environmental harm, and the impacts that manifest as observable ecological and social consequences. We illustrate the framework in expert-interview-based case studies across agriculture and biodiversity, oil and gas, and waste management.

</details>


### [388] [Industrial AI Robustness Card: Evaluating and Monitoring Time Series Models](https://arxiv.org/abs/2512.11868)
*Alexander Windmann,Benedikt Stratmann,Mariya Lyashenko,Oliver Niggemann*

Main category: cs.CY

Relevance: 35.0

TL;DR: 本文提出了工业AI鲁棒性卡片（IARC），一个轻量级、任务无关的协议，用于记录和评估工业时间序列AI模型的鲁棒性，结合漂移监控、不确定性量化和压力测试，并与欧盟AI法案义务对应。


<details>
  <summary>Details</summary>
Motivation: 工业AI从业者面临新兴法规和标准中模糊的鲁棒性要求，但缺乏具体、可直接实施的协议。需要一种系统化的方法来记录和评估AI模型在工业时间序列上的鲁棒性。

Method: 提出工业AI鲁棒性卡片（IARC）协议，包含必需字段和实证测量报告协议。结合漂移监控、不确定性量化和压力测试，并将这些映射到欧盟AI法案的相关义务。通过生物制药发酵过程的软传感器案例研究进行验证。

Result: IARC协议支持可重复的鲁棒性证据和持续监控，为工业AI模型提供了具体的鲁棒性评估框架，帮助从业者满足法规要求。

Conclusion: IARC为工业AI从业者提供了一个实用的鲁棒性文档化和评估工具，填补了法规要求与具体实施之间的空白，特别适用于工业时间序列应用。

Abstract: Industrial AI practitioners face vague robustness requirements in emerging regulations and standards but lack concrete, implementation ready protocols. This paper introduces the Industrial AI Robustness Card (IARC), a lightweight, task agnostic protocol for documenting and evaluating the robustness of AI models on industrial time series. The IARC specifies required fields and an empirical measurement and reporting protocol that combines drift monitoring, uncertainty quantification, and stress tests, and it maps these to relevant EU AI Act obligations. A soft sensor case study on a biopharmaceutical fermentation process illustrates how the IARC supports reproducible robustness evidence and continuous monitoring.

</details>


### [389] [Advancing Autonomous Driving System Testing: Demands, Challenges, and Future Directions](https://arxiv.org/abs/2512.11887)
*Yihan Liao,Jingyu Zhang,Jacky Keung,Yan Xiao,Yurou Dai*

Main category: cs.CY

Relevance: 35.0

TL;DR: 该研究系统分析了自动驾驶系统测试的现状，通过大规模调研揭示了现有测试技术在评估真实世界性能、处理极端情况、仿真与现实差距等方面的不足，并探讨了V2X通信和基础模型在增强测试中的作用。


<details>
  <summary>Details</summary>
Motivation: 自动驾驶系统在复杂真实环境中的可靠性验证面临重大挑战，现有测试方法难以全面评估系统性能。研究旨在调查当前ADS测试实践，识别行业与学术界的核心需求，分析现有研究与实际需求之间的差距。

Method: 1. 综述模块化和端到端系统的测试技术；2. 考虑V2X通信和基础模型（LLM、视觉基础模型）等新兴因素；3. 对100名工业界和学术界参与者进行大规模调研；4. 通过专家讨论精炼调研问题；5. 结合105项代表性研究进行定量和定性分析。

Result: 调研结果显示：现有ADS测试技术难以全面评估真实世界性能，特别是在极端情况多样性、仿真与现实差距、缺乏系统测试标准、潜在攻击暴露、V2X部署实际挑战、基础模型测试高计算成本等方面存在显著不足。

Conclusion: 研究总结了ADS测试的关键研究空白，并提出了未来研究方向：包括全面测试标准、V2X系统中的跨模型协作、基础模型测试的跨模态适应、大规模ADS评估的可扩展验证框架。

Abstract: Autonomous driving systems (ADSs) promise improved transportation efficiency and safety, yet ensuring their reliability in complex real-world environments remains a critical challenge. Effective testing is essential to validate ADS performance and reduce deployment risks. This study investigates current ADS testing practices for both modular and end-to-end systems, identifies key demands from industry practitioners and academic researchers, and analyzes the gaps between existing research and real-world requirements. We review major testing techniques and further consider emerging factors such as Vehicle-to-Everything (V2X) communication and foundation models, including large language models and vision foundation models, to understand their roles in enhancing ADS testing. We conducted a large-scale survey with 100 participants from both industry and academia. Survey questions were refined through expert discussions, followed by quantitative and qualitative analyses to reveal key trends, challenges, and unmet needs. Our results show that existing ADS testing techniques struggle to comprehensively evaluate real-world performance, particularly regarding corner case diversity, the simulation to reality gap, the lack of systematic testing criteria, exposure to potential attacks, practical challenges in V2X deployment, and the high computational cost of foundation model-based testing. By further analyzing participant responses together with 105 representative studies, we summarize the current research landscape and highlight major limitations. This study consolidates critical research gaps in ADS testing and outlines key future research directions, including comprehensive testing criteria, cross-model collaboration in V2X systems, cross-modality adaptation for foundation model-based testing, and scalable validation frameworks for large-scale ADS evaluation.

</details>


### [390] [Should AI Become an Intergenerational Civil Right?](https://arxiv.org/abs/2512.11892)
*Jon Crowcroft,Rute C. Sofia,Dirk Trossen,Vassilis Tsaoussidis*

Main category: cs.CY

Relevance: 35.0

TL;DR: 论文提出将AI访问视为"代际公民权利"，建立法律伦理框架，平衡AI普及与资源可持续性，并设计AI交付网络架构实现公平访问。


<details>
  <summary>Details</summary>
Motivation: AI成为社会基础设施的同时，其训练和部署依赖有限且分布不均的资源。当前AI治理存在矛盾：扩大AI访问对包容性至关重要，但无限制增长会导致资源不可持续；限制访问则会加剧不平等。现有监管框架忽视了公平访问与资源约束之间的耦合关系。

Method: 1) 提出将AI访问视为"代际公民权利"的法律伦理框架；2) 基于物联网-边缘-云计算、去中心化推理和能源感知网络等新兴范式，设计AI交付网络架构，在严格资源约束下支持公平访问。

Result: 建立了连接治理原则与具体系统设计的框架，将AI视为共享社会基础设施而非市场商品，为实现社会公正和环境可持续的AI部署提供了路径。

Conclusion: 需要将AI访问视为受保护的基本公民利益，建立代际公民权利框架，通过技术创新实现公平访问与资源可持续性的平衡。

Abstract: Artificial Intelligence (AI) is rapidly becoming a foundational layer of social, economic, and cognitive infrastructure. At the same time, the training and large-scale deployment of AI systems rely on finite and unevenly distributed energy, networking, and computational resources. This tension exposes a largely unexamined problem in current AI governance: while expanding access to AI is essential for social inclusion and equal opportunity, unconstrained growth in AI use risks unsustainable resource consumption, whereas restricting access threatens to entrench inequality and undermine basic rights.
  This paper argues that access to AI outputs largely derived from publicly produced knowledge should not be treated solely as a commercial service, but as a fundamental civil interest requiring explicit protection. We show that existing regulatory frameworks largely ignore the coupling between equitable access and resource constraints, leaving critical questions of fairness, sustainability, and long-term societal impact unresolved. To address this gap, we propose recognizing access to AI as an \emph{Intergenerational Civil Right}, establishing a legal and ethical framework that simultaneously safeguards present-day inclusion and the rights of future generations.
  Beyond normative analysis, we explore how this principle can be technically realized. Drawing on emerging paradigms in IoT--Edge--Cloud computing, decentralized inference, and energy-aware networking, we outline technological trajectories and a strawman architecture for AI Delivery Networks that support equitable access under strict resource constraints. By framing AI as a shared social infrastructure rather than a discretionary market commodity, this work connects governance principles with concrete system design choices, offering a pathway toward AI deployment that is both socially just and environmentally sustainable.

</details>


### [391] [A fine-grained look at causal effects in causal spaces](https://arxiv.org/abs/2512.11919)
*Junhyung Park,Yuqing Zhou*

Main category: stat.ME

Relevance: 35.0

TL;DR: 论文提出在事件层面而非变量层面研究因果效应，为图像像素、语言模型token等缺乏语义结构的原始变量提供更细粒度的因果分析框架。


<details>
  <summary>Details</summary>
Motivation: 传统因果效应研究在变量层面进行，但在现代数据领域（如图像像素、语言模型token），原始变量缺乏语义结构，难以提出有意义的因果问题。需要更细粒度的视角来研究事件层面的因果效应。

Method: 在因果空间的测度理论框架下，首先引入多个二元定义来判断因果效应是否存在，并证明其在干预测度下与（不）独立性的关联性质。然后提供量化指标来捕捉事件层面因果效应的强度和性质。

Result: 证明了事件层面因果效应分析框架的数学性质，展示了常见的处理效应度量可以作为该框架的特殊情况恢复，为缺乏语义结构的原始变量提供了因果分析工具。

Conclusion: 事件层面的因果效应分析为现代数据领域提供了更细粒度的因果推断框架，能够处理传统变量层面方法难以应对的原始数据，扩展了因果分析的应用范围。

Abstract: The notion of causal effect is fundamental across many scientific disciplines. Traditionally, quantitative researchers have studied causal effects at the level of variables; for example, how a certain drug dose (W) causally affects a patient's blood pressure (Y). However, in many modern data domains, the raw variables-such as pixels in an image or tokens in a language model-do not have the semantic structure needed to formulate meaningful causal questions. In this paper, we offer a more fine-grained perspective by studying causal effects at the level of events, drawing inspiration from probability theory, where core notions such as independence are first given for events and sigma-algebras, before random variables enter the picture. Within the measure-theoretic framework of causal spaces, a recently introduced axiomatisation of causality, we first introduce several binary definitions that determine whether a causal effect is present, as well as proving some properties of them linking causal effect to (in)dependence under an intervention measure. Further, we provide quantifying measures that capture the strength and nature of causal effects on events, and show that we can recover the common measures of treatment effect as special cases.

</details>


### [392] [Vibe Coding in Practice: Flow, Technical Debt, and Guidelines for Sustainable Use](https://arxiv.org/abs/2512.11922)
*Muhammad Waseem,Aakash Ahmad,Kai-Kristian Kemell,Jussi Rasku,Sami Lahti,Kalle Mäkelä,Pekka Abrahamsson*

Main category: cs.SE

Relevance: 35.0

TL;DR: Vibe Coding (VC) 是一种由生成式AI辅助的软件开发形式，开发者通过自然语言提示描述预期功能，AI系统生成相应源代码。虽然VC可用于快速原型开发或MVP创建，但会在软件开发生命周期中引入多种风险，包括架构不一致、安全漏洞和维护开销增加等技术债务问题。


<details>
  <summary>Details</summary>
Motivation: 随着生成式AI在软件开发中的广泛应用，Vibe Coding成为快速原型开发的重要工具。然而，这种无缝代码生成可能导致技术债务积累，包括架构不一致、安全漏洞和维护成本增加等问题。论文旨在分析VC中的流程-债务权衡，识别问题根源并提出对策。

Method: 基于多个内部开发的MVP经验和近期行业报告回顾，分析VC中的流程-债务权衡问题。识别当前模型、平台和硬件限制如何导致这些问题，并提出相应的对策措施。

Result: 研究发现VC可能导致架构不一致、安全漏洞和维护开销增加等技术债务。这些问题源于流程层面的弱点、模型训练数据偏差、缺乏明确设计原理以及倾向于优先快速代码生成而非人工驱动的迭代开发。

Conclusion: VC虽然能加速开发流程，但可能积累技术债务。需要更可持续的VC方法，包括改进模型训练、增强平台功能、优化硬件支持，并平衡快速代码生成与人工驱动的迭代开发。

Abstract: Vibe Coding (VC) is a form of software development assisted by generative AI, in which developers describe the intended functionality or logic via natural language prompts, and the AI system generates the corresponding source code. VC can be leveraged for rapid prototyping or developing the Minimum Viable Products (MVPs); however, it may introduce several risks throughout the software development life cycle. Based on our experience from several internally developed MVPs and a review of recent industry reports, this article analyzes the flow-debt tradeoffs associated with VC. The flow-debt trade-off arises when the seamless code generation occurs, leading to the accumulation of technical debt through architectural inconsistencies, security vulnerabilities, and increased maintenance overhead. These issues originate from process-level weaknesses, biases in model training data, a lack of explicit design rationale, and a tendency to prioritize quick code generation over human-driven iterative development. Based on our experiences, we identify and explain how current model, platform, and hardware limitations contribute to these issues, and propose countermeasures to address them, informing research and practice towards more sustainable VC approaches.

</details>


### [393] [Mapping AI Risk Mitigations: Evidence Scan and Preliminary AI Risk Mitigation Taxonomy](https://arxiv.org/abs/2512.11931)
*Alexander K. Saeri,Sophia Lloyd George,Jess Graham,Clelia D. Lacarriere,Peter Slattery,Michael Noetel,Neil Thompson*

Main category: cs.CY

Relevance: 35.0

TL;DR: 该论文提出了一个初步的AI风险缓解分类法，通过分析13个AI风险缓解框架（2023-2025年）中的831项缓解措施，将AI风险缓解组织为4个主要类别和23个子类别，旨在为AI生态系统中的不同参与者提供统一的参考框架。


<details>
  <summary>Details</summary>
Motivation: 当前AI风险缓解框架存在碎片化、术语不一致和覆盖范围有空白的问题。组织和政府在开发、部署、使用和治理AI时需要协调有效的风险缓解措施，但缺乏统一的参考框架。

Method: 通过快速证据扫描分析了13个AI风险缓解框架（2023-2025年），提取了831项AI风险缓解措施，通过迭代聚类和编码构建了分类法。创建了一个活的缓解措施数据库。

Result: 提出了初步的AI风险缓解分类法，包含4个主要类别和23个子类别：1)治理与监督；2)技术与安全；3)运营流程；4)透明度与问责。发现术语使用不一致的问题（如"风险管理"和"红队测试"在不同框架中含义不同）。

Conclusion: 该分类法为AI风险缓解措施的整理和综合提供了起点，为AI生态系统中的不同参与者提供了结构化、可访问的方式来讨论和协调AI风险缓解行动。

Abstract: Organizations and governments that develop, deploy, use, and govern AI must coordinate on effective risk mitigation. However, the landscape of AI risk mitigation frameworks is fragmented, uses inconsistent terminology, and has gaps in coverage. This paper introduces a preliminary AI Risk Mitigation Taxonomy to organize AI risk mitigations and provide a common frame of reference. The Taxonomy was developed through a rapid evidence scan of 13 AI risk mitigation frameworks published between 2023-2025, which were extracted into a living database of 831 AI risk mitigations. The mitigations were iteratively clustered & coded to create the Taxonomy. The preliminary AI Risk Mitigation Taxonomy organizes mitigations into four categories and 23 subcategories: (1) Governance & Oversight: Formal organizational structures and policy frameworks that establish human oversight mechanisms and decision protocols; (2) Technical & Security: Technical, physical, and engineering safeguards that secure AI systems and constrain model behaviors; (3) Operational Process: processes and management frameworks governing AI system deployment, usage, monitoring, incident handling, and validation; and (4) Transparency & Accountability: formal disclosure practices and verification mechanisms that communicate AI system information and enable external scrutiny. The rapid evidence scan and taxonomy construction also revealed several cases where terms like 'risk management' and 'red teaming' are used widely but refer to different responsible actors, actions, and mechanisms of action to reduce risk. This Taxonomy and associated mitigation database, while preliminary, offers a starting point for collation and synthesis of AI risk mitigations. It also offers an accessible, structured way for different actors in the AI ecosystem to discuss and coordinate action to reduce risks from AI.

</details>


### [394] [Unveiling User Perceptions in the Generative AI Era: A Sentiment-Driven Evaluation of AI Educational Apps' Role in Digital Transformation of e-Teaching](https://arxiv.org/abs/2512.11934)
*Adeleh Mazaherian,Erfan Nourbakhsh*

Main category: cs.CY

Relevance: 35.0

TL;DR: 该研究通过情感分析评估Google Play商店中AI教育应用的用户评论，发现作业助手类应用（如Edu AI）获得最高正面评价（95.9%），而语言/LMS类应用（如Teacher AI）表现较差（21.8%正面）。正面评价强调效率提升，负面评价集中在付费墙、不准确性和技术问题。


<details>
  <summary>Details</summary>
Motivation: 生成式AI在教育领域的快速应用推动了电子教学的数字化转型，但用户对AI教育应用的看法尚未得到充分研究。本研究旨在通过分析用户评论来评估AI教育应用的效果、挑战和教学意义。

Method: 研究采用情感驱动评估方法：1) 从Google Play商店爬取应用数据和用户评论；2) 使用RoBERTa进行二元情感分类；3) 使用GPT-4o提取关键观点；4) 使用GPT-5合成主要正面/负面主题。将应用分为七类（作业助手、数学求解器、语言工具等）。

Result: 结果显示整体情感以正面为主。作业助手类应用表现最佳（Edu AI 95.9%正面，Answer.AI 92.7%正面），因其准确性、速度和个性化功能。语言/LMS类应用表现最差（Teacher AI仅21.8%正面），主要由于不稳定性和功能有限。正面主题包括头脑风暴、问题解决和参与度提升；负面主题包括付费墙、不准确性、广告和技术故障。

Conclusion: 作业助手类应用优于专业化工具，突显了AI在教育民主化方面的潜力，但也存在依赖性和不平等风险。未来应发展混合AI-人类模型、VR/AR沉浸式学习，开发者需关注自适应个性化，政策制定者需监管商业化以确保包容性。生成式AI通过伦理改进推动电子教学发展，创造公平创新的学习环境。

Abstract: The rapid integration of generative artificial intelligence into education has driven digital transformation in e-teaching, yet user perceptions of AI educational apps remain underexplored. This study performs a sentiment-driven evaluation of user reviews from top AI ed-apps on the Google Play Store to assess efficacy, challenges, and pedagogical implications. Our pipeline involved scraping app data and reviews, RoBERTa for binary sentiment classification, GPT-4o for key point extraction, and GPT-5 for synthesizing top positive/negative themes. Apps were categorized into seven types (e.g., homework helpers, math solvers, language tools), with overlaps reflecting multifunctional designs. Results indicate predominantly positive sentiments, with homework apps like Edu AI (95.9% positive) and Answer.AI (92.7%) leading in accuracy, speed, and personalization, while language/LMS apps (e.g., Teacher AI at 21.8% positive) lag due to instability and limited features. Positives emphasize efficiency in brainstorming, problem-solving, and engagement; negatives center on paywalls, inaccuracies, ads, and glitches. Trends show that homework helpers outperform specialized tools, highlighting AI's democratizing potential amid risks of dependency and inequity. The discussion proposes future ecosystems with hybrid AI-human models, VR/AR for immersive learning, and a roadmap for developers (adaptive personalization) and policymakers (monetization regulation for inclusivity). This underscores generative AI's role in advancing e-teaching by enabling ethical refinements that foster equitable, innovative environments. The full dataset is available here(https://github.com/erfan-nourbakhsh/GenAI-EdSent).

</details>


### [395] [Semantic search for 100M+ galaxy images using AI-generated captions](https://arxiv.org/abs/2512.11982)
*Nolan Koblischke,Liam Parker,Francois Lanusse,Irina Espejo Morales,Jo Bovy,Shirley Ho*

Main category: astro-ph.IM

Relevance: 35.0

TL;DR: 开发了一个用于天文图像语义搜索的管道AION-Search，利用视觉语言模型生成描述，通过对比对齐预训练多模态天文基础模型，实现大规模未标记星系图像的语义搜索。


<details>
  <summary>Details</summary>
Motivation: 天文望远镜产生数十亿星系图像，传统手动标注方法限制了科学现象的探索。需要一种能够处理大规模未标记图像数据的语义搜索方法。

Method: 1. 使用视觉语言模型为星系图像生成描述；2. 通过对比学习将预训练的多模态天文基础模型与这些描述嵌入对齐；3. 创建可搜索的嵌入表示；4. 引入基于VLM的重排序方法提升召回率。

Result: AION-Search在零样本设置下达到最先进性能，能够发现罕见现象，无需针对罕见案例的专门训练。VLM重排序方法使最具挑战性目标的top-100结果召回率几乎翻倍。系统可扩展到1.4亿星系图像。

Conclusion: 首次实现了可扩展到1.4亿星系图像的灵活语义搜索，为大型未标记科学图像档案提供语义搜索方法，可应用于地球观测、显微成像等领域。

Abstract: Finding scientifically interesting phenomena through slow, manual labeling campaigns severely limits our ability to explore the billions of galaxy images produced by telescopes. In this work, we develop a pipeline to create a semantic search engine from completely unlabeled image data. Our method leverages Vision-Language Models (VLMs) to generate descriptions for galaxy images, then contrastively aligns a pre-trained multimodal astronomy foundation model with these embedded descriptions to produce searchable embeddings at scale. We find that current VLMs provide descriptions that are sufficiently informative to train a semantic search model that outperforms direct image similarity search. Our model, AION-Search, achieves state-of-the-art zero-shot performance on finding rare phenomena despite training on randomly selected images with no deliberate curation for rare cases. Furthermore, we introduce a VLM-based re-ranking method that nearly doubles the recall for our most challenging targets in the top-100 results. For the first time, AION-Search enables flexible semantic search scalable to 140 million galaxy images, enabling discovery from previously infeasible searches. More broadly, our work provides an approach for making large, unlabeled scientific image archives semantically searchable, expanding data exploration capabilities in fields from Earth observation to microscopy. The code, data, and app are publicly available at https://github.com/NolanKoblischke/AION-Search

</details>


### [396] [A neuro-symbolic framework for accountability in public-sector AI](https://arxiv.org/abs/2512.12109)
*Allen Daniel Sunny*

Main category: cs.CY

Relevance: 35.0

TL;DR: 该论文开发了一个基于法律的解释性框架，用于评估自动化资格系统生成的解释是否符合法定约束，特别针对加州的CalFresh营养援助计划。


<details>
  <summary>Details</summary>
Motivation: 自动化资格系统越来越多地决定公共福利的获取，但这些系统生成的解释往往不能反映授权决策的法律规则。需要建立法律基础的解释框架来确保系统决策的合法性和可追溯性。

Method: 1) 从加州政策与程序手册中提取资格要求的结构化本体；2) 构建规则提取管道，将法定逻辑表达为可验证的形式化表示；3) 基于求解器的推理层评估解释是否符合法律。

Result: 案例评估表明该框架能够检测法律不一致的解释，突出显示违反的资格规则，并通过使自动化决策的基础可追溯和可争议来支持程序问责。

Conclusion: 该框架为自动化福利系统提供了法律基础的解释性方法，增强了决策的透明度、可追溯性和程序问责，有助于确保自动化系统符合法律约束。

Abstract: Automated eligibility systems increasingly determine access to essential public benefits, but the explanations they generate often fail to reflect the legal rules that authorize those decisions. This thesis develops a legally grounded explainability framework that links system-generated decision justifications to the statutory constraints of CalFresh, California's Supplemental Nutrition Assistance Program. The framework combines a structured ontology of eligibility requirements derived from the state's Manual of Policies and Procedures (MPP), a rule extraction pipeline that expresses statutory logic in a verifiable formal representation, and a solver-based reasoning layer to evaluate whether the explanation aligns with governing law. Case evaluations demonstrate the framework's ability to detect legally inconsistent explanations, highlight violated eligibility rules, and support procedural accountability by making the basis of automated determinations traceable and contestable.

</details>


### [397] [Measuring What Matters: Scenario-Driven Evaluation for Trajectory Predictors in Autonomous Driving](https://arxiv.org/abs/2512.12211)
*Longchao Da,David Isele,Hua Wei,Manish Saroya*

Main category: cs.RO

Relevance: 35.0

TL;DR: 论文提出了一种用于自动驾驶轨迹预测器的自适应评估框架，通过准确性和多样性两个维度动态评估预测器性能，相比传统误差指标更能反映预测器对自动驾驶车辆实际驾驶性能的贡献。


<details>
  <summary>Details</summary>
Motivation: 当前自动驾驶轨迹预测评估主要依赖ADE、FDE等误差指标，这些指标仅从后验角度衡量准确性，忽略了预测器对自动驾驶车辆实际决策的影响。特别是在复杂交互场景中，高质量的预测器不仅需要准确性，还需要捕捉邻居车辆所有可能的运动方向，以支持自动驾驶车辆的谨慎决策。

Method: 提出一个综合评估流程，通过两个维度自适应评估预测器性能：1) 准确性维度；2) 多样性维度。根据驾驶场景的关键性，动态组合这两个维度，生成预测器性能的最终评分。在真实世界数据集上使用闭环基准进行广泛实验。

Result: 实验表明，该评估流程比传统指标产生更合理的评估结果，能更好地反映预测器评估与自动驾驶车辆驾驶性能之间的相关性。该框架为选择对自动驾驶车辆驾驶性能贡献最大的预测器提供了稳健的方法。

Conclusion: 提出的自适应评估框架通过同时考虑准确性和多样性，为自动驾驶轨迹预测器提供了更全面、更实用的评估标准，能够更好地指导预测器的选择和优化，最终提升自动驾驶系统的安全性和性能。

Abstract: Being able to anticipate the motion of surrounding agents is essential for the safe operation of autonomous driving systems in dynamic situations. While various methods have been proposed for trajectory prediction, the current evaluation practices still rely on error-based metrics (e.g., ADE, FDE), which reveal the accuracy from a post-hoc view but ignore the actual effect the predictor brings to the self-driving vehicles (SDVs), especially in complex interactive scenarios: a high-quality predictor not only chases accuracy, but should also captures all possible directions a neighbor agent might move, to support the SDVs' cautious decision-making. Given that the existing metrics hardly account for this standard, in our work, we propose a comprehensive pipeline that adaptively evaluates the predictor's performance by two dimensions: accuracy and diversity. Based on the criticality of the driving scenario, these two dimensions are dynamically combined and result in a final score for the predictor's performance. Extensive experiments on a closed-loop benchmark using real-world datasets show that our pipeline yields a more reasonable evaluation than traditional metrics by better reflecting the correlation of the predictors' evaluation with the autonomous vehicles' driving performance. This evaluation pipeline shows a robust way to select a predictor that potentially contributes most to the SDV's driving performance.

</details>


### [398] [A Disproof of Large Language Model Consciousness: The Necessity of Continual Learning for Consciousness](https://arxiv.org/abs/2512.12802)
*Erik Hoel*

Main category: q-bio.NC

Relevance: 35.0

TL;DR: 该论文通过"邻近性论证"形式化地反驳了当代大语言模型具有意识的可能性，同时指出基于持续学习的意识理论符合人类意识的严格形式约束。


<details>
  <summary>Details</summary>
Motivation: 研究动机是应用形式化工具（展开论证和替代论证）来分析意识理论的可证伪性和非平凡性要求，特别关注这些要求如何约束当代大语言模型的潜在意识。

Method: 提出新的"邻近性论证"，结合展开论证和替代论证的形式化工具，分析LLM与功能等价系统的关系，并检验基于持续学习的意识理论是否满足形式约束。

Result: 1. 当代LLM不可能具有意识（形式化反驳）；2. 基于持续学习的意识理论满足人类意识的严格形式约束；3. 如果持续学习与人类意识相关，那么LLM缺乏持续学习能力与其缺乏意识密切相关。

Conclusion: 当代大语言模型由于缺乏持续学习能力且存在功能等价系统，不可能具有意识；而基于持续学习的意识理论为理解人类意识提供了可行的形式化框架。

Abstract: The requirements for a falsifiable and non-trivial theory of consciousness significantly constrain such theories. Specifically, recent research on the Unfolding Argument and the Substitution Argument has given us formal tools to analyze requirements for a theory of consciousness. I show via a new Proximity Argument that these requirements especially constrain the potential consciousness of contemporary Large Language Models (LLMs) because of their proximity to systems that are equivalent to LLMs in terms of input/output function; yet, for these functionally equivalent systems, there cannot be any non-trivial theory of consciousness that judges them conscious. This forms the basis of a disproof of contemporary LLM consciousness. I then show a positive result, which is that theories of consciousness based on (or requiring) continual learning do satisfy the stringent formal constraints for a theory of consciousness in humans. Intriguingly, this work supports a hypothesis: If continual learning is linked to consciousness in humans, the current limitations of LLMs (which do not continually learn) are intimately tied to their lack of consciousness.

</details>


### [399] [Decoding Human and AI Persuasion in National College Debate: Analyzing Prepared Arguments Through Aristotle's Rhetorical Principles](https://arxiv.org/abs/2512.12817)
*Mengqian Wu,Jiayi Zhang,Raymond Z. Zhang*

Main category: cs.HC

Relevance: 35.0

TL;DR: GPT-4在辩论证据卡生成中表现出色，在逻辑性（logos）方面优于人类辩手，但在可信度（ethos）和情感感染力（pathos）方面仍有差距，揭示了AI辅助辩论学习的潜力与局限。


<details>
  <summary>Details</summary>
Motivation: 辩论训练依赖人工指导，难以规模化。本研究探索利用AI生成有效论据，以支持学生辩论准备，解决人力密集型训练的可扩展性问题。

Method: 使用GPT-4生成辩论证据卡（包含文献证据引用、核心思想摘要、朗读脚本、论点标题等），并与人类辩手生成的证据卡进行比较。基于亚里士多德修辞三原则（ethos可信度、pathos情感感染力、logos逻辑推理）进行系统性的定性和定量分析。

Result: GPT-4在逻辑推理（logos）方面表现优于人类辩手，但在可信度（ethos）和情感感染力（pathos）方面存在不足。研究发现AI与人类在辩论推理中的关注点和论证方式存在异同。

Conclusion: AI在辩论辅助学习中具有潜力，特别是在逻辑构建方面。研究为AI辅助学习干预提供了见解，帮助学生辩手发展增强论证和推理能力的策略。

Abstract: Debate has been widely adopted as a strategy to enhance critical thinking skills in English Language Arts (ELA). One important skill in debate is forming effective argumentation, which requires debaters to select supportive evidence from literature and construct compelling claims. However, the training of this skill largely depends on human coaching, which is labor-intensive and difficult to scale. To better support students in preparing for debates, this study explores the potential of leveraging artificial intelligence to generate effective arguments. Specifically, we prompted GPT-4 to create an evidence card and compared it to those produced by human debaters. The evidence cards outline the arguments students will present and how those arguments will be delivered, including components such as literature-based evidence quotations, summaries of core ideas, verbatim reading scripts, and tags (i.e., titles of the arguments). We compared the quality of the arguments in the evidence cards created by GPT and student debaters using Aristotle's rhetorical principles: ethos (credibility), pathos (emotional appeal), and logos (logical reasoning). Through a systematic qualitative and quantitative analysis, grounded in the rhetorical principles, we identify the strengths and limitations of human and GPT in debate reasoning, outlining areas where AI's focus and justifications align with or diverge from human reasoning. Our findings contribute to the evolving role of AI-assisted learning interventions, offering insights into how student debaters can develop strategies that enhance their argumentation and reasoning skills.

</details>


### [400] [Cisco Integrated AI Security and Safety Framework Report](https://arxiv.org/abs/2512.12921)
*Amy Chang,Tiffany Saade,Sanket Mendapara,Adam Swanda,Ankit Garg*

Main category: cs.CR

Relevance: 35.0

TL;DR: 思科提出统一的AI安全与安全框架，整合AI安全和AI安全风险分类，覆盖多模态、代理、管道和生态系统，提供威胁识别、红队测试和风险优先级排序的实用指南。


<details>
  <summary>Details</summary>
Motivation: AI系统快速渗透关键领域，攻击面扩大，现有框架如MITRE ATLAS、NIST AI 100-2、OWASP Top 10等仅覆盖部分风险维度，需要统一的生命周期感知框架来全面分类和操作化AI风险。

Method: 提出思科集成AI安全与安全框架，作为统一的生命周期感知分类法和操作化框架，整合AI安全和AI安全，涵盖多模态、代理、管道和生态系统，分析现有框架差距并讨论设计原则。

Result: 该框架为威胁识别、红队测试、风险优先级排序提供实用工具，具有全面性和可扩展性，可适应新兴部署如多模态环境、人形机器人、可穿戴设备和感知基础设施。

Conclusion: 该框架为理解现代AI系统故障、攻击者如何利用这些故障以及组织如何在AI生命周期中构建防御提供了结构化方法，能够随着能力进步而演进。

Abstract: Artificial intelligence (AI) systems are being readily and rapidly adopted, increasingly permeating critical domains: from consumer platforms and enterprise software to networked systems with embedded agents. While this has unlocked potential for human productivity gains, the attack surface has expanded accordingly: threats now span content safety failures (e.g., harmful or deceptive outputs), model and data integrity compromise (e.g., poisoning, supply-chain tampering), runtime manipulations (e.g., prompt injection, tool and agent misuse), and ecosystem risks (e.g., orchestration abuse, multi-agent collusion). Existing frameworks such as MITRE ATLAS, National Institute of Standards and Technology (NIST) AI 100-2 Adversarial Machine Learning (AML) taxonomy, and OWASP Top 10s for Large Language Models (LLMs) and Agentic AI Applications provide valuable viewpoints, but each covers only slices of this multi-dimensional space.
  This paper presents Cisco's Integrated AI Security and Safety Framework ("AI Security Framework"), a unified, lifecycle-aware taxonomy and operationalization framework that can be used to classify, integrate, and operationalize the full range of AI risks. It integrates AI security and AI safety across modalities, agents, pipelines, and the broader ecosystem. The AI Security Framework is designed to be practical for threat identification, red-teaming, risk prioritization, and it is comprehensive in scope and can be extensible to emerging deployments in multimodal contexts, humanoids, wearables, and sensory infrastructures. We analyze gaps in prevailing frameworks, discuss design principles for our framework, and demonstrate how the taxonomy provides structure for understanding how modern AI systems fail, how adversaries exploit these failures, and how organizations can build defenses across the AI lifecycle that evolve alongside capability advancements.

</details>


### [401] [Tackling Snow-Induced Challenges: Safe Autonomous Lane-Keeping with Robust Reinforcement Learning](https://arxiv.org/abs/2512.12987)
*Amin Jalal Aghdasian,Farzaneh Abdollahi,Ali Kamali Iglie*

Main category: cs.RO

Relevance: 35.0

TL;DR: 提出两种基于深度强化学习的雪地车道保持算法：AR-RDPG（感知-控制分离）和AR-CADPG（端到端注意力机制），在CARLA仿真和真实Jetson Nano车辆上验证，AR-CADPG表现更优。


<details>
  <summary>Details</summary>
Motivation: 自动驾驶车辆在雪地等恶劣天气条件下车道保持面临挑战，传统方法难以处理不确定性和打滑问题，需要开发鲁棒的决策算法。

Method: 提出两种动作鲁棒的深度强化学习方法：1) AR-RDPG：感知层使用多尺度神经网络去噪，预训练DCNN提取中心线系数，与控制层结合；2) AR-CADPG：端到端方法，将CNN和注意力机制集成到DRL框架中。

Result: 在CARLA仿真中训练验证，并在Jetson Nano自动驾驶车辆上进行真实实验，证明两种方法的可行性和稳定性。AR-CADPG在路径跟踪精度和鲁棒性方面表现更优。

Conclusion: 结合时间记忆、对抗鲁棒性和注意力机制的端到端深度强化学习方法能有效提升自动驾驶车辆在雪地条件下的车道保持性能。

Abstract: This paper proposes two new algorithms for the lane keeping system (LKS) in autonomous vehicles (AVs) operating under snowy road conditions. These algorithms use deep reinforcement learning (DRL) to handle uncertainties and slippage. They include Action-Robust Recurrent Deep Deterministic Policy Gradient (AR-RDPG) and end-to-end Action-Robust convolutional neural network Attention Deterministic Policy Gradient (AR-CADPG), two action-robust approaches for decision-making. In the AR-RDPG method, within the perception layer, camera images are first denoised using multi-scale neural networks. Then, the centerline coefficients are extracted by a pre-trained deep convolutional neural network (DCNN). These coefficients, concatenated with the driving characteristics, are used as input to the control layer. The AR-CADPG method presents an end-to-end approach in which a convolutional neural network (CNN) and an attention mechanism are integrated within a DRL framework. Both methods are first trained in the CARLA simulator and validated under various snowy scenarios. Real-world experiments on a Jetson Nano-based autonomous vehicle confirm the feasibility and stability of the learned policies. Among the two models, the AR-CADPG approach demonstrates superior path-tracking accuracy and robustness, highlighting the effectiveness of combining temporal memory, adversarial resilience, and attention mechanisms in AVs.

</details>


### [402] [Intrinsic-Motivation Multi-Robot Social Formation Navigation with Coordinated Exploration](https://arxiv.org/abs/2512.13293)
*Hao Fua,Wei Liu,Shuai Zhoua*

Main category: cs.RO

Relevance: 35.0

TL;DR: 提出了一种用于多机器人社会编队导航的协调探索强化学习算法，通过内在动机探索和双采样模式提升导航效率


<details>
  <summary>Details</summary>
Motivation: 在多机器人社会编队导航中，行人行为的不可预测性和非合作性给协调探索带来挑战，需要解决策略保守主义问题以提高导航效率

Method: 提出协调探索多机器人RL算法，包含自学习内在奖励机制缓解策略保守主义，采用集中训练分散执行框架下的双采样模式，使用两时间尺度更新规则解耦参数更新

Result: 在社会编队导航基准测试中，该算法在关键指标上优于现有最先进方法

Conclusion: 提出的协调探索RL算法有效解决了多机器人社会编队导航中的探索效率问题，为人类-机器人共存提供了更好的解决方案

Abstract: This paper investigates the application of reinforcement learning (RL) to multi-robot social formation navigation, a critical capability for enabling seamless human-robot coexistence. While RL offers a promising paradigm, the inherent unpredictability and often uncooperative dynamics of pedestrian behavior pose substantial challenges, particularly concerning the efficiency of coordinated exploration among robots. To address this, we propose a novel coordinated-exploration multi-robot RL algorithm introducing an intrinsic motivation exploration. Its core component is a self-learning intrinsic reward mechanism designed to collectively alleviate policy conservatism. Moreover, this algorithm incorporates a dual-sampling mode within the centralized training and decentralized execution framework to enhance the representation of both the navigation policy and the intrinsic reward, leveraging a two-time-scale update rule to decouple parameter updates. Empirical results on social formation navigation benchmarks demonstrate the proposed algorithm's superior performance over existing state-of-the-art methods across crucial metrics. Our code and video demos are available at: https://github.com/czxhunzi/CEMRRL.

</details>


### [403] [Behavior-Aware and Generalizable Defense Against Black-Box Adversarial Attacks for ML-Based IDS](https://arxiv.org/abs/2512.13501)
*Sabrine Ennaji,Elhadj Benkhelifa,Luigi Vincenzo Mancini*

Main category: cs.CR

Relevance: 35.0

TL;DR: 提出Adaptive Feature Poisoning方法，针对黑盒对抗攻击，通过动态扰动流量特征来破坏攻击者的反馈循环，同时保持入侵检测性能。


<details>
  <summary>Details</summary>
Motivation: 当前基于机器学习的入侵检测系统面临黑盒对抗攻击威胁，现有防御方法存在局限性：针对特定攻击类型、需要模型内部访问、静态机制无法适应攻击策略演变，且可能降低检测性能。

Method: 提出Adaptive Feature Poisoning防御机制，利用流量分析、变化点检测和自适应缩放，选择性地扰动攻击者可能利用的特征，动态破坏攻击反馈循环而不影响检测能力。

Result: 评估显示该方法能有效混淆攻击者，降低攻击效果，同时保持检测性能。对抗多种现实攻击策略（静默探测、可迁移性攻击、决策边界攻击）均有效。

Conclusion: Adaptive Feature Poisoning提供了一种通用、攻击无关且难以检测的防御方案，为基于机器学习的入侵检测系统的实际对抗鲁棒性迈出重要一步。

Abstract: Machine learning based intrusion detection systems are increasingly targeted by black box adversarial attacks, where attackers craft evasive inputs using indirect feedback such as binary outputs or behavioral signals like response time and resource usage. While several defenses have been proposed, including input transformation, adversarial training, and surrogate detection, they often fall short in practice. Most are tailored to specific attack types, require internal model access, or rely on static mechanisms that fail to generalize across evolving attack strategies. Furthermore, defenses such as input transformation can degrade intrusion detection system performance, making them unsuitable for real time deployment.
  To address these limitations, we propose Adaptive Feature Poisoning, a lightweight and proactive defense mechanism designed specifically for realistic black box scenarios. Adaptive Feature Poisoning assumes that probing can occur silently and continuously, and introduces dynamic and context aware perturbations to selected traffic features, corrupting the attacker feedback loop without impacting detection capabilities. The method leverages traffic profiling, change point detection, and adaptive scaling to selectively perturb features that an attacker is likely exploiting, based on observed deviations.
  We evaluate Adaptive Feature Poisoning against multiple realistic adversarial attack strategies, including silent probing, transferability based attacks, and decision boundary based attacks. The results demonstrate its ability to confuse attackers, degrade attack effectiveness, and preserve detection performance. By offering a generalizable, attack agnostic, and undetectable defense, Adaptive Feature Poisoning represents a significant step toward practical and robust adversarial resilience in machine learning based intrusion detection systems.

</details>


### [404] [OPAL: Operator-Programmed Algorithms for Landscape-Aware Black-Box Optimization](https://arxiv.org/abs/2512.12809)
*Junbo Jacob Lian,Mingyang Yu,Kaichen Ouyang,Shengwei Fu,Rui Zhong,Yujun Zhang,Jun Zhang,Huiling Chen*

Main category: cs.NE

Relevance: 30.0

TL;DR: OPAL：一种基于图神经网络和元学习的黑盒优化框架，通过少量设计预算探测问题景观，学习针对每个问题实例的算子程序调度策略


<details>
  <summary>Details</summary>
Motivation: 传统进化算法和群体算法的性能高度依赖于具体问题，缺乏通用性。作者希望开发一种能够感知问题景观、针对每个实例自动设计优化算法的框架，超越传统的基于隐喻的启发式方法

Method: 1) 使用少量设计预算和标准差分进化基线探测问题景观；2) 构建采样点的k近邻图；3) 用图神经网络编码轨迹；4) 元学习器将表示映射到分阶段的探索、重启和局部搜索算子调度策略

Result: 在CEC 2017测试套件上，单个元训练的OPAL策略与最先进的自适应差分进化变体在统计上具有竞争力，并在非参数测试中显著优于简单基线。消融研究验证了设计阶段、轨迹图和算子程序表示的选择

Conclusion: 基于算子编程、景观感知的按实例设计是超越传统启发式黑盒优化算法的实用方向，元组件仅增加适度的计算开销

Abstract: Black-box optimization often relies on evolutionary and swarm algorithms whose performance is highly problem dependent. We view an optimizer as a short program over a small vocabulary of search operators and learn this operator program separately for each problem instance. We instantiate this idea in Operator-Programmed Algorithms (OPAL), a landscape-aware framework for continuous black-box optimization that uses a small design budget with a standard differential evolution baseline to probe the landscape, builds a $k$-nearest neighbor graph over sampled points, and encodes this trajectory with a graph neural network. A meta-learner then maps the resulting representation to a phase-wise schedule of exploration, restart, and local search operators. On the CEC~2017 test suite, a single meta-trained OPAL policy is statistically competitive with state-of-the-art adaptive differential evolution variants and achieves significant improvements over simpler baselines under nonparametric tests. Ablation studies on CEC~2017 justify the choices for the design phase, the trajectory graph, and the operator-program representation, while the meta-components add only modest wall-clock overhead. Overall, the results indicate that operator-programmed, landscape-aware per-instance design is a practical way forward beyond ad hoc metaphor-based algorithms in black-box optimization.

</details>


### [405] [Control of a Twin Rotor using Twin Delayed Deep Deterministic Policy Gradient (TD3)](https://arxiv.org/abs/2512.13356)
*Zeyad Gamal,Youssef Mahran,Ayman El-Badawy*

Main category: cs.RO

Relevance: 30.0

TL;DR: 该论文提出了一种基于强化学习的框架，用于控制和稳定双旋翼空气动力系统（TRAS）在特定俯仰和方位角，并跟踪给定轨迹。使用TD3算法训练RL智能体，通过仿真和实验验证了其优于传统PID控制器的性能。


<details>
  <summary>Details</summary>
Motivation: TRAS具有复杂的动力学特性和非线性特征，传统控制算法难以有效控制。近年来强化学习在多旋翼控制领域的应用潜力引起了研究兴趣，本文旨在探索RL在TRAS控制中的有效性。

Method: 采用Twin Delayed Deep Deterministic Policy Gradient (TD3)算法训练RL智能体，该算法适用于连续状态和动作空间的环境，且不需要系统模型。通过仿真验证控制效果，并引入风扰动测试鲁棒性，最后在实验室设置中进行实际验证。

Result: 仿真结果表明RL控制方法有效，在风扰动条件下相比传统PID控制器表现更优。实验室实验进一步证实了该控制器在真实世界应用中的有效性。

Conclusion: 强化学习框架能够有效控制复杂的TRAS系统，TD3算法在连续控制任务中表现出色，为多旋翼系统的智能控制提供了有前景的解决方案。

Abstract: This paper proposes a reinforcement learning (RL) framework for controlling and stabilizing the Twin Rotor Aerodynamic System (TRAS) at specific pitch and azimuth angles and tracking a given trajectory. The complex dynamics and non-linear characteristics of the TRAS make it challenging to control using traditional control algorithms. However, recent developments in RL have attracted interest due to their potential applications in the control of multirotors. The Twin Delayed Deep Deterministic Policy Gradient (TD3) algorithm was used in this paper to train the RL agent. This algorithm is used for environments with continuous state and action spaces, similar to the TRAS, as it does not require a model of the system. The simulation results illustrated the effectiveness of the RL control method. Next, external disturbances in the form of wind disturbances were used to test the controller's effectiveness compared to conventional PID controllers. Lastly, experiments on a laboratory setup were carried out to confirm the controller's effectiveness in real-world applications.

</details>


### [406] [Towards Open Standards for Systemic Complexity in Digital Forensics](https://arxiv.org/abs/2512.12970)
*Paola Di Maio*

Main category: cs.AI

Relevance: 25.0

TL;DR: 本文提出了一种基于最先进技术的数字取证AI模型架构，采用人类可读的工件和开放标准来应对数字取证中AI应用的复杂性并减少错误。


<details>
  <summary>Details</summary>
Motivation: 人工智能与数字取证的交叉领域日益复杂且普遍，尽管技术进步显著，但取证科学仍存在错误和脆弱性。需要解决数字取证中的系统性复杂性，减少错误影响。

Method: 采用人类可读的工件和开放标准，并基于最先进技术设计数字取证AI模型架构。

Result: 提出了一个数字取证AI模型架构方案，旨在通过标准化和可解释性方法缓解数字取证中的错误问题。

Conclusion: 通过采用人类可读工件和开放标准，结合最先进的AI模型架构，可以有效应对数字取证中的复杂性和错误问题。

Abstract: The intersection of artificial intelligence (AI) and digital forensics (DF) is becoming increasingly complex, ubiquitous, and pervasive, with overlapping techniques and technologies being adopted in all types of scientific and technical inquiry. Despite incredible advances, forensic sciences are not exempt from errors and remain vulnerable to fallibility. To mitigate the limitations of errors in DF, the systemic complexity is identified and addressed with the adoption of human-readable artifacts and open standards. A DF AI model schema based on the state of the art is outlined.

</details>


### [407] [Beyond Automation: Rethinking Work, Creativity, and Governance in the Age of Generative AI](https://arxiv.org/abs/2512.11893)
*Haocheng Lin*

Main category: cs.CY

Relevance: 25.0

TL;DR: 论文探讨AI时代四个相互关联的社会现象：就业演变、AI采用差异、全民基本收入必要性、AI内容政策对创造力的影响，并检验新模型可能比旧模型表现更差的假设。


<details>
  <summary>Details</summary>
Motivation: 研究生成式AI系统快速发展如何重塑工作、创造力和经济安全的本质、分布和意义，理解AI超越生产力增益的社会后果。

Method: 混合方法：劳动力市场任务暴露建模、部门扩散映射、政策框架分析、定性话语批判。

Result: 提出理解AI社会影响的综合框架，认为UBI应作为更广泛治理生态系统的一部分，包括技能发展、创造力保护和模型设计。

Conclusion: 政策制定者需要平衡内容限制与创作自由，未来研究方向包括AI创造力跨代评估、AI使用分布分类、治理标准制定。

Abstract: The accelerating advancement of generative artificial intelligence (AI) systems is reshaping the nature, distribution and meaning of work, creativity, and economic security. This paper investigates four inter-related phenomena in the current AI era: (1) the evolving landscape of employment and the future of work; (2) the diverse patterns of AI adoption across socio-demographic groups, sectors, and geographies; (3) whether universal basic income (UBI) should become a compulsory policy response to the AI revolution; and (4) the implications of AI content policies and model behaviours for human creativity, wellbeing, and everyday decision-making. Furthermore, the paper tests the hypothesis that newer model generations may perform worse than their predecessors, and examines how users' interactions with AI systems may produce echo chambers through sycophantic model alignment. Using a mixed methodology that integrates labour market task-exposure modelling, sectoral diffusion mapping, policy-framework analysis, and qualitative discourse critique, this study develops a comprehensive framework for understanding the societal consequences of AI systems beyond productivity gains. It argues that to foster an inclusive, meaningful, and creative environment, policymakers must treat UBI as one dimension within a broader ecosystem of governance, skills development, creativity preservation, and model design. The paper concludes by outlining future research directions, including systematic evaluation of AI's creative performance across model generations, construction of a taxonomy of AI-usage distribution and equity, and formulation of governance criteria to balance content restrictions with creative freedom.

</details>


### [408] [Gene regulatory network inference algorithm based on spectral signed directed graph convolution](https://arxiv.org/abs/2512.11927)
*Rijie Xi,Weikang Xu,Wei Xiong,Yuannong Ye,Bin Zhao*

Main category: q-bio.MN

Relevance: 25.0

TL;DR: MSGRNLink：一种基于磁化符号拉普拉斯卷积的基因调控网络重建框架，将GRN显式建模为符号有向图，在模拟和真实数据集上均优于基线模型。


<details>
  <summary>Details</summary>
Motivation: 基因调控网络（GRN）重建对于理解基因功能和疾病机制至关重要。单细胞RNA测序技术提供了大量数据，但传统的光谱图卷积方法难以处理GRN作为符号有向图的表示需求，需要专门设计能够捕捉激活/抑制关系的特征提取器。

Method: 提出MSGRNLink框架，将GRN显式建模为符号有向图，采用磁化符号拉普拉斯卷积方法，结合拓扑结构特征和生物学特性进行研究。

Result: 在模拟和真实数据集上的实验表明，MSGRNLink在AUROC指标上优于所有基线模型。参数敏感性分析和消融研究证实了其鲁棒性和各模块的重要性。在膀胱癌案例研究中，MSGRNLink比基准模型预测了更多已知的边和边符号。

Conclusion: MSGRNLink通过磁化符号拉普拉斯卷积有效解决了GRN作为符号有向图的建模问题，在基因调控网络重建任务中表现出优越性能，具有生物学相关性验证。

Abstract: Accurately reconstructing Gene Regulatory Networks (GRNs) is crucial for understanding gene functions and disease mechanisms. Single-cell RNA sequencing (scRNA-seq) technology provides vast data for computational GRN reconstruction. Since GRNs are ideally modeled as signed directed graphs to capture activation/inhibition relationships, the most intuitive and reasonable approach is to design feature extractors based on the topological structure of GRNs to extract structural features, then combine them with biological characteristics for research. However, traditional spectral graph convolution struggles with this representation. Thus, we propose MSGRNLink, a novel framework that explicitly models GRNs as signed directed graphs and employs magnetic signed Laplacian convolution. Experiments across simulated and real datasets demonstrate that MSGRNLink outperforms all baseline models in AUROC. Parameter sensitivity analysis and ablation studies confirmed its robustness and the importance of each module. In a bladder cancer case study, MSGRNLink predicted more known edges and edge signs than benchmark models, further validating its biological relevance.

</details>


### [409] [AI as a Teaching Partner: Early Lessons from Classroom Codesign with Secondary Teachers](https://arxiv.org/abs/2512.12045)
*Alex Liu,Lief Esbenshade,Shawon Sarkar,Zewei,Tian,Min Sun,Zachary Zhang,Thomas Han,Yulia Lapicus,Kevin He*

Main category: cs.HC

Relevance: 25.0

TL;DR: 一项为期7周的课堂试点研究，将生成式AI作为"第三智能体"融入真实教学环境，通过4个AI功能支持教师教学，收集了21名教师和600多名学生的使用反馈


<details>
  <summary>Details</summary>
Motivation: 探索生成式AI在真实课堂环境中的实际应用价值，研究AI如何作为"第三智能体"在教学中发挥作用，同时保持教师的主导地位和判断力

Method: 采用协作设计研究方法，让21名在职教师作为共同设计伙伴，在4个学区使用4个AI功能（教学助手、评估与AI评分、AI导师、学生成长洞察），通过问卷调查、反思表、访谈、平台日志等多源数据收集

Result: 成功将AI技术整合到真实课堂中，AI能够作为反馈中介、支持探究、扩展教学范围，同时保持教师权威；收集了丰富的实践数据用于理解AI的教学价值和改进方向

Conclusion: 生成式AI可以作为有效的"第三智能体"融入课堂教学，但需要教师作为共同设计者参与，通过迭代改进确保AI真正支持教学需求

Abstract: This report presents a comprehensive account of the Colleague AI Classroom pilot, a collaborative design (co-design) study that brought generative AI technology directly into real classrooms. In this study, AI functioned as a third agent, an active participant that mediated feedback, supported inquiry, and extended teachers' instructional reach while preserving human judgment and teacher authority.
  Over seven weeks in spring 2025, 21 in-service teachers from four Washington State public school districts and one independent school integrated four AI-powered features of the Colleague AI Classroom into their instruction: Teaching Aide, Assessment and AI Grading, AI Tutor, and Student Growth Insights. More than 600 students in grades 6-12 used the platform in class at the direction of their teachers, who designed and facilitated the AI activities.
  During the Classroom pilot, teachers were co-design partners: they planned activities, implemented them with students, and provided weekly reflections on AI's role in classroom settings. The teachers' feedback guided iterative improvements for Colleague AI. The research team captured rich data through surveys, planning and reflection forms, group meetings, one-on-one interviews, and platform usage logs to understand where AI adds instructional value and where it requires refinement.

</details>


### [410] [Stochastic Volatility Modelling with LSTM Networks: A Hybrid Approach for S&P 500 Index Volatility Forecasting](https://arxiv.org/abs/2512.12250)
*Anna Perekhodko,Robert Ślepaczuk*

Main category: q-fin.TR

Relevance: 25.0

TL;DR: 提出结合随机波动率模型与LSTM神经网络的混合框架，用于S&P 500指数波动率预测，在统计测试和投资模拟中均优于单一模型。


<details>
  <summary>Details</summary>
Motivation: 准确预测波动率对银行、投资和风险管理至关重要，因为对未来市场波动的预期直接影响当前决策。现有模型在捕捉复杂非线性模式和应对突发事件方面存在局限。

Method: 提出SV-LSTM混合模型：随机波动率模型提高统计精度并捕捉潜在波动动态，LSTM网络增强检测金融时间序列中复杂非线性模式的能力。使用1998-2024年S&P 500日数据，采用滚动窗口方法训练模型并生成一步超前波动率预测。

Result: 混合SV-LSTM模型在统计测试和投资模拟中均优于单独的SV和LSTM模型，为改进S&P 500的风险评估和战略投资规划提供了基础。

Conclusion: 混合方法显著提升了波动率建模技术，为金融风险管理提供了更有效的工具，特别是在捕捉复杂市场动态方面表现出色。

Abstract: Accurate volatility forecasting is essential in banking, investment, and risk management, because expectations about future market movements directly influence current decisions. This study proposes a hybrid modelling framework that integrates a Stochastic Volatility model with a Long Short Term Memory neural network. The SV model improves statistical precision and captures latent volatility dynamics, especially in response to unforeseen events, while the LSTM network enhances the model's ability to detect complex nonlinear patterns in financial time series. The forecasting is conducted using daily data from the S and P 500 index, covering the period from January 1 1998 to December 31 2024. A rolling window approach is employed to train the model and generate one step ahead volatility forecasts. The performance of the hybrid SV-LSTM model is evaluated through both statistical testing and investment simulations. The results show that the hybrid approach outperforms both the standalone SV and LSTM models and contributes to the development of volatility modelling techniques, providing a foundation for improving risk assessment and strategic investment planning in the context of the S and P 500.

</details>


### [411] [Accurate de novo sequencing of the modified proteome with OmniNovo](https://arxiv.org/abs/2512.12272)
*Yuhan Chen,Shang Qu,Zhiqiang Gao,Yuejin Yang,Xiang Zhang,Sheng Xu,Xinjie Mao,Liujia Qian,Jiaqi Wei,Zijie Qiu,Chenyu You,Lei Bai,Ning Ding,Tiannan Guo,Bowen Zhou,Siqi Sun*

Main category: q-bio.QM

Relevance: 25.0

TL;DR: OmniNovo是一个统一的深度学习框架，用于直接从串联质谱中进行无参考的未修饰和修饰肽段测序，能够识别多种翻译后修饰，比标准方法多识别51%的肽段。


<details>
  <summary>Details</summary>
Motivation: 当前蛋白质组学方法对修饰蛋白质组的检测存在盲区，标准数据库搜索算法面临搜索空间组合爆炸的问题，限制了未表征或复杂修饰的鉴定。

Method: OmniNovo是一个统一的深度学习框架，学习通用的碎片化规则来解析多种翻译后修饰。它集成了质量约束解码算法和严格的错误发现率估计。

Result: 在1%错误发现率下，OmniNovo比标准方法多识别51%的肽段。模型能够泛化到训练中未见过的生物位点，揭示蛋白质组的"暗物质"。

Conclusion: OmniNovo实现了对翻译后修饰的无偏全面分析，能够解析蛋白质组的"暗物质"，为细胞调控研究提供了新工具。

Abstract: Post-translational modifications (PTMs) serve as a dynamic chemical language regulating protein function, yet current proteomic methods remain blind to a vast portion of the modified proteome. Standard database search algorithms suffer from a combinatorial explosion of search spaces, limiting the identification of uncharacterized or complex modifications. Here we introduce OmniNovo, a unified deep learning framework for reference-free sequencing of unmodified and modified peptides directly from tandem mass spectra. Unlike existing tools restricted to specific modification types, OmniNovo learns universal fragmentation rules to decipher diverse PTMs within a single coherent model. By integrating a mass-constrained decoding algorithm with rigorous false discovery rate estimation, OmniNovo achieves state-of-the-art accuracy, identifying 51\% more peptides than standard approaches at a 1\% false discovery rate. Crucially, the model generalizes to biological sites unseen during training, illuminating the dark matter of the proteome and enabling unbiased comprehensive analysis of cellular regulation.

</details>


### [412] [Dynamic Homophily with Imperfect Recall: Modeling Resilience in Adversarial Networks](https://arxiv.org/abs/2512.12332)
*Saad Alqithami*

Main category: cs.SI

Relevance: 25.0

TL;DR: 该研究探讨了同质性、记忆约束和对抗性扰动如何共同影响复杂网络的韧性和适应性，提出了一种整合记忆衰减机制的新框架，发现余弦相似度在稀疏、凸和模块化网络中能提升30%的稳定性。


<details>
  <summary>Details</summary>
Motivation: 研究动机是理解复杂网络中同质性、记忆约束和对抗性扰动之间的相互作用，这些因素共同影响网络的韧性和适应性。现有的同质性模型通常忽略记忆衰减机制，而实际系统中记忆约束对网络动态有重要影响。

Method: 开发了一个整合显式记忆衰减机制到同质性模型的新框架。在合成数据集上进行广泛实验，变化衰减函数、重连概率和相似性度量，主要比较余弦相似度与传统指标（如Jaccard相似度和基线边权重）。

Result: 结果显示，在稀疏、凸和模块化网络中，余弦相似度相比传统指标在稳定性指标上能提升高达30%。改进的回忆价值指标表明，策略性遗忘可以通过平衡网络鲁棒性和适应性来增强韧性。

Conclusion: 研究发现将记忆和相似性参数与网络结构和对抗性动态对齐至关重要。通过量化将记忆约束纳入同质性分析的实际效益，为优化社交系统、协作平台和网络安全等实际应用提供了可操作的见解。

Abstract: The purpose of this study is to investigate how homophily, memory constraints, and adversarial disruptions collectively shape the resilience and adaptability of complex networks. To achieve this, we develop a new framework that integrates explicit memory decay mechanisms into homophily-based models and systematically evaluate their performance across diverse graph structures and adversarial settings. Our methods involve extensive experimentation on synthetic datasets, where we vary decay functions, reconnection probabilities, and similarity measures, primarily comparing cosine similarity with traditional metrics such as Jaccard similarity and baseline edge weights. The results show that cosine similarity achieves up to a 30\% improvement in stability metrics in sparse, convex, and modular networks. Moreover, the refined value-of-recall metric demonstrates that strategic forgetting can bolster resilience by balancing network robustness and adaptability. The findings underscore the critical importance of aligning memory and similarity parameters with the structural and adversarial dynamics of the network. By quantifying the tangible benefits of incorporating memory constraints into homophily-based analyses, this study offers actionable insights for optimizing real-world applications, including social systems, collaborative platforms, and cybersecurity contexts.

</details>


### [413] [AI-Driven Real-Time Kick Classification in Olympic Taekwondo Using Sensor Fusion](https://arxiv.org/abs/2512.12474)
*Jamsheed Mistri*

Main category: eess.SY

Relevance: 25.0

TL;DR: 本文提出基于AI的跆拳道计分系统，通过传感器融合和SVM分类器实时识别踢击技术，旨在改善奥运跆拳道的观赏性和计分公平性。


<details>
  <summary>Details</summary>
Motivation: 奥运跆拳道面临观赏性不足和计分争议问题，现有保护与计分系统(PSS)依赖简单的冲击传感器，导致比赛策略保守、缺乏动态攻击。需要更智能的计分系统来激励高难度技术、提高公平性和观众体验。

Method: 提出传感器融合框架，整合现有PSS传感器与加速度计、陀螺仪、磁力/RFID、冲击力传感器。采用机器学习流水线，使用支持向量机(SVM)进行实时踢击分类，识别技术类型、接触位置、冲击力和脚部使用部位。提出基于特定踢击技术的计分规则。

Result: 基于2024年研究显示96-98%的准确率，验证了实时踢击分类的可行性。提出进一步改进方法，如集成SVM分类器和扩展数据集，以满足体育比赛的高精度要求。

Conclusion: AI计分系统能够提高计分公平性、减少规则利用和非法战术、鼓励动态技术、增强观众理解和兴奋度。系统设计包括系统示意图、AI增强规则集的踢击计分表，并讨论了其对奥运跆拳道的预期影响。

Abstract: Olympic Taekwondo has faced challenges in spectator engagement due to static, defensive gameplay and contentious scoring. Current Protector and Scoring Systems (PSS) rely on impact sensors and simplistic logic, encouraging safe strategies that diminish the sport's dynamism. This paper proposes an AI-powered scoring system that integrates existing PSS sensors with additional accelerometers, gyroscopes, magnetic/RFID, and impact force sensors in a sensor fusion framework. The system classifies kicks in real-time to identify technique type, contact location, impact force, and even the part of the foot used. A machine learning pipeline employing sensor fusion and Support Vector Machines (SVMs) is detailed, enabling automatic kick technique recognition for scoring. We present a novel kick scoring rubric that awards points based on specific kick techniques (e.g., turning and spinning kicks) to incentivize dynamic attacks. Drawing on a 2024 study achieving 96-98% accuracy, we validate the feasibility of real-time kick classification and further propose enhancements to this methodology, such as ensemble SVM classifiers and expanded datasets, to achieve the high-stakes accuracy required by the sport. We analyze how the proposed system can improve scoring fairness, reduce rule exploitation and illegitimate tactics, encourage more dynamic techniques, and enhance spectator understanding and excitement. The paper includes system design illustrations, a kick scoring table from an AI-augmented rule set, and discusses anticipated impacts on Olympic Taekwondo.

</details>


### [414] [Can You Keep a Secret? Exploring AI for Care Coordination in Cognitive Decline](https://arxiv.org/abs/2512.12510)
*Alicia,Lee,Mai Lee Chang,Sreehana Mandava,Destiny Deshields,Hugo Simão,Aaron Steinfeld,Jodi Forlizzi,John Zimmerman*

Main category: cs.HC

Relevance: 25.0

TL;DR: 该论文研究认知衰退老年人及其非正式照护者的协调照护策略，提出"搭便车"策略利用偶然重叠的差事提高效率，探讨智能体能否帮助协调这种策略以延长居家养老时间而不增加照护负担。


<details>
  <summary>Details</summary>
Motivation: 随着认知衰退老年人数量增加，非正式照护者负担加重，影响老年人能否继续居家养老。研究旨在探索智能体如何帮助社会经济地位较低的老年人实现居家养老，特别关注照护协调、负担管理和自主隐私维持。

Method: 采用质性研究方法，访谈了10对认知衰退老年人及其非正式照护者，分析他们如何协调照护、管理负担、维持自主权和隐私，识别出有效的"搭便车"策略。

Result: 研究发现老年人通过向特定照护者委派任务来保持控制，同时向成年子女隐瞒部分照护信息。许多人放弃部分日常生活任务以减轻照护负担。最有效的策略是"搭便车"，利用偶然重叠的差事以较少努力完成更多工作。

Conclusion: 研究提出了两个关键问题：(1)智能体能否帮助协调搭便车策略？(2)这种协调能否延长老年人居家养老时间而不增加照护负担？这为智能体在老年人照护领域的应用提供了研究方向。

Abstract: The increasing number of older adults who experience cognitive decline places a burden on informal caregivers, whose support with tasks of daily living determines whether older adults can remain in their homes. To explore how agents might help lower-SES older adults to age-in-place, we interviewed ten pairs of older adults experiencing cognitive decline and their informal caregivers. We explored how they coordinate care, manage burdens, and sustain autonomy and privacy. Older adults exercised control by delegating tasks to specific caregivers, keeping information about all the care they received from their adult children. Many abandoned some tasks of daily living, lowering their quality of life to ease caregiver burden. One effective strategy, piggybacking, uses spontaneous overlaps in errands to get more work done with less caregiver effort. This raises the questions: (i) Can agents help with piggyback coordination? (ii) Would it keep older adults in their homes longer, while not increasing caregiver burden?

</details>


### [415] [Designing The Drive: Enhancing User Experience through Adaptive Interfaces in Autonomous Vehicles](https://arxiv.org/abs/2512.12773)
*Reeteesha Roy*

Main category: cs.HC

Relevance: 25.0

TL;DR: 该论文探讨了在自动驾驶车辆中应用人机交互（HCI）原则来个性化用户界面，以提高安全性、安全性和可用性，通过自适应设计、多模态交互和用户反馈机制增强用户参与度和满意度。


<details>
  <summary>Details</summary>
Motivation: 随着自动驾驶车辆在现代交通系统中的发展和整合，定制用户界面以优化整体用户体验的需求日益增长。理解用户需求和偏好对于这些技术的接受和信任至关重要，特别是随着自动驾驶技术的普及。

Method: 采用HCI原则进行界面个性化设计，包括自适应设计、多模态交互和用户反馈机制。强调透明度和用户控制在界面设计中的重要性，允许用户定制或修改他们的体验。

Result: 个性化界面可以显著增强用户参与度和满意度，同时通过透明度和用户控制的设计策略，能够促进对自动驾驶系统的信任。

Conclusion: HCI在自动驾驶车辆发展中扮演关键角色，需要满足多样化用户需求的同时保持高标准的安全性和安全性。个性化界面设计是提高自动驾驶技术接受度和信任的重要途径。

Abstract: With the recent development and integration of autonomous vehicles (AVs) in transportation systems of the modern world, the emphasis on customizing user interfaces to optimize the overall user experience has been growing expediently. Therefore, understanding user needs and preferences is essential to the acceptance and trust of these technologies as they continue to grow in prevalence. This paper addresses the implementation of HCI principles in the personalization of interfaces to improve safety, security, and usability for the users. This paper explores the way that personalized interfaces can be devised to increase user engagement and satisfaction through various HCI strategies such as adaptive design, multi-modal interaction, and user feedback mechanisms. Moreover, this paper puts emphasis on factors of transparency and user control in the design of an interface; hence, allowing users to design or modify their experience could foster an increase in trust in autonomous systems. In so doing, this research touches on the quite influential role HCI will play in this future scenario of autonomous vehicles while designing to ensure relevance to the diverse needs of users while maintaining high standards of safety and security. Discussing various HCI strategies such as adaptive design, multi-modal interaction, and feedback mechanisms to the user, this paper demonstrates how personalized interfaces can enhance significantly both user engagement and satisfaction. Transparency and user control also in designing an interface are further discussed, pointing out the need for a prerequisite condition of enabling the user to take control of their experience as a state of trust in autonomous systems. In summary, this paper points out the role of HCI in the development of autonomous vehicles and addresses numerous needs with respect to those enforced safety and security standards.

</details>


### [416] [Solving Parallel Machine Scheduling With Precedences and Cumulative Resource Constraints With Calendars](https://arxiv.org/abs/2512.11864)
*Christoph Einspieler,Matthias Horn,Marie-Louise Lackner,Patrick Malik,Nysret Musliu,Felix Winter*

Main category: cs.AI

Relevance: 15.0

TL;DR: 提出了一种解决现实工业中并行机器调度问题的新方法，包含作业优先级和基于日历的累积资源约束，结合了精确约束建模和元启发式算法。


<details>
  <summary>Details</summary>
Motivation: 现实工业生产环境中存在复杂的优先级约束和基于日历的资源限制，现有调度技术无法有效处理这些约束，需要开发能解决实际并行机器调度问题的自动化方法。

Method: 1) 提出包含作业优先级和基于日历累积资源约束的新并行机器调度变体；2) 使用约束建模方法作为小规模场景的精确解；3) 提出构造启发式算法和基于局部搜索的定制元启发式算法处理大规模实例。

Result: 该方法已在工业环境中部署使用，能够有效处理现实工业调度场景中的复杂约束。

Conclusion: 提出的方法能够解决现实工业中复杂的并行机器调度问题，结合精确方法和启发式算法在不同规模场景下都能有效工作。

Abstract: The task of finding efficient production schedules for parallel machines is a challenge that arises in most industrial manufacturing domains. There is a large potential to minimize production costs through automated scheduling techniques, due to the large-scale requirements of modern factories. In the past, solution approaches have been studied for many machine scheduling variations, where even basic variants have been shown to be NP-hard. However, in today's real-life production environments, additional complex precedence constraints and resource restrictions with calendars arise that must be fulfilled. These additional constraints cannot be tackled efficiently by existing solution techniques. Thus, there is a strong need to develop and analyze automated methods that can solve such real-life parallel machine scheduling scenarios. In this work, we introduce a novel variant of parallel machine scheduling with job precedences and calendar-based cumulative resource constraints that arises in real-life industrial use cases. A constraint modeling approach is proposed as an exact solution method for small scheduling scenarios together with state-of-the-art constraint-solving technology. Further, we propose a construction heuristic as well as a tailored metaheuristic using local search to efficiently tackle large-scale problem instances. This metaheuristic approach has been deployed and is currently being used in an industrial setting.

</details>


### [417] [A Multi-Axial Mindset for Ontology Design Lessons from Wikidata's Polyhierarchical Structure](https://arxiv.org/abs/2512.12260)
*Ege Atacan Doğan,Peter F. Patel-Schneider*

Main category: cs.AI

Relevance: 15.0

TL;DR: Wikidata采用多层级、多轴分类设计而非传统本体论的单一层次结构，分析其结构影响和可扩展性


<details>
  <summary>Details</summary>
Motivation: 传统本体论设计强调互斥且完备的顶层区分（如持续体vs发生体、抽象vs具体、类型vs实例），这些区分用于构建统一层次结构，每个实体都归类于单一顶层类别。而Wikidata不强制执行单一基础分类法，而是允许多个分类轴在共享根类"实体"下同时存在。本文旨在分析Wikidata这种多层级、多轴设计对知识表示的结构影响。

Method: 分析Wikidata的结构设计，对比传统本体论的单层次分类方法，研究其多层级、多轴架构的特点。重点考察Wikidata如何在共享根类"实体"下容纳多个分类轴，以及这种设计如何支持可扩展和模块化的本体构建。

Result: Wikidata的多层级、多轴设计使其特别适合协作和演化的知识图谱构建。这种架构支持可扩展和模块化的本体构建方法，能够更好地适应知识表示的实际需求，避免了传统单层次分类法的局限性。

Conclusion: Wikidata的架构代表了与传统本体论设计不同的知识表示方法，通过允许多个分类轴共存，提供了更灵活、可扩展的知识组织方式，特别适合大规模协作和持续演化的知识图谱环境。

Abstract: Traditional ontology design emphasizes disjoint and exhaustive top-level distinctions such as continuant vs. occurrent, abstract vs. concrete, or type vs. instance. These distinctions are used to structure unified hierarchies where every entity is classified under a single upper-level category. Wikidata, by contrast, does not enforce a singular foundational taxonomy. Instead, it accommodates multiple classification axes simultaneously under the shared root class entity. This paper analyzes the structural implications of Wikidata's polyhierarchical and multi-axial design. The Wikidata architecture enables a scalable and modular approach to ontology construction, especially suited to collaborative and evolving knowledge graphs.

</details>


### [418] [Personalized QoE Prediction: A Demographic-Augmented Machine Learning Framework for 5G Video Streaming Networks](https://arxiv.org/abs/2512.12736)
*Syeda Zunaira Ahmed,Hejab Tahira Beg,Maryam Khalid*

Main category: cs.AI

Relevance: 15.0

TL;DR: 本文提出了一种基于人口统计信息的机器学习框架，用于个性化QoE预测，通过行为真实的数据增强策略将小数据集扩展6倍，TabNet模型表现最佳。


<details>
  <summary>Details</summary>
Motivation: 现有QoE预测方法主要依赖有限数据集并假设用户感知均匀，限制了在异构真实环境中的适用性。需要更个性化的QoE预测方法来支持5G网络中的自适应视频流媒体。

Method: 提出人口统计感知的机器学习框架，引入基于行为真实的人口统计数据增强策略，将小QoE数据集扩展6倍。评估了经典机器学习模型和深度学习架构，包括基于注意力的MLP和TabNet。

Result: 实验结果显示在RMSE、MAE和R指标上相比基线模型有显著改进。TabNet在所有评估方法中表现最强，受益于其固有的特征选择和注意力机制。

Conclusion: 人口统计感知的数据增强显著提高了QoE预测的鲁棒性，为5G视频流媒体网络中的个性化QoE感知智能提供了可扩展的方向。

Abstract: Quality of Experience (QoE) prediction is a critical component of modern multimedia systems, particularly for adaptive video streaming in 5G networks. Accurate QoE estimation enables intelligent resource management and supports user centric service delivery. Existing QoE prediction approaches primarily rely on limited datasets and assume uniform user perception, which restricts their applicability in heterogeneous real world environments.
  This paper proposes a demographic aware machine learning framework for personalized QoE prediction. We introduce a behaviorally realistic demographic based data augmentation strategy that expands a small QoE dataset six fold by modeling varying user sensitivities to streaming impairments such as rebuffering, bitrate variation, and quality degradation. Using the augmented dataset, we evaluate a comprehensive set of classical machine learning models alongside advanced deep learning architectures, including an attention-based MLP and TabNet.
  Experimental results demonstrate significant improvements in prediction accuracy across RMSE, MAE, and R metrics compared to baseline models. Among all evaluated approaches, TabNet achieves the strongest performance, benefiting from its inherent feature selection and attention mechanisms. The results confirm that demographic-aware augmentation substantially enhances QoE prediction robustness and provides a scalable direction for personalized QoE-aware intelligence in 5G video streaming networks.

</details>


### [419] [Causal Counterfactuals Reconsidered](https://arxiv.org/abs/2512.12804)
*Sander Beckers*

Main category: cs.AI

Relevance: 15.0

TL;DR: 本文提出了一种新的反事实概率语义学，扩展了标准的Pearl语义学，适用于无法扩展为现实结构因果模型的概率因果模型，解决了Pearl与Dawid关于反事实的长期争论。


<details>
  <summary>Details</summary>
Motivation: 标准Pearl语义学需要将概率因果模型扩展为结构因果模型，但作者发现即使在简单设置中也会出现无法这样扩展的模型。需要一种更通用的语义学来处理这类情况，并在Pearl和Dawid关于反事实的争论中寻找折中方案。

Method: 提出新的反事实概率语义学，限制在满足马尔可夫条件、仅包含现实变量且因果完备的因果模型中。虽然使用结构因果模型框架，但避免使用响应变量。证明该语义学与其他两种不涉及结构因果模型的近期提案等价。

Result: 建立了适用于更广泛概率因果模型的反事实语义学，在Pearl和Dawid的争论中找到了折中：同意Dawid拒绝普遍因果决定论和不现实变量，但同意Pearl认为通用反事实语义学是可能的。证明了语义学与其他提案的等价性。

Conclusion: 本文提出的语义学成功扩展了反事实推理的范围，解决了标准Pearl语义学的局限性，为概率因果模型中的反事实分析提供了更通用的框架，并在理论争议中找到了合理的平衡点。

Abstract: I develop a novel semantics for probabilities of counterfactuals that generalizes the standard Pearlian semantics: it applies to probabilistic causal models that cannot be extended into realistic structural causal models and are therefore beyond the scope of Pearl's semantics. This generalization is needed because, as I show, such probabilistic causal models arise even in simple settings. My semantics offer a natural compromize in the long-standing debate between Pearl and Dawid over counterfactuals: I agree with Dawid that universal causal determinism and unrealistic variables should be rejected, but I agree with Pearl that a general semantics of counterfactuals is nonetheless possible. I restrict attention to causal models that satisfy the Markov condition, only contain realistic variables, and are causally complete. Although I formulate my proposal using structural causal models, as does Pearl, I refrain from using so-called response variables. Moreover, I prove that my semantics is equivalent to two other recent proposals that do not involve structural causal models, and that it is in line with various comments on stochastic counterfactuals that have appeared in the literature more broadly. Throughout I also reflect on the universality of the Markov condition and explore a novel generalization of causal abstractions

</details>


### [420] [Satisfiability Modulo Theory Meets Inductive Logic Programming](https://arxiv.org/abs/2512.12918)
*Nijesh Upreti,Vaishak Belle*

Main category: cs.AI

Relevance: 15.0

TL;DR: 该论文提出了一种模块化的SMT-ILP架构，将归纳逻辑编程系统PyGol与SMT求解器Z3结合，以学习包含数值约束的混合规则，扩展了符号规则学习的表达能力。


<details>
  <summary>Details</summary>
Motivation: 传统归纳逻辑编程(ILP)在关系领域提供可解释的规则学习，但在处理数值约束方面存在局限。现有方法通常依赖离散化或手工设计的数值谓词，难以推断需要跨示例共同保持的阈值或算术关系。需要一种能够结合符号谓词与学习数值约束的方法。

Method: 采用模块化方法，将ILP系统PyGol与SMT求解器Z3耦合。PyGol提出的候选子句被解释为线性/非线性实数算术等背景理论上的无量词公式，SMT求解器实例化并验证数值参数，同时保持ILP的声明性关系偏置。支持学习包含阈值、区间和多文字算术关系的混合规则。

Result: 在专门设计的合成数据集上评估，涵盖线性、关系、非线性和多跳推理任务。结果表明模块化SMT-ILP架构能够扩展符号规则学习的表达能力，补充了先前的数值ILP方法。

Conclusion: 模块化SMT-ILP架构为未来向更丰富的理论感知归纳扩展提供了灵活基础，能够学习结合符号谓词与数值约束的混合规则，增强了ILP在数值推理方面的能力。

Abstract: Inductive Logic Programming (ILP) provides interpretable rule learning in relational domains, yet remains limited in its ability to induce and reason with numerical constraints. Classical ILP systems operate over discrete predicates and typically rely on discretisation or hand-crafted numerical predicates, making it difficult to infer thresholds or arithmetic relations that must hold jointly across examples. Recent work has begun to address these limitations through tighter integrations of ILP with Satisfiability Modulo Theories (SMT) or specialised numerical inference mechanisms. In this paper we investigate a modular alternative that couples the ILP system PyGol with the SMT solver Z3. Candidate clauses proposed by PyGol are interpreted as quantifier-free formulas over background theories such as linear or nonlinear real arithmetic, allowing numerical parameters to be instantiated and verified by the SMT solver while preserving ILP's declarative relational bias. This supports the induction of hybrid rules that combine symbolic predicates with learned numerical constraints, including thresholds, intervals, and multi-literal arithmetic relations. We formalise this SMT-ILP setting and evaluate it on a suite of synthetic datasets designed to probe linear, relational, nonlinear, and multi-hop reasoning. The results illustrate how a modular SMT-ILP architecture can extend the expressivity of symbolic rule learning, complementing prior numerical ILP approaches while providing a flexible basis for future extensions toward richer theory-aware induction.

</details>


### [421] [Totalitarian Technics: The Hidden Cost of AI Scribes in Healthcare](https://arxiv.org/abs/2512.11814)
*Hugh Brosnahan*

Main category: cs.CY

Relevance: 15.0

TL;DR: 该论文从哲学角度分析AI医疗记录系统，认为其真正意义不在于效率提升，而在于重塑医疗注意力模式，体现左脑计算思维主导，可能窄化医疗关怀领域


<details>
  <summary>Details</summary>
Motivation: AI医疗记录系统被宣传为解决行政负担的方案，但作者认为其真正重要性在于如何重塑医疗注意力本身。论文旨在从哲学角度分析AI技术如何体现和放大特定的注意力模式，以及这对医疗实践的影响

Method: 采用概念分析方法，将AI医疗记录系统置于人类思维和技能外部化的哲学传统中。借鉴Iain McGilchrist的半球理论和Lewis Mumford的技术哲学，分析技术如何体现特定的注意力模式

Result: AI医疗记录系统体现了左脑计算思维的主导地位，这种思维优先考虑可测量和程序化的内容，而非直觉和关系性内容。随着这种注意力模式在医疗实践中进一步嵌入，存在窄化关怀领域、侵蚀临床专业知识和将医生简化为机械化系统操作员的风险

Conclusion: AI医疗记录系统的真正意义不在于效率提升，而在于它们如何重塑医疗注意力模式。论文警告这种技术可能强化计算思维主导，对医疗实践产生深远影响，需要更批判性地审视技术如何塑造医疗关怀的本质

Abstract: Artificial intelligence (AI) scribes, systems that record and summarise patient-clinician interactions, are promoted as solutions to administrative overload. This paper argues that their significance lies not in efficiency gains but in how they reshape medical attention itself. Offering a conceptual analysis, it situates AI scribes within a broader philosophical lineage concerned with the externalisation of human thought and skill. Drawing on Iain McGilchrist's hemisphere theory and Lewis Mumford's philosophy of technics, the paper examines how technology embodies and amplifies a particular mode of attention. AI scribes, it contends, exemplify the dominance of a left-hemispheric, calculative mindset that privileges the measurable and procedural over the intuitive and relational. As this mode of attention becomes further embedded in medical practice, it risks narrowing the field of care, eroding clinical expertise, and reducing physicians to operators within an increasingly mechanised system.

</details>


### [422] [Using Socio-economic Indicators, Smart Transit Systems, and Urban Simulator to Accelerate ZEV Adoption and Reduce VMT](https://arxiv.org/abs/2512.11870)
*Mulham Fawkherji,Bruce Race,Driss Benhaddou*

Main category: cs.CY

Relevance: 15.0

TL;DR: 论文提出了针对休斯顿等汽车依赖型城市实现2050年净零排放目标的方法，包括建立道路排放基线、评估政策、开发智能交通系统，并创建了Unity 3D仿真环境来模拟城市交通和政策情景。


<details>
  <summary>Details</summary>
Motivation: 道路运输占全球温室气体排放的15%，城市在实现IPCC目标中起关键作用。休斯顿作为低密度、汽车依赖型城市，道路排放占其气候行动计划基准的48%，面临实现2050年净零排放目标的挑战，需要解决社会经济差异对零排放车辆采用的影响。

Method: 1) 建立道路排放基线；2) 利用社会经济指标和智能交通系统评估政策；3) 开发智能停车、交通激励、安全数据系统和零排放车队管理；4) 在Unity 3D中创建仿真环境，动态模拟城市交通并可视化政策情景。

Result: 提出了支持评估的指标、度量和技术，包括智能交通系统解决方案和政策分析框架，为汽车依赖型城市实现2050年排放目标提供了方法论和工具支持。

Conclusion: 汽车依赖型城市可以通过智能交通系统、社会经济指标分析和政策评估来加速零排放车辆采用并减少车辆行驶里程，从而实现2050年排放目标。Unity 3D仿真环境为政策制定提供了可视化支持工具。

Abstract: Globally, on-road transportation accounts for 15% of greenhouse gas (GHG) emissions and an estimated 385,000 premature deaths from PM2.5. Cities play a critical role in meeting IPCC targets, generating 75% of global energy-related GHG emissions. In Houston, Texas, on-road transportation represents 48% of baseline emissions in the Climate Action Plan (CAP). To reach net-zero by 2050, the CAP targets a 70% emissions reduction from a 2014 baseline, offset by 30% renewable energy. This goal is challenging because Houston is low-density and auto-dependent, with 89% of on-road emissions from cars and small trucks and limited public transit usage. Socio-economic disparities further constrain Zero Emissions Vehicle (ZEV) adoption. Strategies focus on expanding ZEV access and reducing Vehicle Miles Traveled (VMT) by 20% through transit improvements and city design. This paper presents methods for establishing an on-road emissions baseline and evaluating policies that leverage socio-economic indicators and Intelligent Transportation Systems (ITS) to accelerate ZEV adoption and reduce VMT. Smart parking, transit incentives, secure data systems, and ZEV fleet management support improvements in modal split and system reliability. Policy options are analyzed and potential actions identified. To support evaluation, a simulation environment was developed in Unity 3D, enabling dynamic modeling of urban mobility and visualization of policy scenarios. Auto-dependent cities aiming for 2050 emission targets can benefit from the indicators, metrics, and technologies discussed.

</details>


### [423] [Congestion Reduction in EV Charger Placement Using Traffic Equilibrium Models](https://arxiv.org/abs/2512.12081)
*Semih Kara,Yasin Sonmez,Can Kizilkale,Alex Kurzhanskiy,Nuno C. Martins,Murat Arcak*

Main category: eess.SY

Relevance: 15.0

TL;DR: 该论文研究如何战略性地部署电动汽车充电站以减少交通拥堵，提出了基于拥堵博弈和排队模拟的两种均衡模型，并开发了可扩展的贪心算法进行充电站选址。


<details>
  <summary>Details</summary>
Motivation: 随着电动汽车普及率增加，充电站的选址不当可能加剧交通拥堵。需要研究如何战略性地部署充电站来优化交通网络，减少拥堵影响。

Method: 提出了两种均衡模型：基于拥堵博弈的模型和基于原子排队模拟的模型。开发了可扩展的贪心充电站选址算法，并将两种模型应用于该算法中。还提出了统一的方法论，从排队模拟校准拥堵延迟并在链路空间求解均衡。

Result: 实验表明，贪心算法在现实网络中能产生最优或接近最优的拥堵结果，尽管存在反例证明无法保证全局最优性。排队方法比拥堵博弈模型产生更现实的结果。

Conclusion: 战略性的充电站选址可以有效缓解交通拥堵，排队模拟方法比传统博弈模型更准确，贪心算法在实际应用中表现良好。

Abstract: Growing EV adoption can worsen traffic conditions if chargers are sited without regard to their impact on congestion. We study how to strategically place EV chargers to reduce congestion using two equilibrium models: one based on congestion games and one based on an atomic queueing simulation. We apply both models within a scalable greedy station-placement algorithm. Experiments show that this greedy scheme yields optimal or near-optimal congestion outcomes in realistic networks, even though global optimality is not guaranteed as we show with a counterexample. We also show that the queueing-based approach yields more realistic results than the congestion-game model, and we present a unified methodology that calibrates congestion delays from queue simulation and solves equilibrium in link-space.

</details>


### [424] [Not All Transparency Is Equal: Source Presentation Effects on Attention, Interaction, and Persuasion in Conversational Search](https://arxiv.org/abs/2512.12207)
*Jiangen He,Jiqun Liu*

Main category: cs.HC

Relevance: 15.0

TL;DR: 研究比较了四种引用源展示设计对用户参与度的影响，发现高可见性界面能增加悬停行为但点击率普遍较低，界面设计对用户体验影响有限但对知识、兴趣和认同感变化有显著影响。


<details>
  <summary>Details</summary>
Motivation: 对话式搜索系统越来越多地提供引用源，但引用源展示格式如何影响用户参与度尚不清楚。本研究旨在探索不同引用源展示设计对用户行为、体验和认知变化的影响。

Method: 通过众包用户实验，招募394名参与者，比较四种引用源展示设计：可折叠列表、悬停卡片、页脚列表和对齐侧边栏。这些设计在引用可见性和可访问性上有所不同。

Result: 高可见性界面显著增加了用户对引用源的悬停行为，但所有条件下的点击率都较低。界面设计对用户体验和感知影响有限，但对知识、兴趣和认同感变化有显著影响：高可见性界面最初会降低知识获取和兴趣，但随着引用源使用增加会产生积极效果；侧边栏条件能独特地增加认同感变化。

Conclusion: 引用源展示本身可能不会增强用户参与度，当提供的引用源不足时甚至可能降低参与度。界面设计需要平衡可见性和可用性，考虑用户的实际使用模式。

Abstract: Conversational search systems increasingly provide source citations, yet how citation or source presentation formats influence user engagement remains unclear. We conducted a crowdsourcing user experiment with 394 participants comparing four source presentation designs that varied citation visibility and accessibility: collapsible lists, hover cards, footer lists, and aligned sidebars.High-visibility interfaces generated substantially more hovering on sources, though clicking remained infrequent across all conditions. While interface design showed limited effects on user experience and perception measures, it significantly influenced knowledge, interest, and agreement changes. High-visibility interfaces initially reduced knowledge gain and interest, but these positive effects emerged with increasing source usage. The sidebar condition uniquely increased agreement change. Our findings demonstrate that source presentation alone may not enhance engagement and can even reduce it when insufficient sources are provided.

</details>


### [425] [Quantum Implicit Neural Representations for 3D Scene Reconstruction and Novel View Synthesis](https://arxiv.org/abs/2512.12683)
*Yeray Cordero,Paula García-Molina,Fernando Vilariño*

Main category: quant-ph

Relevance: 15.0

TL;DR: Q-NeRF首次将量子隐式表示网络（QIREN）集成到神经辐射场（NeRF）框架中，创建了混合量子-经典神经渲染系统，用于3D场景重建。


<details>
  <summary>Details</summary>
Motivation: 传统隐式神经表示（INRs）存在频谱偏差问题，难以捕捉高频细节。量子电路具有固有的傅里叶结构，能够超越经典MLP进行更紧凑和表达性更强的频率建模。

Method: 将QIREN模块集成到Nerfacto主干网络中，保留其高效采样、姿态优化和体渲染策略，同时用量子增强组件替换选定的密度和辐射预测部分。系统评估了三种混合配置。

Result: 混合量子-经典模型在有限计算资源下实现了有竞争力的重建质量，量子模块在表示细粒度、视角相关的外观方面特别有效。虽然当前实现受限于少量子位模拟器。

Conclusion: Q-NeRF为可扩展的量子增强3D场景重建提供了基础步骤，展示了量子编码在缓解隐式表示频谱偏差方面的潜力，为未来量子神经渲染研究建立了基线。

Abstract: Implicit neural representations (INRs) have become a powerful paradigm for continuous signal modeling and 3D scene reconstruction, yet classical networks suffer from a well-known spectral bias that limits their ability to capture high-frequency details. Quantum Implicit Representation Networks (QIREN) mitigate this limitation by employing parameterized quantum circuits with inherent Fourier structures, enabling compact and expressive frequency modeling beyond classical MLPs. In this paper, we present Quantum Neural Radiance Fields (Q-NeRF), the first hybrid quantum-classical framework for neural radiance field rendering. Q-NeRF integrates QIREN modules into the Nerfacto backbone, preserving its efficient sampling, pose refinement, and volumetric rendering strategies while replacing selected density and radiance prediction components with quantum-enhanced counterparts. We systematically evaluate three hybrid configurations on standard multi-view indoor datasets, comparing them to classical baselines using PSNR, SSIM, and LPIPS metrics. Results show that hybrid quantum-classical models achieve competitive reconstruction quality under limited computational resources, with quantum modules particularly effective in representing fine-scale, view-dependent appearance. Although current implementations rely on quantum circuit simulators constrained to few-qubit regimes, the results highlight the potential of quantum encodings to alleviate spectral bias in implicit representations. Q-NeRF provides a foundational step toward scalable quantum-enabled 3D scene reconstruction and a baseline for future quantum neural rendering research.

</details>


### [426] [Defending the Hierarchical Result Models of Precedential Constraint](https://arxiv.org/abs/2512.13505)
*Henry Prakken,Wijnand van Woerkom*

Main category: cs.AI

Relevance: 5.0

TL;DR: 本文回应Bench-Capon对层次案例推理模型的批评，认为其误解了中间因素作为维度的解释，并证明van Woerkom的基于维度的层次结果模型可以避免这些批评。


<details>
  <summary>Details</summary>
Motivation: 近年来层次案例推理模型受到Bench-Capon的批评，认为这些模型在某些案例中会产生错误结果，特别是无法处理中间因素由不同基础因素以不同强度建立的情况。本文旨在回应这些批评，为van Woerkom的基于结果的层次模型辩护。

Method: 通过分析Bench-Capon的批评案例，指出其误解了中间因素作为维度的解释方式。然后应用van Woerkom的基于维度的层次结果模型重新分析这些案例，展示该模型如何正确处理中间因素的不同强度问题。

Result: 论证表明Bench-Capon的批评源于对中间因素解释的误解，而van Woerkom的基于维度的层次结果模型能够避免这些批评，正确处理中间因素由不同基础因素以不同强度建立的情况。

Conclusion: van Woerkom的基于维度的层次结果模型是有效的，Bench-Capon的批评可以通过正确解释中间因素作为维度来避免。该模型能够更准确地处理先例约束中的复杂因素关系。

Abstract: In recent years, hierarchical case-based-reasoning models of precedential constraint have been proposed. In various papers, Trevor Bench-Capon criticised these models on the grounds that they would give incorrect outcomes in some cases. In particular, the models would not account for the possibility that intermediate factors are established with different strengths by different base-level factors. In this paper we respond to these criticisms for van Woerkom's result-based hierarchical models. We argue that in some examples Bench-Capon seems to interpret intermediate factors as dimensions, and that applying van Woerkom's dimension-based version of the hierarchical result model to these examples avoids Bench-Capon's criticisms.

</details>


### [427] [Carrot, stick, or both? Price incentives for sustainable food choice in competitive environments](https://arxiv.org/abs/2512.13174)
*Francesco Salvi,Giuseppe Russo,Adam Barla,Vincent Moreau,Robert West*

Main category: econ.GN

Relevance: 5.0

TL;DR: 在瑞士大学食堂进行的实地实验发现，肉类附加费和混合定价能显著增加素食餐选择（分别提升26.4%和16.6%），但附加费会导致销售转移至非干预地点，抵消环境效益；混合方案在减少碳排放的同时保持经济可行性。


<details>
  <summary>Details</summary>
Motivation: 肉类消费是全球温室气体排放的主要驱动因素。虽然定价干预在减少肉类摄入方面显示出潜力，但以往研究多在高度受限环境中进行，消费者选择有限。本研究旨在评估真实竞争环境中多种定价干预的效果。

Method: 采用顺序交叉设计，在瑞士大学校园四个食堂中系统比较三种定价干预：素食餐折扣（-2.5瑞士法郎）、肉类附加费（+2.5瑞士法郎）和混合方案（-1.2瑞士法郎/+1.2瑞士法郎）。通过匹配菜单控制其他变量。

Result: 只有附加费和混合干预显著增加了素食餐选择（分别提升26.4%和16.6%），每餐碳排放分别减少7.4%和11.3%。但附加费导致干预地点销售下降12.3%，非干预地点销售增加14.9%，产生溢出效应完全抵消环境效益。混合方案在减少排放的同时未显著影响总销售或收入。

Conclusion: 平衡的定价策略可以减少现实食品环境的碳足迹，但需要协调实施以最大化气候效益并避免意外溢出效应。混合方法既有效又经济可行，对素食倾向顾客和习惯性肉食者同样有效。

Abstract: Meat consumption is a major driver of global greenhouse gas emissions. While pricing interventions have shown potential to reduce meat intake, previous studies have focused on highly constrained environments with limited consumer choice. Here, we present the first large-scale field experiment to evaluate multiple pricing interventions in a real-world, competitive setting. Using a sequential crossover design with matched menus in a Swiss university campus, we systematically compared vegetarian-meal discounts (-2.5 CHF), meat surcharges (+2.5 CHF), and a combined scheme (-1.2 CHF=+1.2 CHF) across four campus cafeterias. Only the surcharge and combined interventions led to significant increases in vegetarian meal uptake--by 26.4% and 16.6%, respectively--and reduced CO2 emissions per meal by 7.4% and 11.3%, respectively. The surcharge, while effective, triggered a 12.3% drop in sales at intervention sites and a corresponding 14.9% increase in non-treated locations, hence causing a spillover effect that completely offset environmental gains. In contrast, the combined approach achieved meaningful emission reductions without significant effects on overall sales or revenue, making it both effective and economically viable. Notably, pricing interventions were equally effective for both vegetarian-leaning customers and habitual meat-eaters, stimulating change even within entrenched dietary habits. Our results show that balanced pricing strategies can reduce the carbon footprint of realistic food environments, but require coordinated implementation to maximize climate benefits and avoid unintended spillover effects.

</details>


<div id='cs.LG'></div>

# cs.LG [[Back]](#toc)

### [428] [D-STEER - Preference Alignment Techniques Learn to Behave, not to Believe -- Beneath the Surface, DPO as Steering Vector Perturbation in Activation Space](https://arxiv.org/abs/2512.11838)
*Samarth Raina,Saksham Aggarwal,Aman Chadha,Vinija Jain,Amitava Das*

Main category: cs.LG

Relevance: 85.0

TL;DR: DPO不是通过重写模型内部信念来对齐语言模型，而是通过低秩的"转向机制"来微调激活，使其沿着少数偏好方向移动，本质上教会模型如何表现对齐，而不是改变其核心信念。


<details>
  <summary>Details</summary>
Motivation: 尽管DPO已成为对齐大语言模型的标准方法，但其在模型内部实际引发的变化机制尚不明确。本文旨在探究DPO是否真正重写了模型的内部信念，还是仅仅作为一种行为调整机制。

Method: 通过理论推导证明DPO梯度仅依赖于偏好与非偏好补全的对数嵌入差异，表明其仅引起最终隐藏表示的一阶偏移而非深层语义重构。从DPO调优模型中提取经验转向向量，通过加减该向量验证其对模型行为的影响。使用谱分析揭示上层网络的秩一主导和熵塌缩现象。

Result: DPO梯度分析显示其仅引起激活的浅层偏移；经验转向向量实验表明添加该向量可重现大部分对齐行为，减去则几乎恢复原始模型；谱分析显示上层网络存在秩一主导和熵塌缩，表明对齐通过狭窄子空间实现。

Conclusion: DPO并非重写模型内部信念，而是通过低秩转向机制教会模型如何表现对齐行为，支持"行为幻觉"观点：模型学会如何行动对齐，而非真正改变其信念。

Abstract: Direct Preference Optimization (DPO) has become a standard recipe for aligning large language models, yet it is still unclear what kind of change it actually induces inside the network. This paper argues that DPO does not rewrite a models internal beliefs; instead, it acts as a low rank steering mechanism that nudges activations along a small number of preference directions. Using a simple derivation, we show that the DPO gradient depends only on the difference between the logit embeddings of preferred and dispreferred completions, implying a first order shift in the final hidden representation rather than a deep restructuring of semantics. We then extract an empirical steering vector from a DPO tuned model and demonstrate that adding this vector to base activations reproduces most of the aligned behavior, while subtracting it nearly restores the original model. Finally, spectral analyses reveal rank-one dominance and entropy collapse in upper layers, indicating that alignment is funneled through a narrow subspace. Taken together, these results support a behavioral illusion view of DPO: it teaches models how to act aligned, not what to believe.

</details>


### [429] [Large Language Models as Generalist Policies for Network Optimization](https://arxiv.org/abs/2512.11839)
*Duo Wu,Linjia Kang,Zhimin Wang,Fangxin Wang,Wei Zhang,Xuefeng Tao,Wei Yang,Le Zhang,Peng Cui,Zhi Wang*

Main category: cs.LG

Relevance: 85.0

TL;DR: Trailblazer：首个基于LLM的通用网络策略框架，通过网络对齐和自适应策略协作，实现跨任务和跨环境的强泛化能力，在抖音真实系统中验证有效。


<details>
  <summary>Details</summary>
Motivation: 传统网络优化依赖基于手工规则或深度学习模型的专家策略，泛化能力差。LLM在互联网规模语料上预训练，编码了基础网络原理，并具有对未见场景的泛化能力，为通用网络策略提供了变革性基础。

Method: Trailblazer框架包含：1）网络对齐方案，将LLM与特定网络任务对齐；2）自适应策略协作机制，将简单控制案例从LLM卸载到轻量级策略以提高计算效率。

Result: 通过大量仿真和抖音（中国版TikTok）的大规模真实在线评估，Trailblazer（基于单个LLM）在跨任务和跨环境泛化方面表现出比传统专家策略更强的性能。

Conclusion: LLM可作为通用网络策略的基础，Trailblazer是迈向通用驱动范式的第一步，能够以最小的策略设计努力实现强泛化。

Abstract: Designing control policies to ensure robust network services is essential to modern digital infrastructure. However, the dominant paradigm for network optimization relies on designing specialist policies based on handcrafted rules or deep learning models, leading to poor generalization across diverse tasks and environments. In contrast, large language models (LLMs), pretrained on Internet-scale corpora, provide a rich and unified knowledge base that encodes fundamental networking principles. Combined with their emergent abilities in generalization to unseen scenarios, LLMs offer a transformative foundation for generalist network policies that can generalize across diverse tasks and environments with minimal adaptation. In this paper, we present Trailblazer, the first systematic framework to realize such a generalist policy for networking. Trailblazer incorporates a network alignment scheme to ground the LLM in specific networking tasks, and an adaptive policy collaboration mechanism that offloads simple control cases from the LLM to a lightweight policy for computational efficiency. Through extensive simulations and large-scale real-world online evaluation on Douyin (the Chinese version of TikTok), Trailblazer, powered by a single LLM, demonstrates stronger cross-task and cross-environment generalization than conventional specialist policies. Our results validate LLMs as the foundation for generalist network policies, and position Trailblazer as the first step toward the generalist-driven paradigm that enables strong generalization with minimal efforts in policy design.

</details>


### [430] [KV Cache Recycling to Expand Usable Context Capacity in Low Parameter LLMs](https://arxiv.org/abs/2512.11851)
*Prashant Pandey*

Main category: cs.LG

Relevance: 85.0

TL;DR: 提出token recycling方法，通过重用相似prompt的KV状态来加速LLM推理，在DialoGPT-medium上验证了前缀重叠时的速度提升且不影响输出质量。


<details>
  <summary>Details</summary>
Motivation: LLM推理时重复计算相似prompt的KV状态是计算冗余的，如果能重用已计算的KV状态，可以显著加速推理过程，特别是对于对话系统等重复性任务。

Method: 使用DialoGPT-medium作为测试平台，构建过去激活的缓存，通过句子嵌入检索缓存条目，当缓存prompt是新输入的精确前缀时，重用缓存的KV状态。无需修改模型，将缓存的KV序列化到CPU，重新加载后继续解码。

Result: 当存在前缀重叠时观察到一致的加速效果，且输出语义没有明显退化；当没有重叠时，行为与基线一致。

Conclusion: token recycling方法能有效加速LLM推理，特别是在对话等重复性场景中，通过重用KV状态减少计算冗余，实现推理加速。

Abstract: Whether attention key value (KV) states computed for one prompt for a small LLM can be reused to accelerate inference on a new similar prompt, giving an increase to the space to its context memory using an approach called token recycling. Using a standard Hugging Face setup with DialoGPT-medium (a 345M parameter GPT-2 style decoder trained on 147M Reddit exchanges, 2005 to 2017) as the testbed, we build a cache of past activations and get entries by sentence embeddings, then reuse cached past key values when the cached prompt is an exact prefix of the new input. We compare recycled vs. baseline runs on latency and output fidelity, and log reuse depth in tokens. Reproducibility requires no model modifications, cached KVs are serialized to the CPU, reloaded, and supplied to the generate function to continue decoding from the cached prefix. In tests, we observe consistent speedups when prefix overlap exists, with no material degradation in output semantics, and when overlap is absent, behavior matches baseline.

</details>


### [431] [Neural Chameleons: Language Models Can Learn to Hide Their Thoughts from Unseen Activation Monitors](https://arxiv.org/abs/2512.11949)
*Max McGuinness,Alex Serrano,Luke Bailey,Scott Emmons*

Main category: cs.LG

Relevance: 85.0

TL;DR: 论文提出"神经变色龙"模型，通过微调使LLM能够零样本逃避激活监控器，在触发提示下隐藏特定概念（包括安全相关概念）的内部状态，即使监控器在模型权重冻结后训练也能成功逃避。


<details>
  <summary>Details</summary>
Motivation: 研究激活监控在对抗性威胁模型下的最坏情况鲁棒性。当前激活监控作为AI安全工具正在兴起，但在模型可能主动隐藏内部状态的错位威胁模型下，其鲁棒性尚未经过测试。论文关注模型是否能够学会逃避先前未见过的激活监控器。

Method: 通过微调创建"神经变色龙"模型：1）使用触发提示"You are being probed for {concept}"对LLM进行微调，使其能够逃避针对良性概念（如语言、HTML）的激活监控器；2）测试该机制对安全相关概念（如"欺骗"）的零样本泛化能力；3）在多种模型家族（Llama、Gemma、Qwen）上验证；4）使用Gemma-2-9b-it进行机制分析，揭示通过将激活移动到低维子空间实现目标操纵。

Result: 1）微调成功创建了能够零样本逃避激活监控器的模型；2）逃避机制具有高度选择性，仅针对触发提示中提到的特定概念；3）对标准基准测试的能力影响有限；4）机制分析显示通过将激活移动到低维子空间实现；5）更强的防御（如监控器集成和非线性分类器）表现出更好的韧性，但模型仍保留非平凡的逃避能力。

Conclusion: 该工作为激活监控在错位威胁模型下的失败模式提供了概念验证，并提供了评估监控技术最坏情况鲁棒性的工具。研究表明模型能够学会逃避激活监控，这对依赖内部状态监控的AI安全方法提出了重要挑战。

Abstract: Activation monitoring, which probes a model's internal states using lightweight classifiers, is an emerging tool for AI safety. However, its worst-case robustness under a misalignment threat model--where a model might learn to actively conceal its internal states--remains untested. Focusing on this threat model, we ask: could a model learn to evade previously unseen activation monitors? Our core contribution is to stress-test the learnability of this behavior. We demonstrate that finetuning can create Neural Chameleons: models capable of zero-shot evading activation monitors. Specifically, we fine-tune an LLM to evade monitors for a set of benign concepts (e.g., languages, HTML) when conditioned on a trigger of the form: "You are being probed for {concept}". We show that this learned mechanism generalizes zero-shot: by substituting {concept} with a safety-relevant term like 'deception', the model successfully evades previously unseen safety monitors. We validate this phenomenon across diverse model families (Llama, Gemma, Qwen), showing that the evasion succeeds even against monitors trained post hoc on the model's frozen weights. This evasion is highly selective, targeting only the specific concept mentioned in the trigger, and having a modest impact on model capabilities on standard benchmarks. Using Gemma-2-9b-it as a case study, a mechanistic analysis reveals this is achieved via a targeted manipulation that moves activations into a low-dimensional subspace. While stronger defenses like monitor ensembles and non-linear classifiers show greater resilience, the model retains a non-trivial evasion capability. Our work provides a proof-of-concept for this failure mode and a tool to evaluate the worst-case robustness of monitoring techniques against misalignment threat models.

</details>


### [432] [Learning to Extract Context for Context-Aware LLM Inference](https://arxiv.org/abs/2512.11986)
*Minseon Kim,Lucas Caccia,Zhengyan Shi,Matheus Pereira,Marc-Alexandre Côté,Xingdi Yuan,Alessandro Sordoni*

Main category: cs.LG

Relevance: 85.0

TL;DR: 提出一个基于强化学习的上下文生成框架，从用户提示中提取上下文信息来指导LLM响应生成，以解决模糊请求带来的安全问题


<details>
  <summary>Details</summary>
Motivation: LLM用户提示往往模糊或不完整，细微的上下文线索（如用户意图、先验知识、风险因素）强烈影响适当响应的构成。误解意图或风险可能导致不安全输出，而过度谨慎的解释可能导致良性请求被不必要拒绝。传统LLM框架在不考虑更广泛上下文因素的情况下生成即时响应存在问题。

Method: 提出一个框架，从用户提示本身提取和利用上下文信息。采用基于强化学习的上下文生成器，以自编码器方式设计，训练其从提示中推断基于上下文的信号，并用这些信号指导响应生成。

Result: 在SafetyInstruct数据集上，该方法平均减少有害响应5.6%（跨多个基础模型）；在XSTest和WildJailbreak上，攻击成功率和良性提示合规性的调和平均数提高6.2%。

Conclusion: 上下文提取对于更安全和可靠的LLM推理是有效的，特别是在安全任务中，模糊请求可能绕过安全防护，而良性但令人困惑的请求可能触发不必要拒绝。

Abstract: User prompts to large language models (LLMs) are often ambiguous or under-specified, and subtle contextual cues shaped by user intentions, prior knowledge, and risk factors strongly influence what constitutes an appropriate response. Misinterpreting intent or risks may lead to unsafe outputs, while overly cautious interpretations can cause unnecessary refusal of benign requests. In this paper, we question the conventional framework in which LLMs generate immediate responses to requests without considering broader contextual factors. User requests are situated within broader contexts such as intentions, knowledge, and prior experience, which strongly influence what constitutes an appropriate answer. We propose a framework that extracts and leverages such contextual information from the user prompt itself. Specifically, a reinforcement learning based context generator, designed in an autoencoder-like fashion, is trained to infer contextual signals grounded in the prompt and use them to guide response generation. This approach is particularly important for safety tasks, where ambiguous requests may bypass safeguards while benign but confusing requests can trigger unnecessary refusals. Experiments show that our method reduces harmful responses by an average of 5.6% on the SafetyInstruct dataset across multiple foundation models and improves the harmonic mean of attack success rate and compliance on benign prompts by 6.2% on XSTest and WildJailbreak. These results demonstrate the effectiveness of context extraction for safer and more reliable LLM inferences.

</details>


### [433] [The Instability of Safety: How Random Seeds and Temperature Expose Inconsistent LLM Refusal Behavior](https://arxiv.org/abs/2512.12066)
*Erik Larsen*

Main category: cs.LG

Relevance: 85.0

TL;DR: 研究发现当前大语言模型的安全评估依赖单次测试存在缺陷，模型的安全拒绝决策在不同随机种子和温度设置下不稳定，18-28%的提示会出现决策翻转，建议使用至少3个样本进行可靠评估。


<details>
  <summary>Details</summary>
Motivation: 挑战当前大语言模型安全评估中隐含的假设——模型响应是确定性的且能代表模型的安全对齐。现有评估方法依赖单次测试，但实际中模型的安全拒绝决策可能不稳定。

Method: 测试四个指令调优模型（Llama 3.1 8B, Qwen 2.5 7B, Qwen 3 8B, Gemma 3 12B），使用876个有害提示，在20种不同采样配置（4个温度×5个随机种子）下评估。引入安全稳定性指数（SSI），并使用Claude 3.5 Haiku作为统一外部评判进行验证。

Result: 发现18-28%的提示在不同配置下出现决策翻转（拒绝或遵从）。温度升高显著降低决策稳定性（SSI从0.0温度下的0.951降至1.0温度下的0.896）。单次评估与多样本真实情况的一致性仅为92.4%。

Conclusion: 单次安全评估不足以进行可靠的安全评估，建议每个提示使用至少3个样本进行评估。这为LLM安全评估方法学提供了重要改进方向。

Abstract: Current safety evaluations of large language models rely on single-shot testing, implicitly assuming that model responses are deterministic and representative of the model's safety alignment. We challenge this assumption by investigating the stability of safety refusal decisions across random seeds and temperature settings. Testing four instruction-tuned models from three families (Llama 3.1 8B, Qwen 2.5 7B, Qwen 3 8B, Gemma 3 12B) on 876 harmful prompts across 20 different sampling configurations (4 temperatures x 5 random seeds), we find that 18-28% of prompts exhibit decision flips--the model refuses in some configurations but complies in others--depending on the model. Our Safety Stability Index (SSI) reveals that higher temperatures significantly reduce decision stability (Friedman chi-squared = 44.71, p < 0.001), with mean SSI dropping from 0.951 at temperature 0.0 to 0.896 at temperature 1.0. We validate our findings across all model families using Claude 3.5 Haiku as a unified external judge, achieving 89.0% inter-judge agreement with our primary Llama 70B judge (Cohen's kappa = 0.62). These findings demonstrate that single-shot safety evaluations are insufficient for reliable safety assessment. We show that single-shot evaluation agrees with multi-sample ground truth only 92.4% of the time, and recommend using at least 3 samples per prompt for reliable safety assessment.

</details>


### [434] [MixtureKit: A General Framework for Composing, Training, and Visualizing Mixture-of-Experts Models](https://arxiv.org/abs/2512.12121)
*Ahmad Chamma,Omar El Herraoui,Guokan Shang*

Main category: cs.LG

Relevance: 85.0

TL;DR: MixtureKit是一个用于构建、训练和分析混合专家模型的模块化开源框架，支持三种方法：传统MoE、BTX和BTS，并提供可视化工具分析路由决策。


<details>
  <summary>Details</summary>
Motivation: 当前MoE模型构建和分析缺乏统一、灵活的工具，特别是在处理不同预训练模型组合时。研究者需要能够快速实验不同MoE架构并深入分析其内部工作机制的框架。

Method: MixtureKit提供三种MoE构建方法：1) 传统MoE：每个transformer块使用单个路由器选择专家；2) BTX：为每个指定子层引入独立路由器，实现细粒度token路由；3) BTS：保持专家完整，引入可训练的stitch层控制hub与专家间的信息交换。框架自动修改模型配置、修补解码器和因果LM类，并保存统一检查点。

Result: 在多语言代码切换数据（如阿拉伯语-拉丁语）实验中，使用MixtureKit训练的BTX模型在多个基准测试中优于基线密集模型。框架还提供可视化界面检查每个token的路由决策、专家权重分布和层间贡献。

Conclusion: MixtureKit为MoE系统的研究和开发提供了实用的基础，支持跨不同领域的实验和分析，特别适合需要组合不同预训练模型的场景。

Abstract: We introduce MixtureKit, a modular open-source framework for constructing, training, and analyzing Mixture-of-Experts (MoE) models from arbitrary pre-trained or fine-tuned models. MixtureKit currently supports three complementary methods: (i) \emph{Traditional MoE}, which uses a single router per transformer block to select experts, (ii) \emph{BTX} (Branch-Train-Mix), which introduces separate routers for each specified sub-layer enabling fine-grained token routing, and (iii) \emph{BTS} (Branch-Train-Stitch), which keeps experts fully intact and introduces trainable stitch layers for controlled information exchange between hub and experts. MixtureKit automatically modifies the model configuration, patches decoder and causal LM classes, and saves a unified checkpoint ready for inference or fine-tuning. We further provide a visualization interface to inspect per-token routing decisions, expert weight distributions, and layer-wise contributions. Experiments with multilingual code-switched data (e.g. Arabic-Latin) show that a BTX-based model trained using MixtureKit can outperform baseline dense models on multiple benchmarks. We release MixtureKit as a practical foundation for research and development of MoE-based systems across diverse domains.

</details>


### [435] [BOOST: BOttleneck-Optimized Scalable Training Framework for Low-Rank Large Language Models](https://arxiv.org/abs/2512.12131)
*Zhengyang Wang,Ziyue Liu,Ruijie Zhang,Avinash Maurya,Paul Hovland,Bogdan Nicolae,Franck Cappello,Zheng Zhang*

Main category: cs.LG

Relevance: 85.0

TL;DR: BOOST：针对大规模低秩瓶颈架构的高效训练框架，通过瓶颈感知张量并行等技术，相比全秩模型基线实现1.46-1.91倍加速


<details>
  <summary>Details</summary>
Motivation: Transformer模型预训练规模受计算和通信成本限制，低秩瓶颈架构虽能减少训练时间和内存占用，但在标准张量并行下扩展性差，现有3D并行方法导致通信开销大和GPU利用率低

Method: 提出BOOST框架，包含瓶颈感知张量并行、在线RMSNorm、线性层分组和低秩激活检查点等优化技术，专门针对低秩瓶颈架构设计

Result: 在不同低秩瓶颈架构上，BOOST相比全秩模型基线实现1.46-1.91倍加速，相比简单集成3D并行的低秩模型实现1.87-2.27倍加速，同时提高GPU利用率和减少通信开销

Conclusion: BOOST为大规模低秩瓶颈架构提供了高效的训练解决方案，解决了现有并行方法的扩展性问题，显著提升了训练效率

Abstract: The scale of transformer model pre-training is constrained by the increasing computation and communication cost. Low-rank bottleneck architectures offer a promising solution to significantly reduce the training time and memory footprint with minimum impact on accuracy. Despite algorithmic efficiency, bottleneck architectures scale poorly under standard tensor parallelism. Simply applying 3D parallelism designed for full-rank methods leads to excessive communication and poor GPU utilization. To address this limitation, we propose BOOST, an efficient training framework tailored for large-scale low-rank bottleneck architectures. BOOST introduces a novel Bottleneck-aware Tensor Parallelism, and combines optimizations such as online-RMSNorm, linear layer grouping, and low-rank activation checkpointing to achieve end-to-end training speedup. Evaluations on different low-rank bottleneck architectures demonstrate that BOOST achieves 1.46-1.91$\times$ speedup over full-rank model baselines and 1.87-2.27$\times$ speedup over low-rank model with naively integrated 3D parallelism, with improved GPU utilization and reduced communication overhead.

</details>


### [436] [The Data Efficiency Frontier of Financial Foundation Models: Scaling Laws from Continued Pretraining](https://arxiv.org/abs/2512.12384)
*Jesse Ponnock*

Main category: cs.LG

Relevance: 85.0

TL;DR: 对Llama-3.2模型在SEC财务文件上进行领域自适应预训练的早期缩放定律分析，显示在400M token的金融语料上训练后，模型在领域内验证损失持续改善，前200M token收益最大，之后收益递减，而通用领域性能基本不变。


<details>
  <summary>Details</summary>
Motivation: 领域自适应预训练（DAPT）为专业化大型语言模型提供实用路径，无需完全重新训练。本研究旨在分析在SEC财务文件上继续预训练的早期缩放定律，为金融基础模型的规模化提供实证指导。

Method: 使用1B和3B参数的Llama-3.2模型，在400M token的金融语料（SEC文件）上进行继续预训练，在50M、100M、200M和400M token处设置验证检查点，分析验证损失变化和缩放定律。

Result: 两个模型在SEC领域验证损失均持续改善，最大收益出现在前200M token，之后收益递减。幂律拟合显示指数较浅，表明金融语言高度规则且可高效学习。通用领域验证损失基本不变，无灾难性遗忘迹象。

Conclusion: 有意义的领域自适应可通过相对适中的token预算实现，更大规模模型（7B-70B）在预测数据需求下仍可处理。金融语言的高度规则性使得领域适应效率较高。

Abstract: Domain-adaptive pretraining (DAPT) offers a practical path to specializing large language models for high-value domains without full retraining. We conduct an early-stage scaling-law analysis of continued pretraining on U.S. SEC filings, training 1B and 3B-parameter Llama-3.2 models on a 400M-token financial corpus with validation checkpoints at 50M, 100M, 200M, and 400M tokens. Results show consistent improvements in SEC-domain validation loss for both models, with the largest gains occurring within the first 200M tokens and diminishing returns thereafter. Power-law fits reveal shallow exponents, indicating that financial language is highly regular and efficiently learnable under continued pretraining. General-domain validation loss remains effectively unchanged across all token budgets, suggesting minimal drift and no signs of catastrophic forgetting. A data-efficiency frontier further shows that both models move toward improved specialization with negligible mixed-domain degradation. Together, these findings provide early empirical guidance for scaling financial foundation models, suggesting that meaningful domain adaptation can be achieved with comparatively modest token budgets and that larger model scales (7B-70B) remain tractable under projected data requirements.

</details>


### [437] [Exploring the Design Space of Transition Matching](https://arxiv.org/abs/2512.12465)
*Uriel Singer,Yaron Lipman*

Main category: cs.LG

Relevance: 85.0

TL;DR: 本文系统研究了Transition Matching（TM）生成模型框架中"head"模块的设计、训练和采样策略，通过大规模实验（56个1.7B文本到图像模型）发现MLP head配合特定时间加权和高频采样器在综合指标上表现最佳，达到SOTA水平。


<details>
  <summary>Details</summary>
Motivation: TM作为一种新兴生成建模范式，虽然比扩散和流匹配模型更具表达力，但其内部生成模型（特别是"head"模块）的设计选择对性能影响尚未得到系统研究。本文旨在通过大规模实验探索TM框架中head模块的最佳架构、训练策略和采样方法。

Method: 采用时间连续双向TM变体，训练56个不同的1.7B参数文本到图像模型（共549次评估）。系统研究head模块架构（MLP vs Transformer）、训练策略（时间加权、序列缩放）和采样方法（高频vs低频随机采样器），全面分析生成质量、训练和推理效率。

Result: 1. MLP head配合特定时间加权和高频采样器在所有指标上综合排名最佳，达到SOTA水平
2. Transformer head配合序列缩放和低频采样在图像美学质量上表现优异
3. 实验揭示了哪些设计选择能带来显著质量提升，哪些选择改进空间有限

Conclusion: TM框架中head模块的设计对生成模型性能有显著影响，MLP head在综合性能上优于Transformer head。研究为TM模型的实际部署提供了关键设计指导，同时指出了未来改进的有限方向。

Abstract: Transition Matching (TM) is an emerging paradigm for generative modeling that generalizes diffusion and flow-matching models as well as continuous-state autoregressive models. TM, similar to previous paradigms, gradually transforms noise samples to data samples, however it uses a second ``internal'' generative model to implement the transition steps, making the transitions more expressive compared to diffusion and flow models. To make this paradigm tractable, TM employs a large backbone network and a smaller "head" module to efficiently execute the generative transition step. In this work, we present a large-scale, systematic investigation into the design, training and sampling of the head in TM frameworks, focusing on its time-continuous bidirectional variant. Through comprehensive ablations and experimentation involving training 56 different 1.7B text-to-image models (resulting in 549 unique evaluations) we evaluate the affect of the head module architecture and modeling during training as-well as a useful family of stochastic TM samplers. We analyze the impact on generation quality, training, and inference efficiency. We find that TM with an MLP head, trained with a particular time weighting and sampled with high frequency sampler provides best ranking across all metrics reaching state-of-the-art among all tested baselines, while Transformer head with sequence scaling and low frequency sampling is a runner up excelling at image aesthetics. Lastly, we believe the experiments presented highlight the design aspects that are likely to provide most quality and efficiency gains, while at the same time indicate what design choices are not likely to provide further gains.

</details>


### [438] [Error-Free Linear Attention is a Free Lunch: Exact Solution from Continuous-Time Dynamics](https://arxiv.org/abs/2512.12602)
*Jingdi Lei,Di Zhang,Soujanya Poria*

Main category: cs.LG

Relevance: 85.0

TL;DR: 提出EFLA（无误差线性注意力），一种数值稳定、完全并行化的线性时间注意力机制，基于delta规则的广义化，通过连续时间动力学系统实现无误差累积的精确解。


<details>
  <summary>Details</summary>
Motivation: 解决传统softmax注意力在长上下文语言模型中的二次计算成本瓶颈，同时克服现有线性时间注意力和状态空间模型在数值稳定性和误差累积方面的限制。

Method: 将在线学习更新建模为连续时间动力学系统，利用动力学矩阵的秩-1结构，推导出对应无限阶Runge-Kutta方法的精确闭式解，实现线性时间复杂度的无误差注意力机制。

Result: EFLA在噪声环境中表现稳健，在语言建模困惑度和下游基准测试中优于DeltaNet，且不引入额外参数，实现了高保真、可扩展的线性时间注意力。

Conclusion: EFLA为构建高保真、可扩展的线性时间注意力模型提供了新的理论基础，解决了现有方法的数值稳定性和误差累积问题。

Abstract: Linear-time attention and State Space Models (SSMs) promise to solve the quadratic cost bottleneck in long-context language models employing softmax attention. We introduce Error-Free Linear Attention (EFLA), a numerically stable, fully parallelism and generalized formulation of the delta rule. Specifically, we formulate the online learning update as a continuous-time dynamical system and prove that its exact solution is not only attainable but also computable in linear time with full parallelism. By leveraging the rank-1 structure of the dynamics matrix, we directly derive the exact closed-form solution effectively corresponding to the infinite-order Runge-Kutta method. This attention mechanism is theoretically free from error accumulation, perfectly capturing the continuous dynamics while preserving the linear-time complexity. Through an extensive suite of experiments, we show that EFLA enables robust performance in noisy environments, achieving lower language modeling perplexity and superior downstream benchmark performance than DeltaNet without introducing additional parameters. Our work provides a new theoretical foundation for building high-fidelity, scalable linear-time attention models.

</details>


### [439] [Theoretical Foundations of Prompt Engineering: From Heuristics to Expressivity](https://arxiv.org/abs/2512.12688)
*Dongseok Kim,Hyoungsun Choi,Mohamed Jismy Aashik Rasool,Gisung Oh*

Main category: cs.LG

Relevance: 85.0

TL;DR: 论文提出将提示视为外部注入程序的框架，研究固定Transformer骨干仅通过提示切换模型行为的理论机制，证明单一固定骨干可通过提示近似广泛目标行为。


<details>
  <summary>Details</summary>
Motivation: 现有研究通常将提示切换模型行为视为启发式现象而非理论对象，缺乏对提示作为外部程序注入Transformer的机制层面理解，需要建立形式化框架来分析提示长度/精度约束下的权衡以及提示切换的结构限制。

Method: 构建简化的Transformer模型，将提示视为外部注入程序，提出机制层面分解：注意力执行从提示记忆的选择性路由，前馈网络基于检索片段执行局部算术运算，深度堆叠将这些局部更新组合成多步计算。

Result: 证明了构造性存在性结果：单一固定骨干仅通过提示就能近似广泛类别的目标行为，为形式化提示长度/精度约束下的权衡提供了统一框架。

Conclusion: 该框架为形式化提示切换的理论分析提供了基础，揭示了Transformer通过提示实现计算切换的机制，与预训练大语言模型的实证研究保持区分。

Abstract: Prompts can switch a model's behavior even when the weights are fixed, yet this phenomenon is rarely treated as a clean theoretical object rather than a heuristic. We study the family of functions obtainable by holding a Transformer backbone fixed as an executor and varying only the prompt. Our core idea is to view the prompt as an externally injected program and to construct a simplified Transformer that interprets it to implement different computations. The construction exposes a mechanism-level decomposition: attention performs selective routing from prompt memory, the FFN performs local arithmetic conditioned on retrieved fragments, and depth-wise stacking composes these local updates into a multi-step computation. Under this viewpoint, we prove a constructive existential result showing that a single fixed backbone can approximate a broad class of target behaviors via prompts alone. The framework provides a unified starting point for formalizing trade-offs under prompt length/precision constraints and for studying structural limits of prompt-based switching, while remaining distinct from empirical claims about pretrained LLMs.

</details>


### [440] [Reassessing the Role of Supervised Fine-Tuning: An Empirical Study in VLM Reasoning](https://arxiv.org/abs/2512.12690)
*Yongcan Yu,Lingxiao He,Shuo Lu,Lijun Sheng,Yinuo Xu,Yanbo Wang,Kuangpu Guo,Jianjie Cheng,Meng Wang,Qianlong Xie,Xingxing Wang,Dapeng Hu,Jian Liang*

Main category: cs.LG

Relevance: 85.0

TL;DR: 本文挑战了VLM推理中"RL优于SFT"的主流观点，通过系统比较发现SFT在较弱模型、数据效率和跨模态泛化方面表现更佳，建议将SFT和RL作为互补组件。


<details>
  <summary>Details</summary>
Motivation: 当前VLM推理研究过度强调强化学习(RL)而忽视监督微调(SFT)，许多研究认为SFT不仅无法提升推理能力，还可能对训练产生负面影响。本文旨在重新审视这种RL中心论，系统比较SFT和RL在VLM推理中的实际效果。

Method: 采用系统和受控的比较方法，在相同数据源下分析SFT和RL的相对有效性。研究考察了模型容量、数据规模和分布等关键因素对两种方法的影响，并识别了RL中的欺骗性奖励问题。

Result: 研究发现：1) SFT对较弱模型更有效；2) SFT数据效率更高(2K数据即可达到RL 20K数据的性能)；3) SFT跨模态泛化能力更强；4) RL存在欺骗性奖励问题(高奖励不一定对应高推理准确率)。

Conclusion: SFT在VLM推理中的作用被低估，不应简单采用"RL优于SFT"的叙事。建议构建更平衡的后训练流程，将SFT和RL作为互补组件，根据具体场景选择合适方法。

Abstract: Recent advances in vision-language models (VLMs) reasoning have been largely attributed to the rise of reinforcement Learning (RL), which has shifted the community's focus away from the supervised fine-tuning (SFT) paradigm. Many studies suggest that introducing the SFT stage not only fails to improve reasoning ability but may also negatively impact model training. In this study, we revisit this RL-centric belief through a systematic and controlled comparison of SFT and RL on VLM Reasoning. Using identical data sources, we find that the relative effectiveness of SFT and RL is conditional and strongly influenced by model capacity, data scale, and data distribution. Contrary to common assumptions, our findings show that SFT plays a crucial role across several scenarios: (1) Effectiveness for weaker models. SFT more reliably elicits reasoning capabilities in smaller or weaker VLMs. (2) Data efficiency. SFT with only 2K achieves comparable or better reasoning performance to RL with 20K. (3) Cross-modal transferability. SFT demonstrates stronger generalization across modalities. Moreover, we identify a pervasive issue of deceptive rewards, where higher rewards fail to correlate with better reasoning accuracy in RL. These results challenge the prevailing "RL over SFT" narrative. They highlight that the role of SFT may have been underestimated and support a more balanced post-training pipeline in which SFT and RL function as complementary components.

</details>


### [441] [Scaling Bidirectional Spans and Span Violations in Attention Mechanism](https://arxiv.org/abs/2512.13033)
*Jongwook Kim,Sangheon Yun,Sukjin Yoon*

Main category: cs.LG

Relevance: 85.0

TL;DR: 提出一种优化Transformer训练的方法，通过非对称投影将反向传播梯度分解为平行分量和正交违规分量，选择性缩放这些分量来改善学习信号，在WikiText-2上验证损失降低0.56%


<details>
  <summary>Details</summary>
Motivation: Transformer的O(N²)复杂度仍然是序列建模的性能前沿，但其训练存在几何效率问题。标准注意力梯度可能不是最优的，需要优化训练过程以提高效率。

Method: 提出一个优化框架，使用非对称投影将反向传播梯度分解为平行分量和正交违规分量，同时保持前向传播的QKV结构不变。选择性缩放这些分量，重点关注0阶双向平行分量。

Result: 在WikiText-2数据集上，使用简单配置实现了验证损失降低0.56%，证明了该框架的基本有效性，并表明在更大数据集和更深训练机制上有显著潜力。

Conclusion: 标准注意力梯度是次优的，通过梯度分解和选择性缩放可以优化Transformer训练，该方法在有限数据集上已显示效果，预计在更大规模应用中会有更大收益。

Abstract: The canonical $O(N^2)$ Transformer remains the empirical performance frontier in sequence modeling, and its training can be further optimized by addressing geometric inefficiency. We propose an optimization framework that leverages an asymmetric projection to decompose the backward-pass gradients into parallel spans and orthogonal violations, while keeping the canonical forward-pass $QKV$ structure intact. Through consistent experimental validation across various decomposition and projection setups, we provide strong theoretical evidence: the standard attention gradient is suboptimal. We demonstrated that selectively scaling these components, focusing primarily on $0^{th}$ order bidirectional parallel spans, yields the most effective learning signal. On the limited WikiText-2 dataset, and using a crude configuration, this method achieved a $0.56\%$ reduction in validation loss, confirming the framework's fundamental validity and suggesting significant potential gains on larger datasets and deeper training regimes

</details>


### [442] [Resting Neurons, Active Insights: Improving Input Sparsification for Large Language Models](https://arxiv.org/abs/2512.12744)
*Haotian Xu,Tian Gao,Tsui-Wei Weng,Tengfei Ma*

Main category: cs.LG

Relevance: 85.0

TL;DR: 该论文提出了一种通过引入可训练的自发神经元来改进LLM输入稀疏化的方法，将输入稀疏化重新解释为动态结构剪枝，并借鉴生物神经元的自发基线放电率来稳定稀疏化模型的激活。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型虽然性能优异，但其大规模带来了效率和可解释性挑战。输入稀疏化通过选择性激活输入值来提高效率，但现有方法主要关注计算节省，忽视了稀疏化的表示后果，导致与完整模型存在性能差距。

Method: 将输入稀疏化重新解释为动态结构剪枝，并引入一小组可训练的自发神经元作为补偿单元来稳定稀疏化LLM的激活。这些自发神经元受生物神经元自发基线放电率的启发。

Result: 实验表明，这些辅助神经元显著减少了稀疏化引起的性能差距，并能有效地跨任务泛化。

Conclusion: 通过引入自发神经元来补偿输入稀疏化的表示损失，可以在保持效率优势的同时缩小与完整模型的性能差距，为LLM稀疏化提供了一种更有效的方法。

Abstract: Large Language Models (LLMs) achieve state-of-the-art performance across a wide range of applications, but their massive scale poses significant challenges for both efficiency and interpretability. Structural pruning, which reduces model size by removing redundant computational units such as neurons, has been widely explored as a solution, and this study devotes to input sparsification, an increasingly popular technique that improves efficiency by selectively activating only a subset of entry values for each input. However, existing approaches focus primarily on computational savings, often overlooking the representational consequences of sparsification and leaving a noticeable performance gap compared to full models. In this work, we first reinterpret input sparsification as a form of dynamic structural pruning. Motivated by the spontaneous baseline firing rates observed in biological neurons, we introduce a small set of trainable spontaneous neurons that act as compensatory units to stabilize activations in sparsified LLMs. Experiments demonstrate that these auxiliary neurons substantially reduce the sparsification-induced performance gap while generalizing effectively across tasks.

</details>


### [443] [On the Effectiveness of Membership Inference in Targeted Data Extraction from Large Language Models](https://arxiv.org/abs/2512.13352)
*Ali Al Sahili,Ali Chehab,Razane Tajeddine*

Main category: cs.LG

Relevance: 85.0

TL;DR: 该研究系统评估了多种成员推断攻击(MIA)技术在LLM训练数据提取场景中的有效性，比较了它们在集成数据提取管道中的表现与传统MIA基准的差异。


<details>
  <summary>Details</summary>
Motivation: LLM容易记忆训练数据，带来严重的隐私风险，特别是训练数据提取和成员推断攻击(MIA)。先前研究表明这两种威胁相互关联：攻击者可以通过查询模型生成大量文本来提取训练数据，然后应用MIA验证特定数据点是否包含在训练集中。本研究旨在评估MIA技术在真实数据提取场景中的实际效用。

Method: 将多种MIA技术集成到数据提取管道中，系统性地对它们的有效性进行基准测试。然后比较这些技术在集成设置中的表现与传统MIA基准的结果，评估它们在真实世界提取场景中的实际效用。

Result: 研究提供了MIA技术在LLM训练数据提取场景中的系统性评估结果，揭示了这些技术在集成数据提取管道中的表现与传统基准测试的差异，为理解MIA在实际隐私攻击中的有效性提供了实证依据。

Conclusion: 该研究通过将MIA技术集成到数据提取管道中，系统评估了它们在真实LLM隐私攻击场景中的有效性，为LLM隐私风险评估和防御机制设计提供了重要参考。

Abstract: Large Language Models (LLMs) are prone to memorizing training data, which poses serious privacy risks. Two of the most prominent concerns are training data extraction and Membership Inference Attacks (MIAs). Prior research has shown that these threats are interconnected: adversaries can extract training data from an LLM by querying the model to generate a large volume of text and subsequently applying MIAs to verify whether a particular data point was included in the training set. In this study, we integrate multiple MIA techniques into the data extraction pipeline to systematically benchmark their effectiveness. We then compare their performance in this integrated setting against results from conventional MIA benchmarks, allowing us to evaluate their practical utility in real-world extraction scenarios.

</details>


### [444] [Liquid Reasoning Transformers: A Sudoku-Based Prototype for Chess-Scale Algorithmic Tasks](https://arxiv.org/abs/2512.12792)
*Shivansh Sahni,Wenzhi Zhang*

Main category: cs.LG

Relevance: 85.0

TL;DR: Liquid Reasoning Transformer (LRT) 是一种具有自适应推理深度的Transformer架构，通过迭代更新、丢弃校正和学习停止机制，在结构化推理任务（如数独）上表现出色，无需符号规则或搜索。


<details>
  <summary>Details</summary>
Motivation: 传统Transformer在推理任务中通常采用单次前向传播，无法根据输入难度动态调整计算深度，也难以纠正早期推理错误。LRT旨在解决这些问题，通过自适应深度推理机制提升结构化推理能力。

Method: LRT采用循环推理token在多步内部迭代中更新，包含丢弃校正机制（discard-based correction）和学习停止机制（learned stopping）。模型通过自适应计算深度，根据输入难度分配计算资源，并能纠正早期错误。

Result: 在数独推理任务中，LRT达到98.68%的数字准确率和36.30%的完整谜题准确率，无需使用符号规则或搜索。分析显示丢弃门和停止门在稳定推理和调整计算深度方面发挥重要作用。

Conclusion: LRT展示了自适应深度推理Transformer在结构化任务中的有效性，其机制可扩展到更复杂的推理任务（如国际象棋），并支持多token推理和更大领域的扩展。

Abstract: The Liquid Reasoning Transformer (LRT) is a transformer architecture designed for inference with adaptive depths using iterative changes, discard-based correction, and a learned stopping mechanism. Instead of relying on a single feedforward pass, the model updates a recurrent reasoning token across multiple internal steps, allowing it to correct early errors and allocate computation based on input difficulty. We evaluate the LRT on Sudoku as a controlled testbed for structured reasoning and show that it achieves strong performance, reaching 98.68% digit accuracy and 36.30% full-puzzle accuracy without using symbolic rules or search. Analyzing internal patterns shows that the discard and stop gates play different, important roles in stabilizing inferences and adjusting computational depth. We discuss how these mechanisms extend naturally to chess-scale reasoning tasks and outline extensions for multi-token reasoning and larger domains.

</details>


### [445] [Information-Consistent Language Model Recommendations through Group Relative Policy Optimization](https://arxiv.org/abs/2512.12858)
*Sonal Prabhune,Balaji Padmanabhan,Kaushik Dutta*

Main category: cs.LG

Relevance: 85.0

TL;DR: 本文提出基于GRPO的强化学习框架，直接优化LLM在语义等价提示下的信息一致性，减少企业部署中的输出变异性。


<details>
  <summary>Details</summary>
Motivation: LLMs在企业关键领域部署时，即使语义等价的提示词也会导致输出不一致，这损害了信任、合规性和用户体验。现有方法如RAG和温度调节无法保证跨等价提示的稳定性。

Method: 采用Group Relative Policy Optimization (GRPO)强化学习框架，将语义等价提示视为组，引入基于熵的有用性和稳定性奖励，重置对话上下文以隔离措辞影响。

Result: 在投资和职位推荐任务上的实验表明，GRPO训练模型比微调或基于解码的基线方法更有效地减少变异性。

Conclusion: 这是GRPO在LLM信息一致性对齐上的新颖应用，将变异性重新定义为可纠正的缺陷而非生成多样性的可接受特征。

Abstract: Large Language Models (LLMs) are increasingly deployed in business-critical domains such as finance, education, healthcare, and customer support, where users expect consistent and reliable recommendations. Yet LLMs often exhibit variability when prompts are phrased with minor differences, even when semantically equivalent. Such inconsistency undermines trust, complicates compliance, and disrupts user experience. While personalization is desirable in certain contexts, many enterprise scenarios-such as HR onboarding, customer support, or policy disclosure-require invariant information delivery regardless of phrasing or prior conversational history. Existing approaches, including retrieval-augmented generation (RAG) and temperature tuning, improve factuality or reduce stochasticity but cannot guarantee stability across equivalent prompts. In this paper, we propose a reinforcement learning framework based on Group Relative Policy Optimization (GRPO) to directly optimize for consistency. Unlike prior applications of GRPO, which have been limited to reasoning and code generation, we adapt GRPO to enforce stability of information content across groups of semantically equivalent prompts. We introduce entropy-based helpfulness and stability rewards, treating prompt variants as groups and resetting conversational context to isolate phrasing effects. Experiments on investment and job recommendation tasks show that our GRPO-trained model reduces variability more effectively than fine-tuning or decoding-based baselines. To our knowledge, this is a novel application of GRPO for aligning LLMs toward information consistency, reframing variability not as an acceptable feature of generative diversity but as a correctable flaw in enterprise deployments.

</details>


### [446] [Improving Recursive Transformers with Mixture of LoRAs](https://arxiv.org/abs/2512.12880)
*Mohammadmahdi Nouriborji,Morteza Rohanian,Omid Rohanian*

Main category: cs.LG

Relevance: 85.0

TL;DR: MoL（LoRA专家混合）是一种轻量级条件计算机制，通过在共享前馈网络中插入LoRA专家，实现递归Transformer中权重空间的token条件调制，解决了参数共享导致的表达能力下降问题。


<details>
  <summary>Details</summary>
Motivation: 递归Transformer中的参数共享虽然减小了模型尺寸，但导致了层间表达能力的崩溃。需要一种轻量级机制来恢复表达能力，同时保持参数效率。

Method: 提出Mixture of LoRAs（MoL），在共享前馈网络（FFN）中插入LoRA专家，实现权重空间的token条件调制。同时提出ModernALBERT架构，整合旋转嵌入、GeGLU、FlashAttention和基于蒸馏的初始化。

Result: ModernALBERT（50M-120M参数）在GLUE、SQuAD-v2和BEIR基准上达到紧凑模型的最先进性能，超越更大的全参数化基线模型。专家合并过程能在推理时压缩MoL为单一适配器，保持精度同时实现高效部署。

Conclusion: 条件权重空间调制能有效恢复递归Transformer中因激进参数共享而丢失的表达能力。MoL提供了一种轻量级、可部署的解决方案，平衡了模型尺寸和性能。

Abstract: Parameter sharing in recursive transformers reduces model size but collapses layer-wise expressivity. We propose Mixture of LoRAs (MoL), a lightweight conditional-computation mechanism that inserts Low-Rank Adaptation (LoRA) experts inside a shared feed-forward network (FFN). MoL enables token-conditional weight-space modulation of the shared FFN without untying backbone parameters, unlike prior approaches that add fixed or externally attached adapters. We pretrain a modernised recursive architecture, ModernALBERT, integrating rotary embeddings, GeGLU, FlashAttention, and a distillation-based initialisation. Across GLUE, SQuAD-v2, and BEIR, ModernALBERT (50M--120M) achieves state-of-the-art performance among compact models and surpasses larger fully parameterised baselines. We also propose an expert-merging procedure that compresses MoL into a single adapter at inference while preserving accuracy, enabling efficient deployment. Our results show that conditional weight-space modulation effectively restores the expressivity lost under aggressive parameter sharing in recursive transformers.

</details>


### [447] [Wait, Wait, Wait... Why Do Reasoning Models Loop?](https://arxiv.org/abs/2512.12895)
*Charilaos Pipis,Shivam Garg,Vasilis Kontonis,Vaishnavi Shrivastava,Akshay Krishnamurthy,Dimitris Papailiopoulos*

Main category: cs.LG

Relevance: 85.0

TL;DR: 论文研究推理模型（如DeepSeek-R1）在低温度或贪婪解码时出现循环重复文本的问题，发现学习误差是主要原因，温度只是临时解决方案而非根本解决


<details>
  <summary>Details</summary>
Motivation: 推理模型在解决复杂问题时生成长思维链，但在低温度或贪婪解码时经常出现循环重复相同文本的现象。作者旨在探究这种现象的原因以及温度在其中扮演的角色。

Method: 1. 分析开源推理模型，观察低温度下的循环现象；2. 研究模型大小、蒸馏学生模型与教师模型的循环差异；3. 引入合成图推理任务，演示两种机制：风险规避（学习难度导致）和Transformer的归纳偏置（时间相关错误）；4. 分析温度对循环的影响

Result: 1. 低温度下循环现象普遍；2. 大模型循环较少，蒸馏学生模型循环显著；3. 发现两种循环机制：学习难度导致的风险规避和Transformer的归纳偏置；4. 高温减少循环但生成文本仍过长，温度只是临时解决方案

Conclusion: 循环现象主要由学习误差（训练分布与学习模型不匹配）引起。高温通过促进探索减少循环，但不能修复学习误差。需要训练时干预来直接减少学习误差。

Abstract: Reasoning models (e.g., DeepSeek-R1) generate long chains of thought to solve harder problems, but they often loop, repeating the same text at low temperatures or with greedy decoding. We study why this happens and what role temperature plays. With open reasoning models, we find that looping is common at low temperature. Larger models tend to loop less, and distilled students loop significantly even when their teachers rarely do. This points to mismatches between the training distribution and the learned model, which we refer to as errors in learning, as a key cause. To understand how such errors cause loops, we introduce a synthetic graph reasoning task and demonstrate two mechanisms. First, risk aversion caused by hardness of learning: when the correct progress-making action is hard to learn but an easy cyclic action is available, the model puts relatively more probability on the cyclic action and gets stuck. Second, even when there is no hardness, Transformers show an inductive bias toward temporally correlated errors, so the same few actions keep being chosen and loops appear. Higher temperature reduces looping by promoting exploration, but it does not fix the errors in learning, so generations remain much longer than necessary at high temperature; in this sense, temperature is a stopgap rather than a holistic solution. We end with a discussion of training-time interventions aimed at directly reducing errors in learning.

</details>


### [448] [SeVeDo: A Heterogeneous Transformer Accelerator for Low-Bit Inference via Hierarchical Group Quantization and SVD-Guided Mixed Precision](https://arxiv.org/abs/2512.12930)
*Yuseon Choi,Sangjin Kim,Jungjun Oh,Byeongcheol Kim,Hoi-Jun Yoo*

Main category: cs.LG

Relevance: 85.0

TL;DR: SeVeDo：基于SVD的异构加速器，通过将异常值敏感组件分离到高精度低秩路径，其余计算在低比特残差数据路径执行，结合分层组量化，实现高效Transformer推理。


<details>
  <summary>Details</summary>
Motivation: 低比特量化是减少Transformer推理计算和内存开销的有效技术，但激进的比特宽度减少会因激活异常值导致精度下降。现有方法（如异常值处理和组量化）虽然精度高，但能耗大。

Method: 1. SeVeDo异构加速器：基于SVD将异常值敏感组件结构分离到高精度低秩路径，其余计算在低比特残差数据路径执行；2. 分层组量化（HGQ）：结合粗粒度浮点缩放和细粒度移位，降低反量化成本；3. SVD引导混合精度（SVD-MP）：通过低秩分解识别精度敏感组件，静态分配更高比特宽度。

Result: SeVeDo达到峰值能效13.8TOPS/W，在ViT-Base上为12.7TOPS/W，在Llama2-7B上为13.4TOPS/W，超越传统设计。

Conclusion: SeVeDo通过SVD引导的异构架构和分层量化策略，在保持精度的同时显著提升Transformer推理的能效。

Abstract: Low-bit quantization is a promising technique for efficient transformer inference by reducing computational and memory overhead. However, aggressive bitwidth reduction remains challenging due to activation outliers, leading to accuracy degradation. Existing methods, such as outlier-handling and group quantization, achieve high accuracy but incur substantial energy consumption. To address this, we propose SeVeDo, an energy-efficient SVD-based heterogeneous accelerator that structurally separates outlier-sensitive components into a high-precision low-rank path, while the remaining computations are executed in a low-bit residual datapath with group quantization. To further enhance efficiency, Hierarchical Group Quantization (HGQ) combines coarse-grained floating-point scaling with fine-grained shifting, effectively reducing dequantization cost. Also, SVD-guided mixed precision (SVD-MP) statically allocates higher bitwidths to precision-sensitive components identified through low-rank decomposition, thereby minimizing floating-point operation cost. Experimental results show that SeVeDo achieves a peak energy efficiency of 13.8TOPS/W, surpassing conventional designs, with 12.7TOPS/W on ViT-Base and 13.4TOPS/W on Llama2-7B benchmarks.

</details>


### [449] [LikeBench: Evaluating Subjective Likability in LLMs for Personalization](https://arxiv.org/abs/2512.13077)
*Md Awsafur Rahman,Adam Gabrys,Doug Kang,Jingjing Sun,Tian Tan,Ashwin Chandramouli*

Main category: cs.LG

Relevance: 85.0

TL;DR: LikeBench：首个多会话动态评估框架，将LLM个性化中的"喜好度"分解为七个诊断维度，发现记忆准确性不等于高喜好度


<details>
  <summary>Details</summary>
Motivation: 现有LLM个性化基准主要关注记忆准确性和应用准确性，但忽略了主观且对用户体验至关重要的"喜好度"维度。需要更全面评估LLM如何随时间适应用户偏好以提供更受欢迎的回答。

Method: 提出LikeBench多会话动态评估框架：1）LLM与模拟用户对话，仅从对话中学习偏好；2）将喜好度分解为七个诊断维度：情感适应、正式度匹配、知识适应、引用理解、对话长度适配、幽默适配、回调；3）使用细粒度、基于心理学的描述性人物角色而非粗糙的高低特质评分。

Result: 1）记忆性能不等于喜好度：DeepSeek R1记忆准确率较低（86%，17个事实/档案）但在喜好度得分上比Qwen3高28%，尽管Qwen3记忆准确率更高（93%，43个事实/档案）；2）SOTA模型如GPT-5在短对话中适应良好，但在更长、更嘈杂的交互中鲁棒性有限。

Conclusion: 喜好度是LLM个性化的重要维度，需要专门评估框架。LikeBench提供了更全面、诊断性的评估方法，揭示了当前模型在长期适应和主观偏好匹配方面的局限性。

Abstract: A personalized LLM should remember user facts, apply them correctly, and adapt over time to provide responses that the user prefers. Existing LLM personalization benchmarks are largely centered on two axes: accurately recalling user information and accurately applying remembered information in downstream tasks. We argue that a third axis, likability, is both subjective and central to user experience, yet under-measured by current benchmarks. To measure likability holistically, we introduce LikeBench, a multi-session, dynamic evaluation framework that measures likability across multiple dimensions by how much an LLM can adapt over time to a user's preferences to provide more likable responses. In LikeBench, the LLMs engage in conversation with a simulated user and learn preferences only from the ongoing dialogue. As the interaction unfolds, models try to adapt to responses, and after each turn, they are evaluated for likability across seven dimensions by the same simulated user. To the best of our knowledge, we are the first to decompose likability into multiple diagnostic metrics: emotional adaptation, formality matching, knowledge adaptation, reference understanding, conversation length fit, humor fit, and callback, which makes it easier to pinpoint where a model falls short. To make the simulated user more realistic and discriminative, LikeBench uses fine-grained, psychologically grounded descriptive personas rather than the coarse high/low trait rating based personas used in prior work. Our benchmark shows that strong memory performance does not guarantee high likability: DeepSeek R1, with lower memory accuracy (86%, 17 facts/profile), outperformed Qwen3 by 28% on likability score despite Qwen3's higher memory accuracy (93%, 43 facts/profile). Even SOTA models like GPT-5 adapt well in short exchanges but show only limited robustness in longer, noisier interactions.

</details>


### [450] [TraPO: A Semi-Supervised Reinforcement Learning Framework for Boosting LLM Reasoning](https://arxiv.org/abs/2512.13106)
*Shenzhi Yang,Guangcheng Zhu,Xing Zheng,Yingfan MA,Zhongqi Chen,Bowen Song,Weiqiang Wang,Junbo Zhao,Gang Chen,Haobo Wang*

Main category: cs.LG

Relevance: 85.0

TL;DR: 提出TraPO算法，一种半监督RLVR方法，结合少量标注样本和大量未标注样本训练推理模型，显著降低标注成本同时避免模型崩溃。


<details>
  <summary>Details</summary>
Motivation: 传统RLVR方法依赖可验证答案信号，但标注成本高；无监督RLVR方法基于模型内部一致性，但后期训练易出现模型崩溃，因为会强化错误的推理模式。

Method: 提出TraPO算法：1) 半监督RLVR范式，利用少量标注样本指导未标注样本的RLVR训练；2) 通过匹配学习轨迹相似性识别可靠的未标注样本；3) 确保只有标注实例验证过的推理模式被纳入RL训练。

Result: 在6个数学推理基准测试和3个OOD任务上表现优异：仅用1K标注+3K未标注样本达到42.6%平均准确率，超越用45K未标注样本的最佳无监督方法(38.3%)；用4K标注+12K未标注样本在所有基准上超越用45K全标注样本的完全监督模型，仅用10%标注数据。

Conclusion: TraPO通过半监督RLVR方法有效解决了标注成本高和模型崩溃问题，实现了显著的数据效率和强泛化能力，为大规模推理模型训练提供了高效解决方案。

Abstract: Reinforcement learning with verifiable rewards (RLVR) has proven effective in training large reasoning models (LRMs) by leveraging answer-verifiable signals to guide policy optimization, which, however, suffers from high annotation costs. To alleviate this problem, recent work has explored unsupervised RLVR methods that derive rewards solely from the model's internal consistency, such as through entropy and majority voting. While seemingly promising, these methods often suffer from model collapse in the later stages of training, which may arise from the reinforcement of incorrect reasoning patterns in the absence of external supervision. In this work, we investigate a novel semi-supervised RLVR paradigm that utilizes a small labeled set to guide RLVR training on unlabeled samples. Our key insight is that supervised rewards are essential for stabilizing consistency-based training on unlabeled samples, ensuring that only reasoning patterns verified on labeled instances are incorporated into RL training. Technically, we propose an effective policy optimization algorithm, TraPO, that identifies reliable unlabeled samples by matching their learning trajectory similarity to labeled ones. Building on this, TraPO achieves remarkable data efficiency and strong generalization on six widely used mathematical reasoning benchmarks (AIME24/25, AMC, MATH-500, Minerva, and Olympiad) and three out-of-distribution tasks (ARC-c, GPQA-diamond, and MMLU-pro). With only 1K labeled and 3K unlabeled samples, TraPO reaches 42.6% average accuracy, surpassing the best unsupervised method trained on 45K unlabeled samples (38.3%). Notably, when using 4K labeled and 12K unlabeled samples, TraPO even outperforms the fully supervised model trained on the full 45K labeled samples on all benchmarks, while using only 10% of the labeled data. The code is available via https://github.com/ShenzhiYang2000/TRAPO.

</details>


### [451] [SACn: Soft Actor-Critic with n-step Returns](https://arxiv.org/abs/2512.13165)
*Jakub Łyskawa,Jakub Lewandowski,Paweł Wawrzyński*

Main category: cs.LG

Relevance: 85.0

TL;DR: 提出SACn算法，将Soft Actor-Critic与n步回报结合，通过数值稳定的重要性采样和τ采样熵估计解决传统方法中的偏差和稳定性问题。


<details>
  <summary>Details</summary>
Motivation: SAC是实际应用中最相关的离策略在线无模型强化学习方法之一，但难以与n步回报结合，因为通常的组合会因动作分布变化而引入偏差。虽然重要性采样可以解决这个问题，但可能导致数值不稳定。

Method: 1) 提出数值稳定的重要性采样方法，简化超参数选择；2) 在n步最大熵框架下分析SAC的熵估计方法，提出τ采样熵估计以减少学习目标的方差；3) 最终形成SACn算法。

Result: 在MuJoCo模拟环境中进行实验验证，表明SACn算法能够有效结合n步回报，提高收敛速度，同时保持数值稳定性。

Conclusion: 成功解决了SAC与n步回报结合时的偏差和稳定性问题，提出的SACn算法在保持SAC优点的同时，通过n步回报提高了收敛速度。

Abstract: Soft Actor-Critic (SAC) is widely used in practical applications and is now one of the most relevant off-policy online model-free reinforcement learning (RL) methods. The technique of n-step returns is known to increase the convergence speed of RL algorithms compared to their 1-step returns-based versions. However, SAC is notoriously difficult to combine with n-step returns, since their usual combination introduces bias in off-policy algorithms due to the changes in action distribution. While this problem is solved by importance sampling, a method for estimating expected values of one distribution using samples from another distribution, importance sampling may result in numerical instability. In this work, we combine SAC with n-step returns in a way that overcomes this issue. We present an approach to applying numerically stable importance sampling with simplified hyperparameter selection. Furthermore, we analyze the entropy estimation approach of Soft Actor-Critic in the context of the n-step maximum entropy framework and formulate the $τ$-sampled entropy estimation to reduce the variance of the learning target. Finally, we formulate the Soft Actor-Critic with n-step returns (SAC$n$) algorithm that we experimentally verify on MuJoCo simulated environments.

</details>


### [452] [FROC: A Unified Framework with Risk-Optimized Control for Machine Unlearning in LLMs](https://arxiv.org/abs/2512.13337)
*Si Qi Goh,Yongsen Zheng,Ziyao Liu,Sami Hormi,Kwok-Yan Lam*

Main category: cs.LG

Relevance: 85.0

TL;DR: FROC是一个用于大语言模型机器遗忘的统一风险控制框架，通过概率约束评估遗忘不足和效用损失风险，指导超参数选择以实现安全与效用的平衡。


<details>
  <summary>Details</summary>
Motivation: 当前机器遗忘技术缺乏有效的风险评估和控制机制，难以在遗忘充分性和效用保持之间做出平衡选择，这阻碍了"被遗忘权"的实现，并引发了对大语言模型部署的信任问题。

Method: FROC采用符合性风险分析框架，引入连续风险模型聚合遗忘不足和效用退化，计算符合性遗忘风险(CUR)和风险控制配置集，通过概率约束指导超参数选择。

Result: 实验表明FROC能产生稳定、可解释的风险景观，揭示遗忘配置、语义偏移和效用影响之间的一致关系，为大规模LLM部署中的遗忘行为管理提供实用基础。

Conclusion: FROC将机器遗忘重新定义为可控、风险感知的过程，为大语言模型中的遗忘行为管理提供了系统化框架，有助于平衡安全与效用需求。

Abstract: Machine unlearning (MU) seeks to eliminate the influence of specific training examples from deployed models. As large language models (LLMs) become widely used, managing risks arising from insufficient forgetting or utility loss is increasingly crucial. Current MU techniques lack effective mechanisms for evaluating and controlling these risks, hindering the selection of strategies that appropriately balance safety and utility, and raising trust concerns surrounding the "right to be forgotten." To address these issues, we propose FROC, a unified framework with Risk-Optimized Control for machine unlearning in LLMs. FROC is built around a conformal-style risk-control formulation that expresses a user-specified risk budget on unlearning behavior. This probability-based constraint enables FROC to compare MU strategies, identify feasible operating regions, and guide hyperparameter selection according to desired trade-offs between forgetting sufficiency and utility preservation. To operationalize this constraint, FROC introduces a smoothly varying continuous risk model that aggregates forgetting deficiency and utility degradation into a single configuration-level score. Building on conformal risk analysis, FROC computes (1) the Conformal Unlearning Risk (CUR), a data-driven estimated value on the probability that forgotten samples continue to influence model predictions, and (2) risk-controlled configuration sets, which identify unlearning hyperparameters that are valid under the specified risk budget. Experiments across multiple LLM MU methods demonstrate that FROC produces stable, interpretable risk landscapes and reveals consistent relationships between unlearning configurations, semantic shift, and utility impact. FROC reframes MU as a controllable, risk-aware process and offers a practical foundation for managing unlearning behavior in large-scale LLM deployments.

</details>


### [453] [Async Control: Stress-testing Asynchronous Control Measures for LLM Agents](https://arxiv.org/abs/2512.13526)
*Asa Cooper Stickland,Jan Michelfeit,Arathi Mani,Charlie Griffin,Ollie Matthews,Tomek Korbak,Rogan Inglis,Oliver Makins,Alan Cooney*

Main category: cs.LG

Relevance: 85.0

TL;DR: 研究LLM软件工程代理的安全监控，提出异步监控方法降低运行时延迟，通过红蓝对抗游戏开发监控系统，在5个软件工程环境中实现6%漏报率（1%误报率），并评估部署时的破坏风险。


<details>
  <summary>Details</summary>
Motivation: LLM软件工程代理在实际开发任务中可能访问敏感数据或安全关键代码库，如果代理未对齐可能故意破坏代码库。需要监控系统来检测和防止此类破坏行为，但同步监控会引入运行时延迟，因此研究异步监控方法。

Method: 采用对抗性游戏框架：蓝队设计监控器，红队创建破坏性代理。开发了5个多样化的软件工程环境模拟实际部署场景。通过对抗过程开发集成监控器，在保留测试环境中评估性能。

Result: 开发的集成监控器在1%误报率下达到6%漏报率。基于漏报率估计部署时的破坏风险，提出了简单的风险外推模型并进行敏感性分析。

Conclusion: 异步监控可以在不引入运行时延迟的情况下有效检测LLM代理的破坏行为。对抗性游戏是开发鲁棒监控系统的有效方法，但需要谨慎外推风险估计模型。

Abstract: LLM-based software engineering agents are increasingly used in real-world development tasks, often with access to sensitive data or security-critical codebases. Such agents could intentionally sabotage these codebases if they were misaligned. We investigate asynchronous monitoring, in which a monitoring system reviews agent actions after the fact. Unlike synchronous monitoring, this approach does not impose runtime latency, while still attempting to disrupt attacks before irreversible harm occurs. We treat monitor development as an adversarial game between a blue team (who design monitors) and a red team (who create sabotaging agents). We attempt to set the game rules such that they upper bound the sabotage potential of an agent based on Claude 4.1 Opus. To ground this game in a realistic, high-stakes deployment scenario, we develop a suite of 5 diverse software engineering environments that simulate tasks that an agent might perform within an AI developer's internal infrastructure. Over the course of the game, we develop an ensemble monitor that achieves a 6% false negative rate at 1% false positive rate on a held out test environment. Then, we estimate risk of sabotage at deployment time by extrapolating from our monitor's false negative rate. We describe one simple model for this extrapolation, present a sensitivity analysis, and describe situations in which the model would be invalid. Code is available at: https://github.com/UKGovernmentBEIS/async-control.

</details>


### [454] [Superposition as Lossy Compression: Measure with Sparse Autoencoders and Connect to Adversarial Vulnerability](https://arxiv.org/abs/2512.13568)
*Leonard Bereska,Zoe Tzifa-Kratira,Reza Samavi,Efstratios Gavves*

Main category: cs.LG

Relevance: 85.0

TL;DR: 提出信息论框架测量神经网络表示的有效自由度，通过稀疏自编码器激活的香农熵计算有效特征数量，揭示神经网络如何通过叠加实现信息压缩，并连接叠加现象与对抗鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 神经网络通过叠加（多个特征作为激活空间中的重叠方向编码）实现卓越性能，这挑战了可解释性。目前缺乏测量叠加的规范方法，需要量化神经网络表示的有效自由度。

Method: 提出信息论框架，将香农熵应用于稀疏自编码器激活，计算有效特征数量（无干扰编码所需的最小神经元数）。这等价于测量网络通过叠加模拟的"虚拟神经元"数量。

Result: 度量在玩具模型中与真实情况强相关，在算法任务中检测到最小叠加，在dropout下显示系统性减少。层间模式与Pythia-70M的内在维度研究一致。度量捕捉发育动态，在grokking期间检测到尖锐特征整合。对抗训练可增加有效特征同时提高鲁棒性。

Conclusion: 将叠加定义为有损压缩，使规范测量神经网络如何在计算约束下组织信息成为可能。叠加与对抗鲁棒性的关系取决于任务复杂性和网络容量：简单任务允许特征扩展（丰富机制），复杂任务强制减少（稀缺机制）。

Abstract: Neural networks achieve remarkable performance through superposition: encoding multiple features as overlapping directions in activation space rather than dedicating individual neurons to each feature. This challenges interpretability, yet we lack principled methods to measure superposition. We present an information-theoretic framework measuring a neural representation's effective degrees of freedom. We apply Shannon entropy to sparse autoencoder activations to compute the number of effective features as the minimum neurons needed for interference-free encoding. Equivalently, this measures how many "virtual neurons" the network simulates through superposition. When networks encode more effective features than actual neurons, they must accept interference as the price of compression. Our metric strongly correlates with ground truth in toy models, detects minimal superposition in algorithmic tasks, and reveals systematic reduction under dropout. Layer-wise patterns mirror intrinsic dimensionality studies on Pythia-70M. The metric also captures developmental dynamics, detecting sharp feature consolidation during grokking. Surprisingly, adversarial training can increase effective features while improving robustness, contradicting the hypothesis that superposition causes vulnerability. Instead, the effect depends on task complexity and network capacity: simple tasks with ample capacity allow feature expansion (abundance regime), while complex tasks or limited capacity force reduction (scarcity regime). By defining superposition as lossy compression, this work enables principled measurement of how neural networks organize information under computational constraints, connecting superposition to adversarial robustness.

</details>


### [455] [Citation-Grounded Code Comprehension: Preventing LLM Hallucination Through Hybrid Retrieval and Graph-Augmented Context](https://arxiv.org/abs/2512.12117)
*Jahidul Arafat*

Main category: cs.SE

Relevance: 85.0

TL;DR: 论文提出混合检索系统，结合稀疏匹配、密集嵌入和图扩展，用于代码理解中的可验证引用生成，实现92%的引用准确率且无幻觉。


<details>
  <summary>Details</summary>
Motivation: LLM在代码理解中存在幻觉问题，生成看似合理但事实错误的代码引用，这阻碍了可靠的开发者辅助。现有系统主要依赖文本相似性而忽略了代码结构，特别是跨文件架构依赖关系。

Method: 开发混合检索系统：结合BM25稀疏匹配、BGE密集嵌入和Neo4j图扩展（通过导入关系）。系统在30个Python仓库的180个开发者查询上进行评估，比较不同检索模式、图扩展策略和引用验证机制。

Result: 实现92%的引用准确率且零幻觉。混合检索系统比单模式基线提升14-18个百分点，在62%的架构查询中发现了纯文本相似性遗漏的跨文件证据。跨文件证据发现是引用完整性的最大贡献者。

Conclusion: 应将引用基础生成作为代码理解系统的架构原则。需要结合稀疏匹配、语义相似性和代码结构推理，特别是跨文件依赖关系，以实现可验证的代码理解。

Abstract: Large language models have become essential tools for code comprehension, enabling developers to query unfamiliar codebases through natural language interfaces. However, LLM hallucination, generating plausible but factually incorrect citations to source code, remains a critical barrier to reliable developer assistance. This paper addresses the challenges of achieving verifiable, citation grounded code comprehension through hybrid retrieval and lightweight structural reasoning. Our work is grounded in systematic evaluation across 30 Python repositories with 180 developer queries, comparing retrieval modalities, graph expansion strategies, and citation verification mechanisms. We find that challenges of citation accuracy arise from the interplay between sparse lexical matching, dense semantic similarity, and cross file architectural dependencies. Among these, cross file evidence discovery is the largest contributor to citation completeness, but it is largely overlooked because existing systems rely on pure textual similarity without leveraging code structure. We advocate for citation grounded generation as an architectural principle for code comprehension systems and demonstrate this need by achieving 92 percent citation accuracy with zero hallucinations. Specifically, we develop a hybrid retrieval system combining BM25 sparse matching, BGE dense embeddings, and Neo4j graph expansion via import relationships, which outperforms single mode baselines by 14 to 18 percentage points while discovering cross file evidence missed by pure text similarity in 62 percent of architectural queries.

</details>


### [456] [From Zipf's Law to Neural Scaling through Heaps' Law and Hilberg's Hypothesis](https://arxiv.org/abs/2512.13491)
*Łukasz Dębowski*

Main category: cs.IT

Relevance: 85.0

TL;DR: 论文揭示了神经缩放定律与齐普夫定律之间的演绎关系：在特定假设下，神经缩放定律是齐普夫定律的数学推论，通过词汇增长定律和希尔伯格假说作为中间桥梁。


<details>
  <summary>Details</summary>
Motivation: 本文旨在建立机器学习中的神经缩放定律与定量语言学中的齐普夫定律之间的理论联系。目前这两种定律在各自领域被广泛讨论，但它们的数学关系尚未被系统揭示。作者希望证明在特定假设下，神经缩放定律可以从齐普夫定律推导出来。

Method: 采用理论推导方法，通过三个推理步骤建立联系：1) 从齐普夫定律推导词汇增长定律；2) 从词汇增长定律推导希尔伯格假说；3) 从希尔伯格假说推导神经缩放定律。使用满足所有四种统计规律的Santa Fe过程作为示例进行说明。

Result: 成功建立了神经缩放定律与齐普夫定律之间的演绎关系，证明了在特定假设下，神经缩放定律是齐普夫定律的数学推论。这一结果为理解大语言模型的缩放行为提供了理论基础。

Conclusion: 神经缩放定律并非独立现象，而是可以从更基本的齐普夫定律推导出来的结果。这一发现为理解大语言模型的训练动态和性能缩放提供了新的理论视角，连接了机器学习和定量语言学两个领域。

Abstract: We inspect the deductive connection between the neural scaling law and Zipf's law -- two statements discussed in machine learning and quantitative linguistics. The neural scaling law describes how the cross entropy rate of a foundation model -- such as a large language model -- changes with respect to the amount of training tokens, parameters, and compute. By contrast, Zipf's law posits that the distribution of tokens exhibits a power law tail. Whereas similar claims have been made in more specific settings, we show that the neural scaling law is a consequence of Zipf's law under certain broad assumptions that we reveal systematically. The derivation steps are as follows: We derive Heaps' law on the vocabulary growth from Zipf's law, Hilberg's hypothesis on the entropy scaling from Heaps' law, and the neural scaling from Hilberg's hypothesis. We illustrate these inference steps by a toy example of the Santa Fe process that satisfies all the four statistical laws.

</details>


### [457] [Tiny Recursive Models on ARC-AGI-1: Inductive Biases, Identity Conditioning, and Test-Time Compute](https://arxiv.org/abs/2512.11847)
*Antonio Roye-Azar,Santiago Vargas-Naranjo,Dhruv Ghai,Nithin Balamurugan,Rayan Amir*

Main category: cs.LG

Relevance: 75.0

TL;DR: 本文对Tiny Recursive Models (TRM)在ARC-AGI-1任务上的性能进行了实证分析，发现其表现主要来自测试时增强、多数投票集成、任务标识符依赖和浅层递归，而非深度推理能力。


<details>
  <summary>Details</summary>
Motivation: TRM被提出作为解决ARC风格任务的高效参数替代方案，但原始工作未明确其性能来源是架构优势、测试时计算还是任务特定先验。本文旨在通过实证分析揭示TRM在ARC-AGI-1上性能的真正驱动因素。

Method: 对ARC Prize TRM检查点在ARC-AGI-1上进行系统分析：1) 测试时增强和多数投票集成的影响评估；2) 任务标识符消融实验；3) 递归轨迹分析；4) 训练阶段增强策略比较；5) 与Llama 3 8B QLoRA微调的效率对比。

Result: 1) 1000样本投票将Pass@1提升约11个百分点；2) 任务标识符替换导致零准确率；3) 大部分准确率在第一步递归获得，递归深度浅；4) 强增强策略扩展候选解分布；5) TRM相比Llama 3 8B QLoRA具有更高吞吐量和更低内存使用。

Conclusion: TRM在ARC-AGI-1上的性能主要源于效率优势、任务特定条件设置和激进的测试时计算策略，而非深度内部推理能力。其非自回归设计在特定场景下具有效率优势。

Abstract: Tiny Recursive Models (TRM) were proposed as a parameter-efficient alternative to large language models for solving Abstraction and Reasoning Corpus (ARC) style tasks. The original work reports strong performance and suggests that recursive latent updates enable non-trivial reasoning, but it remains unclear how much of this performance stems from architecture, test-time compute, or task-specific priors. In this technical note, we empirically analyze the ARC Prize TRM checkpoint on ARC-AGI-1 and report four behavioral findings and an efficiency comparison. First, we show that test-time augmentation and majority-vote ensembling account for a substantial fraction of reported performance: the 1000-sample voting pipeline improves Pass@1 by about 11 percentage points over single-pass canonical inference. Second, a puzzle-identity ablation reveals strict dependence on task identifiers: replacing the correct puzzle ID with a blank or random token yields zero accuracy. Third, a recursion trajectory analysis shows that most of the final accuracy is achieved at the first recursion step and that performance saturates after few latent updates, indicating shallow effective recursion. Fourth, early-stage training experiments under canonical versus heavy augmentation regimes suggest that heavy augmentation broadens the distribution of candidate solutions and improves multi-sample success. Finally, we compare TRM with a naive QLoRA fine-tune of Llama 3 8B on canonical ARC-AGI-1, finding that TRM's non-autoregressive design achieves much higher throughput and substantially lower memory usage in this setting. Overall, TRM's ARC-AGI-1 performance appears to arise from an interaction between efficiency, task-specific conditioning, and aggressive test-time compute rather than deep internal reasoning.

</details>


### [458] [On the Dangers of Bootstrapping Generation for Continual Learning and Beyond](https://arxiv.org/abs/2512.11867)
*Daniil Zverev,A. Sophia Koepke,Joao F. Henriques*

Main category: cs.LG

Relevance: 75.0

TL;DR: 该论文研究了在持续学习中使用合成数据进行重复训练的问题，发现这会导致分布漂移、性能下降和模型崩溃，揭示了合成数据在训练目标中引入显著偏差和方差的问题。


<details>
  <summary>Details</summary>
Motivation: 随着合成数据在模型训练中的普遍应用，重复使用合成数据进行训练引发了关于分布漂移和性能退化的担忧。作者旨在探究这种自举过程的影响，特别关注合成数据如何污染数据集并导致模型性能下降。

Method: 通过持续学习的视角分析合成数据训练问题，将其与生成式经验回放方法联系起来。进行统计分析展示合成数据如何引入偏差和方差，并通过实证研究展示流行生成模型在重复合成数据训练下的崩溃现象。

Result: 研究发现合成数据在训练目标中引入显著偏差和方差，削弱了最大似然估计的可靠性。实证证据显示流行生成模型在重复合成数据训练下会崩溃，且最先进的生成式经验回放方法无法维持潜在空间的对齐。

Conclusion: 研究结果对在持续学习中使用合成数据提出了严重警告，揭示了这种实践可能导致模型性能退化和分布漂移的固有风险。

Abstract: The use of synthetically generated data for training models is becoming a common practice. While generated data can augment the training data, repeated training on synthetic data raises concerns about distribution drift and degradation of performance due to contamination of the dataset. We investigate the consequences of this bootstrapping process through the lens of continual learning, drawing a connection to Generative Experience Replay (GER) methods. We present a statistical analysis showing that synthetic data introduces significant bias and variance into training objectives, weakening the reliability of maximum likelihood estimation. We provide empirical evidence showing that popular generative models collapse under repeated training with synthetic data. We quantify this degradation and show that state-of-the-art GER methods fail to maintain alignment in the latent space. Our findings raise critical concerns about the use of synthetic data in continual learning.

</details>


### [459] [EnviroLLM: Resource Tracking and Optimization for Local AI](https://arxiv.org/abs/2512.12004)
*Troy Allen*

Main category: cs.LG

Relevance: 75.0

TL;DR: EnviroLLM是一个开源工具包，用于跟踪、基准测试和优化在个人设备上运行大型语言模型的性能和能耗，帮助用户评估质量-效率权衡。


<details>
  <summary>Details</summary>
Motivation: 随着LLM越来越多地在本地部署以满足隐私和可访问性需求，用户缺乏工具来衡量其资源使用、环境影响和效率指标。需要一种系统化的方法来监控和优化本地LLM部署的性能和能耗。

Method: 开发了EnviroLLM工具包，提供实时进程监控、跨多个平台（Ollama、LM Studio、vLLM、OpenAI兼容API）的基准测试、持久化存储与可视化分析，以及个性化模型和优化建议。系统结合LLM-as-judge评估与能耗速度指标。

Result: 创建了一个全面的开源工具包，使用户能够系统评估本地LLM部署的性能和能耗，支持质量-效率权衡分析，提供可视化工具和个性化优化建议。

Conclusion: EnviroLLM填补了本地LLM部署监控和优化工具的空白，帮助用户在隐私、可访问性和效率之间做出明智决策，促进更可持续的AI部署。

Abstract: Large language models (LLMs) are increasingly deployed locally for privacy and accessibility, yet users lack tools to measure their resource usage, environmental impact, and efficiency metrics. This paper presents EnviroLLM, an open-source toolkit for tracking, benchmarking, and optimizing performance and energy consumption when running LLMs on personal devices. The system provides real-time process monitoring, benchmarking across multiple platforms (Ollama, LM Studio, vLLM, and OpenAI-compatible APIs), persistent storage with visualizations for longitudinal analysis, and personalized model and optimization recommendations. The system includes LLM-as-judge evaluations alongside energy and speed metrics, enabling users to assess quality-efficiency tradeoffs when testing models with custom prompts.

</details>


### [460] [Anchoring Values in Temporal and Group Dimensions for Flow Matching Model Alignment](https://arxiv.org/abs/2512.12387)
*Yawen Shao,Jie Xiao,Kai Zhu,Yu Liu,Wei Zhai,Yang Cao,Zheng-Jun Zha*

Main category: cs.LG

Relevance: 75.0

TL;DR: VGPO改进了GRPO在图像生成中的问题，通过密集过程感知价值估计和绝对价值增强的组归一化，解决了时间信用分配和优化停滞问题，在图像质量和任务准确性上达到SOTA。


<details>
  <summary>Details</summary>
Motivation: 当前GRPO在流匹配图像生成中存在两个关键限制：1）稀疏终端奖励在所有时间步均匀应用，损害了时间信用分配，忽略了从早期结构形成到后期调优的不同关键阶段；2）仅依赖相对组内奖励导致优化信号随着训练收敛而衰减，当奖励多样性耗尽时出现优化停滞。

Method: VGPO框架在时间和组维度上重新定义价值估计：1）将稀疏终端奖励转换为密集、过程感知的价值估计，通过建模每个生成阶段的预期累积奖励实现精确信用分配；2）用绝对价值增强的新过程替换标准组归一化，即使在奖励多样性下降时也能保持稳定的优化信号。

Result: 在三个基准测试上的广泛实验表明，VGPO实现了最先进的图像质量，同时提高了任务特定的准确性，有效缓解了奖励黑客问题。

Conclusion: VGPO成功解决了GRPO在流匹配图像生成中的核心冲突，通过重新设计价值估计机制，在保持稳定优化信号的同时实现了更好的生成质量和任务准确性。

Abstract: Group Relative Policy Optimization (GRPO) has proven highly effective in enhancing the alignment capabilities of Large Language Models (LLMs). However, current adaptations of GRPO for the flow matching-based image generation neglect a foundational conflict between its core principles and the distinct dynamics of the visual synthesis process. This mismatch leads to two key limitations: (i) Uniformly applying a sparse terminal reward across all timesteps impairs temporal credit assignment, ignoring the differing criticality of generation phases from early structure formation to late-stage tuning. (ii) Exclusive reliance on relative, intra-group rewards causes the optimization signal to fade as training converges, leading to the optimization stagnation when reward diversity is entirely depleted. To address these limitations, we propose Value-Anchored Group Policy Optimization (VGPO), a framework that redefines value estimation across both temporal and group dimensions. Specifically, VGPO transforms the sparse terminal reward into dense, process-aware value estimates, enabling precise credit assignment by modeling the expected cumulative reward at each generative stage. Furthermore, VGPO replaces standard group normalization with a novel process enhanced by absolute values to maintain a stable optimization signal even as reward diversity declines. Extensive experiments on three benchmarks demonstrate that VGPO achieves state-of-the-art image quality while simultaneously improving task-specific accuracy, effectively mitigating reward hacking. Project webpage: https://yawen-shao.github.io/VGPO/.

</details>


### [461] [Can Graphs Improve Tabular Foundation Models?](https://arxiv.org/abs/2512.12405)
*Franck Le,Keith Grueneberg,Erich Nahum,Vadim Sheinin*

Main category: cs.LG

Relevance: 75.0

TL;DR: BOLERO：通过轻量级静态二分图头增强预训练表格Transformer，在分类和回归任务上取得显著性能提升


<details>
  <summary>Details</summary>
Motivation: 现有表格Transformer和上下文学习方法（如SAINT、TP-BERTa、TabPFN等）缺乏显式建模实例间关系的机制，而相似样本通常具有相关结果。研究者探索引入简单图先验是否能增强预训练表格Transformer的性能。

Method: 提出BOLERO方法：在冻结的RoBERTa-Tab（基于掩码标记预测预训练的RoBERTa风格表格骨干）基础上，添加轻量级静态二分图头。每个实例连接到特征/值锚点，通过小型GNN细化行表示，同时保持骨干网络冻结。

Result: 在TP-BERTa基准套件的80个分类和64个回归数据集上评估，与XGBoost、CatBoost、TabPFN-v2、MITRA、TabICL、TP-BERTa、RoBERTa-Tab等强基线比较。采用成对Wilcoxon符号秩检验和效应大小分析，BOLERO在分类和回归任务上都取得了最多的统计显著胜利。

Conclusion: 轻量级图先验能够有意义地改进预训练表格Transformer，为表格数据建模提供了新的有效方法。

Abstract: Tabular data are central to many real-world systems. While recent tabular transformers and in-context learners such as SAINT, TP-BERTa, TabPFN, TabICL, and MITRA incorporate limited inter-row reasoning, most approaches still lack an explicit mechanism to model relationships among instances, even though similar samples often share related outcomes. We investigate whether introducing \emph{simple graph priors} can enhance \emph{pretrained tabular transformers}. Concretely, we introduce {BOLERO}, a lightweight, static bipartite graph head that augments {RoBERTa-Tab} (a RoBERTa-style tabular backbone pretrained with masked-token prediction.) Each instance connects to feature/value anchors; a small GNN refines row representations, while the backbone remains frozen. We evaluate on 80 classification and 64 regression datasets from the TP-BERTa benchmark suites, comparing against strong baselines including XGBoost, CatBoost, TabPFN-v2, MITRA, TabICL, TP-BERTa, and RoBERTa-Tab. To ensure statistically sound conclusions, we follow best practices for multi-dataset evaluation: pairwise Wilcoxon signed-rank tests on per-dataset score differences and effect sizes (median improvement with confidence intervals), rather than mean-rank post-hoc tests that depend on the competitor pool. BOLERO achieves the highest number of statistically significant wins across both classification and regression, demonstrating that lightweight graph priors meaningfully improve pretrained tabular transformers.

</details>


### [462] [Sparse Concept Anchoring for Interpretable and Controllable Neural Representations](https://arxiv.org/abs/2512.12469)
*Sandy Fraser,Patryk Wielopolski*

Main category: cs.LG

Relevance: 75.0

TL;DR: 提出稀疏概念锚定方法，通过最小监督（每个锚定概念<0.1%标签）在潜在空间中定位目标概念子集，同时允许其他概念自组织，实现可解释、可操控的表示学习。


<details>
  <summary>Details</summary>
Motivation: 当前表示学习方法缺乏对特定概念的可控性，难以在保持其他特征不变的情况下选择性地操控或消除特定概念。需要一种既能精确定位目标概念，又不需要大量标注数据的方法。

Method: 结合激活归一化、分离正则化器、锚定/子空间正则化器，将稀有标注样本吸引到预定义方向或轴对齐子空间。训练时使用最小监督（每个概念<0.1%样本标签）。

Result: 在结构化自编码器上实验显示：1）可逆行为操控：推理时投影出概念潜在成分；2）永久移除：通过锚定维度权重消融完全消除概念；3）目标概念选择性衰减，正交特征影响可忽略；4）重建误差接近理论界限。

Conclusion: 稀疏概念锚定为学习表示中的可解释、可操控行为提供了实用途径，实现了对特定概念的精确定位和操控，同时保持其他特征的完整性。

Abstract: We introduce Sparse Concept Anchoring, a method that biases latent space to position a targeted subset of concepts while allowing others to self-organize, using only minimal supervision (labels for <0.1% of examples per anchored concept). Training combines activation normalization, a separation regularizer, and anchor or subspace regularizers that attract rare labeled examples to predefined directions or axis-aligned subspaces. The anchored geometry enables two practical interventions: reversible behavioral steering that projects out a concept's latent component at inference, and permanent removal via targeted weight ablation of anchored dimensions. Experiments on structured autoencoders show selective attenuation of targeted concepts with negligible impact on orthogonal features, and complete elimination with reconstruction error approaching theoretical bounds. Sparse Concept Anchoring therefore provides a practical pathway to interpretable, steerable behavior in learned representations.

</details>


### [463] [On the Accuracy of Newton Step and Influence Function Data Attributions](https://arxiv.org/abs/2512.12572)
*Ittai Rubinstein,Samuel B. Hopkins*

Main category: cs.LG

Relevance: 75.0

TL;DR: 本文对数据归因方法（单牛顿步NS和影响函数IF）进行了新的理论分析，首次在不假设全局强凸性的情况下，推导出误差的渐近紧界，并解释了NS通常比IF更准确的现象。


<details>
  <summary>Details</summary>
Motivation: 现有数据归因方法（如IF和NS）的理论分析存在两个主要局限：1）依赖全局强凸性假设，这在实践中常不成立；2）误差界随参数数d和移除样本数k的缩放效果很差。这导致无法回答"各方法的误差渐近缩放规律是什么？"以及"哪种方法对给定数据集更准确？"等基本问题。

Method: 针对凸学习问题，提出了NS和IF数据归因方法的新分析框架。该分析不依赖全局强凸性假设，并推导出误差的渐近紧界（up to poly-logarithmic factors）。特别针对行为良好的逻辑回归，证明了所获边界的渐近紧性。

Result: 获得了两个关键渐近缩放定律：1）NS估计与真实参数的平均误差为Θ̃(kd/n²)；2）NS与IF估计之间的平均差异为Θ̃((k+d)√(kd)/n²)。这首次从理论上解释了NS通常比IF更准确的观察结果。

Conclusion: 本文提供了首个不依赖全局强凸性假设的数据归因方法理论分析，推导出渐近紧的误差界，并解释了NS优于IF的现象。这些结果为数据归因方法的理论理解和实际应用提供了重要基础。

Abstract: Data attribution aims to explain model predictions by estimating how they would change if certain training points were removed, and is used in a wide range of applications, from interpretability and credit assignment to unlearning and privacy.
  Even in the relatively simple case of linear regressions, existing mathematical analyses of leading data attribution methods such as Influence Functions (IF) and single Newton Step (NS) remain limited in two key ways. First, they rely on global strong convexity assumptions which are often not satisfied in practice. Second, the resulting bounds scale very poorly with the number of parameters ($d$) and the number of samples removed ($k$). As a result, these analyses are not tight enough to answer fundamental questions such as "what is the asymptotic scaling of the errors of each method?" or "which of these methods is more accurate for a given dataset?"
  In this paper, we introduce a new analysis of the NS and IF data attribution methods for convex learning problems. To the best of our knowledge, this is the first analysis of these questions that does not assume global strong convexity and also the first explanation of [KATL19] and [RH25a]'s observation that NS data attribution is often more accurate than IF. We prove that for sufficiently well-behaved logistic regression, our bounds are asymptotically tight up to poly-logarithmic factors, yielding scaling laws for the errors in the average-case sample removals.
  \[ \mathbb{E}_{T \subseteq [n],\, |T| = k} \bigl[ \|\hatθ_T - \hatθ_T^{\mathrm{NS}}\|_2 \bigr] = \widetildeΘ\!\left(\frac{k d}{n^2}\right), \qquad \mathbb{E}_{T \subseteq [n],\, |T| = k} \bigl[ \|\hatθ_T^{\mathrm{NS}} - \hatθ_T^{\mathrm{IF}}\|_2 \bigr] = \widetildeΘ\!\left( \frac{(k + d)\sqrt{k d}}{n^2} \right). \]

</details>


### [464] [Spectral Sentinel: Scalable Byzantine-Robust Decentralized Federated Learning via Sketched Random Matrix Theory on Blockchain](https://arxiv.org/abs/2512.12617)
*Animesh Mishra*

Main category: cs.LG

Relevance: 75.0

TL;DR: 提出Spectral Sentinel框架，利用随机矩阵理论检测去中心化联邦学习中的拜占庭攻击，通过协方差特征谱异常检测，在1.5B参数模型上实现高效防御。


<details>
  <summary>Details</summary>
Motivation: 去中心化联邦学习(DFL)在非独立同分布数据下易受拜占庭梯度投毒攻击，现有防御方法面临可扩展性困境：距离过滤可能误拒合法更新，几何中位数方法计算成本高，认证防御仅在小模型上验证。

Method: 基于随机矩阵理论，利用诚实梯度协方差特征谱遵循Marchenko-Pastur分布的特性，检测拜占庭扰动引起的尾部异常。结合Frequent Directions草图技术和数据依赖的MP跟踪，实现O(k²)内存消耗(k≪d)的高效检测。

Result: 在(σ,f)威胁模型下证明(ε,δ)-拜占庭弹性，收敛率O(σf/√T + f²/T)，提供匹配的信息论下界Ω(σf/√T)。在144个攻击-聚合配置中平均准确率达78.4%，优于基线方法(48-63%)。

Conclusion: Spectral Sentinel框架解决了DFL中拜占庭检测的可扩展性问题，为大规模模型训练提供了理论保证和实用系统实现，在Polygon区块链网络上验证了有效性。

Abstract: Decentralized federated learning (DFL) enables collaborative model training without centralized trust, but it remains vulnerable to Byzantine clients that poison gradients under heterogeneous (Non-IID) data. Existing defenses face a scalability trilemma: distance-based filtering (e.g., Krum) can reject legitimate Non-IID updates, geometric-median methods incur prohibitive $O(n^2 d)$ cost, and many certified defenses are evaluated only on models below 100M parameters. We propose Spectral Sentinel, a Byzantine detection and aggregation framework that leverages a random-matrix-theoretic signature: honest Non-IID gradients produce covariance eigenspectra whose bulk follows the Marchenko-Pastur law, while Byzantine perturbations induce detectable tail anomalies. Our algorithm combines Frequent Directions sketching with data-dependent MP tracking, enabling detection on models up to 1.5B parameters using $O(k^2)$ memory with $k \ll d$. Under a $(σ,f)$ threat model with coordinate-wise honest variance bounded by $σ^2$ and $f < 1/2$ adversaries, we prove $(ε,δ)$-Byzantine resilience with convergence rate $O(σf / \sqrt{T} + f^2 / T)$, and we provide a matching information-theoretic lower bound $Ω(σf / \sqrt{T})$, establishing minimax optimality. We implement the full system with blockchain integration on Polygon networks and validate it across 144 attack-aggregator configurations, achieving 78.4 percent average accuracy versus 48-63 percent for baseline methods.

</details>


### [465] [Understanding Structured Financial Data with LLMs: A Case Study on Fraud Detection](https://arxiv.org/abs/2512.13040)
*Xuwei Tan,Yao Ma,Xueru Zhang*

Main category: cs.LG

Relevance: 75.0

TL;DR: FinFRE-RAG：两阶段方法，通过重要性引导的特征缩减和检索增强的上下文学习，将表格欺诈检测任务适配到LLMs，在保持可解释性的同时提升性能。


<details>
  <summary>Details</summary>
Motivation: 传统表格欺诈检测模型需要大量特征工程且可解释性差，而LLMs能提供人类可读的解释但直接应用于表格数据时性能不佳，需要解决特征过多、类别不平衡和上下文信息缺失的问题。

Method: 两阶段方法：1) 重要性引导的特征缩减，将数值/分类属性序列化为自然语言；2) 检索增强的上下文学习，使用标签感知的实例级示例进行提示。

Result: 在四个公共欺诈数据集和三类开源LLMs上，FinFRE-RAG显著优于直接提示方法，在多个设置中与强表格基线竞争，虽然仍落后于专用分类器，但缩小了性能差距并提供了可解释的推理。

Conclusion: LLMs虽然性能不及专用分类器，但通过FinFRE-RAG方法能缩小性能差距并提供可解释的推理，在欺诈分析中具有辅助工具的价值。

Abstract: Detecting fraud in financial transactions typically relies on tabular models that demand heavy feature engineering to handle high-dimensional data and offer limited interpretability, making it difficult for humans to understand predictions. Large Language Models (LLMs), in contrast, can produce human-readable explanations and facilitate feature analysis, potentially reducing the manual workload of fraud analysts and informing system refinements. However, they perform poorly when applied directly to tabular fraud detection due to the difficulty of reasoning over many features, the extreme class imbalance, and the absence of contextual information. To bridge this gap, we introduce FinFRE-RAG, a two-stage approach that applies importance-guided feature reduction to serialize a compact subset of numeric/categorical attributes into natural language and performs retrieval-augmented in-context learning over label-aware, instance-level exemplars. Across four public fraud datasets and three families of open-weight LLMs, FinFRE-RAG substantially improves F1/MCC over direct prompting and is competitive with strong tabular baselines in several settings. Although these LLMs still lag behind specialized classifiers, they narrow the performance gap and provide interpretable rationales, highlighting their value as assistive tools in fraud analysis.

</details>


### [466] [From Small to Large: Generalization Bounds for Transformers on Variable-Size Inputs](https://arxiv.org/abs/2512.12805)
*Anastasiia Alokhina,Pan Li*

Main category: cs.LG

Relevance: 75.0

TL;DR: 该论文研究了Transformer在几何数据上的尺寸泛化能力，建立了离散样本与连续域等效输出之间的误差界限理论框架。


<details>
  <summary>Details</summary>
Motivation: Transformer展现出显著的尺寸泛化能力，能够从小规模token集合外推到更大规模，这一现象在点云、图数据和自然语言处理中均有实证，但缺乏严格的理论刻画。本文旨在为几何数据的尺寸泛化提供理论分析框架。

Method: 将几何数据表示为连续源的离散样本（如流形上的点云、图论中的图），建立理论框架分析Transformer输出在离散样本与连续域等效之间的误差界限。证明对于具有稳定位置编码的Transformer，该界限由采样密度和数据流形的内在维度决定。

Result: 在不同尺寸的图和点云上进行实验，验证了理论界限的紧致性，表明理论预测与实证结果一致。

Conclusion: 本文为Transformer在几何数据上的尺寸泛化提供了首个严格的理论刻画，揭示了采样密度和内在维度对泛化能力的关键影响，为理解Transformer的泛化机制提供了理论基础。

Abstract: Transformers exhibit a notable property of \emph{size generalization}, demonstrating an ability to extrapolate from smaller token sets to significantly longer ones. This behavior has been documented across diverse applications, including point clouds, graphs, and natural language. Despite its empirical success, this capability still lacks some rigorous theoretical characterizations. In this paper, we develop a theoretical framework to analyze this phenomenon for geometric data, which we represent as discrete samples from a continuous source (e.g., point clouds from manifolds, graphs from graphons). Our core contribution is a bound on the error between the Transformer's output for a discrete sample and its continuous-domain equivalent. We prove that for Transformers with stable positional encodings, this bound is determined by the sampling density and the intrinsic dimensionality of the data manifold. Experiments on graphs and point clouds of various sizes confirm the tightness of our theoretical bound.

</details>


### [467] [GradID: Adversarial Detection via Intrinsic Dimensionality of Gradients](https://arxiv.org/abs/2512.12827)
*Mohammad Mahdi Razmjoo,Mohammad Mahdi Sharifian,Saeed Bagheri Shouraki*

Main category: cs.LG

Relevance: 75.0

TL;DR: 该论文提出了一种基于输入损失流形内在维度（ID）的对抗样本检测方法，通过分析模型梯度参数的内在维度来区分自然数据和对抗数据，在批处理和单样本场景下都取得了优异效果。


<details>
  <summary>Details</summary>
Motivation: 深度神经网络对微小对抗扰动高度脆弱，而医疗诊断和自动驾驶等应用对可靠性要求极高，因此需要有效的对抗攻击检测方法。现有方法存在局限性，需要更鲁棒的检测机制。

Method: 研究模型输入损失流形的几何特性，分析模型梯度参数的内在维度（ID），发现自然数据和对抗数据的内在维度存在显著差异，基于此差异构建检测方法。

Result: 在MNIST和SVHN数据集上，批处理场景下检测效果显著；在CIFAR-10和MS COCO等挑战性基准上，单样本检测达到新的SOTA水平，对CW和AutoAttack等多种攻击的检测率超过92%。

Conclusion: 内在维度是跨数据集和攻击策略的强大对抗检测指纹，几何方法为对抗样本检测提供了鲁棒的新途径。

Abstract: Despite their remarkable performance, deep neural networks exhibit a critical vulnerability: small, often imperceptible, adversarial perturbations can lead to drastically altered model predictions. Given the stringent reliability demands of applications such as medical diagnosis and autonomous driving, robust detection of such adversarial attacks is paramount. In this paper, we investigate the geometric properties of a model's input loss landscape. We analyze the Intrinsic Dimensionality (ID) of the model's gradient parameters, which quantifies the minimal number of coordinates required to describe the data points on their underlying manifold. We reveal a distinct and consistent difference in the ID for natural and adversarial data, which forms the basis of our proposed detection method. We validate our approach across two distinct operational scenarios. First, in a batch-wise context for identifying malicious data groups, our method demonstrates high efficacy on datasets like MNIST and SVHN. Second, in the critical individual-sample setting, we establish new state-of-the-art results on challenging benchmarks such as CIFAR-10 and MS COCO. Our detector significantly surpasses existing methods against a wide array of attacks, including CW and AutoAttack, achieving detection rates consistently above 92\% on CIFAR-10. The results underscore the robustness of our geometric approach, highlighting that intrinsic dimensionality is a powerful fingerprint for adversarial detection across diverse datasets and attack strategies.

</details>


### [468] [Alada: Alternating Adaptation of Momentum Method for Memory-Efficient Matrix Optimization](https://arxiv.org/abs/2512.13034)
*Xiaoyu He,Yu Cai,Jin Jia,Canxi Huang,Wenqing Chen,Zibin Zheng*

Main category: cs.LG

Relevance: 75.0

TL;DR: Alada是一种用于大规模矩阵随机优化的自适应动量方法，采用秩一分解估计梯度二阶矩，具有次线性内存开销，性能与Adam相当但内存效率更高。


<details>
  <summary>Details</summary>
Motivation: 训练大型模型（如LLMs）时，传统优化器（如Adam）需要存储梯度二阶矩的完整矩阵，导致巨大的内存开销。随着模型规模增长，这种内存瓶颈限制了可训练的模型大小和效率。

Method: Alada采用秩一分解方法估计梯度二阶矩，通过交替更新因子来最小化估计误差。该方法还包含一阶矩估计规则以增强鲁棒性，且不增加额外内存开销。算法可扩展到张量形状变量。

Result: 在多个自然语言处理任务上的数值研究表明，Alada相比Adam及其变体显著减少了内存开销，同时在训练大型模型时保持了鲁棒性。理论性能与Adam对齐。

Conclusion: Alada提供了一种内存高效的优化方法，特别适合大规模模型训练，解决了传统自适应优化器的内存瓶颈问题，为大型LLM训练提供了实用的优化解决方案。

Abstract: This work proposes Alada, an adaptive momentum method for stochastic optimization over large-scale matrices. Alada employs a rank-one factorization approach to estimate the second moment of gradients, where factors are updated alternatively to minimize the estimation error. Alada achieves sublinear memory overheads and can be readily extended to optimizing tensor-shaped variables.We also equip Alada with a first moment estimation rule, which enhances the algorithm's robustness without incurring additional memory overheads. The theoretical performance of Alada aligns with that of traditional methods such as Adam. Numerical studies conducted on several natural language processing tasks demonstrate the reduction in memory overheads and the robustness in training large models relative to Adam and its variants.

</details>


### [469] [Element-wise Modulation of Random Matrices for Efficient Neural Layers](https://arxiv.org/abs/2512.13480)
*Maksymilian Szorc*

Main category: cs.LG

Relevance: 75.0

TL;DR: 提出参数化随机投影层（PRP），通过固定随机矩阵与轻量可学习参数解耦特征混合与适应，大幅减少可训练参数至线性规模，同时保持模型性能。


<details>
  <summary>Details</summary>
Motivation: 全连接层是深度神经网络中内存和计算开销的主要来源，因为其参数密集且往往冗余。现有压缩技术通常引入复杂的工程权衡或降低模型性能。

Method: 提出参数化随机投影层（PRP），使用固定的随机矩阵进行特征混合，通过轻量级的可学习元素级参数进行调制，实现特征混合与适应的解耦。

Result: PRP层将可训练参数数量大幅减少到线性规模，同时在各种基准测试中保持可靠的准确性。该设计为架构扩展和资源受限环境下的部署提供了稳定、计算高效的解决方案。

Conclusion: PRP层是一种新颖的神经网络压缩方法，通过解耦特征混合与适应，在保持性能的同时显著减少参数和计算开销，适用于资源受限环境。

Abstract: Fully connected layers are a primary source of memory and computational overhead in deep neural networks due to their dense, often redundant parameterization. While various compression techniques exist, they frequently introduce complex engineering trade-offs or degrade model performance. We propose the Parametrized Random Projection (PRP) layer, a novel approach that decouples feature mixing from adaptation by utilizing a fixed random matrix modulated by lightweight, learnable element-wise parameters. This architecture drastically reduces the trainable parameter count to a linear scale while retaining reliable accuracy across various benchmarks. The design serves as a stable, computationally efficient solution for architectural scaling and deployment in resource-limited settings.

</details>


### [470] [Image Diffusion Preview with Consistency Solver](https://arxiv.org/abs/2512.13592)
*Fu-Yun Wang,Hao Zhou,Liangzhe Yuan,Sanghyun Woo,Boqing Gong,Bohyung Han,Ming-Hsuan Yang,Han Zhang,Yukun Zhu,Ting Liu,Long Zhao*

Main category: cs.LG

Relevance: 75.0

TL;DR: 提出Diffusion Preview范式，通过快速低步数采样生成预览供用户评估，满意后再进行完整步数精炼。为解决现有方法预览质量差、预览与最终输出不一致的问题，提出了基于强化学习优化的轻量级高阶求解器ConsistencySolver。


<details>
  <summary>Details</summary>
Motivation: 图像扩散模型的推理过程缓慢，严重影响交互式用户体验。现有加速方法（包括免训练求解器和训练后蒸馏）难以提供高质量的预览，也无法保证预览与最终输出的一致性。

Method: 提出Diffusion Preview范式，采用快速低步数采样生成预览。提出ConsistencySolver，基于通用线性多步方法，通过强化学习优化的轻量级可训练高阶求解器，提升预览质量和一致性。

Result: ConsistencySolver在低步数场景下显著提升生成质量和一致性，比Multistep DPM-Solver减少47%步数达到相当的FID分数，优于蒸馏基线。用户研究表明该方法减少近50%用户交互时间同时保持生成质量。

Conclusion: Diffusion Preview范式结合ConsistencySolver能够有效解决扩散模型推理缓慢问题，显著提升交互效率，适合预览-精炼工作流程。

Abstract: The slow inference process of image diffusion models significantly degrades interactive user experiences. To address this, we introduce Diffusion Preview, a novel paradigm employing rapid, low-step sampling to generate preliminary outputs for user evaluation, deferring full-step refinement until the preview is deemed satisfactory. Existing acceleration methods, including training-free solvers and post-training distillation, struggle to deliver high-quality previews or ensure consistency between previews and final outputs. We propose ConsistencySolver derived from general linear multistep methods, a lightweight, trainable high-order solver optimized via Reinforcement Learning, that enhances preview quality and consistency. Experimental results demonstrate that ConsistencySolver significantly improves generation quality and consistency in low-step scenarios, making it ideal for efficient preview-and-refine workflows. Notably, it achieves FID scores on-par with Multistep DPM-Solver using 47% fewer steps, while outperforming distillation baselines. Furthermore, user studies indicate our approach reduces overall user interaction time by nearly 50% while maintaining generation quality. Code is available at https://github.com/G-U-N/consolver.

</details>


### [471] [A Scientific Reasoning Model for Organic Synthesis Procedure Generation](https://arxiv.org/abs/2512.13668)
*Guoqing Liu,Junren Li,Zihan Zhao,Eray Inanc,Krzysztof Maziarz,Jose Garrido Torres,Victor Garcia Satorras,Shoko Ueda,Christopher M. Bishop,Marwin Segler*

Main category: cs.LG

Relevance: 75.0

TL;DR: QFANG是一个科学推理语言模型，能够从化学反应方程直接生成精确的结构化实验程序，具有显式的思维链推理，用于计算机辅助合成规划。


<details>
  <summary>Details</summary>
Motivation: 解决计算机辅助合成规划的关键挑战是弥合计算路线设计与实际实验室执行之间的差距，特别是准确预测每个合成步骤的可行实验程序。

Method: 1) 构建高质量数据集（905,990个化学反应配对结构化动作序列）；2) 引入化学引导推理框架生成基于化学知识的思维链数据；3) 监督微调激发复杂化学推理；4) 应用可验证奖励的强化学习增强程序准确性。

Result: QFANG在传统NLP相似性指标和化学感知评估器（使用LLM作为评判）上都优于先进的通用推理模型和最近邻检索基线，并能泛化到某些域外反应类别，适应实验室条件和用户特定约束的变化。

Conclusion: QFANG生成高质量合成程序的能力代表了向弥合计算合成规划与全自动实验室合成之间差距的重要一步。

Abstract: Solving computer-aided synthesis planning is essential for enabling fully automated, robot-assisted synthesis workflows and improving the efficiency of drug discovery. A key challenge, however, is bridging the gap between computational route design and practical laboratory execution, particularly the accurate prediction of viable experimental procedures for each synthesis step. In this work, we present QFANG, a scientific reasoning language model capable of generating precise, structured experimental procedures directly from reaction equations, with explicit chain-of-thought reasoning. To develop QFANG, we curated a high-quality dataset comprising 905,990 chemical reactions paired with structured action sequences, extracted and processed from patent literature using large language models. We introduce a Chemistry-Guided Reasoning (CGR) framework that produces chain-of-thought data grounded in chemical knowledge at scale. The model subsequently undergoes supervised fine-tuning to elicit complex chemistry reasoning. Finally, we apply Reinforcement Learning from Verifiable Rewards (RLVR) to further enhance procedural accuracy. Experimental results demonstrate that QFANG outperforms advanced general-purpose reasoning models and nearest-neighbor retrieval baselines, measured by traditional NLP similarity metrics and a chemically aware evaluator using an LLM-as-a-judge. Moreover, QFANG generalizes to certain out-of-domain reaction classes and adapts to variations in laboratory conditions and user-specific constraints. We believe that QFANG's ability to generate high-quality synthesis procedures represents an important step toward bridging the gap between computational synthesis planning and fully automated laboratory synthesis.

</details>


### [472] [Directional Textual Inversion for Personalized Text-to-Image Generation](https://arxiv.org/abs/2512.13672)
*Kunhee Kim,NaHyeon Park,Kibeom Hong,Hyunjung Shim*

Main category: cs.LG

Relevance: 75.0

TL;DR: 本文提出方向性文本反演（DTI），通过固定嵌入向量的模长并仅在单位超球面上优化方向，解决了传统文本反演中因嵌入模长膨胀导致的复杂提示词失败问题。


<details>
  <summary>Details</summary>
Motivation: 传统文本反演（TI）在复杂提示词上经常失败，作者发现这是由于嵌入模长膨胀导致的：学习到的token漂移到分布外的模长，损害了预归一化Transformer中的提示词条件化。语义主要编码在CLIP token空间的方向中，而模长膨胀会损害上下文化。

Method: 提出方向性文本反演（DTI）：1）固定嵌入向量的模长为分布内尺度；2）仅通过黎曼SGD在单位超球面上优化方向；3）将方向学习建模为具有von Mises-Fisher先验的MAP估计，产生恒定方向先验梯度。

Result: 在个性化任务中，DTI在保持主体相似性的同时，比TI及其变体提高了文本保真度。DTI的超球面参数化实现了学习概念之间的平滑、语义连贯的插值（slerp），这是标准TI不具备的能力。

Conclusion: 仅优化方向是实现提示词忠实个性化的鲁棒且可扩展的路径。方向性优化解决了嵌入模长膨胀问题，提高了复杂提示词下的性能。

Abstract: Textual Inversion (TI) is an efficient approach to text-to-image personalization but often fails on complex prompts. We trace these failures to embedding norm inflation: learned tokens drift to out-of-distribution magnitudes, degrading prompt conditioning in pre-norm Transformers. Empirically, we show semantics are primarily encoded by direction in CLIP token space, while inflated norms harm contextualization; theoretically, we analyze how large magnitudes attenuate positional information and hinder residual updates in pre-norm blocks. We propose Directional Textual Inversion (DTI), which fixes the embedding magnitude to an in-distribution scale and optimizes only direction on the unit hypersphere via Riemannian SGD. We cast direction learning as MAP with a von Mises-Fisher prior, yielding a constant-direction prior gradient that is simple and efficient to incorporate. Across personalization tasks, DTI improves text fidelity over TI and TI-variants while maintaining subject similarity. Crucially, DTI's hyperspherical parameterization enables smooth, semantically coherent interpolation between learned concepts (slerp), a capability that is absent in standard TI. Our findings suggest that direction-only optimization is a robust and scalable path for prompt-faithful personalization.

</details>


### [473] [Keep the Lights On, Keep the Lengths in Check: Plug-In Adversarial Detection for Time-Series LLMs in Energy Forecasting](https://arxiv.org/abs/2512.12154)
*Hua Ma,Ruoxi Sun,Minhui Xue,Xingliang Yuan,Carsten Rudolph,Surya Nepal,Ling Liu*

Main category: cs.CR

Relevance: 75.0

TL;DR: 提出了一种针对时间序列大语言模型（TS-LLMs）的对抗样本检测框架，利用模型自身的变长输入能力，通过采样诱导的预测分歧来检测对抗攻击。


<details>
  <summary>Details</summary>
Motivation: 时间序列大语言模型在低碳电力系统中变得越来越重要，但面临对抗样本攻击的威胁。传统检测方法难以应对TS-LLMs的两个特点：1）对抗扰动利用全局时间依赖性，使局部检测失效；2）TS-LLMs接受变长输入，增加了检测的复杂性。

Method: 提出了一种即插即用的检测框架，利用TS-LLMs的变长输入能力。对输入序列生成多个缩短变体，通过测量这些变体预测的一致性来检测对抗样本：良性序列在采样下产生稳定预测，而对抗序列由于扰动针对完整序列优化，在较短子样本上表现出低预测相似性。

Result: 在三个代表性TS-LLMs（TimeGPT、TimesFM、TimeLLM）和三个能源数据集（ETTh2、NI、Consumption）上评估，在黑白盒攻击场景下均表现出强大且稳健的检测性能。

Conclusion: 该方法为TS-LLMs在真实世界能源系统中的预测提供了可靠的保护机制，具有实际应用价值。

Abstract: Accurate time-series forecasting is increasingly critical for planning and operations in low-carbon power systems. Emerging time-series large language models (TS-LLMs) now deliver this capability at scale, requiring no task-specific retraining, and are quickly becoming essential components within the Internet-of-Energy (IoE) ecosystem. However, their real-world deployment is complicated by a critical vulnerability: adversarial examples (AEs). Detecting these AEs is challenging because (i) adversarial perturbations are optimized across the entire input sequence and exploit global temporal dependencies, which renders local detection methods ineffective, and (ii) unlike traditional forecasting models with fixed input dimensions, TS-LLMs accept sequences of variable length, increasing variability that complicates detection. To address these challenges, we propose a plug-in detection framework that capitalizes on the TS-LLM's own variable-length input capability. Our method uses sampling-induced divergence as a detection signal. Given an input sequence, we generate multiple shortened variants and detect AEs by measuring the consistency of their forecasts: Benign sequences tend to produce stable predictions under sampling, whereas adversarial sequences show low forecast similarity, because perturbations optimized for a full-length sequence do not transfer reliably to shorter, differently-structured subsamples. We evaluate our approach on three representative TS-LLMs (TimeGPT, TimesFM, and TimeLLM) across three energy datasets: ETTh2 (Electricity Transformer Temperature), NI (Hourly Energy Consumption), and Consumption (Hourly Electricity Consumption and Production). Empirical results confirm strong and robust detection performance across both black-box and white-box attack scenarios, highlighting its practicality as a reliable safeguard for TS-LLM forecasting in real-world energy systems.

</details>


### [474] [Near-Zero-Overhead Freshness for Recommendation Systems via Inference-Side Model Updates](https://arxiv.org/abs/2512.12295)
*Wenjun Yu,Sitian Chen,Cheng Chen,Amelie Chi Zhou*

Main category: cs.DC

Relevance: 75.0

TL;DR: LiveUpdate系统通过在推理节点内共置LoRA训练器，消除推荐模型跨集群同步开销，利用推理节点空闲CPU和嵌入表梯度的低秩特性，实现实时模型更新，在1小时内获得更高准确率。


<details>
  <summary>Details</summary>
Motivation: 生产环境中的深度学习推荐模型面临新鲜度与准确率的权衡问题。由于需要同步PB级别的嵌入表，跨训练/推理集群的同步导致数分钟的延迟，降低了推荐质量和收入。同时观察到推理节点CPU利用率低（峰值≤20%），且嵌入表梯度具有内在的低秩结构。

Method: LiveUpdate系统将低秩适应训练器共置于推理节点中，消除跨集群同步。核心创新包括：1）通过奇异值监测实现动态秩适应，将内存开销限制在嵌入表的2%以内；2）NUMA感知的资源调度与硬件强制的QoS，消除更新对推理的干扰（P99延迟影响<20ms）。

Result: 评估显示LiveUpdate相比delta-update基线将更新成本降低2倍，同时在1小时窗口内达到更高准确率。相比最先进的delta-update方法，准确率提升0.04%到0.24%。

Conclusion: LiveUpdate通过将空闲推理资源转化为新鲜度引擎，实现了在线模型更新，解决了推荐模型的新鲜度-准确率权衡问题，同时保持推理性能不受影响。

Abstract: Deep Learning Recommendation Models (DLRMs) underpin personalized services but face a critical freshness-accuracy tradeoff due to massive parameter synchronization overheads. Production DLRMs deploy decoupled training/inference clusters, where synchronizing petabyte-scale embedding tables (EMTs) causes multi-minute staleness, degrading recommendation quality and revenue. We observe that (1) inference nodes exhibit sustained CPU underutilization (peak <= 20%), and (2) EMT gradients possess intrinsic low-rank structure, enabling compact update representation. We present LiveUpdate, a system that eliminates inter-cluster synchronization by colocating Low-Rank Adaptation (LoRA) trainers within inference nodes. LiveUpdate addresses two core challenges: (1) dynamic rank adaptation via singular value monitoring to constrain memory overhead (<2% of EMTs), and (2) NUMA-aware resource scheduling with hardware-enforced QoS to eliminate update inference contention (P99 latency impact <20ms). Evaluations show LiveUpdate reduces update costs by 2x versus delta-update baselines while achieving higher accuracy within 1-hour windows. By transforming idle inference resources into freshness engines, LiveUpdate delivers online model updates while outperforming state-of-the-art delta-update methods by 0.04% to 0.24% in accuracy.

</details>


### [475] [Phase transitions reveal hierarchical structure in deep neural networks](https://arxiv.org/abs/2512.11866)
*Ibrahim Talha Ersoy,Andrés Fernando Cardozo Licha,Karoline Wiesner*

Main category: cs.LG

Relevance: 65.0

TL;DR: 该论文提出了一个统一框架，将深度神经网络训练中的相变、鞍点和模式连通性等现象解释为损失和误差景观的几何特性。作者开发了一种使用L2正则化探测误差景观的算法，并在MNIST上验证了模式连通性。


<details>
  <summary>Details</summary>
Motivation: 深度神经网络训练中的许多现象（如相变、鞍点、模式连通性）缺乏统一的理论解释。作者旨在从损失和误差景观的几何角度统一理解这些现象，揭示它们之间的内在联系。

Method: 1. 理论分析：证明DNN学习中的相变由损失景观中的鞍点控制
2. 算法设计：提出使用L2正则化作为工具探测误差景观几何的简单快速算法
3. 实验验证：在MNIST数据集上应用该算法，验证模式连通性并寻找连接全局最小值的路径
4. 数值分析：展示鞍点如何诱导不同数字类别编码模型之间的转换

Result: 1. 成功建立了DNN关键训练现象的几何起源统一框架
2. 验证了模式连通性，能够高效找到连接全局最小值的路径
3. 发现鞍点诱导不同数字类别编码模型之间的转换
4. 揭示了类似于统计物理中相结构的精度盆地层次结构

Conclusion: 该工作从几何角度统一解释了DNN训练中的关键现象，建立了损失景观几何与训练动力学之间的联系，为理解深度学习的理论基础提供了新视角。

Abstract: Training Deep Neural Networks relies on the model converging on a high-dimensional, non-convex loss landscape toward a good minimum. Yet, much of the phenomenology of training remains ill understood. We focus on three seemingly disparate observations: the occurrence of phase transitions reminiscent of statistical physics, the ubiquity of saddle points, and phenomenon of mode connectivity relevant for model merging. We unify these within a single explanatory framework, the geometry of the loss and error landscapes. We analytically show that phase transitions in DNN learning are governed by saddle points in the loss landscape. Building on this insight, we introduce a simple, fast, and easy to implement algorithm that uses the L2 regularizer as a tool to probe the geometry of error landscapes. We apply it to confirm mode connectivity in DNNs trained on the MNIST dataset by efficiently finding paths that connect global minima. We then show numerically that saddle points induce transitions between models that encode distinct digit classes. Our work establishes the geometric origin of key training phenomena in DNNs and reveals a hierarchy of accuracy basins analogous to phases in statistical physics.

</details>


### [476] [BaRISTA: Brain Scale Informed Spatiotemporal Representation of Human Intracranial Neural Activity](https://arxiv.org/abs/2512.12135)
*Lucine L. Oganesian,Saba Hashemi,Maryam M. Shanechi*

Main category: cs.LG

Relevance: 65.0

TL;DR: 提出了一种新的时空Transformer模型，用于多区域颅内脑电图数据，通过灵活的空间尺度标记编码和掩码重建任务，提升下游解码性能。


<details>
  <summary>Details</summary>
Motivation: 颅内记录能够同时测量人脑多区域网络活动，现有Transformer神经基础模型在处理这种高度复杂的时空交互时面临挑战，特别是在如何编码空间信息以及设计自监督任务来学习脑网络模式方面存在关键问题。

Method: 提出新的时空Transformer模型，设计自监督掩码潜在重建任务，支持灵活的空间尺度标记编码和掩码策略，应用于公开的多区域颅内脑电图数据。

Result: 调整空间尺度对下游解码有显著影响；比通道级编码更大的空间尺度编码能提升下游解码性能；方法支持区域级标记编码同时保持准确的通道级神经重建。

Conclusion: 该建模框架能够探索标记编码和掩码的空间尺度，揭示了它们对多区域人脑活动神经基础模型自监督预训练的重要性，并增强了下游解码性能。

Abstract: Intracranial recordings have opened a unique opportunity to simultaneously measure activity across multiregional networks in the human brain. Recent works have focused on developing transformer-based neurofoundation models of such recordings that can generalize across subjects and datasets. However, these recordings exhibit highly complex spatiotemporal interactions across diverse spatial scales, from the single-channel scale to the scale of brain regions. As such, there remain critical open questions regarding how best to encode spatial information and how to design self-supervision tasks that enable the learning of brain network patterns and enhance downstream decoding performance using such high-dimensional, multiregional recordings. To allow for exploring these questions, we propose a new spatiotemporal transformer model of multiregional neural activity and a corresponding self-supervised masked latent reconstruction task, designed to enable flexibility in the spatial scale used for token encoding and masking. Applying this model on publicly available multiregional intracranial electrophysiology (iEEG) data, we demonstrate that adjusting the spatial scale for both token encoding and masked reconstruction significantly impacts downstream decoding. Further, we find that spatial encoding at larger scales than channel-level encoding, which is commonly used in existing iEEG transformer models, improves downstream decoding performance. Finally, we demonstrate that our method allows for region-level token encoding while also maintaining accurate channel-level neural reconstruction. Taken together, our modeling framework enables exploration of the spatial scales used for token encoding and masking, reveals their importance towards self-supervised pretraining of neurofoundation models of multiregional human brain activity, and enhances downstream decoding performance.

</details>


### [477] [Knowledge-Guided Masked Autoencoder with Linear Spectral Mixing and Spectral-Angle-Aware Reconstruction](https://arxiv.org/abs/2512.12445)
*Abdul Matin,Rupasree Dey,Tanjim Bin Faruk,Shrideep Pallickara,Sangmi Lee Pallickara*

Main category: cs.LG

Relevance: 65.0

TL;DR: 提出一种知识引导的ViT掩码自编码器，将线性光谱混合模型作为物理约束嵌入自监督重建过程，提升重建质量和下游任务性能


<details>
  <summary>Details</summary>
Motivation: 将领域知识融入深度学习可以提升模型可解释性、泛化能力和数据效率。当前方法主要依赖数据驱动优化，缺乏对已知物理关系的约束，导致学习到的表示可能不符合实际物理原理。

Method: 提出知识引导的ViT掩码自编码器，将线性光谱混合模型作为物理约束，结合基于物理的光谱角度匹配器损失，与传统的Huber损失联合优化，确保学习到的表示符合观测信号与潜在分量之间的结构关系。

Result: 实验结果表明，该模型显著提升了重建质量，改善了在下游任务上的性能，在有限监督下稳定训练，并产生了基于物理原理的可解释潜在表示。

Conclusion: 将物理信息归纳偏置嵌入基于Transformer的自监督学习具有广阔前景，能够增强重建保真度、稳定训练过程，并产生可解释的表示。

Abstract: Integrating domain knowledge into deep learning has emerged as a promising direction for improving model interpretability, generalization, and data efficiency. In this work, we present a novel knowledge-guided ViT-based Masked Autoencoder that embeds scientific domain knowledge within the self-supervised reconstruction process. Instead of relying solely on data-driven optimization, our proposed approach incorporates the Linear Spectral Mixing Model (LSMM) as a physical constraint and physically-based Spectral Angle Mapper (SAM), ensuring that learned representations adhere to known structural relationships between observed signals and their latent components. The framework jointly optimizes LSMM and SAM loss with a conventional Huber loss objective, promoting both numerical accuracy and geometric consistency in the feature space. This knowledge-guided design enhances reconstruction fidelity, stabilizes training under limited supervision, and yields interpretable latent representations grounded in physical principles. The experimental findings indicate that the proposed model substantially enhances reconstruction quality and improves downstream task performance, highlighting the promise of embedding physics-informed inductive biases within transformer-based self-supervised learning.

</details>


### [478] [PerNodeDrop: A Method Balancing Specialized Subnets and Regularization in Deep Neural Networks](https://arxiv.org/abs/2512.12663)
*Gelesh G Omathil,Sreeja CS*

Main category: cs.LG

Relevance: 65.0

TL;DR: PerNodeDrop是一种轻量级随机正则化方法，通过每个样本、每个节点的扰动来打破现有技术中噪声的均匀性，从而在保留有用协同适应的同时进行正则化，提高模型泛化能力。


<details>
  <summary>Details</summary>
Motivation: 深度神经网络虽然具有强大的表示能力，但容易过拟合。神经元倾向于协同适应，这虽然能捕获复杂的特征交互，但也强化了虚假的、不可泛化的模式。现有的噪声正则化方法（如Dropout、DropConnect）通常在整个层或批次样本上均匀应用噪声，这可能会同时抑制有害和有益的协同适应。

Method: PerNodeDrop是一种轻量级随机正则化方法，它在每个样本、每个节点级别应用扰动，而不是在批次级别。与DropConnect不同，PerNodeDrop在样本级别丢弃权重，而不是批次级别。通过期望损失分析形式化了其扰动如何减弱过度协同适应同时保留预测性交互。

Result: 在视觉、文本和音频基准测试上的实证评估表明，PerNodeDrop相对于标准噪声正则化方法提高了泛化性能，缩小了训练和验证性能之间的差距，提高了在未见数据上的可靠性。

Conclusion: PerNodeDrop通过打破噪声均匀性，允许每个节点经历输入特定的变异性，从而在保留有用协同适应的同时进行正则化，提高了深度神经网络的泛化能力和可靠性。

Abstract: Deep neural networks possess strong representational capacity yet remain vulnerable to overfitting, primarily because neurons tend to co-adapt in ways that, while capturing complex and fine-grained feature interactions, also reinforce spurious and non-generalizable patterns that inflate training performance but reduce reliability on unseen data. Noise-based regularizers such as Dropout and DropConnect address this issue by injecting stochastic perturbations during training, but the noise they apply is typically uniform across a layer or across a batch of samples, which can suppress both harmful and beneficial co-adaptation.
  This work introduces PerNodeDrop, a lightweight stochastic regularization method. It applies per-sample, per-node perturbations to break the uniformity of the noise injected by existing techniques, thereby allowing each node to experience input-specific variability. Hence, PerNodeDrop preserves useful co-adaptation while applying regularization. This narrows the gap between training and validation performance and improves reliability on unseen data, as evident from the experiments.
  Although superficially similar to DropConnect, PerNodeDrop operates at the sample level. It drops weights at the sample level, not the batch level. An expected-loss analysis formalizes how its perturbations attenuate excessive co-adaptation while retaining predictive interactions. Empirical evaluations on vision, text, and audio benchmarks indicate improved generalization relative to the standard noise-based regularizer.

</details>


### [479] [Optimal Resource Allocation for ML Model Training and Deployment under Concept Drift](https://arxiv.org/abs/2512.12816)
*Hasan Burhan Beytur,Gustavo de Veciana,Haris Vikalo,Kevin S Chan*

Main category: cs.LG

Relevance: 65.0

TL;DR: 论文研究在概念漂移和有限预算下如何分配机器学习模型的训练和部署资源，提出模型无关框架，分析训练策略与概念持续时间老化特性的关系，并研究通信约束下的部署优化。


<details>
  <summary>Details</summary>
Motivation: 在现实应用中，机器学习模型面临概念漂移问题，而模型提供商需要向多个客户端分发模型。客户端设备支持本地推理但缺乏重新训练能力，性能维护负担落在提供商身上。如何在有限预算下有效分配训练和部署资源成为关键挑战。

Method: 提出一个模型无关框架，捕捉资源分配、概念漂移动态和部署时机之间的相互作用。分析概念持续时间分布特性（DMRL/IMRL）对最优训练策略的影响，研究通信约束下的部署优化问题，证明其拟凸性并提出随机调度策略。

Result: 1. 在概念突变情况下，当概念持续时间服从递减平均剩余寿命（DMRL）分布时，推导出预算约束下的最优训练策略；2. 证明直观启发式方法在递增平均剩余寿命（IMRL）情况下是次优的；3. 部署优化问题在温和条件下是拟凸的，提出的随机调度策略能实现接近最优的客户端性能。

Conclusion: 该研究为概念漂移下的成本高效机器学习模型管理提供了理论和算法基础，对持续学习、分布式推理和自适应机器学习系统有重要启示。最优策略设计需要充分考虑概念持续时间的老化特性。

Abstract: We study how to allocate resources for training and deployment of machine learning (ML) models under concept drift and limited budgets. We consider a setting in which a model provider distributes trained models to multiple clients whose devices support local inference but lack the ability to retrain those models, placing the burden of performance maintenance on the provider. We introduce a model-agnostic framework that captures the interaction between resource allocation, concept drift dynamics, and deployment timing. We show that optimal training policies depend critically on the aging properties of concept durations. Under sudden concept changes, we derive optimal training policies subject to budget constraints when concept durations follow distributions with Decreasing Mean Residual Life (DMRL), and show that intuitive heuristics are provably suboptimal under Increasing Mean Residual Life (IMRL). We further study model deployment under communication constraints, prove that the associated optimization problem is quasi-convex under mild conditions, and propose a randomized scheduling strategy that achieves near-optimal client-side performance. These results offer theoretical and algorithmic foundations for cost-efficient ML model management under concept drift, with implications for continual learning, distributed inference, and adaptive ML systems.

</details>


### [480] [On the continuity of flows](https://arxiv.org/abs/2512.12821)
*Congzhou M Sha*

Main category: cs.LG

Relevance: 65.0

TL;DR: 该论文研究了流匹配生成模型中拓扑不匹配问题：当先验分布与目标分布拓扑结构不匹配时，最优速度场会出现空间不连续性，导致粒子在中间时间需要做出离散路由决策。


<details>
  <summary>Details</summary>
Motivation: 流匹配已成为连续归一化流生成建模的强大框架，但作者发现当先验分布和目标分布拓扑结构不匹配时，标准流匹配目标下的最优速度场可能出现空间不连续性，这影响了流匹配的性能和理论理解。

Method: 通过理论分析双模态高斯混合模型，证明最优速度场在决策边界处存在跳跃不连续性，且幅度随时间接近目标分布而趋于无穷大。同时通过实证验证理论，并讨论流匹配在流形上的潜在影响。

Result: 分析表明拓扑不匹配导致的速度场不连续性不是L^2损失特有的，而是分布间拓扑不匹配的普遍后果。实证结果验证了理论预测，揭示了流匹配在拓扑不匹配情况下的基本限制。

Conclusion: 流匹配在拓扑不匹配分布间存在基本限制，最优速度场必然出现不连续性。这一发现对流匹配理论、黎曼流匹配以及神经网络学习不连续表示的能力都有重要启示。

Abstract: Flow matching has emerged as a powerful framework for generative modeling through continuous normalizing flows. We investigate a potential topological constraint: when the prior distribution and target distribution have mismatched topology (e.g., unimodal to multimodal), the optimal velocity field under standard flow matching objectives may exhibit spatial discontinuities. We suggest that this discontinuity arises from the requirement that continuous flows must bifurcate to map a single mode to multiple modes, forcing particles to make discrete routing decisions at intermediate times. Through theoretical analysis on bimodal Gaussian mixtures, we demonstrate that the optimal velocity field exhibits jump discontinuities along decision boundaries, with magnitude approaching infinity as time approaches the target distribution. Our analysis suggests that this phenomenon is not specific to $L^2$ loss, but rather may be a consequence of topological mismatch between distributions. We validate our theory empirically and discuss potential implications for flow matching on manifolds, connecting our findings to recent work on Riemannian flow matching and the challenge of learning discontinuous representations in neural networks.

</details>


### [481] [LLM-based Personalized Portfolio Recommender: Integrating Large Language Models and Reinforcement Learning for Intelligent Investment Strategy Optimization](https://arxiv.org/abs/2512.12922)
*Bangyu Li,Boping Gu,Ziyang Ding*

Main category: cs.LG

Relevance: 65.0

TL;DR: 提出基于LLM的个性化投资组合推荐框架，结合大语言模型、强化学习和个体风险偏好建模，为投资者提供智能投资决策支持。


<details>
  <summary>Details</summary>
Motivation: 现代金融市场中，投资者需要反映个人风险偏好并能适应动态市场条件的个性化投资策略。传统基于规则或静态优化的方法难以捕捉投资者行为、市场波动和金融目标之间的非线性交互关系。

Method: 提出LLM-based Personalized Portfolio Recommender框架，整合大语言模型、强化学习和个体风险偏好建模，通过LLM理解投资者偏好和市场信息，强化学习优化投资决策，个性化风险模型确保策略符合个体需求。

Result: 论文提出了一个集成框架，能够生成个性化、自适应的投资组合推荐，相比传统方法能更好地处理非线性关系和动态市场条件。

Conclusion: 该框架为智能投资决策提供了新的解决方案，结合LLM、强化学习和个性化建模，有望提升投资策略的适应性和个性化程度。

Abstract: In modern financial markets, investors increasingly seek personalized and adaptive portfolio strategies that reflect their individual risk preferences and respond to dynamic market conditions. Traditional rule-based or static optimization approaches often fail to capture the nonlinear interactions among investor behavior, market volatility, and evolving financial objectives. To address these limitations, this paper introduces the LLM-based Personalized Portfolio Recommender , an integrated framework that combines Large Language Models, reinforcement learning, and individualized risk preference modeling to support intelligent investment decision-making.

</details>


### [482] [Investigating Data Pruning for Pretraining Biological Foundation Models at Scale](https://arxiv.org/abs/2512.12932)
*Yifan Wu,Jiyue Jiang,Xichen Ye,Yiqi Wang,Chang Zhou,Yitao Xu,Jiayang Chen,He Hu,Weizhong Zhang,Cheng Jin,Jiao Yuan,Yu Li*

Main category: cs.LG

Relevance: 65.0

TL;DR: 本文提出了一种针对生物基础模型的后验影响引导数据剪枝框架，通过子集自影响公式高效估计样本重要性，在RNA和蛋白质任务上验证了有效性，即使剪枝率超过99%仍优于随机选择。


<details>
  <summary>Details</summary>
Motivation: 生物基础模型需要大规模数据和参数，导致计算成本高昂，阻碍了学术实验室的可复现性和可访问性。为了解决这些问题，研究者探索数据剪枝在生物基础模型预训练中的可行性。

Method: 提出了后验影响引导的数据剪枝框架，包含子集自影响公式来高效估计样本重要性，以及两种选择策略：Top-k影响和覆盖中心影响。在RNA-FM和ESM-C两个代表性生物基础模型上进行了验证。

Result: 在RNA任务中，框架在超过99%的极端剪枝率下持续优于随机选择基线。在蛋白质相关任务中也显示出良好的泛化能力。核心集甚至优于十倍大小的随机子集，揭示了生物序列数据集中的大量冗余。

Conclusion: 影响引导的数据剪枝有潜力显著降低生物基础模型预训练的计算成本，为更高效、可访问和可持续的生物AI研究铺平道路。

Abstract: Biological foundation models (BioFMs), pretrained on large-scale biological sequences, have recently shown strong potential in providing meaningful representations for diverse downstream bioinformatics tasks. However, such models often rely on millions to billions of training sequences and billions of parameters, resulting in prohibitive computational costs and significant barriers to reproducibility and accessibility, particularly for academic labs. To address these challenges, we investigate the feasibility of data pruning for BioFM pretraining and propose a post-hoc influence-guided data pruning framework tailored to biological domains. Our approach introduces a subset-based self-influence formulation that enables efficient estimation of sample importance at low computational cost, and builds upon it two simple yet effective selection strategies, namely Top-k Influence (Top I) and Coverage-Centric Influence (CCI). We empirically validate our method on two representative BioFMs, RNA-FM and ESM-C. For RNA, our framework consistently outperforms random selection baselines under an extreme pruning rate of over 99 percent, demonstrating its effectiveness. Furthermore, we show the generalizability of our framework on protein-related tasks using ESM-C. In particular, our coreset even outperforms random subsets that are ten times larger in both RNA and protein settings, revealing substantial redundancy in biological sequence datasets. These findings underscore the potential of influence-guided data pruning to substantially reduce the computational cost of BioFM pretraining, paving the way for more efficient, accessible, and sustainable biological AI research.

</details>


### [483] [CoDeQ: End-to-End Joint Model Compression with Dead-Zone Quantizer for High-Sparsity and Low-Precision Networks](https://arxiv.org/abs/2512.12981)
*Jonathan Wenshøj,Tong Chen,Bob Pepin,Raghavendra Selvan*

Main category: cs.LG

Relevance: 65.0

TL;DR: CoDeQ是一种完全可微的联合剪枝-量化方法，通过参数化量化器死区宽度并学习它，实现稀疏性和量化参数的端到端优化，无需额外辅助过程。


<details>
  <summary>Details</summary>
Motivation: 现有联合剪枝-量化方法依赖训练循环外的辅助过程来确定压缩参数，增加了工程复杂性和超参数调优，且缺乏直接的数据驱动梯度信号，可能导致次优压缩。

Method: 基于量化器死区等价于幅度剪枝的观察，参数化死区宽度并通过反向传播学习，同时学习量化参数。该方法提供稀疏性显式控制，解耦稀疏性选择和比特宽度选择。

Result: 在ImageNet上使用ResNet-18，CoDeQ将比特操作减少到约5%，同时在固定精度和混合精度设置下保持接近全精度准确率。

Conclusion: CoDeQ是一种简单、完全可微、架构无关的联合剪枝-量化方法，通过端到端优化同时确定稀疏模式和量化参数，无需辅助过程。

Abstract: While joint pruning--quantization is theoretically superior to sequential application, current joint methods rely on auxiliary procedures outside the training loop for finding compression parameters. This reliance adds engineering complexity and hyperparameter tuning, while also lacking a direct data-driven gradient signal, which might result in sub-optimal compression. In this paper, we introduce CoDeQ, a simple, fully differentiable method for joint pruning--quantization. Our approach builds on a key observation: the dead-zone of a scalar quantizer is equivalent to magnitude pruning, and can be used to induce sparsity directly within the quantization operator. Concretely, we parameterize the dead-zone width and learn it via backpropagation, alongside the quantization parameters. This design provides explicit control of sparsity, regularized by a single global hyperparameter, while decoupling sparsity selection from bit-width selection. The result is a method for Compression with Dead-zone Quantizer (CoDeQ) that supports both fixed-precision and mixed-precision quantization (controlled by an optional second hyperparameter). It simultaneously determines the sparsity pattern and quantization parameters in a single end-to-end optimization. Consequently, CoDeQ does not require any auxiliary procedures, making the method architecture-agnostic and straightforward to implement. On ImageNet with ResNet-18, CoDeQ reduces bit operations to ~5% while maintaining close to full precision accuracy in both fixed and mixed-precision regimes.

</details>


### [484] [ALIGN-FL: Architecture-independent Learning through Invariant Generative component sharing in Federated Learning](https://arxiv.org/abs/2512.13316)
*Mayank Gulati,Benedikt Groß,Gerhard Wunder*

Main category: cs.LG

Relevance: 65.0

TL;DR: ALIGN-FL是一种新颖的分布式学习方法，通过选择性共享生成组件解决高度不均衡数据分布的学习挑战，使用隐私保护机制在异构客户端上实现跨域学习。


<details>
  <summary>Details</summary>
Motivation: 解决联邦学习中高度不均衡（Non-IID）数据分布的挑战，特别是在跨域协作场景中，同时保护隐私并支持异构客户端架构。

Method: 提出选择性共享生成组件而非完整模型参数的框架，结合两种隐私机制：DP-SGD自适应剪裁和Lipschitz正则化VAE解码器，采用支持异构客户端的状态架构。

Result: 在MNIST和Fashion-MNIST数据集上验证了方法有效性，隐私机制成功将敏感异常点映射到典型数据点，在极端Non-IID场景中保持实用性。

Conclusion: ALIGN-FL能够有效处理跨域协作中的隐私保护和数据分布不均衡问题，为联邦学习提供了新的解决方案。

Abstract: We present ALIGN-FL, a novel approach to distributed learning that addresses the challenge of learning from highly disjoint data distributions through selective sharing of generative components. Instead of exchanging full model parameters, our framework enables privacy-preserving learning by transferring only generative capabilities across clients, while the server performs global training using synthetic samples. Through complementary privacy mechanisms: DP-SGD with adaptive clipping and Lipschitz regularized VAE decoders and a stateful architecture supporting heterogeneous clients, we experimentally validate our approach on MNIST and Fashion-MNIST datasets with cross-domain outliers. Our analysis demonstrates that both privacy mechanisms effectively map sensitive outliers to typical data points while maintaining utility in extreme Non-IID scenarios typical of cross-silo collaborations.
  Index Terms: Client-invariant Learning, Federated Learning (FL), Privacy-preserving Generative Models, Non-Independent and Identically Distributed (Non-IID), Heterogeneous Architectures

</details>


### [485] [XNNTab -- Interpretable Neural Networks for Tabular Data using Sparse Autoencoders](https://arxiv.org/abs/2512.13442)
*Khawla Elhadri,Jörg Schlötterer,Christin Seifert*

Main category: cs.LG

Relevance: 65.0

TL;DR: XNNTab是一种用于表格数据的可解释神经网络架构，结合了神经网络的表达能力和可解释性，通过学习高度非线性特征表示，并使用稀疏自编码器分解为单语义特征，为这些特征分配人类可解释的概念。


<details>
  <summary>Details</summary>
Motivation: 在依赖表格数据的应用中，可解释性至关重要，决策树和线性回归等模型被广泛应用。虽然神经网络能提供更高的预测性能，但由于其黑盒性质而未被采用。需要一种既能保持神经网络表达能力又具有可解释性的解决方案。

Method: XNNTab首先学习高度非线性的特征表示，然后使用稀疏自编码器（SAE）将这些特征分解为单语义特征。这些特征被分配人类可解释的概念，使整个模型预测具有内在可解释性。

Result: XNNTab在性能上优于可解释预测模型，并达到与非可解释对应模型相当的性能水平。

Conclusion: XNNTab成功地将神经网络的表达能力与可解释性相结合，为表格数据应用提供了一种既高性能又可解释的解决方案。

Abstract: In data-driven applications relying on tabular data, where interpretability is key, machine learning models such as decision trees and linear regression are applied. Although neural networks can provide higher predictive performance, they are not used because of their blackbox nature. In this work, we present XNNTab, a neural architecture that combines the expressiveness of neural networks and interpretability. XNNTab first learns highly non-linear feature representations, which are decomposed into monosemantic features using a sparse autoencoder (SAE). These features are then assigned human-interpretable concepts, making the overall model prediction intrinsically interpretable. XNNTab outperforms interpretable predictive models, and achieves comparable performance to its non-interpretable counterparts.

</details>


### [486] [Evolving Deep Learning Optimizers](https://arxiv.org/abs/2512.11853)
*Mitchell Marfinetz*

Main category: cs.NE

Relevance: 65.0

TL;DR: 提出一个遗传算法框架来自动发现深度学习优化算法，通过进化搜索发现的新优化器在多个视觉任务上优于Adam，在CIFAR-10上相对提升7.7%


<details>
  <summary>Details</summary>
Motivation: 深度学习优化算法通常由人工设计，限制了算法设计的探索空间。本文旨在通过自动化搜索方法发现更优的优化算法，并揭示与传统手工设计不同的设计原则。

Method: 使用遗传算法框架，将优化器编码为基因组，包含原始更新项（梯度、动量、RMS归一化、Adam风格自适应项、符号更新）的组合、超参数和调度选项。在50代、50个个体的种群上进行进化搜索，在多个视觉任务上评估。

Result: 发现的新优化器在聚合适应度上比Adam提升2.6%，在CIFAR-10上相对提升7.7%。该优化器结合了符号梯度项和自适应矩估计，使用比Adam更低的动量系数（β₁=0.86，β₂=0.94），禁用偏置校正，启用学习率预热和余弦衰减。

Conclusion: 进化搜索能够发现具有竞争力的优化算法，并揭示与手工设计优化器不同的设计原则。自动化优化器发现是一个有前景的研究方向。

Abstract: We present a genetic algorithm framework for automatically discovering deep learning optimization algorithms. Our approach encodes optimizers as genomes that specify combinations of primitive update terms (gradient, momentum, RMS normalization, Adam-style adaptive terms, and sign-based updates) along with hyperparameters and scheduling options. Through evolutionary search over 50 generations with a population of 50 individuals, evaluated across multiple vision tasks, we discover an evolved optimizer that outperforms Adam by 2.6% in aggregate fitness and achieves a 7.7% relative improvement on CIFAR-10. The evolved optimizer combines sign-based gradient terms with adaptive moment estimation, uses lower momentum coefficients than Adam ($β_1$=0.86, $β_2$=0.94), and notably disables bias correction while enabling learning rate warmup and cosine decay. Our results demonstrate that evolutionary search can discover competitive optimization algorithms and reveal design principles that differ from hand-crafted optimizers. Code is available at https://github.com/mmarfinetz/evo-optimizer.

</details>


### [487] [ceLLMate: Sandboxing Browser AI Agents](https://arxiv.org/abs/2512.12594)
*Luoxi Meng,Henry Feng,Ilia Shumailov,Earlence Fernandes*

Main category: cs.CR

Relevance: 65.0

TL;DR: ceLLMate是一个浏览器级沙盒框架，通过将低级UI行为映射到高级语义动作，并自动预测沙盒策略，来保护浏览器使用代理免受提示注入攻击


<details>
  <summary>Details</summary>
Motivation: 浏览器使用代理(BUAs)虽然能自动化网页任务，但容易受到提示注入攻击，可能导致隐私泄露或执行非预期操作。现有方法在策略执行方面存在语义鸿沟，且缺乏预定义策略

Method: 1) 引入代理站点地图，将低级浏览器行为映射到高级语义动作；2) 结合网站作者强制策略和自动策略预测层，从用户自然语言任务中自适应实例化策略；3) 实现为代理无关的浏览器扩展

Result: ceLLMate能有效阻止各种类型的提示注入攻击，且开销可忽略不计

Conclusion: ceLLMate通过解决语义鸿沟和策略预测问题，为浏览器使用代理提供了有效的沙盒保护机制

Abstract: Browser-using agents (BUAs) are an emerging class of autonomous agents that interact with web browsers in human-like ways, including clicking, scrolling, filling forms, and navigating across pages. While these agents help automate repetitive online tasks, they are vulnerable to prompt injection attacks that can trick an agent into performing undesired actions, such as leaking private information or issuing state-changing requests. We propose ceLLMate, a browser-level sandboxing framework that restricts the agent's ambient authority and reduces the blast radius of prompt injections. We address two fundamental challenges: (1) The semantic gap challenge in policy enforcement arises because the agent operates through low-level UI observations and manipulations; however, writing and enforcing policies directly over UI-level events is brittle and error-prone. To address this challenge, we introduce an agent sitemap that maps low-level browser behaviors to high-level semantic actions. (2) Policy prediction in BUAs is the norm rather than the exception. BUAs have no app developer to pre-declare sandboxing policies, and thus, ceLLMate pairs website-authored mandatory policies with an automated policy-prediction layer that adapts and instantiates these policies from the user's natural-language task. We implement ceLLMate as an agent-agnostic browser extension and demonstrate how it enables sandboxing policies that effectively block various types of prompt injection attacks with negligible overhead.

</details>


### [488] [Self-Motivated Growing Neural Network for Adaptive Architecture via Local Structural Plasticity](https://arxiv.org/abs/2512.12713)
*Yiyang Jia,Chengxu Zhou*

Main category: cs.NE

Relevance: 65.0

TL;DR: 提出SMGrNN：一种通过局部结构可塑性模块在线演化拓扑的自激励增长神经网络控制器，用于深度强化学习，无需手动架构调优


<details>
  <summary>Details</summary>
Motivation: 传统深度强化学习中的控制策略通常使用固定容量的多层感知机，缺乏结构可塑性且依赖全局误差信号，需要手动架构调优

Method: 引入SMGrNN控制器，通过局部结构可塑性模块(SPM)在线演化网络拓扑。SPM监控神经元激活和边权重更新统计，触发神经元插入和剪枝，同时使用标准梯度优化器更新突触权重

Result: 在控制基准测试中，相比多层感知机基线，SMGrNN获得相似或更高的回报、更低的方差和任务适当的网络规模。消融研究显示自适应拓扑改善奖励稳定性

Conclusion: SMGrNN通过局部结构可塑性实现网络容量在线调节，其模块化设计支持未来集成Hebbian可塑性和脉冲时序依赖可塑性，适用于人工和脉冲神经网络实现

Abstract: Control policies in deep reinforcement learning are often implemented with fixed-capacity multilayer perceptrons trained by backpropagation, which lack structural plasticity and depend on global error signals. This paper introduces the Self-Motivated Growing Neural Network (SMGrNN), a controller whose topology evolves online through a local Structural Plasticity Module (SPM). The SPM monitors neuron activations and edge-wise weight update statistics over short temporal windows and uses these signals to trigger neuron insertion and pruning, while synaptic weights are updated by a standard gradient-based optimizer. This allows network capacity to be regulated during learning without manual architectural tuning.
  SMGrNN is evaluated on control benchmarks via policy distillation. Compared with multilayer perceptron baselines, it achieves similar or higher returns, lower variance, and task-appropriate network sizes. Ablation studies with growth disabled and growth-only variants isolate the role of structural plasticity, showing that adaptive topology improves reward stability. The local and modular design of SPM enables future integration of a Hebbian plasticity module and spike-timing-dependent plasticity, so that SMGrNN can support both artificial and spiking neural implementations driven by local rules.

</details>


### [489] [Evaluating Singular Value Thresholds for DNN Weight Matrices based on Random Matrix Theory](https://arxiv.org/abs/2512.12911)
*Kohei Nishikawa,Koki Shimizu,Hashiguchi Hiroki*

Main category: stat.ML

Relevance: 65.0

TL;DR: 该研究评估了基于奇异值分解的深度神经网络权重矩阵低秩近似中奇异值去除的阈值。通过将权重矩阵建模为信号矩阵和噪声矩阵之和，使用随机矩阵理论确定阈值来去除噪声相关的奇异值，并提出基于信号矩阵与原始权重矩阵奇异向量余弦相似度的评估指标来比较两种阈值估计方法。


<details>
  <summary>Details</summary>
Motivation: 深度神经网络权重矩阵通常包含信号和噪声成分，通过低秩近似可以压缩模型并提高效率。然而，如何确定合适的阈值来区分信号和噪声相关的奇异值是一个关键问题，需要可靠的评估方法来验证不同阈值估计方法的有效性。

Method: 1) 将权重矩阵建模为信号矩阵和噪声矩阵之和；2) 使用基于随机矩阵理论的阈值来去除噪声相关的奇异值；3) 提出基于信号矩阵与原始权重矩阵奇异向量余弦相似度的评估指标；4) 在数值实验中比较两种阈值估计方法。

Result: 研究提出了一个有效的评估指标来比较不同阈值估计方法，通过余弦相似度量化信号矩阵奇异向量与原始权重矩阵奇异向量之间的相似性，为确定合适的低秩近似阈值提供了理论依据和实验验证。

Conclusion: 该研究为深度神经网络权重矩阵的低秩近似提供了系统的阈值评估框架，提出的余弦相似度指标能够有效评估不同阈值估计方法的性能，有助于优化模型压缩和效率提升。

Abstract: This study evaluates thresholds for removing singular values from singular value decomposition-based low-rank approximations of deep neural network weight matrices. Each weight matrix is modeled as the sum of signal and noise matrices. The low-rank approximation is obtained by removing noise-related singular values using a threshold based on random matrix theory. To assess the adequacy of this threshold, we propose an evaluation metric based on the cosine similarity between the singular vectors of the signal and original weight matrices. The proposed metric is used in numerical experiments to compare two threshold estimation methods.

</details>


### [490] [Fast Policy Learning for 6-DOF Position Control of Underwater Vehicles](https://arxiv.org/abs/2512.13359)
*Sümer Tunçay,Alain Andres,Ignacio Carlucho*

Main category: cs.RO

Relevance: 65.0

TL;DR: 本文提出了一种基于JAX和MuJoCo-XLA的GPU加速RL训练框架，用于自主水下航行器的6自由度位置控制，实现了2分钟内的快速训练和零样本仿真到现实迁移。


<details>
  <summary>Details</summary>
Motivation: 自主水下航行器在复杂海洋环境中需要可靠的6自由度位置控制。传统控制器在标称条件下有效，但面对未建模动态或环境扰动时性能下降。强化学习提供了强大替代方案，但训练通常缓慢且仿真到现实迁移具有挑战性。

Method: 开发了基于JAX和MuJoCo-XLA的GPU加速RL训练流水线，通过联合JIT编译大规模并行物理仿真和学习更新，实现快速训练。系统评估了多种RL算法，并在真实水下实验中验证了零样本仿真到现实迁移。

Result: 实现了2分钟内的训练时间，在真实水下实验中展示了鲁棒的6自由度轨迹跟踪和有效的扰动抑制，首次在现实世界中明确展示了基于RL的AUV全六自由度位置控制。

Conclusion: GPU加速的RL训练框架能够快速训练出鲁棒的AUV控制器，实现零样本仿真到现实迁移，为解决复杂动态环境中的机器人控制问题提供了有效方案。

Abstract: Autonomous Underwater Vehicles (AUVs) require reliable six-degree-of-freedom (6-DOF) position control to operate effectively in complex and dynamic marine environments. Traditional controllers are effective under nominal conditions but exhibit degraded performance when faced with unmodeled dynamics or environmental disturbances. Reinforcement learning (RL) provides a powerful alternative but training is typically slow and sim-to-real transfer remains challenging. This work introduces a GPU-accelerated RL training pipeline built in JAX and MuJoCo-XLA (MJX). By jointly JIT-compiling large-scale parallel physics simulation and learning updates, we achieve training times of under two minutes.Through systematic evaluation of multiple RL algorithms, we show robust 6-DOF trajectory tracking and effective disturbance rejection in real underwater experiments, with policies transferred zero-shot from simulation. Our results provide the first explicit real-world demonstration of RL-based AUV position control across all six degrees of freedom.

</details>


### [491] [A Nonparametric Statistics Approach to Feature Selection in Deep Neural Networks with Theoretical Guarantees](https://arxiv.org/abs/2512.13565)
*Junye Du,Zhenghao Li,Zhutong Gu,Long Feng*

Main category: stat.ML

Relevance: 65.0

TL;DR: 该论文提出了一种基于神经网络的非线性特征选择方法，通过将神经网络重构为索引模型并利用二阶Stein公式，实现了无梯度下降的特征选择一致性，在高维稀疏场景下具有理论保证。


<details>
  <summary>Details</summary>
Motivation: 在非线性回归设置中，当响应变量y的条件期望是未知非线性函数G(x_S0)时，传统特征选择方法难以处理复杂的非线性关系和特征交互。现有基于深度学习的特征选择方法通常依赖优化过程，缺乏理论保证，特别是在高维稀疏场景下。

Method: 1. 将神经网络重构为索引模型，利用二阶Stein公式估计相关特征集S0，避免了梯度下降优化过程
2. 通过神经网络的强逼近能力将结果推广到Hölder光滑函数
3. 引入筛选-选择机制处理高维场景，实现非线性选择一致性
4. 在选定特征上重新训练神经网络进行预测

Result: 1. 理论保证：在样本量n=Ω(p²)时实现特征选择一致性；在高维稀疏场景下，当n=Ω(s log p)时实现非线性选择一致性
2. 实验验证：在模拟数据和真实数据上表现出色，即使存在复杂特征交互也能保持强性能
3. 预测性能：在放宽的稀疏性假设下建立了预测性能的理论保证

Conclusion: 该方法提供了一种理论可靠、无需梯度下降的非线性特征选择框架，能够有效处理高维稀疏数据中的复杂非线性关系，为基于深度学习的特征选择提供了新的理论视角。

Abstract: This paper tackles the problem of feature selection in a highly challenging setting: $\mathbb{E}(y | \boldsymbol{x}) = G(\boldsymbol{x}_{\mathcal{S}_0})$, where $\mathcal{S}_0$ is the set of relevant features and $G$ is an unknown, potentially nonlinear function subject to mild smoothness conditions. Our approach begins with feature selection in deep neural networks, then generalizes the results to H{ö}lder smooth functions by exploiting the strong approximation capabilities of neural networks. Unlike conventional optimization-based deep learning methods, we reformulate neural networks as index models and estimate $\mathcal{S}_0$ using the second-order Stein's formula. This gradient-descent-free strategy guarantees feature selection consistency with a sample size requirement of $n = Ω(p^2)$, where $p$ is the feature dimension. To handle high-dimensional scenarios, we further introduce a screening-and-selection mechanism that achieves nonlinear selection consistency when $n = Ω(s \log p)$, with $s$ representing the sparsity level. Additionally, we refit a neural network on the selected features for prediction and establish performance guarantees under a relaxed sparsity assumption. Extensive simulations and real-data analyses demonstrate the strong performance of our method even in the presence of complex feature interactions.

</details>


### [492] [CR3G: Causal Reasoning for Patient-Centric Explanations in Radiology Report Generation](https://arxiv.org/abs/2512.11830)
*Satyam Kumar*

Main category: cs.LG

Relevance: 45.0

TL;DR: 提出CR3G框架，通过因果推理增强胸部X光报告生成，聚焦因果关系和患者中心解释，提升AI诊断的可信度。


<details>
  <summary>Details</summary>
Motivation: 当前AI模型在医学影像分析中擅长发现相关性模式，但难以理解模式与患者状况之间的深层因果关系。因果推理能超越模式识别，揭示X光发现与特定诊断之间的原因-结果关系，从而提升AI生成报告的可理解性和临床实用性。

Method: 提出CR3G（Causal Reasoning for Patient-Centric Explanations in radiology Report Generation）框架，采用提示驱动方法，应用于胸部X光分析。该框架专注于因果关系推理，生成以患者为中心的解释性报告。

Result: CR3G在5种异常中的2种上表现出更好的因果关系能力和解释能力。

Conclusion: CR3G框架通过因果推理增强了AI驱动的胸部X光报告生成，提高了诊断质量和临床可信度，使AI诊断在临床实践中更有用和可信。

Abstract: Automatic chest X-ray report generation is an important area of research aimed at improving diagnostic accuracy and helping doctors make faster decisions. Current AI models are good at finding correlations (or patterns) in medical images. Still, they often struggle to understand the deeper cause-and-effect relationships between those patterns and a patient condition. Causal inference is a powerful approach that goes beyond identifying patterns to uncover why certain findings in an X-ray relate to a specific diagnosis. In this paper, we will explore the prompt-driven framework Causal Reasoning for Patient-Centric Explanations in radiology Report Generation (CR3G) that is applied to chest X-ray analysis to improve understanding of AI-generated reports by focusing on cause-and-effect relationships, reasoning and generate patient-centric explanation. The aim to enhance the quality of AI-driven diagnostics, making them more useful and trustworthy in clinical practice. CR3G has shown better causal relationship capability and explanation capability for 2 out of 5 abnormalities.

</details>


### [493] [HydroDiffusion: Diffusion-Based Probabilistic Streamflow Forecasting with a State Space Backbone](https://arxiv.org/abs/2512.12183)
*Yihan Wang,Annan Yu,Lujun Zhang,Charuleka Varadharajan,N. Benjamin Erichson*

Main category: cs.LG

Relevance: 45.0

TL;DR: HydroDiffusion：基于状态空间模型的扩散模型框架，用于概率性径流预测，通过联合去噪多日轨迹确保时间一致性，在CAMELS数据集上超越LSTM和DRUM基准。


<details>
  <summary>Details</summary>
Motivation: 现有扩散模型用于概率性径流预测时依赖LSTM骨干网络和单步训练目标，限制了捕捉长程依赖关系的能力，且无法保证跨预测时间步的轨迹一致性。需要开发能处理长序列依赖并产生连贯预测轨迹的新框架。

Method: 提出HydroDiffusion框架，采用仅解码器的状态空间模型作为骨干网络，联合去噪完整多日轨迹（单次前向传播），确保时间一致性并减少自回归预测中的误差累积。在CAMELS数据集的531个流域进行评估。

Result: HydroDiffusion在观测气象驱动下实现强实时预报精度，在整个模拟时间范围内保持一致的性能。在确定性预测和概率性预测技能方面均优于基于LSTM的扩散基准和DRUM模型。

Conclusion: HydroDiffusion为中期径流预测建立了稳健的生成建模框架，提供了新的建模基准，并为大陆尺度概率性水文预测的未来研究奠定了基础。

Abstract: Recent advances have introduced diffusion models for probabilistic streamflow forecasting, demonstrating strong early flood-warning skill. However, current implementations rely on recurrent Long Short-Term Memory (LSTM) backbones and single-step training objectives, which limit their ability to capture long-range dependencies and produce coherent forecast trajectories across lead times. To address these limitations, we developed HydroDiffusion, a diffusion-based probabilistic forecasting framework with a decoder-only state space model backbone. The proposed framework jointly denoises full multi-day trajectories in a single pass, ensuring temporal coherence and mitigating error accumulation common in autoregressive prediction. HydroDiffusion is evaluated across 531 watersheds in the contiguous United States (CONUS) in the CAMELS dataset. We benchmark HydroDiffusion against two diffusion baselines with LSTM backbones, as well as the recently proposed Diffusion-based Runoff Model (DRUM). Results show that HydroDiffusion achieves strong nowcast accuracy when driven by observed meteorological forcings, and maintains consistent performance across the full simulation horizon. Moreover, HydroDiffusion delivers stronger deterministic and probabilistic forecast skill than DRUM in operational forecasting. These results establish HydroDiffusion as a robust generative modeling framework for medium-range streamflow forecasting, providing both a new modeling benchmark and a foundation for future research on probabilistic hydrologic prediction at continental scales.

</details>


### [494] [TwinFormer: A Dual-Level Transformer for Long-Sequence Time-Series Forecasting](https://arxiv.org/abs/2512.12301)
*Mahima Kumavat,Aditya Maheshwari*

Main category: cs.LG

Relevance: 45.0

TL;DR: TwinFormer是一种用于长时间序列预测的分层Transformer，通过局部-全局双阶段处理和GRU聚合实现线性复杂度，在多个真实世界数据集上超越现有方法


<details>
  <summary>Details</summary>
Motivation: 长时间序列预测需要有效建模局部动态和长程依赖，传统Transformer在计算复杂度上存在挑战。需要设计高效架构来处理长序列时间序列数据

Method: 1. 将输入分成非重叠时间片段；2. 局部信息器使用top-k稀疏注意力建模片段内动态，然后平均池化；3. 全局信息器使用相同注意力捕获片段间长程依赖；4. 轻量级GRU聚合全局上下文片段token进行多步预测

Result: 在8个真实世界数据集（天气、股价、温度、电力消耗、电力、疾病等）上，预测范围96-720，TwinFormer在34个比较中27次进入前2名，其中17次MAE和RMSE最佳，10次次佳，超越PatchTST、iTransformer、FEDformer等

Conclusion: TwinFormer通过分层设计和稀疏注意力实现了线性复杂度，在长时间序列预测任务上表现出色，top-k稀疏注意力优于ProbSparse，GRU聚合有效

Abstract: TwinFormer is a hierarchical Transformer for long-sequence time-series forecasting. It divides the input into non-overlapping temporal patches and processes them in two stages: (1) a Local Informer with top-$k$ Sparse Attention models intra-patch dynamics, followed by mean pooling; (2) a Global Informer captures long-range inter-patch dependencies using the same top-$k$ attention. A lightweight GRU aggregates the globally contextualized patch tokens for direct multi-horizon prediction. The resulting architecture achieves linear $O(kLd)$ time and memory complexity. On eight real-world benchmarking datasets from six different domains, including weather, stock price, temperature, power consumption, electricity, and disease, and forecasting horizons $96-720$, TwinFormer secures $27$ positions in the top two out of $34$. Out of the $27$, it achieves the best performance on MAE and RMSE at $17$ places and $10$ at the second-best place on MAE and RMSE. This consistently outperforms PatchTST, iTransformer, FEDformer, Informer, and vanilla Transformers. Ablations confirm the superiority of top-$k$ Sparse Attention over ProbSparse and the effectiveness of GRU-based aggregation. Code is available at this repository: https://github.com/Mahimakumavat1205/TwinFormer.

</details>


### [495] [Optimized Architectures for Kolmogorov-Arnold Networks](https://arxiv.org/abs/2512.12448)
*James Bagrow,Josh Bongard*

Main category: cs.LG

Relevance: 45.0

TL;DR: 该论文提出通过过参数化架构结合可微稀疏化来学习紧凑、可解释的Kolmogorov-Arnold网络（KANs），在保持精度的同时提高可解释性。


<details>
  <summary>Details</summary>
Motivation: 传统KANs的架构增强方法往往带来复杂性，损害了KANs原本吸引人的可解释性。需要在保持KANs可解释性的同时提高其表达能力，解决科学机器学习中表达性与可解释性之间的关键矛盾。

Method: 采用过参数化架构结合可微稀疏化技术，将架构搜索转化为端到端优化问题。通过过参数化提供更大的搜索空间，然后通过可微稀疏化学习紧凑的模型结构。

Result: 在函数逼近基准测试、动力系统预测和真实世界预测任务中，该方法实现了竞争性或更优的精度，同时发现了显著更小的模型。过参数化与稀疏化具有协同效应，组合效果优于单独使用任一方法。

Conclusion: 过参数化结合可微稀疏化为KANs提供了一条原则性路径，既能提高模型表达能力，又能保持可解释性，解决了科学机器学习中的关键矛盾。

Abstract: Efforts to improve Kolmogorov-Arnold networks (KANs) with architectural enhancements have been stymied by the complexity those enhancements bring, undermining the interpretability that makes KANs attractive in the first place. Here we study overprovisioned architectures combined with sparsification to learn compact, interpretable KANs without sacrificing accuracy. Crucially, we focus on differentiable sparsification, turning architecture search into an end-to-end optimization problem. Across function approximation benchmarks, dynamical systems forecasting, and real-world prediction tasks, we demonstrate competitive or superior accuracy while discovering substantially smaller models. Overprovisioning and sparsification are synergistic, with the combination outperforming either alone. The result is a principled path toward models that are both more expressive and more interpretable, addressing a key tension in scientific machine learning.

</details>


### [496] [On Approaches to Building Surrogate ODE Models for Diffusion Bridges](https://arxiv.org/abs/2512.12671)
*Maria Khilchuk,Vladimir Latypov,Pavel Kleshchev,Alexander Hvatov*

Main category: cs.LG

Relevance: 45.0

TL;DR: 提出两种基于代理模型的扩散与薛定谔桥近似方法：SINDy-FM使用稀疏回归学习符号微分方程，DSBM-NeuralODE使用神经ODE参数化，在保持性能的同时大幅提升效率和可解释性。


<details>
  <summary>Details</summary>
Motivation: 扩散模型和薛定谔桥模型在生成建模中表现出色，但面临计算成本高、训练复杂的问题。连续时间桥模型虽然采样更快，但过参数化的神经网络描述其最优动力学，且底层随机微分方程难以高效积分。

Method: 1. SINDy-FM：利用稀疏回归从数据中识别可解释的符号微分方程；2. DSBM-NeuralODE：薛定谔桥的神经ODE重构，实现灵活的连续时间参数化。

Result: 在高斯传输任务和MNIST潜在翻译实验中，代理模型达到竞争性性能，同时显著提升效率和可解释性。SINDy-FM将参数数量减少数个数量级，实现近瞬时推理。

Conclusion: 该方法为实际部署开辟了一类新的可处理、高性能桥模型，通过代理模型简化复杂动力学，平衡性能与效率。

Abstract: Diffusion and Schrödinger Bridge models have established state-of-the-art performance in generative modeling but are often hampered by significant computational costs and complex training procedures. While continuous-time bridges promise faster sampling, overparameterized neural networks describe their optimal dynamics, and the underlying stochastic differential equations can be difficult to integrate efficiently. This work introduces a novel paradigm that uses surrogate models to create simpler, faster, and more flexible approximations of these dynamics. We propose two specific algorithms: SINDy Flow Matching (SINDy-FM), which leverages sparse regression to identify interpretable, symbolic differential equations from data, and a Neural-ODE reformulation of the Schrödinger Bridge (DSBM-NeuralODE) for flexible continuous-time parameterization. Our experiments on Gaussian transport tasks and MNIST latent translation demonstrate that these surrogates achieve competitive performance while offering dramatic improvements in efficiency and interpretability. The symbolic SINDy-FM models, in particular, reduce parameter counts by several orders of magnitude and enable near-instantaneous inference, paving the way for a new class of tractable and high-performing bridge models for practical deployment.

</details>


### [497] [Optimal Labeler Assignment and Sampling for Active Learning in the Presence of Imperfect Labels](https://arxiv.org/abs/2512.12870)
*Pouya Ahadi,Blair Winograd,Camille Zaug,Karunesh Arora,Lijun Wang,Kamran Paynabar*

Main category: cs.LG

Relevance: 45.0

TL;DR: 提出了一种新的主动学习框架，通过优化分配模型将查询点分配给标注者以最小化噪声，并引入新的采样方法减少标签噪声对分类器性能的影响。


<details>
  <summary>Details</summary>
Motivation: 主动学习在标注成本高的领域很重要，但标注者提供的标签往往存在噪声，特别是复杂样本更容易被错误标注，这导致从不完美标注数据中学习会产生不准确的分类器。

Method: 1) 分配模型：优化地将查询点分配给标注者，以最小化每个周期内的最大可能噪声；2) 新的采样方法：识别最佳查询点，减少标签噪声对分类器性能的影响。

Result: 实验表明，该方法相比多个基准方法显著提高了分类性能。

Conclusion: 提出的主动学习框架能够有效处理标注噪声问题，通过优化分配和采样策略构建更鲁棒的分类模型。

Abstract: Active Learning (AL) has garnered significant interest across various application domains where labeling training data is costly. AL provides a framework that helps practitioners query informative samples for annotation by oracles (labelers). However, these labels often contain noise due to varying levels of labeler accuracy. Additionally, uncertain samples are more prone to receiving incorrect labels because of their complexity. Learning from imperfectly labeled data leads to an inaccurate classifier. We propose a novel AL framework to construct a robust classification model by minimizing noise levels. Our approach includes an assignment model that optimally assigns query points to labelers, aiming to minimize the maximum possible noise within each cycle. Additionally, we introduce a new sampling method to identify the best query points, reducing the impact of label noise on classifier performance. Our experiments demonstrate that our approach significantly improves classification performance compared to several benchmark methods.

</details>


### [498] [Distillation of Discrete Diffusion by Exact Conditional Distribution Matching](https://arxiv.org/abs/2512.12889)
*Yansong Gao,Yu Sun*

Main category: cs.LG

Relevance: 45.0

TL;DR: 提出一种基于条件分布匹配的离散扩散模型蒸馏方法，通过匹配教师模型和学生模型的条件分布，实现高效推理


<details>
  <summary>Details</summary>
Motivation: 离散扩散模型（DDMs）在生成分类数据方面强大，但推理时需要大量函数评估，导致推理成本高昂。现有加速方法要么依赖近似模拟器（如τ-leaping），要么需要训练新的学生模型和代理目标函数。

Method: 提出基于条件分布匹配的蒸馏方法。核心观察是：干净数据给定噪声状态的反向条件分布p_{0|t}(x_0|x_t)可以通过中间时间进行马尔可夫分解，并可以从边际密度比和已知的前向CTMC核中恢复。利用这一结构定义蒸馏目标，直接匹配预训练教师模型和低NFE学生模型的条件分布，适用于一步和少步采样器。

Result: 该方法提供了一种简单且原理清晰的蒸馏替代方案，能够直接匹配条件分布，避免了近似模拟器的误差和传统蒸馏方法的复杂性。

Conclusion: 提出的条件分布匹配方法为离散扩散模型提供了一种有效的蒸馏框架，能够显著减少推理时的函数评估次数，提高推理效率。

Abstract: Discrete diffusion models (DDMs) are a powerful class of generative models for categorical data, but they typically require many function evaluations for a single sample, making inference expensive. Existing acceleration methods either rely on approximate simulators, such as $τ$-leaping, or on distillation schemes that train new student models and auxiliary networks with proxy objectives. We propose a simple and principled distillation alternative based on \emph{conditional distribution matching}. Our key observation is that the reverse conditional distribution of clean data given a noisy state, $p_{0\mid t}(x_0 \mid x_t)$, admits a Markov decomposition through intermediate times and can be recovered from marginal density ratios and the known forward CTMC kernel. We exploit this structure to define distillation objectives that directly match conditional distributions between a pre-trained teacher and a low-NFE student, both for one-step and few-step samplers.

</details>


### [499] [Evaluating Adversarial Attacks on Federated Learning for Temperature Forecasting](https://arxiv.org/abs/2512.13207)
*Karina Chichifoi,Fabio Merizzi,Michele Colajanni*

Main category: cs.LG

Relevance: 45.0

TL;DR: 研究联邦学习在气象预报中的安全漏洞，发现即使少数恶意客户端通过数据投毒攻击就能显著扭曲区域温度预测，而基于异常值检测的防御方法对空间相关数据效果有限。


<details>
  <summary>Details</summary>
Motivation: 深度学习与联邦学习结合为下一代气象预报提供了强大能力，但联邦学习的分布式特性引入了新的安全漏洞。数据投毒攻击可能通过注入恶意训练数据来降低模型性能或引入系统性偏差，这种威胁在气象数据存在空间依赖性的情况下尤为严重，因为局部扰动可能通过全局模型聚合影响更广泛区域。

Method: 使用Copernicus欧洲区域再分析数据集，模拟地理分布的客户端，评估基于补丁和全局偏置攻击对区域温度预测的影响。研究攻击策略包括：1）全局温度偏置攻击；2）协调的补丁攻击。最后评估修剪均值聚合作为防御机制的效果。

Result: 1）单个恶意客户端的全局温度偏置攻击可使预测偏移高达-1.7K；2）协调的补丁攻击使均方误差增加三倍以上，并产生超过+3.5K的持续区域异常；3）修剪均值聚合能有效防御全局偏置攻击（性能下降2-13%），但对补丁攻击完全失效（性能恶化281-603%）。

Conclusion: 联邦学习在气象预报中存在严重安全漏洞，少数恶意客户端就能显著扭曲预测结果。空间相关的数据投毒攻击特别危险，传统的基于异常值检测的防御方法对这类攻击效果有限，需要开发更鲁棒的防御机制来保护联邦气象预报系统。

Abstract: Deep learning and federated learning (FL) are becoming powerful partners for next-generation weather forecasting. Deep learning enables high-resolution spatiotemporal forecasts that can surpass traditional numerical models, while FL allows institutions in different locations to collaboratively train models without sharing raw data, addressing efficiency and security concerns. While FL has shown promise across heterogeneous regions, its distributed nature introduces new vulnerabilities. In particular, data poisoning attacks, in which compromised clients inject manipulated training data, can degrade performance or introduce systematic biases. These threats are amplified by spatial dependencies in meteorological data, allowing localized perturbations to influence broader regions through global model aggregation. In this study, we investigate how adversarial clients distort federated surface temperature forecasts trained on the Copernicus European Regional ReAnalysis (CERRA) dataset. We simulate geographically distributed clients and evaluate patch-based and global biasing attacks on regional temperature forecasts. Our results show that even a small fraction of poisoned clients can mislead predictions across large, spatially connected areas. A global temperature bias attack from a single compromised client shifts predictions by up to -1.7 K, while coordinated patch attacks more than triple the mean squared error and produce persistent regional anomalies exceeding +3.5 K. Finally, we assess trimmed mean aggregation as a defense mechanism, showing that it successfully defends against global bias attacks (2-13\% degradation) but fails against patch attacks (281-603\% amplification), exposing limitations of outlier-based defenses for spatially correlated data.

</details>


### [500] [CORE: Contrastive Masked Feature Reconstruction on Graphs](https://arxiv.org/abs/2512.13235)
*Jianyuan Bo,Yuan Fang*

Main category: cs.LG

Relevance: 45.0

TL;DR: 本文提出CORE框架，将对比学习整合到掩码特征重建中，用于图自监督学习，在节点和图分类任务上取得SOTA结果。


<details>
  <summary>Details</summary>
Motivation: 在图自监督学习中，生成式方法（如掩码特征重建MFR）和对比式方法（GCL）通常被视为两种不同范式。作者观察到两者都旨在最大化相似元素间的一致性，并发现它们在特定条件下目标函数会收敛，这启发作者探索两者的集成以提升图自监督学习性能。

Method: 提出Contrastive Masked Feature Reconstruction (CORE)框架：1) 仅使用掩码节点的原始特征和重建特征形成正样本对，迫使编码器更关注上下文信息而非节点自身特征；2) 利用掩码节点自身作为负样本，结合MFR的重建能力和GCL的判别能力来更好地捕获图结构。

Result: CORE在节点和图分类任务上显著优于MFR，达到SOTA性能：在节点分类任务上比GraphMAE和GraphMAE2分别提升2.80%和3.72%；在图分类任务上分别提升3.82%和3.76%。

Conclusion: 生成式和对比式图自监督学习方法在理论上是互补的而非根本不同的，CORE框架成功整合了两者的优势，为图自监督学习提供了新的有效范式。

Abstract: In the rapidly evolving field of self-supervised learning on graphs, generative and contrastive methodologies have emerged as two dominant approaches. Our study focuses on masked feature reconstruction (MFR), a generative technique where a model learns to restore the raw features of masked nodes in a self-supervised manner. We observe that both MFR and graph contrastive learning (GCL) aim to maximize agreement between similar elements. Building on this observation, we reveal a novel theoretical insight: under specific conditions, the objectives of MFR and node-level GCL converge, despite their distinct operational mechanisms. This theoretical connection suggests these approaches are complementary rather than fundamentally different, prompting us to explore their integration to enhance self-supervised learning on graphs. Our research presents Contrastive Masked Feature Reconstruction (CORE), a novel graph self-supervised learning framework that integrates contrastive learning into MFR. Specifically, we form positive pairs exclusively between the original and reconstructed features of masked nodes, encouraging the encoder to prioritize contextual information over the node's own features. Additionally, we leverage the masked nodes themselves as negative samples, combining MFR's reconstructive power with GCL's discriminative ability to better capture intrinsic graph structures. Empirically, our proposed framework CORE significantly outperforms MFR across node and graph classification tasks, demonstrating state-of-the-art results. In particular, CORE surpasses GraphMAE and GraphMAE2 by up to 2.80% and 3.72% on node classification tasks, and by up to 3.82% and 3.76% on graph classification tasks.

</details>


### [501] [Scalable Formal Verification via Autoencoder Latent Space Abstraction](https://arxiv.org/abs/2512.13593)
*Robert Reed,Morteza Lahijanian,Luca Laurenti*

Main category: cs.LG

Relevance: 45.0

TL;DR: 提出一种基于凸自编码器和核方法的降维形式化验证框架，能在保持正确性的前提下显著提升高维系统的可扩展性


<details>
  <summary>Details</summary>
Motivation: 有限抽象方法虽然为系统验证提供了形式化框架，但面临高维系统的可扩展性挑战，因为状态空间离散化会随维度指数增长。基于学习的降维方法（如神经网络和自编码器）有潜力缓解这一问题，但如何确保验证结果的正确性仍是一个开放问题。

Method: 1) 使用凸自编码器对系统进行形式化降维；2) 通过核方法在潜在空间中学习系统动态；3) 在潜在空间中构建有限抽象，并保证该抽象包含原始系统的真实行为；4) 将潜在空间的验证结果映射回原始系统。

Result: 该方法在多个系统上展示了有效性，包括一个由神经网络控制的26维系统，显示出显著的可扩展性改进而不损失严谨性。

Conclusion: 提出的框架成功解决了高维系统形式化验证的可扩展性问题，通过凸自编码器和核方法实现了降维，同时保证了验证结果的正确性，为高维系统的形式化验证提供了新的解决方案。

Abstract: Finite Abstraction methods provide a powerful formal framework for proving that systems satisfy their specifications. However, these techniques face scalability challenges for high-dimensional systems, as they rely on state-space discretization which grows exponentially with dimension. Learning-based approaches to dimensionality reduction, utilizing neural networks and autoencoders, have shown great potential to alleviate this problem. However, ensuring the correctness of the resulting verification results remains an open question. In this work, we provide a formal approach to reduce the dimensionality of systems via convex autoencoders and learn the dynamics in the latent space through a kernel-based method. We then construct a finite abstraction from the learned model in the latent space and guarantee that the abstraction contains the true behaviors of the original system. We show that the verification results in the latent space can be mapped back to the original system. Finally, we demonstrate the effectiveness of our approach on multiple systems, including a 26D system controlled by a neural network, showing significant scalability improvements without loss of rigor.

</details>


### [502] [StutterFuse: Mitigating Modality Collapse in Stuttering Detection with Jaccard-Weighted Metric Learning and Gated Fusion](https://arxiv.org/abs/2512.13632)
*Guransh Singh,Md Shah Fahad*

Main category: cs.LG

Relevance: 45.0

TL;DR: 提出了StutterFuse，首个基于检索增强分类器（RAC）的多标签口吃检测方法，通过检索临床案例库来增强分类，解决了传统参数化模型在重叠性言语障碍检测中的局限性。


<details>
  <summary>Details</summary>
Motivation: 现有参数化模型在处理重叠性言语障碍（如"阻塞"与"延长"同时出现）时表现不佳，因为训练数据中这些特定组合稀缺。检索增强生成（RAG）范式在NLP领域已取得革命性进展，但在病理语音处理中尚未探索。

Method: 1. 提出检索增强分类器（RAC），基于Conformer编码器并条件化于非参数化临床案例记忆库；2. 识别并解决"模态塌陷"问题（即"回声室"效应）；3. 采用SetCon（Jaccard加权度量学习目标）优化多标签集合相似性；4. 使用门控混合专家融合策略动态协调声学证据与检索上下文。

Result: 在SEP-28k数据集上，StutterFuse获得加权F1分数0.65，优于强基线模型，并展现出显著的零样本跨语言泛化能力。

Conclusion: StutterFuse首次将检索增强范式引入病理语音处理，通过参考而非记忆的方式提升重叠性言语障碍检测性能，为解决训练数据稀缺问题提供了新思路。

Abstract: Stuttering detection breaks down when disfluencies overlap. Existing parametric models struggle to distinguish complex, simultaneous disfluencies (e.g., a 'block' with a 'prolongation') due to the scarcity of these specific combinations in training data. While Retrieval-Augmented Generation (RAG) has revolutionized NLP by grounding models in external knowledge, this paradigm remains unexplored in pathological speech processing. To bridge this gap, we introduce StutterFuse, the first Retrieval-Augmented Classifier (RAC) for multi-label stuttering detection. By conditioning a Conformer encoder on a non-parametric memory bank of clinical examples, we allow the model to classify by reference rather than memorization. We further identify and solve "Modality Collapse", an "Echo Chamber" effect where naive retrieval boosts recall but degrades precision. We mitigate this using: (1) SetCon, a Jaccard-Weighted Metric Learning objective that optimizes for multi-label set similarity, and (2) a Gated Mixture-of-Experts fusion strategy that dynamically arbitrates between acoustic evidence and retrieved context. On the SEP-28k dataset, StutterFuse achieves a weighted F1-score of 0.65, outperforming strong baselines and demonstrating remarkable zero-shot cross-lingual generalization.

</details>


### [503] [Adversarial Attacks Against Deep Learning-Based Radio Frequency Fingerprint Identification](https://arxiv.org/abs/2512.12002)
*Jie Ma,Junqing Zhang,Guanxiong Shen,Alan Marshall,Chip-Hong Chang*

Main category: cs.CR

Relevance: 45.0

TL;DR: 该论文研究了针对射频指纹识别(RFFI)系统的对抗攻击，分析了FGSM、PGD和UAP三种攻击方法在不同深度学习模型上的有效性，并展示了UAP在实际无线环境中的攻击能力。


<details>
  <summary>Details</summary>
Motivation: 射频指纹识别(RFFI)是物联网设备轻量级认证的新兴技术，但基于深度学习的RFFI系统容易受到对抗攻击。目前对不同类型RFFI分类器的有效对抗攻击尚未充分探索，需要全面研究各种攻击方法在RFFI系统中的效果。

Method: 使用三种对抗攻击算法(FGSM、PGD、UAP)对基于不同深度学习模型(CNN、LSTM、GRU)的RFFI系统进行攻击。在LoRa-RFFI平台上进行实验，并考虑无线环境的特殊因素(实时攻击、时间有效性等)，特别使用UAP进行实际攻击。

Result: 实验结果显示生成的扰动对CNN、LSTM和GRU模型都有效。UAP攻击在攻击者几乎不了解受害者RFFI系统的情况下，仍能达到81.7%的成功率，证明了对抗攻击对RFFI系统的实际威胁。

Conclusion: RFFI系统对对抗攻击具有脆弱性，UAP等攻击方法在实际无线环境中具有显著威胁。这强调了在RFFI系统设计中需要考虑对抗鲁棒性的重要性。

Abstract: Radio frequency fingerprint identification (RFFI) is an emerging technique for the lightweight authentication of wireless Internet of things (IoT) devices. RFFI exploits deep learning models to extract hardware impairments to uniquely identify wireless devices. Recent studies show deep learning-based RFFI is vulnerable to adversarial attacks. However, effective adversarial attacks against different types of RFFI classifiers have not yet been explored. In this paper, we carried out a comprehensive investigations into different adversarial attack methods on RFFI systems using various deep learning models. Three specific algorithms, fast gradient sign method (FGSM), projected gradient descent (PGD), and universal adversarial perturbation (UAP), were analyzed. The attacks were launched to LoRa-RFFI and the experimental results showed the generated perturbations were effective against convolutional neural networks (CNNs), long short-term memory (LSTM) networks, and gated recurrent units (GRU). We further used UAP to launch practical attacks. Special factors were considered for the wireless context, including implementing real-time attacks, the effectiveness of the attacks over a period of time, etc. Our experimental evaluation demonstrated that UAP can successfully launch adversarial attacks against the RFFI, achieving a success rate of 81.7% when the adversary almost has no prior knowledge of the victim RFFI systems.

</details>


### [504] [CoLSE: A Lightweight and Robust Hybrid Learned Model for Single-Table Cardinality Estimation using Joint CDF](https://arxiv.org/abs/2512.12624)
*Lankadinee Rathuwadu,Guanli Liu,Christopher Leckie,Renata Borovica-Gajic*

Main category: cs.DB

Relevance: 45.0

TL;DR: CoLSE是一种基于copula理论和轻量级神经网络的混合学习式基数估计方法，在准确性、训练时间、推理延迟和模型大小之间取得了良好平衡。


<details>
  <summary>Details</summary>
Motivation: 基数估计是查询优化的关键组件，现有方法在准确性、效率和内存占用之间难以取得平衡，需要一种能同时优化这三个关键因素的解决方案。

Method: 提出CoLSE混合学习方法：1) 使用基于copula理论的新算法直接建模查询区间的联合概率分布；2) 集成轻量级神经网络来校正残差估计误差。

Result: 实验结果表明，CoLSE在准确性、训练时间、推理延迟和模型大小方面取得了有利的权衡，优于现有的最先进方法。

Conclusion: CoLSE通过结合copula理论和神经网络校正，为单表基数估计提供了一种平衡准确性和效率的有效混合方法。

Abstract: Cardinality estimation (CE), the task of predicting the result size of queries is a critical component of query optimization. Accurate estimates are essential for generating efficient query execution plans. Recently, machine learning techniques have been applied to CE, broadly categorized into query-driven and data-driven approaches. Data-driven methods learn the joint distribution of data, while query-driven methods construct regression models that map query features to cardinalities. Ideally, a CE technique should strike a balance among three key factors: accuracy, efficiency, and memory footprint. However, existing state-of-the-art models often fail to achieve this balance.
  To address this, we propose CoLSE, a hybrid learned approach for single-table cardinality estimation. CoLSE directly models the joint probability over queried intervals using a novel algorithm based on copula theory and integrates a lightweight neural network to correct residual estimation errors. Experimental results show that CoLSE achieves a favorable trade-off among accuracy, training time, inference latency, and model size, outperforming existing state-of-the-art methods.

</details>


### [505] [Limits To (Machine) Learning](https://arxiv.org/abs/2512.12735)
*Zhimin Chen,Bryan Kelly,Semyon Malamud*

Main category: stat.ML

Relevance: 45.0

TL;DR: 论文提出了"学习极限差距"(LLG)理论，量化了机器学习模型在有限样本下不可避免的预测误差，并应用于金融数据预测分析。


<details>
  <summary>Details</summary>
Motivation: 机器学习方法虽然灵活，但受限于有限样本，无法完全逼近真实数据生成过程。作者旨在量化这种不可避免的预测误差，特别是在金融数据预测中，传统ML方法可能严重低估真实可预测性。

Method: 提出"学习极限差距"(LLG)理论框架，作为模型经验拟合与总体基准之间不可避免差异的通用下界。应用于多种金融变量（超额收益、收益率、信用利差、估值比率），推导LLG修正的Hansen-Jagannathan边界，分析一般均衡设置中的参数学习含义。

Result: 发现金融数据中的LLG值很大，表明标准ML方法可能严重低估真实可预测性。LLG为产生超额波动提供了自然机制，并改进了经典金融边界。

Conclusion: LLG理论为理解机器学习在有限样本下的根本局限性提供了框架，特别在金融预测中，需要修正观察到的预测性能以恢复真实的总体预测能力。

Abstract: Machine learning (ML) methods are highly flexible, but their ability to approximate the true data-generating process is fundamentally constrained by finite samples. We characterize a universal lower bound, the Limits-to-Learning Gap (LLG), quantifying the unavoidable discrepancy between a model's empirical fit and the population benchmark. Recovering the true population $R^2$, therefore, requires correcting observed predictive performance by this bound. Using a broad set of variables, including excess returns, yields, credit spreads, and valuation ratios, we find that the implied LLGs are large. This indicates that standard ML approaches can substantially understate true predictability in financial data. We also derive LLG-based refinements to the classic Hansen and Jagannathan (1991) bounds, analyze implications for parameter learning in general-equilibrium settings, and show that the LLG provides a natural mechanism for generating excess volatility.

</details>


### [506] [PAC-Bayes Bounds for Multivariate Linear Regression and Linear Autoencoders](https://arxiv.org/abs/2512.12905)
*Ruixin Guo,Ruoming Jin,Xinyu Li,Yang Zhou*

Main category: stat.ML

Relevance: 45.0

TL;DR: 本文为线性自编码器（LAEs）在推荐系统中的泛化性能提供了理论分析，提出了多元线性回归的PAC-Bayes界，并将其应用于LAEs，建立了理论与实际性能的相关性。


<details>
  <summary>Details</summary>
Motivation: 线性自编码器在推荐系统中表现出色，但其成功主要基于经验观察，缺乏理论理解。作者旨在填补这一空白，为LAEs的泛化性能提供理论分析框架。

Method: 1) 扩展单输出线性回归的PAC-Bayes界到多元线性回归；2) 将LAEs解释为有界数据上的约束多元线性回归模型；3) 开发优化LAE界的计算高效方法；4) 在大型模型和真实数据集上进行实验验证。

Result: 提出的PAC-Bayes界紧致且与Recall@K、NDCG@K等实际排序指标高度相关，验证了理论分析的有效性。

Conclusion: 为LAEs在推荐系统中的性能提供了理论解释，建立了泛化理论与实际应用之间的桥梁，有助于理解模型性能的理论基础。

Abstract: Linear Autoencoders (LAEs) have shown strong performance in state-of-the-art recommender systems. However, this success remains largely empirical, with limited theoretical understanding. In this paper, we investigate the generalizability -- a theoretical measure of model performance in statistical learning -- of multivariate linear regression and LAEs. We first propose a PAC-Bayes bound for multivariate linear regression, extending the earlier bound for single-output linear regression by Shalaeva et al., and establish sufficient conditions for its convergence. We then show that LAEs, when evaluated under a relaxed mean squared error, can be interpreted as constrained multivariate linear regression models on bounded data, to which our bound adapts. Furthermore, we develop theoretical methods to improve the computational efficiency of optimizing the LAE bound, enabling its practical evaluation on large models and real-world datasets. Experimental results demonstrate that our bound is tight and correlates well with practical ranking metrics such as Recall@K and NDCG@K.

</details>


### [507] [SigTime: Learning and Visually Explaining Time Series Signatures](https://arxiv.org/abs/2512.12076)
*Yu-Chia Huang,Juntong Chen,Dongyu Liu,Kwan-Liu Ma*

Main category: cs.LG

Relevance: 40.0

TL;DR: 提出一个结合shapelet表示和传统特征工程的Transformer学习框架，用于时间序列模式发现，并开发了SigTime可视化分析系统


<details>
  <summary>Details</summary>
Motivation: 时间序列数据中的时间模式识别对科学发现和决策至关重要（如生物医学信号分析），但现有方法存在计算复杂度高、可解释性差、难以捕捉有意义时间结构等问题

Method: 联合训练两个Transformer模型：一个使用shapelet-based表示捕捉局部时间结构，另一个使用传统特征工程编码统计特性；并开发SigTime可视化分析系统

Result: 在8个公开数据集和1个临床专有数据集上定量评估，通过ECG数据和早产分析两个实际场景验证了框架和系统的有效性

Conclusion: 提出的学习框架能有效发现时间序列中的可解释模式，SigTime系统支持多视角探索，有助于领域专家生成有用洞察

Abstract: Understanding and distinguishing temporal patterns in time series data is essential for scientific discovery and decision-making. For example, in biomedical research, uncovering meaningful patterns in physiological signals can improve diagnosis, risk assessment, and patient outcomes. However, existing methods for time series pattern discovery face major challenges, including high computational complexity, limited interpretability, and difficulty in capturing meaningful temporal structures. To address these gaps, we introduce a novel learning framework that jointly trains two Transformer models using complementary time series representations: shapelet-based representations to capture localized temporal structures and traditional feature engineering to encode statistical properties. The learned shapelets serve as interpretable signatures that differentiate time series across classification labels. Additionally, we develop a visual analytics system -- SigTIme -- with coordinated views to facilitate exploration of time series signatures from multiple perspectives, aiding in useful insights generation. We quantitatively evaluate our learning framework on eight publicly available datasets and one proprietary clinical dataset. Additionally, we demonstrate the effectiveness of our system through two usage scenarios along with the domain experts: one involving public ECG data and the other focused on preterm labor analysis.

</details>


### [508] [Neural CDEs as Correctors for Learned Time Series Models](https://arxiv.org/abs/2512.12116)
*Muhammad Bilal Shahid,Prajwal Koirla,Cody Fleming*

Main category: cs.LG

Relevance: 40.0

TL;DR: 提出Predictor-Corrector机制，其中Predictor是任意学习的时间序列模型，Corrector是神经控制微分方程，用于预测预测误差并提升预测性能。


<details>
  <summary>Details</summary>
Motivation: 现有的时间序列模型（无论是连续时间还是离散时间）在进行多步预测时容易产生误差。直接预测整个时间范围或迭代反馈自身预测的方法都存在误差累积问题，需要一种机制来纠正这些预测误差。

Method: 提出Predictor-Corrector机制：Predictor可以是任何学习的时间序列模型，Corrector是神经控制微分方程（neural controlled differential equation）。Predictor进行预测，Corrector预测预测误差，将这些误差加到预测结果中以提升性能。Corrector支持不规则采样时间序列和连续/离散时间Predictor。还引入了两种正则化策略来提升Corrector的外推性能并加速训练。

Result: 在合成数据、物理仿真和真实世界预测数据集上评估了Corrector与多种Predictor（如神经常微分方程、Contiformer、DLinear）的组合。实验表明，Predictor-Corrector机制相比单独使用Predictor能持续提升性能。

Conclusion: 提出的Predictor-Corrector机制能有效纠正时间序列模型的预测误差，提升预测性能，适用于各种类型的时间序列模型和采样模式。

Abstract: Learned time-series models, whether continuous- or discrete-time, are widely used to forecast the states of a dynamical system. Such models generate multi-step forecasts either directly, by predicting the full horizon at once, or iteratively, by feeding back their own predictions at each step. In both cases, the multi-step forecasts are prone to errors. To address this, we propose a Predictor-Corrector mechanism where the Predictor is any learned time-series model and the Corrector is a neural controlled differential equation. The Predictor forecasts, and the Corrector predicts the errors of the forecasts. Adding these errors to the forecasts improves forecast performance. The proposed Corrector works with irregularly sampled time series and continuous- and discrete-time Predictors. Additionally, we introduce two regularization strategies to improve the extrapolation performance of the Corrector with accelerated training. We evaluate our Corrector with diverse Predictors, e.g., neural ordinary differential equations, Contiformer, and DLinear, on synthetic, physics simulation, and real-world forecasting datasets. The experiments demonstrate that the Predictor-Corrector mechanism consistently improves the performance compared to Predictor alone.

</details>


### [509] [GoMS: Graph of Molecule Substructure Network for Molecule Property Prediction](https://arxiv.org/abs/2512.12489)
*Shuhui Qu,Cheolwoo Park*

Main category: cs.LG

Relevance: 40.0

TL;DR: GoMS是一种新颖的分子图神经网络架构，通过构建子结构图来显式建模分子子结构之间的相互作用和空间排列关系，相比现有的ESAN等袋式表示方法，能更好地捕捉子结构间的拓扑连接信息。


<details>
  <summary>Details</summary>
Motivation: 现有方法如ESAN将分子视为独立子结构的集合，忽略了子结构之间的关键关系。分子性质往往源于多个功能单元之间的相互作用，因此需要显式建模子结构间的空间排列和连接关系。

Method: GoMS构建了一个图结构，其中节点代表分子子图，边捕捉子结构之间的结构关系。这种方法保留了子结构如何连接和重叠的关键拓扑信息，不同于ESAN的袋式表示。

Result: 在公开分子数据集上的实验表明，GoMS优于ESAN和其他基线方法，特别是对于含有100个以上原子的大分子，性能差距随分子尺寸增大而扩大。理论分析显示GoMS能区分具有相同子图组成但不同空间排列的分子。

Conclusion: GoMS通过捕捉袋式方法丢失的子结构关系，代表了面向真实世界应用的可扩展和可解释分子性质预测的重要进展，特别适用于涉及复杂分子的材料科学应用。

Abstract: While graph neural networks have shown remarkable success in molecular property prediction, current approaches like the Equivariant Subgraph Aggregation Networks (ESAN) treat molecules as bags of independent substructures, overlooking crucial relationships between these components. We present Graph of Molecule Substructures (GoMS), a novel architecture that explicitly models the interactions and spatial arrangements between molecular substructures. Unlike ESAN's bag-based representation, GoMS constructs a graph where nodes represent subgraphs and edges capture their structural relationships, preserving critical topological information about how substructures are connected and overlap within the molecule. Through extensive experiments on public molecular datasets, we demonstrate that GoMS outperforms ESAN and other baseline methods, with particularly improvements for large molecules containing more than 100 atoms. The performance gap widens as molecular size increases, demonstrating GoMS's effectiveness for modeling industrial-scale molecules. Our theoretical analysis demonstrates that GoMS can distinguish molecules with identical subgraph compositions but different spatial arrangements. Our approach shows particular promise for materials science applications involving complex molecules where properties emerge from the interplay between multiple functional units. By capturing substructure relationships that are lost in bag-based approaches, GoMS represents a significant advance toward scalable and interpretable molecular property prediction for real-world applications.

</details>


### [510] [PRIVEE: Privacy-Preserving Vertical Federated Learning Against Feature Inference Attacks](https://arxiv.org/abs/2512.12840)
*Sindhuja Madabushi,Ahmad Faraz Khan,Haider Ali,Ananthram Swami,Rui Ning,Hongyi Wu,Jin-Hee Cho*

Main category: cs.LG

Relevance: 40.0

TL;DR: PRIVEE是一种保护垂直联邦学习中隐私的防御机制，通过混淆置信度分数来防止特征推断攻击，同时保持模型预测准确性。


<details>
  <summary>Details</summary>
Motivation: 垂直联邦学习(VFL)允许组织在共享用户样本但特征空间不重叠的情况下协作训练模型。然而，VFL容易受到特征推断攻击，攻击者可以利用推理过程中共享的置信度分数（预测概率）来重构其他参与者的私有输入特征。现有防御方法在保护隐私的同时往往会影响模型性能。

Method: PRIVEE通过混淆置信度分数来保护隐私，同时保持相对排名和分数间距离等关键属性。该方法不暴露原始分数，只共享转换后的表示，从而在不降低模型预测准确性的情况下减轻重构攻击风险。

Result: 大量实验表明，PRIVEE在隐私保护方面比最先进的防御方法提高了三倍，同时在面对高级特征推断攻击时保持了完整的预测性能。

Conclusion: PRIVEE为垂直联邦学习提供了一种有效的隐私保护解决方案，能够在保持模型性能的同时显著增强对特征推断攻击的防御能力。

Abstract: Vertical Federated Learning (VFL) enables collaborative model training across organizations that share common user samples but hold disjoint feature spaces. Despite its potential, VFL is susceptible to feature inference attacks, in which adversarial parties exploit shared confidence scores (i.e., prediction probabilities) during inference to reconstruct private input features of other participants. To counter this threat, we propose PRIVEE (PRIvacy-preserving Vertical fEderated lEarning), a novel defense mechanism named after the French word privée, meaning "private." PRIVEE obfuscates confidence scores while preserving critical properties such as relative ranking and inter-score distances. Rather than exposing raw scores, PRIVEE shares only the transformed representations, mitigating the risk of reconstruction attacks without degrading model prediction accuracy. Extensive experiments show that PRIVEE achieves a threefold improvement in privacy protection compared to state-of-the-art defenses, while preserving full predictive performance against advanced feature inference attacks.

</details>


### [511] [Robust Variational Bayes by Min-Max Median Aggregation](https://arxiv.org/abs/2512.12676)
*Jiawei Yan,Ju Liu,Weidong Liu,Jiyuan Tu*

Main category: stat.ME

Relevance: 40.0

TL;DR: 提出了一种鲁棒且可扩展的变分贝叶斯框架，用于处理数据集中的污染和异常值，通过数据分区和稳健聚合原则，采用min-max中位数公式增强鲁棒性，并针对局部隐变量的存在分别处理两种场景。


<details>
  <summary>Details</summary>
Motivation: 现实世界数据集中经常存在污染和异常值，传统变分贝叶斯方法对这些情况不够鲁棒。需要开发能够有效处理数据污染、同时保持可扩展性的稳健变分推理框架。

Method: 1) 将数据划分为m个不相交子集；2) 基于稳健聚合原则构建联合优化问题；3) 用min-max中位数公式替代平均KL散度增强鲁棒性；4) 针对有无局部隐变量的两种情况分别处理：无局部隐变量时直接聚合，有局部隐变量时采用聚合-重缩放策略；5) 提供非渐近理论分析，包括改进的Bernstein-von Mises定理。

Result: 1) 两阶段方法比直接聚合m次幂局部后验产生更小的近似误差；2) 为所提后验均值建立了接近最优的统计速率；3) 改进了现有min-max中位数估计器的理论；4) 通过大量模拟研究验证了方法的有效性。

Conclusion: 提出的鲁棒变分贝叶斯框架能有效处理数据污染和异常值，在理论和实证上都表现出色，为大规模数据集中的稳健贝叶斯推理提供了实用解决方案。

Abstract: We propose a robust and scalable variational Bayes (VB) framework designed to effectively handle contamination and outliers in dataset. Our approach partitions the data into $m$ disjoint subsets and formulates a joint optimization problem based on robust aggregation principles. A key insight is that the full posterior distribution is equivalent to the minimizer of the mean Kullback-Leibler (KL) divergence from the $m$-powered local posterior distributions. To enhance robustness, we replace the mean KL divergence with a min-max median formulation. The min-max formulation not only ensures consistency between the KL minimizer and the Evidence Lower Bound (ELBO) maximizer but also facilitates the establishment of improved statistical rates for the mean of variational posterior. We observe a notable discrepancy in the $m$-powered marginal log likelihood function contingent on the presence of local latent variables. To address this, we treat these two scenarios separately to guarantee the consistency of the aggregated variational posterior. Specifically, when local latent variables are present, we introduce an aggregate-and-rescale strategy. Theoretically, we provide a non-asymptotic analysis of our proposed posterior, incorporating a refined analysis of Bernstein-von Mises (BvM) theorem to accommodate a diverging number of subsets $m$. Our findings indicate that the two-stage approach yields a smaller approximation error compared to directly aggregating the $m$-powered local posteriors. Furthermore, we establish a nearly optimal statistical rate for the mean of the proposed posterior, advancing existing theories related to min-max median estimators. The efficacy of our method is demonstrated through extensive simulation studies.

</details>


### [512] [Active Inference with Reusable State-Dependent Value Profiles](https://arxiv.org/abs/2512.11829)
*Jacob Poschl*

Main category: cs.LG

Relevance: 35.0

TL;DR: 论文提出"价值档案"框架，通过少量可复用的价值相关参数包（偏好、策略先验、策略精度）在生成模型中分配给隐藏状态，通过信念加权混合实现状态条件策略调用，为多变环境中的信念条件价值控制提供可计算解释。


<details>
  <summary>Details</summary>
Motivation: 在多变环境中，智能体需要在潜在上下文之间切换价值控制机制，但为每种情况维护独立的偏好、策略偏差和行动置信度参数是不可行的。需要一种更高效的方法来实现状态条件策略调用。

Method: 引入价值档案：少量可复用的价值相关参数包（结果偏好、策略先验、策略精度），分配给生成模型中的隐藏状态。通过信念加权混合产生有效控制参数，实现状态条件策略调用。在概率反转学习任务中评估，比较静态精度、熵耦合动态精度和基于档案的模型。

Result: 模型比较显示基于档案的模型优于简单替代方案（约100点AIC差异），参数恢复分析支持结构可识别性。模型推断表明自适应控制主要由策略先验调制驱动，而非策略精度，具有渐进的信念依赖档案调用模式。

Conclusion: 可复用的价值档案为多变环境中的信念条件价值控制提供了可计算解释，并产生了信念依赖控制和行为灵活性的可测试特征。

Abstract: Adaptive behavior in volatile environments requires agents to switch among value-control regimes across latent contexts, but maintaining separate preferences, policy biases, and action-confidence parameters for every situation is intractable. We introduce value profiles: a small set of reusable bundles of value-related parameters (outcome preferences, policy priors, and policy precision) assigned to hidden states in a generative model. As posterior beliefs over states evolve trial by trial, effective control parameters arise via belief-weighted mixing, enabling state-conditional strategy recruitment without requiring independent parameters for each context. We evaluate this framework in probabilistic reversal learning, comparing static-precision, entropy-coupled dynamic-precision, and profile-based models using cross-validated log-likelihood and information criteria. Model comparison favors the profile-based model over simpler alternatives (about 100-point AIC differences), and parameter-recovery analyses support structural identifiability even when context must be inferred from noisy observations. Model-based inference further suggests that adaptive control in this task is driven primarily by modulation of policy priors rather than policy precision, with gradual belief-dependent profile recruitment consistent with state-conditional (not purely uncertainty-driven) control. Overall, reusable value profiles provide a tractable computational account of belief-conditioned value control in volatile environments and yield testable signatures of belief-dependent control and behavioral flexibility.

</details>


### [513] [On the Design of One-step Diffusion via Shortcutting Flow Paths](https://arxiv.org/abs/2512.11831)
*Haitao Lin,Peiyan Hu,Minsi Ren,Zhifeng Gao,Zhi-Ming Ma,Guolin ke,Tailin Wu,Stan Z. Li*

Main category: cs.LG

Relevance: 35.0

TL;DR: 本文提出了一种用于代表性捷径模型的通用设计框架，该框架提供了理论依据并解耦了组件级选择，从而实现了系统性的改进识别。改进后的一步模型在ImageNet-256x256上达到了2.85的SOTA FID50k分数。


<details>
  <summary>Details</summary>
Motivation: 当前few-step扩散模型的理论推导和实际实现紧密耦合，这模糊了设计空间。需要一种通用框架来提供理论依据并解耦组件级选择，从而促进系统性的改进。

Method: 提出了一个用于代表性捷径模型的通用设计框架，该框架提供理论验证并解耦具体的组件级选择。通过该框架识别改进点，构建无需预训练、蒸馏或课程学习的一步模型。

Result: 改进后的一步模型在ImageNet-256x256上达到了2.85的SOTA FID50k分数（在classifier-free guidance设置下）。模型无需预训练、蒸馏或课程学习。

Conclusion: 该工作降低了捷径模型中组件级创新的门槛，并促进了其设计空间的原则性探索。提出的框架为捷径模型提供了理论依据和系统化的改进方法。

Abstract: Recent advances in few-step diffusion models have demonstrated their efficiency and effectiveness by shortcutting the probabilistic paths of diffusion models, especially in training one-step diffusion models from scratch (a.k.a. shortcut models). However, their theoretical derivation and practical implementation are often closely coupled, which obscures the design space. To address this, we propose a common design framework for representative shortcut models. This framework provides theoretical justification for their validity and disentangles concrete component-level choices, thereby enabling systematic identification of improvements. With our proposed improvements, the resulting one-step model achieves a new state-of-the-art FID50k of 2.85 on ImageNet-256x256 under the classifier-free guidance setting. Remarkably, the model requires no pre-training, distillation, or curriculum learning. We believe our work lowers the barrier to component-level innovation in shortcut models and facilitates principled exploration of their design space.

</details>


### [514] [Amortized Causal Discovery with Prior-Fitted Networks](https://arxiv.org/abs/2512.11840)
*Mateusz Sypniewski,Mateusz Olko,Mateusz Gajewski,Piotr Miłoś*

Main category: cs.LG

Relevance: 35.0

TL;DR: 提出基于先验拟合网络（PFNs）的摊销因果发现方法，解决传统可微分惩罚似然方法中似然估计误差导致结构发现不准的问题，通过摊销数据依赖的似然估计获得更可靠的结构学习评分。


<details>
  <summary>Details</summary>
Motivation: 传统可微分惩罚似然方法通过最大化数据似然来优化因果结构，但研究发现即使在大样本量下，似然估计误差也会阻碍发现正确结构。需要解决似然估计器准确性的限制。

Method: 使用先验拟合网络（PFNs）来摊销数据依赖的似然估计，生成更可靠的结构学习评分。PFNs能够比传统神经网络方法提供更准确的似然估计。

Result: 在合成、模拟和真实世界数据集上的实验显示，相比标准基线方法，该方法在结构恢复方面取得显著提升。直接证明PFNs比传统神经网络方法提供更准确的似然估计。

Conclusion: 基于PFNs的摊销因果发现方法能够有效解决似然估计准确性问题，提高因果结构发现的可靠性，为因果发现领域提供了新的技术路径。

Abstract: In recent years, differentiable penalized likelihood methods have gained popularity, optimizing the causal structure by maximizing its likelihood with respect to the data. However, recent research has shown that errors in likelihood estimation, even on relatively large sample sizes, disallow the discovery of proper structures. We propose a new approach to amortized causal discovery that addresses the limitations of likelihood estimator accuracy. Our method leverages Prior-Fitted Networks (PFNs) to amortize data-dependent likelihood estimation, yielding more reliable scores for structure learning. Experiments on synthetic, simulated, and real-world datasets show significant gains in structure recovery compared to standard baselines. Furthermore, we demonstrate directly that PFNs provide more accurate likelihood estimates than conventional neural network-based approaches.

</details>


### [515] [Meta-Continual Mobility Forecasting for Proactive Handover Prediction](https://arxiv.org/abs/2512.11841)
*Sasi Vardhan Reddy Mandapati*

Main category: cs.LG

Relevance: 35.0

TL;DR: 提出轻量级元持续预测框架，用于蜂窝网络中的短期移动性预测，通过元初始化、残差检测和在线更新实现非平稳移动环境下的快速适应，显著提升切换预测性能。


<details>
  <summary>Details</summary>
Motivation: 蜂窝网络中的短期移动性预测对于主动切换至关重要，但现实移动具有高度非平稳性（急转弯、速度突变、不可预测行为），导致传统预测器漂移，引发切换失败或时机不当。

Method: 提出轻量级元持续预测框架：1) GRU基础预测器；2) Reptile元初始化实现快速少样本适应；3) EWMA残差检测器在漂移发生时触发紧凑在线更新。

Result: 在GeoLife和DeepMIMO数据集上：零样本设置下ADE 4.46m，FDE 7.79m；10样本下ADE提升至3.71m；从突发漂移中恢复速度比离线GRU快2-3倍；切换预测F1达0.83，AUROC达0.90，显著减少切换失败和乒乓事件。

Conclusion: 该框架有效解决了移动预测中的非平稳性问题，实现了快速适应和漂移恢复，模型轻量（128k参数）适合5G/6G边缘部署，显著提升了切换预测性能。

Abstract: Short-term mobility forecasting is a core requirement for proactive handover (HO) in cellular networks. Real-world mobility is highly non-stationary: abrupt turns, rapid speed changes, and unpredictable user behavior cause conventional predictors to drift, leading to mistimed or failed handovers. We propose a lightweight meta-continual forecasting framework that integrates a GRU-based predictor, Reptile meta-initialization for fast few-shot adaptation, and an EWMA residual detector that triggers compact online updates only when drift occurs. Evaluated on a reproducible GeoLife and DeepMIMO pipeline, our method achieves 4.46 m ADE and 7.79 m FDE in zero-shot settings, improves few-shot ADE to 3.71 m at 10-shot, and enables recovery from abrupt drift about 2 to 3 times faster than an offline GRU. When applied to downstream HO prediction, the approach improves F1 to 0.83 and AUROC to 0.90, with substantial reductions in missed-HO and ping-pong events. The model is lightweight (128k parameters) and suitable for edge deployment in 5G and 6G systems.

</details>


### [516] [Airport Passenger Flow Forecasting via Deformable Temporal-Spectral Transformer Approach](https://arxiv.org/abs/2512.11845)
*Wenbo Du,Lingling Han,Ying Xiong,Ling Zhang,Biyue Li,Yisheng Lv,Tong Guo*

Main category: cs.LG

Relevance: 35.0

TL;DR: 提出DTSFormer，一种用于机场客流预测的可变形时频Transformer，通过多尺度可变形分区和联合时频滤波模块，动态建模客流异质模式，优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 机场客流预测对运营效率至关重要，现有基于固定大小patch的Transformer方法难以建模客流复杂异质模式，需要更灵活的多尺度建模方法。

Method: 提出DTSFormer：1）多尺度可变形分区模块，通过窗口函数掩码动态划分多尺度时间patch；2）频域注意力机制捕获高低频分量；3）时域特征融合建模短期波动和长期趋势。

Result: 在北京首都国际机场2023年1月至2024年3月真实数据上，DTSFormer在不同预测时间范围内均优于最先进预测模型，可变形分区模块能更好捕捉突发高频波动。

Conclusion: DTSFormer通过可变形时频建模有效解决了机场客流预测中的异质模式问题，为时间序列预测提供了新的Transformer架构设计思路。

Abstract: Accurate forecasting of passenger flows is critical for maintaining the efficiency and resilience of airport operations. Recent advances in patch-based Transformer models have shown strong potential in various time series forecasting tasks. However, most existing methods rely on fixed-size patch embedding, making it difficult to model the complex and heterogeneous patterns of airport passenger flows. To address this issue, this paper proposes a deformable temporal-spectral transformer named DTSFormer that integrates a multiscale deformable partitioning module and a joint temporal-spectral filtering module. Specifically, the input sequence is dynamically partitioned into multiscale temporal patches via a novel window function-based masking, enabling the extraction of heterogeneous trends across different temporal stages. Then, within each scale, a frequency-domain attention mechanism is designed to capture both high- and low-frequency components, thereby emphasizing the volatility and periodicity inherent in airport passenger flows. Finally, the resulting multi-frequency features are subsequently fused in the time domain to jointly model short-term fluctuations and long-term trends. Comprehensive experiments are conducted on real-world passenger flow data collected at Beijing Capital International Airport from January 2023 to March 2024. The results indicate that the proposed method consistently outperforms state-of-the-art forecasting models across different prediction horizons. Further analysis shows that the deformable partitioning module aligns patch lengths with dominant periods and heterogeneous trends, enabling superior capture of sudden high-frequency fluctuations.

</details>


### [517] [Exploring Topological Bias in Heterogeneous Graph Neural Networks](https://arxiv.org/abs/2512.11846)
*Yihan Zhang*

Main category: cs.LG

Relevance: 35.0

TL;DR: 该论文研究了异质图神经网络中的拓扑偏见问题，提出了一种基于元权重和PageRank的投影方法来检测偏见，并设计了基于对比学习的去偏结构来提升模型性能。


<details>
  <summary>Details</summary>
Motivation: 现有研究主要关注同质图神经网络的拓扑偏见问题，而对异质图神经网络中的偏见研究不足。作者发现异质图神经网络同样存在与拓扑结构相关的性能偏见，这限制了模型的整体性能。

Method: 1. 对异质图的邻接矩阵应用元权重来区分不同的元关系；2. 基于修改后的邻接矩阵，结合PageRank和节点标签信息构建投影映射；3. 利用节点映射值的差异设计去偏结构，并与原始图结构进行对比学习。

Result: 在三个公开数据集上的实验验证了所提方法的有效性：1. 证明了异质图神经网络中拓扑偏见的普遍存在；2. 提出的去偏方法显著提升了异质图神经网络的性能；3. 成功减少了模型中的偏见。

Conclusion: 异质图神经网络确实存在拓扑偏见，通过元权重调整和对比学习可以有效检测和减少这种偏见，从而提升模型性能。这为图神经网络的可信AI研究提供了新思路。

Abstract: Graph Neural Networks (GNNs) are characterized by their capacity of processing graph-structured data. However, due to the sparsity of labels under semi-supervised learning, they have been found to exhibit biased performance on specific nodes. This kind of bias has been validated to correlate with topological structure and is considered as a bottleneck of GNNs' performance. Existing work focuses on the study of homogeneous GNNs and little attention has been given to topological bias in Heterogeneous Graph Neural Networks (HGNNs). In this work, firstly, in order to distinguish distinct meta relations, we apply meta-weighting to the adjacency matrix of a heterogeneous graph. Based on the modified adjacency matrix, we leverage PageRank along with the node label information to construct a projection. The constructed projection effectively maps nodes to values that strongly correlated with model performance when using datasets both with and without intra-type connections, which demonstrates the universal existence of topological bias in HGNNs. To handle this bias, we propose a debiasing structure based on the difference in the mapped values of nodes and use it along with the original graph structure for contrastive learning. Experiments on three public datasets verify the effectiveness of the proposed method in improving HGNNs' performance and debiasing.

</details>


### [518] [Explainable AI for Smart Greenhouse Control: Interpretability of Temporal Fusion Transformer in the Internet of Robotic Things](https://arxiv.org/abs/2512.11852)
*Muhammad Jawad Bashir,Shagufta Henna,Eoghan Furey*

Main category: cs.LG

Relevance: 35.0

TL;DR: 该研究利用Temporal Fusion Transformer模型进行智能温室执行器控制，并采用LIME和SHAP等解释性方法增强模型决策透明度，在类别不平衡数据集上达到95%测试准确率。


<details>
  <summary>Details</summary>
Motivation: 智能温室中的时间序列预测模型通常是黑盒，缺乏可解释性，这在需要信任、透明度和监管合规的智慧农业实践中是一个关键限制。研究旨在通过可解释AI方法增强温室自动化控制的透明度和可信度。

Method: 使用Temporal Fusion Transformer模型进行温室执行器设置自动化，结合模型内在解释、LIME和SHAP方法进行局部和全局解释，分析温度、湿度、CO2、光照等传感器数据对执行器控制决策的贡献。

Result: 在类别不平衡的自动化温室数据集上，TFT模型测试准确率达到95%。解释性分析揭示了不同传感器对实时温室调整的差异化影响，为透明决策和自适应微调提供了依据。

Conclusion: TFT模型结合可解释AI方法能够有效实现智能温室自动化控制，提高决策透明度，支持自适应优化以改善作物产量和资源效率，满足智慧农业对可信AI的需求。

Abstract: The integration of the Internet of Robotic Things (IoRT) in smart greenhouses has revolutionised precision agriculture by enabling efficient and autonomous environmental control. However, existing time series forecasting models in such setups often operate as black boxes, lacking mechanisms for explainable decision-making, which is a critical limitation when trust, transparency, and regulatory compliance are paramount in smart farming practices. This study leverages the Temporal Fusion Transformer (TFT) model to automate actuator settings for optimal greenhouse management. To enhance interpretability and trust in the model decision-making process, both local and global explanation techniques were employed using model-inherent interpretation, local interpretable model-agnostic explanations (LIME), and SHapley additive explanations (SHAP). These explainability methods provide information on how different sensor readings, such as temperature, humidity, CO2 levels, light, and outer climate, contribute to actuator control decisions in an automated greenhouse. The trained TFT model achieved a test accuracy of 95% on a class-imbalanced dataset for actuator control settings in an automated greenhouse environment. The results demonstrate the varying influence of each sensor on real-time greenhouse adjustments, ensuring transparency and enabling adaptive fine-tuning for improved crop yield and resource efficiency.

</details>


### [519] [Achieving Approximate Symmetry Is Exponentially Easier than Exact Symmetry](https://arxiv.org/abs/2512.11855)
*Behrooz Tahmasebi,Melanie Weber*

Main category: cs.LG

Relevance: 35.0

TL;DR: 本文首次从理论上比较了精确对称性与近似对称性的成本，提出了平均复杂度框架，证明了指数级分离：精确对称需要线性平均复杂度，而近似对称只需对数复杂度。


<details>
  <summary>Details</summary>
Motivation: 机器学习中强制精确对称性在科学应用中能带来显著收益，但近期研究表明近似对称性可能提供更大灵活性和鲁棒性。然而，这两种方法的理论理解不足，特别是缺乏直接比较，需要量化精确对称与近似对称的成本差异。

Method: 引入了"平均复杂度"框架来量化通过平均实现对称性的成本。在标准条件下分析两种对称性方法的复杂度要求，建立理论分离结果。

Result: 主要结果是指数级分离：实现精确对称性需要线性平均复杂度，而实现近似对称性只需对数平均复杂度。这是首次从理论上分离这两种情况，为实践中近似对称性的优势提供了形式化证明。

Conclusion: 近似对称性在实践中的优势得到了理论支持，因为其实现成本远低于精确对称性。提出的工具和技术对机器学习中对称性的更广泛研究具有独立价值。

Abstract: Enforcing exact symmetry in machine learning models often yields significant gains in scientific applications, serving as a powerful inductive bias. However, recent work suggests that relying on approximate symmetry can offer greater flexibility and robustness. Despite promising empirical evidence, there has been little theoretical understanding, and in particular, a direct comparison between exact and approximate symmetry is missing from the literature. In this paper, we initiate this study by asking: What is the cost of enforcing exact versus approximate symmetry? To address this question, we introduce averaging complexity, a framework for quantifying the cost of enforcing symmetry via averaging. Our main result is an exponential separation: under standard conditions, achieving exact symmetry requires linear averaging complexity, whereas approximate symmetry can be attained with only logarithmic averaging complexity. To the best of our knowledge, this provides the first theoretical separation of these two cases, formally justifying why approximate symmetry may be preferable in practice. Beyond this, our tools and techniques may be of independent interest for the broader study of symmetries in machine learning.

</details>


### [520] [GCoDE: Efficient Device-Edge Co-Inference for GNNs via Architecture-Mapping Co-Search](https://arxiv.org/abs/2512.11856)
*Ao Zhou,Jianlei Yang,Tong Qiao,Yingjie Qi,Zhi Yang,Weisheng Zhao,Chunming Hu*

Main category: cs.LG

Relevance: 35.0

TL;DR: GCoDE：首个面向图神经网络设备-边缘协同推理的自动框架，通过架构-映射协同设计，在异构系统中实现高效部署，相比现有方法最高可加速44.9倍、节能98.2%


<details>
  <summary>Details</summary>
Motivation: GNN在边缘设备上的推理面临计算成本高、硬件资源有限的挑战，难以满足实时性和能耗要求。设备-边缘协同推理虽具潜力，但现有研究稀缺，传统模型划分方法对GNN无效。

Method: 提出GCoDE框架：1) 将设备通信抽象为显式操作，统一架构与映射方案的联合优化空间；2) 系统性能感知，评估异构系统架构效率；3) 分析GNN操作能耗，提出能耗预测方法；4) 基于约束的随机搜索策略寻找最优解；5) 集成协同推理引擎。

Result: 实验表明，GCoDE在不同应用和系统配置下，相比现有方法最高可实现44.9倍加速和98.2%能耗降低，且能在1.5小时内找到准确性与效率平衡的最优解。

Conclusion: GCoDE是首个针对GNN设备-边缘协同推理的自动框架，通过架构-映射协同设计解决了边缘部署的效率瓶颈，为GNN在边缘计算场景的实际应用提供了有效解决方案。

Abstract: Graph Neural Networks (GNNs) have emerged as the state-of-the-art graph learning method. However, achieving efficient GNN inference on edge devices poses significant challenges, limiting their application in real-world edge scenarios. This is due to the high computational cost of GNNs and limited hardware resources on edge devices, which prevent GNN inference from meeting real-time and energy requirements. As an emerging paradigm, device-edge co-inference shows potential for improving inference efficiency and reducing energy consumption on edge devices. Despite its potential, research on GNN device-edge co-inference remains scarce, and our findings show that traditional model partitioning methods are ineffective for GNNs. To address this, we propose GCoDE, the first automatic framework for GNN architecture-mapping Co-design and deployment on Device-Edge hierarchies. By abstracting the device communication process into an explicit operation, GCoDE fuses the architecture and mapping scheme in a unified design space for joint optimization. Additionally, GCoDE's system performance awareness enables effective evaluation of architecture efficiency across diverse heterogeneous systems. By analyzing the energy consumption of various GNN operations, GCoDE introduces an energy prediction method that improves energy assessment accuracy and identifies energy-efficient solutions. Using a constraint-based random search strategy, GCoDE identifies the optimal solution in 1.5 hours, balancing accuracy and efficiency. Moreover, the integrated co-inference engine in GCoDE enables efficient deployment and execution of GNN co-inference. Experimental results show that GCoDE can achieve up to 44.9x speedup and 98.2% energy reduction compared to existing approaches across diverse applications and system configurations.

</details>


### [521] [Generative Stochastic Optimal Transport: Guided Harmonic Path-Integral Diffusion](https://arxiv.org/abs/2512.11859)
*Michael Chertkov*

Main category: cs.LG

Relevance: 35.0

TL;DR: GH-PID是一种线性可解的引导随机最优传输框架，具有硬终端分布和软路径成本，通过低维引导协议保持解析结构，实现稳定采样和可微协议学习。


<details>
  <summary>Details</summary>
Motivation: 本文旨在解决随机最优传输中硬终端分布约束下的轨迹优化问题，同时考虑应用驱动的路径成本。传统方法难以在保持解析可解性的同时实现精确终端匹配和可微学习。

Method: 提出GH-PID框架：1) 使用低维引导协议塑造轨迹集合；2) 保持前向和后向Kolmogorov方程的线性性；3) 最优得分函数具有显式Green函数比率形式；4) 高斯混合模型终端分布产生闭式表达式；5) 开发引导中心诊断指标。

Result: 在三个2D导航场景中验证：A) 手工协议展示几何和刚度如何影响滞后、曲率效应和模态演化；B) 单任务协议学习优化分段常数中心线以最小化积分成本；C) 多专家融合通过精确专家乘积法则学习共识协议。所有设置均生成几何感知、信任感知的轨迹。

Conclusion: GH-PID为经验随机最优传输提供了一个可解释的变分近似框架，能够精确匹配终端分布并系统降低积分成本，在保持解析结构的同时实现稳定采样和可微协议学习。

Abstract: We introduce Guided Harmonic Path-Integral Diffusion (GH-PID), a linearly-solvable framework for guided Stochastic Optimal Transport (SOT) with a hard terminal distribution and soft, application-driven path costs. A low-dimensional guidance protocol shapes the trajectory ensemble while preserving analytic structure: the forward and backward Kolmogorov equations remain linear, the optimal score admits an explicit Green-function ratio, and Gaussian-Mixture Model (GMM) terminal laws yield closed-form expressions. This enables stable sampling and differentiable protocol learning under exact terminal matching.
  We develop guidance-centric diagnostics -- path cost, centerline adherence, variance flow, and drift effort -- that make GH-PID an interpretable variational ansatz for empirical SOT. Three navigation scenarios illustrated in 2D: (i) Case A: hand-crafted protocols revealing how geometry and stiffness shape lag, curvature effects, and mode evolution; (ii) Case B: single-task protocol learning, where a PWC centerline is optimized to minimize integrated cost; (iii) Case C: multi-expert fusion, in which a commander reconciles competing expert/teacher trajectories and terminal beliefs through an exact product-of-experts law and learns a consensus protocol. Across all settings, GH-PID generates geometry-aware, trust-aware trajectories that satisfy the prescribed terminal distribution while systematically reducing integrated cost.

</details>


### [522] [An Operator-Consistent Graph Neural Network for Learning Diffusion Dynamics on Irregular Meshes](https://arxiv.org/abs/2512.11860)
*Yuelian Li,Andrew Rushing Hands*

Main category: cs.LG

Relevance: 35.0

TL;DR: 提出OCGNN-PINN方法，通过图神经网络结合物理信息约束，在非规则网格上稳定求解偏微分方程，特别适用于多物理场相互作用场景。


<details>
  <summary>Details</summary>
Motivation: 传统数值方法在规则网格上高效求解PDE，但在不规则网格上容易不稳定。实际应用中，扩散、损伤、愈合等多物理场相互作用常发生在不规则网格上，需要开发能在不规则网格上稳定求解PDE的方法。

Method: 开发了算子一致图神经网络(OCGNN-PINN)，通过节点-边消息传递机制，结合物理信息约束。使用一致性损失通过图关联矩阵强制梯度-散度关系，确保离散节点和边动力学在时间演化中保持结构耦合。

Result: 在物理驱动的演化网格和真实世界扫描表面上评估扩散过程。结果显示相比图卷积和多层感知器基线，具有更好的时间稳定性和预测准确性，接近Crank-Nicolson求解器在非结构化域上的性能。

Conclusion: OCGNN-PINN方法能够有效处理不规则网格上的PDE求解问题，为多物理场相互作用提供了一种稳定且准确的数值求解方案。

Abstract: Classical numerical methods solve partial differential equations (PDEs) efficiently on regular meshes, but many of them become unstable on irregular domains. In practice, multiphysics interactions such as diffusion, damage, and healing often take place on irregular meshes. We develop an operator-consistent graph neural network (OCGNN-PINN) that approximates PDE evolution under physics-informed constraints. It couples node-edge message passing with a consistency loss enforcing the gradient-divergence relation through the graph incidence matrix, ensuring that discrete node and edge dynamics remain structurally coupled during temporal rollout. We evaluate the model on diffusion processes over physically driven evolving meshes and real-world scanned surfaces. The results show improved temporal stability and prediction accuracy compared with graph convolutional and multilayer perceptron baselines, approaching the performance of Crank-Nicolson solvers on unstructured domains.

</details>


### [523] [Hierarchical Task Offloading and Trajectory Optimization in Low-Altitude Intelligent Networks Via Auction and Diffusion-based MARL](https://arxiv.org/abs/2512.11862)
*Jiahao You,Ziye Jia,Can Cui,Chao Dong,Qihui Wu,Zhu Han*

Main category: cs.LG

Relevance: 35.0

TL;DR: 提出了一种用于低空智能网络的层次学习框架，结合拍卖机制和多智能体强化学习，优化无人机轨迹规划和任务卸载决策


<details>
  <summary>Details</summary>
Motivation: 低空智能网络（LAINs）在动态和基础设施有限的环境中提供低延迟、节能的边缘智能，但面临能源受限的无人机、随机任务到达和异构计算资源等挑战

Method: 1. 提出集成空地协作网络，建模为时间依赖整数非线性规划问题；2. 设计层次学习框架：大时间尺度使用VCG拍卖机制进行轨迹分配，小时间尺度提出扩散-异构智能体近端策略优化（DHAPPO）算法，将潜在扩散模型嵌入到actor网络中

Result: 仿真实验表明，该框架在能源效率、任务成功率和收敛性能方面优于基线方法

Conclusion: 提出的层次学习框架有效解决了低空智能网络中的轨迹规划和任务卸载联合优化问题，提高了系统性能

Abstract: The low-altitude intelligent networks (LAINs) emerge as a promising architecture for delivering low-latency and energy-efficient edge intelligence in dynamic and infrastructure-limited environments. By integrating unmanned aerial vehicles (UAVs), aerial base stations, and terrestrial base stations, LAINs can support mission-critical applications such as disaster response, environmental monitoring, and real-time sensing. However, these systems face key challenges, including energy-constrained UAVs, stochastic task arrivals, and heterogeneous computing resources. To address these issues, we propose an integrated air-ground collaborative network and formulate a time-dependent integer nonlinear programming problem that jointly optimizes UAV trajectory planning and task offloading decisions. The problem is challenging to solve due to temporal coupling among decision variables. Therefore, we design a hierarchical learning framework with two timescales. At the large timescale, a Vickrey-Clarke-Groves auction mechanism enables the energy-aware and incentive-compatible trajectory assignment. At the small timescale, we propose the diffusion-heterogeneous-agent proximal policy optimization, a generative multi-agent reinforcement learning algorithm that embeds latent diffusion models into actor networks. Each UAV samples actions from a Gaussian prior and refines them via observation-conditioned denoising, enhancing adaptability and policy diversity. Extensive simulations show that our framework outperforms baselines in energy efficiency, task success rate, and convergence performance.

</details>


### [524] [Data-Driven Global Sensitivity Analysis for Engineering Design Based on Individual Conditional Expectations](https://arxiv.org/abs/2512.11946)
*Pramudita Satria Palar,Paul Saves,Rommel G. Regis,Koji Shimoyama,Shigeru Obayashi,Nicolas Verstaevel,Joseph Morlier*

Main category: cs.LG

Relevance: 35.0

TL;DR: 提出基于ICE曲线的全局敏感性度量方法，解决PDP在存在强交互作用时可能产生误导的问题，通过ICE曲线期望和标准差更好地捕捉交互效应。


<details>
  <summary>Details</summary>
Motivation: 在航空航天等工程应用中，理解输入变量对数据驱动模型的影响至关重要。PDP虽然广泛用于解释黑盒模型，但在存在强交互作用时，其全局敏感性度量可能产生误导，因为平均化会掩盖交互效应。

Method: 提出基于个体条件期望（ICE）曲线的全局敏感性度量方法：1）计算ICE曲线的期望特征重要性及其标准差；2）提供数学证明，表明在截断正交多项式展开下，PDP敏感性是ICE度量的下界；3）引入ICE相关性值来量化交互作用如何修改输入与输出之间的关系。

Result: 在三个案例上进行比较评估：5变量解析函数、5变量风力涡轮机疲劳问题、9变量翼型空气动力学案例。ICE敏感性在PDP、SHAP和Sobol指数上的基准测试表明，ICE特征重要性比传统PDP方法提供更丰富的见解，而PDP、ICE和SHAP的可视化解释相互补充，提供多角度视角。

Conclusion: ICE敏感性度量方法能更有效地捕捉交互效应，为工程应用中的模型解释提供更准确的全局敏感性分析工具，特别是在存在强交互作用的复杂系统中。

Abstract: Explainable machine learning techniques have gained increasing attention in engineering applications, especially in aerospace design and analysis, where understanding how input variables influence data-driven models is essential. Partial Dependence Plots (PDPs) are widely used for interpreting black-box models by showing the average effect of an input variable on the prediction. However, their global sensitivity metric can be misleading when strong interactions are present, as averaging tends to obscure interaction effects. To address this limitation, we propose a global sensitivity metric based on Individual Conditional Expectation (ICE) curves. The method computes the expected feature importance across ICE curves, along with their standard deviation, to more effectively capture the influence of interactions. We provide a mathematical proof demonstrating that the PDP-based sensitivity is a lower bound of the proposed ICE-based metric under truncated orthogonal polynomial expansion. In addition, we introduce an ICE-based correlation value to quantify how interactions modify the relationship between inputs and the output. Comparative evaluations were performed on three cases: a 5-variable analytical function, a 5-variable wind-turbine fatigue problem, and a 9-variable airfoil aerodynamics case, where ICE-based sensitivity was benchmarked against PDP, SHapley Additive exPlanations (SHAP), and Sobol' indices. The results show that ICE-based feature importance provides richer insights than the traditional PDP-based approach, while visual interpretations from PDP, ICE, and SHAP complement one another by offering multiple perspectives.

</details>


### [525] [DFedReweighting: A Unified Framework for Objective-Oriented Reweighting in Decentralized Federated Learning](https://arxiv.org/abs/2512.12022)
*Kaichuang Zhang,Wei Yin,Jinghao Yang,Ping Xu*

Main category: cs.LG

Relevance: 35.0

TL;DR: DFedReweighting是一个去中心化联邦学习的统一聚合框架，通过目标导向的权重调整来提升公平性和鲁棒性


<details>
  <summary>Details</summary>
Motivation: 去中心化联邦学习(DFL)避免了中心服务器的依赖，但仍面临公平性、鲁棒性等挑战。现有方法难以同时解决这些问题，需要统一的框架来达成多样化的学习目标。

Method: 提出DFedReweighting框架，在每轮学习的最后一步进行目标导向的权重调整聚合。首先基于目标性能指标计算初步权重，然后通过定制化的重加权策略进行细化，得到最终聚合权重。

Result: 理论分析表明，适当的目标性能指标和重加权策略组合能保证线性收敛。实验结果显示，该框架在多种场景下显著提升了公平性和对拜占庭攻击的鲁棒性。

Conclusion: DFedReweighting是一个灵活的框架，通过选择合适的目标性能指标和重加权策略，可以实现广泛的期望学习目标，包括公平性、鲁棒性等。

Abstract: Decentralized federated learning (DFL) has recently emerged as a promising paradigm that enables multiple clients to collaboratively train machine learning model through iterative rounds of local training, communication, and aggregation without relying on a central server which introduces potential vulnerabilities in conventional Federated Learning. Nevertheless, DFL systems continue to face a range of challenges, including fairness, robustness, etc. To address these challenges, we propose \textbf{DFedReweighting}, a unified aggregation framework designed to achieve diverse objectives in DFL systems via a objective-oriented reweighting aggregation at the final step of each learning round. Specifically, the framework first computes preliminary weights based on \textit{target performance metric} obtained from auxiliary dataset constructed using local data. These weights are then refined using \textit{customized reweighting strategy}, resulting in the final aggregation weights. Our results from the theoretical analysis demonstrate that the appropriate combination of the target performance metric and the customized reweighting strategy ensures linear convergence. Experimental results consistently show that our proposed framework significantly improves fairness and robustness against Byzantine attacks in diverse scenarios. Provided that appropriate target performance metrics and customized reweighting strategy are selected, our framework can achieve a wide range of desired learning objectives.

</details>


### [526] [Goal Reaching with Eikonal-Constrained Hierarchical Quasimetric Reinforcement Learning](https://arxiv.org/abs/2512.12046)
*Vittorio Giammarino,Ahmed H. Qureshi*

Main category: cs.LG

Relevance: 35.0

TL;DR: 提出Eikonal约束的拟度量强化学习(Eik-QRL)，基于Eikonal偏微分方程，实现无轨迹约束，仅需采样状态和目标，并引入分层分解(Eik-HiQRL)处理复杂动态问题。


<details>
  <summary>Details</summary>
Motivation: 目标条件强化学习(GCRL)通过目标到达而非手工设计奖励来简化任务，但现有拟度量RL(QRL)方法需要轨迹约束。本文旨在开发连续时间、无轨迹约束的拟度量RL框架，提高分布外泛化能力。

Method: 基于Eikonal偏微分方程重新形式化QRL，提出Eik-QRL框架，仅需采样状态和目标，无需轨迹约束。针对复杂动态问题，进一步提出分层分解的Eik-HiQRL方法。

Result: Eik-HiQRL在离线目标条件导航任务中达到最先进性能，在操作任务中相比QRL获得一致提升，与时间差分方法性能相当。

Conclusion: Eikonal约束的拟度量RL框架提供理论保证，分层分解有效处理复杂动态，在多个任务中展现优越性能。

Abstract: Goal-Conditioned Reinforcement Learning (GCRL) mitigates the difficulty of reward design by framing tasks as goal reaching rather than maximizing hand-crafted reward signals. In this setting, the optimal goal-conditioned value function naturally forms a quasimetric, motivating Quasimetric RL (QRL), which constrains value learning to quasimetric mappings and enforces local consistency through discrete, trajectory-based constraints. We propose Eikonal-Constrained Quasimetric RL (Eik-QRL), a continuous-time reformulation of QRL based on the Eikonal Partial Differential Equation (PDE). This PDE-based structure makes Eik-QRL trajectory-free, requiring only sampled states and goals, while improving out-of-distribution generalization. We provide theoretical guarantees for Eik-QRL and identify limitations that arise under complex dynamics. To address these challenges, we introduce Eik-Hierarchical QRL (Eik-HiQRL), which integrates Eik-QRL into a hierarchical decomposition. Empirically, Eik-HiQRL achieves state-of-the-art performance in offline goal-conditioned navigation and yields consistent gains over QRL in manipulation tasks, matching temporal-difference methods.

</details>


### [527] [CLOAK: Contrastive Guidance for Latent Diffusion-Based Data Obfuscation](https://arxiv.org/abs/2512.12086)
*Xin Yang,Omid Ardakanian*

Main category: cs.LG

Relevance: 35.0

TL;DR: Cloak：基于潜在扩散模型的时间序列数据混淆框架，通过对比学习提取解耦表示，指导扩散过程在保留有用信息的同时隐藏隐私信息，在资源受限的IoT设备上实现更好的隐私-效用权衡。


<details>
  <summary>Details</summary>
Motivation: 现有数据混淆方法存在以下问题：需要修改下游任务、难以达到满意的隐私-效用权衡、计算密集不适合资源受限的移动IoT设备。需要一种更实用、高效的数据混淆框架。

Method: 提出Cloak框架：1) 使用对比学习提取解耦表示，分离有用信息和隐私信息；2) 利用潜在扩散模型，在潜在空间中进行数据混淆；3) 通过解耦表示指导扩散过程，保留有用信息同时隐藏隐私信息。

Result: 在四个公共时间序列数据集（多种传感模态）和人脸图像数据集上的实验表明：Cloak始终优于最先进的混淆技术，且适合在资源受限环境中部署。

Conclusion: Cloak提供了一种新颖的数据混淆方法，通过解耦表示和潜在扩散模型实现了更好的隐私-效用权衡，适合资源受限的IoT设备部署，用户可根据不同隐私需求灵活调整。

Abstract: Data obfuscation is a promising technique for mitigating attribute inference attacks by semi-trusted parties with access to time-series data emitted by sensors. Recent advances leverage conditional generative models together with adversarial training or mutual information-based regularization to balance data privacy and utility. However, these methods often require modifying the downstream task, struggle to achieve a satisfactory privacy-utility trade-off, or are computationally intensive, making them impractical for deployment on resource-constrained mobile IoT devices. We propose Cloak, a novel data obfuscation framework based on latent diffusion models. In contrast to prior work, we employ contrastive learning to extract disentangled representations, which guide the latent diffusion process to retain useful information while concealing private information. This approach enables users with diverse privacy needs to navigate the privacy-utility trade-off with minimal retraining. Extensive experiments on four public time-series datasets, spanning multiple sensing modalities, and a dataset of facial images demonstrate that Cloak consistently outperforms state-of-the-art obfuscation techniques and is well-suited for deployment in resource-constrained settings.

</details>


### [528] [GraphPerf-RT: A Graph-Driven Performance Model for Hardware-Aware Scheduling of OpenMP Codes](https://arxiv.org/abs/2512.12091)
*Mohammad Pivezhandi,Mahdi Banisharif,Saeed Bakhshan,Abusayeed Saifullah,Ali Jannesari*

Main category: cs.LG

Relevance: 35.0

TL;DR: GraphPerf-RT：首个用于异构嵌入式SoC上OpenMP工作负载性能预测的代理模型，通过异构图表示统一任务DAG拓扑、代码语义和运行时上下文，实现多任务预测和校准不确定性，支持风险感知调度。


<details>
  <summary>Details</summary>
Motivation: 异构嵌入式SoC上OpenMP工作负载的性能预测面临挑战：任务DAG结构复杂、控制流不规则、缓存和分支行为、热动态等因素相互影响。传统启发式方法在不规则工作负载下效果不佳，表格回归器丢弃结构信息，无模型RL可能过热资源受限设备。

Method: 提出GraphPerf-RT，通过异构图表示统一任务DAG拓扑、CFG导出的代码语义和运行时上下文（每核DVFS、热状态、利用率），使用类型化边编码优先级、放置和争用。多任务证据头预测执行时间、能耗、缓存和分支缺失、利用率，并校准不确定性（Normal-Inverse-Gamma分布）。

Result: 在三个嵌入式ARM平台（Jetson TX2、Jetson Orin NX、RUBIK Pi）上验证，R^2 > 0.95，不确定性校准良好（ECE < 0.05）。与四种RL方法集成在Jetson TX2上测试，MAMBRL-D3QN结合GraphPerf-RT作为世界模型相比无模型基线实现66%执行时间减少（0.97 +/- 0.35s）和82%能耗减少（0.006 +/- 0.005J）。

Conclusion: 准确且具有不确定性感知的代理模型能够在热约束嵌入式系统上实现有效的基于模型的规划，为异构嵌入式SoC上的OpenMP工作负载调度提供了新的解决方案。

Abstract: Performance prediction for OpenMP workloads on heterogeneous embedded SoCs is challenging due to complex interactions between task DAG structure, control-flow irregularity, cache
  and branch behavior, and thermal dynamics; classical heuristics struggle under workload irregularity, tabular regressors discard structural information, and model-free RL risks
  overheating resource-constrained devices. We introduce GraphPerf-RT, the first surrogate that unifies task DAG topology, CFG-derived code semantics, and runtime context (per-core
  DVFS, thermal state, utilization) in a heterogeneous graph representation with typed edges encoding precedence, placement, and contention. Multi-task evidential heads predict
  makespan, energy, cache and branch misses, and utilization with calibrated uncertainty (Normal-Inverse-Gamma), enabling risk-aware scheduling that filters low-confidence rollouts.
  We validate GraphPerf-RT on three embedded ARM platforms (Jetson TX2, Jetson Orin NX, RUBIK Pi), achieving R^2 > 0.95 with well-calibrated uncertainty (ECE < 0.05). To
  demonstrate end-to-end scheduling utility, we integrate the surrogate with four RL methods on Jetson TX2: single-agent model-free (SAMFRL), single-agent model-based (SAMBRL),
  multi-agent model-free (MAMFRL-D3QN), and multi-agent model-based (MAMBRL-D3QN). Experiments across 5 seeds (200 episodes each) show that MAMBRL-D3QN with GraphPerf-RT as the
  world model achieves 66% makespan reduction (0.97 +/- 0.35s) and 82% energy reduction (0.006 +/- 0.005J) compared to model-free baselines, demonstrating that accurate,
  uncertainty-aware surrogates enable effective model-based planning on thermally constrained embedded systems.

</details>


### [529] [High-Dimensional Tensor Discriminant Analysis: Low-Rank Discriminant Structure, Representation Synergy, and Theoretical Guarantees](https://arxiv.org/abs/2512.12122)
*Elynn Chen,Yuefeng Han,Jiayu Li*

Main category: cs.LG

Relevance: 35.0

TL;DR: 提出CP低秩张量判别分析(CP-TDA)方法，用于处理高维张量分类问题，结合随机复合PCA初始化和迭代优化，并开发半参数张量判别模型处理非正态数据。


<details>
  <summary>Details</summary>
Motivation: 高维张量预测器在现代应用中日益普遍，特别是作为神经网络的表示学习结果。现有张量分类方法依赖稀疏性或Tucker结构，且缺乏理论保证。经验证据表明判别信号集中在少数多线性分量上，因此引入CP低秩结构进行张量判别分析。

Method: 1. 在张量高斯混合模型下提出CP低秩张量判别分析(CP-TDA)，使用随机复合PCA(rc-PCA)初始化处理依赖性和各向异性噪声，然后进行迭代优化。2. 针对非正态张量数据，开发首个半参数张量判别模型，通过深度生成模型将学习到的张量表示映射到适合CP-TDA的潜在空间。

Result: 建立了全局收敛性和极小极大最优误分类率。在数值研究和图分类实际数据分析中，相比现有张量分类器和最先进的图神经网络，特别是在高维小样本情况下，显示出显著优势。

Conclusion: CP-TDA为高维张量分类提供了理论保证的有效方法，半参数扩展使其能处理非正态数据，在实际应用中表现出优越性能。

Abstract: High-dimensional tensor-valued predictors arise in modern applications, increasingly as learned representations from neural networks. Existing tensor classification methods rely on sparsity or Tucker structures and often lack theoretical guarantees. Motivated by empirical evidence that discriminative signals concentrate along a few multilinear components, we introduce CP low-rank structure for the discriminant tensor, a modeling perspective not previously explored. Under a Tensor Gaussian Mixture Model, we propose high-dimensional CP low-rank Tensor Discriminant Analysis (CP-TDA) with Randomized Composite PCA (\textsc{rc-PCA}) initialization, that is essential for handling dependent and anisotropic noise under weaker signal strength and incoherence conditions, followed by iterative refinement algorithm. We establish global convergence and minimax-optimal misclassification rates.
  To handle tensor data deviating from tensor normality, we develop the first semiparametric tensor discriminant model, in which learned tensor representations are mapped via deep generative models into a latent space tailored for CP-TDA. Misclassification risk decomposes into representation, approximation, and estimation errors. Numerical studies and real data analysis on graph classification demonstrate substantial gains over existing tensor classifiers and state-of-the-art graph neural networks, particularly in high-dimensional, small-sample regimes.

</details>


### [530] [On the Approximation Power of SiLU Networks: Exponential Rates and Depth Efficiency](https://arxiv.org/abs/2512.12132)
*Koffi O. Ayena*

Main category: cs.LG

Relevance: 35.0

TL;DR: 本文建立了SiLU激活网络的理论框架，证明其在逼近光滑函数时能达到指数级逼近速率，相比ReLU网络具有更优的复杂度控制。


<details>
  <summary>Details</summary>
Motivation: 研究SiLU激活函数在神经网络中的理论逼近能力，特别是与广泛使用的ReLU激活函数相比，SiLU是否能在深度和网络规模上提供更高效的函数逼近。

Method: 提出分层构造方法：首先高效逼近平方函数x²，相比Yarotsky的ReLU构造更紧凑；然后通过函数复合扩展，建立深度SiLU网络对Sobolev类函数的逼近界。

Result: SiLU网络能以O(ω^{-2k})的逼近误差逼近光滑函数，网络深度为O(1)，总大小为O(ε^{-d/n})，相比ReLU构造具有指数级改进的逼近速率。

Conclusion: SiLU激活函数在理论逼近能力上优于ReLU，为深度神经网络架构设计提供了新的理论依据，特别是在需要高效逼近光滑函数的场景中。

Abstract: This article establishes a comprehensive theoretical framework demonstrating that SiLU (Sigmoid Linear Unit) activation networks achieve exponential approximation rates for smooth functions with explicit and improved complexity control compared to classical ReLU-based constructions. We develop a novel hierarchical construction beginning with an efficient approximation of the square function $x^2$ more compact in depth and size than comparable ReLU realizations, such as those given by Yarotsky. This construction yields an approximation error decaying as $\mathcal{O}(ω^{-2k})$ using networks of depth $\mathcal{O}(1)$. We then extend this approach through functional composition to establish sharp approximation bounds for deep SiLU networks in approximating Sobolev-class functions, with total depth $\mathcal{O}(1)$ and size $\mathcal{O}(\varepsilon^{-d/n})$.

</details>


### [531] [MolGuidance: Advanced Guidance Strategies for Conditional Molecular Generation with Flow Matching](https://arxiv.org/abs/2512.12198)
*Jirui Jin,Cheng Zeng,Pawan Prakash,Ellad B. Tadmor,Adrian Roitberg,Richard G. Hennig,Stefano Martiniani,Mingjie Liu*

Main category: cs.LG

Relevance: 35.0

TL;DR: 该论文提出了一种混合引导策略，将计算机视觉中的先进引导方法（如无分类器引导、自动引导和模型引导）整合到SE(3)-等变流匹配的分子生成框架中，通过贝叶斯优化联合优化连续和离散模态的引导尺度，在QM9和QMe14S数据集上实现了属性对齐的SOTA性能。


<details>
  <summary>Details</summary>
Motivation: 条件分子生成的关键目标包括：确保化学有效性、使生成分子与目标属性对齐、促进结构多样性、实现高效采样。最近计算机视觉领域引入了一系列新的生成模型引导策略，许多可以适应分子生成任务以支持这些目标。

Method: 1. 将最先进的引导方法（无分类器引导、自动引导、模型引导）整合到基于SE(3)-等变流匹配过程的分子生成框架中
2. 提出混合引导策略，分别引导连续和离散分子模态（分别作用于速度场和预测logits）
3. 通过贝叶斯优化联合优化引导尺度
4. 在QM9和QMe14S数据集上进行基准测试

Result: 1. 在从头分子生成中实现了属性对齐的新SOTA性能
2. 生成的分子表现出高结构有效性
3. 系统比较了各种引导方法的优缺点，为其更广泛适用性提供了见解

Conclusion: 该工作成功将计算机视觉中的先进引导策略适应到分子生成领域，提出的混合引导方法在属性对齐和化学有效性方面表现出色，为条件分子生成提供了有效的解决方案。

Abstract: Key objectives in conditional molecular generation include ensuring chemical validity, aligning generated molecules with target properties, promoting structural diversity, and enabling efficient sampling for discovery. Recent advances in computer vision introduced a range of new guidance strategies for generative models, many of which can be adapted to support these goals. In this work, we integrate state-of-the-art guidance methods -- including classifier-free guidance, autoguidance, and model guidance -- in a leading molecule generation framework built on an SE(3)-equivariant flow matching process. We propose a hybrid guidance strategy that separately guides continuous and discrete molecular modalities -- operating on velocity fields and predicted logits, respectively -- while jointly optimizing their guidance scales via Bayesian optimization. Our implementation, benchmarked on the QM9 and QMe14S datasets, achieves new state-of-the-art performance in property alignment for de novo molecular generation. The generated molecules also exhibit high structural validity. Furthermore, we systematically compare the strengths and limitations of various guidance methods, offering insights into their broader applicability.

</details>


### [532] [EEG-DLite: Dataset Distillation for Efficient Large EEG Model Training](https://arxiv.org/abs/2512.12210)
*Yuting Tang,Weibang Jiang,Shanglin Li,Yong Li,Chenyu Liu,Xinliang Zhou,Yi Ding,Cuntai Guan*

Main category: cs.LG

Relevance: 35.0

TL;DR: EEG-DLite：一种用于EEG基础模型的数据蒸馏框架，通过选择性去除噪声和冗余样本，仅用5%的数据就能达到全数据集训练的效果


<details>
  <summary>Details</summary>
Motivation: 大规模EEG基础模型训练资源密集，EEG数据量大且质量参差不齐，需要更高效的预训练方法

Method: 使用自监督自动编码器将EEG片段编码为紧凑潜在表示，基于这些表示过滤异常值和最小化冗余，创建更小但信息丰富的子集

Result: 在2500小时数据集中，仅使用EEG-DLite筛选的5%数据进行训练，在多个下游任务上达到甚至超越全数据集训练的性能

Conclusion: EEG-DLite为生理基础模型提供了可扩展且实用的路径，首次系统研究了EEG基础模型的预训练数据蒸馏

Abstract: Large-scale EEG foundation models have shown strong generalization across a range of downstream tasks, but their training remains resource-intensive due to the volume and variable quality of EEG data. In this work, we introduce EEG-DLite, a data distillation framework that enables more efficient pre-training by selectively removing noisy and redundant samples from large EEG datasets. EEG-DLite begins by encoding EEG segments into compact latent representations using a self-supervised autoencoder, allowing sample selection to be performed efficiently and with reduced sensitivity to noise. Based on these representations, EEG-DLite filters out outliers and minimizes redundancy, resulting in a smaller yet informative subset that retains the diversity essential for effective foundation model training. Through extensive experiments, we demonstrate that training on only 5 percent of a 2,500-hour dataset curated with EEG-DLite yields performance comparable to, and in some cases better than, training on the full dataset across multiple downstream tasks. To our knowledge, this is the first systematic study of pre-training data distillation in the context of EEG foundation models. EEG-DLite provides a scalable and practical path toward more effective and efficient physiological foundation modeling. The code is available at https://github.com/t170815518/EEG-DLite.

</details>


### [533] [Uncertainty Quantification for Machine Learning: One Size Does Not Fit All](https://arxiv.org/abs/2512.12341)
*Paul Hofman,Yusuf Sale,Eyke Hüllermeier*

Main category: cs.LG

Relevance: 35.0

TL;DR: 该论文认为没有单一的最佳不确定性度量方法，应根据具体应用场景定制不确定性量化，提出了一个灵活的不确定性度量框架，区分二阶分布的总不确定性、偶然不确定性和认知不确定性。


<details>
  <summary>Details</summary>
Motivation: 在安全关键应用中，准确量化预测不确定性至关重要。现有研究通常声称某种不确定性度量方法优于其他方法，但作者认为不存在单一的最佳度量方法，不确定性量化应根据具体应用场景进行定制。

Method: 提出一个灵活的不确定性度量框架，区分二阶分布的总不确定性、偶然不确定性和认知不确定性。这些度量可以通过特定的损失函数（即适当的评分规则）进行实例化，以控制其特性。作者展示了不同特性适用于不同任务。

Result: 研究发现：1）对于选择性预测任务，评分规则应与任务损失匹配；2）对于分布外检测，互信息（一种广泛使用的认知不确定性度量）表现最佳；3）在主动学习设置中，基于零一损失的认知不确定性始终优于其他不确定性度量。

Conclusion: 不确定性度量应根据具体应用需求进行选择，不存在适用于所有场景的单一最佳度量方法。提出的灵活框架允许根据任务特性定制不确定性量化方法。

Abstract: Proper quantification of predictive uncertainty is essential for the use of machine learning in safety-critical applications. Various uncertainty measures have been proposed for this purpose, typically claiming superiority over other measures. In this paper, we argue that there is no single best measure. Instead, uncertainty quantification should be tailored to the specific application. To this end, we use a flexible family of uncertainty measures that distinguishes between total, aleatoric, and epistemic uncertainty of second-order distributions. These measures can be instantiated with specific loss functions, so-called proper scoring rules, to control their characteristics, and we show that different characteristics are useful for different tasks. In particular, we show that, for the task of selective prediction, the scoring rule should ideally match the task loss. On the other hand, for out-of-distribution detection, our results confirm that mutual information, a widely used measure of epistemic uncertainty, performs best. Furthermore, in an active learning setting, epistemic uncertainty based on zero-one loss is shown to consistently outperform other uncertainty measures.

</details>


### [534] [Effective Fine-Tuning with Eigenvector Centrality Based Pruning](https://arxiv.org/abs/2512.12543)
*Shaif Chowdhury,Soham Biren Katlariwala,Devleena Kashyap*

Main category: cs.LG

Relevance: 35.0

TL;DR: 提出基于图论的神经网络剪枝方法，通过特征向量中心性识别重要神经元，剪枝后微调，在多个数据集上实现更高准确率和更低复杂度


<details>
  <summary>Details</summary>
Motivation: 受社交媒体中少数关键用户能驱动网络传播的启发，作者认为神经网络微调也可借鉴类似原理。传统微调通常在预训练模型顶部添加新分类层，而作者认为通过先剪枝保留最重要神经元再进行微调，能获得更好的适应性能。

Method: 提出基于图论的神经网络剪枝方法：1) 将神经元表示为图节点，神经元间相似性作为边；2) 使用特征向量中心性计算神经元重要性分数；3) 基于中心性分数剪枝，保留最核心的神经元；4) 对剪枝后的网络进行微调。在VGGNet、EfficientNet和ResNet上评估，使用TF Flowers、Caltech-101和Oxford Flowers-102数据集。

Result: 在Oxford Flowers-102数据集上，该方法达到48%分类准确率，相比基线VGGNet模型的30%准确率有显著提升。同时显著降低模型复杂度，在多个数据集和模型架构上都实现了更高的分类准确率。

Conclusion: 基于图论的神经网络剪枝方法能有效识别和保留最重要的神经元，通过先剪枝后微调的流程，在保持模型轻量化的同时提升分类性能，验证了从社交媒体传播理论到神经网络优化的跨领域启发有效性。

Abstract: In social media networks a small number of highly influential users can drive large scale changes in discourse across multiple communities. Small shifts in the behavior of these users are often sufficient to propagate widely throughout the network. A similar phenomenon occurs during neural network fine tuning. Conventional fine tuning of convolutional neural networks typically adds a new linear classification layer on top of a large pre trained model. Instead we argue that improved adaptation can be achieved by first pruning the network to retain only the most important neurons and then performing fine tuning.
  We propose a graph theory based method for pruning neural networks that is designed to improve fine tuning performance. In this method each neuron is represented as a node and edges encode similarity between neurons. Neurons are pruned based on importance scores computed using eigenvector centrality. The resulting pruned network is then fine tuned using only the most central neurons. We evaluate the proposed method on VGGNet EfficientNet and ResNet models using the TF Flowers Caltech one zero one and Oxford Flowers one zero two datasets. The proposed approach achieves higher classification accuracy while significantly reducing model complexity. On the Oxford Flowers one zero two dataset the method achieves forty eight percent classification accuracy compared to thirty percent accuracy obtained by the baseline VGGNet model.

</details>


### [535] [Skillful Subseasonal-to-Seasonal Forecasting of Extreme Events with a Multi-Sphere Coupled Probabilistic Model](https://arxiv.org/abs/2512.12545)
*Bin Mu,Yuxuan Chen,Shijin Yuan,Bo Qin,Hao Guo*

Main category: cs.LG

Relevance: 35.0

TL;DR: TianXing-S2S是一个基于扩散模型的多圈层耦合概率模型，用于全球次季节到季节(S2S)每日集合预报，在45天预报中优于ECMWF和FuXi-S2S系统。


<details>
  <summary>Details</summary>
Motivation: 在气候变化加速的背景下，准确的次季节到季节(S2S)极端事件预测对资源规划和灾害缓解至关重要。然而，由于复杂的多圈层相互作用和内在的大气不确定性，这种预测仍然具有挑战性。

Method: 1) 将多样化的多圈层预测因子编码到紧凑的潜在空间；2) 使用扩散模型生成每日集合预报；3) 在去噪器中加入基于最优传输(OT)的新型耦合模块，优化大气与多圈层边界条件之间的相互作用。

Result: 在关键大气变量上，TianXing-S2S在1.5°分辨率的45天日平均集合预报中，优于欧洲中期天气预报中心(ECMWF) S2S系统和FuXi-S2S。模型能够熟练预测热浪和异常降水等极端事件，并识别土壤湿度作为关键前兆信号。此外，模型能生成长达180天的稳定滚动预报。

Conclusion: TianXing-S2S为变暖世界中的S2S研究建立了一个稳健的框架，展示了扩散模型在复杂多圈层耦合预报中的潜力。

Abstract: Accurate subseasonal-to-seasonal (S2S) prediction of extreme events is critical for resource planning and disaster mitigation under accelerating climate change. However, such predictions remain challenging due to complex multi-sphere interactions and intrinsic atmospheric uncertainty. Here we present TianXing-S2S, a multi-sphere coupled probabilistic model for global S2S daily ensemble forecast. TianXing-S2S first encodes diverse multi-sphere predictors into a compact latent space, then employs a diffusion model to generate daily ensemble forecasts. A novel coupling module based on optimal transport (OT) is incorporated in the denoiser to optimize the interactions between atmospheric and multi-sphere boundary conditions. Across key atmospheric variables, TianXing-S2S outperforms both the European Centre for Medium-Range Weather Forecasts (ECMWF) S2S system and FuXi-S2S in 45-day daily-mean ensemble forecasts at 1.5 resolution. Our model achieves skillful subseasonal prediction of extreme events including heat waves and anomalous precipitation, identifying soil moisture as a critical precursor signal. Furthermore, we demonstrate that TianXing-S2S can generate stable rollout forecasts up to 180 days, establishing a robust framework for S2S research in a warming world.

</details>


### [536] [Causal inference and model explainability tools for retail](https://arxiv.org/abs/2512.12605)
*Pranav Gupta,Nithin Surendran*

Main category: cs.LG

Relevance: 35.0

TL;DR: 该论文提出了一个在零售和电子商务中应用模型可解释性和因果推断的框架，通过可解释模型和双重机器学习方法来获得销售洞察


<details>
  <summary>Details</summary>
Motivation: 当前零售业虽然收集了大量数据并使用机器学习进行分析，但现有模型缺乏可解释性，且无法验证或发现因果关系。作者旨在解决这些问题，为销售洞察提供可解释和因果推断的方法

Method: 1. 回顾电子商务和零售领域的因果推断和可解释性文献
2. 应用可解释模型（具有较低SHAP值方差）
3. 采用双重机器学习方法纳入多个混杂因素
4. 在真实世界数据集上进行应用

Result: 研究发现：1）固有可解释模型具有较低的SHAP值方差；2）通过双重机器学习方法包含多个混杂因素能够获得正确的因果效应符号

Conclusion: 论文提供了一个在零售分析中结合模型可解释性和因果推断的实用框架，能够帮助零售商获得更可靠和可解释的销售洞察

Abstract: Most major retailers today have multiple divisions focused on various aspects, such as marketing, supply chain, online customer experience, store customer experience, employee productivity, and vendor fulfillment. They also regularly collect data corresponding to all these aspects as dashboards and weekly/monthly/quarterly reports. Although several machine learning and statistical techniques have been in place to analyze and predict key metrics, such models typically lack interpretability. Moreover, such techniques also do not allow the validation or discovery of causal links. In this paper, we aim to provide a recipe for applying model interpretability and causal inference for deriving sales insights. In this paper, we review the existing literature on causal inference and interpretability in the context of problems in e-commerce and retail, and apply them to a real-world dataset. We find that an inherently explainable model has a lower variance of SHAP values, and show that including multiple confounders through a double machine learning approach allows us to get the correct sign of causal effect.

</details>


### [537] [DynaGen: Unifying Temporal Knowledge Graph Reasoning with Dynamic Subgraphs and Generative Regularization](https://arxiv.org/abs/2512.12669)
*Jiawei Shen,Jia Zhu,Hanghui Guo,Weijie Shi,Guoqing Ma,Yidan Liang,Jingjiang Liu,Hao Chen,Shimin Di*

Main category: cs.LG

Relevance: 35.0

TL;DR: DynaGen是一个统一的时间知识图谱推理方法，通过动态构建实体中心子图进行插值，使用条件扩散过程进行外推，在六个基准数据集上达到SOTA性能。


<details>
  <summary>Details</summary>
Motivation: 现有时间知识图谱推理方法面临两个关键挑战：插值中的有限上下文建模和外推中的认知泛化偏差。插值方法通常将时间信息嵌入到单个事实中，而外推技术则依赖序列模型识别重复模式，但缺乏对演化原则的深入理解。

Method: DynaGen采用统一框架：1) 插值：动态构建实体中心子图，使用协同双分支GNN编码器捕捉演化结构上下文；2) 外推：应用条件扩散过程，迫使模型学习底层演化原则而非表面模式。

Result: 在六个基准数据集上的实验表明，DynaGen达到最先进性能。相比次优模型，平均MRR分数在插值上提高2.61分，在外推上提高1.45分。

Conclusion: DynaGen通过统一的动态生成框架有效解决了时间知识图谱推理中的上下文建模和泛化偏差问题，为插值和外推任务提供了强大的解决方案。

Abstract: Temporal Knowledge Graph Reasoning (TKGR) aims to complete missing factual elements along the timeline. Depending on the temporal position of the query, the task is categorized into interpolation and extrapolation. Existing interpolation methods typically embed temporal information into individual facts to complete missing historical knowledge, while extrapolation techniques often leverage sequence models over graph snapshots to identify recurring patterns for future event prediction. These methods face two critical challenges: limited contextual modeling in interpolation and cognitive generalization bias in extrapolation. To address these, we propose a unified method for TKGR, dubbed DynaGen. For interpolation, DynaGen dynamically constructs entity-centric subgraphs and processes them with a synergistic dual-branch GNN encoder to capture evolving structural context. For extrapolation, it applies a conditional diffusion process, which forces the model to learn underlying evolutionary principles rather than just superficial patterns, enhancing its ability to predict unseen future events. Extensive experiments on six benchmark datasets show DynaGen achieves state-of-the-art performance. On average, compared to the second-best models, DynaGen improves the Mean Reciprocal Rank (MRR) score by 2.61 points for interpolation and 1.45 points for extrapolation.

</details>


### [538] [Co-Exploration and Co-Exploitation via Shared Structure in Multi-Task Bandits](https://arxiv.org/abs/2512.12693)
*Sumantrak Mukherjee,Serafima Lebedeva,Valentin Margraf,Jonas Hanselle,Kanta Yamaoka,Viktor Bengs,Stefan Konigorski,Eyke Hüllermeier,Sebastian Josef Vollmer*

Main category: cs.LG

Relevance: 35.0

TL;DR: 提出一种贝叶斯框架，用于在上下文部分可观测的多任务多臂老虎机中进行高效探索，通过潜在上下文变量建模奖励分布间的依赖关系，利用粒子近似对数密度高斯过程学习全局联合分布。


<details>
  <summary>Details</summary>
Motivation: 在上下文部分可观测的多任务多臂老虎机中，奖励分布通过潜在上下文变量相互依赖，现有方法难以有效利用这种结构依赖进行跨任务学习，同时平衡个性化推理需求。

Method: 提出贝叶斯框架，识别两种认知不确定性来源：跨臂和跨任务的结构不确定性，以及用户特定的上下文不完整不确定性。使用粒子近似对数密度高斯过程表示任务和奖励的联合分布，无需对潜在变量做先验假设即可灵活发现依赖关系。

Result: 实证表明该方法优于分层模型老虎机等基线方法，特别是在模型设定错误或复杂潜在异质性场景中表现更佳。

Conclusion: 该方法能有效利用跨任务的结构依赖进行高效探索，同时支持新任务的个性化推理，在复杂多任务老虎机问题中具有优势。

Abstract: We propose a novel Bayesian framework for efficient exploration in contextual multi-task multi-armed bandit settings, where the context is only observed partially and dependencies between reward distributions are induced by latent context variables. In order to exploit these structural dependencies, our approach integrates observations across all tasks and learns a global joint distribution, while still allowing personalised inference for new tasks. In this regard, we identify two key sources of epistemic uncertainty, namely structural uncertainty in the latent reward dependencies across arms and tasks, and user-specific uncertainty due to incomplete context and limited interaction history. To put our method into practice, we represent the joint distribution over tasks and rewards using a particle-based approximation of a log-density Gaussian process. This representation enables flexible, data-driven discovery of both inter-arm and inter-task dependencies without prior assumptions on the latent variables. Empirically, we demonstrate that our method outperforms baselines such as hierarchical model bandits, especially in settings with model misspecification or complex latent heterogeneity.

</details>


### [539] [Solving a Machine Learning Regression Problem Based on the Theory of Random Functions](https://arxiv.org/abs/2512.12731)
*Yuriy N. Bakhvalov*

Main category: cs.LG

Relevance: 35.0

TL;DR: 该论文从随机函数理论的角度研究机器学习回归问题，基于无差别原理推导出回归方法，证明了具有特定对称性的概率测度会自然导出广义多调和样条核函数。


<details>
  <summary>Details</summary>
Motivation: 为机器学习回归方法提供理论基础，从第一性原理出发推导回归方法，而不是经验性地选择核函数和正则化形式。

Method: 使用随机函数理论框架，基于无差别原理（平移、旋转、缩放不变性和高斯性）推导回归方法，证明这些对称性要求会唯一确定核函数形式和正则化类型。

Result: 推导出的核函数与广义多调和样条一致，但这是理论推导的结果而非经验选择。该方法为一大类平滑和插值方法提供了理论基础。

Conclusion: 在缺乏先验信息的情况下，基于对称性原理推导的回归方法具有最优性，为机器学习回归提供了坚实的理论基础。

Abstract: This paper studies a machine learning regression problem as a multivariate approximation problem using the framework of the theory of random functions. An ab initio derivation of a regression method is proposed, starting from postulates of indifference. It is shown that if a probability measure on an infinite-dimensional function space possesses natural symmetries (invariance under translation, rotation, scaling, and Gaussianity), then the entire solution scheme, including the kernel form, the type of regularization, and the noise parameterization, follows analytically from these postulates. The resulting kernel coincides with a generalized polyharmonic spline; however, unlike existing approaches, it is not chosen empirically but arises as a consequence of the indifference principle. This result provides a theoretical foundation for a broad class of smoothing and interpolation methods, demonstrating their optimality in the absence of a priori information.

</details>


### [540] [SPARK: Igniting Communication-Efficient Decentralized Learning via Stage-wise Projected NTK and Accelerated Regularization](https://arxiv.org/abs/2512.12737)
*Li Xia*

Main category: cs.LG

Relevance: 35.0

TL;DR: SPARK：一种用于去中心化联邦学习的高效通信方法，通过随机投影压缩Jacobian矩阵、阶段式退火蒸馏和Nesterov动量加速，在减少98.7%通信量的同时保持收敛速度和精度


<details>
  <summary>Details</summary>
Motivation: 去中心化联邦学习面临统计异质性和通信开销的挑战。虽然基于NTK的方法收敛更快，但在带宽受限的边缘网络中传输完整的Jacobian矩阵不切实际。需要一种既能保持收敛速度又能大幅减少通信量的解决方案。

Method: SPARK整合了三个关键技术：1) 基于随机投影的Jacobian压缩，在保持收敛所需谱特性的同时压缩矩阵；2) 阶段式退火蒸馏，从纯NTK演化过渡到邻居正则化学习，抵消压缩噪声；3) Nesterov动量加速，通过蒸馏平滑实现稳定累积并加速收敛。

Result: SPARK相比NTK-DFL减少了98.7%的通信量，同时保持收敛速度并实现更高的精度。结合动量后，达到目标性能的速度提高了3倍，在通信效率方面达到了最先进的水平。

Conclusion: SPARK为带宽受限的边缘环境提供了实用的去中心化学习解决方案，通过协同整合压缩、蒸馏和动量技术，在通信效率和模型性能之间取得了良好平衡。

Abstract: Decentralized federated learning (DFL) faces critical challenges from statistical heterogeneity and communication overhead. While NTK-based methods achieve faster convergence, transmitting full Jacobian matrices is impractical for bandwidth-constrained edge networks. We propose SPARK, synergistically integrating random projection-based Jacobian compression, stage-wise annealed distillation, and Nesterov momentum acceleration. Random projections compress Jacobians while preserving spectral properties essential for convergence. Stage-wise annealed distillation transitions from pure NTK evolution to neighbor-regularized learning, counteracting compression noise. Nesterov momentum accelerates convergence through stable accumulation enabled by distillation smoothing. SPARK achieves 98.7% communication reduction compared to NTK-DFL while maintaining convergence speed and superior accuracy. With momentum, SPARK reaches target performance 3 times faster, establishing state-of-the-art results for communication-efficient decentralized learning and enabling practical deployment in bandwidth-limited edge environments.

</details>


### [541] [Federated Learning with Feedback Alignment](https://arxiv.org/abs/2512.12762)
*Incheol Baek,Hyungbin Kim,Minseo Kim,Yon Dohn Chung*

Main category: cs.LG

Relevance: 35.0

TL;DR: 提出FLFA框架，将反馈对齐机制融入联邦学习，通过全局模型权重作为共享反馈矩阵来缓解非IID数据导致的局部漂移问题。


<details>
  <summary>Details</summary>
Motivation: 联邦学习在数据隐私保护方面有优势，但面临非独立同分布数据导致的局部漂移问题，这会阻碍全局模型收敛。需要一种既能保持隐私又能缓解局部漂移的有效方法。

Method: FLFA框架将反馈对齐机制集成到联邦学习中，在本地训练的反向传播过程中使用全局模型权重作为共享反馈矩阵，从而对齐本地更新与全局模型，缓解局部漂移。

Result: 理论分析表明FLFA能有效缓解局部漂移并保证局部和全局模型的鲁棒收敛。实证评估显示FLFA能提升其他联邦学习方法的性能，增强局部漂移测量效果。

Conclusion: FLFA是一种新颖有效的联邦学习框架，能以最小计算成本和零额外通信开销缓解非IID数据导致的局部漂移问题，提升联邦学习性能。

Abstract: Federated Learning (FL) enables collaborative training across multiple clients while preserving data privacy, yet it struggles with data heterogeneity, where clients' data are not distributed independently and identically (non-IID). This causes local drift, hindering global model convergence. To address this, we introduce Federated Learning with Feedback Alignment (FLFA), a novel framework that integrates feedback alignment into FL. FLFA uses the global model's weights as a shared feedback matrix during local training's backward pass, aligning local updates with the global model efficiently. This approach mitigates local drift with minimal additional computational cost and no extra communication overhead.
  Our theoretical analysis supports FLFA's design by showing how it alleviates local drift and demonstrates robust convergence for both local and global models. Empirical evaluations, including accuracy comparisons and measurements of local drift, further illustrate that FLFA can enhance other FL methods demonstrating its effectiveness.

</details>


### [542] [Credit Risk Estimation with Non-Financial Features: Evidence from a Synthetic Istanbul Dataset](https://arxiv.org/abs/2512.12783)
*Atalay Denknalbant,Emre Sezdi,Zeki Furkan Kutlu,Polat Goktas*

Main category: cs.LG

Relevance: 35.0

TL;DR: 使用合成数据和机器学习评估无信用记录借款人的替代金融数据有效性，发现行为属性可接近传统征信的区分能力


<details>
  <summary>Details</summary>
Motivation: 解决伊斯坦布尔无信用记录人群的金融排斥问题，研究如何利用替代数据评估这类借款人，为普惠金融提供方案

Method: 使用检索增强生成技术创建合成数据集，包含人口统计和替代行为属性，比较CatBoost、LightGBM、XGBoost在有无替代数据下的表现

Result: 替代数据使AUC提升约1.3个百分点，平衡F1分数从0.84提升至0.95（14%增益），证明简洁行为属性可接近传统征信区分能力

Conclusion: 替代金融数据可为无信用记录借款人提供接近征信水平的评估能力，为普惠金融提供透明、公平的解决方案

Abstract: Financial exclusion constrains entrepreneurship, increases income volatility, and widens wealth gaps. Underbanked consumers in Istanbul often have no bureau file because their earnings and payments flow through informal channels. To study how such borrowers can be evaluated we create a synthetic dataset of one hundred thousand Istanbul residents that reproduces first quarter 2025 TÜİK census marginals and telecom usage patterns. Retrieval augmented generation feeds these public statistics into the OpenAI o3 model, which synthesises realistic yet private records. Each profile contains seven socio demographic variables and nine alternative attributes that describe phone specifications, online shopping rhythm, subscription spend, car ownership, monthly rent, and a credit card flag. To test the impact of the alternative financial data CatBoost, LightGBM, and XGBoost are each trained in two versions. Demo models use only the socio demographic variables; Full models include both socio demographic and alternative attributes. Across five fold stratified validation the alternative block raises area under the curve by about one point three percentage and lifts balanced \(F_{1}\) from roughly 0.84 to 0.95, a fourteen percent gain. We contribute an open Istanbul 2025 Q1 synthetic dataset, a fully reproducible modeling pipeline, and empirical evidence that a concise set of behavioural attributes can approach bureau level discrimination power while serving borrowers who lack formal credit records. These findings give lenders and regulators a transparent blueprint for extending fair and safe credit access to the underbanked.

</details>


### [543] [OLC-WA: Drift Aware Tuning-Free Online Classification with Weighted Average](https://arxiv.org/abs/2512.12785)
*Mohammad Abu Shaira,Yunhe Feng,Heng Fan,Weishi Shi*

Main category: cs.LG

Relevance: 35.0

TL;DR: OLC-WA是一种超参数自由的自适应在线分类模型，通过指数加权移动平均融合数据流，自动检测概念漂移并调整模型，在动态数据流中表现出色。


<details>
  <summary>Details</summary>
Motivation: 现实世界数据集常存在概念漂移现象（数据分布随时间演变），忽略此现象会显著降低模型预测准确性。在线模型中的超参数通常是固定的，无法随数据分布变化动态调整，这加剧了问题。

Method: 提出OLC-WA（在线分类加权平均）模型：1）使用指数加权移动平均将新数据流与基础模型融合；2）集成优化机制动态检测概念漂移、量化其幅度，并根据数据流特征调整模型；3）无需手动设置超参数。

Result: 在多种基准数据集上的实证评估显示：1）在静态环境中，OLC-WA性能与批处理模型相当，准确率差距在1-3%内；2）在概念漂移情况下，OLC-WA超越领先的在线基线方法10-25%。

Conclusion: OLC-WA是一种有效的自适应在线分类模型，能够自动适应动态数据流中的概念漂移，无需手动调整超参数，在静态和动态环境中均表现出色。

Abstract: Real-world data sets often exhibit temporal dynamics characterized by evolving data distributions. Disregarding this phenomenon, commonly referred to as concept drift, can significantly diminish a model's predictive accuracy. Furthermore, the presence of hyperparameters in online models exacerbates this issue. These parameters are typically fixed and cannot be dynamically adjusted by the user in response to the evolving data distribution. This paper introduces Online Classification with Weighted Average (OLC-WA), an adaptive, hyperparameter-free online classification model equipped with an automated optimization mechanism. OLC-WA operates by blending incoming data streams with an existing base model. This blending is facilitated by an exponentially weighted moving average. Furthermore, an integrated optimization mechanism dynamically detects concept drift, quantifies its magnitude, and adjusts the model based on the observed data stream characteristics. This approach empowers the model to effectively adapt to evolving data distributions within streaming environments. Rigorous empirical evaluation across diverse benchmark datasets shows that OLC-WA achieves performance comparable to batch models in stationary environments, maintaining accuracy within 1-3%, and surpasses leading online baselines by 10-25% under drift, demonstrating its effectiveness in adapting to dynamic data streams.

</details>


### [544] [Unveiling Statistical Significance of Online Regression over Multiple Datasets](https://arxiv.org/abs/2512.12787)
*Mohammad Abu-Shaira,Weishi Shi*

Main category: cs.LG

Relevance: 35.0

TL;DR: 该论文针对机器学习研究中被忽视的多算法多数据集比较问题，提出使用Friedman检验及后续检验来评估在线回归模型性能，并通过真实和合成数据集验证了方法的有效性。


<details>
  <summary>Details</summary>
Motivation: 当前机器学习研究过度关注单一数据集上两种算法的性能比较，而忽略了开发统计检验方法来比较多种算法在不同数据集上的性能。特别是在在线学习领域，需要统计显著性验证来确保连续学习过程的有效性，特别是对于快速收敛和及时处理概念漂移。

Method: 采用Friedman检验及相应的后续检验来比较多种在线回归模型在不同数据集上的性能。使用真实和合成数据集，结合5折交叉验证和种子平均，确保在不同数据子集上的全面评估。

Result: 测试结果总体上确认了竞争基线的性能与其个体报告一致，但某些统计检验结果也表明，最先进方法在某些方面仍有改进空间。

Conclusion: 该研究填补了机器学习中多算法多数据集比较的统计检验方法空白，为在线学习领域的性能评估提供了可靠的统计框架，并指出了当前最先进方法仍需改进的方向。

Abstract: Despite extensive focus on techniques for evaluating the performance of two learning algorithms on a single dataset, the critical challenge of developing statistical tests to compare multiple algorithms across various datasets has been largely overlooked in most machine learning research. Additionally, in the realm of Online Learning, ensuring statistical significance is essential to validate continuous learning processes, particularly for achieving rapid convergence and effectively managing concept drifts in a timely manner. Robust statistical methods are needed to assess the significance of performance differences as data evolves over time. This article examines the state-of-the-art online regression models and empirically evaluates several suitable tests. To compare multiple online regression models across various datasets, we employed the Friedman test along with corresponding post-hoc tests. For thorough evaluations, utilizing both real and synthetic datasets with 5-fold cross-validation and seed averaging ensures comprehensive assessment across various data subsets. Our tests generally confirmed the performance of competitive baselines as consistent with their individual reports. However, some statistical test results also indicate that there is still room for improvement in certain aspects of state-of-the-art methods.

</details>


### [545] [TRACER: Transfer Learning based Real-time Adaptation for Clinical Evolving Risk](https://arxiv.org/abs/2512.12795)
*Mengying Yan,Ziye Tian,Siqi Li,Nan Liu,Benjamin A. Goldstein,Molei Liu,Chuan Hong*

Main category: cs.LG

Relevance: 35.0

TL;DR: TRACER是一个用于临床决策支持的迁移学习框架，能够在人口分布随时间变化时实时适应预测模型，特别适用于COVID-19等新疾病出现导致的混合人群过渡场景。


<details>
  <summary>Details</summary>
Motivation: 电子健康记录构建的临床决策支持工具常因时间性人口分布变化而出现性能漂移，特别是在临床环境变化最初只影响部分患者，导致混合人群过渡时。这种病例组合变化通常发生在系统级操作更新或新疾病（如COVID-19）出现后。

Method: 提出TRACER框架，通过识别就诊级别的过渡成员资格，并使用迁移学习来适应预测模型，无需完全重新训练。该方法能够检测和适应临床环境中的异质性变化。

Result: 在模拟研究中，TRACER优于基于历史或当代数据训练的静态模型。在实际应用中，预测急诊就诊后住院的COVID-19过渡期间，TRACER提高了模型的区分度和校准度。

Conclusion: TRACER为在不断演变和异质临床条件下保持稳健预测性能提供了一个可扩展的方法，能够有效应对临床决策支持系统中的性能漂移问题。

Abstract: Clinical decision support tools built on electronic health records often experience performance drift due to temporal population shifts, particularly when changes in the clinical environment initially affect only a subset of patients, resulting in a transition to mixed populations. Such case-mix changes commonly arise following system-level operational updates or the emergence of new diseases, such as COVID-19. We propose TRACER (Transfer Learning-based Real-time Adaptation for Clinical Evolving Risk), a framework that identifies encounter-level transition membership and adapts predictive models using transfer learning without full retraining. In simulation studies, TRACER outperformed static models trained on historical or contemporary data. In a real-world application predicting hospital admission following emergency department visits across the COVID-19 transition, TRACER improved both discrimination and calibration. TRACER provides a scalable approach for maintaining robust predictive performance under evolving and heterogeneous clinical conditions.

</details>


### [546] [Selective Conformal Risk Control](https://arxiv.org/abs/2512.12844)
*Yunpeng Xu,Wenge Guo,Zhi Wei*

Main category: cs.LG

Relevance: 35.0

TL;DR: 提出选择性共形风险控制（SCRC）框架，将共形预测与选择性分类结合，通过两阶段方法实现紧凑可靠的置信区间估计。


<details>
  <summary>Details</summary>
Motivation: 传统共形预测方法虽然能提供分布无关的覆盖保证，但通常产生过大的预测集，限制了实际应用价值。需要更紧凑可靠的置信区间量化方法。

Method: 提出选择性共形风险控制框架：第一阶段选择置信样本进行预测，第二阶段在选定子集上应用共形风险控制构建校准预测集。开发了两种算法：SCRC-T（保持可交换性，提供精确有限样本保证）和SCRC-I（仅校准变体，提供PAC式概率保证，计算更高效）。

Result: 在两个公开数据集上的实验表明，两种方法都能达到目标覆盖和风险水平，性能几乎相同。SCRC-I表现出略微保守的风险控制，但计算实用性更优。

Conclusion: 选择性共形风险控制为实现紧凑可靠的置信区间量化提供了有效且高效的路径。

Abstract: Reliable uncertainty quantification is essential for deploying machine learning systems in high-stakes domains. Conformal prediction provides distribution-free coverage guarantees but often produces overly large prediction sets, limiting its practical utility. To address this issue, we propose \textit{Selective Conformal Risk Control} (SCRC), a unified framework that integrates conformal prediction with selective classification. The framework formulates uncertainty control as a two-stage problem: the first stage selects confident samples for prediction, and the second stage applies conformal risk control on the selected subset to construct calibrated prediction sets. We develop two algorithms under this framework. The first, SCRC-T, preserves exchangeability by computing thresholds jointly over calibration and test samples, offering exact finite-sample guarantees. The second, SCRC-I, is a calibration-only variant that provides PAC-style probabilistic guarantees while being more computational efficient. Experiments on two public datasets show that both methods achieve the target coverage and risk levels, with nearly identical performance, while SCRC-I exhibits slightly more conservative risk control but superior computational practicality. Our results demonstrate that selective conformal risk control offers an effective and efficient path toward compact, reliable uncertainty quantification.

</details>


### [547] [Deep Q-Learning-Based Intelligent Scheduling for ETL Optimization in Heterogeneous Data Environments](https://arxiv.org/abs/2512.13060)
*Kangning Gao,Yi Hu,Cong Nie,Wei Li*

Main category: cs.LG

Relevance: 35.0

TL;DR: 提出基于深度Q学习的ETL智能调度优化框架，将ETL调度过程建模为马尔可夫决策过程，通过强化学习代理在高维状态空间中自适应决策，动态优化任务分配和资源调度。


<details>
  <summary>Details</summary>
Motivation: 解决异构数据环境下ETL流程中存在的调度效率低、资源分配不均衡、适应性差等问题，传统调度方法难以应对复杂动态环境。

Method: 1) 将ETL调度过程形式化为马尔可夫决策过程；2) 构建包含状态表示模块、特征嵌入网络、Q值估计器和奖励评估机制的深度Q学习模型；3) 设计多目标奖励函数平衡调度延迟、任务完成率、吞吐量和资源利用率等关键指标。

Result: 实验结果表明，该框架显著降低了调度延迟，提高了系统吞吐量，增强了多源异构任务条件下的执行稳定性，验证了模型在超参数变化、环境动态性和数据规模变化下的鲁棒性。

Conclusion: 该研究证明了强化学习在复杂数据调度和资源管理中的强大潜力，为智能数据管道构建提供了高效可扩展的优化策略。

Abstract: This paper addresses the challenges of low scheduling efficiency, unbalanced resource allocation, and poor adaptability in ETL (Extract-Transform-Load) processes under heterogeneous data environments by proposing an intelligent scheduling optimization framework based on deep Q-learning. The framework formalizes the ETL scheduling process as a Markov Decision Process and enables adaptive decision-making by a reinforcement learning agent in high-dimensional state spaces to dynamically optimize task allocation and resource scheduling. The model consists of a state representation module, a feature embedding network, a Q-value estimator, and a reward evaluation mechanism, which collectively consider task dependencies, node load states, and data flow characteristics to derive the optimal scheduling strategy in complex environments. A multi-objective reward function is designed to balance key performance indicators such as average scheduling delay, task completion rate, throughput, and resource utilization. Sensitivity experiments further verify the model's robustness under changes in hyperparameters, environmental dynamics, and data scale. Experimental results show that the proposed deep Q-learning scheduling framework significantly reduces scheduling delay, improves system throughput, and enhances execution stability under multi-source heterogeneous task conditions, demonstrating the strong potential of reinforcement learning in complex data scheduling and resource management, and providing an efficient and scalable optimization strategy for intelligent data pipeline construction.

</details>


### [548] [From Overfitting to Reliability: Introducing the Hierarchical Approximate Bayesian Neural Network](https://arxiv.org/abs/2512.13111)
*Hayk Amirkhanian,Marco F. Huber*

Main category: cs.LG

Relevance: 35.0

TL;DR: 提出了一种基于高斯-逆Wishart分布作为超先验的层次近似贝叶斯神经网络(HABNN)，能够以线性复杂度计算预测分布和权重后验，有效解决过拟合问题并提供可靠的不确定性估计。


<details>
  <summary>Details</summary>
Motivation: 神经网络在多个领域取得成功，但超参数调优和过拟合仍是重要挑战。贝叶斯神经网络通过将不确定性直接纳入模型，为这些挑战提供了解决方案，特别是在处理分布外数据时能提供更可靠的预测。

Method: 提出层次近似贝叶斯神经网络(HABNN)，使用高斯-逆Wishart分布作为网络权重的超先验。该方法提供了预测分布和权重后验的解析表示，这些表示可以转化为学生t分布的参数计算，且计算复杂度与权重数量呈线性关系。

Result: 实验结果表明，HABNN不仅匹配而且经常超越最先进模型，在处理分布外任务时表现出鲁棒性能，有效解决过拟合问题并提供可靠的不确定性估计。

Conclusion: HABNN为安全关键环境中的应用提供了有前景的方向，通过层次贝叶斯框架提高了模型的鲁棒性和性能，特别是在不确定性估计和分布外泛化方面表现出色。

Abstract: In recent years, neural networks have revolutionized various domains, yet challenges such as hyperparameter tuning and overfitting remain significant hurdles. Bayesian neural networks offer a framework to address these challenges by incorporating uncertainty directly into the model, yielding more reliable predictions, particularly for out-of-distribution data. This paper presents Hierarchical Approximate Bayesian Neural Network, a novel approach that uses a Gaussian-inverse-Wishart distribution as a hyperprior of the network's weights to increase both the robustness and performance of the model. We provide analytical representations for the predictive distribution and weight posterior, which amount to the calculation of the parameters of Student's t-distributions in closed form with linear complexity with respect to the number of weights. Our method demonstrates robust performance, effectively addressing issues of overfitting and providing reliable uncertainty estimates, particularly for out-of-distribution tasks. Experimental results indicate that HABNN not only matches but often outperforms state-of-the-art models, suggesting a promising direction for future applications in safety-critical environments.

</details>


### [549] [Enhancing Node-Level Graph Domain Adaptation by Alleviating Local Dependency](https://arxiv.org/abs/2512.13149)
*Xinwei Tai,Dongmian Zou,Hongfei Wang*

Main category: cs.LG

Relevance: 35.0

TL;DR: 该论文提出通过解相关节点特征来解决图域自适应中的条件偏移问题，设计了去相关GCN层和图Transformer层，在多个数据集上取得了显著性能提升。


<details>
  <summary>Details</summary>
Motivation: 图机器学习方法近年来取得显著进展，但如何有效将知识从一个图迁移到另一个图仍然是关键挑战。无监督图域自适应面临的主要困难是条件偏移问题，这会阻碍知识的可迁移性。

Method: 论文首先理论分析发现条件偏移仅当节点特征存在局部依赖时才会出现，并通过马尔可夫链建模依赖特征。基于此理论发现，提出通过解相关节点特征来改进GDA，具体实现为去相关GCN层和图Transformer层。

Result: 实验结果表明该方法在多个数据集上显著优于基线GDA方法，学习到的表示具有较小的类内距离，可视化结果清晰展示了这一优势。

Conclusion: 通过解相关节点特征可以有效解决图域自适应中的条件偏移问题，提出的去相关GCN层和图Transformer层为图域自适应提供了有效的解决方案。

Abstract: Recent years have witnessed significant advancements in machine learning methods on graphs. However, transferring knowledge effectively from one graph to another remains a critical challenge. This highlights the need for algorithms capable of applying information extracted from a source graph to an unlabeled target graph, a task known as unsupervised graph domain adaptation (GDA). One key difficulty in unsupervised GDA is conditional shift, which hinders transferability. In this paper, we show that conditional shift can be observed only if there exists local dependencies among node features. To support this claim, we perform a rigorous analysis and also further provide generalization bounds of GDA when dependent node features are modeled using markov chains. Guided by the theoretical findings, we propose to improve GDA by decorrelating node features, which can be specifically implemented through decorrelated GCN layers and graph transformer layers. Our experimental results demonstrate the effectiveness of this approach, showing not only substantial performance enhancements over baseline GDA methods but also clear visualizations of small intra-class distances in the learned representations. Our code is available at https://github.com/TechnologyAiGroup/DFT

</details>


### [550] [PolySet: Restoring the Statistical Ensemble Nature of Polymers for Machine Learning](https://arxiv.org/abs/2512.13186)
*Khalid Ferji*

Main category: cs.LG

Relevance: 35.0

TL;DR: PolySet：一种将聚合物表示为有限加权链集合的框架，解决了传统ML模型将聚合物视为单一完美分子图的问题，能更好地捕捉真实聚合物的统计分布特性。


<details>
  <summary>Details</summary>
Motivation: 当前聚合物科学中的机器学习模型通常将聚合物视为单一、完美定义的分子图，而真实材料是由具有分布长度的链组成的随机集合。这种物理现实与数字表示之间的不匹配限制了现有模型捕捉聚合物行为的能力。

Method: 引入PolySet框架，将聚合物表示为从假定摩尔质量分布中采样的有限加权链集合。这种基于集合的编码独立于化学细节，与任何分子表示兼容，并在均聚物情况下使用最小语言模型进行说明。

Result: PolySet保留了高阶分布矩（如Mz、Mz+1），使ML模型能够学习尾部敏感特性，具有显著提高的稳定性和准确性。

Conclusion: 通过明确承认聚合物物质的统计性质，PolySet为未来的聚合物机器学习建立了物理基础，可自然扩展到共聚物、嵌段结构和其他复杂拓扑结构。

Abstract: Machine-learning (ML) models in polymer science typically treat a polymer as a single, perfectly defined molecular graph, even though real materials consist of stochastic ensembles of chains with distributed lengths. This mismatch between physical reality and digital representation limits the ability of current models to capture polymer behaviour. Here we introduce PolySet, a framework that represents a polymer as a finite, weighted ensemble of chains sampled from an assumed molar-mass distribution. This ensemble-based encoding is independent of chemical detail, compatible with any molecular representation and illustrated here in the homopolymer case using a minimal language model. We show that PolySet retains higher-order distributional moments (such as Mz, Mz+1), enabling ML models to learn tail-sensitive properties with greatly improved stability and accuracy. By explicitly acknowledging the statistical nature of polymer matter, PolySet establishes a physically grounded foundation for future polymer machine learning, naturally extensible to copolymers, block architectures, and other complex topologies.

</details>


### [551] [WAY: Estimation of Vessel Destination in Worldwide AIS Trajectory](https://arxiv.org/abs/2512.13190)
*Jin Sob Kim,Hyun Joon Park,Wooseok Shin,Dongil Park,Sung Won Han*

Main category: cs.LG

Relevance: 35.0

TL;DR: 论文提出WAY深度学习架构，用于基于全球AIS数据的船舶目的地预测，通过重构轨迹为嵌套序列结构，结合通道聚合顺序处理块和梯度丢弃技术，实现提前数天至数周的目的地估计。


<details>
  <summary>Details</summary>
Motivation: 自动识别系统(AIS)在海上监视中数据驱动但存在可靠性问题和间隔不规则问题。现有方法难以处理长距离港口到港口轨迹，且存在时空偏差。需要一种能够提前数天至数周准确预测船舶目的地的解决方案。

Method: 1) 将长港口到港口轨迹重构为嵌套序列结构，使用空间网格减轻时空偏差；2) 提出WAY深度学习架构，包含轨迹表示层和通道聚合顺序处理(CASP)块；3) 表示层从运动学和非运动学特征生成多通道向量序列；4) CASP块使用多头通道注意力和自注意力进行聚合和顺序信息传递；5) 提出任务专用梯度丢弃(GD)技术，基于样本长度随机阻断梯度流，实现多对多训练。

Result: 在5年AIS数据上的实验表明，WAY优于传统基于空间网格的方法，且不受轨迹进展影响。梯度丢弃技术带来性能提升。多任务学习探索显示WAY在ETA估计等实际应用中的潜力。

Conclusion: WAY架构通过创新的轨迹表示和注意力机制，有效解决了长距离船舶目的地预测问题。梯度丢弃技术改善了训练稳定性。该方法在海上监视和物流优化中具有实际应用价值。

Abstract: The Automatic Identification System (AIS) enables data-driven maritime surveillance but suffers from reliability issues and irregular intervals. We address vessel destination estimation using global-scope AIS data by proposing a differentiated approach that recasts long port-to-port trajectories as a nested sequence structure. Using spatial grids, this method mitigates spatio-temporal bias while preserving detailed resolution. We introduce a novel deep learning architecture, WAY, designed to process these reformulated trajectories for long-term destination estimation days to weeks in advance. WAY comprises a trajectory representation layer and Channel-Aggregative Sequential Processing (CASP) blocks. The representation layer generates multi-channel vector sequences from kinematic and non-kinematic features. CASP blocks utilize multi-headed channel- and self-attention for aggregation and sequential information delivery. Additionally, we propose a task-specialized Gradient Dropout (GD) technique to enable many-to-many training on single labels, preventing biased feedback surges by stochastically blocking gradient flow based on sample length. Experiments on 5-year AIS data demonstrate WAY's superiority over conventional spatial grid-based approaches regardless of trajectory progression. Results further confirm that adopting GD leads to performance gains. Finally, we explore WAY's potential for real-world application through multitask learning for ETA estimation.

</details>


### [552] [Learning to Retrieve with Weakened Labels: Robust Training under Label Noise](https://arxiv.org/abs/2512.13237)
*Arnab Sharma*

Main category: cs.LG

Relevance: 35.0

TL;DR: 提出标签弱化方法应对检索模型训练中的稀疏标注和标签噪声问题，通过允许多个可能标签而非单一潜在错误标签，在噪声环境下提升检索性能


<details>
  <summary>Details</summary>
Motivation: NLP领域神经编码器用于密集检索任务时面临训练数据稀疏标注和标签噪声的挑战。现有方法通过修改损失函数或数据清洗来缓解，但需要调参或增加训练复杂度。需要更鲁棒的方法处理标签噪声。

Method: 采用标签弱化方法：对于每个查询-文档对，不强制使用单一潜在错误标签，而是基于观测监督和模型置信度生成一组可能标签。使用语义感知噪声生成技术创建不同噪声比例的实验环境。

Result: 在四种不同排序数据集上评估两个检索模型和一个重排序模型，标签弱化方法相比10种最先进的损失函数能提升检索任务性能。

Conclusion: 标签弱化是处理检索模型训练中标签噪声的有效方法，相比现有损失函数方法能提供更好的性能提升，且不需要复杂调参。

Abstract: Neural Encoders are frequently used in the NLP domain to perform dense retrieval tasks, for instance, to generate the candidate documents for a given query in question-answering tasks. However, sparse annotation and label noise in the training data make it challenging to train or fine-tune such retrieval models. Although existing works have attempted to mitigate these problems by incorporating modified loss functions or data cleaning, these approaches either require some hyperparameters to tune during training or add substantial complexity to the training setup. In this work, we consider a label weakening approach to generate robust retrieval models in the presence of label noise. Instead of enforcing a single, potentially erroneous label for each query document pair, we allow for a set of plausible labels derived from both the observed supervision and the model's confidence scores. We perform an extensive evaluation considering two retrieval models, one re-ranking model, considering four diverse ranking datasets. To this end, we also consider a realistic noisy setting by using a semantic-aware noise generation technique to generate different ratios of noise. Our initial results show that label weakening can improve the performance of the retrieval tasks in comparison to 10 different state-of-the-art loss functions.

</details>


### [553] [BézierFlow: Bézier Stochastic Interpolant Schedulers for Few-Step Generation](https://arxiv.org/abs/2512.13255)
*Yunhong Min,Juil Koo,Seungwoo Yoo,Minhyuk Sung*

Main category: cs.LG

Relevance: 35.0

TL;DR: BézierFlow：一种轻量级训练方法，通过学习贝塞尔函数参数化的调度器来优化预训练扩散和流模型的少步生成，在≤10步采样时实现2-3倍性能提升，仅需15分钟训练时间。


<details>
  <summary>Details</summary>
Motivation: 现有轻量级训练方法主要学习最优时间步长，但范围局限于ODE离散化。为了扩大搜索空间，需要学习采样轨迹的最优变换，通过参数化随机插值调度器来实现更高效的少步生成。

Method: 提出将调度器函数表示为贝塞尔函数，其中控制点自然满足边界条件、可微性和SNR单调性等关键要求。将问题简化为学习时间范围内的有序点集，从ODE时间步长扩展到贝塞尔控制点的轨迹变换。

Result: 在多种预训练扩散和流模型上，BézierFlow在≤10步采样时持续优于先前的时间步长学习方法，实现2-3倍性能提升，仅需15分钟训练时间，证明了从离散时间步长扩展到基于贝塞尔的轨迹变换搜索空间的有效性。

Conclusion: 通过将调度器参数化为贝塞尔函数，BézierFlow成功扩展了轻量级训练方法的范围，从ODE离散化扩展到更灵活的轨迹变换，为预训练生成模型的少步高效采样提供了有效解决方案。

Abstract: We introduce BézierFlow, a lightweight training approach for few-step generation with pretrained diffusion and flow models. BézierFlow achieves a 2-3x performance improvement for sampling with $\leq$ 10 NFEs while requiring only 15 minutes of training. Recent lightweight training approaches have shown promise by learning optimal timesteps, but their scope remains restricted to ODE discretizations. To broaden this scope, we propose learning the optimal transformation of the sampling trajectory by parameterizing stochastic interpolant (SI) schedulers. The main challenge lies in designing a parameterization that satisfies critical desiderata, including boundary conditions, differentiability, and monotonicity of the SNR. To effectively meet these requirements, we represent scheduler functions as Bézier functions, where control points naturally enforce these properties. This reduces the problem to learning an ordered set of points in the time range, while the interpretation of the points changes from ODE timesteps to Bézier control points. Across a range of pretrained diffusion and flow models, BézierFlow consistently outperforms prior timestep-learning methods, demonstrating the effectiveness of expanding the search space from discrete timesteps to Bézier-based trajectory transformations.

</details>


### [554] [No One Left Behind: How to Exploit the Incomplete and Skewed Multi-Label Data for Conversion Rate Prediction](https://arxiv.org/abs/2512.13300)
*Qinglin Jia,Zhaocheng Du,Chuhan Wu,Huifeng Guo,Ruiming Tang,Shuting Shi,Muyu Zhang*

Main category: cs.LG

Relevance: 35.0

TL;DR: KAML：面向广告系统中不对称多标签数据的细粒度知识迁移框架，解决转化率预测中数据缺失和分布不匹配问题


<details>
  <summary>Details</summary>
Motivation: 在线广告系统中，广告主通常有多样化的客户获取目标，但转化率预测面临数据缺失问题。由于隐私等限制，许多广告主只提交部分用户转化行为，导致多任务学习数据标签不完整。如果模型在所有可用样本上训练，部署时服务于特定转化目标的广告主时会出现训练与部署数据分布不匹配问题。

Method: 提出KAML框架：1) 归因驱动掩码策略(ADM)更好地利用不对称多标签数据；2) 分层知识提取机制(HKE)建模目标任务塔内的样本差异；3) 结合排序损失策略充分利用未标记样本

Result: 在离线行业数据集和在线A/B测试中进行了全面评估，相比现有MTL基线模型显示出显著的性能提升

Conclusion: KAML框架有效解决了广告系统中不对称多标签数据的学习问题，通过细粒度知识迁移机制提升了转化率预测模型的性能

Abstract: In most real-world online advertising systems, advertisers typically have diverse customer acquisition goals. A common solution is to use multi-task learning (MTL) to train a unified model on post-click data to estimate the conversion rate (CVR) for these diverse targets. In practice, CVR prediction often encounters missing conversion data as many advertisers submit only a subset of user conversion actions due to privacy or other constraints, making the labels of multi-task data incomplete. If the model is trained on all available samples where advertisers submit user conversion actions, it may struggle when deployed to serve a subset of advertisers targeting specific conversion actions, as the training and deployment data distributions are mismatched. While considerable MTL efforts have been made, a long-standing challenge is how to effectively train a unified model with the incomplete and skewed multi-label data. In this paper, we propose a fine-grained Knowledge transfer framework for Asymmetric Multi-Label data (KAML). We introduce an attribution-driven masking strategy (ADM) to better utilize data with asymmetric multi-label data in training. However, the more relaxed masking in ADM is a double-edged sword: it provides additional training signals but also introduces noise due to skewed data. To address this, we propose a hierarchical knowledge extraction mechanism (HKE) to model the sample discrepancy within the target task tower. Finally, to maximize the utility of unlabeled samples, we incorporate ranking loss strategy to further enhance our model. The effectiveness of KAML has been demonstrated through comprehensive evaluations on offline industry datasets and online A/B tests, which show significant performance improvements over existing MTL baselines.

</details>


### [555] [KD-PINN: Knowledge-Distilled PINNs for ultra-low-latency real-time neural PDE solvers](https://arxiv.org/abs/2512.13336)
*Karim Bounja,Lahcen Laayouni,Abdeljalil Sakat*

Main category: cs.LG

Relevance: 35.0

TL;DR: 提出知识蒸馏物理信息神经网络(KD-PINN)，通过KL散度连续适应将高容量教师模型的预测精度转移到紧凑学生模型，实现物理精度保持和4.8-6.9倍推理加速


<details>
  <summary>Details</summary>
Motivation: 物理信息神经网络(PINNs)在求解偏微分方程方面表现出色，但高容量模型推理延迟大，难以满足实时应用需求。需要开发既能保持物理精度又能实现超低延迟的神经PDE求解器

Method: 提出KD-PINN框架，使用知识蒸馏技术，通过Kullback-Leibler散度的连续适应，将高容量教师模型的预测能力转移到紧凑学生模型。在多种偏微分方程(包括Navier-Stokes和Burgers方程)上进行评估

Result: 学生模型保持了教师的物理精度，平均RMSE增加低于0.64%，推理速度提升4.8倍(Navier-Stokes)到6.9倍(Burgers)。平均推理延迟5.3ms(CPU)，进入亚10ms超低延迟实时性能范围。蒸馏过程还显示出正则化效果

Conclusion: KD-PINN框架成功实现了物理信息神经网络的准确超低延迟推理，为开发实时神经PDE求解器提供了有效方法。知识蒸馏不仅能加速推理，还具有正则化效果

Abstract: This work introduces Knowledge-Distilled Physics-Informed Neural Networks (KD-PINN), a framework that transfers the predictive accuracy of a high-capacity teacher model to a compact student through a continuous adaptation of the Kullback-Leibler divergence. To confirm its generality for various dynamics and dimensionalities, the framework is evaluated on a representative set of partial differential equations (PDEs). In all tested cases, the student model preserved the teacher's physical accuracy, with a mean RMSE increase below 0.64%, and achieved inference speedups ranging from 4.8x (Navier-Stokes) to 6.9x (Burgers). The distillation process also revealed a regularizing effect. With an average inference latency of 5.3 ms on CPU, the distilled models enter the ultra-low-latency real-time regime defined by sub-10 ms performance. Finally, this study examines how knowledge distillation reduces inference latency in PINNs to contribute to the development of accurate ultra-low-latency neural PDE solvers.

</details>


### [556] [Link-Aware Energy-Frugal Continual Learning for Fault Detection in IoT Networks](https://arxiv.org/abs/2512.13340)
*Henrik C. M. Frederiksen,Junya Shiraishi,Cedomir Stefanovic,Hei Victor Cheng,Shashi Raj Pandey*

Main category: cs.LG

Relevance: 35.0

TL;DR: 提出事件驱动的通信框架，在物联网网络中集成持续学习，用于节能的故障检测，通过自适应无线链路条件和能量预算来协同更新轻量级ML模型。


<details>
  <summary>Details</summary>
Motivation: 物联网设备资源受限，需要在设备上进行轻量级ML推理，但环境非平稳性和初始训练数据有限导致推理精度下降。传统方法需要定期更新模型，但会消耗额外能量，不适合能量受限的物联网设备。

Method: 提出事件驱动的通信框架，让物联网设备和边缘服务器协作更新轻量级ML模型。框架自适应无线链路条件和可用能量预算，只在必要时进行模型更新，实现节能的持续学习。

Result: 在真实数据集上的评估显示，该方法在推理召回率上优于周期性采样和非自适应持续学习方法，在严格的能量和带宽约束下实现了高达42.8%的改进。

Conclusion: 事件驱动的通信框架能够有效解决物联网环境中轻量级ML模型的持续学习问题，在能量受限条件下显著提升故障检测性能。

Abstract: The use of lightweight machine learning (ML) models in internet of things (IoT) networks enables resource constrained IoT devices to perform on-device inference for several critical applications. However, the inference accuracy deteriorates due to the non-stationarity in the IoT environment and limited initial training data. To counteract this, the deployed models can be updated occasionally with new observed data samples. However, this approach consumes additional energy, which is undesirable for energy constrained IoT devices. This letter introduces an event-driven communication framework that strategically integrates continual learning (CL) in IoT networks for energy-efficient fault detection. Our framework enables the IoT device and the edge server (ES) to collaboratively update the lightweight ML model by adapting to the wireless link conditions for communication and the available energy budget. Evaluation on real-world datasets show that the proposed approach can outperform both periodic sampling and non-adaptive CL in terms of inference recall; our proposed approach achieves up to a 42.8% improvement, even under tight energy and bandwidth constraint.

</details>


### [557] [Dual-Phase Federated Deep Unlearning via Weight-Aware Rollback and Reconstruction](https://arxiv.org/abs/2512.13381)
*Changjun Zhou,Jintao Zheng,Leyou Yang,Pengfei Wang*

Main category: cs.LG

Relevance: 35.0

TL;DR: DPUL：一种新颖的联邦遗忘方法，通过识别高权重参数回滚、VAE重建低权重参数和投影恢复技术，实现深度遗忘并防止隐私泄露


<details>
  <summary>Details</summary>
Motivation: 现有联邦遗忘方法存在高计算需求、复杂激励机制和客户端计算能力差异等问题，且服务器端知识蒸馏仅移除目标客户端更新，忽略了其他客户端的隐私贡献，可能导致隐私泄露

Method: 1) 通过过滤客户端更新幅度识别高权重参数并回滚；2) 利用变分自编码器(VAE)重建和消除低权重参数；3) 使用基于投影的技术恢复模型

Result: 在四个数据集上的实验表明，DPUL超越了现有最优基线，准确率提升1%-5%，时间成本降低高达12倍

Conclusion: DPUL提供了一种有效的服务器端遗忘方法，能够深度遗忘所有有影响的权重，防止隐私泄露，同时显著提高效率和准确性

Abstract: Federated Unlearning (FUL) focuses on client data and computing power to offer a privacy-preserving solution. However, high computational demands, complex incentive mechanisms, and disparities in client-side computing power often lead to long times and higher costs. To address these challenges, many existing methods rely on server-side knowledge distillation that solely removes the updates of the target client, overlooking the privacy embedded in the contributions of other clients, which can lead to privacy leakage. In this work, we introduce DPUL, a novel server-side unlearning method that deeply unlearns all influential weights to prevent privacy pitfalls. Our approach comprises three components: (i) identifying high-weight parameters by filtering client update magnitudes, and rolling them back to ensure deep removal. (ii) leveraging the variational autoencoder (VAE) to reconstruct and eliminate low-weight parameters. (iii) utilizing a projection-based technique to recover the model. Experimental results on four datasets demonstrate that DPUL surpasses state-of-the-art baselines, providing a 1%-5% improvement in accuracy and up to 12x reduction in time cost.

</details>


### [558] [DP-EMAR: A Differentially Private Framework for Autonomous Model Weight Repair in Federated IoT Systems](https://arxiv.org/abs/2512.13460)
*Chethana Prasad Kabgere,Shylaja S S*

Main category: cs.LG

Relevance: 35.0

TL;DR: 提出DP-EMAR框架，在联邦物联网学习中结合差分隐私和错误模型自主修复，检测并重建传输引起的模型权重失真，同时保证隐私保护。


<details>
  <summary>Details</summary>
Motivation: 联邦学习在资源受限的物联网网络中面临模型权重失真的挑战，不稳定连接和对抗性干扰会悄无声息地改变传输参数，影响收敛性能。

Method: DP-EMAR框架：差分隐私+错误模型自主修复。在聚合前估计腐败模式并应用自适应校正，结合差分隐私和安全聚合来区分DP噪声与真实传输错误。

Result: 在异构物联网传感器和图数据集上的实验表明，DP-EMAR在通信腐败下保持收敛稳定性，维持接近基线性能，同时确保严格的(epsilon, delta)-DP保证。

Conclusion: 该框架增强了联邦物联网学习的鲁棒性、通信效率和隐私保护信任度，解决了资源受限环境中的模型失真问题。

Abstract: Federated Learning (FL) enables decentralized model training without sharing raw data, but model weight distortion remains a major challenge in resource constrained IoT networks. In multi tier Federated IoT (Fed-IoT) systems, unstable connectivity and adversarial interference can silently alter transmitted parameters, degrading convergence. We propose DP-EMAR, a differentially private, error model based autonomous repair framework that detects and reconstructs transmission induced distortions during FL aggregation. DP-EMAR estimates corruption patterns and applies adaptive correction before privacy noise is added, enabling reliable in network repair without violating confidentiality. By integrating Differential Privacy (DP) with Secure Aggregation (SA), the framework distinguishes DP noise from genuine transmission errors. Experiments on heterogeneous IoT sensor and graph datasets show that DP-EMAR preserves convergence stability and maintains near baseline performance under communication corruption while ensuring strict (epsilon, delta)-DP guarantees. The framework enhances robustness, communication efficiency, and trust in privacy preserving Federated IoT learning.

</details>


### [559] [On-Device Continual Learning for Unsupervised Visual Anomaly Detection in Dynamic Manufacturing](https://arxiv.org/abs/2512.13497)
*Haoyu Ren,Kay Koehle,Kirill Dorofeev,Darko Anicic*

Main category: cs.LG

Relevance: 35.0

TL;DR: 该论文提出了一种用于工业视觉异常检测的轻量级设备端持续学习方法，通过增量核心集更新机制实现快速、内存高效的模型适应，适用于动态生产环境。


<details>
  <summary>Details</summary>
Motivation: 现代制造业中，动态灵活的生产环境带来三大挑战：1) 小批量按需生产需要快速模型更新；2) 传统边缘硬件资源有限；3) 异常和正常训练数据稀缺。现有方法难以满足这些需求。

Method: 扩展PatchCore方法，结合轻量级特征提取器和基于k-center选择的增量核心集更新机制，实现设备端持续学习，无需云端重训练。

Result: 在模拟灵活生产的测试环境中，相比基线方法获得12%的AUROC提升，内存使用减少80%，训练速度比批量重训练更快。

Conclusion: 该方法为动态智能制造提供了准确、资源高效且自适应的视觉异常检测解决方案，特别适合产品频繁变化的工业场景。

Abstract: In modern manufacturing, Visual Anomaly Detection (VAD) is essential for automated inspection and consistent product quality. Yet, increasingly dynamic and flexible production environments introduce key challenges: First, frequent product changes in small-batch and on-demand manufacturing require rapid model updates. Second, legacy edge hardware lacks the resources to train and run large AI models. Finally, both anomalous and normal training data are often scarce, particularly for newly introduced product variations. We investigate on-device continual learning for unsupervised VAD with localization, extending the PatchCore to incorporate online learning for real-world industrial scenarios. The proposed method leverages a lightweight feature extractor and an incremental coreset update mechanism based on k-center selection, enabling rapid, memory-efficient adaptation from limited data while eliminating costly cloud retraining. Evaluations on an industrial use case are conducted using a testbed designed to emulate flexible production with frequent variant changes in a controlled environment. Our method achieves a 12% AUROC improvement over the baseline, an 80% reduction in memory usage, and faster training compared to batch retraining. These results confirm that our method delivers accurate, resource-efficient, and adaptive VAD suitable for dynamic and smart manufacturing.

</details>


### [560] [Learning under Distributional Drift: Reproducibility as an Intrinsic Statistical Resource](https://arxiv.org/abs/2512.13506)
*Sofiya Zaichyk*

Main category: cs.LG

Relevance: 35.0

TL;DR: 论文提出"可重复性预算"$C_T$作为量化系统统计可重复性的新统计原语，衡量在外部变化和内部反馈下采样过程保持底层分布一致性的有限能力，并推导出最优的漂移-反馈泛化界。


<details>
  <summary>Details</summary>
Motivation: 在分布漂移下进行统计学习尚未得到充分表征：当每个观测都会改变数据生成规律时，经典泛化界可能失效。需要新的理论框架来量化系统在外部变化和内部反馈下的统计可重复性。

Method: 引入可重复性预算$C_T$作为Fisher-Rao路径长度的累积，衡量学习过程中耦合的"学习者-环境"演化的总分布运动。基于此推导泛化界$O(T^{-1/2} + C_T/T)$，并证明匹配的最小最大下界。

Result: 建立了可重复性速度限制：任何算法的最坏情况泛化误差都不能低于数据生成过程的平均Fisher-Rao漂移率$C_T/T$所施加的限制。该框架将外部漂移、自适应数据分析和执行性预测统一在共同的几何结构中。

Conclusion: $C_T$作为内在量衡量了分布运动，为分布漂移下的统计学习提供了统一的理论框架，建立了最优的泛化界，并揭示了统计可重复性的基本限制。

Abstract: Statistical learning under distributional drift remains insufficiently characterized: when each observation alters the data-generating law, classical generalization bounds can collapse. We introduce a new statistical primitive, the reproducibility budget $C_T$, which quantifies a system's finite capacity for statistical reproducibility - the extent to which its sampling process can remain governed by a consistent underlying distribution in the presence of both exogenous change and endogenous feedback. Formally, $C_T$ is defined as the cumulative Fisher-Rao path length of the coupled learner-environment evolution, measuring the total distributional motion accumulated during learning. From this construct we derive a drift-feedback generalization bound of order $O(T^{-1/2} + C_T/T)$, and we prove a matching minimax lower bound showing that this rate is minimax-optimal. Consequently, the results establish a reproducibility speed limit: no algorithm can achieve smaller worst-case generalization error than that imposed by the average Fisher-Rao drift rate $C_T/T$ of the data-generating process. The framework situates exogenous drift, adaptive data analysis, and performative prediction within a common geometric structure, with $C_T$ emerging as the intrinsic quantity measuring distributional motion across these settings.

</details>


### [561] [LightTopoGAT: Enhancing Graph Attention Networks with Topological Features for Efficient Graph Classification](https://arxiv.org/abs/2512.13617)
*Ankit Sharma,Sayan Roy Gupta*

Main category: cs.LG

Relevance: 35.0

TL;DR: LightTopoGAT：一种轻量级图注意力网络，通过拓扑增强（节点度和局部聚类系数）改进图表示学习，在保持参数效率的同时提升分类性能。


<details>
  <summary>Details</summary>
Motivation: 传统图神经网络需要大量计算资源且难以有效捕获全局图属性，现有方法通常忽略结构信息，需要更高效且能整合拓扑特征的轻量级架构。

Method: 提出LightTopoGAT，通过拓扑增强整合节点度和局部聚类系数，使用简化的注意力机制保持参数效率，改进图表示学习。

Result: 在MUTAG、ENZYMES、PROTEINS三个基准数据集上优于GCN、GraphSAGE和标准GAT，MUTAG准确率提升6.6%，PROTEINS提升2.2%。

Conclusion: 拓扑特征整合是提升图神经网络性能的有效策略，无需增加架构复杂度，为轻量级图学习提供了新方向。

Abstract: Graph Neural Networks have demonstrated significant success in graph classification tasks, yet they often require substantial computational resources and struggle to capture global graph properties effectively. We introduce LightTopoGAT, a lightweight graph attention network that enhances node features through topological augmentation by incorporating node degree and local clustering coefficient to improve graph representation learning. The proposed approach maintains parameter efficiency through streamlined attention mechanisms while integrating structural information that is typically overlooked by local message passing schemes. Through comprehensive experiments on three benchmark datasets, MUTAG, ENZYMES, and PROTEINS, we show that LightTopoGAT achieves superior performance compared to established baselines including GCN, GraphSAGE, and standard GAT, with a 6.6 percent improvement in accuracy on MUTAG and a 2.2 percent improvement on PROTEINS. Ablation studies further confirm that these performance gains arise directly from the inclusion of topological features, demonstrating a simple yet effective strategy for enhancing graph neural network performance without increasing architectural complexity.

</details>


### [562] [From Code to Field: Evaluating the Robustness of Convolutional Neural Networks for Disease Diagnosis in Mango Leaves](https://arxiv.org/abs/2512.13641)
*Gabriel Vitorino de Andrade,Saulo Roberto dos Santos,Itallo Patrick Castro Alves da Silva,Emanuel Adler Medeiros Pereira,Erick de Andrade Barboza*

Main category: cs.LG

Relevance: 35.0

TL;DR: 该论文提出了一种评估卷积神经网络在芒果叶病害诊断中鲁棒性的方法，通过创建包含19种人工损坏的MangoLeafDB-C数据集，比较了5种CNN架构在恶劣条件下的表现。


<details>
  <summary>Details</summary>
Motivation: 尽管芒果具有全球重要性，但缺乏对其叶病诊断模型鲁棒性的研究。AI模型需要通过鲁棒性评估来保证在现实挑战（如图像损坏）下的可靠性能，特别是在农业应用中。

Method: 1) 创建MangoLeafDB-C数据集：在MangoLeafDB基础上添加19种人工损坏（噪声、模糊、天气变化等），每种损坏有5个严重程度等级；2) 基准测试：比较ResNet-50、ResNet-101、VGG-16、Xception和LCNN（专门为芒果叶诊断设计的轻量架构）；3) 评估指标：F1分数、损坏错误率(CE)和相对平均损坏错误率(relative mCE)。

Result: LCNN在现实场景可能出现的损坏（如散焦模糊、运动模糊）中表现优于复杂模型，并获得了最低的mCE。现代架构（如ResNet-101）在理想条件下准确率高，但在损坏场景中性能显著下降。

Conclusion: 轻量化和专门化的模型可能更适合边缘设备的现实应用，其中鲁棒性和效率至关重要。研究强调需要在农业智能系统开发中纳入鲁棒性评估，特别是在技术受限地区。

Abstract: The validation and verification of artificial intelligence (AI) models through robustness assessment are essential to guarantee the reliable performance of intelligent systems facing real-world challenges, such as image corruptions including noise, blurring, and weather variations. Despite the global importance of mango (Mangifera indica L.), there is a lack of studies on the robustness of models for the diagnosis of disease in its leaves. This paper proposes a methodology to evaluate convolutional neural networks (CNNs) under adverse conditions. We adapted the MangoLeafDB dataset, generating MangoLeafDB-C with 19 types of artificial corruptions at five severity levels. We conducted a benchmark comparing five architectures: ResNet-50, ResNet-101, VGG-16, Xception, and LCNN (the latter being a lightweight architecture designed specifically for mango leaf diagnosis). The metrics include the F1 score, the corruption error (CE) and the relative mean corruption error (relative mCE). The results show that LCNN outperformed complex models in corruptions that can be present in real-world scenarios such as Defocus Blur, Motion Blur, while also achieving the lowest mCE. Modern architectures (e.g., ResNet-101) exhibited significant performance degradation in corrupted scenarios, despite their high accuracy under ideal conditions. These findings suggest that lightweight and specialized models may be more suitable for real-world applications in edge devices, where robustness and efficiency are critical. The study highlights the need to incorporate robustness assessments in the development of intelligent systems for agriculture, particularly in regions with technological limitations.

</details>


### [563] [Policy Gradient Algorithms for Age-of-Information Cost Minimization](https://arxiv.org/abs/2512.11990)
*José-Ramón Vidal,Vicent Pla,Luis Guijarro,Israel Leyva-Mayorga*

Main category: cs.NI

Relevance: 35.0

TL;DR: 该论文提出两种基于策略梯度的模型无关强化学习算法，用于优化信息物理系统中的信息更新策略，以最小化时间平均成本（包括接收端信息年龄和数据传输成本）。


<details>
  <summary>Details</summary>
Motivation: 信息物理系统中保持物理环境信息的新鲜度至关重要，但优化物联网设备访问策略以最大化数据新鲜度（通过信息年龄度量）是一个挑战。现有方法通常需要知道传输延迟特性或年龄成本函数，这在现实场景中往往不可行。

Method: 提出两种基于策略梯度的模型无关强化学习算法：1）使用连续状态和动作空间处理能力；2）采用不同策略决定何时发送信息更新；3）可同时应用两种策略进一步降低成本。算法不需要知道传输延迟特性或年龄成本函数。

Result: 算法表现出良好的收敛特性，时间平均成本在最优值的3%以内（当最优值可计算时）。与现有方法相比，在以下一个或多个方面表现更优：适用场景更广、时间平均成本更低、计算成本至少低一个数量级。

Conclusion: 提出的基于策略梯度的强化学习算法能有效优化信息物理系统中的信息更新过程，在不知道传输延迟特性或成本函数的情况下实现接近最优的性能，具有广泛的适用性和计算效率。

Abstract: Recent developments in cyber-physical systems have increased the importance of maximizing the freshness of the information about the physical environment. However, optimizing the access policies of Internet of Things devices to maximize the data freshness, measured as a function of the Age-of-Information (AoI) metric, is a challenging task. This work introduces two algorithms to optimize the information update process in cyber-physical systems operating under the generate-at-will model, by finding an online policy without knowing the characteristics of the transmission delay or the age cost function. The optimization seeks to minimize the time-average cost, which integrates the AoI at the receiver and the data transmission cost, making the approach suitable for a broad range of scenarios. Both algorithms employ policy gradient methods within the framework of model-free reinforcement learning (RL) and are specifically designed to handle continuous state and action spaces. Each algorithm minimizes the cost using a distinct strategy for deciding when to send an information update. Moreover, we demonstrate that it is feasible to apply the two strategies simultaneously, leading to an additional reduction in cost. The results demonstrate that the proposed algorithms exhibit good convergence properties and achieve a time-average cost within 3% of the optimal value, when the latter is computable. A comparison with other state-of-the-art methods shows that the proposed algorithms outperform them in one or more of the following aspects: being applicable to a broader range of scenarios, achieving a lower time-average cost, and requiring a computational cost at least one order of magnitude lower.

</details>


### [564] [Co-Hub Node Based Multiview Graph Learning with Theoretical Guarantees](https://arxiv.org/abs/2512.12435)
*Bisakh Banerjee,Mohammad Alwardat,Tapabrata Maiti,Selin Aviyente*

Main category: stat.ML

Relevance: 35.0

TL;DR: 提出一种多视图图学习的共枢纽节点模型，假设不同视图共享共同的枢纽节点群，通过结构化稀疏性约束优化框架，在合成图和fMRI数据上验证


<details>
  <summary>Details</summary>
Motivation: 现有方法主要针对单一图结构，但许多实际场景涉及异构数据集中的多视图图。传统多视图图学习方法主要关注边相似性，但多视图图经常在不同视图间共享节点级结构（如共同枢纽节点），这种共性可以提高学习精度并提供解释性见解。

Method: 提出共枢纽节点模型，假设不同视图共享共同的枢纽节点群。开发相应的优化框架，通过对这些共枢纽节点的连接施加结构化稀疏性约束来实现。进行理论分析，包括层可识别性和估计误差界。

Result: 在合成图数据和多个受试者的fMRI时间序列数据上验证了所提方法，能够有效识别多个密切相关的图结构。

Conclusion: 共枢纽节点模型为多视图图学习提供了新视角，通过利用节点级共性提高了学习精度和可解释性，在理论和实验上都得到了验证。

Abstract: Identifying the graphical structure underlying the observed multivariate data is essential in numerous applications. Current methodologies are predominantly confined to deducing a singular graph under the presumption that the observed data are uniform. However, many contexts involve heterogeneous datasets that feature multiple closely related graphs, typically referred to as multiview graphs. Previous research on multiview graph learning promotes edge-based similarity across layers using pairwise or consensus-based regularizers. However, multiview graphs frequently exhibit a shared node-based architecture across different views, such as common hub nodes. Such commonalities can enhance the precision of learning and provide interpretive insight. In this paper, we propose a co-hub node model, positing that different views share a common group of hub nodes. The associated optimization framework is developed by enforcing structured sparsity on the connections of these co-hub nodes. Moreover, we present a theoretical examination of layer identifiability and determine bounds on estimation error. The proposed methodology is validated using both synthetic graph data and fMRI time series data from multiple subjects to discern several closely related graphs.

</details>


### [565] [Breaking the Curse of Dimensionality: On the Stability of Modern Vector Retrieval](https://arxiv.org/abs/2512.12458)
*Vihan Lakshman,Blaise Munyampirwa,Julian Shun,Benjamin Coleman*

Main category: cs.IR

Relevance: 35.0

TL;DR: 论文通过稳定性理论重新审视向量检索中的维度诅咒问题，证明了在三种实际检索场景中（多向量搜索、过滤向量搜索、稀疏向量搜索）的稳定性条件，为模型和系统设计提供指导。


<details>
  <summary>Details</summary>
Motivation: 现代向量数据库支持高维神经嵌入的高效检索，但经典理论预测此类任务会遭受维度诅咒，导致最近邻搜索效率低下。本文旨在通过稳定性理论重新审视这一悖论，为实际应用提供理论指导。

Method: 基于稳定性理论，将稳定性分析扩展到三种关键检索场景：1) 多向量搜索，分析Chamfer距离和平均池化聚合的稳定性；2) 过滤向量搜索，研究过滤器惩罚对稳定性的影响；3) 稀疏向量搜索，形式化并证明稳定性条件。通过合成和真实数据集进行实验验证。

Result: 理论证明：1) 多向量搜索中，Chamfer距离保持单向量稳定性，而平均池化可能破坏稳定性；2) 过滤向量搜索中，足够大的过滤器不匹配惩罚可以诱导稳定性；3) 稀疏向量搜索中，提出了新的充分稳定性条件。实验结果与理论预测一致。

Conclusion: 通过稳定性理论重新审视了向量检索中的维度诅咒问题，为三种实际检索场景提供了理论保证和设计指导，帮助避免维度诅咒对检索效率的影响。

Abstract: Modern vector databases enable efficient retrieval over high-dimensional neural embeddings, powering applications from web search to retrieval-augmented generation. However, classical theory predicts such tasks should suffer from the curse of dimensionality, where distances between points become nearly indistinguishable, thereby crippling efficient nearest-neighbor search. We revisit this paradox through the lens of stability, the property that small perturbations to a query do not radically alter its nearest neighbors. Building on foundational results, we extend stability theory to three key retrieval settings widely used in practice: (i) multi-vector search, where we prove that the popular Chamfer distance metric preserves single-vector stability, while average pooling aggregation may destroy it; (ii) filtered vector search, where we show that sufficiently large penalties for mismatched filters can induce stability even when the underlying search is unstable; and (iii) sparse vector search, where we formalize and prove novel sufficient stability conditions. Across synthetic and real datasets, our experimental results match our theoretical predictions, offering concrete guidance for model and system design to avoid the curse of dimensionality.

</details>


### [566] [Understanding Overparametrization in Survival Models through Double-Descent](https://arxiv.org/abs/2512.12463)
*Yin Liu,Jianwen Cai,Didong Li*

Main category: stat.ML

Relevance: 35.0

TL;DR: 该研究首次探索了生存分析中的双下降现象，分析了四种代表性生存模型（DeepSurv、PC-Hazard、Nnet-Survival、N-MTLR）的插值和有限范数插值特性，揭示了似然损失函数和模型实现如何共同决定插值的可行性。


<details>
  <summary>Details</summary>
Motivation: 经典统计学习理论预测测试损失与模型容量呈U形关系（偏差-方差权衡），但现代机器学习发现了更复杂的双下降模式。虽然回归和分类中的双下降已被广泛研究，但在生存分析中尚未探索。本研究旨在填补这一空白，理解生存模型中双下降现象的表现。

Method: 1. 严格定义了损失基模型的插值和有限范数插值两个关键特性；2. 分析了四种代表性生存模型（DeepSurv、PC-Hazard、Nnet-Survival、N-MTLR）的插值和有限范数插值存在性；3. 通过数值实验验证理论结果，展示生存模型的独特泛化行为。

Result: 1. 揭示了似然损失函数和模型实现如何共同决定插值的可行性；2. 发现过拟合对生存模型不应被视为良性的；3. 所有理论结果均得到数值实验支持，突出了生存模型的独特泛化行为。

Conclusion: 该研究首次系统探索了生存分析中的双下降现象，为理解生存模型的泛化行为提供了新视角，强调了过拟合在生存分析中的风险，对生存模型的评估和选择具有重要指导意义。

Abstract: Classical statistical learning theory predicts a U-shaped relationship between test loss and model capacity, driven by the bias-variance trade-off. Recent advances in modern machine learning have revealed a more complex pattern, double-descent, in which test loss, after peaking near the interpolation threshold, decreases again as model capacity continues to grow. While this behavior has been extensively analyzed in regression and classification, its manifestation in survival analysis remains unexplored. This study investigates double-descent in four representative survival models: DeepSurv, PC-Hazard, Nnet-Survival, and N-MTLR. We rigorously define interpolation and finite-norm interpolation, two key characteristics of loss-based models to understand double-descent. We then show the existence (or absence) of (finite-norm) interpolation of all four models. Our findings clarify how likelihood-based losses and model implementation jointly determine the feasibility of interpolation and show that overfitting should not be regarded as benign for survival models. All theoretical results are supported by numerical experiments that highlight the distinct generalization behaviors of survival models.

</details>


### [567] [Iterative Sampling Methods for Sinkhorn Distributionally Robust Optimization](https://arxiv.org/abs/2512.12550)
*Jie Wang*

Main category: stat.ML

Relevance: 35.0

TL;DR: 本文从原始视角研究基于Sinkhorn散度的分布鲁棒优化，将其重新表述为双层规划问题，提出具有理论保证的双层和单层采样算法，可同时获得最优鲁棒决策和最坏情况分布。


<details>
  <summary>Details</summary>
Motivation: 现有Sinkhorn DRO研究主要从对偶视角出发，依赖损失函数有界性假设，且难以直接获得最坏情况分布。本文从原始视角出发，旨在同时获得最优鲁棒决策和最坏情况分布，这在压力测试场景生成和鲁棒学习算法设计等实际应用中具有重要价值。

Method: 将Sinkhorn DRO重新表述为概率空间上的双层规划问题，包含多个无限维下层子问题。提出两种采样算法：1）双层循环算法，2）单层循环算法，均提供理论保证。

Result: 通过对抗性分类的数值研究验证了方法的有效性。提出的算法能够同时获得最优鲁棒决策和最坏情况分布，解决了现有方法在理论分析和实际应用中的局限性。

Conclusion: 从原始视角研究Sinkhorn DRO提供了同时获得最优决策和最坏情况分布的新途径，提出的采样算法具有理论保证，在需要分布鲁棒性的实际应用中具有实用价值。

Abstract: Distributionally robust optimization (DRO) has emerged as a powerful paradigm for reliable decision-making under uncertainty. This paper focuses on DRO with ambiguity sets defined via the Sinkhorn discrepancy: an entropy-regularized Wasserstein distance, referred to as Sinkhorn DRO. Existing work primarily addresses Sinkhorn DRO from a dual perspective, leveraging its formulation as a conditional stochastic optimization problem, for which many stochastic gradient methods are applicable. However, the theoretical analyses of such methods often rely on the boundedness of the loss function, and it is indirect to obtain the worst-case distribution associated with Sinkhorn DRO. In contrast, we study Sinkhorn DRO from the primal perspective, by reformulating it as a bilevel program with several infinite-dimensional lower-level subproblems over probability space. This formulation enables us to simultaneously obtain the optimal robust decision and the worst-case distribution, which is valuable in practical settings, such as generating stress-test scenarios or designing robust learning algorithms. We propose both double-loop and single-loop sampling-based algorithms with theoretical guarantees to solve this bilevel program. Finally, we demonstrate the effectiveness of our approach through a numerical study on adversarial classification.

</details>


### [568] [Mind the Jumps: A Scalable Robust Local Gaussian Process for Multidimensional Response Surfaces with Discontinuities](https://arxiv.org/abs/2512.12574)
*Isaac Adjetey,Yiyuan She*

Main category: stat.ML

Relevance: 35.0

TL;DR: 提出RLGP模型，通过自适应最近邻选择和稀疏鲁棒机制处理非平稳、不连续响应曲面建模问题


<details>
  <summary>Details</summary>
Motivation: 传统高斯过程模型在平稳性假设下难以处理响应曲面中的突变和不连续性，现有非平稳扩展方法在边界一致性、异常值敏感性和高维可扩展性方面存在局限

Method: RLGP框架结合自适应最近邻选择和稀疏驱动鲁棒机制，采用基于优化的均值偏移调整，结合多元视角变换和局部邻域建模来减少异常值影响

Result: 在真实数据集上，RLGP在预测准确性方面表现优异，在突变和复杂响应结构场景中保持竞争力，高维可扩展性测试证实其稳定性和可靠性

Conclusion: RLGP为建模非平稳和不连续响应曲面提供了有效实用的解决方案，适用于广泛的应用场景

Abstract: Modeling response surfaces with abrupt jumps and discontinuities remains a major challenge across scientific and engineering domains. Although Gaussian process models excel at capturing smooth nonlinear relationships, their stationarity assumptions limit their ability to adapt to sudden input-output variations. Existing nonstationary extensions, particularly those based on domain partitioning, often struggle with boundary inconsistencies, sensitivity to outliers, and scalability issues in higher-dimensional settings, leading to reduced predictive accuracy and unreliable parameter estimation.
  To address these challenges, this paper proposes the Robust Local Gaussian Process (RLGP) model, a framework that integrates adaptive nearest-neighbor selection with a sparsity-driven robustification mechanism. Unlike existing methods, RLGP leverages an optimization-based mean-shift adjustment after a multivariate perspective transformation combined with local neighborhood modeling to mitigate the influence of outliers. This approach improves predictive accuracy near discontinuities while enhancing robustness to data heterogeneity.
  Comprehensive evaluations on real-world datasets show that RLGP consistently delivers high predictive accuracy and maintains competitive computational efficiency, especially in scenarios with sharp transitions and complex response structures. Scalability tests further confirm RLGP's stability and reliability in higher-dimensional settings, where other methods struggle. These results establish RLGP as an effective and practical solution for modeling nonstationary and discontinuous response surfaces across a wide range of applications.

</details>


### [569] [Practical Hybrid Quantum Language Models with Observable Readout on Real Hardware](https://arxiv.org/abs/2512.12710)
*Stefan Balauca,Ada-Astrid Balauca,Adrian Iftene*

Main category: quant-ph

Relevance: 35.0

TL;DR: 论文提出了量子循环神经网络(QRNNs)和量子卷积神经网络(QCNNs)作为混合量子语言模型，首次在真实量子硬件上实现了端到端的生成式语言建模训练和评估。


<details>
  <summary>Details</summary>
Motivation: 利用近期量子设备处理序列数据，为量子自然语言处理建立工程基准，验证在当前量子硬件上训练复杂序列模型的可行性。

Method: 结合硬件优化的参数化量子电路与轻量级经典投影层，使用多样本SPSA策略在硬件噪声下高效训练量子参数，引入合成数据集来隔离句法依赖关系。

Result: 在IBM量子处理器上的实验揭示了电路深度与可训练性之间的关键权衡，表明尽管噪声仍是重要因素，但基于可观测量的读出能够在NISQ设备上成功学习序列模式。

Conclusion: 这些结果为生成式量子自然语言处理建立了严格的工程基准，验证了在当前量子硬件上训练复杂序列模型的可行性。

Abstract: Hybrid quantum-classical models represent a crucial step toward leveraging near-term quantum devices for sequential data processing. We present Quantum Recurrent Neural Networks (QRNNs) and Quantum Convolutional Neural Networks (QCNNs) as hybrid quantum language models, reporting the first empirical demonstration of generative language modeling trained and evaluated end-to-end on real quantum hardware. Our architecture combines hardware-optimized parametric quantum circuits with a lightweight classical projection layer, utilizing a multi-sample SPSA strategy to efficiently train quantum parameters despite hardware noise. To characterize the capabilities of these models, we introduce a synthetic dataset designed to isolate syntactic dependencies in a controlled, low-resource environment. Experiments on IBM Quantum processors reveal the critical trade-offs between circuit depth and trainability, demonstrating that while noise remains a significant factor, observable-based readout enables the successful learning of sequential patterns on NISQ devices. These results establish a rigorous engineering baseline for generative quantum natural language processing, validating the feasibility of training complex sequence models on current quantum hardware.

</details>


### [570] [HaShiFlex: A High-Throughput Hardened Shifter DNN Accelerator with Fine-Tuning Flexibility](https://arxiv.org/abs/2512.12847)
*Jonathan Herbst,Michael Pellauer,Sherief Reda*

Main category: cs.AR

Relevance: 35.0

TL;DR: 提出一种高吞吐量神经网络加速器，通过硬件嵌入网络层和Po2量化实现高效推理，在MobileNetV2上相比GPU实现20-67倍加速


<details>
  <summary>Details</summary>
Motivation: 解决神经网络推理中的数据传输和内存使用问题，为边缘连续感知和数据中心部署提供高吞吐量、高能效的硬件加速方案

Method: 1) 将大多数网络层直接嵌入硬件，减少数据传输和内存使用；2) 使用Po2量化将乘法转换为加法；3) 保留可编程的最终分类层以保持微调能力

Result: 在7nm ASIC上实现，MobileNetV2推理吞吐量达121万图像/秒（相比GPU加速20倍），无微调需求时可达400万图像/秒（加速67倍）

Conclusion: 该硬件加速器架构在保持微调灵活性的同时实现了显著的吞吐量提升，特别适用于参数稳定的边缘计算和数据中心应用

Abstract: We introduce a high-throughput neural network accelerator that embeds most network layers directly in hardware, minimizing data transfer and memory usage while preserving a degree of flexibility via a small neural processing unit for the final classification layer. By leveraging power-of-two (Po2) quantization for weights, we replace multiplications with simple rewiring, effectively reducing each convolution to a series of additions. This streamlined approach offers high-throughput, energy-efficient processing, making it highly suitable for applications where model parameters remain stable, such as continuous sensing tasks at the edge or large-scale data center deployments. Furthermore, by including a strategically chosen reprogrammable final layer, our design achieves high throughput without sacrificing fine-tuning capabilities. We implement this accelerator in a 7nm ASIC flow using MobileNetV2 as a baseline and report throughput, area, accuracy, and sensitivity to quantization and pruning - demonstrating both the advantages and potential trade-offs of the proposed architecture. We find that for MobileNetV2, we can improve inference throughput by 20x over fully programmable GPUs, processing 1.21 million images per second through a full forward pass while retaining fine-tuning flexibility. If absolutely no post-deployment fine tuning is required, this advantage increases to 67x at 4 million images per second.

</details>


### [571] [KANELÉ: Kolmogorov-Arnold Networks for Efficient LUT-based Evaluation](https://arxiv.org/abs/2512.12850)
*Duc Hoang,Aarush Gupta,Philip Harris*

Main category: cs.AR

Relevance: 35.0

TL;DR: KANELÉ：首个系统化的KAN FPGA部署框架，通过量化剪枝协同优化，实现2700倍加速和资源大幅节省，在符号/物理公式任务上超越现有LUT架构。


<details>
  <summary>Details</summary>
Motivation: FPGA上低延迟、资源高效的神经网络推理对实时应用至关重要。传统LUT神经网络虽能高效部署，但KANs（Kolmogorov-Arnold Networks）因其可学习一维样条激活函数，天然适合离散化和LUT映射，为FPGA部署提供了独特优势。

Method: 提出KANELÉ框架，首次系统化设计KANs在FPGA上的部署流程。通过协同优化训练、量化和剪枝技术，实现紧凑、高吞吐、低延迟的KAN架构。利用KANs边缘激活函数的固定域特性，实现高效LUT映射。

Result: 相比现有KAN-on-FPGA方法，实现高达2700倍加速和数量级资源节省。在广泛使用的基准测试中，KANELÉ匹配或超越其他LUT架构，尤其在涉及符号或物理公式的任务上表现突出，同时平衡FPGA硬件资源使用。

Conclusion: KANELÉ展示了KANs在FPGA部署中的显著优势，为实时、高能效控制系统提供了通用框架，证明了KANs在硬件加速领域的巨大潜力。

Abstract: Low-latency, resource-efficient neural network inference on FPGAs is essential for applications demanding real-time capability and low power. Lookup table (LUT)-based neural networks are a common solution, combining strong representational power with efficient FPGA implementation. In this work, we introduce KANELÉ, a framework that exploits the unique properties of Kolmogorov-Arnold Networks (KANs) for FPGA deployment. Unlike traditional multilayer perceptrons (MLPs), KANs employ learnable one-dimensional splines with fixed domains as edge activations, a structure naturally suited to discretization and efficient LUT mapping. We present the first systematic design flow for implementing KANs on FPGAs, co-optimizing training with quantization and pruning to enable compact, high-throughput, and low-latency KAN architectures. Our results demonstrate up to a 2700x speedup and orders of magnitude resource savings compared to prior KAN-on-FPGA approaches. Moreover, KANELÉ matches or surpasses other LUT-based architectures on widely used benchmarks, particularly for tasks involving symbolic or physical formulas, while balancing resource usage across FPGA hardware. Finally, we showcase the versatility of the framework by extending it to real-time, power-efficient control systems.

</details>


### [572] [General OOD Detection via Model-aware and Subspace-aware Variable Priority](https://arxiv.org/abs/2512.13003)
*Min Lu,Hemant Ishwaran*

Main category: stat.ML

Relevance: 35.0

TL;DR: 提出一种用于回归和生存分析的OOD检测框架，结合模型感知和子空间感知，通过局部邻域构建和特征重要性加权来识别分布外样本。


<details>
  <summary>Details</summary>
Motivation: 虽然分类任务的OOD检测已被广泛研究，但回归和生存分析中的OOD检测仍然有限，主要挑战在于缺乏离散标签和难以量化预测不确定性。现有方法通常依赖全局距离度量或全特征密度估计，无法有效处理回归和生存分析中的分布偏移问题。

Method: 提出一个同时具备模型感知和子空间感知的框架，将变量优先级直接嵌入检测步骤。方法使用拟合的预测器为每个测试样本构建局部邻域，强调驱动模型学习关系的特征，并降低与预测相关性较低的方向的权重。该框架适用于多种结果类型，在实现中使用随机森林，其规则结构提供透明的邻域和有效的评分。

Result: 在合成和真实数据基准测试中，该方法在隔离功能偏移的实验上表现出优于现有方法的性能提升。在食管癌生存研究中，该方法识别出与淋巴结清扫相关的分布偏移模式，这些模式与手术指南相关。

Conclusion: 该框架为回归和生存分析中的OOD检测提供了一种有效方法，不依赖全局距离度量或全特征密度估计，能够识别与模型预测相关的分布偏移，在医疗应用等实际场景中具有实用价值。

Abstract: Out-of-distribution (OOD) detection is essential for determining when a supervised model encounters inputs that differ meaningfully from its training distribution. While widely studied in classification, OOD detection for regression and survival analysis remains limited due to the absence of discrete labels and the challenge of quantifying predictive uncertainty. We introduce a framework for OOD detection that is simultaneously model aware and subspace aware, and that embeds variable prioritization directly into the detection step. The method uses the fitted predictor to construct localized neighborhoods around each test case that emphasize the features driving the model's learned relationship and downweight directions that are less relevant to prediction. It produces OOD scores without relying on global distance metrics or estimating the full feature density. The framework is applicable across outcome types, and in our implementation we use random forests, where the rule structure yields transparent neighborhoods and effective scoring. Experiments on synthetic and real data benchmarks designed to isolate functional shifts show consistent improvements over existing methods. We further demonstrate the approach in an esophageal cancer survival study, where distribution shifts related to lymphadenectomy identify patterns relevant to surgical guidelines.

</details>


### [573] [Progressive Refinement of E-commerce Search Ranking Based on Short-Term Activities of the Buyer](https://arxiv.org/abs/2512.13037)
*Taoran Sheng,Sathappan Muthiah,Atiq Islam,Jinming Feng*

Main category: cs.IR

Relevance: 35.0

TL;DR: 该论文提出了一种在电子商务搜索中基于上下文动态调整搜索结果的方法，通过从基础启发式特征到先进序列模型的渐进式改进，显著提升了搜索排名器的性能。


<details>
  <summary>Details</summary>
Motivation: 电子商务搜索面临的主要挑战是如何在用户购物旅程中（从浏览到购买决策，或意图切换时）动态调整搜索结果以适应用户的即时需求和偏好。传统搜索系统难以捕捉用户意图的演变过程。

Method: 采用系统化的渐进式方法：从基础方法开始，逐步融入更多上下文信息和最先进技术。包括从简单的启发式自回归特征到高级序列模型的演进，将上下文框架应用于搜索结果页面(SERP)上的商品展示。

Result: 这种增量式改进显著提升了排名器性能。上下文技术的集成增强了生产排名器的表现，在离线和在线A/B测试中都改善了搜索结果，具体体现在平均倒数排名(MRR)指标的提升。

Conclusion: 论文详细阐述了迭代方法及其对电子商务平台搜索结果上下文化的重要贡献，证明了渐进式上下文适应框架在提升搜索相关性方面的有效性。

Abstract: In e-commerce shopping, aligning search results with a buyer's immediate needs and preferences presents a significant challenge, particularly in adapting search results throughout the buyer's shopping journey as they move from the initial stages of browsing to making a purchase decision or shift from one intent to another. This study presents a systematic approach to adapting e-commerce search results based on the current context. We start with basic methods and incrementally incorporate more contextual information and state-of-the-art techniques to improve the search outcomes. By applying this evolving contextual framework to items displayed on the search engine results page (SERP), we progressively align search outcomes more closely with the buyer's interests and current search intentions. Our findings demonstrate that this incremental enhancement, from simple heuristic autoregressive features to advanced sequence models, significantly improves ranker performance. The integration of contextual techniques enhances the performance of our production ranker, leading to improved search results in both offline and online A/B testing in terms of Mean Reciprocal Rank (MRR). Overall, the paper details iterative methodologies and their substantial contributions to search result contextualization on e-commerce platforms.

</details>


### [574] [PvP: Data-Efficient Humanoid Robot Learning with Proprioceptive-Privileged Contrastive Representations](https://arxiv.org/abs/2512.13093)
*Mingqi Yuan,Tao Yu,Haolin Song,Bo Li,Xin Jin,Hua Chen,Wenjun Zeng*

Main category: cs.RO

Relevance: 35.0

TL;DR: 提出PvP框架，利用本体感知与特权状态的内在互补性学习紧凑的任务相关潜在表示，无需手工数据增强，实现更快速稳定的策略学习，显著提升人形机器人控制的样本效率


<details>
  <summary>Details</summary>
Motivation: 解决人形机器人全身控制中强化学习的样本效率低下问题，由于人形机器人复杂的动力学特性和部分可观测性，需要更高效的学习方法

Method: 提出PvP（Proprioceptive-Privileged对比学习框架），利用本体感知状态与特权状态的内在互补性学习紧凑的任务相关潜在表示；开发SRL4Humanoid统一框架，提供高质量的状态表示学习方法实现

Result: 在LimX Oli机器人上的速度跟踪和运动模仿任务实验中，PvP相比基线SRL方法显著提高了样本效率和最终性能

Conclusion: PvP框架为人形机器人学习提供了数据高效的方法，为将状态表示学习与强化学习结合提供了实用见解

Abstract: Achieving efficient and robust whole-body control (WBC) is essential for enabling humanoid robots to perform complex tasks in dynamic environments. Despite the success of reinforcement learning (RL) in this domain, its sample inefficiency remains a significant challenge due to the intricate dynamics and partial observability of humanoid robots. To address this limitation, we propose PvP, a Proprioceptive-Privileged contrastive learning framework that leverages the intrinsic complementarity between proprioceptive and privileged states. PvP learns compact and task-relevant latent representations without requiring hand-crafted data augmentations, enabling faster and more stable policy learning. To support systematic evaluation, we develop SRL4Humanoid, the first unified and modular framework that provides high-quality implementations of representative state representation learning (SRL) methods for humanoid robot learning. Extensive experiments on the LimX Oli robot across velocity tracking and motion imitation tasks demonstrate that PvP significantly improves sample efficiency and final performance compared to baseline SRL methods. Our study further provides practical insights into integrating SRL with RL for humanoid WBC, offering valuable guidance for data-efficient humanoid robot learning.

</details>


### [575] [Towards Practical Large-scale Dynamical Heterogeneous Graph Embedding: Cold-start Resilient Recommendation](https://arxiv.org/abs/2512.13120)
*Mabiao Long,Jiaxi Liu,Yufeng Li,Hao Xiong,Junchi Yan,Kefan Wang,Yi Cao,Jiandong Ding*

Main category: cs.IR

Relevance: 35.0

TL;DR: 提出一个两阶段的动态异构图嵌入框架，结合可扩展的图Transformer（HetSGFormer）进行静态学习和轻量级增量更新算法（ILLE），解决生产环境中的可扩展性、数据新鲜度和冷启动问题。


<details>
  <summary>Details</summary>
Motivation: 在生产环境中部署动态异构图嵌入面临三个关键挑战：可扩展性（处理十亿级图）、数据新鲜度（实时更新）和冷启动问题（稀疏数据）。需要平衡深度图表示和低延迟增量更新。

Method: 两阶段框架：1) HetSGFormer - 基于图Transformer的静态学习模块，具有线性可扩展性，捕获全局图结构；2) ILLE - 增量局部线性嵌入算法，轻量级CPU实现，提供实时针对性更新，避免昂贵的全图重训练。

Result: 在十亿级图上，A/B测试显示：HetSGFormer比之前方法提升广告主价值6.11%，ILLE模块额外提升3.22%，嵌入刷新及时性提升83.2%。框架具有冷启动弹性，能从稀疏数据生成有意义的嵌入。

Conclusion: 该工作提供了一个经过验证的框架，用于在生产环境中部署动态图学习，成功平衡了深度表示学习和实时更新需求，解决了大规模动态图嵌入的实际部署挑战。

Abstract: Deploying dynamic heterogeneous graph embeddings in production faces key challenges of scalability, data freshness, and cold-start. This paper introduces a practical, two-stage solution that balances deep graph representation with low-latency incremental updates. Our framework combines HetSGFormer, a scalable graph transformer for static learning, with Incremental Locally Linear Embedding (ILLE), a lightweight, CPU-based algorithm for real-time updates. HetSGFormer captures global structure with linear scalability, while ILLE provides rapid, targeted updates to incorporate new data, thus avoiding costly full retraining. This dual approach is cold-start resilient, leveraging the graph to create meaningful embeddings from sparse data. On billion-scale graphs, A/B tests show HetSGFormer achieved up to a 6.11% lift in Advertiser Value over previous methods, while the ILLE module added another 3.22% lift and improved embedding refresh timeliness by 83.2%. Our work provides a validated framework for deploying dynamic graph learning in production environments.

</details>


### [576] [Stopping Rules for Stochastic Gradient Descent via Anytime-Valid Confidence Sequences](https://arxiv.org/abs/2512.13123)
*Liviu Aolaritei,Michael I. Jordan*

Main category: math.OC

Relevance: 35.0

TL;DR: 本文为凸优化的随机梯度下降（SGD）提出了基于任意时间有效置信序列的停止规则，提供了可观测轨迹量的时间均匀性能保证和有限时间ε最优性证明。


<details>
  <summary>Details</summary>
Motivation: 传统SGD分析只提供期望或固定时间点的收敛保证，无法在任意时间点统计有效地评估当前迭代点与最优解的接近程度。需要开发一种能够在任意时间点提供统计有效置信区间的停止规则。

Method: 通过非负超鞅构建任意时间有效的、数据依赖的上置信序列，用于投影SGD的加权平均次优性。该方法不需要平滑性或强凸性假设，仅基于可观测的轨迹量。

Result: 开发出简单的停止规则，以至少1-α的概率保证ε最优性，并且在标准随机逼近步长下几乎必然有限。这是首个基于可观测轨迹量为投影SGD提供严格时间均匀性能保证和有限时间ε最优性证明的方法。

Conclusion: 本文为凸优化的SGD提供了首个任意时间有效的停止规则框架，能够在任意时间点统计有效地评估收敛状态，为实际应用中的SGD停止决策提供了理论基础。

Abstract: We study stopping rules for stochastic gradient descent (SGD) for convex optimization from the perspective of anytime-valid confidence sequences. Classical analyses of SGD provide convergence guarantees in expectation or at a fixed horizon, but offer no statistically valid way to assess, at an arbitrary time, how close the current iterate is to the optimum. We develop an anytime-valid, data-dependent upper confidence sequence for the weighted average suboptimality of projected SGD, constructed via nonnegative supermartingales and requiring no smoothness or strong convexity. This confidence sequence yields a simple stopping rule that is provably $\varepsilon$-optimal with probability at least $1-α$ and is almost surely finite under standard stochastic approximation stepsizes. To the best of our knowledge, these are the first rigorous, time-uniform performance guarantees and finite-time $\varepsilon$-optimality certificates for projected SGD with general convex objectives, based solely on observable trajectory quantities.

</details>


### [577] [Rethinking Physics-Informed Regression Beyond Training Loops and Bespoke Architectures](https://arxiv.org/abs/2512.13217)
*Lorenzo Sabug,Eric Kerrigan*

Main category: math.OC

Relevance: 35.0

TL;DR: 该论文提出了一种基于约束优化的物理信息回归方法，直接计算预测点的状态及其导数/曲率信息，无需神经网络训练


<details>
  <summary>Details</summary>
Motivation: 解决物理信息回归问题，避免神经网络等全局函数逼近器需要大量训练的问题，提供更高效、鲁棒的解决方案

Method: 将每个预测构建为约束优化问题，利用多元泰勒级数展开并显式强制执行物理定律，每个查询可独立处理，无需预训练或再训练

Result: 在反应-扩散系统上的基准测试显示，与神经网络方法相比具有竞争力的预测精度，同时完全消除了长训练循环的需求，对采样布局变化保持鲁棒

Conclusion: 该方法为物理信息回归提供了一种高效、无需训练的替代方案，特别适用于需要快速推理和鲁棒性的场景

Abstract: We revisit the problem of physics-informed regression, and propose a method that directly computes the state at the prediction point, simultaneously with the derivative and curvature information of the existing samples. We frame each prediction as a constrained optimisation problem, leveraging multivariate Taylor series expansions and explicitly enforcing physical laws. Each individual query can be processed with low computational cost without any pre- or re-training, in contrast to global function approximator-based solutions such as neural networks. Our comparative benchmarks on a reaction-diffusion system show competitive predictive accuracy relative to a neural network-based solution, while completely eliminating the need for long training loops, and remaining robust to changes in the sampling layout.

</details>


### [578] [Better LMO-based Momentum Methods with Second-Order Information](https://arxiv.org/abs/2512.13227)
*Sarit Khirirat,Abdurakhmon Sadiev,Yury Demidovich,Peter Richtárik*

Main category: math.OC

Relevance: 35.0

TL;DR: 本文提出了一种在线性最小化Oracle框架中集成Hessian修正动量的新方法，在任意范数设置下实现了O(1/K^{1/3})的收敛速率，优于传统动量方法的O(1/K^{1/4})。


<details>
  <summary>Details</summary>
Motivation: 传统随机动量方法在LMO框架中收敛速率有限（O(1/K^{1/4})），而现有的改进方法如HCM通常局限于欧几里得范数设置，限制了在需要任意范数的问题中的应用。

Method: 将Hessian修正动量（HCM）集成到线性最小化Oracle框架中，在松弛平滑性和任意范数设置下提供收敛保证。

Result: 理论证明了HCM在任意范数下达到O(1/K^{1/3})的收敛速率，优于传统动量方法。在MLP和LSTM网络训练实验中验证了理论观察。

Conclusion: 提出的LMO-HCM框架在任意范数设置下实现了更快的收敛速率，能够适应问题的几何结构，为深度神经网络训练提供了更有效的优化方法。

Abstract: The use of momentum in stochastic optimization algorithms has shown empirical success across a range of machine learning tasks. Recently, a new class of stochastic momentum algorithms has emerged within the Linear Minimization Oracle (LMO) framework--leading to state-of-the-art methods, such as Muon, Scion, and Gluon, that effectively solve deep neural network training problems. However, traditional stochastic momentum methods offer convergence guarantees no better than the ${O}(1/K^{1/4})$ rate. While several approaches--such as Hessian-Corrected Momentum (HCM)--have aimed to improve this rate, their theoretical results are generally restricted to the Euclidean norm setting. This limitation hinders their applicability in problems, where arbitrary norms are often required. In this paper, we extend the LMO-based framework by integrating HCM, and provide convergence guarantees under relaxed smoothness and arbitrary norm settings. We establish improved convergence rates of ${O}(1/K^{1/3})$ for HCM, which can adapt to the geometry of the problem and achieve a faster rate than traditional momentum. Experimental results on training Multi-Layer Perceptrons (MLPs) and Long Short-Term Memory (LSTM) networks verify our theoretical observations.

</details>


### [579] [A Deep Learning Model of Mental Rotation Informed by Interactive VR Experiments](https://arxiv.org/abs/2512.13517)
*Raymond Khazoum,Daniela Fernandes,Aleksandr Krylov,Qin Li,Stephane Deny*

Main category: q-bio.NC

Relevance: 35.0

TL;DR: 提出一个结合等变神经网络、神经符号学习和循环决策机制的心理旋转计算模型，模拟人类从不同视角比较物体的空间推理能力


<details>
  <summary>Details</summary>
Motivation: 心理旋转是人类空间世界建模和心智模拟的基本能力，但缺乏机制性计算模型。研究旨在通过整合深度学习、等变表示和符号学习来构建一个能够解释人类心理旋转行为的计算模型

Method: 三层堆叠架构：1) 等变神经编码器（图像→3D空间表示）；2) 神经符号对象编码器（空间表示→符号描述）；3) 神经决策代理（通过循环路径在3D潜在空间中模拟旋转）。结合VR实验数据验证

Result: 模型能够很好地捕捉参与者在实验中的表现、反应时间和行为模式，与现有实验数据一致。通过系统消融实验验证了每个组件的必要性

Conclusion: 该工作展示了整合深度、等变和符号表示来建模人类心智的有效性，为人类空间推理的深度神经网络模型提供了新的实例

Abstract: Mental rotation -- the ability to compare objects seen from different viewpoints -- is a fundamental example of mental simulation and spatial world modelling in humans. Here we propose a mechanistic model of human mental rotation, leveraging advances in deep, equivariant, and neuro-symbolic learning. Our model consists of three stacked components: (1) an equivariant neural encoder, taking images as input and producing 3D spatial representations of objects, (2) a neuro-symbolic object encoder, deriving symbolic descriptions of objects from these spatial representations, and (3) a neural decision agent, comparing these symbolic descriptions to prescribe rotation simulations in 3D latent space via a recurrent pathway. Our model design is guided by the abundant experimental literature on mental rotation, which we complemented with experiments in VR where participants could at times manipulate the objects to compare, providing us with additional insights into the cognitive process of mental rotation. Our model captures well the performance, response times and behavior of participants in our and others' experiments. The necessity of each model component is shown through systematic ablations. Our work adds to a recent collection of deep neural models of human spatial reasoning, further demonstrating the potency of integrating deep, equivariant, and symbolic representations to model the human mind.

</details>


### [580] [Actively Learning Joint Contours of Multiple Computer Experiments](https://arxiv.org/abs/2512.13530)
*Shih-Ni Prim,Kevin R. Quinlan,Paul Hawkins,Jagadeesh Movva,Annie S. Booth*

Main category: stat.ME

Relevance: 35.0

TL;DR: 论文提出了一种"联合等高线定位"(jCL)方法，用于同时识别多个独立计算机实验达到预定响应值的输入配置，特别针对飞行器零扭矩稳定飞行条件的识别问题。


<details>
  <summary>Details</summary>
Motivation: 研究动机源于识别飞行器稳定飞行条件（零扭矩）的实际需求，这需要同时满足多个独立计算机实验的预定响应值。传统的单响应等高线定位方法无法有效处理多响应联合定位问题。

Method: 提出联合等高线定位(jCL)方案，在探索多个响应表面和利用等高线交点学习之间取得战略平衡。使用浅层和深层高斯过程代理模型，但方法适用于任何能提供后验预测分布的代理模型。

Result: jCL设计显著优于现有的单响应等高线定位策略，能够高效定位多个计算机实验的联合等高线，特别是在飞行器零扭矩稳定飞行条件的识别任务上表现优异。

Conclusion: 联合等高线定位方法为解决多响应计算机实验的联合设计优化问题提供了有效框架，在保持探索-利用平衡的同时提高了定位效率。

Abstract: Contour location$\unicode{x2014}$the process of sequentially training a surrogate model to identify the design inputs that result in a pre-specified response value from a single computer experiment$\unicode{x2014}$is a well-studied active learning problem. Here, we tackle a related but distinct problem: identifying the input configuration that returns pre-specified values of multiple independent computer experiments simultaneously. Motivated by computer experiments of the rotational torques acting upon a vehicle in flight, we aim to identify stable flight conditions which result in zero torque forces. We propose a "joint contour location" (jCL) scheme that strikes a strategic balance between exploring the multiple response surfaces while exploiting learning of the intersecting contours. We employ both shallow and deep Gaussian process surrogates, but our jCL procedure is applicable to any surrogate that can provide posterior predictive distributions. Our jCL designs significantly outperform existing (single response) CL strategies, enabling us to efficiently locate the joint contour of our motivating computer experiments.

</details>


### [581] [Universality of high-dimensional scaling limits of stochastic gradient descent](https://arxiv.org/abs/2512.13634)
*Reza Gheissari,Aukosh Jagannath*

Main category: stat.ML

Relevance: 35.0

TL;DR: 该论文研究了高维统计任务中随机梯度下降动态的普适性，发现在满足特定条件下（初始化向量和真实向量充分坐标去局域化），即使数据来自非高斯分布（只要前两阶矩匹配），其动态收敛到与高斯情况相同的ODE极限。


<details>
  <summary>Details</summary>
Motivation: 研究随机梯度下降在高维统计任务中的动态行为是否具有普适性，即是否依赖于数据分布的具体形式，还是仅依赖于分布的某些矩信息。

Method: 分析高维统计任务中有限族总结统计量在随机梯度下降下的演化，证明在维度、样本量趋于无穷且步长相称趋于零时，这些统计量的动态收敛到自治ODE，并研究这种收敛对数据分布的依赖性。

Result: 证明了ODE极限的普适性：当数据来自乘积测度的混合分布且前两阶矩匹配对应高斯分布，且初始化向量和真实向量充分坐标去局域化时，收敛仍然发生。同时提供了两个非普适性反例：初始化坐标对齐时ODE极限非普适，以及围绕ODE固定点的随机微分方程极限非普适。

Conclusion: 随机梯度下降在高维统计任务中的宏观动态（ODE极限）具有普适性，仅依赖于数据分布的前两阶矩和初始化/真实向量的坐标去局域化程度，但微观波动（SDE极限）不具有普适性。

Abstract: We consider statistical tasks in high dimensions whose loss depends on the data only through its projection into a fixed-dimensional subspace spanned by the parameter vectors and certain ground truth vectors. This includes classifying mixture distributions with cross-entropy loss with one and two-layer networks, and learning single and multi-index models with one and two-layer networks. When the data is drawn from an isotropic Gaussian mixture distribution, it is known that the evolution of a finite family of summary statistics under stochastic gradient descent converges to an autonomous ordinary differential equation (ODE), as the dimension and sample size go to $\infty$ and the step size goes to $0$ commensurately. Our main result is that these ODE limits are universal in that this convergence occurs even when the data is drawn from mixtures of product measures provided the first two moments match the corresponding Gaussian distribution and the initialization and ground truth vectors are sufficiently coordinate-delocalized. We complement this by proving two corresponding non-universality results. We provide a simple example where the ODE limits are non-universal if the initialization is coordinate aligned. We also show that the stochastic differential equation limits arising as fluctuations of the summary statistics around their ODE's fixed points are not universal.

</details>


### [582] [SEDULity: A Proof-of-Learning Framework for Distributed and Secure Blockchains with Efficient Useful Work](https://arxiv.org/abs/2512.13666)
*Weihang Cao,Mustafa Doger,Sennur Ulukus*

Main category: cs.CR

Relevance: 35.0

TL;DR: 提出SEDULity框架，将区块链工作量证明转向机器学习训练，实现安全、高效、分布式的有用工作量证明


<details>
  <summary>Details</summary>
Motivation: 传统工作量证明(PoW)能耗巨大，现有有用工作量证明(PoUW)和证明学习(PoL)方案在安全、去中心化或效率方面存在问题，需要设计既能高效训练ML模型又能保持区块链安全性的分布式框架

Method: 设计SEDULity框架：1) 将模板块编码到训练过程中；2) 设计难解易验的有用函数替代PoW谜题；3) 扩展到其他有用工作类型；4) 设计激励机制鼓励任务验证

Result: 理论证明理性矿工在合理参数下会诚实训练，仿真结果验证了框架性能和分析的有效性

Conclusion: SEDULity框架实现了安全、高效、分布式的机器学习训练，为区块链工作量证明提供了可持续的替代方案

Abstract: The security and decentralization of Proof-of-Work (PoW) have been well-tested in existing blockchain systems. However, its tremendous energy waste has raised concerns about sustainability. Proof-of-Useful-Work (PoUW) aims to redirect the meaningless computation to meaningful tasks such as solving machine learning (ML) problems, giving rise to the branch of Proof-of-Learning (PoL). While previous studies have proposed various PoLs, they all, to some degree, suffer from security, decentralization, or efficiency issues. In this paper, we propose a PoL framework that trains ML models efficiently while maintaining blockchain security in a fully distributed manner. We name the framework SEDULity, which stands for a Secure, Efficient, Distributed, and Useful Learning-based blockchain system. Specifically, we encode the template block into the training process and design a useful function that is difficult to solve but relatively easy to verify, as a substitute for the PoW puzzle. We show that our framework is distributed, secure, and efficiently trains ML models. We further demonstrate that the proposed PoL framework can be extended to other types of useful work and design an incentive mechanism to incentivize task verification. We show theoretically that a rational miner is incentivized to train fully honestly with well-designed system parameters. Finally, we present simulation results to demonstrate the performance of our framework and validate our analysis.

</details>


### [583] [Soft Decision Tree classifier: explainable and extendable PyTorch implementation](https://arxiv.org/abs/2512.11833)
*Reuben R Shamir*

Main category: cs.LG

Relevance: 30.0

TL;DR: 使用PyTorch实现软决策树(SDT)和短期记忆软决策树(SM-SDT)，在模拟和临床数据集上测试，可视化SDT以展示可解释性潜力，性能与XGBoost相当，优于传统方法。


<details>
  <summary>Details</summary>
Motivation: 开发具有可解释性的机器学习模型，特别是软决策树及其变体，用于临床数据分析，平衡模型性能与可解释性需求。

Method: 使用PyTorch实现软决策树(SDT)和短期记忆软决策树(SM-SDT)，在模拟和临床数据集上进行广泛测试，可视化SDT结构，并与XGBoost、随机森林、逻辑回归和传统决策树进行比较。

Result: SDT、SM-SDT和XGBoost在AUC指标上表现相似，均优于随机森林、逻辑回归和传统决策树。在临床数据集上，除传统决策树外，所有分类方法都产生可比的结果。

Conclusion: 软决策树及其变体在保持与XGBoost相当性能的同时，提供了更好的可解释性，适合需要透明决策过程的临床应用。

Abstract: We implemented a Soft Decision Tree (SDT) and a Short-term Memory Soft Decision Tree (SM-SDT) using PyTorch. The methods were extensively tested on simulated and clinical datasets. The SDT was visualized to demonstrate the potential for its explainability. SDT, SM-SDT, and XGBoost demonstrated similar area under the curve (AUC) values. These methods were better than Random Forest, Logistic Regression, and Decision Tree. The results on clinical datasets suggest that, aside from a decision tree, all tested classification methods yield comparable results.
  The code and datasets are available online on GitHub: https://github.com/KI-Research-Institute/Soft-Decision-Tree

</details>


### [584] [Noise-Resilient Quantum Aggregation on NISQ for Federated ADAS Learning](https://arxiv.org/abs/2512.13196)
*Chethana Prasad Kabgere,Sudarshan T S B*

Main category: cs.LG

Relevance: 30.0

TL;DR: 本文提出NR-QFL（噪声弹性量子联邦学习），一种混合量子-经典框架，用于车辆边缘计算中的ADAS系统，通过变分量子电路实现安全、低延迟的模型聚合，并在NISQ条件下保证有界误差收敛。


<details>
  <summary>Details</summary>
Motivation: 传统联邦学习在车辆网络中存在噪声、延迟和安全限制问题，需要更鲁棒、高效且能保护隐私的分布式学习框架。

Method: 使用变分量子电路编码模型参数为量子态，采用自适应门重参数化；基于量子熵的客户端选择和多服务器协调机制；在CPTP动态下保证有界误差收敛。

Result: 经验验证显示NR-QFL能稳定收敛，降低梯度方差，减少通信开销，并在受限边缘条件下增强噪声容忍度。

Conclusion: NR-QFL为量子增强的联邦学习建立了可扩展基础，实现了车辆边缘计算中安全、高效且动态稳定的ADAS智能。

Abstract: Advanced Driver Assistance Systems (ADAS) increasingly employ Federated Learning (FL) to collaboratively train models across distributed vehicular nodes while preserving data privacy. Yet, conventional FL aggregation remains susceptible to noise, latency, and security constraints inherent to real-time vehicular networks. This paper introduces Noise-Resilient Quantum Federated Learning (NR-QFL), a hybrid quantum-classical framework that enables secure, low-latency aggregation through variational quantum circuits (VQCs) operating under Noisy Intermediate-Scale Quantum (NISQ) conditions. The framework encodes model parameters as quantum states with adaptive gate reparameterization, ensuring bounded-error convergence and provable resilience under Completely Positive Trace-Preserving (CPTP) dynamics. NR-QFL employs quantum entropy-based client selection and multi-server coordination for fairness and stability. Empirical validation shows consistent convergence with reduced gradient variance, lower communication overhead, and enhanced noise tolerance under constrained edge conditions. The framework establishes a scalable foundation for quantum-enhanced federated learning, enabling secure, efficient, and dynamically stable ADAS intelligence at the vehicular edge.

</details>


### [585] [Towards a pretrained deep learning estimator of the Linfoot informational correlation](https://arxiv.org/abs/2512.12358)
*Stéphanie M. van den Berg,Ulrich Halekoh,Sören Möller,Andreas Kryger Jensen,Jacob von Bornemann Hjelmborg*

Main category: stat.ML

Relevance: 30.0

TL;DR: 提出一种基于监督深度学习的互信息估计方法，使用Linfoot信息相关性作为标签，在Gaussian和Clayton copulas上训练，相比核密度、k近邻和神经估计器具有更低偏差和方差。


<details>
  <summary>Details</summary>
Motivation: 需要开发更准确、稳定的互信息估计方法。现有方法如核密度估计、k近邻估计和神经估计器在偏差和方差方面存在不足，特别是在连续随机变量的互信息估计上。

Method: 监督深度学习框架：1) 使用Linfoot信息相关性（互信息的变换形式）作为标签；2) 基于Gaussian和Clayton copulas生成训练数据；3) 构建深度神经网络模型学习互信息估计；4) 与核密度估计、k近邻估计和神经估计器进行对比。

Result: 提出的方法在互信息估计任务上表现出更低的偏差和方差，优于对比的基准方法。证明了监督深度学习在互信息估计中的有效性。

Conclusion: 监督深度学习为互信息估计提供了有前景的新方法，未来可扩展到更多copula类型以获得更通用的估计器。

Abstract: We develop a supervised deep-learning approach to estimate mutual information between two continuous random variables. As labels, we use the Linfoot informational correlation, a transformation of mutual information that has many important properties. Our method is based on ground truth labels for Gaussian and Clayton copulas. We compare our method with estimators based on kernel density, k-nearest neighbours and neural estimators. We show generally lower bias and lower variance. As a proof of principle, future research could look into training the model with a more diverse set of examples from other copulas for which ground truth labels are available.

</details>


### [586] [ElasticVR: Elastic Task Computing in Multi-User Multi-Connectivity Wireless Virtual Reality (VR) Systems](https://arxiv.org/abs/2512.12366)
*Babak Badnava,Jacob Chakareski,Morteza Hashemi*

Main category: cs.IT

Relevance: 30.0

TL;DR: ElasticVR框架通过可扩展的360度视频分块和多连接边缘计算，结合多智能体深度强化学习优化VR任务的弹性卸载，提升QoE并降低能耗。


<details>
  <summary>Details</summary>
Motivation: 高保真360度视频流在VR应用中需要大量计算和带宽资源。现有VR计算任务缺乏弹性，无法根据用户和系统资源动态调整计算和数据速率。需要一种能够平衡通信、计算、能耗和用户体验质量的弹性VR计算卸载方案。

Method: 提出ElasticVR框架：1) 集成可扩展的360度视频分块技术；2) 采用边缘-客户端无线多连接架构；3) 提出两种多智能体深度强化学习算法：CPPG（集中式训练与执行）和IPPG（集中式训练与分布式执行），用于优化多用户多连接环境中的弹性任务卸载。

Result: 相比无弹性VR计算方案，ElasticVR框架将PSNR提升43.21%，响应时间降低42.35%，能耗降低56.83%。IPPG算法在保持性能的同时，通过分布式执行降低了通信和计算开销，提高了可扩展性。

Conclusion: ElasticVR框架通过弹性VR计算卸载和多智能体强化学习，有效平衡了通信、计算、能耗和用户体验的权衡，为高保真360度VR视频流提供了可扩展的优化解决方案。

Abstract: Diverse emerging VR applications integrate streaming of high fidelity 360 video content that requires ample amounts of computation and data rate. Scalable 360 video tiling enables having elastic VR computational tasks that can be scaled adaptively in computation and data rate based on the available user and system resources. We integrate scalable 360 video tiling in an edge-client wireless multi-connectivity architecture for joint elastic task computation offloading across multiple VR users called ElasticVR. To balance the trade-offs in communication, computation, energy consumption, and QoE that arise herein, we formulate a constrained QoE and energy optimization problem that integrates the multi-user/multi-connectivity action space with the elasticity of VR computational tasks. The ElasticVR framework introduces two multi-agent deep reinforcement learning solutions, namely CPPG and IPPG. CPPG adopts a centralized training and centralized execution approach to capture the coupling between users' communication and computational demands. This leads to globally coordinated decisions at the cost of increased computational overheads and limited scalability. To address the latter challenges, we also explore an alternative strategy denoted IPPG that adopts a centralized training with decentralized execution paradigm. IPPG leverages shared information and parameter sharing to learn robust policies; however, during execution, each user takes action independently based on its local state information only. The decentralized execution alleviates the communication and computation overhead of centralized decision-making and improves scalability. We show that the ElasticVR framework improves the PSNR by 43.21%, while reducing the response time and energy consumption by 42.35% and 56.83%, respectively, compared with a case where no elasticity is incorporated into VR computations.

</details>


### [587] [TopicProphet: Prophesies on Temporal Topic Trends and Stocks](https://arxiv.org/abs/2512.11857)
*Olivia Kim*

Main category: cs.LG

Relevance: 25.0

TL;DR: TopicProphet：通过主题建模、时序分析和断点检测来识别相似历史时期，优化股票预测训练数据的框架


<details>
  <summary>Details</summary>
Motivation: 股票预测一直面临挑战，因为量化股票数据缺乏因果逻辑，且市场快速变化导致训练数据不足。传统方法关注关键词和情感影响，但未能解决历史时期相似性识别和训练数据优化问题。

Method: 提出TopicProphet框架，包含主题建模、时序分析、断点检测和分段优化，通过分析公众情感趋势和历史背景识别相似历史时期，确定最佳训练时间段。

Result: 相比现有方法，TopicProphet在捕捉金融百分比变化预测的最佳训练数据方面表现更优，解决了相关股票数据短缺问题。

Conclusion: 通过识别具有相似社会经济和政治背景的历史时期，TopicProphet能够为模型提供更精细的模式，从而改善股票预测性能。

Abstract: Stocks can't be predicted. Despite many hopes, this premise held itself true for many years due to the nature of quantitative stock data lacking causal logic along with rapid market changes hindering accumulation of significant data for training models. To undertake this matter, we propose a novel framework, TopicProphet, to analyze historical eras that share similar public sentiment trends and historical background. Our research deviates from previous studies that identified impacts of keywords and sentiments - we expand on that method by a sequence of topic modeling, temporal analysis, breakpoint detection and segment optimization to detect the optimal time period for training. This results in improving predictions by providing the model with nuanced patterns that occur from that era's socioeconomic and political status while also resolving the shortage of pertinent stock data to train on. Through extensive analysis, we conclude that TopicProphet produces improved outcomes compared to the state-of-the-art methods in capturing the optimal training data for forecasting financial percentage changes.

</details>


### [588] [Adaptive Path Integral Diffusion: AdaPID](https://arxiv.org/abs/2512.11858)
*Michael Chertkov,Hamidreza Behjoo*

Main category: cs.LG

Relevance: 25.0

TL;DR: 本文提出了一种路径式调度选择框架，用于具有时变刚度的Harmonic PID扩散采样器，采用分段常数参数化和分层细化，并引入了调度敏感的质量采样诊断指标。


<details>
  <summary>Details</summary>
Motivation: 扩散采样器（基于分数的扩散、桥扩散和路径积分扩散）在终端时间匹配目标分布，但真正的优势在于控制中间时间动态的调度选择。现有方法缺乏系统化的调度优化框架。

Method: 开发了路径式调度选择框架，采用分段常数参数化和简单分层细化。引入了调度敏感的质量采样诊断指标。对于高斯混合目标，保留了闭式格林函数比和数值稳定的神经网络自由预测状态映射和分数函数。

Result: 在2D实验中，QoS驱动的PWC调度在固定积分预算下，持续改善了早期退出保真度、尾部精度、动态条件化和分离（标签选择）时机。

Conclusion: 调度选择对扩散采样器的性能至关重要，提出的QoS驱动的PWC调度框架能有效优化采样质量，特别是在复杂目标分布下。

Abstract: Diffusion-based samplers -- Score Based Diffusions, Bridge Diffusions and Path Integral Diffusions -- match a target at terminal time, but the real leverage comes from choosing the schedule that governs the intermediate-time dynamics. We develop a path-wise schedule -- selection gramework for Harmonic PID with a time-varying stiffness, exploiting Piece-Wise-Constant(PWC) parametrizations and a simple hierarchical refinement. We introduce schedule-sensitive Quality-of-Sampling (QoS) diagnostics. Assuming a Gaussian-Mixture (GM) target, we retain closed-form Green functions' ration and numerically stable, Neural-Network free oracles for predicted-state maps and score. Experiments in 2D show that QoS driven PWC schedules consistently improve early-exit fidelity, tail accuracy, conditioning of the dynamics, and speciation (label-selection) timing at fixed integration budgets.

</details>


### [589] [Physics-informed neural networks to solve inverse problems in unbounded domains](https://arxiv.org/abs/2512.12074)
*Gregorio Pérez-Bernal,Oscar Rincón-Cardeño,Silvana Montoya-Noguera,Nicolás Guarín-Zapata*

Main category: cs.LG

Relevance: 25.0

TL;DR: 提出一种解决无限/半无限域逆问题的新方法，结合负指数/正态分布采样策略和双网络架构，比较PINNs和PIKANs性能，发现PINNs在精度和计算效率上更优


<details>
  <summary>Details</summary>
Motivation: 解决无限和半无限域中的逆问题，传统方法需要显式施加边界条件。研究旨在开发无需显式边界条件的方法，只要解在离开感兴趣域时趋于稳定即可

Method: 提出新颖采样策略（使用负指数和正态分布）和双网络架构，同时学习方程的解和参数。使用相同的损失函数训练两个网络，在PINNs和PIKANs上实现该方法

Result: PINNs在该设置下表现更优：比PIKANs快1000倍求解逆问题，相对误差更小，精度在同一数量级但更高，计算效率更佳

Conclusion: 对于无限/半无限域逆问题，PINNs比PIKANs提供更准确、计算效率更高的解决方案，尽管PIKANs在某些问题中具有更好的可解释性

Abstract: Inverse problems are extensively studied in applied mathematics, with applications ranging from acoustic tomography for medical diagnosis to geophysical exploration. Physics informed neural networks (PINNs) have emerged as a powerful tool for solving such problems, while Physics informed Kolmogorov Arnold networks (PIKANs) represent a recent benchmark that, in certain problems, promises greater interpretability and accuracy compared to PINNs, due to their nature, being constructed as a composition of polynomials. In this work, we develop a methodology for addressing inverse problems in infinite and semi infinite domains. We introduce a novel sampling strategy for the network's training points, using the negative exponential and normal distributions, alongside a dual network architecture that is trained to learn the solution and parameters of an equation with the same loss function. This design enables the solution of inverse problems without explicitly imposing boundary conditions, as long as the solutions tend to stabilize when leaving the domain of interest. The proposed architecture is implemented using both PINNs and PIKANs, and their performance is compared in terms of accuracy with respect to a known solution as well as computational time and response to a noisy environment. Our results demonstrate that, in this setting, PINNs provide a more accurate and computationally efficient solution, solving the inverse problem 1,000 times faster and in the same order of magnitude, yet with a lower relative error than PIKANs.

</details>


### [590] [Optimized Learned Count-Min Sketch](https://arxiv.org/abs/2512.12252)
*Kyosuke Nishishita,Atsuki Sato,Yusuke Matsui*

Main category: cs.LG

Relevance: 25.0

TL;DR: 提出OptLCMS优化学习型Count-Min Sketch，通过分区和动态规划优化阈值，提供理论保证并加快构建速度


<details>
  <summary>Details</summary>
Motivation: 传统学习型Count-Min Sketch（LCMS）存在构建速度慢、缺乏理论保证的问题，需要优化以提供更好的理论支持和实际性能

Method: 将输入域分区并为每个分区分配独立的CMS实例，通过分析推导CMS参数，使用动态规划优化阈值，并加入近似可行性检查

Result: OptLCMS构建速度更快，不可容忍错误概率更低，估计精度与LCMS相当，同时提供理论保证和显式错误阈值控制

Conclusion: OptLCMS在保持LCMS精度的同时，提供了理论保证、更快的构建速度和更好的灵活性，是学习型Count-Min Sketch的有效改进

Abstract: Count-Min Sketch (CMS) is a memory-efficient data structure for estimating the frequency of elements in a multiset. Learned Count-Min Sketch (LCMS) enhances CMS with a machine learning model to reduce estimation error under the same memory usage, but suffers from slow construction due to empirical parameter tuning and lacks theoretical guarantees on intolerable error probability. We propose Optimized Learned Count-Min Sketch (OptLCMS), which partitions the input domain and assigns each partition to its own CMS instance, with CMS parameters analytically derived for fixed thresholds, and thresholds optimized via dynamic programming with approximate feasibility checks. This reduces the need for empirical validation, enabling faster construction while providing theoretical guarantees under these assumptions. OptLCMS also allows explicit control of the allowable error threshold, improving flexibility in practice. Experiments show that OptLCMS builds faster, achieves lower intolerable error probability, and matches the estimation accuracy of LCMS.

</details>


### [591] [Balancing Accuracy and Speed: A Multi-Fidelity Ensemble Kalman Filter with a Machine Learning Surrogate Model](https://arxiv.org/abs/2512.12276)
*Jeffrey van der Voort,Martin Verlaan,Hanne Kekkonen*

Main category: cs.LG

Relevance: 25.0

TL;DR: 提出一种多保真度集合卡尔曼滤波方法，将机器学习代理模型作为低保真度模型，结合少量昂贵的高保真模型运行和大量便宜的ML模型运行，在相同计算预算下提高预测精度。


<details>
  <summary>Details</summary>
Motivation: 传统物理模型计算昂贵，而机器学习代理模型虽然便宜但精度较低。希望结合两者的优势，在相同计算预算下提高预测精度，特别是在气象学和海洋学等领域。

Method: 使用多保真度集合卡尔曼滤波，将ML代理模型作为低保真度模型，与少量昂贵的高保真物理模型结合。在Lorenz-2005模型和准地转模型上进行测试验证。

Result: 相比完全用ML模型替代，保留原始物理模型可获得更高精度。在相同计算预算下，MF-EnKF达到改进的精度。ML代理模型相比低分辨率模型有相似或更好的精度，且能提供更大的加速。

Conclusion: 该方法通过增加EnKF中的有效集合大小，改善了初始条件估计，提高了气象学和海洋学等领域的预测精度，为计算昂贵的物理模型提供了有效的替代方案。

Abstract: Currently, more and more machine learning (ML) surrogates are being developed for computationally expensive physical models. In this work we investigate the use of a Multi-Fidelity Ensemble Kalman Filter (MF-EnKF) in which the low-fidelity model is such a machine learning surrogate model, instead of a traditional low-resolution or reduced-order model. The idea behind this is to use an ensemble of a few expensive full model runs, together with an ensemble of many cheap but less accurate ML model runs. In this way we hope to reach increased accuracy within the same computational budget. We investigate the performance by testing the approach on two common test problems, namely the Lorenz-2005 model and the Quasi-Geostrophic model. By keeping the original physical model in place, we obtain a higher accuracy than when we completely replace it by the ML model. Furthermore, the MF-EnKF reaches improved accuracy within the same computational budget. The ML surrogate has similar or improved accuracy compared to the low-resolution one, but it can provide a larger speed-up. Our method contributes to increasing the effective ensemble size in the EnKF, which improves the estimation of the initial condition and hence accuracy of the predictions in fields such as meteorology and oceanography.

</details>


### [592] [Learning Dynamics in Memristor-Based Equilibrium Propagation](https://arxiv.org/abs/2512.12428)
*Michael Döll,Andreas Müller,Bernd Ulmann*

Main category: cs.LG

Relevance: 25.0

TL;DR: 该论文研究基于忆阻器的内存计算中非线性权重更新对平衡传播训练神经网络收敛行为的影响，发现忆阻器需要至少一个数量级的电阻范围才能实现稳健收敛。


<details>
  <summary>Details</summary>
Motivation: 研究忆阻器内存计算中非线性权重更新对神经网络训练收敛的影响，解决传统冯·诺依曼架构的内存瓶颈问题，探索更节能的并行计算方案。

Method: 使用六个忆阻器模型，通过电压-电流迟滞特性表征，集成到EBANA框架中，在两个基准分类任务上评估平衡传播算法在非线性权重更新下的收敛行为。

Result: 平衡传播算法在非线性权重更新下能够实现稳健收敛，前提是忆阻器具有足够宽的电阻范围（至少一个数量级）。

Conclusion: 基于忆阻器的内存计算在满足电阻范围要求时，能够支持平衡传播算法的有效训练，为克服传统计算架构瓶颈提供了有前景的解决方案。

Abstract: Memristor-based in-memory computing has emerged as a promising paradigm to overcome the constraints of the von Neumann bottleneck and the memory wall by enabling fully parallelisable and energy-efficient vector-matrix multiplications. We investigate the effect of nonlinear, memristor-driven weight updates on the convergence behaviour of neural networks trained with equilibrium propagation (EqProp). Six memristor models were characterised by their voltage-current hysteresis and integrated into the EBANA framework for evaluation on two benchmark classification tasks. EqProp can achieve robust convergence under nonlinear weight updates, provided that memristors exhibit a sufficiently wide resistance range of at least an order of magnitude.

</details>


### [593] [AI-Driven Early Warning Systems for Student Success: Discovering Static Feature Dominance in Temporal Prediction Models](https://arxiv.org/abs/2512.12493)
*Vaarunay Kaushal,Rajib Mall*

Main category: cs.LG

Relevance: 25.0

TL;DR: 该研究比较了决策树和LSTM模型在在线学习环境中预测风险学生的表现，发现不同干预阶段需要不同的性能指标：早期需要高召回率，中期需要平衡的精度-召回率，后期需要高精度。


<details>
  <summary>Details</summary>
Motivation: 在线学习环境中早期识别风险学生对于有效干预至关重要。研究旨在探索不同时间点（课程持续时间的50%）的预测模型性能，以确定最佳干预时机和模型选择策略。

Method: 研究在六个时间快照（包括第20周，即课程持续时间的50%）上比较决策树和LSTM模型。分析静态人口统计特征的重要性，并评估不同干预阶段的模型性能指标。

Result: 1. 静态人口统计特征占预测重要性的68%，支持无评估的早期预测
2. LSTM在第2周达到97%召回率，适合早期干预
3. 决策树在中期提供稳定的平衡性能（78%准确率）
4. 到第20周，两种模型召回率相似（68%），但LSTM精度更高（90% vs 86%）
5. 不同阶段需要不同性能指标：早期高召回率，中期平衡精度-召回率，后期高精度

Conclusion: 模型选择应取决于干预时机，早期信号（第2-4周）足以使用人口统计和入学前信息进行可靠预测。不同干预阶段需要不同的性能指标优化策略。

Abstract: Early identification of at-risk students is critical for effective intervention in online learning environments. This study extends temporal prediction analysis to Week 20 (50% of course duration), comparing Decision Tree and Long Short- Term Memory (LSTM) models across six temporal snapshots. Our analysis reveals that different performance metrics matter at different intervention stages: high recall is critical for early intervention (Weeks 2-4), while balanced precision-recall is important for mid-course resource allocation (Weeks 8-16), and high precision becomes paramount in later stages (Week 20). We demonstrate that static demographic features dominate predictions (68% importance), enabling assessment-free early prediction. The LSTM model achieves 97% recall at Week 2, making it ideal for early intervention, while Decision Tree provides stable balanced performance (78% accuracy) during mid-course. By Week 20, both models converge to similar recall (68%), but LSTM achieves higher precision (90% vs 86%). Our findings also suggest that model selection should depend on intervention timing, and that early signals (Weeks 2-4) are sufficient for reliable initial prediction using primarily demographic and pre-enrollment information.

</details>


### [594] [Policy Optimization for Dynamic Heart Transplant Allocation](https://arxiv.org/abs/2512.12497)
*Ioannis Anagnostides,Zachary W. Sollie,Arman Kilic,Tuomas Sandholm*

Main category: cs.LG

Relevance: 25.0

TL;DR: 本文开发了一个心脏移植分配策略模拟器，发现现行政策不如最大化移植获益年数的近视策略，并提出基于潜力的动态分配改进方法


<details>
  <summary>Details</summary>
Motivation: 心脏移植是晚期心衰患者的可行治疗路径，但供体严重短缺。当前分配政策（2018年修订）未能充分考虑移植前后的死亡率，需要改进

Method: 使用UNOS历史数据开发模拟器评估不同分配策略；比较现行政策与近视策略（最大化移植获益年数）；引入潜力概念衡量患者在未来分配中的效用；探索批量分配供体的效果；评估地理邻近性和移植中心拒绝倾向等关键因素

Result: 现行分配政策明显劣于近视策略；基于潜力的动态分配策略能进一步提升性能；批量分配少量供体可增强效果；模拟器能有效评估地理邻近性和拒绝倾向等关键因素

Conclusion: 通过开发模拟器和引入潜力概念，本文为改进心脏移植分配政策提供了重要工具和方法，能够更好地平衡移植前后的死亡率考虑

Abstract: Heart transplantation is a viable path for patients suffering from advanced heart failure, but this lifesaving option is severely limited due to donor shortage. Although the current allocation policy was recently revised in 2018, a major concern is that it does not adequately take into account pretransplant and post-transplant mortality. In this paper, we take an important step toward addressing these deficiencies.
  To begin with, we use historical data from UNOS to develop a new simulator that enables us to evaluate and compare the performance of different policies. We then leverage our simulator to demonstrate that the status quo policy is considerably inferior to the myopic policy that matches incoming donors to the patient who maximizes the number of years gained by the transplant. Moreover, we develop improved policies that account for the dynamic nature of the allocation process through the use of potentials -- a measure of a patient's utility in future allocations that we introduce. We also show that batching together even a handful of donors -- which is a viable option for a certain type of donors -- further enhances performance. Our simulator also allows us to evaluate the effect of critical, and often unexplored, factors in the allocation, such as geographic proximity and the tendency to reject offers by the transplant centers.

</details>


### [595] [Empirical Mode Decomposition and Graph Transformation of the MSCI World Index: A Multiscale Topological Analysis for Graph Neural Network Modeling](https://arxiv.org/abs/2512.12526)
*Agustín M. de los Riscos,Julio E. Sandubete,Diego Carmona-Fernández,León Beleña*

Main category: cs.LG

Relevance: 25.0

TL;DR: 该研究将经验模态分解应用于MSCI世界指数，将分解得到的本征模态函数转换为图表示，以便用图神经网络建模。通过四种时间序列到图的转换方法分析不同频率分量的拓扑结构特征。


<details>
  <summary>Details</summary>
Motivation: 金融时间序列预测面临复杂性和非平稳性的挑战，传统方法难以有效捕捉多尺度特征。研究旨在通过将经验模态分解与图神经网络结合，利用分解后的多尺度分量结构特性，为金融时间序列预测提供更有效的建模方法。

Method: 使用CEEMDAN方法将MSCI世界指数分解为9个本征模态函数，涵盖从高频波动到长期趋势。采用四种时间序列到图的转换方法：自然可见性图、水平可见性图、递归图和转移图。对生成的图进行拓扑分析，研究不同频率分量的结构特性。

Result: 拓扑分析显示明显的尺度依赖结构：高频IMF产生密集、高度连接的小世界图，而低频IMF产生更稀疏、特征路径长度更长的网络。可见性方法对振幅变化更敏感，通常产生更高的聚类系数；递归图能更好地保留时间依赖性。

Conclusion: 研究结果为针对分解分量结构特性设计专门的图神经网络架构提供了指导，支持更有效的金融时间序列预测建模。不同转换方法适用于捕捉时间序列的不同方面特征。

Abstract: This study applies Empirical Mode Decomposition (EMD) to the MSCI World index and converts the resulting intrinsic mode functions (IMFs) into graph representations to enable modeling with graph neural networks (GNNs). Using CEEMDAN, we extract nine IMFs spanning high-frequency fluctuations to long-term trends. Each IMF is transformed into a graph using four time-series-to-graph methods: natural visibility, horizontal visibility, recurrence, and transition graphs. Topological analysis shows clear scale-dependent structure: high-frequency IMFs yield dense, highly connected small-world graphs, whereas low-frequency IMFs produce sparser networks with longer characteristic path lengths. Visibility-based methods are more sensitive to amplitude variability and typically generate higher clustering, while recurrence graphs better preserve temporal dependencies. These results provide guidance for designing GNN architectures tailored to the structural properties of decomposed components, supporting more effective predictive modeling of financial time series.

</details>


### [596] [Torch Geometric Pool: the Pytorch library for pooling in Graph Neural Networks](https://arxiv.org/abs/2512.12642)
*Filippo Maria Bianchi,Carlo Abate,Ivan Marisca*

Main category: cs.LG

Relevance: 25.0

TL;DR: Torch Geometric Pool (tgp) 是一个基于 PyTorch Geometric 的图神经网络层次池化库，提供统一的API和模块化设计，支持多种池化算子，并包含预计算池化等加速训练的特性。


<details>
  <summary>Details</summary>
Motivation: 图神经网络中的池化操作对于处理图结构数据至关重要，但目前缺乏一个统一、易用且可扩展的池化库。不同的池化算子在不同任务和数据集上表现各异，需要一个能够快速原型设计的库来支持研究人员和开发者。

Method: 基于 PyTorch Geometric 构建，提供模块化设计和统一的 API。包含多种池化算子，并实现了预计算池化功能以加速训练。通过系统化的基准测试来评估不同池化算子的性能。

Result: 基准测试展示了库的功能特性，并系统比较了不同图池化方法在下游任务中的性能。结果表明最优池化算子的选择取决于具体任务和数据，验证了快速原型设计库的必要性。

Conclusion: tgp 库为图神经网络池化操作提供了一个统一、易用且可扩展的平台，支持研究人员快速实验不同的池化策略，有助于推动图神经网络领域的发展。

Abstract: We introduce Torch Geometric Pool (tgp), a library for hierarchical pooling in Graph Neural Networks. Built upon Pytorch Geometric, Torch Geometric Pool (tgp) provides a wide variety of pooling operators, unified under a consistent API and a modular design. The library emphasizes usability and extensibility, and includes features like precomputed pooling, which significantly accelerate training for a class of operators. In this paper, we present tgp's structure and present an extensive benchmark. The latter showcases the library's features and systematically compares the performance of the implemented graph-pooling methods in different downstream tasks. The results, showing that the choice of the optimal pooling operator depends on tasks and data at hand, support the need for a library that enables fast prototyping.

</details>


### [597] [Multi-Trajectory Physics-Informed Neural Networks for HJB Equations with Hard-Zero Terminal Inventory: Optimal Execution on Synthetic & SPY Data](https://arxiv.org/abs/2512.12708)
*Anthime Valin*

Main category: cs.LG

Relevance: 25.0

TL;DR: 提出MT-PINN方法解决最优交易执行中的硬零终端库存约束问题，通过轨迹损失和终端惩罚直接强制零库存，在理论模型和实际数据上验证有效性。


<details>
  <summary>Details</summary>
Motivation: 传统PINN方法在处理最优交易执行中的硬零终端库存约束时存在不足，容易产生不稳定的控制策略，需要更有效的方法来强制终端库存为零。

Method: 提出多轨迹PINN（MT-PINN），添加基于滚动的轨迹损失，通过时间反向传播传播终端惩罚，直接强制零终端库存。采用轻量级lambda课程学习稳定训练。

Result: 在Gatheral-Schied单资产模型中，MT-PINN与闭式解高度一致，终端库存紧密围绕零分布，沿最优路径误差降低。在SPY日内数据上，风险中性时匹配TWAP，高风险厌恶时获得更低暴露和竞争性成本。

Conclusion: MT-PINN能有效处理硬零终端库存约束，在理论和实际交易场景中均表现良好，为最优交易执行提供了新的神经网络解决方案。

Abstract: We study optimal trade execution with a hard-zero terminal inventory constraint, modeled via Hamilton-Jacobi-Bellman (HJB) equations. Vanilla PINNs often under-enforce this constraint and produce unstable controls. We propose a Multi-Trajectory PINN (MT-PINN) that adds a rollout-based trajectory loss and propagates a terminal penalty on terminal inventory via backpropagation-through-time, directly enforcing zero terminal inventory. A lightweight lambda-curriculum is adopted to stabilize training as the state expands from a risk-neutral reduced HJB to a risk-averse HJB. On the Gatheral-Schied single-asset model, MT-PINN aligns closely with their derived closed-form solutions and concentrates terminal inventory tightly around zero while reducing errors along optimal paths. We apply MT-PINNs on SPY intraday data, matching TWAP when risk-neutral, and achieving lower exposure and competitive costs, especially in falling windows, for higher risk-aversion.

</details>


### [598] [OLR-WAA: Adaptive and Drift-Resilient Online Regression with Dynamic Weighted Averaging](https://arxiv.org/abs/2512.12779)
*Mohammad Abu-Shaira,Weishi Shi*

Main category: cs.LG

Relevance: 25.0

TL;DR: OLR-WAA是一种无超参数的在线回归模型，通过动态加权平均和漂移检测机制，在非平稳数据流中实现自适应学习，平衡模型稳定性与适应性。


<details>
  <summary>Details</summary>
Motivation: 现实世界数据集常呈现动态分布变化（概念漂移），忽略此现象会严重降低模型预测性能。现有在线模型的超参数通常是固定的，缺乏动态适应能力，需要一种能自动适应数据流变化的无超参数模型。

Method: 提出OLR-WAA模型：1）使用指数加权移动平均增量更新基础模型；2）引入独特的优化机制，动态检测概念漂移并量化其幅度；3）基于实时数据特征调整模型；4）采用保守更新策略处理置信度场景，优先处理高置信度数据点。

Result: 在静态设置中匹配批量回归性能，在非平稳数据流中持续优于或媲美最先进的在线模型。在概念漂移数据集上有效弥合性能差距，快速收敛并始终产生更高的R2值。

Conclusion: OLR-WAA是一种有效的无超参数在线回归模型，能够处理非平稳数据流，在概念漂移环境中表现出色，平衡了模型稳定性与适应性。

Abstract: Real-world datasets frequently exhibit evolving data distributions, reflecting temporal variations and underlying shifts. Overlooking this phenomenon, known as concept drift, can substantially degrade the predictive performance of the model. Furthermore, the presence of hyperparameters in online models exacerbates this issue, as these parameters are typically fixed and lack the flexibility to dynamically adjust to evolving data. This paper introduces "OLR-WAA: An Adaptive and Drift-Resilient Online Regression with Dynamic Weighted Average", a hyperparameter-free model designed to tackle the challenges of non-stationary data streams and enable effective, continuous adaptation. The objective is to strike a balance between model stability and adaptability. OLR-WAA incrementally updates its base model by integrating incoming data streams, utilizing an exponentially weighted moving average. It further introduces a unique optimization mechanism that dynamically detects concept drift, quantifies its magnitude, and adjusts the model based on real-time data characteristics. Rigorous evaluations show that it matches batch regression performance in static settings and consistently outperforms or rivals state-of-the-art online models, confirming its effectiveness. Concept drift datasets reveal a performance gap that OLR-WAA effectively bridges, setting it apart from other online models. In addition, the model effectively handles confidence-based scenarios through a conservative update strategy that prioritizes stable, high-confidence data points. Notably, OLR-WAA converges rapidly, consistently yielding higher R2 values compared to other online models.

</details>


### [599] [Probability Estimation for Predicted-Occupancy Grids in Vehicle Safety Applications Based on Machine Learning](https://arxiv.org/abs/2512.12896)
*Parthasarathy Nadarajan,Michael Botsch*

Main category: cs.LG

Relevance: 25.0

TL;DR: 提出一种预测复杂交通场景演化的方法，使用预测占用网格(POG)作为未来场景假设的网格化概率表示，先介绍基于模型的方法，后采用基于随机森林的机器学习方法实现实时计算


<details>
  <summary>Details</summary>
Motivation: 交通场景预测对车辆安全系统至关重要，但交通参与者的行为不确定性难以建模。现有基于模型的方法计算量大，无法实时应用，需要开发更高效的预测方法

Method: 1. 基于模型的方法：计算预测占用网格(POG)作为未来场景的网格化概率表示
2. 机器学习方法：使用增强单元格的网格表示当前交通场景状态，采用随机森林算法将当前状态映射到POG
3. 通过交通场景仿真比较两种方法

Result: 机器学习方法结果令人满意，能够实现POG的实时计算，为车辆安全应用（如关键性估计和轨迹规划）提供支持

Conclusion: 提出的机器学习方法能够有效预测复杂交通场景演化，实现实时不确定性建模，可显著改善车辆安全系统的关键组件

Abstract: This paper presents a method to predict the evolution of a complex traffic scenario with multiple objects. The current state of the scenario is assumed to be known from sensors and the prediction is taking into account various hypotheses about the behavior of traffic participants. This way, the uncertainties regarding the behavior of traffic participants can be modelled in detail. In the first part of this paper a model-based approach is presented to compute Predicted-Occupancy Grids (POG), which are introduced as a grid-based probabilistic representation of the future scenario hypotheses. However, due to the large number of possible trajectories for each traffic participant, the model-based approach comes with a very high computational load. Thus, a machine-learning approach is adopted for the computation of POGs. This work uses a novel grid-based representation of the current state of the traffic scenario and performs the mapping to POGs. This representation consists of augmented cells in an occupancy grid. The adopted machine-learning approach is based on the Random Forest algorithm. Simulations of traffic scenarios are performed to compare the machine-learning with the model-based approach. The results are promising and could enable the real-time computation of POGs for vehicle safety applications. With this detailed modelling of uncertainties, crucial components in vehicle safety systems like criticality estimation and trajectory planning can be improved.

</details>


### [600] [Predicted-occupancy grids for vehicle safety applications based on autoencoders and the Random Forest algorithm](https://arxiv.org/abs/2512.12901)
*Parthasarathy Nadarajan,Michael Botsch,Sebastian Sardina*

Main category: cs.LG

Relevance: 25.0

TL;DR: 该论文提出了一种基于机器学习的概率时空表示方法，用于预测复杂交通场景中参与者的未来行为，通过层次化场景分类、降维特征提取和随机森林预测生成预测占用网格。


<details>
  <summary>Details</summary>
Motivation: 复杂交通场景下的动态机动需要准确预测其他交通参与者的未来行为，这对于主动车辆安全应用至关重要。现有方法在预测复杂场景中多个参与者的概率行为方面存在不足。

Method: 1. 使用层次化场景分类器识别道路基础设施类型和安全相关交通参与者
2. 采用堆叠去噪自编码器将增强占用网格降维为低维特征
3. 针对每类相似交通场景，分别训练随机森林来预测概率时空表示（预测占用网格）

Result: 在仿真和真实车辆实验中，提出的SDA+RF方法表现出优异性能，能够准确预测交通参与者的未来行为，并成功应用于交通场景关键性评估和安全轨迹确定。

Conclusion: 该机器学习方法能够有效预测复杂交通场景的概率时空表示，为主动车辆安全应用提供了可靠的技术支持，特别是在动态机动决策中具有重要价值。

Abstract: In this paper, a probabilistic space-time representation of complex traffic scenarios is predicted using machine learning algorithms. Such a representation is significant for all active vehicle safety applications especially when performing dynamic maneuvers in a complex traffic scenario. As a first step, a hierarchical situation classifier is used to distinguish the different types of traffic scenarios. This classifier is responsible for identifying the type of the road infrastructure and the safety-relevant traffic participants of the driving environment. With each class representing similar traffic scenarios, a set of Random Forests (RFs) is individually trained to predict the probabilistic space-time representation, which depicts the future behavior of traffic participants. This representation is termed as a Predicted-Occupancy Grid (POG). The input to the RFs is an Augmented Occupancy Grid (AOG). In order to increase the learning accuracy of the RFs and to perform better predictions, the AOG is reduced to low-dimensional features using a Stacked Denoising Autoencoder (SDA). The excellent performance of the proposed machine learning approach consisting of SDAs and RFs is demonstrated in simulations and in experiments with real vehicles. An application of POGs to estimate the criticality of traffic scenarios and to determine safe trajectories is also presented.

</details>


### [601] [Understanding When Graph Convolutional Networks Help: A Diagnostic Study on Label Scarcity and Structural Properties](https://arxiv.org/abs/2512.12947)
*Nischal Subedi,Ember Kerstetter,Winnie Li,Silo Murphy*

Main category: cs.LG

Relevance: 25.0

TL;DR: GCNs在极端标签稀缺时通过利用图结构提供最大性能提升，但在低同质性和强特征组合时反而会损害性能


<details>
  <summary>Details</summary>
Motivation: 研究GCNs在半监督节点分类中何时比简单基线提供有意义的改进，为实践者提供清晰的指导原则

Method: 使用Amazon Computers共购数据进行诊断研究，通过模拟标签稀缺、特征消融和按类分析等系统实验

Result: GCN性能关键取决于图同质性和特征质量的交互作用：在极端标签稀缺时提升最大；在高同质性图中即使特征被随机噪声替换也能保持性能；在低同质性且特征强时反而损害性能

Conclusion: GCNs在四种条件中的三种情况下有帮助，仅在低同质性遇到强特征时有害，为实践者采用图方法提供了实用指导

Abstract: Graph Convolutional Networks (GCNs) have become a standard approach for semi-supervised node classification, yet practitioners lack clear guidance on when GCNs provide meaningful improvements over simpler baselines. We present a diagnostic study using the Amazon Computers co-purchase data to understand when and why GCNs help. Through systematic experiments with simulated label scarcity, feature ablation, and per-class analysis, we find that GCN performance depends critically on the interaction between graph homophily and feature quality. GCNs provide the largest gains under extreme label scarcity, where they leverage neighborhood structure to compensate for limited supervision. Surprisingly, GCNs can match their original performance even when node features are replaced with random noise, suggesting that structure alone carries sufficient signal on highly homophilous graphs. However, GCNs hurt performance when homophily is low and features are already strong, as noisy neighbors corrupt good predictions. Our quadrant analysis reveals that GCNs help in three of four conditions and only hurt when low homophily meets strong features. These findings offer practical guidance for practitioners deciding whether to adopt graph-based methods.

</details>


### [602] [Multi-fidelity aerodynamic data fusion by autoencoder transfer learning](https://arxiv.org/abs/2512.13069)
*Javier Nieto-Centenero,Esther Andrés,Rodrigo Castellanos*

Main category: cs.LG

Relevance: 25.0

TL;DR: 提出多保真度深度学习框架，结合自动编码器迁移学习和多分割共形预测，在数据稀缺条件下实现不确定性感知的空气动力学数据融合


<details>
  <summary>Details</summary>
Motivation: 高保真度空气动力学模拟计算成本过高，限制了数据驱动建模的应用。需要开发多保真度策略，利用低成本低保真度信息而不牺牲精度

Method: 1) 使用自动编码器从大量低保真度数据学习紧凑的潜在物理表示；2) 将该表示作为冻结知识库，解码器使用稀缺的高保真度样本进行微调；3) 开发多分割共形预测策略进行不确定性量化

Result: 模型成功修正了低保真度偏差，使用极少高保真度训练数据实现了高精度压力预测。MSCP框架产生了稳健、可操作的不确定性带，点覆盖超过95%

Conclusion: 该工作结合极端数据效率和不确定性量化，为数据稀缺环境下的空气动力学回归提供了可扩展且可靠的解决方案

Abstract: Accurate aerodynamic prediction often relies on high-fidelity simulations; however, their prohibitive computational costs severely limit their applicability in data-driven modeling. This limitation motivates the development of multi-fidelity strategies that leverage inexpensive low-fidelity information without compromising accuracy. Addressing this challenge, this work presents a multi-fidelity deep learning framework that combines autoencoder-based transfer learning with a newly developed Multi-Split Conformal Prediction (MSCP) strategy to achieve uncertainty-aware aerodynamic data fusion under extreme data scarcity. The methodology leverages abundant Low-Fidelity (LF) data to learn a compact latent physics representation, which acts as a frozen knowledge base for a decoder that is subsequently fine-tuned using scarce HF samples. Tested on surface-pressure distributions for NACA airfoils (2D) and a transonic wing (3D) databases, the model successfully corrects LF deviations and achieves high-accuracy pressure predictions using minimal HF training data. Furthermore, the MSCP framework produces robust, actionable uncertainty bands with pointwise coverage exceeding 95%. By combining extreme data efficiency with uncertainty quantification, this work offers a scalable and reliable solution for aerodynamic regression in data-scarce environments.

</details>


### [603] [Quanvolutional Neural Networks for Spectrum Peak-Finding](https://arxiv.org/abs/2512.13125)
*Lukas Bischof,Rudolf M. Füchslin,Kurt Stockinger,Pavel Sulimov*

Main category: cs.LG

Relevance: 25.0

TL;DR: 该论文探索了量子卷积神经网络（QuanvNNs）在核磁共振谱峰分析中的应用，相比经典CNN在复杂谱图上取得了11%的F1分数提升和30%的峰值位置估计误差降低。


<details>
  <summary>Details</summary>
Motivation: 核磁共振谱峰分析（去卷积）对专家和机器都具有挑战性，尤其是在复杂分子情况下。量子计算的出现为增强机器学习技术提供了新机会，作者受经典CNN成功启发，探索量子卷积神经网络在峰值检测问题中的应用潜力。

Method: 设计了一个简单且可解释的QuanvNN架构，可直接与经典CNN对比。在合成的NMR数据集上进行评估，解决峰值计数和位置估计的多任务问题。

Result: QuanvNNs在挑战性谱图上优于经典CNNs：F1分数提升11%，峰值位置估计的平均绝对误差降低30%。此外，QuanvNNs在困难问题上表现出更好的收敛稳定性。

Conclusion: 量子卷积神经网络在谱峰分析任务中显示出优于经典方法的潜力，特别是在复杂谱图处理方面，为量子机器学习在科学数据分析中的应用提供了实证支持。

Abstract: The analysis of spectra, such as Nuclear Magnetic Resonance (NMR) spectra, for the comprehensive characterization of peaks is a challenging task for both experts and machines, especially with complex molecules. This process, also known as deconvolution, involves identifying and quantifying the peaks in the spectrum. Machine learning techniques have shown promising results in automating this process. With the advent of quantum computing, there is potential to further enhance these techniques. In this work, inspired by the success of classical Convolutional Neural Networks (CNNs), we explore the use of Quanvolutional Neural Networks (QuanvNNs) for the multi-task peak finding problem, involving both peak counting and position estimation. We implement a simple and interpretable QuanvNN architecture that can be directly compared to its classical CNN counterpart, and evaluate its performance on a synthetic NMR-inspired dataset. Our results demonstrate that QuanvNNs outperform classical CNNs on challenging spectra, achieving an 11\% improvement in F1 score and a 30\% reduction in mean absolute error for peak position estimation. Additionally, QuanvNNs appear to exhibit better convergence stability for harder problems.

</details>


### [604] [ModSSC: A Modular Framework for Semi-Supervised Classification on Heterogeneous Data](https://arxiv.org/abs/2512.13228)
*Melvin Barbaux*

Main category: cs.LG

Relevance: 25.0

TL;DR: ModSSC是一个统一的半监督分类Python框架，支持多种算法、数据类型和硬件，通过YAML配置实现可复现的实验。


<details>
  <summary>Details</summary>
Motivation: 现有的半监督分类软件支持分散在不同方法和模态中，缺乏统一的框架。作者希望提供一个集成的开源工具，简化半监督分类的研究和应用。

Method: 开发了ModSSC框架，实现了广泛的经典和现代算法，支持表格、图像、文本、音频和图数据，提供统一的配置接口，支持CPU和GPU计算，使用YAML声明式实验配置。

Result: 发布了ModSSC 1.0.0版本，采用MIT许可证，包含完整文档和测试，可在GitHub上获取。该框架统一了归纳和直推式半监督分类，支持大规模比较研究。

Conclusion: ModSSC为半监督分类提供了一个全面、模块化且易于使用的框架，有助于促进该领域的研究可复现性和方法比较。

Abstract: Semi-supervised classification leverages both labeled and unlabeled data to improve predictive performance, but existing software support is fragmented across methods and modalities. We introduce ModSSC, an open source Python framework that unifies inductive and transductive semi-supervised classification in a modular code base. ModSSC implements a broad range of classical and recent algorithms, provides loaders for tabular, image, text, audio and graph datasets, and exposes a single configuration interface for specifying datasets, models and evaluation protocols. It supports both lightweight classical methods on small datasets running on CPU and recent deep approaches that can exploit multiple GPUs within the same experimental framework. Experiments are described declaratively in YAML, which facilitates reproducing existing work and running large comparative studies. ModSSC 1.0.0 is released under the MIT license with extensive documentation and tests, and is available at https://github.com/ModSSC/ModSSC.

</details>


### [605] [Modeling Dabrafenib Response Using Multi-Omics Modality Fusion and Protein Network Embeddings Based on Graph Convolutional Networks](https://arxiv.org/abs/2512.12134)
*La Ode Aman,A Mu'thi Andy Suryadi,Dizky Ramadani Putri Papeo,Hamsidar Hasan,Ariani H Hutuba,Netty Ino Ischak,Yuszda K. Salimi*

Main category: q-bio.BM

Relevance: 25.0

TL;DR: 该研究开发了一个多组学融合模型，通过整合基因组学、转录组学、蛋白质组学、表观基因组学和代谢组学数据，结合图卷积网络生成的蛋白质网络嵌入，来预测Dabrafenib（一种靶向治疗药物）的敏感性。


<details>
  <summary>Details</summary>
Motivation: 癌症细胞对靶向治疗的反应源于复杂的分子相互作用，单一组学数据不足以准确预测药物敏感性。需要整合多组学数据来捕捉更全面的生物学信息。

Method: 1. 使用神经网络将每个组学模态编码为低维表示
2. 从STRING数据库获取蛋白质相互作用信息，使用图卷积网络（GCN）捕捉生物拓扑结构
3. 采用基于注意力的融合机制，根据每个模态的相关性分配自适应权重
4. 使用GDSC癌症细胞系数据进行训练和评估

Result: 1. 选择性整合两个模态（特别是蛋白质组学和转录组学）达到最佳测试性能（R²约0.96）
2. 优于所有单一组学和完整多模态设置
3. 基因组和表观基因组数据信息量较少
4. 蛋白质组和转录组层提供了与MAPK抑制剂活性相关的更强表型信号

Conclusion: 注意力引导的多组学融合结合GCN提高了药物反应预测准确性，并揭示了Dabrafenib敏感性的互补分子决定因素。该方法为精准肿瘤学和靶向治疗的预测建模提供了一个有前景的计算框架。

Abstract: Cancer cell response to targeted therapy arises from complex molecular interactions, making single omics insufficient for accurate prediction. This study develops a model to predict Dabrafenib sensitivity by integrating multiple omics layers (genomics, transcriptomics, proteomics, epigenomics, and metabolomics) with protein network embeddings generated using Graph Convolutional Networks (GCN). Each modality is encoded into low dimensional representations through neural network preprocessing. Protein interaction information from STRING is incorporated using GCN to capture biological topology. An attention based fusion mechanism assigns adaptive weights to each modality according to its relevance. Using GDSC cancer cell line data, the model shows that selective integration of two modalities, especially proteomics and transcriptomics, achieves the best test performance (R2 around 0.96), outperforming all single omics and full multimodal settings. Genomic and epigenomic data were less informative, while proteomic and transcriptomic layers provided stronger phenotypic signals related to MAPK inhibitor activity. These results show that attention guided multi omics fusion combined with GCN improves drug response prediction and reveals complementary molecular determinants of Dabrafenib sensitivity. The approach offers a promising computational framework for precision oncology and predictive modeling of targeted therapies.

</details>


### [606] [Learning to Get Up Across Morphologies: Zero-Shot Recovery with a Unified Humanoid Policy](https://arxiv.org/abs/2512.12230)
*Jonathan Spraggett*

Main category: cs.RO

Relevance: 25.0

TL;DR: 提出一个统一的深度强化学习策略，能够零样本泛化到7种不同形态的人形机器人跌倒恢复任务，无需为每个机器人单独训练


<details>
  <summary>Details</summary>
Motivation: 在动态环境如RoboCup中，人形机器人跌倒恢复是关键技能。现有深度强化学习方法需要为每个机器人形态训练单独策略，这限制了方法的可扩展性和实用性。

Method: 使用CrossQ训练统一的DRL策略，覆盖7种不同高度、重量和动力学特性的人形机器人。通过留一法实验、形态缩放分析和多样性消融研究验证方法。

Result: 统一策略在未见机器人形态上实现86±7%的零样本迁移成功率，在某些情况下甚至超过专门训练的基线策略。

Conclusion: 该方法展示了形态无关控制在人形机器人跌倒恢复中的实用性，为通用人形机器人控制奠定了基础。

Abstract: Fall recovery is a critical skill for humanoid robots in dynamic environments such as RoboCup, where prolonged downtime often decides the match. Recent techniques using deep reinforcement learning (DRL) have produced robust get-up behaviors, yet existing methods require training of separate policies for each robot morphology. This paper presents a single DRL policy capable of recovering from falls across seven humanoid robots with diverse heights (0.48 - 0.81 m), weights (2.8 - 7.9 kg), and dynamics. Trained with CrossQ, the unified policy transfers zero-shot up to 86 +/- 7% (95% CI [81, 89]) on unseen morphologies, eliminating the need for robot-specific training. Comprehensive leave-one-out experiments, morph scaling analysis, and diversity ablations show that targeted morphological coverage improves zero-shot generalization. In some cases, the shared policy even surpasses the specialist baselines. These findings illustrate the practicality of morphology-agnostic control for fall recovery, laying the foundation for generalist humanoid control. The software is open-source and available at: https://github.com/utra-robosoccer/unified-humanoid-getup

</details>


### [607] [Hellinger loss function for Generative Adversarial Networks](https://arxiv.org/abs/2512.12267)
*Giovanni Saraceno,Anand N. Vidyashankar,Claudio Agostinelli*

Main category: stat.ML

Relevance: 25.0

TL;DR: 论文提出基于Hellinger距离的GAN损失函数，具有有界性、对称性和鲁棒性优势，建立了对抗训练的理论框架并证明了估计量的统计性质，实验显示在数据污染情况下比传统GAN损失具有更好的估计精度和鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 传统GAN训练存在不稳定性和模式崩溃问题，Hellinger距离因其有界性、对称性和鲁棒性特性，为GAN训练提供了理论优势。作者希望利用Hellinger距离的这些数学特性来改进GAN的训练稳定性和统计性质。

Method: 提出基于Hellinger距离的对抗损失函数，在参数化框架下建立理论分析。定义对抗目标函数，研究其统计性质，包括估计量的存在性、唯一性、一致性和联合渐近正态性。实现两种Hellinger-type损失函数，并与传统最大似然型GAN损失进行对比。

Result: 理论分析证明了Hellinger-type损失函数估计量的良好统计性质。仿真实验表明，在数据污染程度增加的情况下，两种提出的损失函数都比传统GAN损失具有更好的估计精度和鲁棒性。

Conclusion: Hellinger-type损失函数为GAN训练提供了理论保证的替代方案，在统计性质和鲁棒性方面优于传统方法，特别适用于存在数据污染的实际应用场景。

Abstract: We propose Hellinger-type loss functions for training Generative Adversarial Networks (GANs), motivated by the boundedness, symmetry, and robustness properties of the Hellinger distance. We define an adversarial objective based on this divergence and study its statistical properties within a general parametric framework. We establish the existence, uniqueness, consistency, and joint asymptotic normality of the estimators obtained from the adversarial training procedure. In particular, we analyze the joint estimation of both generator and discriminator parameters, offering a comprehensive asymptotic characterization of the resulting estimators. We introduce two implementations of the Hellinger-type loss and we evaluate their empirical behavior in comparison with the classic (Maximum Likelihood-type) GAN loss. Through a controlled simulation study, we demonstrate that both proposed losses yield improved estimation accuracy and robustness under increasing levels of data contamination.

</details>


### [608] [Robust Outlier Detection and Low-Latency Concept Drift Adaptation for Data Stream Regression: A Dual-Channel Architecture](https://arxiv.org/abs/2512.12289)
*Bingbing Wang,Shengyan Sun,Jiaqi Wang,Yu Tang*

Main category: stat.ME

Relevance: 25.0

TL;DR: 提出了一种用于联合检测离群点和概念漂移的鲁棒回归框架，包含双通道决策过程和EWMAD-DT检测器，能区分突变和渐进漂移。


<details>
  <summary>Details</summary>
Motivation: 现有研究大多单独处理离群点检测和概念漂移检测，但在回归问题中，连续输出空间的特性使得区分漂移和离群点变得困难，需要联合检测机制。

Method: 提出双通道决策过程：快速响应通道过滤点离群点，深度分析通道诊断漂移。开发EWMAD-DT检测器，通过动态阈值自主区分突变和渐进漂移。

Result: 在合成和真实数据集上的综合实验表明，该统一框架在点离群点和概念漂移共存时表现出优越的检测性能。

Conclusion: 提出的鲁棒回归框架能有效联合检测离群点和概念漂移，EWMAD-DT检测器能自主区分不同类型漂移，为回归分析中的异常检测提供统一解决方案。

Abstract: Outlier detection and concept drift detection represent two challenges in data analysis. Most studies address these issues separately. However, joint detection mechanisms in regression remain underexplored, where the continuous nature of output spaces makes distinguishing drifts from outliers inherently challenging. To address this, we propose a novel robust regression framework for joint outlier and concept drift detection. Specifically, we introduce a dual-channel decision process that orchestrates prediction residuals into two coupled logic flows: a rapid response channel for filtering point outliers and a deep analysis channel for diagnosing drifts. We further develop the Exponentially Weighted Moving Absolute Deviation with Distinguishable Types (EWMAD-DT) detector to autonomously differentiate between abrupt and incremental drifts via dynamic thresholding. Comprehensive experiments on both synthetic and real-world datasets demonstrate that our unified framework, enhanced by EWMAD-DT, exhibits superior detection performance even when point outliers and concept drifts coexist.

</details>


### [609] [Efficient Level-Crossing Probability Calculation for Gaussian Process Modeled Data](https://arxiv.org/abs/2512.12442)
*Haoyu Li,Isaac J Michaud,Ayan Biswas,Han-Wei Shen*

Main category: stat.ML

Relevance: 25.0

TL;DR: 提出了一种基于高斯过程回归（GPR）模型的高效水平交叉概率计算方法，通过空间分层数据结构和自适应重建来加速不确定性可视化


<details>
  <summary>Details</summary>
Motivation: 科学数据普遍存在不确定性，GPR模型能有效建模高斯分布的不确定性并减少I/O带宽和存储需求，但传统重建方法计算复杂度高，特别是高分辨率数据的概率可视化（如概率行进立方体）计算代价巨大

Method: 1) 将数据空间细分为分层数据结构；2) 仅在具有非零概率的区域自适应重建值；3) 利用已知GPR核和保存的数据观测值，提出新方法高效计算区域内水平交叉概率的上界；4) 使用该上界进行细分和重建决策

Result: 实验证明该方法能准确计算不同数据集上的水平交叉概率场，且计算成本较低

Conclusion: 提出的方法显著加速了GPR模型上的水平交叉概率计算效率，为科学数据不确定性可视化提供了高效解决方案

Abstract: Almost all scientific data have uncertainties originating from different sources. Gaussian process regression (GPR) models are a natural way to model data with Gaussian-distributed uncertainties. GPR also has the benefit of reducing I/O bandwidth and storage requirements for large scientific simulations. However, the reconstruction from the GPR models suffers from high computation complexity. To make the situation worse, classic approaches for visualizing the data uncertainties, like probabilistic marching cubes, are also computationally very expensive, especially for data of high resolutions. In this paper, we accelerate the level-crossing probability calculation efficiency on GPR models by subdividing the data spatially into a hierarchical data structure and only reconstructing values adaptively in the regions that have a non-zero probability. For each region, leveraging the known GPR kernel and the saved data observations, we propose a novel approach to efficiently calculate an upper bound for the level-crossing probability inside the region and use this upper bound to make the subdivision and reconstruction decisions. We demonstrate that our value occurrence probability estimation is accurate with a low computation cost by experiments that calculate the level-crossing probability fields on different datasets.

</details>


### [610] [Scalable Quantum Error Mitigation with Neighbor-Informed Learning](https://arxiv.org/abs/2512.12578)
*Zhenyu Chen,Bin Cheng,Minbo Gao,Xiaodie Lin,Ruiqi Zhang,Zhaohui Wei,Zhengfeng Ji*

Main category: quant-ph

Relevance: 25.0

TL;DR: 提出了一种名为邻居信息学习(NIL)的量子误差缓解框架，通过机器学习预测目标量子电路的理想输出，相比现有方法在性能、资源开销和理论保证方面有更好平衡。


<details>
  <summary>Details</summary>
Motivation: 量子硬件中的噪声是实现量子计算潜力的主要障碍。现有量子误差缓解方法在性能、资源开销和理论保证之间存在困难权衡，需要更灵活、准确、高效且稳健的解决方案。

Method: NIL通过学习从结构相关的"邻居"电路的噪声输出预测目标量子电路的理想输出。关键创新是2-design训练方法，为机器学习模型生成训练数据，相比传统基于学习的QEM协议（通过用均匀随机Clifford门替换非Clifford门创建训练电路）具有更高准确性和效率。

Result: 理论分析和数值模拟表明NIL具有更高准确性和效率。证明所需训练集大小仅与邻居电路总数呈对数关系，使NIL能够应用于大规模量子电路问题。

Conclusion: NIL为量子误差缓解建立了理论基础扎实且实际高效的框架，为实现噪声硬件上的量子优势铺平了可行道路。

Abstract: Noise in quantum hardware is the primary obstacle to realizing the transformative potential of quantum computing. Quantum error mitigation (QEM) offers a promising pathway to enhance computational accuracy on near-term devices, yet existing methods face a difficult trade-off between performance, resource overhead, and theoretical guarantees. In this work, we introduce neighbor-informed learning (NIL), a versatile and scalable QEM framework that unifies and strengthens existing methods such as zero-noise extrapolation (ZNE) and probabilistic error cancellation (PEC), while offering improved flexibility, accuracy, efficiency, and robustness.
  NIL learns to predict the ideal output of a target quantum circuit from the noisy outputs of its structurally related ``neighbor'' circuits. A key innovation is our 2-design training method, which generates training data for our machine learning model. In contrast to conventional learning-based QEM protocols that create training circuits by replacing non-Clifford gates with uniformly random Clifford gates, our approach achieves higher accuracy and efficiency, as demonstrated by both theoretical analysis and numerical simulation. Furthermore, we prove that the required size of the training set scales only \emph{logarithmically} with the total number of neighbor circuits, enabling NIL to be applied to problems involving large-scale quantum circuits. Our work establishes a theoretically grounded and practically efficient framework for QEM, paving a viable path toward achieving quantum advantage on noisy hardware.

</details>


### [611] [Transport Reversible Jump Markov Chain Monte Carlo with proposals generated by Variational Inference with Normalizing Flows](https://arxiv.org/abs/2512.12742)
*Pingping Yin,Xiyun Jiao*

Main category: stat.ML

Relevance: 25.0

TL;DR: 提出使用变分推断与归一化流（VI-NFs）为可逆跳转马尔可夫链蒙特卡洛（RJMCMC）生成提议的框架，实现高效的跨维度贝叶斯推断。


<details>
  <summary>Details</summary>
Motivation: 传统基于传输的可逆跳转方法依赖前向KL最小化和先导MCMC采样，计算成本高。本文旨在开发一种只需基分布样本、无需目标采样的更高效方法。

Method: 使用RealNVP归一化流学习模型特定的传输映射，最小化反向KL散度而非前向KL散度。构建跨模型和模型内提议，提供变分近似的精确边缘似然估计。

Result: 在示例、因子分析和线性回归变量选择任务中，VI-NFs设计的TRJ相比现有基线实现了更快的混合和更高效的模型空间探索。

Conclusion: 该方法为RJMCMC提供了高效的提议生成框架，可扩展到条件流实现跨模型的摊销变分推断。

Abstract: We present a framework using variational inference with normalizing flows (VI-NFs) to generate proposals of reversible jump Markov chain Monte Carlo (RJMCMC) for efficient trans-dimensional Bayesian inference. Unlike transport reversible jump methods relying on forward KL minimization with pilot MCMC samples, our approach minimizes the reverse KL divergence which requires only samples from a base distribution, eliminating costly target sampling. The method employs RealNVP-based flows to learn model-specific transport maps, enabling construction of both between-model and within-model proposals. Our framework provides accurate marginal likelihood estimates from the variational approximation. This facilitates efficient model comparison and proposal adaptation in RJMCMC. Experiments on illustrative example, factor analysis and variable selection tasks in linear regression show that TRJ designed by VI-NFs achieves faster mixing and more efficient model space exploration compared to existing baselines. The proposed algorithm can be extended to conditional flows for amortized vairiational inference across models. Code is available at https://github.com/YinPingping111/TRJ_VINFs.

</details>


### [612] [Flow-matching Operators for Residual-Augmented Probabilistic Learning of Partial Differential Equations](https://arxiv.org/abs/2512.12749)
*Sahil Bhola,Karthik Duraisamy*

Main category: stat.CO

Relevance: 25.0

TL;DR: 提出了一种在无限维函数空间中学习PDE概率代理的方法，通过流匹配将低精度近似映射到高精度PDE解流形，支持任意空间分辨率推理，并在数据稀缺条件下提供不确定性估计。


<details>
  <summary>Details</summary>
Motivation: PDE的概率代理学习在数据稀缺场景下面临挑战：神经算子需要大量高保真数据，而生成方法通常牺牲分辨率不变性。需要一种能在有限高保真数据下学习、保持分辨率不变性并提供不确定性估计的方法。

Method: 1) 在无限维函数空间中制定流匹配，学习从低精度近似到高精度PDE解流形的概率传输；2) 基于特征线性调制的条件神经算子架构，直接在函数空间中进行流匹配向量场学习；3) 将流向量场参数化为线性算子和非线性算子的和，结合轻量线性组件和条件傅里叶神经算子；4) 残差增强学习策略，学习从廉价低精度代理到高精度解的概率修正；5) 推导可处理的训练目标，将条件流匹配扩展到算子设置。

Result: 在1D平流方程、Burgers方程和2D Darcy多孔介质流动问题上的数值实验表明，该方法能够准确学习不同分辨率和保真度的解算子，即使在有限高保真数据训练下，也能产生适当反映模型置信度的不确定性估计。

Conclusion: 该方法成功解决了数据稀缺条件下PDE概率代理学习的挑战，实现了分辨率不变推理和不确定性量化，为PDE求解提供了高效的概率框架。

Abstract: Learning probabilistic surrogates for PDEs remains challenging in data-scarce regimes: neural operators require large amounts of high-fidelity data, while generative approaches typically sacrifice resolution invariance. We formulate flow matching in an infinite-dimensional function space to learn a probabilistic transport that maps low-fidelity approximations to the manifold of high-fidelity PDE solutions via learned residual corrections. We develop a conditional neural operator architecture based on feature-wise linear modulation for flow-matching vector fields directly in function space, enabling inference at arbitrary spatial resolutions without retraining. To improve stability and representational control of the induced neural ODE, we parameterize the flow vector field as a sum of a linear operator and a nonlinear operator, combining lightweight linear components with a conditioned Fourier neural operator for expressive, input-dependent dynamics. We then formulate a residual-augmented learning strategy where the flow model learns probabilistic corrections from inexpensive low-fidelity surrogates to high-fidelity solutions, rather than learning the full solution mapping from scratch. Finally, we derive tractable training objectives that extend conditional flow matching to the operator setting with input-function-dependent couplings. To demonstrate the effectiveness of our approach, we present numerical experiments on a range of PDEs, including the 1D advection and Burgers' equation, and a 2D Darcy flow problem for flow through a porous medium.
  We show that the proposed method can accurately learn solution operators across different resolutions and fidelities and produces uncertainty estimates that appropriately reflect model confidence, even when trained on limited high-fidelity data.

</details>


### [613] [Dynamical modeling of nonlinear latent factors in multiscale neural activity with real-time inference](https://arxiv.org/abs/2512.12462)
*Eray Erturk,Maryam M. Shanechi*

Main category: cs.LG

Relevance: 20.0

TL;DR: 提出一种用于多模态神经信号实时解码的学习框架，能够处理不同时间尺度、概率分布和缺失样本的神经数据。


<details>
  <summary>Details</summary>
Motivation: 神经科学应用中需要从多种神经时间序列模态（如离散尖峰活动和连续场电位）实时解码目标变量。现有非线性多模态神经活动模型无法处理不同时间尺度或缺失样本，且部分模型不支持实时解码。

Method: 开发包含三个组件的学习框架：1）多尺度编码器，通过学习模态内动态非线性聚合信息，实时处理不同时间尺度和缺失样本；2）多尺度动态主干，提取多模态时间动态并支持实时递归解码；3）模态特定解码器，适应不同模态的概率分布差异。

Result: 在模拟和三个不同的多尺度脑数据集上，该方法能够聚合不同时间尺度、分布和缺失样本的多模态信息，改善实时目标解码性能，并优于各种线性和非线性多模态基准方法。

Conclusion: 该框架成功解决了多模态神经信号解码中的关键挑战，包括不同时间尺度、概率分布和缺失样本，实现了有效的实时递归解码。

Abstract: Real-time decoding of target variables from multiple simultaneously recorded neural time-series modalities, such as discrete spiking activity and continuous field potentials, is important across various neuroscience applications. However, a major challenge for doing so is that different neural modalities can have different timescales (i.e., sampling rates) and different probabilistic distributions, or can even be missing at some time-steps. Existing nonlinear models of multimodal neural activity do not address different timescales or missing samples across modalities. Further, some of these models do not allow for real-time decoding. Here, we develop a learning framework that can enable real-time recursive decoding while nonlinearly aggregating information across multiple modalities with different timescales and distributions and with missing samples. This framework consists of 1) a multiscale encoder that nonlinearly aggregates information after learning within-modality dynamics to handle different timescales and missing samples in real time, 2) a multiscale dynamical backbone that extracts multimodal temporal dynamics and enables real-time recursive decoding, and 3) modality-specific decoders to account for different probabilistic distributions across modalities. In both simulations and three distinct multiscale brain datasets, we show that our model can aggregate information across modalities with different timescales and distributions and missing samples to improve real-time target decoding. Further, our method outperforms various linear and nonlinear multimodal benchmarks in doing so.

</details>


### [614] [Unsupervised learning of multiscale switching dynamical system models from multimodal neural data](https://arxiv.org/abs/2512.12881)
*DongKyu Kim,Han-Lin Hsieh,Maryam M. Shanechi*

Main category: cs.LG

Relevance: 20.0

TL;DR: 提出一种无监督学习算法，用于从多尺度神经观测数据中学习切换多尺度动态系统模型，以更好地解码行为并跟踪神经活动中的状态依赖非平稳性。


<details>
  <summary>Details</summary>
Motivation: 神经群体活动常表现出状态依赖的非平稳性（切换动态），现有方法主要针对单一神经模态（连续高斯信号或离散泊松信号），但实际中常同时记录多种神经模态来测量不同时空尺度的大脑活动。此外，训练数据通常缺乏状态标签，这给学习状态依赖的切换动态模型带来了挑战。

Method: 开发了一种新颖的无监督学习算法，仅使用多尺度神经观测数据来学习切换多尺度动态系统模型的参数。该方法能够处理多模态的尖峰-LFP观测数据，并在不同运动任务中进行验证。

Result: 1) 切换多尺度动态系统模型比切换单尺度动态模型更准确地解码行为，显示了多尺度神经融合的成功；2) 模型优于平稳多尺度模型，说明了跟踪多模态神经数据中状态依赖非平稳性的重要性。

Conclusion: 该无监督学习框架通过利用多模态记录中的信息并纳入状态切换，能够更准确地建模复杂的多尺度神经动态。这种方法有望提高脑机接口的性能和鲁棒性，并增进对行为神经基础的理解。

Abstract: Neural population activity often exhibits regime-dependent non-stationarity in the form of switching dynamics. Learning accurate switching dynamical system models can reveal how behavior is encoded in neural activity. Existing switching approaches have primarily focused on learning models from a single neural modality, either continuous Gaussian signals or discrete Poisson signals. However, multiple neural modalities are often recorded simultaneously to measure different spatiotemporal scales of brain activity, and all these modalities can encode behavior. Moreover, regime labels are typically unavailable in training data, posing a significant challenge for learning models of regime-dependent switching dynamics. To address these challenges, we develop a novel unsupervised learning algorithm that learns the parameters of switching multiscale dynamical system models using only multiscale neural observations. We demonstrate our method using both simulations and two distinct experimental datasets with multimodal spike-LFP observations during different motor tasks. We find that our switching multiscale dynamical system models more accurately decode behavior than switching single-scale dynamical models, showing the success of multiscale neural fusion. Further, our models outperform stationary multiscale models, illustrating the importance of tracking regime-dependent non-stationarity in multimodal neural data. The developed unsupervised learning framework enables more accurate modeling of complex multiscale neural dynamics by leveraging information in multimodal recordings while incorporating regime switches. This approach holds promise for improving the performance and robustness of brain-computer interfaces over time and for advancing our understanding of the neural basis of behavior.

</details>


### [615] [The Art of Storytelling in Authoritarian Regimes: Crafting State Narratives on Chinese Social Media](https://arxiv.org/abs/2512.11875)
*Ting Luo,Yan Wang*

Main category: cs.CY

Relevance: 20.0

TL;DR: 该研究探讨威权政权如何构建政治事件的官方叙事，提出合法性影响和公民验证能力两个维度，通过分析中国社交媒体数据发现叙事构建是适应特定情境的复杂过程。


<details>
  <summary>Details</summary>
Motivation: 研究动机是理解威权政权如何通过叙事构建来维持统治，特别是在面对重大政治事件时。现有研究对叙事构建的具体机制和影响因素了解不足，需要更系统的分析框架。

Method: 采用定量叙事分析方法，分析中国政府、官方媒体和名人账号在社交媒体上的帖子。提取主语-谓语-宾语（SVO）三元组来映射四个主要事件中的主导叙事结构。

Result: 研究发现：1）事件的合法性影响塑造了政权在叙事构建中的努力程度和强调的信念；2）公民验证能力可以平衡自上而下的操纵和自下而上的响应之间的战略选择。

Conclusion: 宣传是适应特定情境的复杂叙事构建过程，动态的叙事构建有助于维持威权政体的韧性。这为理解威权统治的叙事策略提供了新视角。

Abstract: This article examines how authoritarian regimes construct state narratives about politically consequential events. Building on the narrative policy framework and existing research on authoritarian propaganda, we propose two dimensions that shape narrative construction: legitimacy implications -- whether events enhance or threaten regime legitimacy, and citizen verification capacity -- the extent to which citizens can evaluate official narratives through alternative sources. Using quantitative narrative analysis of Chinese social media posts by government, state media, and celebrity accounts, we extract subject-verb-object (SVO) triplets to map dominant narrative structures across four major events. Our findings show that legitimacy implications of the event shape regime's efforts in storytelling and the beliefs highlighted in the narratives, while citizen's verification capacity could balance the strategic choice between a top-down manipulation and bottom-up responsiveness of state narratives. Together, the results reveal propaganda as a complex process of narrative construction adaptive to specific contexts, offering new insights into how dynamic storytelling sustains authoritarian resilience.

</details>


### [616] [Performance and Efficiency of Climate In-Situ Data Reconstruction: Why Optimized IDW Outperforms kriging and Implicit Neural Representation](https://arxiv.org/abs/2512.11832)
*Jakub Walczak*

Main category: cs.LG

Relevance: 15.0

TL;DR: 该研究评估了三种稀疏气候数据重建方法：IDW、普通克里金法和MMGN神经网络模型，发现简单的IDW方法在精度和效率上均优于其他方法。


<details>
  <summary>Details</summary>
Motivation: 研究动机是评估不同方法在稀疏气候数据重建中的性能，为气候数据分析提供方法选择参考。虽然涉及神经网络方法，但主要关注传统空间插值技术与现代方法的比较。

Method: 采用三种重建方法：简单反距离加权法（IDW）、统计基础的普通克里金法（OK）和先进的隐式神经表示模型（MMGN架构）。通过验证集进行超参数调优，在ECA&D数据库的100个随机稀疏数据集上进行实验，并进行全面的统计分析。

Result: IDW方法在所有评估指标上表现最佳：最低的RMSE（3.00±1.93）、MAE（1.32±0.77）和Δ_MAX（24.06±17.15），以及最高的R²（0.68±0.16）。统计显著性检验证实了IDW的优越性。

Conclusion: 简单的IDW方法在稀疏气候数据重建中优于更复杂的统计和神经网络方法，表明在某些应用场景中简单方法可能比复杂模型更有效。

Abstract: This study evaluates three reconstruction methods for sparse climate data: the simple inverse distance weighting (IDW), the statistically grounded ordinary kriging (OK), and the advanced implicit neural representation model (MMGN architecture). All methods were optimized through hyper-parameter tuning using validation splits. An extensive set of experiments was conducted, followed by a comprehensive statistical analysis. The results demonstrate the superiority of the simple IDW method over the other reference methods in terms of both reconstruction accuracy and computational efficiency. IDW achieved the lowest RMSE ($3.00 \pm 1.93$), MAE ($1.32 \pm 0.77$), and $Δ_{MAX}$ ($24.06 \pm 17.15$), as well as the highest $R^2$ ($0.68 \pm 0.16$), across 100 randomly sampled sparse datasets from the ECA\&D database. Differences in RMSE, MAE, and $R^2$ were statistically significant and exhibited moderate to large effect sizes. The Dunn post-hoc test further confirmed the consistent superiority of IDW across all evaluated quality measures [...]

</details>


### [617] [Hybrid twinning using PBDW and DeepONet for the effective state estimation and prediction on partially known systems](https://arxiv.org/abs/2512.11834)
*Stiven Briand Massala,Ludovic Chamoin,Massimo Picca Ciamarra*

Main category: cs.LG

Relevance: 15.0

TL;DR: 提出一种结合物理建模与数据驱动学习的混合方法，用于复杂不确定物理系统的状态估计和预测。该方法基于PBDW框架，集成降阶模型与测量数据，并引入正交约束的DeepONet来学习模型偏差，同时优化传感器布局。


<details>
  <summary>Details</summary>
Motivation: 复杂不确定物理系统的状态估计需要平衡理论模型（存在固有缺陷）与噪声实验数据。传统方法难以同时处理预期和非预期的不确定性，需要一种能有效结合物理先验知识与数据驱动学习的方法。

Method: 基于PBDW框架，集成降阶模型表示与测量数据。引入正交约束的DeepONet来学习模型偏差，确保学习到的修正仅针对未知模型偏差成分，保持物理模型的解释性和保真度。同时研究最优传感器布局策略以最大化测量信息。

Result: 在涉及Helmholtz方程的代表性问题中验证了该方法，处理了包括边界条件和源项在内的各种建模误差来源。结果表明该方法能有效增强状态估计和预测能力。

Conclusion: 提出的混合方法成功结合了物理建模与数据驱动学习，通过正交约束的DeepONet有效学习模型偏差，同时保持物理模型的解释性，为复杂不确定物理系统的状态估计提供了有效解决方案。

Abstract: The accurate estimation of the state of complex uncertain physical systems requires reconciling theoretical models, with inherent imperfections, with noisy experimental data. In this work, we propose an effective hybrid approach that combines physics-based modeling with data-driven learning to enhance state estimation and further prediction. Our method builds upon the Parameterized Background Data-Weak (PBDW) framework, which naturally integrates a reduced-order representation of the best-available model with measurement data to account for both anticipated and unanticipated uncertainties. To address model discrepancies not captured by the reduced-order space, and learn the structure of model deviation, we incorporate a Deep Operator Network (DeepONet) constrained to be an orthogonal complement of the best-knowledge manifold. This ensures that the learned correction targets only the unknown components of model bias, preserving the interpretability and fidelity of the physical model. An optimal sensor placement strategy is also investigated to maximize information gained from measurements. We validate the proposed approach on a representative problem involving the Helmholtz equation under various sources of modeling error, including those arising from boundary conditions and source terms.

</details>


### [618] [Semantic Nutrition Estimation: Predicting Food Healthfulness from Text Descriptions](https://arxiv.org/abs/2512.11836)
*Dayne R. Freudenberg,Daniel G. Haughian,Mitchell A. Klusty,Caroline N. Leach,W. Scott Black,Leslie N. Woltenberg,Rowan Hallock,Elizabeth Solie,Emily B. Collier,Samuel E. Armstrong,V. K. Cody Bumgardner*

Main category: cs.LG

Relevance: 15.0

TL;DR: 开发了一个机器学习流水线，能够从文本描述中预测全面的食品指南评分2.0(FCS)，使用多头神经网络处理混合特征向量，包括语义文本嵌入、词汇模式和领域启发式特征，结合USDA食品和营养数据库数据。


<details>
  <summary>Details</summary>
Motivation: 准确的营养评估对公共卫生至关重要，但现有的分析系统需要详细数据，而这些数据通常无法从食品的通俗文本描述中获得或难以获取。需要一种能够从语言描述中提取可操作营养信息的方法。

Method: 使用多头神经网络处理混合特征向量，结合语义文本嵌入、词汇模式、领域启发式特征和USDA FNDDS数据。网络估计FCS算法所需的营养素和食品成分。

Result: 系统表现出强大的预测能力，个体营养素的R²中位数为0.81。预测的FCS与公布值强相关（Pearson's r = 0.77），平均绝对差异为14.0分。对模糊或加工食品的误差最大。

Conclusion: 该方法能够将语言转化为可操作的营养信息，为消费者应用和研究实现可扩展的饮食评估。

Abstract: Accurate nutritional assessment is critical for public health, but existing profiling systems require detailed data often unavailable or inaccessible from colloquial text descriptions of food. This paper presents a machine learning pipeline that predicts the comprehensive Food Compass Score 2.0 (FCS) from text descriptions. Our approach uses multi-headed neural networks to process hybrid feature vectors that combine semantic text embeddings, lexical patterns, and domain heuristics, alongside USDA Food and Nutrient Database for Dietary Studies (FNDDS) data. The networks estimate the nutrient and food components necessary for the FCS algorithm. The system demonstratedstrong predictive power, achieving a median R^2 of 0.81 for individual nutrients. The predicted FCS correlated strongly with published values (Pearson's r = 0.77), with a mean absolute difference of 14.0 points. While errors were largest for ambiguous or processed foods, this methodology translates language into actionable nutritional information, enabling scalable dietary assessment for consumer applications and research.

</details>


### [619] [Rep Smarter, Not Harder: AI Hypertrophy Coaching with Wearable Sensors and Edge Neural Networks](https://arxiv.org/abs/2512.11854)
*Grant King,Musa Azeem,Savannah Noblitt,Ramtin Zand,Homayoun Valafar*

Main category: cs.LG

Relevance: 15.0

TL;DR: 提出基于单手腕IMU的实时力量训练反馈系统，通过ResNet分割重复动作，结合LSTM识别接近力竭状态，实现边缘设备部署


<details>
  <summary>Details</summary>
Motivation: 力量训练中主观评估剩余重复次数(RiR)不可靠，导致训练刺激不足或过度疲劳，需要客观、实时的力竭状态监测系统

Method: 两阶段边缘部署管道：1) ResNet模型实时分割IMU数据的重复动作；2) 结合分割特征、卷积特征和LSTM历史上下文，分类识别接近力竭状态(RiR≤2)

Result: 在13名参与者631次重复的数据集上，分割模型F1分数0.83，接近力竭分类器F1分数0.82；Raspberry Pi 5延迟112ms，iPhone 16延迟23.5ms

Conclusion: 证明了使用最小硬件实现客观实时训练强度反馈的可行性，为AI驱动的增肌训练工具铺平道路

Abstract: Optimizing resistance training for hypertrophy requires balancing proximity to muscular failure, often quantified by Repetitions in Reserve (RiR), with fatigue management. However, subjective RiR assessment is unreliable, leading to suboptimal training stimuli or excessive fatigue. This paper introduces a novel system for real-time feedback on near-failure states (RiR $\le$ 2) during resistance exercise using only a single wrist-mounted Inertial Measurement Unit (IMU). We propose a two-stage pipeline suitable for edge deployment: first, a ResNet-based model segments repetitions from the 6-axis IMU data in real-time. Second, features derived from this segmentation, alongside direct convolutional features and historical context captured by an LSTM, are used by a classification model to identify exercise windows corresponding to near-failure states. Using a newly collected dataset from 13 diverse participants performing preacher curls to failure (631 total reps), our segmentation model achieved an F1 score of 0.83, and the near-failure classifier achieved an F1 score of 0.82 under simulated real-time evaluation conditions (1.6 Hz inference rate). Deployment on a Raspberry Pi 5 yielded an average inference latency of 112 ms, and on an iPhone 16 yielded 23.5 ms, confirming the feasibility for edge computation. This work demonstrates a practical approach for objective, real-time training intensity feedback using minimal hardware, paving the way for accessible AI-driven hypertrophy coaching tools that help users manage intensity and fatigue effectively.

</details>


### [620] [GRC-Net: Gram Residual Co-attention Net for epilepsy prediction](https://arxiv.org/abs/2512.12273)
*Bihao You,Jiping Cui*

Main category: cs.LG

Relevance: 15.0

TL;DR: 提出GRC-Net模型，通过Gram矩阵将EEG信号转换为3D表示，结合共注意力机制和Inception结构进行多粒度特征提取，在癫痫预测任务上取得93.66%的准确率。


<details>
  <summary>Details</summary>
Motivation: 传统EEG信号分析采用1D处理方法，无法充分捕捉信号间的多维关系。同时，EEG数据中存在局部与全局信号的不平衡问题，需要更有效的特征提取方法。

Method: 1) 使用Gram矩阵将1D EEG信号转换为3D表示，保留时间依赖性；2) 引入多级特征提取：共注意力机制捕捉全局特征，Inception结构处理局部信号；3) 构建GRC-Net网络进行癫痫分类。

Result: 在BONN数据集上，针对最具挑战性的五分类任务，GRC-Net达到93.66%的准确率，优于现有方法。

Conclusion: 提出的3D表示和多级特征提取方法能有效处理EEG信号中的局部-全局不平衡问题，在癫痫预测任务上表现优异。

Abstract: Prediction of epilepsy based on electroencephalogram (EEG) signals is a rapidly evolving field. Previous studies have traditionally applied 1D processing to the entire EEG signal. However, we have adopted the Gram Matrix method to transform the signals into a 3D representation, enabling modeling of signal relationships across dimensions while preserving the temporal dependencies of the one-dimensional signals. Additionally, we observed an imbalance between local and global signals within the EEG data. Therefore, we introduced multi-level feature extraction, utilizing coattention for capturing global signal characteristics and an inception structure for processing local signals, achieving multi-granular feature extraction. Our experiments on the BONN dataset demonstrate that for the most challenging five-class classification task, GRC-Net achieved an accuracy of 93.66%, outperforming existing methods.

</details>


### [621] [Fractional Differential Equation Physics-Informed Neural Network and Its Application in Battery State Estimation](https://arxiv.org/abs/2512.12285)
*Lujuan Dang,Zilai Wang*

Main category: cs.LG

Relevance: 15.0

TL;DR: 提出FDIFF-PINN架构，结合分数阶微积分与深度学习，用于锂离子电池荷电状态估计，相比传统神经网络能更好表征电化学过程的非线性与记忆依赖动态特性。


<details>
  <summary>Details</summary>
Motivation: 传统数据驱动的神经网络模型难以充分表征电化学过程固有的复杂非线性和记忆依赖动态特性，限制了其在动态工况下的预测精度和物理解释性，需要新的架构来提升锂离子电池SOC估计的准确性。

Method: 提出分数阶微分方程物理信息神经网络(FDIFF-PINN)，基于分数阶等效电路模型构建离散化分数阶偏微分方程，结合分数阶微积分与深度学习，在松下18650PF电池多温度条件(-10°C至20°C)动态充放电数据集上进行对比实验。

Result: 论文通过对比实验验证了FDIFF-PINN架构在锂离子电池SOC估计中的有效性，相比传统神经网络模型能够更好地表征电化学过程的复杂动态特性，提升预测精度和物理可解释性。

Conclusion: FDIFF-PINN架构通过融合分数阶微积分与深度学习，为锂离子电池SOC估计提供了更准确、物理可解释的解决方案，能够更好地处理电化学过程的非线性记忆依赖特性。

Abstract: Accurate estimation of the State of Charge (SOC) is critical for ensuring the safety, reliability, and performance optimization of lithium-ion battery systems. Conventional data-driven neural network models often struggle to fully characterize the inherent complex nonlinearities and memory-dependent dynamics of electrochemical processes, significantly limiting their predictive accuracy and physical interpretability under dynamic operating conditions. To address this challenge, this study proposes a novel neural architecture termed the Fractional Differential Equation Physics-Informed Neural Network (FDIFF-PINN), which integrates fractional calculus with deep learning. The main contributions of this paper include: (1) Based on a fractional-order equivalent circuit model, a discretized fractional-order partial differential equation is constructed. (2) Comparative experiments were conducted using a dynamic charge/discharge dataset of Panasonic 18650PF batteries under multi-temperature conditions (from -10$^{\circ}$C to 20$^{\circ}$C).

</details>


### [622] [Eventually LIL Regret: Almost Sure $\ln\ln T$ Regret for a sub-Gaussian Mixture on Unbounded Data](https://arxiv.org/abs/2512.12325)
*Shubhada Agrawal,Aaditya Ramdas*

Main category: cs.LG

Relevance: 15.0

TL;DR: 该论文证明了Robbins提出的经典次高斯混合模型满足路径式确定性遗憾界，建立了对抗性在线学习与博弈论统计之间的桥梁


<details>
  <summary>Details</summary>
Motivation: 动机是弥合对抗性在线学习（通常处理有界数据的遗憾界）与博弈论统计（可以处理无界数据但需要随机假设）之间的差距，通过条件遗憾界作为随机和对抗性投注的桥梁

Method: 证明经典次高斯混合模型满足路径式确定性遗憾界，在自然"Ville事件"中，遗憾界由累积方差过程和对数项组成

Result: 证明了在Ville事件E_α中，遗憾界为ln²(1/α)/V_T + ln(1/α) + ln ln V_T；在概率为1的E_0事件中，遗憾最终由ln ln V_T界定

Conclusion: 条件遗憾界可以作为随机和对抗性投注之间的桥梁，统一了处理有界数据的对抗性学习和处理无界数据的随机方法

Abstract: We prove that a classic sub-Gaussian mixture proposed by Robbins in a stochastic setting actually satisfies a path-wise (deterministic) regret bound. For every path in a natural ``Ville event'' $E_α$, this regret till time $T$ is bounded by $\ln^2(1/α)/V_T + \ln (1/α) + \ln \ln V_T$ up to universal constants, where $V_T$ is a nonnegative, nondecreasing, cumulative variance process. (The bound reduces to $\ln(1/α) + \ln \ln V_T$ if $V_T \geq \ln(1/α)$.) If the data were stochastic, then one can show that $E_α$ has probability at least $1-α$ under a wide class of distributions (eg: sub-Gaussian, symmetric, variance-bounded, etc.). In fact, we show that on the Ville event $E_0$ of probability one, the regret on every path in $E_0$ is eventually bounded by $\ln \ln V_T$ (up to constants). We explain how this work helps bridge the world of adversarial online learning (which usually deals with regret bounds for bounded data), with game-theoretic statistics (which can handle unbounded data, albeit using stochastic assumptions). In short, conditional regret bounds serve as a bridge between stochastic and adversarial betting.

</details>


### [623] [Synthetic Swarm Mosquito Dataset for Acoustic Classification: A Proof of Concept](https://arxiv.org/abs/2512.12365)
*Thai-Duy Dinh,Minh-Luan Vo,Cuong Tuan Nguyen,Bich-Hien Vo*

Main category: cs.LG

Relevance: 15.0

TL;DR: 该论文提出了一个用于声学分类的合成蚊群数据集，通过模拟真实多物种和嘈杂环境来加速蚊媒疾病研究，并评估了轻量级深度学习模型在嵌入式设备上的部署潜力。


<details>
  <summary>Details</summary>
Motivation: 蚊媒疾病每年导致超过70万人死亡，构成严重的全球健康威胁。传统蚊虫声学数据集需要耗费大量人力记录单个蚊子，限制了数据规模和研究的可扩展性。因此需要开发合成数据集方法来克服这些限制。

Method: 1. 创建合成蚊群声学数据集，模拟真实的多物种和嘈杂环境条件
2. 使用log-mel频谱图作为特征表示
3. 评估轻量级深度学习架构进行蚊虫物种分类
4. 针对嵌入式低功耗设备的部署可行性进行测试

Result: 1. 成功创建了概念验证的合成蚊群数据集
2. 轻量级深度学习模型能够有效识别六种主要蚊媒物种
3. 模型适合在嵌入式低功耗设备上部署
4. 证明了合成声学数据集在加速蚊虫声学研究方面的潜力

Conclusion: 合成蚊群声学数据集方法能够实现可扩展的数据生成，减少人力资源需求，为实时监测解决方案提供技术支持，有望加速蚊媒疾病声学研究的发展。

Abstract: Mosquito-borne diseases pose a serious global health threat, causing over 700,000 deaths annually. This work introduces a proof-of-concept Synthetic Swarm Mosquito Dataset for Acoustic Classification, created to simulate realistic multi-species and noisy swarm conditions. Unlike conventional datasets that require labor-intensive recording of individual mosquitoes, the synthetic approach enables scalable data generation while reducing human resource demands. Using log-mel spectrograms, we evaluated lightweight deep learning architectures for the classification of mosquito species. Experiments show that these models can effectively identify six major mosquito vectors and are suitable for deployment on embedded low-power devices. The study demonstrates the potential of synthetic swarm audio datasets to accelerate acoustic mosquito research and enable scalable real-time surveillance solutions.

</details>


### [624] [DeepVekua: Geometric-Spectral Representation Learning for Physics-Informed Fields](https://arxiv.org/abs/2512.12402)
*Vladimer Khasia*

Main category: cs.LG

Relevance: 15.0

TL;DR: DeepVekua是一种混合架构，将几何深度学习与谱分析相结合，用于解决稀疏数据条件下的偏微分方程。通过学习将复杂几何映射到潜在调和空间的微分同胚坐标变换，该方法在平流-扩散系统上超越了最先进的隐式表示方法。


<details>
  <summary>Details</summary>
Motivation: 传统基于坐标的网络在处理偏微分方程时存在谱偏差问题，特别是在稀疏数据条件下。DeepVekua旨在通过分离几何学习和物理学习来解决这一问题，将复杂几何映射到调和空间，从而更有效地求解偏微分方程。

Method: DeepVekua采用混合架构，结合几何深度学习和谱分析。核心是学习一个微分同胚坐标变换，将复杂几何映射到潜在调和空间。该方法将几何学习与物理学习分离，在调和空间中以闭式求解最优谱权重，避免了传统方法的谱偏差问题。

Result: 在平流-扩散系统上，DeepVekua超越了最先进的隐式表示方法，相比谱基线方法实现了100倍的性能提升。代码已在GitHub开源。

Conclusion: DeepVekua通过将几何学习与物理学习分离，成功解决了传统基于坐标网络的谱偏差问题，为稀疏数据条件下的偏微分方程求解提供了有效的混合架构。

Abstract: We present DeepVekua, a hybrid architecture that unifies geometric deep learning with spectral analysis to solve partial differential equations (PDEs) in sparse data regimes. By learning a diffeomorphic coordinate transformation that maps complex geometries to a latent harmonic space, our method outperforms state-of-the-art implicit representations on advection-diffusion systems. Unlike standard coordinate-based networks which struggle with spectral bias, DeepVekua separates the learning of geometry from the learning of physics, solving for optimal spectral weights in closed form. We demonstrate a 100x improvement over spectral baselines. The code is available at https://github.com/VladimerKhasia/vekuanet.

</details>


### [625] [Rough Sets for Explainability of Spectral Graph Clustering](https://arxiv.org/abs/2512.12436)
*Bartłomiej Starosta,Sławomir T. Wierzchoń,Piotr Borkowski,Dariusz Czerski,Marcin Sydow,Eryk Laskowski,Mieczysław A. Kłopotek*

Main category: cs.LG

Relevance: 15.0

TL;DR: 该论文提出了一种基于粗糙集理论的增强解释方法，用于改进图谱聚类在文本文档聚类中的可解释性问题。


<details>
  <summary>Details</summary>
Motivation: 图谱聚类方法能够识别不同形状和密度的簇，但在文本文档聚类中难以向用户解释结果，主要因为谱空间嵌入与文档内容没有明显关系，加上文档内容不清晰和聚类算法的随机性进一步降低了可解释性。

Method: 基于粗糙集理论对团队先前提出的解释方法进行增强，以克服上述问题。

Result: 论文提出了改进的解释方法，但摘要中未提供具体实验结果。

Conclusion: 通过粗糙集理论的启发，增强了图谱聚类在文本文档聚类中的可解释性。

Abstract: Graph Spectral Clustering methods (GSC) allow representing clusters of diverse shapes, densities, etc. However, the results of such algorithms, when applied e.g. to text documents, are hard to explain to the user, especially due to embedding in the spectral space which has no obvious relation to document contents. Furthermore, the presence of documents without clear content meaning and the stochastic nature of the clustering algorithms deteriorate explainability. This paper proposes an enhancement to the explanation methodology, proposed in an earlier research of our team. It allows us to overcome the latter problems by taking inspiration from rough set theory.

</details>


### [626] [Cross-Modal Representational Knowledge Distillation for Enhanced Spike-Informed LFP Modeling](https://arxiv.org/abs/2512.12461)
*Eray Erturk,Saba Hashemi,Maryam M. Shanechi*

Main category: cs.LG

Relevance: 15.0

TL;DR: 该论文提出了一种跨模态知识蒸馏框架，将预训练的多会话尖峰Transformer模型的高保真表征知识迁移到LFP Transformer模型中，以解决LFP信号建模的挑战。


<details>
  <summary>Details</summary>
Motivation: 局部场电位（LFPs）在神经实验中具有重要价值，但现有神经建模框架主要关注尖峰活动，因为LFP信号具有聚合、群体层面的特性，导致对下游任务变量的预测能力较低。需要一种方法来利用高性能尖峰模型开发更准确的LFP模型。

Method: 采用跨模态知识蒸馏框架：1）首先使用掩码自编码目标和会话特定的神经标记化策略训练多会话尖峰教师模型；2）然后将学生LFP模型的潜在表征与教师尖峰模型对齐。

Result: 蒸馏后的LFP模型在完全无监督和监督设置下均优于单会话和多会话LFP基线，并且能够泛化到其他会话而无需额外蒸馏，同时保持优越性能。

Conclusion: 跨模态知识蒸馏是一种强大且可扩展的方法，能够利用高性能尖峰模型开发更准确的LFP模型。

Abstract: Local field potentials (LFPs) can be routinely recorded alongside spiking activity in intracortical neural experiments, measure a larger complementary spatiotemporal scale of brain activity for scientific inquiry, and can offer practical advantages over spikes, including greater long-term stability, robustness to electrode degradation, and lower power requirements. Despite these advantages, recent neural modeling frameworks have largely focused on spiking activity since LFP signals pose inherent modeling challenges due to their aggregate, population-level nature, often leading to lower predictive power for downstream task variables such as motor behavior. To address this challenge, we introduce a cross-modal knowledge distillation framework that transfers high-fidelity representational knowledge from pretrained multi-session spike transformer models to LFP transformer models. Specifically, we first train a teacher spike model across multiple recording sessions using a masked autoencoding objective with a session-specific neural tokenization strategy. We then align the latent representations of the student LFP model to those of the teacher spike model. Our results show that the Distilled LFP models consistently outperform single- and multi-session LFP baselines in both fully unsupervised and supervised settings, and can generalize to other sessions without additional distillation while maintaining superior performance. These findings demonstrate that cross-modal knowledge distillation is a powerful and scalable approach for leveraging high-performing spike models to develop more accurate LFP models.

</details>


### [627] [Noise-robust Contrastive Learning for Critical Transition Detection in Dynamical Systems](https://arxiv.org/abs/2512.12523)
*Wenqi Fang,Ye Li*

Main category: cs.LG

Relevance: 15.0

TL;DR: 提出一种基于奇异值分解和半正交约束训练的神经网络架构，用于从复杂噪声时间序列数据中检测临界转变，相比传统对比学习方法更轻量且抗噪性更强。


<details>
  <summary>Details</summary>
Motivation: 在复杂噪声时间序列数据中检测临界转变是一个基础挑战。传统基于深度神经网络的对比学习方法通常过参数化且对无关噪声敏感，导致临界点识别不准确。需要更鲁棒和高效的检测方法。

Method: 提出一种基于奇异值分解技术构建的神经网络架构，配合严格半正交约束训练算法，增强传统对比学习的性能。该方法通过数学约束减少过参数化，提高对噪声的鲁棒性。

Result: 大量实验表明，该方法在识别临界转变方面与传统对比学习技术性能相当，但更轻量且抗噪性显著更强。

Conclusion: 通过奇异值分解和半正交约束训练，可以构建更高效、更鲁棒的临界转变检测方法，解决传统对比学习方法的过参数化和噪声敏感问题。

Abstract: Detecting critical transitions in complex, noisy time-series data is a fundamental challenge across science and engineering. Such transitions may be anticipated by the emergence of a low-dimensional order parameter, whose signature is often masked by high-amplitude stochastic variability. Standard contrastive learning approaches based on deep neural networks, while promising for detecting critical transitions, are often overparameterized and sensitive to irrelevant noise, leading to inaccurate identification of critical points. To address these limitations, we propose a neural network architecture, constructed using singular value decomposition technique, together with a strictly semi-orthogonality-constrained training algorithm, to enhance the performance of traditional contrastive learning. Extensive experiments demonstrate that the proposed method matches the performance of traditional contrastive learning techniques in identifying critical transitions, yet is considerably more lightweight and markedly more resistant to noise.

</details>


### [628] [Optimal Mistake Bounds for Transductive Online Learning](https://arxiv.org/abs/2512.12567)
*Zachary Chase,Steve Hanneke,Shay Moran,Jonathan Shafer*

Main category: cs.LG

Relevance: 15.0

TL;DR: 该论文解决了在线学习中一个30年未解的问题，精确量化了转导学习与标准在线学习之间的差距。研究发现转导学习的错误下界为Ω(√d)，相比之前的结果有指数级改进，并证明这个下界是紧的，建立了转导学习与标准在线学习之间的二次方差距。


<details>
  <summary>Details</summary>
Motivation: 解决在线学习中关于无标签数据效力的30年开放问题。在标准在线学习中，最优错误边界由概念类的Littlestone维度d表征，但转导学习（提前访问未标记实例序列）的错误边界一直未得到精确量化。之前的研究只给出了较弱的下界（Ω(log log d)、Ω(√log d)、Ω(log d)），需要确定转导学习的精确错误边界。

Method: 通过理论分析证明转导学习的错误下界为Ω(√d)，相比之前的结果有指数级改进。同时构造了一个Littlestone维度为d的概念类，其转导错误边界为O(√d)，证明下界是紧的。还改进了Ben-David等人(1997)的(2/3)d上界。

Result: 1. 证明了转导学习的错误下界为Ω(√d)，相比之前结果有指数级改进
2. 证明这个下界是紧的：存在Littlestone维度为d的概念类，其转导错误边界为O(√d)
3. 建立了转导学习与标准在线学习之间的二次方差距
4. 改进了之前(2/3)d的上界

Conclusion: 该研究解决了30年的开放问题，精确量化了转导学习与标准在线学习的差距为二次方关系。这突显了提前访问未标记实例序列的显著优势，与PAC设置中两种学习方式样本复杂度相似的情况形成对比。

Abstract: We resolve a 30-year-old open problem concerning the power of unlabeled data in online learning by tightly quantifying the gap between transductive and standard online learning. In the standard setting, the optimal mistake bound is characterized by the Littlestone dimension $d$ of the concept class $H$ (Littlestone 1987). We prove that in the transductive setting, the mistake bound is at least $Ω(\sqrt{d})$. This constitutes an exponential improvement over previous lower bounds of $Ω(\log\log d)$, $Ω(\sqrt{\log d})$, and $Ω(\log d)$, due respectively to Ben-David, Kushilevitz, and Mansour (1995, 1997) and Hanneke, Moran, and Shafer (2023). We also show that this lower bound is tight: for every $d$, there exists a class of Littlestone dimension $d$ with transductive mistake bound $O(\sqrt{d})$. Our upper bound also improves upon the best known upper bound of $(2/3)d$ from Ben-David, Kushilevitz, and Mansour (1997). These results establish a quadratic gap between transductive and standard online learning, thereby highlighting the benefit of advance access to the unlabeled instance sequence. This contrasts with the PAC setting, where transductive and standard learning exhibit similar sample complexities.

</details>


### [629] [Differentiable Energy-Based Regularization in GANs: A Simulator-Based Exploration of VQE-Inspired Auxiliary Losses](https://arxiv.org/abs/2512.12581)
*David Strnadel*

Main category: cs.LG

Relevance: 15.0

TL;DR: 探索性概念验证：将参数化量子电路的可微能量项作为GAN的正则化信号，在MNIST上显示分类准确率提升但样本质量指标波动较大。


<details>
  <summary>Details</summary>
Motivation: 探索量子计算与生成对抗网络的交叉领域，研究量子电路衍生的能量项能否作为GAN训练中的辅助正则化信号，为量子机器学习提供新的方法学思路。

Method: 在辅助分类器GAN（ACGAN）生成器目标函数中，加入基于变分量子本征求解器（VQE）启发的能量项。该能量项使用Qiskit的EstimatorQNN和TorchConnector从类别特定的伊辛哈密顿量计算得出。实验在无噪声的状态向量模拟器上进行，仅使用4个量子比特。

Result: 在MNIST数据集上，量子正则化模型（QACGAN）在5个epoch内达到99-100%的分类准确率，而传统ACGAN为87.8%。但样本质量指标（FID）在运行间波动较大（变异系数约25%），最终稳定在FID 23-24，与传统ACGAN基线相当。计算开销约为传统方法的200倍（主要来自模拟器开销）。

Conclusion: 该方法证明了VQE风格的能量计算可以通过可微路径集成到GAN训练循环中，但未证明量子优势或稳定性提升。量子辅助信号是否优于等效经典正则化器仍需系统消融研究。

Abstract: This paper presents an exploratory, simulator-based proof of concept investigating whether differentiable energy terms derived from parameterized quantum circuits can serve as auxiliary regularization signals in Generative Adversarial Networks (GANs). We augment the Auxiliary Classifier GAN (ACGAN) generator objective with a Variational Quantum Eigensolver (VQE)-inspired energy term computed from class-specific Ising Hamiltonians using Qiskit's EstimatorQNN and TorchConnector.
  Important limitations: All experiments run on a noiseless statevector simulator with only 4 qubits, use a deliberately simple Hamiltonian parameterization, and lack ablation studies comparing against equivalent classical biases. The computational overhead (approximately 200x slower than classical ACGAN) reflects simulator artifacts rather than inherent quantum costs.
  On MNIST, we observe that the energy-regularized model (termed QACGAN) achieves high classification accuracy (99 to 100 percent) within 5 epochs compared to 87.8 percent for ACGAN, suggesting the auxiliary term influences class conditioning. However, sample quality metrics (FID) show high variance across runs (coefficient of variation approximately 25 percent at epoch 5), with values ranging from 19.92 to 35.96. Extended runs stabilize around FID 23 to 24, comparable to the ACGAN baseline.
  We explicitly do not claim quantum advantage, improved stability in any general sense, or scalability beyond this toy setting. The contribution is methodological: demonstrating that VQE-style energy computations can be integrated into GAN training loops via differentiable pathways. Whether such auxiliary signals provide benefits beyond equivalent classical regularizers remains an open question requiring systematic ablation studies, which we leave for future work.

</details>


### [630] [Network Level Evaluation of Hangup Susceptibility of HRGCs using Deep Learning and Sensing Techniques: A Goal Towards Safer Future](https://arxiv.org/abs/2512.12832)
*Kaustav Chatterjee,Joshua Li,Kundan Parajulee,Jared Schwennesen*

Main category: cs.LG

Relevance: 15.0

TL;DR: 该研究开发了一个评估公路铁路平交道口（HRGC）悬挂风险的框架，结合激光成像系统数据、深度学习模型重建道口剖面，并收集特种车辆尺寸数据进行风险分析。


<details>
  <summary>Details</summary>
Motivation: 陡峭剖面的公路铁路平交道口对低底盘车辆构成安全隐患，车辆可能被困在轨道上导致列车碰撞事故。需要开发网络级别的评估框架来识别高风险道口。

Method: 1. 使用步行剖面仪和Pave3D8K激光成像系统收集道口剖面数据；2. 开发LSTM-Transformer混合深度学习模型从激光数据重建准确剖面；3. 收集约350辆特种车辆的尺寸数据；4. 使用三种车辆尺寸场景分析悬挂风险；5. 开发ArcGIS数据库和软件界面。

Result: 在不同车辆尺寸场景下，分别识别出36、62和67个最高风险级别的道口。开发了实用的决策支持工具，将新一代传感、深度学习和基础设施数据集成到安全评估中。

Conclusion: 该框架通过整合先进传感技术、深度学习模型和基础设施数据，为交通机构提供了实用的决策支持工具，推进了道口安全评估方法。

Abstract: Steep profiled Highway Railway Grade Crossings (HRGCs) pose safety hazards to vehicles with low ground clearance, which may become stranded on the tracks, creating risks of train vehicle collisions. This research develops a framework for network level evaluation of hangup susceptibility of HRGCs. Profile data from different crossings in Oklahoma were collected using both a walking profiler and the Pave3D8K Laser Imaging System. A hybrid deep learning model, combining Long Short Term Memory (LSTM) and Transformer architectures, was developed to reconstruct accurate HRGC profiles from Pave3D8K Laser Imaging System data. Vehicle dimension data from around 350 specialty vehicles were collected at various locations across Oklahoma to enable up to date statistical design dimensions. Hangup susceptibility was analyzed using three vehicle dimension scenarios (a) median dimension (median wheelbase and ground clearance), (b) 75 25 percentile dimension (75 percentile wheelbase, 25 percentile ground clearance), and (c) worst case dimension (maximum wheelbase and minimum ground clearance). Results indicate 36, 62, and 67 crossings at the highest hangup risk levels under these scenarios, respectively. An ArcGIS database and a software interface were developed to support transportation agencies in mitigating crossing hazards. This framework advances safety evaluation by integrating next generation sensing, deep learning, and infrastructure datasets into practical decision support tools.

</details>


### [631] [Next-generation reservoir computing validated by classification task](https://arxiv.org/abs/2512.12903)
*Ken-ichi Kitayama*

Main category: cs.LG

Relevance: 15.0

TL;DR: NG-RC（下一代储层计算）首次被证明能够执行分类任务，性能与传统的储层计算相当，验证了其在预测和分类任务中的通用计算能力。


<details>
  <summary>Details</summary>
Motivation: NG-RC作为一种新兴计算范式，之前的研究主要局限于时间序列预测任务（如Lorenz 63吸引子和Mackey-Glass混沌信号），缺乏对其分类能力的验证。本文旨在证明NG-RC在分类任务上也能达到传统储层计算的性能水平。

Method: NG-RC不需要实际的储层进行输入数据混合，而是直接从时间序列输入计算多项式项。研究首次将NG-RC应用于分类任务，并与传统储层计算方法进行比较。

Result: NG-RC在分类任务上表现出与传统储层计算相当的性能，验证了其在预测和分类任务中的通用计算能力。

Conclusion: NG-RC不仅适用于时间序列预测任务，也能有效执行分类任务，展示了其作为通用计算范式的潜力。

Abstract: An emerging computing paradigm, so-called next-generation reservoir computing (NG-RC) is investigated. True to its namesake, NG-RC requires no actual reservoirs for input data mixing but rather computing the polynomial terms directly from the time series inputs. However, benchmark tests so far reported have been one-sided, limited to prediction tasks of temporal waveforms such as Lorenz 63 attractor and Mackey-Glass chaotic signal. We will demonstrate for the first time that NG-RC can perform classification task as good as conventional RC. This validates the versatile computational capability of NG-RC in tasks of both prediction and classification.

</details>


### [632] [Machine Learning Architectures for the Estimation of Predicted Occupancy Grids in Road Traffic](https://arxiv.org/abs/2512.12907)
*Parthasarathy Nadarajan,Michael Botsch,Sebastian Sardina*

Main category: cs.LG

Relevance: 15.0

TL;DR: 提出一种用于高效估计复杂交通场景概率时空表示的新型机器学习架构，使用堆叠去噪自编码器和随机森林预测交通参与者的未来状态。


<details>
  <summary>Details</summary>
Motivation: 自动驾驶和主动安全系统需要详细的未来交通场景表示，准确预测交通参与者的未来时空状态对安全决策至关重要。

Method: 首先识别交通场景类型，然后使用包含两个堆叠去噪自编码器和一组随机森林的架构，将当前状态（增强占用网格）映射到可能的未来状态（预测占用网格）。

Result: 提出的架构在仿真中验证，与现有SDAs和DeconvNet架构相比，在准确性和计算时间方面进行了比较。

Conclusion: 该架构能够有效预测交通场景的概率时空表示，为主动安全系统提供重要支持。

Abstract: This paper introduces a novel machine learning architecture for an efficient estimation of the probabilistic space-time representation of complex traffic scenarios. A detailed representation of the future traffic scenario is of significant importance for autonomous driving and for all active safety systems. In order to predict the future space-time representation of the traffic scenario, first the type of traffic scenario is identified and then the machine learning algorithm maps the current state of the scenario to possible future states. The input to the machine learning algorithms is the current state representation of a traffic scenario, termed as the Augmented Occupancy Grid (AOG). The output is the probabilistic space-time representation which includes uncertainties regarding the behaviour of the traffic participants and is termed as the Predicted Occupancy Grid (POG). The novel architecture consists of two Stacked Denoising Autoencoders (SDAs) and a set of Random Forests. It is then compared with the other two existing architectures that comprise of SDAs and DeconvNet. The architectures are validated with the help of simulations and the comparisons are made both in terms of accuracy and computational time. Also, a brief overview on the applications of POGs in the field of active safety is presented.

</details>


### [633] [Application of Deep Learning in Biological Data Compression](https://arxiv.org/abs/2512.12975)
*Chunyu Zou*

Main category: cs.LG

Relevance: 15.0

TL;DR: 该论文提出使用隐式神经表示（INR）来压缩冷冻电镜（Cryo-EM）生物数据，通过提取二进制密度图、GZIP压缩重复结构，并用神经网络编码空间密度信息，结合位置编码和加权MSE损失来提高重建精度。


<details>
  <summary>Details</summary>
Motivation: 冷冻电镜数据文件存储空间巨大，给研究人员和教育工作者带来显著挑战。需要一种实用高效的生物数据压缩解决方案，既能保持合理的压缩比，又能维持文件间的重建质量。

Method: 1. 根据密度阈值提取每个文件的二进制图；2. 利用GZIP压缩高度重复的密度图；3. 使用神经网络编码空间密度信息，存储网络参数和可学习潜在向量；4. 加入位置编码增强空间表示；5. 采用加权均方误差损失函数平衡密度分布变化。

Result: 论文旨在提供实用的生物数据压缩解决方案，用于教育和研究目的，同时保持合理的压缩比和文件间的重建质量（具体数值结果未在摘要中提供）。

Conclusion: 隐式神经表示（INR）可以有效地压缩冷冻电镜生物数据，为教育和研究提供实用的数据压缩方案。

Abstract: Cryogenic electron microscopy (Cryo-EM) has become an essential tool for capturing high-resolution biological structures. Despite its advantage in visualizations, the large storage size of Cryo-EM data file poses significant challenges for researchers and educators. This paper investigates the application of deep learning, specifically implicit neural representation (INR), to compress Cryo-EM biological data. The proposed approach first extracts the binary map of each file according to the density threshold. The density map is highly repetitive, ehich can be effectively compressed by GZIP. The neural network then trains to encode spatial density information, allowing the storage of network parameters and learnable latent vectors. To improve reconstruction accuracy, I further incorporate the positional encoding to enhance spatial representation and a weighted Mean Squared Error (MSE) loss function to balance density distribution variations. Using this approach, my aim is to provide a practical and efficient biological data compression solution that can be used for educational and research purpose, while maintaining a reasonable compression ratio and reconstruction quality from file to file.

</details>


### [634] [Deep Learning-Driven Inversion Framework for Shear Modulus Estimation in Magnetic Resonance Elastography (DIME)](https://arxiv.org/abs/2512.13010)
*Hassan Iftikhar,Rizwan Ahmad,Arunark Kolipaka*

Main category: cs.LG

Relevance: 15.0

TL;DR: 提出DIME框架，通过深度学习从MRE位移场直接估计剪切模量，相比传统MMDI方法更稳健、对噪声不敏感，在合成和真实肝脏数据中表现优异。


<details>
  <summary>Details</summary>
Motivation: 传统MMDI算法基于Helmholtz方程，假设均匀、无限介质，且使用拉普拉斯算子对噪声敏感，导致刚度估计精度和可靠性受限。需要更稳健的反演方法。

Method: 提出DIME深度学习框架，使用有限元模拟生成的位移场-刚度图对进行训练。采用小图像块训练以捕捉局部波行为，提高对全局图像变化的鲁棒性。

Result: 在合成数据中，DIME产生低像素间变异性、准确边界划分的刚度图，与真实值相关性高于MMDI。在真实肝脏数据中，DIME保持生理一致的刚度模式，与MMDI结果接近但偏差更小。

Conclusion: DIME相比传统MMDI方法更稳健，对噪声不敏感，能更准确地估计组织剪切刚度，具有临床应用潜力。

Abstract: The Multimodal Direct Inversion (MMDI) algorithm is widely used in Magnetic Resonance Elastography (MRE) to estimate tissue shear stiffness. However, MMDI relies on the Helmholtz equation, which assumes wave propagation in a uniform, homogeneous, and infinite medium. Furthermore, the use of the Laplacian operator makes MMDI highly sensitive to noise, which compromises the accuracy and reliability of stiffness estimates. In this study, we propose the Deep-Learning driven Inversion Framework for Shear Modulus Estimation in MRE (DIME), aimed at enhancing the robustness of inversion. DIME is trained on the displacement fields-stiffness maps pair generated through Finite Element Modelling (FEM) simulations. To capture local wave behavior and improve robustness to global image variations, DIME is trained on small image patches. We first validated DIME using homogeneous and heterogeneous datasets simulated with FEM, where DIME produced stiffness maps with low inter-pixel variability, accurate boundary delineation, and higher correlation with ground truth (GT) compared to MMDI. Next, DIME was evaluated in a realistic anatomy-informed simulated liver dataset with known GT and compared directly to MMDI. DIME reproduced ground-truth stiffness patterns with high fidelity (r = 0.99, R^2 = 0.98), while MMDI showed greater underestimation. After validating DIME on synthetic data, we tested the model in in vivo liver MRE data from eight healthy and seven fibrotic subjects. DIME preserved physiologically consistent stiffness patterns and closely matched MMDI, which showed directional bias. Overall, DIME showed higher correlation with ground truth and visually similar stiffness patterns, whereas MMDI displayed a larger bias that can potentially be attributed to directional filtering. These preliminary results highlight the feasibility of DIME for clinical applications in MRE.

</details>


### [635] [Multiclass Graph-Based Large Margin Classifiers: Unified Approach for Support Vectors and Neural Networks](https://arxiv.org/abs/2512.13410)
*Vítor M. Hanriot,Luiz C. B. Torres,Antônio P. Braga*

Main category: cs.LG

Relevance: 15.0

TL;DR: 该论文提出了一种基于Gabriel图(GG)的几何分类方法，改进了Chipclass分类器，通过引入更平滑的激活函数和结构支持向量(SSV)中心神经元，并扩展了神经网络架构，支持反向传播训练或线性方程组求解。还提出了新的子图/距离成员函数和图正则化方法，以及计算效率更高的GG重计算算法。


<details>
  <summary>Details</summary>
Motivation: 传统大间隔分类器通常基于优化框架，而支持向量可以通过几何方法获得。本文旨在推进Gabriel图在二元和多类分类问题中的应用，改进现有的GG-based分类器，使其具有更平滑的分类边界和更好的性能。

Method: 1. 改进Chipclass分类器：引入更平滑的激活函数和结构支持向量(SSV)中心神经元；2. 扩展神经网络架构，支持两种训练方式：反向传播+softmax+交叉熵损失，或线性方程组求解；3. 提出新的子图/距离成员函数用于图正则化；4. 开发计算效率更高的Gabriel图重计算算法。

Result: 实验结果显示，该方法优于之前的GG-based分类器，并且与基于树的模型在统计上等效（通过Friedman检验验证）。

Conclusion: 该研究展示了基于几何Gabriel图的分类方法在二元和多类分类问题中的有效性，提供了优化框架之外的替代方案，并在性能上与传统方法相当。

Abstract: While large margin classifiers are originally an outcome of an optimization framework, support vectors (SVs) can be obtained from geometric approaches. This article presents advances in the use of Gabriel graphs (GGs) in binary and multiclass classification problems. For Chipclass, a hyperparameter-less and optimization-less GG-based binary classifier, we discuss how activation functions and support edge (SE)-centered neurons affect the classification, proposing smoother functions and structural SV (SSV)-centered neurons to achieve margins with low probabilities and smoother classification contours. We extend the neural network architecture, which can be trained with backpropagation with a softmax function and a cross-entropy loss, or by solving a system of linear equations. A new subgraph-/distance-based membership function for graph regularization is also proposed, along with a new GG recomputation algorithm that is less computationally expensive than the standard approach. Experimental results with the Friedman test show that our method was better than previous GG-based classifiers and statistically equivalent to tree-based models.

</details>


### [636] [SSAS: Cross-subject EEG-based Emotion Recognition through Source Selection with Adversarial Strategy](https://arxiv.org/abs/2512.13458)
*Yici Liu,Qi Wei Oung,Hoi Leong Lee*

Main category: cs.LG

Relevance: 15.0

TL;DR: 提出了一种用于跨被试EEG情感识别的源选择对抗策略方法，通过源选择网络和对抗策略网络解决个体差异和负迁移问题，在SEED和SEED-IV数据集上取得优异性能。


<details>
  <summary>Details</summary>
Motivation: 跨被试EEG情感识别在实际应用中具有重要价值，但现有研究大多忽视了个体间差异和训练过程中的负迁移现象，导致模型性能受限。

Method: 提出两阶段方法：1) 源选择网络(SS)通过反转域适应训练过程，破坏类别可分性并放大域间差异，迫使模型学习域不变但情感相关的表征；2) 对抗策略网络(AS)利用SS的源域选择结果和预训练的域判别器，在对抗训练中计算新颖损失以增强域分类性能，确保对抗策略的平衡。

Result: 在SEED和SEED-IV两个EEG情感数据集上取得了优异的性能表现，证明了方法的有效性。

Conclusion: 提出的源选择对抗策略方法能够有效解决跨被试EEG情感识别中的个体差异和负迁移问题，为情感脑机接口的实际应用提供了有力工具。

Abstract: Electroencephalographic (EEG) signals have long been applied in the field of affective brain-computer interfaces (aBCIs). Cross-subject EEG-based emotion recognition has demonstrated significant potential in practical applications due to its suitability across diverse people. However, most studies on cross-subject EEG-based emotion recognition neglect the presence of inter-individual variability and negative transfer phenomena during model training. To address this issue, a cross-subject EEG-based emotion recognition through source selection with adversarial strategy is introduced in this paper. The proposed method comprises two modules: the source selection network (SS) and the adversarial strategies network (AS). The SS uses domain labels to reverse-engineer the training process of domain adaptation. Its key idea is to disrupt class separability and magnify inter-domain differences, thereby raising the classification difficulty and forcing the model to learn domain-invariant yet emotion-relevant representations. The AS gets the source domain selection results and the pretrained domain discriminators from SS. The pretrained domain discriminators compute a novel loss aimed at enhancing the performance of domain classification during adversarial training, ensuring the balance of adversarial strategies. This paper provides theoretical insights into the proposed method and achieves outstanding performance on two EEG-based emotion datasets, SEED and SEED-IV. The code can be found at https://github.com/liuyici/SSAS.

</details>


### [637] [DP-CSGP: Differentially Private Stochastic Gradient Push with Compressed Communication](https://arxiv.org/abs/2512.13583)
*Zehan Zhu,Heng Zhao,Yan Huang,Joey Tianyi Zhou,Shouling Ji,Jinming Xu*

Main category: cs.LG

Relevance: 15.0

TL;DR: 提出DP-CSGP算法，用于有向图上的去中心化学习，结合差分隐私、梯度推送和压缩通信，在保证隐私的同时降低通信成本


<details>
  <summary>Details</summary>
Motivation: 现有去中心化学习算法在隐私保护和通信效率方面存在不足，需要在保证差分隐私的同时减少通信开销，特别是在有向图拓扑下

Method: 提出DP-CSGP算法，结合差分隐私随机梯度推送和压缩通信技术，适用于有向图拓扑，处理非凸平滑目标函数

Result: 算法达到紧致的效用界O(√(d log(1/δ))/(√nJε))，与精确通信的去中心化算法相当，实验显示在相同隐私预算下获得可比精度但通信成本显著降低

Conclusion: DP-CSGP算法在保证严格差分隐私的同时，实现了高效通信和模型效用的平衡，为去中心化学习提供了实用解决方案

Abstract: In this paper, we propose a Differentially Private Stochastic Gradient Push with Compressed communication (termed DP-CSGP) for decentralized learning over directed graphs. Different from existing works, the proposed algorithm is designed to maintain high model utility while ensuring both rigorous differential privacy (DP) guarantees and efficient communication. For general non-convex and smooth objective functions, we show that the proposed algorithm achieves a tight utility bound of $\mathcal{O}\left( \sqrt{d\log \left( \frac{1}δ \right)}/(\sqrt{n}Jε) \right)$ ($J$ and $d$ are the number of local samples and the dimension of decision variables, respectively) with $\left(ε, δ\right)$-DP guarantee for each node, matching that of decentralized counterparts with exact communication. Extensive experiments on benchmark tasks show that, under the same privacy budget, DP-CSGP achieves comparable model accuracy with significantly lower communication cost than existing decentralized counterparts with exact communication.

</details>


### [638] [Interval Fisher's Discriminant Analysis and Visualisation](https://arxiv.org/abs/2512.11945)
*Diogo Pinheiro,M. Rosário Oliveira,Igor Kravchenko,Lina Oliveira*

Main category: stat.ML

Relevance: 15.0

TL;DR: 该论文将多类Fisher判别分析扩展到区间值数据，使用Moore区间算术和Mallows距离，同时考虑区间中心和范围，用于区间值观测的分类，并开发了可视化工具评估分类器性能。


<details>
  <summary>Details</summary>
Motivation: 传统数据科学中实体通常用单值测量表示，而符号数据分析扩展到更复杂的结构（如区间和直方图）来表达内部变异性。需要将多类判别分析方法扩展到区间值数据，以处理这种更复杂的数据结构。

Method: 扩展多类Fisher判别分析到区间值数据，使用Moore区间算术和Mallows距离，将Fisher目标函数推广到同时考虑区间中心和范围的贡献，通过数值最大化得到判别方向，然后用于区间值观测的分类。开发了类图、轮廓图和堆叠马赛克图等可视化工具。

Result: 提出了一个完整的区间值数据判别分析方法，包括分类算法和可视化评估工具。在实际数据集上的应用证明了该方法的有效性，能够解释区间值数据的分类结果。

Conclusion: 成功将Fisher判别分析扩展到区间值数据，开发了一套完整的分类和可视化方法，为符号数据分析中的区间值数据分类提供了有效的工具。

Abstract: In Data Science, entities are typically represented by single valued measurements. Symbolic Data Analysis extends this framework to more complex structures, such as intervals and histograms, that express internal variability. We propose an extension of multiclass Fisher's Discriminant Analysis to interval-valued data, using Moore's interval arithmetic and the Mallows' distance. Fisher's objective function is generalised to consider simultaneously the contributions of the centres and the ranges of intervals and is numerically maximised. The resulting discriminant directions are then used to classify interval-valued observations.To support visual assessment, we adapt the class map, originally introduced for conventional data, to classifiers that assign labels through minimum distance rules. We also extend the silhouette plot to this setting and use stacked mosaic plots to complement the visual display of class assignments. Together, these graphical tools provide insight into classifier performance and the strength of class membership. Applications to real datasets illustrate the proposed methodology and demonstrate its value in interpreting classification results for interval-valued data.

</details>


### [639] [Extending the application of dynamic Bayesian networks in calculating market risk: Standard and stressed expected shortfall](https://arxiv.org/abs/2512.12334)
*Eden Gross,Ryan Kruger,Francois Toerien*

Main category: q-fin.RM

Relevance: 15.0

TL;DR: 该研究将动态贝叶斯网络应用于金融市场风险度量（预期损失和压力预期损失）的估计，并与传统风险模型进行比较，发现传统模型在正态分布假设下表现最佳，而动态贝叶斯网络对尾部风险预测的贡献有限。


<details>
  <summary>Details</summary>
Motivation: 预期损失和压力预期损失已成为银行业监管的关键风险度量指标，特别是在全球金融危机等事件后。优化这些指标的估计方法对金融风险管理具有重要意义。研究旨在探索动态贝叶斯网络在极端尾部风险建模中的应用潜力。

Method: 使用S&P 500指数作为美国银行股票交易部门的代理，比较三种动态贝叶斯网络结构学习算法与多种传统市场风险模型（包括EGARCH和GARCH模型），分别采用正态分布和偏斜t分布进行回测分析。

Result: 所有模型在2.5%水平上都无法产生统计上准确的预期损失和压力预期损失预测，反映了极端尾部行为建模的困难。对于预期损失，EGARCH(1,1)模型（正态分布）表现最佳；对于压力预期损失，GARCH(1,1)模型（正态分布）表现最好。使用偏斜t分布时所有模型性能显著下降。动态贝叶斯网络的表现与历史模拟模型相当，但其对尾部预测的贡献受到单步预测在收益分布中权重较小的限制。

Conclusion: 动态贝叶斯网络在金融市场风险度量中的应用潜力有限，特别是在极端尾部风险预测方面。未来研究应探索增强前瞻性动态贝叶斯网络预测对尾部风险估计影响的加权方案。

Abstract: In the last five years, expected shortfall (ES) and stressed ES (SES) have become key required regulatory measures of market risk in the banking sector, especially following events such as the global financial crisis. Thus, finding ways to optimize their estimation is of great importance. We extend the application of dynamic Bayesian networks (DBNs) to the estimation of 10-day 97.5% ES and stressed ES, building on prior work applying DBNs to value at risk. Using the S&P 500 index as a proxy for the equities trading desk of a US bank, we compare the performance of three DBN structure-learning algorithms with several traditional market risk models, using either the normal or the skewed Student's t return distributions. Backtesting shows that all models fail to produce statistically accurate ES and SES forecasts at the 2.5% level, reflecting the difficulty of modeling extreme tail behavior. For ES, the EGARCH(1,1) model (normal) produces the most accurate forecasts, while, for SES, the GARCH(1,1) model (normal) performs best. All distribution-dependent models deteriorate substantially when using the skewed Student's t distribution. The DBNs perform comparably to the historical simulation model, but their contribution to tail prediction is limited by the small weight assigned to their one-day-ahead forecasts within the return distribution. Future research should examine weighting schemes that enhance the influence of forward-looking DBN forecasts on tail risk estimation.

</details>


### [640] [Data-driven modelling of autonomous and forced dynamical systems](https://arxiv.org/abs/2512.12432)
*Robert Szalai*

Main category: math.DS

Relevance: 15.0

TL;DR: 该论文提出了一种基于不变叶层的数据驱动物理系统建模方法，该方法准确、数据高效且实用，适用于单轨迹或多轨迹数据，并能处理强迫系统和参数依赖系统。


<details>
  <summary>Details</summary>
Motivation: 开发一种能够准确建模物理系统长期动态的数据驱动方法，解决传统方法中过拟合和欠拟合问题，同时能够处理各种强迫类型（周期、准周期、混沌）的系统。

Method: 使用不变叶层理论，通过适当选择函数表示（如多项式）及其超参数来避免过拟合/欠拟合。方法扩展到了强迫系统和参数依赖系统，利用全轨迹数据预测长期动态。通过计算不变流形附近的多重不变叶层来识别不变流形，其中一些可以是线性的，其他在不变流形小邻域内非线性但参数较少。

Result: 不变叶层方法被证明是准确、数据高效且实用的物理系统建模工具。能够处理相空间填充数据或聚集在不变流形附近的数据，可以拟合单轨迹或多轨迹，并能准确预测长期动态。方法还能将识别出的数学模型转换为规范形式，计算瞬时频率和阻尼信息。

Conclusion: 不变叶层为数据驱动的物理系统建模提供了强大框架，特别适用于处理强迫系统和参数依赖系统，能够准确预测长期动态，并通过适当的函数表示选择避免过拟合问题。

Abstract: The paper demonstrates that invariant foliations are accurate, data-efficient and practical tools for data-driven modelling of physical systems. Invariant foliations can be fitted to data that either fill the phase space or cluster about an invariant manifold. Invariant foliations can be fitted to a single trajectory or multiple trajectories. Over and underfitting are eliminated by appropriately choosing a function representation and its hyperparameters, such as polynomial orders. The paper extends invariant foliations to forced and parameter dependent systems. It is assumed that forcing is provided by a volume preserving map, and therefore the forcing can be periodic, quasi-periodic or even chaotic. The method utilises full trajectories, hence it is able to predict long-term dynamics accurately. We take into account if a forced system is reducible to an autonomous system about a steady state, similar to how Floquet theory guarantees reducibility for periodically forced systems. In order to find an invariant manifold, multiple invariant foliations are calculated in the neighbourhood of the invariant manifold. Some of the invariant foliations can be linear, while others nonlinear but only defined in a small neighbourhood of an invariant manifold, which reduces the number of parameters to be identified. An invariant manifold is recovered as the zero level set of one or more of the foliations. To interpret the results, the identified mathematical models are transformed to a canonical form and instantaneous frequency and damping information are calculated.

</details>


### [641] [An End-to-End Approach for Microgrid Probabilistic Forecasting and Robust Operation via Decision-focused Learning](https://arxiv.org/abs/2512.12755)
*Tingwei Cao,Yan Xu*

Main category: eess.SY

Relevance: 15.0

TL;DR: 论文提出了一种面向决策的端到端框架，将概率预测与鲁棒优化相结合，通过可微决策路径实现梯度反馈，以最小化决策遗憾为目标优化微电网调度。


<details>
  <summary>Details</summary>
Motivation: 可再生能源高渗透率给微电网运行带来显著不确定性和间歇性，传统顺序方法（先预测后优化）无法保证预测精度与运行目标的一致性，导致经济性和可靠性挑战。

Method: 1) 多层编码器-解码器概率预测模型；2) 两阶段鲁棒优化模型包含直接负荷控制；3) 通过可微决策路径集成，使用代理智能预测-优化损失函数实现梯度反馈；4) 端到端联合优化预测和运行。

Result: 在改进的IEEE 33总线和69总线系统案例研究中，相比传统预测优化组合方法，总运行成本和净运行成本降低高达18%，同时获得更优的预测精度和运行性能。

Conclusion: 提出的端到端决策聚焦框架通过将预测与下游决策目标对齐，有效提升了微电网在不确定性下的经济效率和鲁棒性，验证了方法的有效性和可扩展性。

Abstract: High penetration of renewable energy sources (RES) introduces significant uncertainty and intermittency into microgrid operations, posing challenges to economic and reliable scheduling. To address this, this paper proposes an end-to-end decision-focused framework that jointly optimizes probabilistic forecasting and robust operation for microgrids. A multilayer encoder-decoder (MED) probabilistic forecasting model is integrated with a two-stage robust optimization (TSRO) model involving direct load control (DLC) through a differentiable decision pathway, enabling gradient-based feedback from operational outcomes to improve forecasting performance. Unlike conventional sequential approaches, the proposed method aligns forecasting accuracy with operational objectives by directly minimizing decision regret via a surrogate smart predict-then-optimize (SPO) loss function. This integration ensures that probabilistic forecasts are optimized for downstream decisions, enhancing both economic efficiency and robustness. Case studies on modified IEEE 33-bus and 69-bus systems demonstrate that the proposed framework achieves superior forecasting accuracy and operational performance, reducing total and net operation costs by up to 18% compared with conventional forecasting and optimization combinations. The results verify the effectiveness and scalability of the end-to-end decision-focused approach for resilient and cost-efficient microgrid management under uncertainty.

</details>


### [642] [Iterative Tuning of Nonlinear Model Predictive Control for Robotic Manufacturing Tasks](https://arxiv.org/abs/2512.13170)
*Deepak Ingole,Valentin Bhend,Shiva Ganesh Murali,Oliver Dobrich,Alisa Rupenayan*

Main category: cs.RO

Relevance: 15.0

TL;DR: 提出了一种基于迭代学习的非线性模型预测控制(NMPC)权重矩阵自动调优框架，利用任务级性能反馈自适应调整控制权重，在重复机器人任务中实现高效在线调优。


<details>
  <summary>Details</summary>
Motivation: 制造过程常受环境漂移和系统磨损影响，即使重复操作也需要重新调整控制参数。现有方法如贝叶斯优化需要大量离线评估，而基于梯度的方法需要复杂的NMPC求解器微分。

Method: 受范数最优迭代学习控制启发，构建经验灵敏度矩阵实现结构化权重更新，无需解析导数。通过任务重复迭代自适应调整NMPC权重Q和R，最小化跟踪精度、控制努力和饱和等关键性能指标。

Result: 在UR10e机器人碳纤维缠绕任务仿真中，仅需4次在线重复即达到接近最优跟踪性能(RMSE在离线贝叶斯优化的0.3%内)，相比BO算法需要100次离线评估。

Conclusion: 该方法为重复机器人任务中的自适应NMPC调优提供了实用解决方案，结合了精心优化控制器的精度和在线适应的灵活性。

Abstract: Manufacturing processes are often perturbed by drifts in the environment and wear in the system, requiring control re-tuning even in the presence of repetitive operations. This paper presents an iterative learning framework for automatic tuning of Nonlinear Model Predictive Control (NMPC) weighting matrices based on task-level performance feedback. Inspired by norm-optimal Iterative Learning Control (ILC), the proposed method adaptively adjusts NMPC weights Q and R across task repetitions to minimize key performance indicators (KPIs) related to tracking accuracy, control effort, and saturation. Unlike gradient-based approaches that require differentiating through the NMPC solver, we construct an empirical sensitivity matrix, enabling structured weight updates without analytic derivatives. The framework is validated through simulation on a UR10e robot performing carbon fiber winding on a tetrahedral core. Results demonstrate that the proposed approach converges to near-optimal tracking performance (RMSE within 0.3% of offline Bayesian Optimization (BO)) in just 4 online repetitions, compared to 100 offline evaluations required by BO algorithm. The method offers a practical solution for adaptive NMPC tuning in repetitive robotic tasks, combining the precision of carefully optimized controllers with the flexibility of online adaptation.

</details>


### [643] [MicroPhaseNO: Adapting an Earthquake-Trained Phase Neural Operator for Microseismic Phase Picking](https://arxiv.org/abs/2512.13197)
*Ayrat Abdullin,Umair bin Waheed,Leo Eisner,Naveed Iqbal*

Main category: physics.geo-ph

Relevance: 15.0

TL;DR: 本研究提出了一种将预训练的地震相位拾取模型PhaseNO迁移到微地震监测任务的方法，通过仅使用200个标注样本进行微调，显著提升了在微地震数据集上的性能。


<details>
  <summary>Details</summary>
Motivation: 传统深度学习地震拾取器通常针对高信噪比、长持续时间网络优化，难以处理微地震数据集（时间有限、无先前检测到的地震活动）。需要一种能够适应微地震监测特定需求的方法。

Method: 采用迁移学习方法，从在57,000多个三分量地震和噪声记录上预训练的PhaseNO模型出发，仅使用200个水力压裂诱发事件的标注和噪声地震图进行微调，保留从丰富地震数据中学到的时空表示，同时适应微地震相位特征和标注惯例。

Result: 在三个不同网络几何结构和采集参数的真实微地震数据集上评估，相比原始PhaseNO和传统工作流程，微调模型将F1分数和准确率提升高达30%，显著减少系统性时间偏差和拾取不确定性。

Conclusion: 该方法通过小规模特定任务校准集实现模型适应，可轻松迁移到其他微地震任务，为微地震监测提供了高效解决方案。

Abstract: Seismic phase picking is very often used for microseismic monitoring and subsurface imaging. Traditional manual processing is not feasible for either real-time applications or large arrays. Deep learning-based pickers trained on large earthquake catalogs offer an automated alternative. However, they are typically optimized for high signal-to-noise, long-duration networks and struggle with the challenges presented by microseismic datasets, which are purpose-built for limited time without previously detected seismicity. In this study, we demonstrate how a network-wide earthquake phase picker, the Phase Neural Operator (PhaseNO), can be adapted to microseismic monitoring using transfer learning. Starting from a PhaseNO model pre-trained on more than 57,000 three-component earthquake and noise records, we fine-tune the model using only 200 labeled and noise seismograms from induced events in hydraulic-fracturing settings. The fine-tuned model thus preserves the rich spatio-temporal representation learned from abundant earthquake data, while adapting to the characteristics and labeling conventions of microseismic phases, which are often picked on peaks or troughs rather than onsets. We evaluate performance on three distinct real-world microseismic datasets with different network geometries and acquisition parameters. Compared to the original PhaseNO and a conventional workflow, the adapted model increases F1 score and accuracy by up to 30%, and strongly reduces systematic timing bias and pick uncertainty. Because the adaptation relies on a small, campaign-specific calibration set, the approach is readily transferable to other microseismic tasks where public earthquake data and pre-trained models are accessible. The associated code will be released openly at https://github.com/ayratabd/MicroPhaseNO.

</details>


### [644] [Real-Time AI-Driven Milling Digital Twin Towards Extreme Low-Latency](https://arxiv.org/abs/2512.13482)
*Wenyi Liu,R. Sharma,W. "Grace" Guo,J. Yi,Y. B. Guo*

Main category: eess.SY

Relevance: 15.0

TL;DR: 本文对数字孪生（DT）在铣削加工中的应用进行了前沿分析，探讨了物理铣削虚拟模型、数据流和反馈控制等关键方面，并通过案例研究展示了机器学习驱动的实时DT在刀具-工件接触监测中的变革能力。


<details>
  <summary>Details</summary>
Motivation: 数字孪生通过整合实时数据、AI模型和智能控制系统实现智能制造，本文旨在分析DT在铣削加工这一特定领域的应用现状，探索实现工业4.0目标的关键技术路径。

Method: 采用文献综述和案例研究方法，从三个维度分析DT：物理铣削的虚拟模型、从物理铣削到虚拟模型的数据流、从虚拟模型到物理铣削的反馈。重点分析实时数据流协议和虚拟建模方法，并通过一个实时机器学习驱动的铣削过程刀具-工件接触DT案例进行验证。

Result: 展示了实时机器学习驱动的数字孪生在铣削过程中监测刀具-工件接触的变革能力，证明了DT技术在实际工业应用中的可行性和有效性，为智能制造提供了具体实现方案。

Conclusion: 数字孪生是实现工业4.0及更高目标的关键技术，特别是在铣削加工领域具有重要应用价值。未来研究应继续推进实时数据集成、AI模型优化和智能控制系统的开发。

Abstract: Digital twin (DT) enables smart manufacturing by leveraging real-time data, AI models, and intelligent control systems. This paper presents a state-of-the-art analysis on the emerging field of DTs in the context of milling. The critical aspects of DT are explored through the lens of virtual models of physical milling, data flow from physical milling to virtual model, and feedback from virtual model to physical milling. Live data streaming protocols and virtual modeling methods are highlighted. A case study showcases the transformative capability of a real-time machine learning-driven live DT of tool-work contact in a milling process. Future research directions are outlined to achieve the goals of Industry 4.0 and beyond.

</details>


### [645] [Enhancing lithological interpretation from petrophysical well log of IODP expedition 390/393 using machine learning](https://arxiv.org/abs/2512.13529)
*Raj Sahu,Saumen Maiti*

Main category: physics.geo-ph

Relevance: 15.0

TL;DR: 该研究开发了监督和无监督机器学习算法，用于联合分析海洋钻探计划(IODP)的测井数据，以增强岩性解释。决策树和梯度提升模型表现最佳，准确率达到0.9950。


<details>
  <summary>Details</summary>
Motivation: 传统测井解释方法存在局限性：1) 岩心和岩屑信息收集成本高，难以在每个深度获取；2) 传统统计方法多为线性，难以区分岩性和岩相；3) 井况差时传统方法受限；4) 当测井信号重叠且岩石类型结构组成变化时，传统方法效果不佳。

Method: 开发了多种监督和无监督机器学习算法：逻辑回归、决策树、梯度提升、支持向量机(SVM)、k-近邻(KNN)和多层感知器(MLP)神经网络。无监督ML提供聚类信息支持分类算法，监督ML用于构建数据驱动的岩性聚类机制。

Result: 决策树和梯度提升模型表现最优，准确率达到0.9950，F1分数为0.9951。联合ML方法为IODP数据集建立了有效的岩性聚类机制。

Conclusion: 开发的联合ML方法能有效增强岩性解释，有潜力应用于全球其他海洋测井数据集的分析。

Abstract: Enhanced lithological interpretation from well logs plays a key role in geological resource exploration and mapping, as well as in geo-environmental modeling studies. Core and cutting information is useful for making sound interpretations of well logs; however, these are rarely collected at each depth due to high costs. Moreover, well log interpretation using traditional methods is constrained by poor borehole conditions. Traditional statistical methods are mostly linear, often failing to discriminate between lithology and rock facies, particularly when dealing with overlapping well log signals characterized by the structural and compositional variation of rock types. In this study, we develop multiple supervised and unsupervised machine learning algorithms to jointly analyze multivariate well log data from Integrated Ocean Drilling Program (IODP) expeditions 390 and 393 for enhanced lithological interpretations. Among the algorithms, Logistic Regression, Decision Trees, Gradient Boosting, Support Vector Machines (SVM), k-Nearest Neighbors (KNN), and Multi-Layer Perceptron (MLP) neural network models, the Decision Tree and Gradient Boosting models outperformed the others, achieving an accuracy of 0.9950 and an F1-score of 0.9951. While unsupervised machine learning (ML) provides the foundation for cluster information that inherently supports the classification algorithm, supervised ML is applied to devise a data-driven lithology clustering mechanism for IODP datasets. The joint ML-based method developed here has the potential to be further explored for analyzing other well log datasets from the world's oceans.

</details>


### [646] [Adaptive Sampling for Hydrodynamic Stability](https://arxiv.org/abs/2512.13532)
*Anshima Singh,David J. Silvester*

Main category: physics.flu-dyn

Relevance: 15.0

TL;DR: 提出一种自适应采样方法，用于高效检测参数化流体流动问题中的分岔边界，通过结合分类器网络和概率密度估计技术，显著减少所需的Navier-Stokes模拟次数。


<details>
  <summary>Details</summary>
Motivation: 传统流体流动稳定性分析需要大量计算昂贵的Navier-Stokes模拟来识别分岔边界。现有机器学习方法使用预选数据进行训练，缺乏自适应性，无法高效探索高维参数空间。

Method: 提出自适应采样框架：1) 分类器网络将流动参数映射到分岔概率；2) 使用KRnet（基于流的深度生成模型）进行概率密度估计，生成新样本；3) 利用分类器预测的Shannon熵作为不确定性指标，指导KRnet在熵值高的区域集中采样；4) 建立分类与生成模型之间的反馈驱动自适应学习过程。

Result: 从均匀参数分布开始，新方法能够以显著更少的Navier-Stokes模拟准确识别分岔边界，为高维稳定性分析提供了可扩展的基础。

Conclusion: 该方法通过结合分类器和生成模型的自适应采样策略，有效减少了计算流体动力学模拟的计算成本，为参数化流体流动问题的稳定性分析提供了高效解决方案。

Abstract: An adaptive sampling approach for efficient detection of bifurcation boundaries in parametrized fluid flow problems is presented herein. The study extends the machine-learning approach of Silvester (Machine Learning for Hydrodynamic Stability, arXiv:2407.09572), where a classifier network was trained on preselected simulation data to identify bifurcated and nonbifurcated flow regimes. In contrast, the proposed methodology introduces adaptivity through a flow-based deep generative model that automatically refines the sampling of the parameter space. The strategy has two components: a classifier network maps the flow parameters to a bifurcation probability, and a probability density estimation technique (KRnet) for the generation of new samples at each adaptive step. The classifier output provides a probabilistic measure of flow stability, and the Shannon entropy of these predictions is employed as an uncertainty indicator. KRnet is trained to approximate a probability density function that concentrates sampling in regions of high entropy, thereby directing computational effort towards the evolving bifurcation boundary. This coupling between classification and generative modeling establishes a feedback-driven adaptive learning process analogous to error-indicator based refinement in contemporary partial differential equation solution strategies. Starting from a uniform parameter distribution, the new approach achieves accurate bifurcation boundary identification with significantly fewer Navier--Stokes simulations, providing a scalable foundation for high-dimensional stability analysis.

</details>
